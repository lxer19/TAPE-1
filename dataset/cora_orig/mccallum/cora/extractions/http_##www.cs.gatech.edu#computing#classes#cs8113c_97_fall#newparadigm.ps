URL: http://www.cs.gatech.edu/computing/classes/cs8113c_97_fall/newparadigm.ps
Refering-URL: http://www.cs.gatech.edu/computing/classes/cs8113c_97_fall/
Root-URL: 
Title: Congestion Control for Best Effort Service:  
Abstract: why we need a new paradigm Abstract Congestion control for best effort service in the Internet and other packet-switched networks (e.g., ATM) has been intensely studied in recent years. Many different congestion control mechanisms have been proposed and analyzed. In this paper we step back from the details of the various proposals and ask a more fundamental question: what principles should guide the design and implementation of best effort service in future packet-switched commercial networks? We ask this question because the current paradigm in congestion control, which is to trust end users to follow mandated congestion control algorithms, may no longer be viable in future public networks. Instead, we believe that the fundamental paradigm for best effort congestion control should be based on two principles. The first principle is that the network should enable users to achieve good service. This requires that the network provide adequate feedback so that end users can use the available bandwidth effectively. The second principle is that the network, in delivering service to a particular user, must not count on cooperation from all other users. This requires that the network protect flows from each other by enforcing restrictions on resource usage. We believe packet scheduling is the most effective tool for this enforcement.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Brakmo, S. O'Malley, and L. Peterson, </author> <title> TCP Vegas: New Techniques for congestion detection and avoidance, </title> <booktitle> Proceedings of SIGCOMM'94, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The scheduling algorithms at the switches interact strongly with the necessary averaging intervals. A final example of implicit feedback is measurement of the end-to-end delay change as one changes the transmission rate (e.g., TCP Vegas <ref> [1] </ref> or delay-based congestion control [10]). With FQ switches, delay will be relatively constant until the fair share bandwidth allocation is reached, at which time queueing will occur and a sharp increase in delay with no increase in bandwidth will be observed by the application.
Reference: [2] <author> S. Casner and S. Deering, </author> <title> "First IETF Internet Audiocast", </title> <journal> ACM Computer Communication Review, </journal> <volume> Vol. 22, No. 3, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: These data applications run on top of the reliable transport protocol TCP [23]. Only recently, due to audio and video transmissions on the Multicast backbone (MBone) <ref> [2] </ref>, has UDP traffic become a significant factor in Internet traffic. One strength of today's Internet is TCP's extremely well-designed adaptive retransmission and congestion control mechanism (Slow-Start) that was designed and implemented by Van Jacobson in mid '80s [9].
Reference: [3] <author> Anne Charney, </author> <title> An Algorithm for Rate Allocation in a Packet-Switching Network with Feedback, </title> <type> master thesis, </type> <institution> MIT Lab for Computer Science, </institution> <month> May </month> <year> 1994, </year> <month> 17 </month>
Reference: [4] <author> A. Demers, S. Keshav, and S. Shenker. </author> <title> Analysis and Simulation of a Fair Queueing Algorithm, </title> <journal> In Journal of Internetworking: Research and Experience, </journal> <volume> 1, </volume> <pages> pp. 3-26, </pages> <year> 1990. </year> <booktitle> Also in Proc. ACM SIGCOMM '89, </booktitle> <pages> pp 3-12. </pages>
Reference-contexts: Our congestion control paradigm must be built on the following two basic principles. The first principle, which was originally articulated in connection with work on Fair Queueing (see <ref> [22, 4] </ref>), states that the adequacy of service delivered to a particular user should not depend on the detailed behavior of other users. <p> As we argue below, buffer management alone cannot provide flexible and robust control of bandwidth usage. Moreover, if we use scheduling effectively, the buffer management algorithm need not be precisely tuned. Two of the most popular scheduling algorithms are FIFO and WFQ <ref> [4] </ref>. FIFO serves packets in the order of arrival. Due to its simplicity, FIFO scheduling can be found in most of today's network implementations. <p> Worse yet, it may also lead to pathological unfairness even when all end users behave properly; this phenomena of flow segregation is described in <ref> [4] </ref> and [5]. <p> This form of clumping can cause congestion at downstream routers. Fair Queueing (FQ), or any of its rough functional equivalents such as round-robin, attempt to split the bandwidth evenly among the currently present flows; see <ref> [4, 12, 21, 8] </ref> for more extensive discussion of such scheduling algorithms. This active management of bandwidth provides each flow with a great degree of protection from other flows. FQ also avoids or reduces the packet clumping problems; because of its round-robin like behavior, FQ interleaves packets from competing flows.
Reference: [5] <author> S. Floyd and V. Jacobson, </author> <title> "On Traffic Phase Effects in Packet-Switched Gateways", Internetworking: </title> <journal> Research and Experience, </journal> <volume> Vol. 3, no. 3, </volume> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: Worse yet, it may also lead to pathological unfairness even when all end users behave properly; this phenomena of flow segregation is described in [4] and <ref> [5] </ref>.
Reference: [6] <author> S. Floyd and V. Jacobson, </author> <title> "Random Early Detection Gateways for Congestion Avoidance", </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 1, no. 4, </volume> <month> August, </month> <year> 1993. </year>
Reference-contexts: As pointed out in [9], however, end host adaptation can only be part of the story. To minimize congestion losses and provide fair services, network switches must also be engaged in the control. The recent Random Early Drop work <ref> [6] </ref>, that we describe next, is one step forward along that direction. 5.2 RED The Random Early Drop (RED) congestion control algorithm design retains the basic design of today's FIFO routers. <p> However, flows which do not implement Slow-Start can capture more than their share of bandwidth, i.e. having more packets forwarded. Therefore, this stateless scheme, by itself, does not provide effective resource usage enforcement. Reference <ref> [6] </ref> suggests that, in the presence of persistent heavy load despite RED, the switch can easily identify specific sources as offenders from the number of dropped packets, and treat them specially (e.g. dropping all of their packets)-that is, to enhance the switch with some per flow state.
Reference: [7] <author> S. Floyd, V. Jacobson, S. McCanne, C. Liu, L. Zhang, </author> <title> "A Reliable Multicast Framework for Light-weight Sessions and Application Level Framing", </title> <journal> Proc. of ACM SIGCOMM 95, Computer Communications Review, </journal> <volume> Vol. 25, No. 4, </volume> <pages> pp. 342-356, </pages> <year> 1995. </year>
Reference-contexts: Second, the diversity of applications on the Internet is ever-increasing. Best-effort service can be used for packet voice and video (such as vat and nv) and reliable multicast <ref> [7] </ref> (such as used in wb) in addition to its more traditional uses of file transfer, electronic mail, and remote login. It no longer makes sense to artificially confine users to a single acceptable congestion control algorithm when their needs differ.
Reference: [8] <author> E. Hahne, </author> <title> "Round-Robin Scheduling for Max-Min Fairness in Data Networks", </title> <journal> IEEE JSAC, </journal> <volume> Vol. 9, </volume> <pages> pp. 1024-1039, </pages> <year> 1991. </year>
Reference-contexts: This form of clumping can cause congestion at downstream routers. Fair Queueing (FQ), or any of its rough functional equivalents such as round-robin, attempt to split the bandwidth evenly among the currently present flows; see <ref> [4, 12, 21, 8] </ref> for more extensive discussion of such scheduling algorithms. This active management of bandwidth provides each flow with a great degree of protection from other flows. FQ also avoids or reduces the packet clumping problems; because of its round-robin like behavior, FQ interleaves packets from competing flows.
Reference: [9] <author> Van Jacobson, </author> <title> "Congestion Avoidance and Control", </title> <booktitle> Proceedings of SIGCOMM'88, </booktitle> <month> August </month> <year> 1988. </year>
Reference-contexts: Moreover, the widespread deployment of Unix and its variants as the operating system of choice allowed the "standard" congestion control algorithms (TCP slow-start <ref> [9] </ref>) to be almost universally deployed. We are now facing a rather dramatic shift in the nature of wide-area computer networks. First, the Internet is now publically accessible, and the user community is no longer small nor close-knit. <p> One strength of today's Internet is TCP's extremely well-designed adaptive retransmission and congestion control mechanism (Slow-Start) that was designed and implemented by Van Jacobson in mid '80s <ref> [9] </ref>. When Slow-Start was designed the Internet had a deployed base of routers with FIFO scheduling, FCFU buffer management and (as a result) drop-tail service. Thus, TCP Slow-Start uses the packet losses from the network as an implicit signal for network congestion. <p> According to our taxonomy, the approach of the current Internet can best be described as: no scheduling control of allocations, no buffer management control of allocations, implicit feedback, and cooperating end hosts. As pointed out in <ref> [9] </ref>, however, end host adaptation can only be part of the story. To minimize congestion losses and provide fair services, network switches must also be engaged in the control.
Reference: [10] <author> R. Jain, </author> <title> "A Delay-Based Approach for Congestion Avoidance in Interconnected Heterogeneous Computer Networks", </title> <journal> Computer Communications Review, </journal> <volume> Vol. 19, No. 5, </volume> <pages> pp. 56-71, </pages> <year> 1989. </year>
Reference-contexts: Moreover, the user population, or more correctly the set of host machines, is much more heterogeneous and so deployment of congestion control algorithms through the distribution of Unix code will not be sufficient to ensure widespread deployment 2 As observed by Jain <ref> [10] </ref>, congestion control really has two separate components: congestion avoidance and congestion recovery. <p> Feedback can either be explicit or implicit. Explicit feedback uses an explicit indicator or field in the packet stream to convey network status. An example of explicit feedback is the DECBit protocol <ref> [10] </ref> in which a single bit of information is conveyed by the network from the sender to the receiver where it is the receiver's responsibility to return the information to the sender. <p> The scheduling algorithms at the switches interact strongly with the necessary averaging intervals. A final example of implicit feedback is measurement of the end-to-end delay change as one changes the transmission rate (e.g., TCP Vegas [1] or delay-based congestion control <ref> [10] </ref>). With FQ switches, delay will be relatively constant until the fair share bandwidth allocation is reached, at which time queueing will occur and a sharp increase in delay with no increase in bandwidth will be observed by the application.
Reference: [11] <author> R. Jain and K. K. Ramakrishnan, </author> <title> "Congestion Avoidance in Computer Networks with a Connectionless Network Layer: Concepts, Goals, and Alternatives", </title> <booktitle> Proceedings of the Computer Networking Symposium, </booktitle> <pages> pp. 134-143, </pages> <year> 1988. </year>
Reference: [12] <author> M. Katevenis, </author> <title> "Fast Switching and Fair Control of Congested Flow in Broadband Networks", </title> <journal> IEEE JSAC, </journal> <volume> Vol. 5, </volume> <pages> pp. 1315-1326, </pages> <year> 1987. </year>
Reference-contexts: This form of clumping can cause congestion at downstream routers. Fair Queueing (FQ), or any of its rough functional equivalents such as round-robin, attempt to split the bandwidth evenly among the currently present flows; see <ref> [4, 12, 21, 8] </ref> for more extensive discussion of such scheduling algorithms. This active management of bandwidth provides each flow with a great degree of protection from other flows. FQ also avoids or reduces the packet clumping problems; because of its round-robin like behavior, FQ interleaves packets from competing flows.
Reference: [13] <author> ITU-T, </author> <title> Study Group 13, "Draft Recomendation I.371," </title> <address> Geneva, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Therefore, it is in the context of ATM that the first attempt at writing down a service specification for a best effort traffic class is being made. As of the fall of 1995 the first phase of this work is mostly done <ref> [13] </ref>, but considerable detail remains to be finalized. The resulting service model has sufficient generality to be useful to the Internet community. The service model described below served as the basic input to the development of the ATM service model described in I.371 [13] but is less formal than the I.371 <p> phase of this work is mostly done <ref> [13] </ref>, but considerable detail remains to be finalized. The resulting service model has sufficient generality to be useful to the Internet community. The service model described below served as the basic input to the development of the ATM service model described in I.371 [13] but is less formal than the I.371 text. We now describe the broad outline of a service model for best effort traffic. The first issue to consider in a service model is the requirements of the applications which will use best effort services.
Reference: [14] <author> S. Keshav, </author> <title> A Control-Theoretic Approach to Flow Control, </title> <booktitle> Proceedings of SIG-COMM'91, </booktitle> <month> September </month> <year> 1991, </year> <pages> pp 3-15. </pages>
Reference-contexts: The most well-known example of implicit feedback is the TCP slow-start algorithm which uses packet drops as implicit feedback of congestion. Two other examples of implicit feedback are the packet pair protocol <ref> [14] </ref> which uses the absolute delay dispersions of packets exiting the network as feedback on the maximum rate at which the network could provide service, and NETBLT which compares the observed throughput with the transmission rate to determine the maximum achievable throughput [18]. <p> Effectively the mechanism takes the packet pair protocol of Keshav <ref> [14] </ref>, which uses implicit feedback (dispersion of packets, packets in transit) to allow the destination to calculate the bottleneck rate and the buffer occupancy in the network, and makes what had been implicit explicit. This explicit data is carried by special ATM cells called Resource Management (RM) cells. <p> The combination of bottleneck rate and bottleneck buffer occupancy allows the end adjustment to carefully control the bottleneck queue length, thus ensuring that neither packets are lost due to queue overflows nor that the user loses throughput by letting the bottleneck queue go empty. Because <ref> [14] </ref> did not address buffer management (although it was clearly understood to be an issue), it was not possible to show robustness and stability in the presence of ill-behaved users.
Reference: [15] <author> H. T. Kung, T. Blackwell, and A. Chapman, </author> <title> Credit-Based Flow Control for ATM Networks: Credit Update Protocol, Adaptive Credit Allocation, </title> <booktitle> and Statistical Multiplexing Proceedings of SIGCOMM'94, </booktitle> <month> August </month> <year> 1994, </year> <pages> pp 101-115. </pages>
Reference-contexts: A bit setting scheme based on per flow measure has been proposed as a fix to this unfairness [25]. But even with that fix, the entire control scheme relies on cooperation from all end hosts. 5.4 Hop-by-Hop Credit Scheme Hop-by-hop credit <ref> [15, 16, 17] </ref> differs from the three examples above in three important ways. First, hop-by-hop credit bases its controls and feedback on the amount of space left in switch buffers. Feedback is explicit and multivalued: the number of unused units worth of buffering. <p> Hop-by-hop credit schemes require per-flow queuing to avoid both head of line blocking and deadlock. Once per-flow queuing is in place then a method of scheduling is required; the algorithms in <ref> [15, 16, 17] </ref> use round-robin scheduling. A side effect of this scheduling is that flows are given fair service. Hop-by-hop credit schemes must do explicit buffer management since the feedback is in terms of the space left in the buffer. <p> Reference [17] describes a scheme in which each flow is given a static allocation of buffer memory. Unfortunately, such a static allocation, while simple, leads to large memory requirements in the wide area. Subsequent schemes have been developed <ref> [15, 16] </ref> which do dynamic allocation of buffer space. Dynamic allocation seems to require knowledge of the rate at which a connection is being serviced. It also introduces 15 statistical sharing of buffers and therefore the possibility of loss. There are two potential issues with hop-by-hop credit schemes.
Reference: [16] <author> H. T. Kung and K. Chang, </author> <title> Receiver-Oriented Adaptive Buffer Allocation in Credit-Based Flow Control for ATM Networks Proceedings of Infocom '95, </title> <month> April </month> <year> 1995, </year> <pages> pp 239-252. </pages>
Reference-contexts: A bit setting scheme based on per flow measure has been proposed as a fix to this unfairness [25]. But even with that fix, the entire control scheme relies on cooperation from all end hosts. 5.4 Hop-by-Hop Credit Scheme Hop-by-hop credit <ref> [15, 16, 17] </ref> differs from the three examples above in three important ways. First, hop-by-hop credit bases its controls and feedback on the amount of space left in switch buffers. Feedback is explicit and multivalued: the number of unused units worth of buffering. <p> Hop-by-hop credit schemes require per-flow queuing to avoid both head of line blocking and deadlock. Once per-flow queuing is in place then a method of scheduling is required; the algorithms in <ref> [15, 16, 17] </ref> use round-robin scheduling. A side effect of this scheduling is that flows are given fair service. Hop-by-hop credit schemes must do explicit buffer management since the feedback is in terms of the space left in the buffer. <p> Reference [17] describes a scheme in which each flow is given a static allocation of buffer memory. Unfortunately, such a static allocation, while simple, leads to large memory requirements in the wide area. Subsequent schemes have been developed <ref> [15, 16] </ref> which do dynamic allocation of buffer space. Dynamic allocation seems to require knowledge of the rate at which a connection is being serviced. It also introduces 15 statistical sharing of buffers and therefore the possibility of loss. There are two potential issues with hop-by-hop credit schemes.
Reference: [17] <author> C. Ozveren, R. Simcoe, and G. Varghese, </author> <title> Reliable and Efficient Hop-by-Hop Flow Control Proceedings of SIGCOMM'94, </title> <month> August </month> <year> 1994, </year> <pages> pp 89-100. </pages>
Reference-contexts: A bit setting scheme based on per flow measure has been proposed as a fix to this unfairness [25]. But even with that fix, the entire control scheme relies on cooperation from all end hosts. 5.4 Hop-by-Hop Credit Scheme Hop-by-hop credit <ref> [15, 16, 17] </ref> differs from the three examples above in three important ways. First, hop-by-hop credit bases its controls and feedback on the amount of space left in switch buffers. Feedback is explicit and multivalued: the number of unused units worth of buffering. <p> Hop-by-hop credit schemes require per-flow queuing to avoid both head of line blocking and deadlock. Once per-flow queuing is in place then a method of scheduling is required; the algorithms in <ref> [15, 16, 17] </ref> use round-robin scheduling. A side effect of this scheduling is that flows are given fair service. Hop-by-hop credit schemes must do explicit buffer management since the feedback is in terms of the space left in the buffer. <p> A side effect of this scheduling is that flows are given fair service. Hop-by-hop credit schemes must do explicit buffer management since the feedback is in terms of the space left in the buffer. Reference <ref> [17] </ref> describes a scheme in which each flow is given a static allocation of buffer memory. Unfortunately, such a static allocation, while simple, leads to large memory requirements in the wide area. Subsequent schemes have been developed [15, 16] which do dynamic allocation of buffer space.
Reference: [18] <author> M. Lambert, </author> <title> "An End-Point Adaptive Rate Control Strategy for the NETBLT Protocol", </title> <type> preprint, </type> <year> 1988. </year>
Reference-contexts: of implicit feedback are the packet pair protocol [14] which uses the absolute delay dispersions of packets exiting the network as feedback on the maximum rate at which the network could provide service, and NETBLT which compares the observed throughput with the transmission rate to determine the maximum achievable throughput <ref> [18] </ref>. We now discuss the implicit and explicit approaches in more detail. 3.3.1 Implicit Feedback Packet dropping is the most common form of implicit signal. The dropping of a packet does not necessarily have to be an indication of buffer exhaustion.
Reference: [19] <author> Bryan Lyles, </author> <title> editor, </title> <note> ABR Baseline Document T1S1.5/94-005R3, revision R3 dated October 14, 1994 Releases of the document prior to October 14, 1994 were entitled Class-Y Baseline Document 18 </note>
Reference-contexts: The users can only achieve good performance when they participate in the closed loop control system. Of course, a full service model will involve much more detailed specifications. Interested readers are referred to <ref> [19] </ref> for the evolving ABR service definition in the ATM community. 4 Fairness does not necessarily imply equal portions. Weighted fairness provides network operators the ability to apportion bandwidth on the basis of policy considerations. I.371 uses the term "defined allocation policy" instead of "fairness" to emphasize this point.
Reference: [20] <author> Bryan Lyles, Arthur Lin, </author> <title> A Class-Y mechanism and preliminary simulations T1S1.5/94--207, </title> <address> July 11-15, 1994, St. Louis, Mo. </address> <year> 1981. </year>
Reference-contexts: everyone adjusting to the slowest receiver, leaving no option for the application to decide on a different policy. 5.5 A strawman proposal In a contribution to ANSI standards committee T1S1, one of the authors of this paper describes a mechanism that was built using the insights described in this paper <ref> [20] </ref>. This mechanism uses WFQ as the scheduling mechanism, buffer allocation based on the user's allocated bandwidth, forward explicit feedback of the bottleneck rate and buffer utilization, and an end system adjustment which tracks the bottleneck rate but allows for overly full buffers to drain. <p> Because [14] did not address buffer management (although it was clearly understood to be an issue), it was not possible to show robustness and stability in the presence of ill-behaved users. While the scheme in <ref> [20] </ref> is not known to be optimal in any sense of the word, it does provide near 100% link utilization without packet loss for the configurations and loads for which it has been simulated.
Reference: [21] <author> S. Morgan, </author> <title> "Queueing Disciplines and Passive Congestion Control in Byte-Stream Networks", </title> <booktitle> Infocom `89, </booktitle> <pages> pp. 771-720, </pages> <year> 1989. </year>
Reference-contexts: This form of clumping can cause congestion at downstream routers. Fair Queueing (FQ), or any of its rough functional equivalents such as round-robin, attempt to split the bandwidth evenly among the currently present flows; see <ref> [4, 12, 21, 8] </ref> for more extensive discussion of such scheduling algorithms. This active management of bandwidth provides each flow with a great degree of protection from other flows. FQ also avoids or reduces the packet clumping problems; because of its round-robin like behavior, FQ interleaves packets from competing flows.
Reference: [22] <author> J. Nagle, </author> <title> "On Packet Switches with Infinite Storage", </title> <journal> IEEE Transactions on Communications, </journal> <volume> Vol. 35, </volume> <pages> pp. 435-438, </pages> <year> 1987. </year>
Reference-contexts: Our congestion control paradigm must be built on the following two basic principles. The first principle, which was originally articulated in connection with work on Fair Queueing (see <ref> [22, 4] </ref>), states that the adequacy of service delivered to a particular user should not depend on the detailed behavior of other users.
Reference: [23] <author> J. Postel. </author> <title> DoD Standard Transmission Control Protocol. </title> <institution> Network Information Center RFC-793, SRI International, </institution> <month> September </month> <year> 1981. </year>
Reference-contexts: These data applications run on top of the reliable transport protocol TCP <ref> [23] </ref>. Only recently, due to audio and video transmissions on the Multicast backbone (MBone) [2], has UDP traffic become a significant factor in Internet traffic.
Reference: [24] <author> K. K. Ramakrishnan and R. Jain, </author> <title> "A Binary Feedback Scheme for Congestion Avoidance in Computer Networks", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 8, </volume> <pages> pp. 158-181, </pages> <year> 1990. </year>
Reference-contexts: RED does not assure fair share of resources. With the enhancement of per flow state for misbehaving hosts, RED may function adequately as long as the vast majority of users adopt the same congestion response algorithm. 5.3 DEC-bit The DECbit congestion control scheme predates Slow-Start <ref> [24] </ref>. It is an end-to-end, explicit feedback control scheme with a binary value signal, called the congestion indication bit. The goal of DECbit design is congestion prevention.
Reference: [25] <author> K. K. Ramakrishnan, D. M. Chiu, and R. Jain, </author> <title> "Congestion Avoidance in Computer Networks with a Connectionless Network Layer Part IV: A Selective Binary Feedback Scheme for General Topologies", </title> <type> DEC Technical REport DEC-TR-510, </type> <year> 1987. </year>
Reference-contexts: Through simulation the DECbit designers noticed unfair service due to the stateless control even when all end hosts obey the control rules. A bit setting scheme based on per flow measure has been proposed as a fix to this unfairness <ref> [25] </ref>. But even with that fix, the entire control scheme relies on cooperation from all end hosts. 5.4 Hop-by-Hop Credit Scheme Hop-by-hop credit [15, 16, 17] differs from the three examples above in three important ways.
Reference: [26] <author> S. Shenker, </author> <title> "A Theoretical Analysis of Feedback Flow Control", </title> <booktitle> Proceedings of SIG-COMM'90, </booktitle> <year> 1990. </year>
Reference-contexts: The second reason is that there seems to be a deep link between network stability and fairness. While it is not known if fairness is a necessary condition for stability, it is known to be a sufficient condition <ref> [26] </ref>. Procedural assurances tell the users what they can expect if they respond to the network's congestion signals appropriately. Although the absence of admission control makes it impossible to assure quantitative delay or throughput in service, there are still possibilities to control packet losses.
Reference: [27] <author> S. Shenker, </author> <title> "Fundamental Design Issues for the Future Internet", </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> Vol. 13, no. 7, </volume> <month> September, </month> <year> 1995. </year>
Reference-contexts: This leads to a major difference between best effort service and previously existing telecommunications circuit services: the bandwidth available to the user, and the delivery delay of their packets, varies moment to moment. More importantly, there should not be an admission control to this service <ref> [27] </ref>. Consequently, there should not be any quantitative bounds to the service; there are no lower bounds on bandwidth and no upper bounds on delay.
Reference: [28] <author> S. Shenker, L. Zhang, and D. Clark. </author> <title> Some Observations on the Dynamics of a Congestion Control Algorithm, </title> <journal> In ACM Computer Communications Review, </journal> <volume> 20(4), </volume> <pages> pp. 30-39, </pages> <month> October, </month> <year> 1990. </year>
Reference: [29] <author> C. Turner and L. Peterson, </author> <title> "Image Transfer: An End-to-End Design", </title> <booktitle> Proceedings of SIGCOMM'92, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: For instance, if an application is designed to tolerate high losses (e.g. the image transfer protocol designed by Turner and Peterson <ref> [29] </ref>), it is unlikely to react to feedback in order to reduce the loss rate as quickly or aggressively as a loss-sensitive application would. Beyond issues of loss, best effort multicast service also raises a new issue: how to handle a multi-valued feedback.
Reference: [30] <author> R. Wilder, K. K. Ramakrishnan, and A. </author> <title> Mankin Dynamics of a Congestion Control and Avoidance of Two-Way Traffic in an OSI Testbed, </title> <journal> ACM Computer Communications Review, </journal> <volume> Vol. 21, No. 2, </volume> <month> April </month> <year> 1991. </year>
Reference: [31] <author> L. Zhang and D. Clark. </author> <title> Oscillating Behavior of Network Traffic: A Case Study Simulation, </title> <journal> In Journal of Internetworking: Research and Experience, </journal> <volume> 1, </volume> <pages> pp. 101-112, </pages> <year> 1990. </year>
Reference: [32] <author> L. Zhang, S. Shenker, and D. Clark., </author> <title> Observations on the Dynamics of a Congestion Control Algorithm: The Effects of Two-Way Traffic, </title> <booktitle> Proceedings of SIGCOMM'91, </booktitle> <year> 1991. </year>
References-found: 32

