URL: http://www.cs.monash.edu.au/~korb/causal-disc.ps.Z
Refering-URL: http://www.cs.monash.edu.au/~korb/
Root-URL: 
Email: fcsw; korb; daig@bruce.cs.monash.edu.au  
Title: Causal Discovery via MML  
Author: Chris Wallace, Kevin B. Korb and Honghua Dai 
Keyword: Causal discovery, minimum message length, MML induction, Bayesian learning, causal modeling, inductive inference, machine learning.  
Date: February 1, 1996  
Address: Clayton, Victoria 3168 AUSTRALIA  
Affiliation: Department of Computer Science, Monash University  
Abstract: Automating the learning of causal models from sample data is a key step toward incorporating machine learning in the automation of decision-making and reasoning under uncertainty. This paper presents a Bayesian approach to the discovery of causal models, using a Minimum Message Length (MML) method. We have developed encoding and search methods for discovering linear causal models. The initial experimental results presented in this paper show that the MML induction approach can recover causal models from generated data which are quite accurate reflections of the original models; our results compare favorably with those of the TETRAD II program of Spirtes et al. [25] even when their algorithm is supplied with prior temporal information and MML is not. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. B. Asher. </author> <title> Causal Modeling. </title> <publisher> Sage Publications, </publisher> <address> Beverly Hills, </address> <year> 1983. </year>
Reference-contexts: It's clear that the MML model is perferable to all of the TETRAD II models. (a) Original Model (b) MML Induction (c) TETRAD II (1) (d) TETRAD II (2) (e) TETRAD II (3) 4.5 Miller and Stokes' Model The Miller and Stokes model <ref> [1] </ref> (p. 45) of the voting behavior of Congressional representatives is shown in Figure 6 (a).
Reference: [2] <author> P. Blau and O. Duncan. </author> <title> The American Occupational Structure. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1967. </year>
Reference-contexts: (a partial order grouping variables into tiers, indicated parenthetically per case study below) and finally the TETRAD II model (s) returned when no prior temporal information was made available. 4.1 Blau and Duncan's Model The Blau and Duncan model of Figure 2 (a) is their stratification process model of occupation <ref> [2] </ref>.
Reference: [3] <author> Graham Brightwell and Peter Winkler. </author> <title> Counting linear extensions is #p-complete. </title> <booktitle> In Proc. of the 23rd Annual ACM Symposium on the Theory of Computing, </booktitle> <address> New Orleans, LA, </address> <year> 1991. </year>
Reference-contexts: Hence, L 1 = log K! + 2 3 Unfortunately, counting the number of linear extensions of a DAG is known to be an NP--hard problem <ref> [3] </ref>. There are efficient means of producing an upper bound to M [12], and we are investigating its use to provide an estimate for the value of M.
Reference: [4] <author> David M. Chickering. </author> <title> A transformational characterization of equivalent Bayesian network structures. </title> <booktitle> In Proc. of the 11th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 87-98, </pages> <year> 1995. </year>
Reference-contexts: Chickering has extended the Verma and Pearl criterion of statistical equivalence by converting it into a topological criterion that is even simpler to apply <ref> [4] </ref>: Theorem 2 (Chickering, 1995) Two causal structures are statistically equivalent if and only if there exists a sequence of covered arc reversals that transforms one into the other. A covered arc is one between two nodes which share all parents, excepting the two nodes in question. <p> Another way of noticing the effect of statistical equivalence is to examine the expected likelihoods of models relative to test data, reported in Table 2. Chickering <ref> [4] </ref> proves that, among other such measures, likelihood measures across statistically equivalent models must be identical. The likelihoods calculated for Table 2 are, of course, calculated for fully specified models|that is, using the estimated parameters.
Reference: [5] <author> G. F. Cooper and E. Herskovits. </author> <title> A Bayesian method for constructing Bayesian belief networks from databases. </title> <booktitle> In Proc. of 7th Conference on Uncertainty in AI. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Bayesian network technology, despite being only a decade old [19, 17], has already been applied to a wide variety of tasks involving reasoning under uncertainty. Even more recently the interest that has developed in the use of Bayesian networks has transferred to the learning of Bayesian networks <ref> [5, 11, 14] </ref>|which is very natural given the known difficulties and limitations of knowledge acquisition techniques, and especially the difficulties of eliciting probability estimates from domain experts [6].
Reference: [6] <author> Daniel Kahneman, Paul Slovic and Amos Tversky. </author> <title> Judgment under Uncertainty: Heuristics and Biases. </title> <address> Cambridge, </address> <year> 1982. </year>
Reference-contexts: Even more recently the interest that has developed in the use of Bayesian networks has transferred to the learning of Bayesian networks [5, 11, 14]|which is very natural given the known difficulties and limitations of knowledge acquisition techniques, and especially the difficulties of eliciting probability estimates from domain experts <ref> [6] </ref>.
Reference: [7] <author> Otis Duncan. </author> <title> Introduction to Structural Equation Models. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1975. </year>
Reference-contexts: The social sciences have developed over the course of the century a battery of statistical methods for studying causal models of social phenomena, including factor analysis [10], path analysis [30], and structural equation modeling <ref> [7, 15] </ref>. The causal models so studied are more limited than Bayesian networks: effect variables are strictly additive, linear functions of exogenous variables.
Reference: [8] <author> Peter B. Evans and Michael Timberlake. </author> <title> Dependence, inequality, and the growth of the tertiary: a comparative analysis of less developed countries. </title> <journal> American Sociological Review, </journal> <volume> 45(August):531-552, </volume> <year> 1980. </year>
Reference: [9] <author> Clark Glymour, Richard Scheines, Peter Spirtes, and Kevin Kelly. </author> <title> Discovering Causal Structure: </title> <booktitle> Artificial Intelligence, Philosophy of Science, and Statistical Modeling. </booktitle> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1987. </year>
Reference-contexts: Despite the centrality of the issues for artificial intelligence, there has been only one substantial research program aimed at the automated learning of such linear causal models, which is that of Clark Glymour, Spirtes et al. at Carnegie Mellon University, underway for the past decade (see <ref> [9] </ref> and [24]). Their approach has shown some successes, leading to a commercially available program TETRAD II [25].
Reference: [10] <author> H.H. Harman. </author> <title> Modern Factor Analysis. </title> <institution> University of Chicago, </institution> <address> Chicago, third edition, </address> <year> 1976. </year>
Reference-contexts: The social sciences have developed over the course of the century a battery of statistical methods for studying causal models of social phenomena, including factor analysis <ref> [10] </ref>, path analysis [30], and structural equation modeling [7, 15]. The causal models so studied are more limited than Bayesian networks: effect variables are strictly additive, linear functions of exogenous variables.
Reference: [11] <author> David Heckerman, Dan Geiger, and David M. Chickering. </author> <title> Learning Bayesian networks: The combination of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20(3) </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Bayesian network technology, despite being only a decade old [19, 17], has already been applied to a wide variety of tasks involving reasoning under uncertainty. Even more recently the interest that has developed in the use of Bayesian networks has transferred to the learning of Bayesian networks <ref> [5, 11, 14] </ref>|which is very natural given the known difficulties and limitations of knowledge acquisition techniques, and especially the difficulties of eliciting probability estimates from domain experts [6]. <p> seed model can be used to encode a user's best guesses about causal connections between variables, giving the algorithm an initial bias in its search that may greatly simplify finding the optimal model; this is the method adopted by Heckerman et al. in their Bayesian network learning algorithm, for example <ref> [11] </ref>. Indeed, stronger measures may be taken by stipulating that certain causal connections must (or must not) occur in any acceptable model (but this would entail changes to our MML encoding methods as well as the search procedure).
Reference: [12] <author> A. Karzanov and L. Khachiyan. </author> <title> On the conductance of order markov chains. </title> <type> Technical Report DCS TR 268, </type> <institution> Computer Science, Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: Hence, L 1 = log K! + 2 3 Unfortunately, counting the number of linear extensions of a DAG is known to be an NP--hard problem [3]. There are efficient means of producing an upper bound to M <ref> [12] </ref>, and we are investigating its use to provide an estimate for the value of M. In the meantime, we calculate M by brute force, with the understanding that this technique will be applicable only to modestly sized causal models.
Reference: [13] <author> Kevin B. Korb. </author> <title> Inductive learning and defeasible inference. </title> <journal> Journal of Experimental and Theoretical AI, </journal> <volume> 7 </volume> <pages> 291-324, </pages> <year> 1995. </year>
Reference-contexts: As these networks are plausibly understood (in many cases) as describing the causal structure of some physical phenomenon, the automation of their learning is potentially tantamount to the automation of scientific inductive practice| and promises, thereby, a substantial advance in solving the "AI problem" in general <ref> [13] </ref> and the Frame Problem in particular. The social sciences have developed over the course of the century a battery of statistical methods for studying causal models of social phenomena, including factor analysis [10], path analysis [30], and structural equation modeling [7, 15].
Reference: [14] <author> Wai Lam and Fahiem Bacchus. </author> <title> Learning Bayesian belief networks: An approach based on the MDL principle. </title> <journal> Computational Intelligence, </journal> <volume> 10 </volume> <pages> 269-292, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Bayesian network technology, despite being only a decade old [19, 17], has already been applied to a wide variety of tasks involving reasoning under uncertainty. Even more recently the interest that has developed in the use of Bayesian networks has transferred to the learning of Bayesian networks <ref> [5, 11, 14] </ref>|which is very natural given the known difficulties and limitations of knowledge acquisition techniques, and especially the difficulties of eliciting probability estimates from domain experts [6].
Reference: [15] <author> John C. Loehlin. </author> <title> Latent Variable Models: An Introduction to Factor, Path and Structural Analysis. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, New Jersey, </address> <note> second edition, </note> <year> 1992. </year>
Reference-contexts: The social sciences have developed over the course of the century a battery of statistical methods for studying causal models of social phenomena, including factor analysis [10], path analysis [30], and structural equation modeling <ref> [7, 15] </ref>. The causal models so studied are more limited than Bayesian networks: effect variables are strictly additive, linear functions of exogenous variables. <p> Hence, a plausible way of measuring predictive adequacy is via the residual variance in test cases. This residual is measured by: fl = N i=1 K j=1 ij (31) where N is the number of samples, K is the number of variables, u ij 2 U i . Following <ref> [15, p. 246] </ref> and [16, pp. 234-251], U i = X i AX i = (I A)X i = BX i , where A is the path coefficient matrix of a discovered causal model, I is the unit matrix, X i is a vector of sample test data with K elements
Reference: [16] <author> J. Jack McArdle and Roderick P. McDonald. </author> <title> Some algebraic properties of the reticular action model for moment structures. </title> <journal> British Journal of Mathematical and Statistical Psychology, </journal> <volume> 37 </volume> <pages> 234-251, </pages> <year> 1984. </year>
Reference-contexts: This residual is measured by: fl = N i=1 K j=1 ij (31) where N is the number of samples, K is the number of variables, u ij 2 U i . Following [15, p. 246] and <ref> [16, pp. 234-251] </ref>, U i = X i AX i = (I A)X i = BX i , where A is the path coefficient matrix of a discovered causal model, I is the unit matrix, X i is a vector of sample test data with K elements and U i =
Reference: [17] <author> Richard Neapolitan. </author> <title> Probabilistic Reasoning in Expert Systems. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction Bayesian network technology, despite being only a decade old <ref> [19, 17] </ref>, has already been applied to a wide variety of tasks involving reasoning under uncertainty.
Reference: [18] <author> J.J. Oliver and R.A. Baxter. </author> <title> MML and Bayesianism: Similarities and differences. </title> <type> Technical Report TR 94/206, </type> <institution> Computer Science, Monash University, </institution> <year> 1994. </year>
Reference-contexts: the data as a random sample from a normal distribution, the likelihood function is P (yjfi) = P (yj; fa k g) = N Y 1 2 2 2 2 (y n k a k x nk ) 2 1 We are simplifying here by ignoring a volume term; see <ref> [18] </ref>. 5 And the message length for encoding the data given the model is given by L (DjH) = lnP (datajparameters) (17) = lnP (yj; fa k g) (18) N Y 1 2 2 P K ] (19) N ln2 + Nln + i=1 i The combined message length for encoding
Reference: [19] <author> Judea Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Bayesian network technology, despite being only a decade old <ref> [19, 17] </ref>, has already been applied to a wide variety of tasks involving reasoning under uncertainty.
Reference: [20] <author> Hans Reichenbach. </author> <title> The Theory of Probability. </title> <institution> University of California, Berkeley, </institution> <note> second edition, </note> <year> 1949. </year>
Reference-contexts: This is a kind of primitive, unaided form of induction which, on the one hand, is a necessary prerequisite to more sophisticated forms of induction (see <ref> [20] </ref>) and, on the other hand, is easier to set up and examine. The drawback with experimenting with primitive induction is that it is generally much harder to get useful results from it than from similar methods that also take commonly available human background knowledge into account.
Reference: [21] <author> R. Rodgers and C. Maranto. </author> <title> Causal models of publishing productivity in psychology. </title> <journal> Journal of Applied Psychology, </journal> <volume> 66 </volume> <pages> 688-701, </pages> <year> 1989. </year>
Reference-contexts: (2) x 3 Representative attitudes (2) x 4 Representative behavior (3) the recovered causal structures, including that of MML, are statistically equivalent, which is reflected in identical expected log likelihoods. 15 (a) Original (b) MML (c) TETRAD II (d) TETRAD II 4.6 Rodgers and Maranto's Model Rodgers and Maranto's model <ref> [21] </ref> of publishing productivity among academic psychologists is shown in Figure 7 (a).
Reference: [22] <author> E.H. Simpson. </author> <title> The interpretation of interaction in contingency tables. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 13 </volume> <pages> 238-241, </pages> <year> 1951. </year>
Reference-contexts: In this case TETRAD II appears to be slightly outperforming the MML search, in particular by picking up the weak link between variables 2 and 4. Of some interest is the coincidental fact that the Fiji model contains a version of Simpson's paradox <ref> [22] </ref>. That is, in the original model the combined effect of the causal links from variables 1 to 2 to 3 almost exactly counterbalances the direct effect from variable 1 to 3, leaving a marginal correlation between 1 and 3 of only 0:0276.
Reference: [23] <author> Peter Spirtes, Clark Glymour, and Richard Scheines. </author> <title> Causality from probability. </title> <editor> In J.E. Tiles, G.T. McKee, and G.C. Dean, editors, </editor> <booktitle> Evolving Knowledge in Natural Science and Artificial Intelligence, </booktitle> <address> London, 1990. </address> <publisher> Pitman. </publisher>
Reference-contexts: Their approach has shown some successes, leading to a commercially available program TETRAD II [25]. Their methods, however, while now incorporating a number of principles based upon Judea Pearl's work (specifically, what they call Principles I and II in <ref> [23] </ref>), otherwise rely upon orthodox statistical techniques, such as significance tests, which ignore the prior probabilities of the candidate causal models. <p> Whereas many arc orientations are not statistically important (the two alternative DAGs are in many cases statistically equivalent), this is not true of v-structures (indeed, the v-structure is crucial to Spirtes et al.'s Principle II, <ref> [23] </ref>). Therefore, the error rate is a normalized function of these three types of error in the recovered model: failure to identify an existing arc; identifying a non-existent arc; misidentifying a v structure in the original model. <p> Since that temporal information forced the orientation of all the arcs of Figure 8 (c), this is not a very surprising result. TETRAD II without temporal information produced all possible trees based upon the original skeleton, namely Figures 8 (d-j). Since TETRAD II relies primarily upon Principle II <ref> [23] </ref> to find arc orientations, and since the latter applies when and only 16 (a) Original Model (b) MML Induction (c) TETRAD II (1) (d) TETRAD II (2) (e) TETRAD II (3) (f) TETRAD II (4) (g) TETRAD II (5) 17 (a) Original Model (b) MML Induction (c) TETRAD II (1)
Reference: [24] <author> Peter Spirtes, Clark Glymour, and Richard Scheines. </author> <title> Causation, Prediction, and Search. </title> <publisher> Springer-Verlag, </publisher> <address> New York, Berlin, Heideberg, </address> <year> 1993. </year>
Reference-contexts: Despite the centrality of the issues for artificial intelligence, there has been only one substantial research program aimed at the automated learning of such linear causal models, which is that of Clark Glymour, Spirtes et al. at Carnegie Mellon University, underway for the past decade (see [9] and <ref> [24] </ref>). Their approach has shown some successes, leading to a commercially available program TETRAD II [25].
Reference: [25] <author> Peter Spirtes, Clark Glymour, Richard Scheines, and C. Meek. TETRAD II: </author> <title> Tools for causal modeling. </title> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, New Jersey, </address> <year> 1994. </year>
Reference-contexts: Their approach has shown some successes, leading to a commercially available program TETRAD II <ref> [25] </ref>. Their methods, however, while now incorporating a number of principles based upon Judea Pearl's work (specifically, what they call Principles I and II in [23]), otherwise rely upon orthodox statistical techniques, such as significance tests, which ignore the prior probabilities of the candidate causal models.
Reference: [26] <author> T. Verma and J. Pearl. </author> <title> Equivalence and synthesis of causal models. </title> <booktitle> In Proceedings of the 6th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 220-227, </pages> <address> Boston, MA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Two recent theorems of utility in understanding statistically equivalent structures are: 3 Theorem 1 (Verma and Pearl, 1990) Two causal structures are statistically equivalent if and only if they have identical skeletons and v-structures <ref> [26] </ref>. V-structures refer to triples of nodes such that in the subgraph in the three nodes one of the them is the common effect of the other two and the two parents are not adjacent.
Reference: [27] <author> Chris Wallace and David Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: Here we report on initial results of our investigation into the use of Wallace's Minimum Message Length Principle (MML) <ref> [27, 29, 28] </ref> to search for and evaluate linear causal models given sample data, including experimental comparisons with TETRAD II.
Reference: [28] <author> Chris Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, B, </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: Here we report on initial results of our investigation into the use of Wallace's Minimum Message Length Principle (MML) <ref> [27, 29, 28] </ref> to search for and evaluate linear causal models given sample data, including experimental comparisons with TETRAD II. <p> Following Wallace and Freeman <ref> [28] </ref>, the message length for encoding the parameters is, within a constant, 1 L (p) = ln ( h (parameters) p ) = 2 lnF ln h (parameters) (7) where F = 2N 2 (K+1) jAj (8) is the Fisher information and where A is the data matrix (x i x
Reference: [29] <author> Chris Wallace and Michael Georgeff. </author> <title> A general selection criterion for inductive inference. </title> <booktitle> ECAI 84, Advances in Artificial Intelligence, </booktitle> <pages> pages 1-18, </pages> <year> 1984. </year>
Reference-contexts: Here we report on initial results of our investigation into the use of Wallace's Minimum Message Length Principle (MML) <ref> [27, 29, 28] </ref> to search for and evaluate linear causal models given sample data, including experimental comparisons with TETRAD II.
Reference: [30] <author> Sewall Wright. </author> <title> The method of path coefficients. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 5 </volume> <pages> 161-215, </pages> <year> 1934. </year>
Reference-contexts: The social sciences have developed over the course of the century a battery of statistical methods for studying causal models of social phenomena, including factor analysis [10], path analysis <ref> [30] </ref>, and structural equation modeling [7, 15]. The causal models so studied are more limited than Bayesian networks: effect variables are strictly additive, linear functions of exogenous variables. <p> The least squares measurement is evaluated by LS = i=1 j=1 0 2 where r ij is the sample correlation value and r 0 ij is the estimated correlation value from the derived model (see <ref> [30] </ref>). 5.
References-found: 30

