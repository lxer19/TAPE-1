URL: http://www.cs.cmu.edu/afs/cs/project/cmcl/archive/Nectar-papers/93compcom.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: Analyzing Communication Latency using the Nectar Communication Processor  
Author: Peter Steenkiste 
Note: This research was supported in part by the Defense Advanced Research Projects Agency (DOD) monitored by DARPA/CMO under Contract MDA972-90-C-0035.  
Address: Pittsburgh, Pennsylvania 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: For multicomputer applications, the most important performance parameters of a network is the latency for short messages. In this paper we present an analysis of communication latency using measurement of the Nectar system. Nectar is a high-performance multicomputer built around a high-bandwidth crosspoint network. Nodes are connected to the Nectar network using network coprocessors that are primarily responsible the protocol processing, but that can also execute application code. This architecture allows us to analyze message latency both between workstations with an outboard protocol engine and between lightweight nodes with a minimal runtime system and a fast, simple network interface (the coprocessors). We study how much context switching, buffer management and protocol processing contribute to the communication latency and we discuss how the latency is influenced by the protocol implementation. We also discuss and analyze two other network performance measures: communication overhead and throughput. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <institution> Special Report. "Gigabit Network Testbeds". IEEE Computer 23, </institution> <month> 9 (September </month> <year> 1990), </year> <pages> 77-80. </pages>
Reference-contexts: 1. Introduction Multicomputers that use existing hosts and a general network are an attractive architecture for many applications [12], and they are one of the main motivations for improving the performance of networks <ref> [1] </ref>. Some users are interested in partioning applications across a large number of workstations, while other users want to combine the resources of a smaller number of supercomputers.
Reference: 2. <author> Thomas E. Anderson, Henry M. Levy, Brian N. Bershad, and Edward D. Lazowska. </author> <title> The Interaction of Architecture and Operating System Design. </title> <booktitle> Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, ACM, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 108-121. </pages>
Reference-contexts: Unfortunately, the thread implementation and, especially, register windows make this organization expensive. The thread-based protocol implementation adds between 45 microseconds to the latency. RISC processors with a flat register file have a considerably lower thread switch overhead <ref> [2] </ref>, and we expect that the additional cost of doing protocol processing in a thread instead of an interrupt handler will be considerably lower on a processor with a flat register file. 8 3.2.2. <p> The context switch overhead is the most troublesome. Although this cost can be reduced by using a flat register file, adding caches and extending the runtime system (e.g. add virtual memory) would increase the cost of context switching. Context switching has traditionally also benefited little from increased CPU speed <ref> [2] </ref>. 4. Host-host breakup A comparison of Figures 3 and 4 shows that the behavior of the threads on the CAB is the same for the CAB-CAB and host-host roundtrip tests. As a result, we can use the CAB-CAB measurements to analyze the CAB component of the host-host roundtrip times.
Reference: 3. <author> Emmanuel Arnould, Francois Bitz, Eric Cooper, H. T. Kung, Robert Sansom and Peter Steenkiste. </author> <title> The Design of Nectar: A Network Backplane for Heterogeneous Multicomputers. </title> <booktitle> Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <address> ACM/IEEE, Boston, </address> <month> April, </month> <year> 1989, </year> <pages> pp. 205-216. </pages>
Reference-contexts: This overhead however is, or should be, of the order of 10s of microseconds [7], and does not account for the order of magnitude difference in latency. In this paper we identify other sources of overhead based on measurements collected on Nectar. The Nectar network <ref> [3, 12] </ref> consists of a high-bandwidth crosspoint network (Figure 1). Host are connected to the network through Communication Acceleration Boards (CABs) that are responsible for protocol processing.
Reference: 4. <author> D. H. Bailey, E. Barszcz, R. A. Fatoohi, H. D. Simon, and S. Weeratunga. </author> <title> Performance Results on the Intel Touchstone Gamma Prototype. </title> <booktitle> Proceedings of the Fifth Distributed Memory Computing Conference, IEEE, </booktitle> <month> April, </month> <year> 1990, </year> <pages> pp. 1236-1245. </pages>
Reference-contexts: Reducing latency has traditionally been the biggest challenge. Dedicated multicomputers with special-purpose interconnects such as the Intel Touchstone system have latencies below 100 microseconds <ref> [4] </ref>, while latencies between Unix workstations communicating over general networks are typically one order of magnitude higher. The latency between two Sun4/330 running Sun OS 4.1 is for example about 800 microseconds. One reason for the higher latency is the difference in communication medium.
Reference: 5. <author> Luis-Felipe Cabrera, Edward Hunter, Michael J. Karels, and David A. </author> <title> Mosher. "User-Process Communication Performance in Networks of Computers". </title> <journal> IEEE Transactions on Software Engineering 14, </journal> <month> 1 (January </month> <year> 1988), </year> <pages> 38-53. </pages>
Reference-contexts: Earlier work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines <ref> [5] </ref>, the performance of the Firefly RPC mechanism [17], the performance of the remote accesses over Ethernet [19], the x-kernel roundtrip time [11], and the performance of the Amoeba RPC [15]. <p> Second, the cost of buffer management for Nectar is typically higher than for the other implementations (if reported), with the exception for the TCP/UDP measurements reported in <ref> [5] </ref>. The reasons are the flexibility of the mailbox module, and the requirement that messages have to be contiguous in memory. 7. Conclusion We presented an analysis of host-host and CAB-CAB communication latency in Nectar. The CAB-CAB measurements give some insights in communication latency between nodes using light-weight runtime systems.
Reference: 6. <author> David D. Clark. </author> <title> The Structuring of Systems Using Upcalls. </title> <booktitle> Proceedings of the Tenth Symposium on Operating System Principles, ACM, </booktitle> <month> December, </month> <year> 1985, </year> <pages> pp. 171-180. </pages>
Reference-contexts: We describe the software in more detail in the remainder of 2 this section. 2.1. Threads and SPARC register windows Previous protocol implementations have demonstrated that multiple threads are useful, but multiple address spaces are unnecessary <ref> [6, 14, 16] </ref>. As a result, we designed the CAB to provide a single physical address space with a runtime system that supports multiple threads. The threads package is based on Mach C threads [9]: it provides mutex locks to enforce critical regions, and condition variables for synchronization. <p> The transport protocol performs the matching End_Put, which wakes up any waiting application thread or process. The transport protocol can be called in one of two ways. The Nectar-native protocols are invoked by an upcall inside the interrupt handler <ref> [6] </ref> (top Figure 3). This implementation was 4 motivated by speed: it avoids waking up a thread. The disadvantage is that since many data structures are accessed at interrupt time, critical regions often have to be enforced by masking interrupts.
Reference: 7. <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> "An Analysis of TCP Processing Overhead". </title> <journal> IEEE Communications Magazine 27, </journal> <month> 6 (June </month> <year> 1989), </year> <pages> 23-29. </pages>
Reference-contexts: The communication protocols that recover from these errors introduce overhead, thus adding to the latency. This overhead however is, or should be, of the order of 10s of microseconds <ref> [7] </ref>, and does not account for the order of magnitude difference in latency. In this paper we identify other sources of overhead based on measurements collected on Nectar. The Nectar network [3, 12] consists of a high-bandwidth crosspoint network (Figure 1). <p> The transport protocol cost ranges from less than 10 microseconds (datagram), to 25-30 microseconds (request-response and RMP), and 50 microseconds (UDP). The overhead for the reliable Nectar-native protocols is of the order of 200 instructions, i.e. similar to that reported in 11 the literature for optimized standard protocols <ref> [7] </ref>. UDP is a lot more expensive, mainly because it has not been optimized. It is also more general than the Nectar-native protocols, since UDP packets can travel outside Nectar. 3.5. CAB-CAB latency summary Table 5 summarizes the results of the CAB-CAB latency analysis.
Reference: 8. <author> Eric Cooper, Peter Steenkiste, Robert Sansom, and Brian Zill. </author> <title> Protocol Implementation on the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '90 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Philadelphia, </address> <month> September, </month> <year> 1990, </year> <pages> pp. 135-143. </pages>
Reference-contexts: The host-host latency over Nectar is about 200 microseconds. One of the goals of the prototype was to experiment with different protocol implementations and make the communication coprocessor customizable by applications. For this reason, we built the CAB around a general-purpose CPU with a flexible runtime system <ref> [8] </ref>. The application-level CAB-CAB latency is about 150 microseconds. The Nectar architecture allows us to study the communication between different types of hosts: traditional workstations with a powerful outboard protocol engine and ``light-weight'' hosts with basic network interface, i.e. the CABs. <p> DMA transfers are supported for data memory only but the SPARC can access both memories equally fast. The memories are built from static RAM and there is no cache. The memories are directly accessible to applications on the host. The SPARC runs a flexible runtime system <ref> [8] </ref> that provides support for multiprogramming (a threads package) and for buffering and synchronization (the mailbox module). Several communication protocols have been implemented on the CAB using these facilities: the Nectar-native datagram, reliable message (RMP) and request-response (RR) protocols, and the standard internet protocols (UDP/TCP/IP). <p> The threads package is based on Mach C threads [9]: it provides mutex locks to enforce critical regions, and condition variables for synchronization. Preemption and priorities were added so that system threads (e.g. protocol threads) can be scheduled quickly, even in the presence of long-running application threads <ref> [8] </ref>. The SPARC CPU has seven register windows and eight global registers. Each window has eight input registers, eight output registers, and eight local registers; the input registers of one window overlap with the output registers of the next window. <p> Throughput Graphs 7 and 8 show the CAB-CAB and host-host throughput as a function of the packet size using the RMP <ref> [8] </ref>. The host-host throughput with 8KByte packets is 3 MByte per second; it is limited by the VME bus. The CAB-CAB throughput with 8KByte packets is about 11 MByte per second.
Reference: 9. <author> Eric C. Cooper and Richard P. Draves. </author> <title> C Threads. </title> <type> Tech. </type> <institution> Rept. CMU-CS-88-154, Computer Science Department, Carnegie Mellon University, </institution> <month> June, </month> <year> 1988. </year>
Reference-contexts: As a result, we designed the CAB to provide a single physical address space with a runtime system that supports multiple threads. The threads package is based on Mach C threads <ref> [9] </ref>: it provides mutex locks to enforce critical regions, and condition variables for synchronization. Preemption and priorities were added so that system threads (e.g. protocol threads) can be scheduled quickly, even in the presence of long-running application threads [8]. The SPARC CPU has seven register windows and eight global registers.
Reference: 10. <institution> TURBOchannel Overview. Digital Equipment Corporation, </institution> <year> 1990. </year>
Reference-contexts: Host accesses to CAB memory take about 1.1 microseconds, and two thirds of the host time is spent reading and writing across the VME bus. Note that the time to read and write individual words across more recent IO busses such as Turbo Channel <ref> [10] </ref> is still about 1 microseconds, so single word accesses across IO busses are not getting much faster. Sending and receiving messages requires so many host accesses to CAB memory for a number of reasons. First, transport protocols are invoked ``indirectly'' through mailboxes, a general-purpose mechanism for host-CAB communication.
Reference: 11. <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> Implementing Protocols in the x-kernel. </title> <type> Tech. </type> <institution> Rept. 89-1, University of Arizona, </institution> <month> January, </month> <year> 1989. </year>
Reference-contexts: Earlier work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines [5], the performance of the Firefly RPC mechanism [17], the performance of the remote accesses over Ethernet [19], the x-kernel roundtrip time <ref> [11] </ref>, and the performance of the Amoeba RPC [15]. Since all these systems are organized differently and often use different protocols, it is hard to compare the results, but we can make some general observations.
Reference: 12. <author> H.T. Kung, Robert Sansom, Steven Schlick, Peter Steenkiste, Matthieu Arnould, Francois J. Bitz, Fred Christianson, Eric C. Cooper, Onat Menzilcioglu, Denise Ombres, and Brian Zill. </author> <title> Network-Based Multicomputers: An Emerging Parallel Architecture. </title> <booktitle> Proceedings of Supercomputing '91, IEEE, </booktitle> <address> Albequerque, </address> <month> November, </month> <year> 1991, </year> <pages> pp. 664-673. 19 </pages>
Reference-contexts: 1. Introduction Multicomputers that use existing hosts and a general network are an attractive architecture for many applications <ref> [12] </ref>, and they are one of the main motivations for improving the performance of networks [1]. Some users are interested in partioning applications across a large number of workstations, while other users want to combine the resources of a smaller number of supercomputers. <p> This overhead however is, or should be, of the order of 10s of microseconds [7], and does not account for the order of magnitude difference in latency. In this paper we identify other sources of overhead based on measurements collected on Nectar. The Nectar network <ref> [3, 12] </ref> consists of a high-bandwidth crosspoint network (Figure 1). Host are connected to the network through Communication Acceleration Boards (CABs) that are responsible for protocol processing. <p> Communication protocols The communication protocols account for between 25% and 40% of the CAB-CAB latency. For the Nectar-native protocol tests, this overhead is dominated by the datalink protocol. As discussed elsewhere <ref> [12] </ref>, the datalink is mostly done in software for flexibility reasons, and most of it could be handled in hardware, thus reducing the protocol overhead. The transport protocol cost ranges from less than 10 microseconds (datagram), to 25-30 microseconds (request-response and RMP), and 50 microseconds (UDP).
Reference: 13. <author> H.T. Kung, Peter Steenkiste, Marco Gubitoso, and Manpreet Khaira. </author> <title> Parallelizing a New Class of Large Applications over High-Speed Networks. </title> <booktitle> Third ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, ACM, </booktitle> <month> April, </month> <year> 1991, </year> <pages> pp. 167-177. </pages>
Reference-contexts: Host are connected to the network through Communication Acceleration Boards (CABs) that are responsible for protocol processing. A 26-node prototype system using 100Mbit/second links has been operational since 1989 and has been used to parallelize several engineering and scientific applications <ref> [13] </ref>. The host-host latency over Nectar is about 200 microseconds. One of the goals of the prototype was to experiment with different protocol implementations and make the communication coprocessor customizable by applications. For this reason, we built the CAB around a general-purpose CPU with a flexible runtime system [8].
Reference: 14. <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: We describe the software in more detail in the remainder of 2 this section. 2.1. Threads and SPARC register windows Previous protocol implementations have demonstrated that multiple threads are useful, but multiple address spaces are unnecessary <ref> [6, 14, 16] </ref>. As a result, we designed the CAB to provide a single physical address space with a runtime system that supports multiple threads. The threads package is based on Mach C threads [9]: it provides mutex locks to enforce critical regions, and condition variables for synchronization.
Reference: 15. <author> Sape J. Mullender, Guido van Rossum, Andrew S. Tanenbaum, Robbert van Renesse, and Hans van Staveren. </author> <title> "Amoeba: A Distributed Operating System for the 1990s". </title> <booktitle> IEEE Computer 23, </booktitle> <month> 5 (May </month> <year> 1990), </year> <pages> 44-53. </pages>
Reference-contexts: work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines [5], the performance of the Firefly RPC mechanism [17], the performance of the remote accesses over Ethernet [19], the x-kernel roundtrip time [11], and the performance of the Amoeba RPC <ref> [15] </ref>. Since all these systems are organized differently and often use different protocols, it is hard to compare the results, but we can make some general observations.
Reference: 16. <author> Larry Peterson, Norman Hutchinson, Sean O'Malley, and Herman Rao. </author> <title> "The x-kernel: A Platform for Accessing Internet Resources". </title> <booktitle> IEEE Computer 23, </booktitle> <month> 5 (May </month> <year> 1990), </year> <pages> 23-34. </pages>
Reference-contexts: We describe the software in more detail in the remainder of 2 this section. 2.1. Threads and SPARC register windows Previous protocol implementations have demonstrated that multiple threads are useful, but multiple address spaces are unnecessary <ref> [6, 14, 16] </ref>. As a result, we designed the CAB to provide a single physical address space with a runtime system that supports multiple threads. The threads package is based on Mach C threads [9]: it provides mutex locks to enforce critical regions, and condition variables for synchronization.
Reference: 17. <author> Michael Schroeder and Michael Burrows. </author> <title> "Performance of Firefly RPC". </title> <journal> ACM Transactions on Computer Systems 8, </journal> <month> 1 (February </month> <year> 1990), </year> <pages> 1-17. </pages>
Reference-contexts: Earlier work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines [5], the performance of the Firefly RPC mechanism <ref> [17] </ref>, the performance of the remote accesses over Ethernet [19], the x-kernel roundtrip time [11], and the performance of the Amoeba RPC [15]. Since all these systems are organized differently and often use different protocols, it is hard to compare the results, but we can make some general observations.
Reference: 18. <institution> MB86900 RISC Processor - Architecture Manual. Fujitsu, </institution> <year> 1987. </year>
Reference-contexts: We also present estimates for how the host-host latency would change if we replaced the flexible CAB by a hardwired protocol engine. In Section 6 we discuss related work. 1 2. The CAB architecture and software The CAB is built around a 16.5 MHz SPARC CPU <ref> [18] </ref> and devices such as timers and DMA controllers (Figure 2). It is connected to the host through a VME bus. To provide the necessary memory bandwidth, the CAB memory is split into two regions: a working memory for the SPARC (program memory), and a packet memory (data memory).
Reference: 19. <author> Alfred Spector. </author> <title> Communication Support in Operating Systems for Distributed Transactions. </title> <type> Tech. </type> <institution> Rept. CMU-CS-86-165, Computer Science Department, Carnegie Mellon University, </institution> <month> November, </month> <year> 1986. </year>
Reference-contexts: Earlier work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines [5], the performance of the Firefly RPC mechanism [17], the performance of the remote accesses over Ethernet <ref> [19] </ref>, the x-kernel roundtrip time [11], and the performance of the Amoeba RPC [15]. Since all these systems are organized differently and often use different protocols, it is hard to compare the results, but we can make some general observations.
Reference: 20. <author> Peter Steenkiste. </author> <title> Analyzing Communication Latency using the Nectar Communication Processor. </title> <booktitle> Proceedings of the SIGCOMM '92 Symposium on Communications Architectures and Protocols, ACM, </booktitle> <address> Baltimore, </address> <month> August, </month> <year> 1992, </year> <pages> pp. 199-209. 20 </pages>
Reference-contexts: The CAB runtime system is similar to the runtime system on dedicated multicomputers and to micro-kernel operating systems. In this paper, which is based on <ref> [20] </ref>, we first give an overview of Nectar, concentrating on the CAB architecture and Nectar communication software (Section 2).
Reference: 1. <institution> Introduction 1 </institution>
Reference-contexts: 1. Introduction Multicomputers that use existing hosts and a general network are an attractive architecture for many applications [12], and they are one of the main motivations for improving the performance of networks <ref> [1] </ref>. Some users are interested in partioning applications across a large number of workstations, while other users want to combine the resources of a smaller number of supercomputers.
Reference: 2. <author> The CAB architecture and software 2 2.1. </author> <title> Threads and SPARC register windows 3 2.2. Mailboxes 3 2.3. Communication over Nectar 4 </title>
Reference-contexts: Unfortunately, the thread implementation and, especially, register windows make this organization expensive. The thread-based protocol implementation adds between 45 microseconds to the latency. RISC processors with a flat register file have a considerably lower thread switch overhead <ref> [2] </ref>, and we expect that the additional cost of doing protocol processing in a thread instead of an interrupt handler will be considerably lower on a processor with a flat register file. 8 3.2.2. <p> The context switch overhead is the most troublesome. Although this cost can be reduced by using a flat register file, adding caches and extending the runtime system (e.g. add virtual memory) would increase the cost of context switching. Context switching has traditionally also benefited little from increased CPU speed <ref> [2] </ref>. 4. Host-host breakup A comparison of Figures 3 and 4 shows that the behavior of the threads on the CAB is the same for the CAB-CAB and host-host roundtrip tests. As a result, we can use the CAB-CAB measurements to analyze the CAB component of the host-host roundtrip times.
Reference: 3. <author> CAB-CAB breakup 6 3.1. </author> <title> Analysis of CAB-CAB latency numbers 7 3.2. Context switching 7 3.2.1. Threads and context saving/restoring overhead 7 3.2.2. Use of register windows 9 3.3. Buffer management 10 3.4. Communication protocols 11 3.5. CAB-CAB latency summary 12 </title>
Reference-contexts: This overhead however is, or should be, of the order of 10s of microseconds [7], and does not account for the order of magnitude difference in latency. In this paper we identify other sources of overhead based on measurements collected on Nectar. The Nectar network <ref> [3, 12] </ref> consists of a high-bandwidth crosspoint network (Figure 1). Host are connected to the network through Communication Acceleration Boards (CABs) that are responsible for protocol processing.
Reference: 4. <author> Host-host breakup 12 4.1. </author> <title> Analysis of host-host latency 13 4.2. Cost of flexibility 14 4.3. Host-host latency summary 15 </title>
Reference-contexts: Reducing latency has traditionally been the biggest challenge. Dedicated multicomputers with special-purpose interconnects such as the Intel Touchstone system have latencies below 100 microseconds <ref> [4] </ref>, while latencies between Unix workstations communicating over general networks are typically one order of magnitude higher. The latency between two Sun4/330 running Sun OS 4.1 is for example about 800 microseconds. One reason for the higher latency is the difference in communication medium.
Reference: 5. <institution> Other performance measures 15 5.1. </institution> <note> Overhead 15 5.2. Throughput 16 5.3. Summary 17 </note>
Reference-contexts: Earlier work Other work analyzing the overhead associated with network communication includes TCP and UDP communication between two Vax 11/750 machines <ref> [5] </ref>, the performance of the Firefly RPC mechanism [17], the performance of the remote accesses over Ethernet [19], the x-kernel roundtrip time [11], and the performance of the Amoeba RPC [15]. <p> Second, the cost of buffer management for Nectar is typically higher than for the other implementations (if reported), with the exception for the TCP/UDP measurements reported in <ref> [5] </ref>. The reasons are the flexibility of the mailbox module, and the requirement that messages have to be contiguous in memory. 7. Conclusion We presented an analysis of host-host and CAB-CAB communication latency in Nectar. The CAB-CAB measurements give some insights in communication latency between nodes using light-weight runtime systems.
Reference: 6. <institution> Earlier work 18 </institution>
Reference-contexts: We describe the software in more detail in the remainder of 2 this section. 2.1. Threads and SPARC register windows Previous protocol implementations have demonstrated that multiple threads are useful, but multiple address spaces are unnecessary <ref> [6, 14, 16] </ref>. As a result, we designed the CAB to provide a single physical address space with a runtime system that supports multiple threads. The threads package is based on Mach C threads [9]: it provides mutex locks to enforce critical regions, and condition variables for synchronization. <p> The transport protocol performs the matching End_Put, which wakes up any waiting application thread or process. The transport protocol can be called in one of two ways. The Nectar-native protocols are invoked by an upcall inside the interrupt handler <ref> [6] </ref> (top Figure 3). This implementation was 4 motivated by speed: it avoids waking up a thread. The disadvantage is that since many data structures are accessed at interrupt time, critical regions often have to be enforced by masking interrupts.
Reference: 7. <institution> Conclusion 18 21 </institution>
Reference-contexts: The communication protocols that recover from these errors introduce overhead, thus adding to the latency. This overhead however is, or should be, of the order of 10s of microseconds <ref> [7] </ref>, and does not account for the order of magnitude difference in latency. In this paper we identify other sources of overhead based on measurements collected on Nectar. The Nectar network [3, 12] consists of a high-bandwidth crosspoint network (Figure 1). <p> The transport protocol cost ranges from less than 10 microseconds (datagram), to 25-30 microseconds (request-response and RMP), and 50 microseconds (UDP). The overhead for the reliable Nectar-native protocols is of the order of 200 instructions, i.e. similar to that reported in 11 the literature for optimized standard protocols <ref> [7] </ref>. UDP is a lot more expensive, mainly because it has not been optimized. It is also more general than the Nectar-native protocols, since UDP packets can travel outside Nectar. 3.5. CAB-CAB latency summary Table 5 summarizes the results of the CAB-CAB latency analysis.
References-found: 27

