URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1995/UM-CS-1995-023.ps
Refering-URL: http://www.cs.umass.edu/~leouski/
Root-URL: 
Email: leouski@cs.umass.edu  
Title: Learning of Position Evaluation in the Game of Othello  
Author: Anton Leouski 
Date: January 20, 1995  
Address: Amherst, Massachusetts 01003  
Affiliation: Department of Computer Science University of Massachusetts  
Pubnum: Master's Project: CMPSCI 701  
Abstract: Conventional Othello programs are based on a thorough analysis of the game, crafting sophisticated evaluation functions and supervised learning techniques with use of large expert-labeled game databases. This paper presents an alternative by training a neural network to evaluate Othello positions via temporal difference (TD) learning. The approach is based on network architecture that reflects the spatial and temporal organization of the domain. The network begins as a random network and by simply playing against itself achieves an intermediate level of play. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Berliner, </author> <title> "On the construction of evaluation functions for large domains", </title> <booktitle> Proceedings of the Sixth International Joint Conference on Artificial Intelligence. </booktitle> <address> Tokyo, Japan: </address> <publisher> Morgan Kaufman, </publisher> <year> (1979). </year>
Reference-contexts: It seems reasonable to have a different network trained for every game phase. This is the subject of the next section. Application Coefficients Three different networks were trained, one for each game stage. Instead of having three different evaluation functions, the transitions between stages are smoothed by application coefficients <ref> [1] </ref>. The disc count provides a good estimate of the game stage and is used as an argument in the following function: AC i n ( ) = Exp - 2 In the experiments s=20, and m assumes values 4, 34, and 64.
Reference: 2. <author> J. Clouse, </author> <title> "Learning Application Coefficients with a Sigma-Pi Unit", </title> <type> Master Thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts at Amherst (1992). </institution>
Reference-contexts: The author thinks that adding some learning mechanism for the application coefficients may actually improve the accuracy of defining the game stage. Although J. Clouse <ref> [2] </ref> argues that using sigma-pi units in a two-layer network do not produce significant improvement, the other techniques (e.g. gating networks) should also be considered. For example, R. Jacobs [3, 4] presents a system composed of several different "expert" networks and a gating network that distributes training instances among experts.
Reference: 3. <author> R. A. Jacobs, </author> <title> "Task Decomposition Through Competition in a Modular Connectionist Architecture", </title> <type> Ph.D. Thesis, Techical Report 90-44, </type> <institution> Department of Computer and Information Science, University of Massachusetts at Amherst, </institution> <year> (1990). </year>
Reference-contexts: Training one network to evaluate board states in different game phases is the same as to train the network to perform different tasks at different times. The network can become a victim of the effect of temporal crosstalk <ref> [3] </ref>. It seems reasonable to have a different network trained for every game phase. This is the subject of the next section. Application Coefficients Three different networks were trained, one for each game stage. <p> Although J. Clouse [2] argues that using sigma-pi units in a two-layer network do not produce significant improvement, the other techniques (e.g. gating networks) should also be considered. For example, R. Jacobs <ref> [3, 4] </ref> presents a system composed of several different "expert" networks and a gating network that distributes training instances among experts. He also shows that the gating network is capable of learning how to make this allocation. 15 8.
Reference: 4. <author> R. A. Jacobs, M. I. Jordan, S. J. Nowlan, and G. E. Hinton, </author> <title> "Adaptive Mixtures of Local Experts", </title> <booktitle> Neural Computation 3 (1991) 79-87. </booktitle>
Reference-contexts: Although J. Clouse [2] argues that using sigma-pi units in a two-layer network do not produce significant improvement, the other techniques (e.g. gating networks) should also be considered. For example, R. Jacobs <ref> [3, 4] </ref> presents a system composed of several different "expert" networks and a gating network that distributes training instances among experts. He also shows that the gating network is capable of learning how to make this allocation. 15 8.
Reference: 5. <author> Y. Le Cun, B. Boser, J. Denker, D. Henderson, R. Howard, W. Hubbard, and L. Jackel, </author> <title> "Backpropagation applied to handwritten zip code recognition", Ne ura l Computation 1 (1989) 541-551. </title>
Reference-contexts: This symmetry was incorporated into the network by appropriate weight sharing and summing of derivatives. Weight sharing was described by D. E. Rumelhart et al. [10] and successfully used by Y. Le Cun et al. <ref> [5] </ref>. (a) (b) Fig. 4. (a) shows how an Othello board is divided by 8 triangles; (b) shows the resulting triangles. The network has 64 input units corresponding to 64 squares on the Othello board. <p> The input units corresponding to a triangle are connected to one unit from the next layout (Fig. 5 (a)), so eight triangles are connected to the eight units from the next layout. Together, eight such units form a plane that Y. Le Cun et al. <ref> [5] </ref> call a feature map (Fig. 5 (b)). a feature map (b) one unit from the hidden layout "triangle" on the Othello board (a) Fig. 5. (a) shows connections from 10 input units corresponding to a board "triangle" to one unit from the hidden layout; (b) shows eight "triangles" give input
Reference: 6. <author> K.-F. Lee, S. Mahajan, </author> <title> "A Pattern Classification Approach to Evaluation Function Learning", </title> <booktitle> Artificial Intelligence 36 (1988) 1-25. </booktitle> <pages> 16 </pages>
Reference-contexts: Fourth, IAGO used a single evaluation function for the whole game, though it is now well known that different strategies are needed for different stages of the game. BILL K.-F. Lee and S. Mahajan <ref> [6, 7] </ref> addressed these drawbacks by creating a program named BILL, which used a slightly extended set of features. These feature representations were significantly improved through use of pre-computed tables that allowed BILL to recognize hundreds of thousands of patterns in constant time.
Reference: 7. <author> K.-F. Lee, S. Mahajan, </author> <title> "The Development of a World Class Othello Program", </title> <booktitle> Artificial Intelligence 43 (1990) 21-36. </booktitle>
Reference-contexts: Fourth, IAGO used a single evaluation function for the whole game, though it is now well known that different strategies are needed for different stages of the game. BILL K.-F. Lee and S. Mahajan <ref> [6, 7] </ref> addressed these drawbacks by creating a program named BILL, which used a slightly extended set of features. These feature representations were significantly improved through use of pre-computed tables that allowed BILL to recognize hundreds of thousands of patterns in constant time.
Reference: 8. <author> D. Moriarty, R. Miikkulainen, </author> <title> "Evolving Complex Othello Strategies Using Marker-Based Genetic Encoding of Neural Networks", </title> <type> Technical Report AI93-206, </type> <institution> Department of Computer Science, University of Texas at Austin (1993). </institution>
Reference-contexts: BILL's evaluation function is a quadratic polynomial, which takes into account linear interrelationships between features; the question of existence of more complex interactions among these concepts remains open. Genetic algorithms A different approach was elaborated by D. Moriarty and R. Miikkulainen <ref> [8] </ref>. They evolved a population of neural networks using a genetic algorithm to evaluate possible moves. Every network sees the current board configuration as its input and indicates the goodness of each possible move as the output.
Reference: 9. <author> P. S. Rosenbloom, </author> <title> "A World-Championship-Level Othello Program", </title> <booktitle> Artificial Intelligence 19 (1982) 279-320. </booktitle>
Reference-contexts: Every disk has one white and one black side. Like chess, it is a deterministic, perfect information, zerosum game of strategy between two players, black and white. Black opens the game from the initial board configuration shown in Fig. 1 <ref> [9] </ref>. A legal move for a player is a placement of a piece on the board resulting in the capture of one or more opponent's pieces. <p> Previous Research The game of Othello has received great attention within Computer Science for more than ten years. The following is the far from complete outline of several pieces of research work in this domain. IAGO It was Paul Rosenbloom <ref> [9] </ref> who pointed out that although the game of Othello has an average branching factor 5 and limited length (less than 64 moves) it still cannot be solved exactly and has a great deal of complexity to be a subject for scientific analysis.
Reference: 10. <author> D. E. Rumelhart, G. E. Hilton, and R. J. Williams, </author> <title> "Learning internal representation by error propagation", In Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </title> <editor> D. E. Rumelhart and J. L. McClelland, eds., </editor> <volume> Vol. I, </volume> <pages> 318-362. </pages> <publisher> Bradford Books, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The Othello board is invariant with respect to reflection and rotation symmetry of the square. This symmetry was incorporated into the network by appropriate weight sharing and summing of derivatives. Weight sharing was described by D. E. Rumelhart et al. <ref> [10] </ref> and successfully used by Y. Le Cun et al. [5]. (a) (b) Fig. 4. (a) shows how an Othello board is divided by 8 triangles; (b) shows the resulting triangles. The network has 64 input units corresponding to 64 squares on the Othello board.
Reference: 11. <author> R. Sutton, </author> <title> "Learning to predict by the methods of temporal differences", </title> <note> Machine Learning 3 (1988) 9-44. </note>
Reference-contexts: Supervised backpropagation networks might have been applied to the game but would have faced a bottleneck in the training data. Someone would need to provide a significant-sized collection of labeled game records. An alternative approach based on the TD (l) predictive algorithm <ref> [11] </ref> has been proposed. This technique was successfully applied to the game of backgammon by G. Tesauro [12]. The advantage of this method is that a neural network can be trained while playing only itself and does not require either precompiled training data or a good opponent. <p> A value close to 0.1 is generally used, higher learning rates degrade performance, whereas lower rates reduce fluctuations in performance (see <ref> [11, 12] </ref>). I set a=0.05 1 . The P t is the current prediction, the output of the network given the current board state. The next prediction, the P t+1 is the result of the minimax search 1 l was set to 0.
Reference: 12. <author> G. Tesauro, </author> <title> "Practical issues in temporal difference learning", </title> <note> Machine Learning 8 (1992) 257-278. </note>
Reference-contexts: Someone would need to provide a significant-sized collection of labeled game records. An alternative approach based on the TD (l) predictive algorithm [11] has been proposed. This technique was successfully applied to the game of backgammon by G. Tesauro <ref> [12] </ref>. The advantage of this method is that a neural network can be trained while playing only itself and does not require either precompiled training data or a good opponent. <p> A value close to 0.1 is generally used, higher learning rates degrade performance, whereas lower rates reduce fluctuations in performance (see <ref> [11, 12] </ref>). I set a=0.05 1 . The P t is the current prediction, the output of the network given the current board state. The next prediction, the P t+1 is the result of the minimax search 1 l was set to 0.
Reference: 13. <author> G. Tesauro, </author> <title> "TD-Gammon, a self-teaching backgammon program, achieves master-level play", </title> <booktitle> Neural Computation 6(2) (1994) 215-219. </booktitle>
Reference-contexts: An evaluation function trained by TD (0) together with a full two-ply lookahead to pick the estimated best move has made TD-Gammon competitive with the best human players in the world <ref> [13] </ref>. A straightforward adaptation of Tesauro's approach to the Othello domain has been investigated [14]. A fully-interconnected network with one hidden layer of 50 units was trained for a period of 30,000 games using raw board positions as input.
Reference: 14. <author> S. Walker, </author> <title> "Neural Neworks Playing the Game of Othello", </title> <type> Undergraduate Thesis, </type> <institution> Department of ECE, University of Queensland. </institution>
Reference-contexts: An evaluation function trained by TD (0) together with a full two-ply lookahead to pick the estimated best move has made TD-Gammon competitive with the best human players in the world [13]. A straightforward adaptation of Tesauro's approach to the Othello domain has been investigated <ref> [14] </ref>. A fully-interconnected network with one hidden layer of 50 units was trained for a period of 30,000 games using raw board positions as input. Although the network quickly learned importance of the corner squares, it had little knowledge of how to protect them.
Reference: 15. <author> D. Mitchell, </author> <title> "Using features to evaluate positions in experts' and novices' othello games", </title> <type> Master thesis, </type> <institution> Evanston, IL: Department of Psychology, Northwestern University (1984). </institution>
Reference-contexts: Together with the a-b search algorithm, interative deepening, and move ordering, this function formed the basis of Rosenbloom's program IAGO. Although IAGO displayed a World Class performance it did have several drawbacks. First, the concepts set used by the program is rather limited <ref> [15] </ref>. Second, the concepts, also called features, were assumed independent and therefore combined into a linear evaluation function. Third, the application coefficients in the evaluation function were selected by 4 hand, which left a significant margin for error.
Reference: 16. <institution> The guide to the game of Othello. </institution> <note> WorldWide Web page at http://web.cs.ualberta.ca/ ~brock/ othello.html </note>
Reference-contexts: 1. Introduction Othello is the third incarnation of the old Japanese board game, developed in its current form in 1974 <ref> [16] </ref>. It is mostly attractive because of the simplicity of its rules. One may start playing after a minute of introduction, and still the game has enough complexity to leave a dedicated person with years to master.
References-found: 16

