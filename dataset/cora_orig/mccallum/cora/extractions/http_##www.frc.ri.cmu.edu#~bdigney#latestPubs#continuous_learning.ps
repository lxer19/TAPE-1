URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/continuous_learning.ps
Refering-URL: http://www.frc.ri.cmu.edu/~bdigney/latestPubs/
Root-URL: 
Email: bdigney@ri.cmu.edu  
Phone: Phone (412) 268-7084 Fax (412) 268-5895  
Title: Learning in Continuous Domains with Delayed Rewards  
Author: Bruce L. Digney 
Address: Pittsburgh, PA, 15232, USA  
Affiliation: Robotics Institute Carnegie Mellon University  
Abstract: Much has been done to develop learning techniques for delayed reward problems in worlds where the actions and/or states are approximated by discrete representations. Although this is acceptable in some applications there are many more situations where such an approximation is difficult and unnatural. For instance, in applications such as robotic,s where real machines interact with the real world, learning techniques that use real valued continuous quantities are required. Presented in this paper is an extension to Q-learning that uses both real valued states and actions. This is achieved by introducing activation strengths to each actuator system of the robot. This allow all actuators to be active to some continuous amount simultaneously. Learning occurs by incrementally adapting both the expected future reward to goal evaluation function and the gradients of that function with respect to each actuator system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barto, A.G., Sutton, R.S. and Watkins C.H. </author> <year> (1989), </year> <title> Learning and Sequential Decision Making, </title> <type> COINS Technical Report. </type>
Reference-contexts: Presently, many learning systems use discrete approximations of continuous domains for states or actions or both <ref> [1] </ref>. In general, real valued quantities are to be expected whenever a real machine interacts with the real world. This is especially true for robotic applications where the robot must perceive, act and learn using continuous quantities and in which discrete approximations quickly become unnatural and intractable. <p> In this paper, the focus is upon learning a single behavior with continuous valued states and actuator activation representations. Learning using continuous states and activations is difficult, but it is clearly a more natural way to represent the world and approach the problem. Recall for a standard Q-learning <ref> [1] </ref> problem with discrete states and actions, the action selected, a fl , at some state, s i , is a fl = argmax a [Q (s i ; a) + E (t)] (1) where Q (s i ; a) is the evaluation over all discrete states and actions.
Reference: [2] <author> Pomerlou, D. </author> <year> (1991), </year> <title> Efficient Training of Artificial Neural Network for Autonomous Navigation, </title> <journal> Neural Computation, </journal> <volume> Vol 3, No1, </volume> <pages> pp 88-97. </pages>
Reference-contexts: While much has been done using continuous valued neural networks in different configurations to learn control in a supervised fashion with immediate rewards <ref> [2] </ref> many difficulties still exist with unsupervised, delayed reward control problems. Some techniques allow continuous valued state inputs but actions are still discrete representations with an evaluations function for each discrete value of the action [3].
Reference: [3] <author> Lin, </author> <year> (1992), </year> <title> Lin, Long-Ji, Self-Improving Reactive Agents Based on Reinforcement Learning, Planning and Teaching, </title> <booktitle> Machine Learning 8 </booktitle> <pages> 293-321. </pages>
Reference-contexts: Some techniques allow continuous valued state inputs but actions are still discrete representations with an evaluations function for each discrete value of the action <ref> [3] </ref>. This is acceptable if the actions are naturally discrete but for many problems it is desirable to use continuous actions. Recent work in Nested Q-learning (NQL) [5] has led to a reinforcement learning algorithm that can learn a hierarchical structure of a control problem with delayed rewards [4].
Reference: [4] <author> Digney, B. L., </author> <year> (1996), </year> <title> Emergent Hierarchical Control Structures: Learning Reactive/Hierarchical Relationships in Reinforcement Environments, From animals to animats 4: </title> <address> SAB 96, Cape Cod, USA, </address> <month> Septemeber </month> <year> 1996, </year> <pages> pp. 363-373, </pages> <publisher> MIT Press-Bradford Books, Massachussets. </publisher>
Reference-contexts: This is acceptable if the actions are naturally discrete but for many problems it is desirable to use continuous actions. Recent work in Nested Q-learning (NQL) [5] has led to a reinforcement learning algorithm that can learn a hierarchical structure of a control problem with delayed rewards <ref> [4] </ref>. This algorithm is considered to be preliminary work and uses discrete representations of states, primitive actions and sub-behaviors that are either fully on (in complete control) or fully off (not in control at all). <p> The learning of such structures is dealt with in detail by Digney in <ref> [4] </ref> and further refined in [5]. In this paper, the focus is upon learning a single behavior with continuous valued states and actuator activation representations. Learning using continuous states and activations is difficult, but it is clearly a more natural way to represent the world and approach the problem.
Reference: [5] <author> Digney, B. L., </author> <year> (1998), </year> <title> Learning Hierarchical Control Structures for Multiple Tasks and Changing Environments, </title> <note> From animals to animats 5: SAB 98(submitted) </note>
Reference-contexts: This is acceptable if the actions are naturally discrete but for many problems it is desirable to use continuous actions. Recent work in Nested Q-learning (NQL) <ref> [5] </ref> has led to a reinforcement learning algorithm that can learn a hierarchical structure of a control problem with delayed rewards [4]. <p> The learning of such structures is dealt with in detail by Digney in [4] and further refined in <ref> [5] </ref>. In this paper, the focus is upon learning a single behavior with continuous valued states and actuator activation representations. Learning using continuous states and activations is difficult, but it is clearly a more natural way to represent the world and approach the problem.
Reference: [6] <author> Albus, J.S. </author> <year> (1991), </year> <title> Outline of a Theory of Intelligence. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21, 3, </volume> <pages> pp. 473-509. </pages>
Reference-contexts: This function is learned and represented in some type of parametric storage device such as a neural network to provide generalization capabilities. In this research, Albus' CMAC type network is used <ref> [6] </ref> [7]. For explanation purposes, consider that an accurate Q (S) somehow exists beforehand. <p> The fifth layer has a tile size of 0.5 unit length and a 0.5 unit width. The working details of the CMAC storage structure will not be discussed further here as they are described in full detail by Albus <ref> [6] </ref> and applied to learning and control problems in Sutton [7]. 3.1 Test 1: Impassable Barrier To evaluate the proposed continuous domain learning algorithm, the animat's rectangular world was augmented with an impassable L shaped barrier as shown in Figure 2 (a).
Reference: [7] <author> Sutton, R., </author> <year> (1996), </year> <title> Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding, </title> <booktitle> Neural Information Processing Systems NIPS8 . 15 </booktitle>
Reference-contexts: This function is learned and represented in some type of parametric storage device such as a neural network to provide generalization capabilities. In this research, Albus' CMAC type network is used [6] <ref> [7] </ref>. For explanation purposes, consider that an accurate Q (S) somehow exists beforehand. <p> A method for determining and learning these functions will now be presented. In the following description CMAC function approximators will be used. Of course, other neural network type storage techniques could be used but CMAC has proven better suited to learning this type of task <ref> [7] </ref>. CMAC storage devices are described elsewhere [7] and in this analysis, the reader may neglect the details of operation and view the CMACs as incremental function approximators. Initially, all CMAC weights are set to small random values. <p> In the following description CMAC function approximators will be used. Of course, other neural network type storage techniques could be used but CMAC has proven better suited to learning this type of task <ref> [7] </ref>. CMAC storage devices are described elsewhere [7] and in this analysis, the reader may neglect the details of operation and view the CMACs as incremental function approximators. Initially, all CMAC weights are set to small random values. <p> The fifth layer has a tile size of 0.5 unit length and a 0.5 unit width. The working details of the CMAC storage structure will not be discussed further here as they are described in full detail by Albus [6] and applied to learning and control problems in Sutton <ref> [7] </ref>. 3.1 Test 1: Impassable Barrier To evaluate the proposed continuous domain learning algorithm, the animat's rectangular world was augmented with an impassable L shaped barrier as shown in Figure 2 (a).
References-found: 7

