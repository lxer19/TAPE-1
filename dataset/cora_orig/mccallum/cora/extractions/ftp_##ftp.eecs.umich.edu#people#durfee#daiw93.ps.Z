URL: ftp://ftp.eecs.umich.edu/people/durfee/daiw93.ps.Z
Refering-URL: http://ai.eecs.umich.edu/people/durfee/vita.html
Root-URL: http://www.cs.umich.edu
Email: piotr@cs.huji.ac.il, durfee@engin.umich.edu  
Title: Reasoning about Other Agents: Philosophy, Theory, and Implementation.  
Author: Piotr J. Gmytrasiewicz and Edmund H. Durfee 
Address: Jerusalem, Israel  Ann Arbor, Michigan 48109  
Affiliation: Department of Computer Science Hebrew University,  Department of Electrical Engineering and Computer Science University of Michigan,  
Abstract: Drawing on on our work in the area of Distributed Artificial Intelligence, we propose the rudiments of a view of multiagent reasoning that relates current philosophical intuitions, theoretical foundations, and preliminary implementation. The philosophical position we take is a combination of Daniel Dennett's philosophy of the ladder of per-sonhood (consisting of rationality, intentionality, stance, reciprocity, communication, and consciousness) on one hand, and the utilitarian philosophy of selfish utility maximization on the other hand. The theories we incorporate are logics of knowledge and belief, which in addressing the multiagent issues can be developed based on a recursive version of the Kripke structure, and the related fields of utility, decision and game theories. Our preliminary implementation, the Recursive Modeling Method (RMM), lets an agent coordinate its actions with the actions of other agents, cooperate with them when appropriate, and rationally choose an optimal form of communication with them.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Afzal Ballim and Yoric Wilks. </author> <title> Artificial Believers. </title> <publisher> Earlbaum Associates, Inc., </publisher> <year> 1991. </year>
Reference-contexts: But, while relations are intuitively clear, there is a considerable gap between the foundations of game theory, mainly rooted in decision and utility theories, and the logically motivated extensions of Dennett's philosophy <ref> [14, 1, 16] </ref>. Consequently, and perhaps not surprisingly, certain reformulations of the ideas are sometimes necessary to show the full correspondence and relations among the ideas.
Reference: [2] <author> G. J. C. Bertrand. Pleasure and Desire: </author> <title> The Case for Hedonism Reviewed. </title> <publisher> Clarendon Press, </publisher> <year> 1969. </year>
Reference-contexts: Utilitarianism may have its roots in hedonistic philosophy <ref> [2] </ref>, and, more recently, it could be seen to influence the philosophy of selfishness and rationality of Ayn Rand [15]. If trivialized, the above paradigm may be taken to say that a rational agent should do what is best for it, and thus could be seen as lacking significant content.
Reference: [3] <author> D. Dennett. </author> <title> Consciousness Explained. Little, </title> <publisher> Brown & Co., </publisher> <year> 1991. </year>
Reference-contexts: For completeness, we should also mention the final level of personhood considered in Dennett's philosophy consciousness, 2 which is taken to mean the agent's ability to access and suitably modify its own high-level preferences (but see <ref> [3] </ref> for an updated version of 1 For the purpose of showing its relations to other work. 2 Which we will not consider any further in this paper. 3 Dennett's view on consciousness).
Reference: [4] <author> Jon Doyle. </author> <title> Rationality and its role in reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 8 </volume> <pages> 376-409, </pages> <year> 1992. </year>
Reference-contexts: the theoretical formulations and the philosophical foundations (Section 4), and close with short discussion in Section 5. 2 Utilitarianism and the Ladder of Personhood Utilitarianism is a paradigm that defines rationality as behavior of an intelligent autonomous agent that maximizes the expected subjective utility of the agent [12] (see also <ref> [4] </ref> for an excellent overview). Utilitarianism may have its roots in hedonistic philosophy [2], and, more recently, it could be seen to influence the philosophy of selfishness and rationality of Ayn Rand [15].
Reference: [5] <author> Edmund H. Durfee, Jaeho Lee, and Piotr Gmytrasieiwcz. </author> <title> Overeager rationality and mixed strategy equilibria. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: The resulting nested hierarchy of payoff matrices is depicted in Figure 4. The solution method for RMM hierarchies is described in detail in [8], with a modification proposed recently in <ref> [5] </ref>; here we only remark that the solution proceeds bottom-up and that, as a result, R1 derives an estimate of what R2 is expected to do, given all of the information contained in the hierarchy.
Reference: [6] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Logic of knowledge and belief for recursive modeling: Preliminary report. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 628-634, </pages> <month> July </month> <year> 1992. </year> <month> 10 </month>
Reference-contexts: In the above formulation, the relation is, as before, the truth assignment to the primitive propositions for each possible world, s k . The binary relation R in RM 1 is a possibility relation defined over the set of augmented possible worlds S. As we detail in <ref> [6] </ref>, the recursive Kripke structures can serve to define a number of concepts useful in multiagent reasoning. For example knowledge and belief can be defined in a way that allows for an intuitive distinction between the two within the same recursive Kripke structure.
Reference: [7] <author> Piotr J. Gmytrasiewicz and Edmund H. Durfee. </author> <title> Toward a theory of honesty and trust among communicating autonomous agents. Group Decision and Negotiation, </title> <note> 1993, to appear. </note>
Reference-contexts: Quite a different dimension in the communicative behavior among autonomous agents arises when the assumptions (implicit so far) that the messages are always true and always 9 believed, are relaxed. We have made a preliminary investigation of these issues, and some of our most encouraging results are presented in <ref> [7] </ref>, but we do not elaborate on them here. 5 Conclusion We have presented a personal, biased view of some of the main philosophical currents in reasoning about other agents, the relevant theories, and our preliminary implementation, with the aim of illustrating how these diverse threads can be interrelated, and thus
Reference: [8] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> A decision-theoretic approach to coordinating multiagent interactions. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: In this paper we present a set of interrelations among elements of the philosophical, theoretical and implementational levels that, we hope, could be seen as a contribution toward such a unifying view. Our considerations draw on our recent work in Distributed Artificial Intelligence <ref> [8, 9] </ref>, which has been directed toward creating a rigorous foundation for addressing problems of 0 This research was supported, in part, by the Golda Meir Fellowship and the Alfassa Fund administered by the Hebrew University of Jerusalem, and by the National Science Foundation under grant IRI-9015423 and PYI award IRI-9158473. <p> RMM is described in more detail in <ref> [8, 9] </ref>; now we just review the simple example of interaction in Figure 3 and concentrate on RMM's relations with previously developed notions. The scenario in Figure 3 is similar to the one in Figure 1, but we have added the element of uncertainty. <p> The resulting nested hierarchy of payoff matrices is depicted in Figure 4. The solution method for RMM hierarchies is described in detail in <ref> [8] </ref>, with a modification proposed recently in [5]; here we only remark that the solution proceeds bottom-up and that, as a result, R1 derives an estimate of what R2 is expected to do, given all of the information contained in the hierarchy.
Reference: [9] <author> Piotr J. Gmytrasiewicz, Edmund H. Durfee, and David K. Wehe. </author> <title> The utility of communication in coordinating intelligent agents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 166-172, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: In this paper we present a set of interrelations among elements of the philosophical, theoretical and implementational levels that, we hope, could be seen as a contribution toward such a unifying view. Our considerations draw on our recent work in Distributed Artificial Intelligence <ref> [8, 9] </ref>, which has been directed toward creating a rigorous foundation for addressing problems of 0 This research was supported, in part, by the Golda Meir Fellowship and the Alfassa Fund administered by the Hebrew University of Jerusalem, and by the National Science Foundation under grant IRI-9015423 and PYI award IRI-9158473. <p> RMM is described in more detail in <ref> [8, 9] </ref>; now we just review the simple example of interaction in Figure 3 and concentrate on RMM's relations with previously developed notions. The scenario in Figure 3 is similar to the one in Figure 1, but we have added the element of uncertainty. <p> Thus, agent R1, by sending the message M to R2, has effectively increased the quality of the interaction (from its standpoint) from 2.01 to 3. The difference between the two can be readily identify as the utility of sending the message (see <ref> [9] </ref> for other examples and more details). To summarize, RMM, when applied to communication, allows an agent to compute the expected value of various messages that it can transmit, and choose the one that causes the maximum increase in the quality of the interaction. <p> The computation can accommodate the risks associated with unreliable communication channels, as detailed in <ref> [9] </ref>. Using RMM, therefore, allows the agents to be rational in their communicative behavior under a wide range of realistic conditions.
Reference: [10] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <booktitle> In Third ACM Conference on Principles of Distributed Computing, </booktitle> <year> 1984. </year>
Reference-contexts: payoffs, or the the ones that are safer? A related question is: how do the players achieve beliefs that are common knowledge, or, how do they converge on a particular equilibrium in the first place? This issue is problematic, particularly in view of the results due to Halpern and Moses <ref> [10] </ref>, that prove that achieving common knowledge in practical situations is impossible in finite time.
Reference: [11] <author> Joseph Y. Halpern and Yoram Moses. </author> <title> A guide to the modal logics of knowledge and belief. </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 480-490, </pages> <address> Los Angeles, California, </address> <month> August </month> <year> 1985. </year>
Reference-contexts: We will concentrate on a theory founded on modal logic with a possible worlds semantics (see <ref> [11] </ref> for details).
Reference: [12] <author> J. Harsanyi. </author> <title> Bayesian decision theory and utilitarian ethics. </title> <journal> American Economic Review, </journal> (68):223-228, 1978. 
Reference-contexts: features common to the theoretical formulations and the philosophical foundations (Section 4), and close with short discussion in Section 5. 2 Utilitarianism and the Ladder of Personhood Utilitarianism is a paradigm that defines rationality as behavior of an intelligent autonomous agent that maximizes the expected subjective utility of the agent <ref> [12] </ref> (see also [4] for an excellent overview). Utilitarianism may have its roots in hedonistic philosophy [2], and, more recently, it could be seen to influence the philosophy of selfishness and rationality of Ayn Rand [15].
Reference: [13] <author> Narayanan. </author> <title> On Being a Machine. </title> <publisher> Ellis Horwood, </publisher> <year> 1991. </year>
Reference-contexts: of modern economical thought|game theory, which is aimed at modeling interactions among rational agents and therefore directly relevant to our considerations|we soon noticed the emerging relations with other theories, particularly with modal logic with its possible worlds semantics, and with an influential strand of philosophical thought developed by Daniel Dennett <ref> [13] </ref>. These relations are obvious at the intuitive level; it seemed clear that everybody was talking about essentially the same issues, but using diverse vocabularies or formalizations. <p> This view would not do justice, though, to the power and depth of theories built based on it, briefly overviewed in the next section. Interestingly, the concept of rationality also constitutes the first step in Daniel Dennett's ladder of personhood (see <ref> [13] </ref> for a good overview of Dennett's philosophy). In essence, it is the minimum requirement that would justify attributing the notion of agenthood to a system.
Reference: [14] <author> J. L. Pollock. </author> <title> How to Build a Person: A Prolegomenon. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: But, while relations are intuitively clear, there is a considerable gap between the foundations of game theory, mainly rooted in decision and utility theories, and the logically motivated extensions of Dennett's philosophy <ref> [14, 1, 16] </ref>. Consequently, and perhaps not surprisingly, certain reformulations of the ideas are sometimes necessary to show the full correspondence and relations among the ideas. <p> are justified in calling a system intentional if we find that, in our interactions with it, it is useful to treat it as a rational being and to see the system's intentions as rationally following from its beliefs about the world, and from its high-level preferences, or desires. (See also <ref> [14] </ref> for related discussion.) Clearly, an agent may be uncertain about a number of important features of its complex environment, and it is in the agent's interest to attempt to remove as much of the uncertainty as possible, since more precise knowledge will likely enable the agent to interact more effectively
Reference: [15] <author> Ayn Rand. </author> <booktitle> On the Virtue of Selfishness. </booktitle> <address> New American Library, </address> <year> 1974. </year>
Reference-contexts: Utilitarianism may have its roots in hedonistic philosophy [2], and, more recently, it could be seen to influence the philosophy of selfishness and rationality of Ayn Rand <ref> [15] </ref>. If trivialized, the above paradigm may be taken to say that a rational agent should do what is best for it, and thus could be seen as lacking significant content.
Reference: [16] <author> Yoav Shoham. </author> <title> Agent-oriented programming. </title> <type> Technical Report STAN-CS-90-1335, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> California 94305, </address> <year> 1990. </year>
Reference-contexts: But, while relations are intuitively clear, there is a considerable gap between the foundations of game theory, mainly rooted in decision and utility theories, and the logically motivated extensions of Dennett's philosophy <ref> [14, 1, 16] </ref>. Consequently, and perhaps not surprisingly, certain reformulations of the ideas are sometimes necessary to show the full correspondence and relations among the ideas.
Reference: [17] <author> Eric Werner. </author> <title> Toward a theory of communication and cooperation for multiagent planning. </title> <booktitle> In Proceedings of the 2nd Conference on Theoretical Aspects of Reasoning about Knowladge, </booktitle> <pages> pages 129-142. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1988. </year> <month> 11 </month>
Reference-contexts: More formally, the result of R1's sending a message to R2 is a transformation of the knowledge contained in the recursive hierarchy, and this transformation can be identified as the pragmatic meaning of the message <ref> [17] </ref>. Let us consider the effect of a message, M, that agent R1 could send to R2 it the situation depicted in Figure 3, saying "There is goal G2 behind the wall". We call this kind of message a modeling message.
References-found: 17

