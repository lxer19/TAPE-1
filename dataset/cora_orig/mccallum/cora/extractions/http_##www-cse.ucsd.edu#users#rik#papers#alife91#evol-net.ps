URL: http://www-cse.ucsd.edu/users/rik/papers/alife91/evol-net.ps
Refering-URL: http://www-cse.ucsd.edu/users/rik/bibliography3_5.html
Root-URL: 
Email: rik@cs.ucsd.edu  
Title: Evolving Networks: Using the Genetic Algorithm with Connectionist Learning  
Author: Richard K. Belew John McInerney Nicol N. Schraudolph 
Date: June, 1990  
Address: La Jolla, CA 92093  
Affiliation: Cognitive Computer Science Research Group Computer Science Engr. Dept. (C-014) Univ. California at San Diego  
Pubnum: CSE Technical Report #CS90-174  
Abstract-found: 0
Intro-found: 1
Reference: [Ackley, 1987] <author> Ackley, D. H. </author> <year> (1987). </year> <title> A connectionist machine for genetic hillclimbing. </title> <publisher> Kluwer, </publisher> <address> Boston. </address>
Reference: [Barnard and Cole, 1989] <author> Barnard, E. and Cole, R. A. </author> <year> (1989). </year> <title> A neural-net training program based on conjugate-gradient optimization. </title> <type> Technical report, </type> <institution> Dept. Computer Science and Engr., Oregon Grad. Ctr., Beaverton, </institution> <address> OR. </address>
Reference-contexts: Conjugate gradient (CG) optimization was used in these experiments because it is known to converge to solutions more quickly and reliably than heuristic second-order optimization techniques like BP (with a momentum term) <ref> [Barnard and Cole, 1989] </ref> 9 .
Reference: [Belew, 1990] <author> Belew, R. K. </author> <year> (1990). </year> <title> Evolution, learning and culture: computational metaphors for adaptive search. </title> <journal> Complex Systems, </journal> <volume> 4(1). </volume>
Reference-contexts: In fact, new computational models of learning and evolution offer theoretical biology new tools for addressing questions about Nature that have dogged that field since Darwin <ref> [Belew, 1990, Kauffman and Smith, 1986] </ref>. However, these same models have proven interesting enough to computer scientists that they can also be treated as artificial algorithms, divorced from the natural phenomena from which the models originally sprung. <p> Previous work has demonstrated that at least some of the desirable interactions between learning and evolution can be explained via indirect mechanisms, such as the "Baldwin Effect" <ref> [Belew, 1990] </ref>. Experiments reported here suggest another computational reason why direct Lamarckian inheritance cannot be possible.
Reference: [Bethke, 1981] <author> Bethke, A. </author> <year> (1981). </year> <title> Genetic algorithms as function optimiz-ers. </title> <type> Technical report, </type> <institution> Logic of Computers Group, CCS Dept., University of Michigan, </institution> <address> Ann Arbor, MI. </address>
Reference-contexts: This is not to say that the GA works on all problems equally well, only that these differences can be attributed to the underlying search spaces rather than the semantics of the problem domain <ref> [Bethke, 1981] </ref>. Having committed ourselves then to the GA and its crossover operator, it is worth noting some of the central "pearls of GA wisdom" that are most salient to the problem of encoding networks onto GA strings.
Reference: [Brady, 1985] <author> Brady, R. M. </author> <year> (1985). </year> <title> Optimization strategies gleaned from nature. </title> <journal> Nature, </journal> <volume> 317 </volume> <pages> 804-806. </pages>
Reference-contexts: Considered separately, both connectionist networks and "evolutionary algorithms" have recently drawn a great deal of attention as new forms of adaptive algorithhm. On occasion, the two techniques have been compared (e.g., <ref> [Brady, 1985] </ref>).
Reference: [Caruana and Schaffer, 1988] <author> Caruana, R. and Schaffer, J. D. </author> <year> (1988). </year> <title> Representation and hidden bias: Gray vs. binary coding for genetic algorithms. </title> <booktitle> In Proc. Fifth Intl. Conf. on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Assume also that B bits have been allocated to represent each weight; these are sufficient to divide the bounded region into 2 B intervals. The B bit index is then Gray-coded to minimize the Hamming distance between indices close in value <ref> [Caruana and Schaffer, 1988] </ref>. Within the specified interval, then, a real number is selected from a random variable uniformly distributed over that interval.
Reference: [Cowan and Friedman, 1990] <author> Cowan, J. D. and Friedman, A. E. </author> <year> (1990). </year> <title> Development and regeneration of eye-brain maps: A computational model. </title> <booktitle> In Advances in Neural Info. Proc. Systems 2. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Purves outlines the basic features of a more elaborate cell adhesion model of neural development [Purves, 1988]; Edelman also desribes an elaborate but idiosyncratic model [Edelman, 1987]. The details of retina-to-optic tectum mappings have been described by Cowan <ref> [Cowan and Friedman, 1990] </ref>. Stork has used a developmental model with the GA to show how evolutionary "pre-adaptations" may be responsible for Belew, McInerney & Schraudolf: Evolving Networks 15 certain anomolous neural connections in the modern crayfish [Stork et al., 1990].
Reference: [DeJong, 1980] <author> DeJong, K. </author> <year> (1980). </year> <title> Adaptive system design: A genetic approach. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics. </journal>
Reference-contexts: But this a priori division of effort is something we hope to avoid; the many successes with which the GA has been used to discover real-valued quantities suggests that it is also unnecessary <ref> [DeJong, 1980] </ref>. region [m; M ]. Assume also that B bits have been allocated to represent each weight; these are sufficient to divide the bounded region into 2 B intervals. <p> The GA is doing function optimization over a set of parameters in the same way that GAs have been used since DeJong <ref> [DeJong, 1980] </ref>. A more important combination of these technologies arises from the observation that local search performed by back propagation and other gradient descent procedures is well complemented by the global sampling performed by the GA; consider Figure 8.
Reference: [Edelman, 1987] <author> Edelman, G. </author> <year> (1987). </year> <title> Neural Darwinism: The theory of neuronal group selection. </title> <publisher> Basic Books, </publisher> <address> New York. </address>
Reference-contexts: These experiments are consistent with only the most basic features of the corresponding biological systems, and we intend to explore more satisfactory models. Purves outlines the basic features of a more elaborate cell adhesion model of neural development [Purves, 1988]; Edelman also desribes an elaborate but idiosyncratic model <ref> [Edelman, 1987] </ref>. The details of retina-to-optic tectum mappings have been described by Cowan [Cowan and Friedman, 1990].
Reference: [Fogel et al., 1966] <author> Fogel, L., Owens, A., and Walsh, M. </author> <year> (1966). </year> <title> Artificial intelligence through simulated evolution. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: The interested reader is advised to begin a more thorough introduction to these algorithms with the excellent new text by Goldberg [Goldberg, 1989]. Attempts to simulate evolutionary search date back as far as the first attempts to simulate neural networks <ref> [Fogel et al., 1966] </ref>. The basic construction is to consider a population of individuals that each represent a potential solution to a problem.
Reference: [Goldberg, 1989] <author> Goldberg, D. </author> <year> (1989). </year> <title> Genetic algorithms in search, optimization, and machine learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: The interested reader is advised to begin a more thorough introduction to these algorithms with the excellent new text by Goldberg <ref> [Goldberg, 1989] </ref>. Attempts to simulate evolutionary search date back as far as the first attempts to simulate neural networks [Fogel et al., 1966]. The basic construction is to consider a population of individuals that each represent a potential solution to a problem.
Reference: [Grefenstette, 1985] <editor> Grefenstette, J., editor (1985). </editor> <booktitle> Proc. First Intl. Conf. on Genetic Algorithms and their applications. </booktitle> <editor> Belew, McInerney & Schraudolf: </editor> <booktitle> Evolving Networks 45 </booktitle>
Reference: [Grefenstette, 1987] <editor> Grefenstette, J., editor (1987). </editor> <booktitle> Proc. 2nd Intl. Conf. on Genetic Algorithms. </booktitle> <publisher> Lawrence Erlbaum. </publisher>
Reference: [Grefenstette, 1981] <author> Grefenstette, J. J. </author> <year> (1981). </year> <title> Parallel adaptive algorithms for function optimization. </title> <type> Technical Report CS-81-19, </type> <institution> Computer Science Dept., Vanderbilt Univ., Nashville, TN. </institution>
Reference-contexts: We have exploited this feature of our hybrids in an extension of Grefenstette's GENESIS simulator developed by our group. The basic architecture of this Distributed GA (DGA) design is shown in Figure 19; Grefenstette has called this a "syn-cronized master-slave" architecture for the GA <ref> [Grefenstette, 1981] </ref>. Assume that some number H of hosts are connected via a local area network. 17 A Host Table with the name and network addresses of these hosts is constructed, a BP Server program is initialized on each. <p> For example, one undesirable bottleneck is created by the end of each generation, when all but the last one or two individuals have been evaluated and the rest of the processors must wait; Grefenstette calls this a "semi-syncronized master-slave" architecture <ref> [Grefenstette, 1981] </ref>. In general, with populations of reasonable size it is unlikely that these last few evaluations are critical, and so it seems reasonable to relax the constraint of a rigidly synchronized generational structure.
Reference: [Hanson and Burr, 1990] <author> Hanson, S. and Burr, D. </author> <year> (1990). </year> <title> What connectionist models learn: Learning and representation in connectionist networks. </title> <journal> Behavioral and Brain Sciences. </journal>
Reference-contexts: Belew, McInerney & Schraudolf: Evolving Networks 20 330]; Miyata has operationalized this as "... uniformly distributed random numbers between 1 2 " [Miyata, 1987]. Final weights, on the other hand, can be widely distributed, and often fall outside this initial distrution <ref> [Hanson and Burr, 1990] </ref>. The distinction between starting and final points in weight space also complicates our characterization of just what the GA is looking for. <p> For these experiment our initial strategy was simply to select upper and lower bounds for W (0) , and the number of bits per weight. Based in part on Hanson and Burr's analysis of two large and well-studied networks <ref> [Hanson and Burr, 1990] </ref>, we allowed W (0) to range between approximately: 0:01 j W (0) j 12 and allowed 10 bits/weight. Using this encoding, the experiments of the last section combining GA and BP were repeated.
Reference: [Harp et al., 1989] <author> Harp, S., Samad, T., and Guha, A. </author> <year> (1989). </year> <title> Towards the genetic synthesis of neural networks. </title> <booktitle> In Proc. Third Intl. Conf. on Genetic Algorithms, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: ID Total Size Dimension 1 Share Dimension 2 Share Dimension 3 Share Start-of-Projection Target Address Address Mode Dimension 1 Radius Dimension 2 Radius Dimension 3 Radius Connection Density Initial Eta Eta Slope End-of-Area Harp et al. describe a very interesting model in which the GA searches for a network "blueprint" <ref> [Harp et al., 1989] </ref>; see Figure 6.
Reference: [Hillis, 1990] <author> Hillis, W. D. </author> <year> (1990). </year> <title> Co-evolving parasites improve simulated evolution as an optimization procedure. </title> <booktitle> In Proc. Conf. on Emergent Computation. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: As massively parallel computers become available, using a similarly profligate strategy of redundant search may become more sensible in algorithmic design as well (e.g., Hillis' search for efficient sort routines <ref> [Hillis, 1990] </ref>). Second, a computational view provides new insight into the exploitation of individuals' learning and the evolution of a species.
Reference: [Holland, 1975] <author> Holland, J. </author> <year> (1975). </year> <title> Adaptation in natural and artificial systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, MI. </address>
Reference-contexts: A software facility that exploits the natural parallelism when the GA is used to control multiple instantiations of BP simulation is discussed, and some features of the time and space complexity of the hybrid systems are considered. 2 Genetic algoritms The GA has been investigated by John Holland <ref> [Holland, 1975] </ref> and students of his for almost twenty years now, with a marked increase in interest within the last few years [Grefenstette, 1985,Grefenstette, 1987,Schaffer, 1989]. The interested reader is advised to begin a more thorough introduction to these algorithms with the excellent new text by Goldberg [Goldberg, 1989]. <p> Briefly, Holland's Schemata Theorem <ref> [Holland, 1975, Thm. 6.2.3] </ref> suggests that the initially random sampling of early generations is concentrated by the GA's search towards those areas of the search space demonstrating better-than-average performance.
Reference: [Huynen and Hogweg, 1989] <author> Huynen, M. A. and Hogweg, P. </author> <year> (1989). </year> <title> Genetic algorithms and information accumulation during the evolution of gene regulation. </title> <editor> In Schaffer, J. D., editor, </editor> <booktitle> Proc. Third Intl. Conf. on Genetic Algorithms, </booktitle> <address> Washington, D.C. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: At the same time, the crossover operator imposes severe constraints on the genomic representation, as the experiments with the representation of connectionist networks here will demonstrate. Conversely, modern genetics continues to uncover biological mechanisms that are potentially even more powerful operators than crossover <ref> [Huynen and Hogweg, 1989] </ref>. This paper, however, will restrict itself to the GA on the grounds that it currently provides the best balance between empirically demonstrated adaptive power and theoretical understanding.
Reference: [Kauffman and Smith, 1986] <author> Kauffman, S. and Smith, R. G. </author> <year> (1986). </year> <title> Adaptive automata based on Darwinian selection. </title> <journal> Physica D, </journal> <volume> 22. </volume>
Reference-contexts: In fact, new computational models of learning and evolution offer theoretical biology new tools for addressing questions about Nature that have dogged that field since Darwin <ref> [Belew, 1990, Kauffman and Smith, 1986] </ref>. However, these same models have proven interesting enough to computer scientists that they can also be treated as artificial algorithms, divorced from the natural phenomena from which the models originally sprung.
Reference: [Kolen and Pollack, 1990] <author> Kolen, J. F. and Pollack, J. B. </author> <year> (1990). </year> <title> Back propagation is sensitive to initial conditions. </title> <type> Technical Report 90-JK-BPSIC, </type> <institution> CIS Dept., Ohio St. Univ., Columbus, OH. </institution>
Reference-contexts: 2 B -1 RANDOMIZERANDOMIZE m M TRANSFORMTRANSFORM Genome B bits Z in [0,2 B -1] R in [m,M] Belew, McInerney & Schraudolf: Evolving Networks 7 will often introduce great variability in the resulting networks' performance, as connectionist networks have been shown to be extremely sensitive to small changes in weights <ref> [Kolen and Pollack, 1990] </ref>. More constructively, it would be desirable if this encoding (the range encoded, the number of bits used) were varied as a consequence of the variability experienced across the population. <p> Recent results by Kolen and Pollack demonstrate that "BP is sensitive to initial conditions" (i.e., what we call W (0) ) <ref> [Kolen and Pollack, 1990] </ref> , and so finding such reliable regions is non-trivial (cf. Section 5.4). Thus the goal of our search is somewhat different from most connectionist systems: we are interested in the distribution of good solutions rather than simply identifying some of these. <p> Some basic bounds on a good W (0) region can limit search to a "donut" around the origin; see Figure 12 and accompanying discussion. But even within this donut there is significant variability. The experiments of Kolen and Pollack provide an interesting comparison with our own <ref> [Kolen and Pollack, 1990] </ref>. Among other experiments, they performed a Monte Carlo search through (what we call) W (0) -space for the problem of 2-bit XOR with two hidden units.
Reference: [Merrill and Port, 1988] <author> Merrill, J. W. L. and Port, R. F. </author> <year> (1988). </year> <title> A stochastic learning algorithm for neural networks. </title> <type> Technical Report 236, </type> <institution> Dept. Linguistics and Computer Science, Indiana Univ., Bloomington, </institution> <note> IN. </note>
Reference-contexts: Consider a standard three-layer, feed-forward network. At least in these networks, the obvious functional units correspond to units in the hidden layer. This suggests that all weights associated with one hidden unit should be placed together on the string. Merrill has performed experiments that substantiate this <ref> [Merrill and Port, 1988] </ref>. Such functional units can be made even more cohesive by introducing "punctuated" crossover operations, which have higher probability of breaking the chromosome at certain punctuated points in the string (e.g., between one hidden unit's weight and another's) [Schaffer and Morishima, 1985].
Reference: [Miller et al., 1989] <author> Miller, G., Todd, P., and Hegde, S. </author> <year> (1989). </year> <title> Designing neural networks using genetic algorithms. </title> <booktitle> In Proc. Third Intl. Conf. on Genetic Algorithms, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher> <editor> Belew, McInerney & Schraudolf: </editor> <booktitle> Evolving Networks 46 </booktitle>
Reference-contexts: Belew, McInerney & Schraudolf: Evolving Networks 10 GA is used to replicate and alter the binary network descriptions to form a new population, and the cycle is then repeated. Miller et al. report on experiments using this sort of wiring diagram <ref> [Miller et al., 1989] </ref>. Their most striking result was with the Four-quadrant problem, a generalization of the XOR problem to a two-dimensional real interval; see Figure 3. This problem is interesting because it admits at least two types of solution, as shown in Figure 4.
Reference: [Minsky and Papert, 1988] <author> Minsky, M. and Papert, S. </author> <year> (1988). </year> <title> Perceptrons (expanded edition). </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Section 3.3), it is known to permit two distinctly different network solutions <ref> [Minsky and Papert, 1988, p. 252-253] </ref>. For the experiments reported here we used the six-bit version of this problem. A three-layer feedforward network with six input units, six hidden units, and one output unit was specified.
Reference: [Miyata, 1987] <author> Miyata, Y. </author> <year> (1987). </year> <title> Sunnet version 5.2. </title> <type> Technical Report 8708, </type> <institution> Inst. for Cognitive Science, </institution> <address> UCSD, La Jolla, CA. </address>
Reference-contexts: In practice, these two sets are often quite different. For example, the PDP volumes recommend "... small random weights" [Rumelhart et al., 1986, p. Belew, McInerney & Schraudolf: Evolving Networks 20 330]; Miyata has operationalized this as "... uniformly distributed random numbers between 1 2 " <ref> [Miyata, 1987] </ref>. Final weights, on the other hand, can be widely distributed, and often fall outside this initial distrution [Hanson and Burr, 1990]. The distinction between starting and final points in weight space also complicates our characterization of just what the GA is looking for. <p> hybrid algorithm is the same as the GA+CG hybrid above: The GA was used to create an initial population of W (0) vectors, BP was then used to optimize each of these, the MSE of each result was used for the individual's fitness, the GA used 13 Miyata's SunNet simulator <ref> [Miyata, 1987] </ref> was used on the problem of six-bit symmetry with six hidden units; = 0:1; ff = 0:2. Belew, McInerney & Schraudolf: Evolving Networks 28 Belew, McInerney & Schraudolf: Evolving Networks 29 these to produce a new population of W (0) vectors, and the cycle repeats itself.
Reference: [Montana and Davis, 1989] <author> Montana, D. J. and Davis, L. </author> <year> (1989). </year> <title> Training feedforward networks using genetic algorithms. </title> <booktitle> In Proc. IJCAI. </booktitle> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Perhaps for this reason, several of the earlier attempts to use the GA with connectionist networks have left real numbers as discrete elements in their representation, thus avoiding this encoding issue <ref> [Montana and Davis, 1989, Whitley and Hanson, 1989] </ref>. The GA is then allowed to search for good combinations of weights, but is not used for finding the value of any one weight.
Reference: [Offutt, 1989] <author> Offutt, D. </author> <year> (1989). </year> <title> A reinforcement learning algorithm for training fully and bidirectionally-interconnected connectionist networks. </title> <type> (draft). </type>
Reference-contexts: Note that this stochastic element 2 Some tentative results suggest that with this encoding the GA can find weights more quickly than back propagation, but only on fairly deep networks (i.e., with many hidden layers) <ref> [Offutt, 1989] </ref>.
Reference: [Purves, 1988] <author> Purves, D. </author> <year> (1988). </year> <title> Body and brain: A trophic theory of neural connections. </title> <publisher> Harvard Univ. Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The process relating genotype to phenotype is, of course, ontogenetic development. The developmental process is an extraordinarily complicated adaptive system in its own right <ref> [Purves, 1988] </ref>, and attempting to incorporate it within the already complicated hybrids being considered here is problematic. 5 But the incorporation of a developmental interpreter means that the GA can be allowed to search through representations for which it is more well-suited than those derived directly from networks. <p> These experiments are consistent with only the most basic features of the corresponding biological systems, and we intend to explore more satisfactory models. Purves outlines the basic features of a more elaborate cell adhesion model of neural development <ref> [Purves, 1988] </ref>; Edelman also desribes an elaborate but idiosyncratic model [Edelman, 1987]. The details of retina-to-optic tectum mappings have been described by Cowan [Cowan and Friedman, 1990].
Reference: [Rissanen, 1989] <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic complexity in statistical inquiry. </title> <type> Technical Report RJ-6901, </type> <institution> IBM Research Div., Yorktown Heights, NY. </institution>
Reference-contexts: Ris-sanen's "minimum description length" formalism provides a rigorous measure of a model's complexity <ref> [Rissanen, 1989] </ref>, and Tenorio and Lee have made an initial attempt to apply this to connectionist network architectures [Tenorio and Lee, 1989].
Reference: [Rizki and Conrad, 1986] <author> Rizki, M. and Conrad, M. </author> <year> (1986). </year> <title> Computing the theory of evolution. </title> <journal> Physica D, </journal> <volume> 42 </volume> <pages> 83-99. </pages>
Reference: [Rumelhart et al., 1986] <author> Rumelhart, D., Hinton, G., and Williams, R. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Mc-Clelland, J. and Rumelhart, D., editors, </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, chapter 8. </booktitle> <publisher> Bradford Books, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Consider again the example of a simple three-layer, feed-forward back propagation (BP) network, and consider the solutions it might discover to the "encoder" problem. 3 A typical solution, reported in the PDP volumes <ref> [Rumelhart et al., 1986, p. 337] </ref>, is 3 The encoder problem involves mapping N orthogonal patterns through a hidden layer of log 2 N hidden units onto a set of N orthogonal output patterns [Rumelhart et al., 1986, p. 335]. <p> it might discover to the "encoder" problem. 3 A typical solution, reported in the PDP volumes [Rumelhart et al., 1986, p. 337], is 3 The encoder problem involves mapping N orthogonal patterns through a hidden layer of log 2 N hidden units onto a set of N orthogonal output patterns <ref> [Rumelhart et al., 1986, p. 335] </ref>. Belew, McInerney & Schraudolf: Evolving Networks 8 shown in Figure 2. Note that while this network discovered the same binary encoding scheme a computer scientist might suggest, it also made use of intermediate activation values.
Reference: [Schaffer and Morishima, 1985] <author> Schaffer, J. and Morishima, A. </author> <year> (1985). </year> <title> An adaptive crossover distribution mechanism for genetic algorithms. </title> <editor> In Grefenstette, J., editor, </editor> <booktitle> Proc. Intl. Conf. on genetic algorithms and their applications. </booktitle>
Reference-contexts: Merrill has performed experiments that substantiate this [Merrill and Port, 1988]. Such functional units can be made even more cohesive by introducing "punctuated" crossover operations, which have higher probability of breaking the chromosome at certain punctuated points in the string (e.g., between one hidden unit's weight and another's) <ref> [Schaffer and Morishima, 1985] </ref>. One important property of the solutions learned by networks, however, is that they are generally far from unique. In the context of the GA, this means that crossover among two relatively good parents who have discovered different solutions can lead to abysmal offspring.
Reference: [Schaffer, 1989] <editor> Schaffer, J. D., editor (1989). </editor> <booktitle> Proc. Third Intl. Conf. on Genetic Algorithms, </booktitle> <address> Washington, D.C. </address> <publisher> Morgan Kaufman. </publisher>
Reference: [Schraudolph and Belew, 1990] <author> Schraudolph, N. N. and Belew, R. K. </author> <year> (1990). </year> <title> Dynamic parameter encoding for Genetic Algorithms. </title> <type> Technical Report LAUR 90-2795, </type> <institution> Los Alamos National Laboratory - Ctr. for Nonlinear Studies, Los Alamos, </institution> <note> NM. </note> <editor> Belew, McInerney & Schraudolf: </editor> <booktitle> Evolving Networks 47 </booktitle>
Reference-contexts: More constructively, it would be desirable if this encoding (the range encoded, the number of bits used) were varied as a consequence of the variability experienced across the population. This kind of dynamic parameter encoding for the GA is being explored separately <ref> [Schraudolph and Belew, 1990] </ref>. 3.2 Crossover with distributed representations Perhaps the most important feature of representation for the GA is proximity. <p> Conversely, the process of dynamic parameter encoding (DPE) can be used to focus the GA's search on those regions with least variability, so that a priori divisions of the search between GA and gradient procedures come to be reconsidered <ref> [Schraudolph and Belew, 1990] </ref>.
Reference: [Stork et al., 1990] <author> Stork, D. G., Walker, S., Burns, M., and Jackson, B. </author> <year> (1990). </year> <booktitle> Preadaptation in neural circuits. In Proc. IJCNN (Vol. 1), </booktitle> <address> New York. </address> <publisher> IEEE. </publisher>
Reference-contexts: The details of retina-to-optic tectum mappings have been described by Cowan [Cowan and Friedman, 1990]. Stork has used a developmental model with the GA to show how evolutionary "pre-adaptations" may be responsible for Belew, McInerney & Schraudolf: Evolving Networks 15 certain anomolous neural connections in the modern crayfish <ref> [Stork et al., 1990] </ref>. The range of connectionist network representations for the GA surveyed in this section still leaves much to be explored. Further, the GA can be used with connectionist networks in other ways than specification of network architecture.
Reference: [Tenorio and Lee, 1989] <author> Tenorio, M. F. and Lee, W. </author> <year> (1989). </year> <title> Self-organizing neural network for optimized supervised learning. </title> <type> Technical report, </type> <institution> School of Electrical Engr., Purdue Univ., W. Lafayette, </institution> <note> IN. </note>
Reference-contexts: Ris-sanen's "minimum description length" formalism provides a rigorous measure of a model's complexity [Rissanen, 1989], and Tenorio and Lee have made an initial attempt to apply this to connectionist network architectures <ref> [Tenorio and Lee, 1989] </ref>. Characterizing the description length of (binary) GA genomes promises to be more straight-forward, and the stage is then set for measuring the cumulative complexity of GA+network solutions in the manner originally suggested by Figure 18.
Reference: [Todd, 1988] <author> Todd, P. </author> <year> (1988). </year> <title> Evolutionary methods for connectionist architectures. </title>
Reference-contexts: One useful dimension along which these alternatives can be organized is what Todd has called "developmental specification": i.e., how complete and literal a representation of the network is encoded on the GA string <ref> [Todd, 1988] </ref>. At one extreme, it is possible to encode each of the network's weights, in full precision, and then use the GA to solve this as a standard multi-parameter function optimization problem; in this case, there is no role for connectionist Belew, McInerney & Schraudolf: Evolving Networks 5 learning. <p> Second, the binary specification of link presence or absence can easily be generalized to a wider range of constraints on network architectures. For example, Todd suggests that a ternary specification might specify that a link was absent, restricted to positive weights or restricted to negative weights <ref> [Todd, 1988] </ref>. The range of search allowed the connectionist learning procedure by the genetic network description can be progressively constrained in this fashion until, in the extreme, the GA is specifying each weight exactly.
Reference: [Whitley and Hanson, 1989] <author> Whitley, D. and Hanson, T. </author> <year> (1989). </year> <title> Optimizing neural networks using faster, more accurate genetic search. </title> <editor> In Schaf-fer, J. D., editor, </editor> <booktitle> Proc. Third Intl. Conf. on Genetic Algorithms, </booktitle> <address> Wash-ington, D.C. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: Perhaps for this reason, several of the earlier attempts to use the GA with connectionist networks have left real numbers as discrete elements in their representation, thus avoiding this encoding issue <ref> [Montana and Davis, 1989, Whitley and Hanson, 1989] </ref>. The GA is then allowed to search for good combinations of weights, but is not used for finding the value of any one weight. <p> If very small populations are used with the GA, there is not "room" for multiple alternatives to develop. In this case, whichever solution is discovered first comes to dominate the population and resist alternatives. This approach has been used successfully by Whitney <ref> [Whitley and Hanson, 1989] </ref>.
References-found: 38

