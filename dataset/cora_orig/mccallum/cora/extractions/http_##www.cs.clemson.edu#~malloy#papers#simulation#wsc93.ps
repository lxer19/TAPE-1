URL: http://www.cs.clemson.edu/~malloy/papers/simulation/wsc93.ps
Refering-URL: http://www.cs.clemson.edu/~malloy/simulation_papers.html
Root-URL: http://www.cs.clemson.edu
Title: THE VALIDATION OF A MULTIPROCESSOR SIMULATOR  
Author: Brian A. Malloy 
Address: 29634-1906  
Affiliation: Department of Computer Science Clemson University Clemson, South Carolina  
Abstract: In this paper, we present the design and implementation of a multiprocessor simulator written in the language SimCal. We use the simulator to test our scheme to partition a sequential program for parallel execution on a shared memory, asynchronous multiprocessor. The results of the simulations indicate that our partitioning scheme can provide significant speed-up by executing the program in parallel. We then execute the partitioned program on an actual multiprocessor and find a high degree of correlation between the simulations and the actual executions. This correlation serves to validate our simulator. We then use the multiprocessor simulator to hypothetically extended the actual multiprocessor and we show that adding more processors will not provide significant improvement in the parallel executions unless the communication structure is also improved to contain more parallelism. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Maurice J. Bach, </author> <title> The Design of the Unix Operating System, </title> <publisher> Prentice-Hall Inc., </publisher> <year> 1986. </year>
Reference-contexts: As we will show, we obtained an excellent correlation between these "actual executions' and the simulations, thereby validating our multiprocessor simulator. The send and receive primitives were implemented on the AViiON using spin-lock operations on unix shared variables <ref> [1] </ref>. In order to obtain the parameters for our simulator, we first conducted a series of experiments to determine the average cost of the send and receive primitives and the cost of using the unibus communication structure.
Reference: 2. <author> Anne Dinning, </author> <title> ``A Survey of Synchronization Methods for Parallel Computers,'' </title> <booktitle> Computer, </booktitle> <pages> pp. 66-76, </pages> <month> July, </month> <year> 1989. </year>
Reference-contexts: Because of the kind of synchronization required here (i.e., based on data dependencies), we assume that the send operation does not require the invoker to wait until a corresponding receive is executed <ref> [2] </ref>. In conjunction with the above system, we employ three parameters that, together, describe the "speed" of the architecture. The first is a function F e (I) that returns the number of cycles required to execute instruction I.
Reference: 3. <author> Data General, </author> <title> Installing and Managing the DG/UX System, Data General Corporation, </title> <year> 1990. </year>
Reference-contexts: We briey discuss our techniques to partition a sequential program into threads for parallel execution on a shared memory, asynchronous multiprocessor. We then use the simulator to execute the threads for parameters that describe an actual multiprocessor system, the Data General AViiON <ref> [3] </ref>. The correlation that we obtain between the simulations and the actual executions verify that the multiprocessor simulator captures the important factors of the multiprocessor system. <p> Performance of the Partitioning Scheme on a Data General Multiprocessor In order to determine the performance of our partitioning scheme on a "real" multiprocessor, we executed our parallel threads on a Data General AViiON shared memory multiprocessor system <ref> [3] </ref> equipped with a unibus communication structure and two identical processors. As we will show, we obtained an excellent correlation between these "actual executions' and the simulations, thereby validating our multiprocessor simulator. The send and receive primitives were implemented on the AViiON using spin-lock operations on unix shared variables [1].
Reference: 4. <author> Rajiv Gupta, </author> <title> ``Employing Register Channels for the Exploitation of Instruction level Parallelism,'' </title> <booktitle> Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle Washington, </address> <month> March, </month> <year> 1990. </year>
Reference-contexts: The second is a function F c = F a + F w , that indicates the number of cycles needed for communication of values through the interconnection structure. By an interconnection structure or communication structure we mean hardware support such as memory channels [5], register channels <ref> [4] </ref> or an interconnection network [7] that provides support for communication of values. Here, the function F a is the access time needed to traverse the communication structure and F w is the number of cycles a processor waits (due to contention) before it can access a required value.
Reference: 5. <author> Janusz S. Kowalik, </author> <title> ``Parallel MIMD Computation: HEP Supercomputer & Its Applications,'' Scientific Computation Series, </title> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: The second is a function F c = F a + F w , that indicates the number of cycles needed for communication of values through the interconnection structure. By an interconnection structure or communication structure we mean hardware support such as memory channels <ref> [5] </ref>, register channels [4] or an interconnection network [7] that provides support for communication of values.
Reference: 6. <author> G. Lamprecht, </author> <title> Introduction to Simula 67, </title> <publisher> Friedr. Vieweg & Sohn, </publisher> <address> Braunschweig/Wiesbaden, </address> <year> 1983. </year>
Reference-contexts: The SimCal language is extended to directly incorporate simulation primitives designed to have essentially the same syntax and semantics as those found in Simula. Therefore, a SimCal user, knowledgeable in Pascal, need only consult previous work [8, 9] or a Simula reference text <ref> [6] </ref> for information regarding the syntax and semantics of the simulation primitives. The simulation primitives are directly incorporated into Pascal, meaning that the user is not responsible for adding any calls to system procedures or declaring any extra data structures.
Reference: 7. <author> Tomas Lang, </author> <title> ``Interconnections Between Processors and Memory Modules Using the Shufe-Exchange Network,'' </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-25, No 5, </volume> <month> May </month> <year> 1976. </year>
Reference-contexts: By an interconnection structure or communication structure we mean hardware support such as memory channels [5], register channels [4] or an interconnection network <ref> [7] </ref> that provides support for communication of values. Here, the function F a is the access time needed to traverse the communication structure and F w is the number of cycles a processor waits (due to contention) before it can access a required value.
Reference: 8. <author> B. A. Malloy and M.L. Soffa , ``Simcal: </author> <title> The Merger of Simula and Pascal,'' </title> <booktitle> Proceedings Winter Simulation Conference, </booktitle> <pages> pp. 397-403, </pages> <address> Washing-ton D. C., </address> <year> 1986. </year>
Reference-contexts: The multiprocessor simulator that we present is coded in a simulation language, SimCal <ref> [8] </ref>, that is based on Simula. Simula is a powerful, process oriented simulation language that possesses a high degree of express-ibility. The paper is organized as follows. In section 2, we briey describe SimCal, the simulation language that we use to implement the multiprocessor simulator. <p> The SimCal language is extended to directly incorporate simulation primitives designed to have essentially the same syntax and semantics as those found in Simula. Therefore, a SimCal user, knowledgeable in Pascal, need only consult previous work <ref> [8, 9] </ref> or a Simula reference text [6] for information regarding the syntax and semantics of the simulation primitives. The simulation primitives are directly incorporated into Pascal, meaning that the user is not responsible for adding any calls to system procedures or declaring any extra data structures.
Reference: 9. <author> B. A. Malloy and M. L. Soffa, </author> <title> ``Conversion of Simulation Processes to Pascal Constructs,'' </title> <journal> Software Practice and Experience, </journal> <volume> vol. 20(2), </volume> <pages> pp. 191-207, </pages> <month> Feb </month> <year> 1990. </year>
Reference-contexts: The SimCal language is extended to directly incorporate simulation primitives designed to have essentially the same syntax and semantics as those found in Simula. Therefore, a SimCal user, knowledgeable in Pascal, need only consult previous work <ref> [8, 9] </ref> or a Simula reference text [6] for information regarding the syntax and semantics of the simulation primitives. The simulation primitives are directly incorporated into Pascal, meaning that the user is not responsible for adding any calls to system procedures or declaring any extra data structures. <p> As a preprocessor, SimCal sits on top of the Pascal compiler and therefore requires no alterations to the compiler in any way. 2.1. Using SimCal Language Primitives for Simulation Modeling We have previously described the design and implementation of SimCal <ref> [9] </ref>, and the reader interested in the preprocessor construction may consult this work. We do n ot discuss the SimCal design and implementation in this paper but rather we summarize the actions of the language primitives and demonstrate how they can be used to construct a simulation model. <p> This is achieved by supplying appropriate values for the parameters p, F e (I), F c , and BW, to the simulator that we constructed using SimCal <ref> [9] </ref>. 4.1.
Reference: 10. <author> B. A. Malloy, E.L. Lloyd, </author> <title> and M.L. Soffa., ``A Fine Grained Approach to Scheduling Asynchronous Multiprocessors,'' </title> <booktitle> 4th International Conference on Computing and Information,, </booktitle> <pages> pp. 131-135, </pages> <month> May, </month> <year> 1992. </year>
Reference-contexts: Threads We have developed techniques to partition sequential code for parallel multiprocessor execution [10-12]. We now present key ideas of the technique for partitioning straight line code, such as the code found in basic blocks, into threads for parallel execution <ref> [10, 12] </ref>. The interested reader may consult our previous work [11] for a discussion of partitioning _____________________________________________ 1 Rv 2 3 Sd 3 4 Rv 5 6 Time P1 P2. entire programs for asynchronous multiprocessor execution.
Reference: 11. <author> B. A. Malloy, R. Gupta, and M. L. Soffa,, </author> <title> ``A Shape Matching Approach for Scheduling Fine-Grained Parallelism,'' </title> <booktitle> MICRO-25, The 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 131-135, </pages> <month> Dec </month> <year> 1992. </year>
Reference-contexts: We now present key ideas of the technique for partitioning straight line code, such as the code found in basic blocks, into threads for parallel execution [10, 12]. The interested reader may consult our previous work <ref> [11] </ref> for a discussion of partitioning _____________________________________________ 1 Rv 2 3 Sd 3 4 Rv 5 6 Time P1 P2. entire programs for asynchronous multiprocessor execution.
Reference: 12. <author> B. A. Malloy, E. L. Lloyd, and M. L. Soffa, </author> <title> ``Scheduling Dags for Asynchronous Multiprocessor Execution,'' </title> <journal> IEEE Transactions on Parallel and Distributed Computing, </journal> <year> 1994. </year>
Reference-contexts: Threads We have developed techniques to partition sequential code for parallel multiprocessor execution [10-12]. We now present key ideas of the technique for partitioning straight line code, such as the code found in basic blocks, into threads for parallel execution <ref> [10, 12] </ref>. The interested reader may consult our previous work [11] for a discussion of partitioning _____________________________________________ 1 Rv 2 3 Sd 3 4 Rv 5 6 Time P1 P2. entire programs for asynchronous multiprocessor execution.
References-found: 12

