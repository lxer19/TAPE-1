URL: http://www.cs.washington.edu/research/projects/lis/www/rapid/pubs/asap97.ps.Z
Refering-URL: http://www.cs.washington.edu/research/projects/lis/www/rapid/index.html
Root-URL: 
Title: Configurable Computing: The Catalyst for High-Performance Architectures  
Author: Carl Ebeling, Darren C. Cronquist and Paul Franklin 
Note: 1: Introduction This work was supported in part by the Defense Advanced Research Projects Agency under Contract DAAH04-94-G0272. D. Cronquist was supported in part by an IBM fellowship. P. Franklin was supported by an NSF fellowship.  
Address: Box 352350 Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Recent trends in the cost and performance of application-specific hardware relative to conventional processors discourage investing much time and energy in special-purpose architectures except for niche applications. These trends, however, may be reversed by the increasing complexity of computer architectures and the advent of configurable computing. Configurable computers have attracted considerable attention recently because they promise to deliver the performance of application-specific hardware along with the flexibility of general-purpose computers. In this paper, we will discuss some of the forces driving configurable computing, and we will argue that new configurable architectures are needed to realize the enormous potential of configurable computing. In particular, we believe that the commercial FPGAs currently used to construct configurable computers are too fine-grained to achieve good cost-performance on computationally-intensive applications that demand high-performance hardware. We then describe a new architecture called RaPiD (Reconfigurable Pipelined Datapaths), which is optimized for highly repetitive, computationally-intensive tasks. Very deep application-specific computation pipelines can be configured in RaPiD that deliver very high performance for a wide range of applications. RaPiD achieves this using a coarse-grained reconfigurable architecture that mixes the appropriate amount of static configuration with dynamic control. Over the past twenty years, mainstream processor technology has achieved enormous gains that have overshadowed much of the research that has been done on application specific hardware. With overall performance that doubles about every two years and the generality and flexibility of programmed hardware, processors have been hard to beat. There are indications, however, that this record of success will be difficult to maintain. The current gains in processor performance have been the result of advances in both process technology and computer architecture. Although process technology shows no sign of losing steam soon, processor architecture has become enormously complicated and achieving further increases in instruction-level parallelism will be difficult at best. We are now at the point where throwing more transistors at the problem yields little improvement for single instruction stream processors. Future advances will have to look to multi-processing solutions, but new breakthroughs are unlikely given the amount research has already been invested in this area. 
Abstract-found: 1
Intro-found: 0
Reference: [1] <author> M. Annaratone et al. </author> <title> The warp computer: architecture, implementation, and performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1523-38, </volume> <year> 1987. </year>
Reference-contexts: Systolic-based architectures have been invented for a wide variety of problems, some of which have even been implemented. But custom systolic arrays inevitably failed because of a lack of generality and flexibility. Programmable versions of systolic arrays attempted to address this problem, but these have had limited success. iWarp <ref> [1, 4] </ref>, which was the culmination of the CMU systolic array effort in partnership with Intel, was essentially a single chip microprocessor with enhancements that allowed systolic arrays to be constructed from multiple iWarp chips. Economic realities relegated iWarp to a small niche from which it did not escape. <p> RaPiD is most closely related to the systolic arrays developed in the 1980s in terms of computation style and application domain. But RaPiD fills the gap between static systolic arrays, which are too inflexible, and programmable systolic arrays like Warp and iWarp <ref> [1] </ref>, which more closely resemble multiprocessors with too much control overhead. Space has precluded any discussion of programming RaPiD. Programmability is a major problem for configurable computers, which has limited their widespread use.
Reference: [2] <author> J. M. Arnold, D. A. Buell, D. T. Hoang, D. V. Pryor, N. Shirazi, and M. R. Thistle. </author> <title> The Splash 2 processor and applications. </title> <booktitle> In Proceedings IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 482-5. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Indeed, the most success in the area of configurable computing has resulted from applying the ideas of application-specific array architectures, primarily systolic arrays. The two best examples are PAM from DEC-PRL [12], which relied heavily on systolic algorithms, and the Splash configurable computer <ref> [2] </ref>, which used FPGAs to construct a programmable systolic architecture. In spite of promising results from this work, configurable computing has still not been able achieve significant success outside the research lab.
Reference: [3] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Pe-terson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting systolic and memory communication in iwarp. </title> <booktitle> In Proceedings, </booktitle> <pages> pages 70-81. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> May </month> <year> 1990. </year> <title> multiprocessor-interconnection-networks, parallel- architectures. </title>
Reference: [4] <author> Darren C. Cronquist et al. </author> <title> Mapping applications to the rapid configurable architecture. </title> <booktitle> In Field-Programmable Custom Computing Machines (FCCM-97), </booktitle> <year> 1997. </year>
Reference-contexts: Systolic-based architectures have been invented for a wide variety of problems, some of which have even been implemented. But custom systolic arrays inevitably failed because of a lack of generality and flexibility. Programmable versions of systolic arrays attempted to address this problem, but these have had limited success. iWarp <ref> [1, 4] </ref>, which was the culmination of the CMU systolic array effort in partnership with Intel, was essentially a single chip microprocessor with enhancements that allowed systolic arrays to be constructed from multiple iWarp chips. Economic realities relegated iWarp to a small niche from which it did not escape.
Reference: [5] <author> A. DeHon. </author> <title> Reconfigurable Architectures for General-Purpose Computing. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1996. </year>
Reference: [6] <author> J. Rose, A. El Gamal, and A Sangiovanni Vincentelli. </author> <title> Architecture of field-programmable gate arrays. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(7) </volume> <pages> 1013-29, </pages> <year> 1993. </year>
Reference-contexts: Unfortunately, this fine-grained circuit structure which makes them so general has a very high cost: Depending on the circuit being constructed, this cost-performance penalty can range from a factor of 20 for random logic to well over 100 for structured circuits like ALUs, multipliers and memory <ref> [6] </ref>. Even if advancing process technology is able to provide the requisite number of transistors, this overhead will always incur a high cost in delay, chip area and power. While a factor of 5 overhead might be tolerated, factors of 100 cannot be.
Reference: [7] <author> K. A. Vissers et al. </author> <title> Architecture and programming of two generations video signal processors. </title> <journal> Micro-processing & Microprogramming, </journal> <volume> 41(5-6):373-90, </volume> <year> 1995. </year>
Reference: [8] <author> J. E. Vuillemin, P. Bertin, D. Roncin, M. Shand, H. H. Touati, and P. Boucard. </author> <title> Programmable active memories: reconfigurable systems come of age. </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> 4(1) </volume> <pages> 56-69, </pages> <year> 1996. </year>
References-found: 8

