URL: http://cobar.cs.umass.edu/mlc94/papers/sazonov-wnek.ps
Refering-URL: 
Root-URL: 
Email: vsazonov@itchy.starlab.csc.com wnek@aic.gmu.edu  
Title: A Hypothesis-driven Constructive Induction Approach to Expanding Neural Networks  
Author: Vladimir N. Sazonov Janusz Wnek 
Address: 3160 Fairview Park Dr. 4400 University Dr. Falls Church, VA 22042 Fairfax, VA 22030  
Affiliation: Computer Sciences Corporation George Mason University  
Abstract: With most machine learning methods, if the given knowledge representation space is inadequate then the learning process will fail. This is also true with methods using neural networks as the form of the representation space. To overcome this limitation, an automatic construction method for a neural network is proposed. This paper describes the BP-HCI method for a hypothesis-driven constructive induction in a neural network trained by the backpropagation algorithm. The method searches for a better representation space by analyzing the hypotheses generated in each step of an iterative learning process. The method was applied to ten problems, which include, in particular, exclusive-or, MONK2, parity-6BIT and inverse parity-6BIT problems. All problems were successfully solved with the same initial set of parameters; the extension of representation space was no more than necessary extension for each problem.
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, E.B. and Haussler, D., </author> <title> "What Size Net Gives Valid Generalization," </title> <journal> Neural Computation, </journal> <volume> Vol. 1, </volume> <pages> pp. 151-160, </pages> <year> 1989. </year>
Reference-contexts: Choosing proper network size is important. If the network is too small, it will not be capable of solving given problem. On the other hand, if the network is too big then it may be over capable as well as too complex <ref> (Baum & Haussler, 1989) </ref>.
Reference: <author> Cios, K.J. and Liu, N., </author> <title> "A Machine Learning Method for Generation of a Neural Network Architecture: A Continuous ID3 Algorithm," </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> Vol. 3, No. 2, </volume> <pages> pp. 280-291, </pages> <year> 1992. </year>
Reference: <author> Fahlman, S.E. and Lebiere, C., </author> <title> "The Cascade Correlation Learning Architecture," </title> <journal> NIPS, </journal> <volume> Vol. 2, </volume> <pages> pp. 524-532, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In a sense the topology of our NN is polar to the topology given by cascade-correlation method <ref> (Fahlman & Lebiere, 1990) </ref> where there may be more than 1 hidden layer and each hidden layer contains only 1 neuron. The cascade-correlation method has an excellent rate of training.
Reference: <author> Hanson, S.J., </author> <title> "Meiosis Networks," </title> <journal> NIPS, </journal> <volume> Vol. 2, </volume> <pages> pp. 533-541, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1990. </year>
Reference-contexts: This is possible because their neurons can have different values of parameter SCALE, that makes computation much more complex. If we applied our method to parity-4BIT we would obtain a hidden layer with 4 neurons. Note that we do not split neurons <ref> (compare with Hanson, 1990 or Wynne-Jones, 1992) </ref>. When we create a new neuron all previous neurons remain intact. We create new neurons to help overloaded connections.
Reference: <author> Le Cun, Y., </author> <title> "A Learning Scheme for Asymmetric Threshold Networks," </title> <booktitle> Proceedings Cognitiva 85, </booktitle> <pages> pp. 599-604, </pages> <year> 1985. </year>
Reference: <author> Maniezzo, V., </author> <title> "Genetic Evolution of the Topology and Weight Distribution of Neural Networks," </title> <year> 1993. </year>
Reference-contexts: to set up topology and connection weights of an initial neural network based on domainspecific inference rules (Towell et al., 1991), techniques for trimming neural networks via relevance assessment (Mozer & Smolensky, 1988; Weigand et al., 1990), and genetic algorithm-based evolution of the topology and weight distribution of neural networks <ref> (Maniezzo, 1993) </ref>. This research concerns automatic determination of neural network topology, and is viewed as a constructive induction method. Constructive induction systems perform a double search, one for the most suitable representation space, and second for the most suitable concept description in this space (Wnek & Michalski, 1994).
Reference: <author> Minsky, M. and Papert, S. </author> <title> Perceptrons, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1969. </year>
Reference: <author> Mozer, M. and Smolensky, P., </author> <title> "Skeletonization: A Technique for Trimming the Fat from a Neural Network via Relevance Assessment," </title> <journal> NIPS, </journal> <volume> Vol. 1, </volume> <pages> pp. 107-115, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1988. </year>
Reference: <author> Nabhan, </author> <title> T.M. and Zomaya, A.Y., "Toward Generating Neural Network Structures for Function Approximation," </title> <journal> Neural Network, </journal> <volume> Vol. 7, </volume> <pages> pp. 89-99, </pages> <year> 1994. </year>
Reference: <author> Odri, S.V., Petrovcki, D.P. and Krstonosic, A., </author> <title> "Evolutional Development of a Multilevel Neural Network," Neural Network, </title> <address> Vol.6, pp.583-595, </address> <year> 1993. </year>
Reference: <author> Parker, </author> <title> D.B., "Learning-logic," </title> <type> Technical Report, </type> <institution> TR-47, Sloan School of Management, MIT, </institution> <year> 1985. </year>
Reference: <author> Redding, N.J., Kowalczyk, A. and Downs, T., </author> <title> "Constructive Higher-Order Network Algorithm That Is Polynomial Time," Neural Network, </title> <booktitle> Vol.6, </booktitle> <pages> pp. 997-1010, </pages> <year> 1993. </year>
Reference: <author> Rumelhart, D.E., Hinton, G.E. and Williams, </author> <title> R.J., "Learning Internal representations by Error Propagation," </title> <booktitle> in Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <publisher> D.E. </publisher>
Reference: <editor> Rumelhart and J.L. McClelland (Eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference: <editor> Rumelhart, D.E. and McClelland, J.L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. 1, </volume> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference: <author> Spackman, K.A., </author> <title> "Learning Categorical Decision Criteria in Biomedical Domains," </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, CA, </address> <pages> pp. 36-46, </pages> <year> 1988. </year>
Reference: <author> Sperduti, A. and Starita, A., </author> <title> "Speed Up Learning and Network Optimization With Extended Back Propagation," Neural Network, </title> <booktitle> Vol.6, </booktitle> <pages> pp. 365-383, </pages> <year> 1993. </year>
Reference: <author> Towell, G., Shavlik, J. and Craven, M., </author> <title> "Constructive Induction in Knowledge-based Neural Networks," </title> <booktitle> Proceedings of the 8th Machine Learning Workshop, </booktitle> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 213-217, </pages> <address> Evanston, IL, </address> <year> 1991. </year>
Reference-contexts: There is a new line of research involving determination of the topology of neural networks. These attempts include knowledge-based neural networks to set up topology and connection weights of an initial neural network based on domainspecific inference rules <ref> (Towell et al., 1991) </ref>, techniques for trimming neural networks via relevance assessment (Mozer & Smolensky, 1988; Weigand et al., 1990), and genetic algorithm-based evolution of the topology and weight distribution of neural networks (Maniezzo, 1993). <p> For this reason, this approach is a hypothesis-driven constructive induction (HCI), as opposed to a knowledge-driven approach (KCI) or data-driven approach (DCI) (Wnek & Michalski, 1994). In the context of neural network knowledge representation, the KCI approach can be identified, for example, with the KBANN system <ref> (Towell et al., 1991) </ref>. The DCI approach in neural networks is yet to be identified. 2 HCI APPROACH The HCI approach is based on repetitively detecting strong patterns in the learned concept descriptions and then using them in transforming the representation space.
Reference: <author> Towell, G. G. and Shavlik, J. W., </author> <title> "Refining Symbolic Knowledge Using Neural Networks," in M a c h i n e Learning: A Multistrategy Approach , Vol. IV, </title> <editor> Michalski, R.S. and G. Tecuci, </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> pp. </pages> <year> 1994. </year>
Reference: <author> Wynne-Jones, M., </author> <title> "Node Splitting: A Constructive Algorithm for Feed Forward Neural Networks," </title> <journal> NIPS, </journal> <volume> Vol. 4, </volume> <pages> pp. 1072-1079, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1992. </year>
Reference: <author> Weigand, A. Rumelhart, D. and Huberman, B., </author> <title> "Generalization by Weight-elimination with Application to Forecasting," </title> <journal> NIPS, </journal> <volume> Vol. 3, </volume> <pages> pp. 875-882, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1990. </year>
Reference: <author> Werbos, P.J. </author> <title> "Beyond Regression: New tools for prediction and analysis in the behavioral sciences," </title> <type> Ph.D. Dissertation, </type> <institution> Harvard University, </institution> <address> Cambridge, MA, </address> <year> 1974. </year>
Reference: <author> Wnek, J. and Michalski, </author> <title> R.S., "Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments," </title> <journal> Machine Learning, </journal> <volume> Vol. 14, </volume> <pages> pp. 139-168, </pages> <publisher> Kluwer Academic, </publisher> <address> Boston, MA, </address> <year> 1994. </year>
Reference-contexts: This research concerns automatic determination of neural network topology, and is viewed as a constructive induction method. Constructive induction systems perform a double search, one for the most suitable representation space, and second for the most suitable concept description in this space <ref> (Wnek & Michalski, 1994) </ref>. They include mechanisms for generating new, more relevant descriptors, as well as modifying or removing less relevant ones from those initially provided. <p> For this reason, this approach is a hypothesis-driven constructive induction (HCI), as opposed to a knowledge-driven approach (KCI) or data-driven approach (DCI) <ref> (Wnek & Michalski, 1994) </ref>. In the context of neural network knowledge representation, the KCI approach can be identified, for example, with the KBANN system (Towell et al., 1991). <p> A pattern is meant to be a component of a generated concept description that is characteristic of a relatively large number of concept examples <ref> (Wnek & Michalski, 1994) </ref>. In the HCI approach, transformations of the representation space may involve both contraction and expansion operations. Contraction decreases the number of possible concepts that can be represented in the space, and expansion increases that number.
Reference: <author> Wnek, J. and Michalski, </author> <title> R.S., "Discovering Representation Space Transformations for Learning Concept Descriptions Combining DNF and M-of-N Rules," </title> <booktitle> Working Notes of the ML'94 Workshop on Constructive Induction and Change of Representation, </booktitle> <year> 1994. </year>
Reference-contexts: This research concerns automatic determination of neural network topology, and is viewed as a constructive induction method. Constructive induction systems perform a double search, one for the most suitable representation space, and second for the most suitable concept description in this space <ref> (Wnek & Michalski, 1994) </ref>. They include mechanisms for generating new, more relevant descriptors, as well as modifying or removing less relevant ones from those initially provided. <p> For this reason, this approach is a hypothesis-driven constructive induction (HCI), as opposed to a knowledge-driven approach (KCI) or data-driven approach (DCI) <ref> (Wnek & Michalski, 1994) </ref>. In the context of neural network knowledge representation, the KCI approach can be identified, for example, with the KBANN system (Towell et al., 1991). <p> A pattern is meant to be a component of a generated concept description that is characteristic of a relatively large number of concept examples <ref> (Wnek & Michalski, 1994) </ref>. In the HCI approach, transformations of the representation space may involve both contraction and expansion operations. Contraction decreases the number of possible concepts that can be represented in the space, and expansion increases that number.
References-found: 24

