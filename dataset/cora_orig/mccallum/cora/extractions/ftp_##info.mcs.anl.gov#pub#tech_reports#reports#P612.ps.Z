URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P612.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: THE INEXACT RATIONAL KRYLOV SEQUENCE METHOD  
Author: R. B. LEHOUCQ AND KARL MEERBERGEN 
Abstract: The rational Krylov sequence (RKS) method is a generalization of Arnoldi's method. It constructs an orthogonal reduction of a matrix pencil into an upper Hessenberg pencil. The RKS method is useful when the matrix pencil may be efficiently factored. However, it requires the solution of a linear system at every step. This article considers solving the resulting linear systems in an inexact manner by using an iterative method. We show that a Cayley transformation used within the RKS method is more efficient and robust than the usual shift-and-invert transformation. A relationship with the recently introduced Jacobi-Davidson method of Sleijpen and van der Vorst is also established. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. Du Croz, A. Greenbaum, S. Ham-marling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LAPACK users' guide. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1995. </year>
Reference-contexts: Without loss of generality, assume that B is invertible. Following standard convention, we refer to (A; B) as a matrix pencil. For us, n is considered large when it is prohibitive to compute all the eigenvalues as a dense algorithm in LAPACK <ref> [1] </ref> would attempt to do. A standard approach is to perform inverse iteration [18, page 405] with the matrix A B: The sequence of iterates v; (A B) 1 Bv; [(A B) 1 B] 2 v; : : :(1.2) is produced. <p> We remark that ~ L y m is the Moore-Penrose generalized inverse of ~ L m : The explicit formation of the inverse of ~ L H m ~ L m is not required. Instead, ~ L y m computed by least squares methods, for example with the LAPACK <ref> [1] </ref> software. The Ritz vector is y = V m+1 ~ L m z: 2.3. Stopping Criterion. <p> Let t 1 = <ref> [1] </ref> and (0) = 0. * For j = 1; 2; : : : ; m: 1. Compute residual r (j1) = AV j t j (j1) BV j t j : 2. If kr (j1) k &lt; tol then exit. 3. <p> ; 5) and B = I: The pencil (A; I) has eigenpairs (j; e j ), j = 1; : : : ; 5: The goal is to compute the eigenpair (1; e 1 ) with I-RKS using a fixed pole j = 0:7 and starting with v 1 = <ref> [ 1; : : : ; 1 ] </ref> T = p Cayley system (A j I)x j = r (j1) is solved as x j = M 1 r (j1) , where M 1 = 6 6 4 10 2 . . . . . . 10 2 (5 j ) <p> B. LEHOUCQ AND KARL MEERBERGEN * Given v 1 , (0) and ~ l 0 = <ref> [1] </ref>: * For j = 1; 2; : : : m: 1. Select a pole j : 2. Let the zero be j = (j1) and the continuation vector be t j = ~ l j1 =k ~ l j1 k 3. <p> The linear systems were solved by 20 iterations of Gauss-Seidel starting with a zero initial vector. Since this solver is stationary, the relative residual norm, o , is almost constant. The initial guess for the eigenvalue was (0) = 0: The initial vector for RKS was v 1 = <ref> [ 1; : : :; 1 ] </ref> T = p n: The poles j were set equal to 5 for all j: The residuals r (j) , f (j) and S j z (j) are shown in Table 3.1. All three sequences decrease when the Cayley transform is used. <p> The matrix arises from the same problem as in Example 3.1, but now n = 200. We ran Algorithm I-RKS from Figure 3 with fixed j = 5, starting with vector v 1 = <ref> [1; ; 1] </ref> T = p n: The linear systems were solved by GMRES preconditioned by ILU. <p> The linear systems were solved by GMRES preconditioned with ILUT (lfil=40,tol=1.e-3) [32] with o = 10 4 . The initial vector v 1 was computed from the system (B A)v 1 = Bv with v = <ref> [ 1; ; 1 ] </ref> T using the GMES-ILUT solver. The algorithm was stopped when kr (j) The numerical results are shown in Table 4.1 for inexact rational Krylov (I-RKS) INEXACT RATIONAL KRYLOV SEQUENCE METHOD 13 Table 4.1 Numerical results for the tilted plane problem from x 4.
Reference: [2] <author> W. E. </author> <title> Arnoldi. The principle of minimized iterations in the solution of the matrix eigenvalue problem. </title> <journal> Quart. Appl. Math., </journal> <volume> 9 </volume> <pages> 17-29, </pages> <year> 1951. </year>
Reference-contexts: This leads to a straightforward extension [20] of the ideas introduced by Ericsson and Ruhe [14] for the spectral (shift-and-invert) transformation Lanczos method. Starting with the vector v, Arnoldi's method <ref> [2] </ref> builds, step by step, an orthogonal basis for the Krylov subspace K m (T SI ; v) j Spanfv; T SI v; : : : ; (T SI ) m1 vg where T SI = (A B) 1 B: One improvement to the inverse iteration scheme given is to vary
Reference: [3] <author> R. Barrett, M. Berry, T. Chan, J. Demmel, J. Donato, J. Dongarra, V. Eijkhout, R. Pozo, C. Romine, and H. van der Vorst. </author> <title> Templates for the solution of linear systems: Building blocks for iterative methods. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1994. </year>
Reference-contexts: GMRES [33], BiCGSTAB (`) [35], and QMR [17] are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See <ref> [3] </ref> for templates for all these solvers. To summarize, then, what we mean by a large error is that o lies in the interval [ 10 8 ; 10 2 ]: 6 R. B.
Reference: [4] <author> A. M. Bruaset. </author> <title> A survey of preconditioned iterative methods. </title> <booktitle> Pitman Research Notes in Mathematics Series. </booktitle> <publisher> Longman Scientific & Technical, Harlow Essex, </publisher> <address> UK, </address> <year> 1995. </year>
Reference-contexts: In all our experiments, x 0 = 0 so that kbCxk = kb CM 1 bk o kbk with o = kI CM 1 k. Thus, we obtain, roughly speaking, the same relative residual norm for any b: In general, Krylov methods <ref> [4, 16] </ref> are much more powerful. GMRES [33], BiCGSTAB (`) [35], and QMR [17] are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See [3] for templates for all these solvers.
Reference: [5] <author> J. W. Daniel, W. B. Gragg, L. Kaufman, and G. W. Stewart. </author> <title> Reorthogonalization and stable algorithms for updating the Gram-Schmidt QR factorization. </title> <journal> Math. Comp., </journal> <volume> 30 </volume> <pages> 772-795, </pages> <year> 1976. </year>
Reference-contexts: Orthogonalization. The orthogonalization of Step 3 of Algorithm 2 uses an iterative classical Gram-Schmidt algorithm. This is the same approach used by Sorensen [37] based on the analysis <ref> [5] </ref> of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs.
Reference: [6] <author> T. A. Davis and I. S. Duff. </author> <title> An unsymmetric-pattern multifrontal method for sparse LU factorization. </title> <note> SIAM J. Matrix Anal. Applic., to appear. (also University of Florida technical report TR-94-038). </note>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [7] <author> Timothy A. Davis and Iain S. Duff. </author> <title> A combined unifrontal/multifrontal method for unsym-metric sparse matrices. </title> <type> Technical Report TR-95-020, </type> <institution> Computer and Information Science and Engineering Department, University of Florida, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [8] <author> G. De Samblanx, K. Meerbergen, and A. Bultheel. </author> <title> The implicit application of a rational filter in the rks method. </title> <type> Technical Report TW239, </type> <institution> Department of Computer Science, K.U.Leuven, Heverlee, Belgium, </institution> <year> 1996. </year>
Reference-contexts: It has been shown that the presence of this eigenvalue can disturb the calculation of a nonzero eigenvalue when the spectral transformation Lanczos method [13, 25], the shift-invert Arnoldi method [28, 22], or the rational Krylov method <ref> [8] </ref> are used. One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 [25]. <p> These techniques include use of complex poles and zeros for real A and B [30], harmonic Ritz pairs, deflation and purging [31], and the implicit application of a rational filter <ref> [8] </ref>. The practical advantage of the inexact rational Krylov method is that the cheaply computed matrices ~ L m and ~ K m are used to compute the Ritz pairs. The Jacobi-Davidson method requires the explicit formation of V H m AV m and V m BV m : A.
Reference: [9] <author> James W. Demmel, Stanley C. Eisenstat, John R. Gilbert, Xiaoye S. Li, and Joseph W. H. Liu. </author> <title> A supernodal approach to sparse partial pivoting. </title> <type> Technical Report CSD-95-883, </type> <institution> Department of Computer Science, University of California, Berkeley, California, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [10] <author> I. S. Duff. ME28: </author> <title> A sparse unsymmetric linear equation solver for complex equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7(4) </volume> <pages> 505-511, </pages> <month> December </month> <year> 1981. </year>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [11] <author> I. S. Duff and J. K. Reid. </author> <title> The design of MA48, a code for direct solution of sparse unsymmetric linear systems of equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(2) </volume> <pages> 187-226, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [12] <author> I. S. Duff and J. A. Scott. </author> <title> The design of a new frontal code for solving sparse unsymmetric systems. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(1) </volume> <pages> 30-45, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: All the methods considered require the solution of (A B)x = By for x: This is typically accomplished by factoring A B: For example, when A B is sparse, a direct method <ref> [6, 7, 9, 10, 12, 11] </ref> may be employed. If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK [19] is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1).
Reference: [13] <author> T. Ericsson. </author> <title> A generalised eigenvalue problem and the Lanczos algorithm. </title> <editor> In J. Cullum and R. A. Willoughby, editors, </editor> <booktitle> Large Scale Eigenvalue Problems, </booktitle> <pages> pages 95-119. </pages> <publisher> Elsevier Science Publishers BV, </publisher> <year> 1986. </year> <note> 16 R. B. LEHOUCQ AND KARL MEERBERGEN </note>
Reference-contexts: It has been shown that the presence of this eigenvalue can disturb the calculation of a nonzero eigenvalue when the spectral transformation Lanczos method <ref> [13, 25] </ref>, the shift-invert Arnoldi method [28, 22], or the rational Krylov method [8] are used. One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 [25].
Reference: [14] <author> T. Ericsson and A. Ruhe. </author> <title> The spectral transformation Lanczos method for the numerical solution of large sparse generalized symmetric eigenvalue problems. </title> <journal> Math. Comp., </journal> <volume> 35 </volume> <pages> 1251-1268, </pages> <year> 1980. </year>
Reference-contexts: Another approach is to extract the approximate eigenpair by using the information from the subspace defined by joining together m iterates of the sequence (1.2). This leads to a straightforward extension [20] of the ideas introduced by Ericsson and Ruhe <ref> [14] </ref> for the spectral (shift-and-invert) transformation Lanczos method.
Reference: [15] <author> D. R. Fokkema, G. L. G. Sleijpen, and H. A. van der Vorst. </author> <title> Jacobi-Davidson style QR and QZ algorithms for the partial reduction of matrix pencils. </title> <type> Technical Report Preprint 941, </type> <institution> Department of Mathematics, Utrecht University, </institution> <address> Utrecht, The Netherlands, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: Fittingly, the literature on approaches for finding a few solutions to the generalized eigenvalue problem (1.1), where only approximate solutions to the linear systems are available, is sparse. Szyld [38] considers the situation where the matrix pencil is symmetric positive definite. Algorithms based on Jacobi-Davidson methods are discussed in <ref> [15, 34] </ref>. The recent report by Meerbergen and Roose [21] provided motivation for the current article. Our article is organized as follows. We introduce the RKS method in x2. The inexact RKS method is introduced in x3 along with a connection with inverse iteration and some examples illustrating our ideas. <p> A Connection with the Jacobi-Davidson Method. Based upon the recent work of Sleijpen and van der Vorst [36] which investigated a new method for standard eigenvalue problems (B = I) that only uses approximate linear systems solutions. The two manuscripts <ref> [15, 34] </ref> extend the method for generalized eigenvalue problems. We now proceed to show a connection between RKS and Jacobi-Davidson when the linear systems are solved exactly.
Reference: [16] <author> R. W. Freund, G. H. Golub, and N. M. Nachtigal. </author> <title> Iterative solution of linear systems. </title> <editor> In A. Iserles, editor, </editor> <booktitle> Acta Numerica 1992, </booktitle> <pages> pages 57-100. </pages> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference-contexts: In all our experiments, x 0 = 0 so that kbCxk = kb CM 1 bk o kbk with o = kI CM 1 k. Thus, we obtain, roughly speaking, the same relative residual norm for any b: In general, Krylov methods <ref> [4, 16] </ref> are much more powerful. GMRES [33], BiCGSTAB (`) [35], and QMR [17] are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See [3] for templates for all these solvers.
Reference: [17] <author> Roland W. Freund and Noel M. Nachtigal. QMRPACK: </author> <title> A package of QMR algorithms. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 22(1) </volume> <pages> 46-77, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Thus, we obtain, roughly speaking, the same relative residual norm for any b: In general, Krylov methods [4, 16] are much more powerful. GMRES [33], BiCGSTAB (`) [35], and QMR <ref> [17] </ref> are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See [3] for templates for all these solvers.
Reference: [18] <author> G. Golub and C. Van Loan. </author> <title> Matrix computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference-contexts: Following standard convention, we refer to (A; B) as a matrix pencil. For us, n is considered large when it is prohibitive to compute all the eigenvalues as a dense algorithm in LAPACK [1] would attempt to do. A standard approach is to perform inverse iteration <ref> [18, page 405] </ref> with the matrix A B: The sequence of iterates v; (A B) 1 Bv; [(A B) 1 B] 2 v; : : :(1.2) is produced. Under some mild assumptions, the sequence converges toward the desired eigenvector, and a Rayleigh quotient calculation gives an estimate of the eigenvalue.
Reference: [19] <author> R. B. Lehoucq, D. C. Sorensen, P. Vu, and C. Yang. ARPACK: </author> <title> An implementation of the implicitly re-started Arnoldi iteration that computes some of the eigenvalues and eigenvectors of a large sparse matrix. </title> <note> Available from netlib@ornl.gov under the directory scalapack, </note> <year> 1995. </year>
Reference-contexts: If the shifts j are not varied, then use of one of these direct methods in conjunction with ARPACK <ref> [19] </ref> is a powerful combination for computing a few solutions of the generalized eigenvalue problem (1.1). However, for large eigenvalue problems (n &gt; 10; 000), direct methods using the RKS method may not provide an efficient solution because of the potentially prohibitive storage requirements.
Reference: [20] <author> K. Meerbergen and D. Roose. </author> <title> Matrix transformations for computing rightmost eigenvalues of real nonsymmetric matrices. </title> <journal> IMA J. Numer. Anal., </journal> <volume> 16 </volume> <pages> 297-346, </pages> <year> 1996. </year>
Reference-contexts: Another approach is to extract the approximate eigenpair by using the information from the subspace defined by joining together m iterates of the sequence (1.2). This leads to a straightforward extension <ref> [20] </ref> of the ideas introduced by Ericsson and Ruhe [14] for the spectral (shift-and-invert) transformation Lanczos method.
Reference: [21] <author> K. Meerbergen and D. Roose. </author> <title> The restarted Arnoldi method applied to iterative linear system solvers for computation of rightmost eigenvalues. </title> <note> SIAM J. Matrix Anal. Applic., 1996. Accepted for publication. </note>
Reference-contexts: Szyld [38] considers the situation where the matrix pencil is symmetric positive definite. Algorithms based on Jacobi-Davidson methods are discussed in [15, 34]. The recent report by Meerbergen and Roose <ref> [21] </ref> provided motivation for the current article. Our article is organized as follows. We introduce the RKS method in x2. The inexact RKS method is introduced in x3 along with a connection with inverse iteration and some examples illustrating our ideas. <p> We present numerical evidence that demonstrates that this situation occurs when using an inexact Cayley transformation, whereas it does not when an inexact shift-and-invert transformation is used. The choice of the zero of the Cayley transformation is crucial to its success, as was pointed out in <ref> [21] </ref>. Suppose that ( (j1) ; y (j1) ) is an (inexact) Ritz pair computed in the (j 1)st iteration. Then, t j and j are chosen such that j = (j1) and V j t j = y (j1) .
Reference: [22] <author> K. Meerbergen and A. Spence. </author> <title> Implicitly restarted Arnoldi and purification for the shift-invert transformation. </title> <journal> Math. Comp., </journal> <note> 1997. To appear. </note>
Reference-contexts: It has been shown that the presence of this eigenvalue can disturb the calculation of a nonzero eigenvalue when the spectral transformation Lanczos method [13, 25], the shift-invert Arnoldi method <ref> [28, 22] </ref>, or the rational Krylov method [8] are used. One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 [25].
Reference: [23] <author> R. B. Morgan. </author> <title> Computing interior eigenvalues of large matrices. Linear Alg. </title> <journal> Appl., </journal> <volume> 154-156:289-309, </volume> <year> 1991. </year>
Reference-contexts: This is the same approach used by Sorensen [37] based on the analysis [5] of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs. Harmonic Ritz pairs <ref> [23, 27, 24, 36] </ref> may also be computed, as was shown by Ruhe [31], but these are not considered here. The main purpose of this article is to study the use of iterative linear system solvers in RKS and not the various ways to extract 4 R. B.
Reference: [24] <author> R. B. Morgan and Min Zeng. </author> <title> Estimates for interior eigenvalues of large nonsymmetric matrices. 1996. </title> <type> Preprint. </type>
Reference-contexts: This is the same approach used by Sorensen [37] based on the analysis [5] of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs. Harmonic Ritz pairs <ref> [23, 27, 24, 36] </ref> may also be computed, as was shown by Ruhe [31], but these are not considered here. The main purpose of this article is to study the use of iterative linear system solvers in RKS and not the various ways to extract 4 R. B.
Reference: [25] <author> B. Nour-Omid, B. N. Parlett, T. Ericsson, and P. S. Jensen. </author> <title> How to implement the spectral transformation. </title> <journal> Math. Comp., </journal> <volume> 48 </volume> <pages> 663-673, </pages> <year> 1987. </year>
Reference-contexts: It has been shown that the presence of this eigenvalue can disturb the calculation of a nonzero eigenvalue when the spectral transformation Lanczos method <ref> [13, 25] </ref>, the shift-invert Arnoldi method [28, 22], or the rational Krylov method [8] are used. One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 [25]. <p> One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 <ref> [25] </ref>. This can be achieved by selecting v 1 = (B A) 1 Bv with v arbitrary. The eigenvalue fl nearest 0:1 was computed by use of I-RKS (Fig. 3) with fixed pole j = 0:1.
Reference: [26] <author> W. E. Olmstead, W. E. Davis, S. H. Rosenblat, and W. L. Kath. </author> <title> Bifurcation with memory. </title> <journal> SIAM J. Appl. Math., </journal> <volume> 40 </volume> <pages> 171-188, </pages> <year> 1986. </year>
Reference-contexts: The entries (0) and v 1 determine the initial guesses for the eigenpair. We now compare inexact inverse iteration computed via the RKS method using the shift-and-invert and Cayley transformations with an example. Example 3.1. The Olmstead model <ref> [26] </ref> represents the flow of a layer of vis-coelastic fluid heated from below. The equations are 8 &gt; : @t @ 2 v @ 2 u B @t with boundary conditions u (0) = u (1) = 0 and v (0) = v (1) = 0.
Reference: [27] <author> C. Paige, B. N. Parlett, and H. A. van der Vorst. </author> <title> Approximate solutions and eigenvalue bounds from Krylov subspaces. </title> <journal> Num. Lin. Alg. Appl., </journal> <volume> 2 </volume> <pages> 115-133, </pages> <year> 1995. </year>
Reference-contexts: This is the same approach used by Sorensen [37] based on the analysis [5] of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs. Harmonic Ritz pairs <ref> [23, 27, 24, 36] </ref> may also be computed, as was shown by Ruhe [31], but these are not considered here. The main purpose of this article is to study the use of iterative linear system solvers in RKS and not the various ways to extract 4 R. B.
Reference: [28] <author> B. Philippe and M. Sadkane. </author> <title> Improving the spectral transformation block Arnoldi method. </title> <editor> In P. S. Vassilevski and S. D. Margenov, editors, </editor> <booktitle> Second IMACS Symposium on Iterative Methods in Linear Algebra, volume 3 of IMACS Series in Computational and Applied Mathematics, </booktitle> <pages> pages 57-63. </pages> <booktitle> IMACS Symposium on Iterative Methods in Linear Algebra, </booktitle> <year> 1996. </year>
Reference-contexts: It has been shown that the presence of this eigenvalue can disturb the calculation of a nonzero eigenvalue when the spectral transformation Lanczos method [13, 25], the shift-invert Arnoldi method <ref> [28, 22] </ref>, or the rational Krylov method [8] are used. One way to reduce the impact of fl = 0 is to start the I-RKS method with an initial vector v 1 that is poor in the eigenspace corresponding to fl = 0 [25].
Reference: [29] <author> A. Ruhe. </author> <title> Rational Krylov sequence methods for eigenvalue computation. Linear Alg. </title> <journal> Appl., </journal> <volume> 58 </volume> <pages> 391-405, </pages> <year> 1984. </year>
Reference-contexts: For example, set j equal to the Rayleigh quotient z H Az=z H Bz, where z is an unit vector in the direction of (T SI ) j v: Ruhe <ref> [29, 31] </ref> elegantly shows how to build an orthogonal basis for the rational Krylov subspace Spanfv; T SI 1 v; ; (T SI 1 )vg; where T SI j = (A j B) 1 B: fl The work of R. B.
Reference: [30] <author> A. Ruhe. </author> <title> The Rational Krylov algorithm for nonsymmetric eigenvalue problems, III: Complex shifts for real matrices. </title> <journal> BIT, </journal> <volume> 34 </volume> <pages> 165-176, </pages> <year> 1994. </year>
Reference-contexts: The fact that I-RKS solves a perturbed problem with small perturbations in the desired eigendirections motivates the application of the RKS techniques developed for the process using exact linear solves. These techniques include use of complex poles and zeros for real A and B <ref> [30] </ref>, harmonic Ritz pairs, deflation and purging [31], and the implicit application of a rational filter [8]. The practical advantage of the inexact rational Krylov method is that the cheaply computed matrices ~ L m and ~ K m are used to compute the Ritz pairs.
Reference: [31] <author> A. Ruhe. </author> <title> Rational Krylov, a practical algorithm for large sparse nonsymmetric matrix pencils. </title> <type> Technical Report UCB/CSD-95-871, </type> <institution> Computer Science Division, University of California, Berkeley, </institution> <address> CA, </address> <year> 1995. </year>
Reference-contexts: For example, set j equal to the Rayleigh quotient z H Az=z H Bz, where z is an unit vector in the direction of (T SI ) j v: Ruhe <ref> [29, 31] </ref> elegantly shows how to build an orthogonal basis for the rational Krylov subspace Spanfv; T SI 1 v; ; (T SI 1 )vg; where T SI j = (A j B) 1 B: fl The work of R. B. <p> The range of the matrix V is denoted by R (V ): The Hermitian transpose of the vector x is denoted by x H : 2. The Rational Krylov Sequence Method. The method is outlined by the algorithm listed in Figure 2. For the practical RKS algorithm given in <ref> [31] </ref>, Ruhe considers the shift-and-invert transformation T SI j = (A j B) 1 B rather than T C j = (A j B) 1 (A j B): In exact arithmetic, both transformations lead to the same rational Krylov space, since T C j :(2.1) However, in finite-precision arithmetic and/or in <p> Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs. Harmonic Ritz pairs [23, 27, 24, 36] may also be computed, as was shown by Ruhe <ref> [31] </ref>, but these are not considered here. The main purpose of this article is to study the use of iterative linear system solvers in RKS and not the various ways to extract 4 R. B. LEHOUCQ AND KARL MEERBERGEN eigenvalues. <p> These techniques include use of complex poles and zeros for real A and B [30], harmonic Ritz pairs, deflation and purging <ref> [31] </ref>, and the implicit application of a rational filter [8]. The practical advantage of the inexact rational Krylov method is that the cheaply computed matrices ~ L m and ~ K m are used to compute the Ritz pairs.
Reference: [32] <author> Y. Saad. </author> <title> SPARSKIT : A basic tool kit for sparse matrix computations. </title> <type> Technical Report 90-20, </type> <institution> Research Institute for Advanced Computer Science, NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <year> 1990. </year>
Reference-contexts: This can be achieved by selecting v 1 = (B A) 1 Bv with v arbitrary. The eigenvalue fl nearest 0:1 was computed by use of I-RKS (Fig. 3) with fixed pole j = 0:1. The linear systems were solved by GMRES preconditioned with ILUT (lfil=40,tol=1.e-3) <ref> [32] </ref> with o = 10 4 . The initial vector v 1 was computed from the system (B A)v 1 = Bv with v = [ 1; ; 1 ] T using the GMES-ILUT solver.
Reference: [33] <author> Y. Saad and M. H. Schultz. </author> <title> GMRES: A generalized minimal residual algorithm for solving nonsymmetric linear systems. </title> <journal> SIAM J. Sci. Statist. Comput., </journal> <volume> 7 </volume> <pages> 856-869, </pages> <year> 1986. </year>
Reference-contexts: In all our experiments, x 0 = 0 so that kbCxk = kb CM 1 bk o kbk with o = kI CM 1 k. Thus, we obtain, roughly speaking, the same relative residual norm for any b: In general, Krylov methods [4, 16] are much more powerful. GMRES <ref> [33] </ref>, BiCGSTAB (`) [35], and QMR [17] are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See [3] for templates for all these solvers.
Reference: [34] <author> G. L. G. Sleijpen, G. L. Booten, D. R. Fokkema, and H. A. van der Vorst. </author> <title> Jacobi Davidson type methods for generalized eigenproblems and polynomial eigenproblems: Part I. </title> <type> Preprint nr. 923, </type> <institution> Department of Mathematics, Universiteit Utrecht, </institution> <address> Utrecht, The Netherlands, </address> <year> 1995. </year>
Reference-contexts: Fittingly, the literature on approaches for finding a few solutions to the generalized eigenvalue problem (1.1), where only approximate solutions to the linear systems are available, is sparse. Szyld [38] considers the situation where the matrix pencil is symmetric positive definite. Algorithms based on Jacobi-Davidson methods are discussed in <ref> [15, 34] </ref>. The recent report by Meerbergen and Roose [21] provided motivation for the current article. Our article is organized as follows. We introduce the RKS method in x2. The inexact RKS method is introduced in x3 along with a connection with inverse iteration and some examples illustrating our ideas. <p> A Connection with the Jacobi-Davidson Method. Based upon the recent work of Sleijpen and van der Vorst [36] which investigated a new method for standard eigenvalue problems (B = I) that only uses approximate linear systems solutions. The two manuscripts <ref> [15, 34] </ref> extend the method for generalized eigenvalue problems. We now proceed to show a connection between RKS and Jacobi-Davidson when the linear systems are solved exactly. <p> This is not the case for the RKS method with Cayley transformation defined by Equation (5.3). The solution of the linear system (5.2) leads to quadratic convergence when j = ~ j . Theorem 3.2 in <ref> [34] </ref> establishes this result under some mild conditions while Appendix A in [34] demonstrates a connection with Newton's method. 6. Conclusions. This paper demonstrated the use of iterative linear system solvers in Ruhe's rational Krylov sequence method. <p> This is not the case for the RKS method with Cayley transformation defined by Equation (5.3). The solution of the linear system (5.2) leads to quadratic convergence when j = ~ j . Theorem 3.2 in <ref> [34] </ref> establishes this result under some mild conditions while Appendix A in [34] demonstrates a connection with Newton's method. 6. Conclusions. This paper demonstrated the use of iterative linear system solvers in Ruhe's rational Krylov sequence method. The analysis of the convergence of inexact inverse iteration showed the importance of the use of the Cayley transformation instead of the usual shift-and-invert transformation.
Reference: [35] <author> G. L. G. Sleijpen, D. R. Fokkema, and H. A van der Vorst. </author> <title> BiCGstab(`) and other hybrid Bi-CG methods. </title> <journal> Numer. Algor., </journal> <volume> 7 </volume> <pages> 75-109, </pages> <year> 1994. </year>
Reference-contexts: Thus, we obtain, roughly speaking, the same relative residual norm for any b: In general, Krylov methods [4, 16] are much more powerful. GMRES [33], BiCGSTAB (`) <ref> [35] </ref>, and QMR [17] are among those most widely used. The performance of these solvers often substantially improves when a suitable preconditioner is employed. See [3] for templates for all these solvers.
Reference: [36] <author> G. L. G. Sleijpen and H. A. van der Vorst. </author> <title> A Jacobi-Davidson iteration method for linear eigenvalue problems. </title> <journal> SIAM J. Matrix Anal. Applic., </journal> <volume> 17 </volume> <pages> 401-425, </pages> <year> 1996. </year>
Reference-contexts: This is the same approach used by Sorensen [37] based on the analysis [5] of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs. Harmonic Ritz pairs <ref> [23, 27, 24, 36] </ref> may also be computed, as was shown by Ruhe [31], but these are not considered here. The main purpose of this article is to study the use of iterative linear system solvers in RKS and not the various ways to extract 4 R. B. <p> Both methods converge to = fl 1 9:486. Finally, note that I-RKS is faster than inverse iteration. 5. A Connection with the Jacobi-Davidson Method. Based upon the recent work of Sleijpen and van der Vorst <ref> [36] </ref> which investigated a new method for standard eigenvalue problems (B = I) that only uses approximate linear systems solutions. The two manuscripts [15, 34] extend the method for generalized eigenvalue problems. We now proceed to show a connection between RKS and Jacobi-Davidson when the linear systems are solved exactly.
Reference: [37] <author> D. C. Sorensen. </author> <title> Implicit application of polynomial filters in a k-step Arnoldi method. </title> <journal> SIAM J. Matrix Analysis and Applications, </journal> <volume> 13(1) </volume> <pages> 357-385, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Orthogonalization. The orthogonalization of Step 3 of Algorithm 2 uses an iterative classical Gram-Schmidt algorithm. This is the same approach used by Sorensen <ref> [37] </ref> based on the analysis [5] of reorthogonalization in the Gram-Schmidt algorithm. 2.2. Computing Eigenvalue Estimates. We now consider the calculation of approximate eigenpairs for the RKS method. We discuss how to compute Ritz pairs.
Reference: [38] <author> Daniel B. Szyld. </author> <title> Criteria for combining inverse and Rayleigh quotient iteration. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 25 </volume> <pages> 1369-1375, </pages> <year> 1988. </year>
Reference-contexts: Fittingly, the literature on approaches for finding a few solutions to the generalized eigenvalue problem (1.1), where only approximate solutions to the linear systems are available, is sparse. Szyld <ref> [38] </ref> considers the situation where the matrix pencil is symmetric positive definite. Algorithms based on Jacobi-Davidson methods are discussed in [15, 34]. The recent report by Meerbergen and Roose [21] provided motivation for the current article. Our article is organized as follows. We introduce the RKS method in x2.
References-found: 38

