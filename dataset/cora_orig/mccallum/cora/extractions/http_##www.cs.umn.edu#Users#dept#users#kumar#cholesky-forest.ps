URL: http://www.cs.umn.edu/Users/dept/users/kumar/cholesky-forest.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Title: A High Performance Sparse Cholesky Factorization Algorithm For Scalable Parallel Computers  
Author: George Karypis and Vipin Kumar 
Date: 94-41  
Address: Minneapolis, MN 55455  
Affiliation: Department of Computer Science University of Minnesota  
Pubnum: Technical Report  
Abstract: This paper presents a new parallel algorithm for sparse matrix factorization. This algorithm uses subforest-to-subcube mapping instead of the subtree-to-subcube mapping of another recently introduced scheme by Gupta and Kumar [13]. Asymptotically, both formulations are equally scalable on a wide range of architectures and a wide variety of problems. But the subtree-to-subcube mapping of the earlier formulation causes significant load imbalance among processors, limiting overall efficiency and speedup. The new mapping largely eliminates the load imbalance among processors. Furthermore, the algorithm has a number of enhancements to improve the overall performance substantially. This new algorithm achieves up to 6GFlops on a 256-processor Cray T3D for moderately large problems. To our knowledge, this is the highest performance ever obtained on an MPP for sparse Cholesky factorization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Cleve Ashcraft, S. C. Eisenstat, J. W.-H. Liu, and A. H. Sherman. </author> <title> A comparison of three column based distributed sparse factorization schemes. </title> <type> Technical Report YALEU/DCS/RR-810, </type> <institution> Yale University, </institution> <address> New Haven, CT, </address> <year> 1990. </year> <booktitle> Also appears in Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference: [2] <author> I. S. Duff and J. K. Reid. </author> <title> The multifrontal solution of indefinite sparse symmetric linear equations. </title> <journal> ACM Transactions on Mathematical Software, </journal> (9):302-325, 1983. 
Reference-contexts: This reduces the communication overhead and improves the isoefficiency to O (p 1:5 log 3 p). Gupta and Kumar [13] recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method <ref> [2, 23] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices.
Reference: [3] <author> Kalluri Eswar, Ponnuswamy Sadayappan, and V. Visvanathan. </author> <title> Supernodal Sparse Cholesky factorization on distributed-memory multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages 18-22 (vol. 3), </pages> <year> 1993. </year>
Reference: [4] <author> K. A. Gallivan, R. J. Plemmons, and A. H. Sameh. </author> <title> Parallel algorithms for dense linear algebra computations. </title> <journal> SIAM Review, </journal> <volume> 32(1) </volume> <pages> 54-135, </pages> <month> March </month> <year> 1990. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 27, 7, 22] </ref>.
Reference: [5] <author> D. M. Gay. </author> <title> Electronic Mail Distribution of Linear Programming Test Problems. </title> <journal> &lt;mathematical Programming Society COAL Newsletter, </journal> <month> December </month> <year> 1985. </year>
Reference-contexts: For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB <ref> [5] </ref>, even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% [13, 20, 19]. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that.
Reference: [6] <author> G. A. Geist and E. G.-Y. Ng. </author> <title> Task scheduling for parallel sparse Cholesky factorization. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 18(4) </volume> <pages> 291-314, </pages> <year> 1989. </year>
Reference: [7] <author> G. A. Geist and C. H. Romine. </author> <title> LU factorization algorithms on distributed-memory multiprocessor architectures. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 639-649, </pages> <year> 1988. </year> <note> Also available as Technical Report ORNL/TM-10383, </note> <institution> Oak Ridge National Laboratory, Oak Ridge, TN, </institution> <year> 1987. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 27, 7, 22] </ref>.
Reference: [8] <author> A. George, M. T. Heath, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Sparse Cholesky Factorization on a local memory multiprocessor. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 </volume> <pages> 327-340, </pages> <year> 1988. </year>
Reference-contexts: In [38], Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly (p 256) and massively (p 4096) parallel computers. A parallel formulation for sparse matrix factorization can be easily obtained by simply distributing rows to different processors <ref> [8] </ref>. Due to the sparsity of the matrix, communication overhead is a large fraction of the computation for this method, resulting in poor scalability.
Reference: [9] <author> A. George and J. W.-H. Liu. </author> <title> Computer Solution of Large Sparse Positive Definite Systems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year>
Reference-contexts: The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [41], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [9, 14, 10] </ref> is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix.
Reference: [10] <author> A. George and J. W.-H. Liu. </author> <title> The evolution of the minimum degree ordering algorithm. </title> <journal> SIAM Review, </journal> <volume> 31(1) </volume> <pages> 1-19, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [41], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [9, 14, 10] </ref> is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix.
Reference: [11] <author> A. George, J. W.-H. Liu, and E. G.-Y. Ng. </author> <title> Communication Results for Parallel Sparse Cholesky Factorization on a Hypercube. </title> <journal> Parallel Computing, </journal> <volume> 10(3) </volume> <pages> 287-298, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In a smarter parallel formulation <ref> [11] </ref>, the rows of the matrix are allocated to processors using the subtree-to-subcube mapping. This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O (p 3 ). Rothberg and Gupta [36, 35] used a different method to reduce the communication overhead. <p> Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in [19]. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy <ref> [11, 14] </ref> illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. The two subtrees of the root node are assigned to subcubes of p=2 processors each.
Reference: [12] <author> John R. Gilbert and Robert Schreiber. </author> <title> Highly Parallel Sparse Cholesky Factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13 </volume> <pages> 1151-1172, </pages> <year> 1992. </year> <month> 14 </month>
Reference: [13] <author> Anshul Gupta and Vipin Kumar. </author> <title> A scalable parallel algorithm for sparse matrix factorization. </title> <type> Technical Re--port 94-19, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year> <note> A shorter version appears in Supercomputing '94. TR available in users/kumar/sparse-cholesky.ps at anonymous FTP site ftp.cs.umn.edu. </note>
Reference-contexts: In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O (p 1:5 log 3 p). Gupta and Kumar <ref> [13] </ref> recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method [2, 23] is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices. <p> For larger problems, even higher performance can be achieved). To our knowledge, this is the highest performance ever obtained on an MPP for sparse Cholesky factorization. Our new scheme, like the scheme of Gupta and Kumar <ref> [13] </ref>, has an asymptotic isoefficiency of O (p 1:5 ) for matrices arising out of two- and three-dimensional finite element problems on a wide variety of architectures such as hypercube, mesh, fat tree, and three-dimensional torus. The rest of the paper is organized as follows. <p> The rest of the paper is organized as follows. Section 2 presents a general overview of the Cholesky factorization process and multifrontal methods. Section 3 provides a brief description of the algorithm in <ref> [13] </ref>. Section 4 describes our new algorithm. Section 5 describes some further enhancements of the algorithm that significantly improve the performance. Section 6 provides the experimental evaluation of our new algorithms on a Cray T3D. Section 7 contains concluding remarks. <p> For a more detailed description the reader should refer to <ref> [13] </ref>. Consider a p-processors hypercube-connected computer. Let A be the n fi n matrix to be factored, and let T be its supernodal elimination tree. The algorithm requires the elimination tree to be binary for the first log p levels. <p> Each processor participates in log p distributed extend-add operations, in which the update matrices from the factorization at level l are redistributed to perform the extend-add operation at level l 1 prior to factoring the frontal matrix. In the algorithm proposed in <ref> [13] </ref>, each processor exchanges data with only one other processor during each one of these log p distributed extend-adds. The above is achieved by a careful embedding of the processor grids on the hypercube, and by carefully mapping rows and columns of each frontal matrix onto this grid. <p> This mapping is described in [21], and is also given in Appendix B. 4 The New Algorithm As mentioned in the introduction, the subtree-to-subcube mapping scheme used in <ref> [13] </ref> does not distribute the work equally among the processors. This load imbalance puts an upper bound on the achievable efficiency. For example, consider the supernodal elimination tree shown in Figure 2. This elimination tree is partitioned among 8 processors using the subtree-to-subcube allocation scheme. <p> For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB [5], even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% <ref> [13, 20, 19] </ref>. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that. <p> Furthermore, as shown in Appendix D, the amount of data being transmitted in each parallel extend-add step is no more than it is in the earlier algorithm <ref> [13] </ref>. The reason is that even though more update matrices are being transmitted, these update matrices correspond to nodes that are deeper in the elimination tree and the size of these matrices is much smaller.
Reference: [14] <author> M. T. Heath, E. Ng, and B. W. Payton. </author> <title> Parallel Algorithms for Sparse Linear Systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <year> 1991. </year>
Reference-contexts: The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete [41], therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering <ref> [9, 14, 10] </ref> is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix. <p> Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in [19]. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy <ref> [11, 14] </ref> illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. The two subtrees of the root node are assigned to subcubes of p=2 processors each.
Reference: [15] <author> M. T. Heath, E. G.-Y. Ng, and Barry W. Peyton. </author> <title> Parallel Algorithms for Sparse Linear Systems. </title> <journal> SIAM Review, </journal> <volume> 33 </volume> <pages> 420-460, </pages> <year> 1991. </year> <note> Also appears in K. </note> <author> A. Gallivan et al. </author> <title> Parallel Algorithms for Matrix Computations. </title> <publisher> SIAM, </publisher> <address> Philadelphia, PA, </address> <year> 1990. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [4, 27, 7, 22]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [15, 38] </ref>, and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers.
Reference: [16] <author> M. T. Heath and P. Raghavan. </author> <title> A Cartesian nested dissection algorithm. </title> <type> Technical Report UIUCDCS-R-92-1772, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> October </month> <year> 1992. </year> <note> to appear in SIMAX. </note>
Reference-contexts: Another step that is quite time consuming, and has not been parallelized effectively is that of ordering. In our current research we are investigating ordering algorithms that can be implemented fast on parallel computers <ref> [16, 34] </ref>. Acknowledgment We would like to thank the Minnesota Supercomputing Center for providing access to a 64-processor Cray T3D, and Cray Research Inc. for providing access to a 256-processor Cray T3D. Finally we wish to thank Dr. Alex Pothen for his guidance with spectral nested dissection ordering.
Reference: [17] <author> M. T. Heath and P. Raghavan. </author> <title> Distributed solution of sparse linear systems. </title> <type> Technical Report 93-1793, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [18] <author> Laurie Hulbert and Earl Zmijewski. </author> <title> Limiting communication in parallel sparse Cholesky factorization. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 12(5) </volume> <pages> 1184-1197, </pages> <month> September </month> <year> 1991. </year>
Reference: [19] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> Ordering and load balancing for parallel factorization of sparse matrices. </title> <note> Technical Report (in preparation), </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: Elimination of rows in different subtrees can proceed concurrently. For a given matrix, elimination trees of smaller height usually have greater concurrency than trees of larger height. A desirable ordering for parallel computers must increase the amount of concurrency without increasing fill-in substantially. Spectral nested dissection <ref> [29, 30, 19] </ref> has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings to the performance of our algorithm refer to [21]. <p> The algorithm requires the elimination tree to be binary for the first log p levels. Any elimination tree of arbitrary shape can be converted to a binary tree using a simple tree restructuring algorithm described in <ref> [19] </ref>. In this scheme, portions of the elimination tree are assigned to processors using the standard subtree-to-subcube assignment strategy [11, 14] illustrated in Figure 1. With subtree-to-subcube assignment, all p processors in the system cooperate to factor the frontal matrix associated with the root node of the elimination tree. <p> Table 1 shows the load imbalance at the top level of the elimination tree for some matrices from the Boeing-Harwell matrix set. These matrices were ordered using the spectral nested dissection <ref> [29, 30, 19] </ref>. Note that for all matrices the load imbalance in terms of operation count is substantially higher than the relative difference in the number of nodes in the left and right subtrees. <p> For elimination trees of general sparse matrices, the load imbalance can be usually decreased by performing some simple elimination tree reorderings described in <ref> [19] </ref>. However, these techniques have two serious limitations. First, they increase the fill-in as they try to balance the elimination tree by adding extra dependencies. Thus, the total time required to perform the factorization increases. <p> For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB [5], even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% <ref> [13, 20, 19] </ref>. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that.
Reference: [20] <author> George Karypis, Anshul Gupta, and Vipin Kumar. </author> <title> A Parallel Formulation of Interior Point Algorithms. </title> <booktitle> In Supercomputing 94, </booktitle> <year> 1994. </year> <note> Also available as Technical Report TR 94-20, </note> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN. </institution>
Reference-contexts: For example, for a wide variety of problems from the Boeing-Harwell matrix set and linear programming (LP) matrices from NETLIB [5], even after applying the tree balancing heuristics, the efficiency bound due to load imbalance is still around 80% to 60% <ref> [13, 20, 19] </ref>. If the increased fill-in is taken into account, then the maximum achievable efficiency is even lower than that.
Reference: [21] <author> George Karypis and Vipin Kumar. </author> <title> A High Performance Sparse Cholesky Factorization Algorithm For Scala-bale Parallel Computers. </title> <type> Technical Report 94-41, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, MN, </institution> <year> 1994. </year>
Reference-contexts: For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings to the performance of our algorithm refer to <ref> [21] </ref>. In the multifrontal method for Cholesky factorization, a frontal matrix F k and an update matrix U k is associated with each node k of the elimination tree. The rows and columns of F k corresponds to t + 1 indices of L in increasing order. <p> The above is achieved by a careful embedding of the processor grids on the hypercube, and by carefully mapping rows and columns of each frontal matrix onto this grid. This mapping is described in <ref> [21] </ref>, and is also given in Appendix B. 4 The New Algorithm As mentioned in the introduction, the subtree-to-subcube mapping scheme used in [13] does not distribute the work equally among the processors. This load imbalance puts an upper bound on the achievable efficiency. <p> In the following sections we briefly describe these modifications. For a more detailed description of these enhancements the reader should refer to <ref> [21] </ref>. 5.1 Block Cyclic Mapping As discussed in Appendix D, for the factorization of a supernode, we use the pipelined variant of the grid-based dense Cholesky algorithm [22]. <p> A number of other design issues involved in using block cyclic mapping and ways to further improve the performance are described in <ref> [21] </ref>. 5.2 Pipelined Cholesky Factorization In the parallel portion of our multifrontal algorithm, each frontal matrix is factored using a grid based pipelined Cholesky factorization algorithm. This pipelined algorithms works as follows [22]. <p> Furthermore, because these transposed messages are initiated at different times cause little or no contention. As a result, each diagonal processor now will have to sit idle for a very small amount of time <ref> [21] </ref>. 6 Experimental Results We implemented our new parallel sparse multifrontal algorithm on a 256-processors Cray T3D parallel computer. Each processor on the T3D is a 150Mhz Dec Alpha chip, with peak performance of 150MFlops for 64-bit operations (double precision).
Reference: [22] <author> Vipin Kumar, Ananth Grama, Anshul Gupta, and George Karypis. </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms. </title> <publisher> Benjamin/Cummings Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1994. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 27, 7, 22] </ref>. <p> Note that the isoefficiency of the best known parallel formulation of dense matrix factorization is also O (p 1:5 ) <ref> [22] </ref>. On a variety of problems, Gupta and Kumar report speedup of up to 364 on a 1024-processor nCUBE 2, which is a major improvement over the previously existing algorithms. <p> If each of the matrices is factored by all the processors, then the total communication time for factoring the two matrices is n 2 = p p <ref> [22] </ref>. If A and B are factored concurrently by p=2 processors each, then the communication time is n 2 =(2 p p=2) which is smaller. <p> In the following sections we briefly describe these modifications. For a more detailed description of these enhancements the reader should refer to [21]. 5.1 Block Cyclic Mapping As discussed in Appendix D, for the factorization of a supernode, we use the pipelined variant of the grid-based dense Cholesky algorithm <ref> [22] </ref>. In this algorithm, successive rows of the frontal matrix are factored one after the other, and the communication and computation proceeds in a pipelined fashion. Even though this scheme is simple, it has two major limitations. <p> Hence, level three BLAS operations can better exploit the multiple functional units, and deep pipelines available in these processors. However, by distributing the frontal matrices using a block cyclic mapping <ref> [22] </ref>, we are able to eliminate both of the above limitations and greatly improve the performance of our algorithm. <p> This pipelined algorithms works as follows <ref> [22] </ref>. Assume that the processor grid stores the upper triangular part of the frontal matrix, and that the processor grid is square. <p> When a similar algorithm is used for Gaussian elimination, the problem doesn't arise because data start from a column and a row of processors, and messages from these rows and columns arrive at each processor at roughly the same time <ref> [22] </ref>. On machines with very high bandwidth, the overhead involved in managing buffers significantly reduces the percentage of the obtainable bandwidth. This effect is even more pronounced for small messages. For this reason, we decided to implement our algorithm with only a single message buffer per neighbor.
Reference: [23] <author> Joseph W. H. Liu. </author> <title> The Multifrontal Method for Sparse Matrix Solution: </title> <journal> Theory and Practice. SIAM Review, </journal> <volume> 34(1) </volume> <pages> 82-109, </pages> <year> 1992. </year>
Reference-contexts: This reduces the communication overhead and improves the isoefficiency to O (p 1:5 log 3 p). Gupta and Kumar [13] recently developed a parallel formulation of sparse Cholesky factorization based on the multifrontal method. The multifrontal method <ref> [2, 23] </ref> is a form of submatrix Cholesky, in which single elimination steps are performed on a sequence of small, dense frontal matrices. <p> Since matrices are symmetric, only the upper triangular part is stored. For further details on the multifrontal method, the reader should refer to Appendix A, and to the excellent tutorial by Liu <ref> [23] </ref>. If some consecutively numbered nodes form a chain in the elimination tree, and the corresponding rows of L have identical nonzero structure, then this chain is called a supernode. The supernodal elimination tree is similar to the elimination tree, but nodes forming a supernode are collapsed together.
Reference: [24] <author> Robert F. Lucas. </author> <title> Solving planar systems of equations on distributed-memory multiprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <address> Palo Alto, CA, </address> <year> 1987. </year> <journal> Also see IEEE Transactions on Computer Aided Design, </journal> <volume> 6 </volume> <pages> 981-991, </pages> <year> 1987. </year>
Reference: [25] <author> Robert F. Lucas, Tom Blank, and Jerome J. Tiemann. </author> <title> A parallel solution method for large sparse systems of equations. </title> <journal> IEEE Transactions on Computer Aided Design, </journal> <volume> CAD-6(6):981-991, </volume> <month> November </month> <year> 1987. </year>
Reference: [26] <author> Mo Mu and John R. Rice. </author> <title> A grid-based subtree-subcube assignment strategy for solving partial differential equations on hypercubes. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 13(3) </volume> <pages> 826-839, </pages> <month> May </month> <year> 1992. </year>
Reference: [27] <author> Dianne P. O'Leary and G. W. Stewart. </author> <title> Assignment and Scheduling in Parallel Matrix Factorization. </title> <journal> Linear Algebra and its Applications, </journal> <volume> 77 </volume> <pages> 275-299, </pages> <year> 1986. </year>
Reference-contexts: For linear systems arising in certain applications, such as linear programming and some structural engineering applications, they are the only feasible methods for numerical factorization. It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers <ref> [4, 27, 7, 22] </ref>.
Reference: [28] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization, Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: Fortunately, this is a typical bin-packing problem, and even though, bin-packing is NP complete, a number of good approximate algorithms exist <ref> [28] </ref>. The use of bin-packing makes it possible to balance the computation and to significantly reduce the load imbalance. Acceptable Partitions A partition is acceptable if the percentage difference in the amount of work in the two parts is less that a small constant *.
Reference: [29] <author> A. Pothen and C-J. Fan. </author> <title> Computing the block triangular form of a sparse matrix. </title> <journal> ACM Transactions on Mathematical Software, </journal> <year> 1990. </year>
Reference-contexts: Elimination of rows in different subtrees can proceed concurrently. For a given matrix, elimination trees of smaller height usually have greater concurrency than trees of larger height. A desirable ordering for parallel computers must increase the amount of concurrency without increasing fill-in substantially. Spectral nested dissection <ref> [29, 30, 19] </ref> has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings to the performance of our algorithm refer to [21]. <p> Table 1 shows the load imbalance at the top level of the elimination tree for some matrices from the Boeing-Harwell matrix set. These matrices were ordered using the spectral nested dissection <ref> [29, 30, 19] </ref>. Note that for all matrices the load imbalance in terms of operation count is substantially higher than the relative difference in the number of nodes in the left and right subtrees. <p> MAROS-R7 is from a linear programming problem taken from NETLIB. COPTER2 11 comes from a model of a helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. In all of our experiments, we used spectral nested dissection <ref> [29, 30] </ref> to order the matrices. The performance obtained by our multifrontal algorithm in some of these matrices is shown in Table 2. The operation count shows only the number of operations required to factor the nodes of the elimination tree (it does not include the operations involved in extend-add).
Reference: [30] <author> Alex Pothen, Horst D. Simon, and Kang-Pu Liou. </author> <title> Partitioning Sparse Matrices With Eigenvectors of Graphs. </title> <journal> SIAM J. on Matrix Analysis and Applications, </journal> <volume> 11(3) </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: Elimination of rows in different subtrees can proceed concurrently. For a given matrix, elimination trees of smaller height usually have greater concurrency than trees of larger height. A desirable ordering for parallel computers must increase the amount of concurrency without increasing fill-in substantially. Spectral nested dissection <ref> [29, 30, 19] </ref> has been found to generate orderings that have both low fill-in and good parallelism. For the experiments presented in this paper we used spectral nested dissection. For a more extensive discussion on the effect of orderings to the performance of our algorithm refer to [21]. <p> Table 1 shows the load imbalance at the top level of the elimination tree for some matrices from the Boeing-Harwell matrix set. These matrices were ordered using the spectral nested dissection <ref> [29, 30, 19] </ref>. Note that for all matrices the load imbalance in terms of operation count is substantially higher than the relative difference in the number of nodes in the left and right subtrees. <p> MAROS-R7 is from a linear programming problem taken from NETLIB. COPTER2 11 comes from a model of a helicopter rotor. CUBE35 is a 35 fi 35 fi 35 regular three-dimensional grid. In all of our experiments, we used spectral nested dissection <ref> [29, 30] </ref> to order the matrices. The performance obtained by our multifrontal algorithm in some of these matrices is shown in Table 2. The operation count shows only the number of operations required to factor the nodes of the elimination tree (it does not include the operations involved in extend-add).
Reference: [31] <author> Alex Pothen and Chunguang Sun. </author> <title> Distributed multifrontal factorization using clique trees. </title> <booktitle> In Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 34-40, </pages> <year> 1991. </year>
Reference: [32] <author> Roland Pozo and Sharon L. Smith. </author> <title> Performance evaluation of the parallel multifrontal method in a distributed-memory environment. </title> <booktitle> In Proceedings of the Sixth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <pages> pages 453-456, </pages> <year> 1993. </year>
Reference: [33] <author> P. Raghavan. </author> <title> Distributed sparse Gaussian elimination and orthogonal factorization. </title> <type> Technical Report 93-1818, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1993. </year>
Reference: [34] <author> P. Raghavan. </author> <title> Line and plane separators. </title> <type> Technical Report UIUCDCS-R-93-1794, </type> <institution> Department of Computer Science, University of Illinois, Urbana, </institution> <address> IL 61801, </address> <month> February </month> <year> 1993. </year> <month> 15 </month>
Reference-contexts: Another step that is quite time consuming, and has not been parallelized effectively is that of ordering. In our current research we are investigating ordering algorithms that can be implemented fast on parallel computers <ref> [16, 34] </ref>. Acknowledgment We would like to thank the Minnesota Supercomputing Center for providing access to a 64-processor Cray T3D, and Cray Research Inc. for providing access to a 256-processor Cray T3D. Finally we wish to thank Dr. Alex Pothen for his guidance with spectral nested dissection ordering.
Reference: [35] <author> Edward Rothberg. </author> <title> Performance of Panel and Block Approaches to Sparse Cholesky Factorization on the iPSC/860 and Paragon Multicomputers. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <month> May </month> <year> 1994. </year>
Reference-contexts: In a smarter parallel formulation [11], the rows of the matrix are allocated to processors using the subtree-to-subcube mapping. This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O (p 3 ). Rothberg and Gupta <ref> [36, 35] </ref> used a different method to reduce the communication overhead. In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O (p 1:5 log 3 p).
Reference: [36] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse Cholesky factorization. </title> <booktitle> In Supercomputing '93 Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: In a smarter parallel formulation [11], the rows of the matrix are allocated to processors using the subtree-to-subcube mapping. This localizes the communication among groups of processors, and thus improves the isoefficiency of the scheme to O (p 3 ). Rothberg and Gupta <ref> [36, 35] </ref> used a different method to reduce the communication overhead. In their method, the entire sparse matrix is partitioned among processors using a two-dimensional block cyclic mapping. This reduces the communication overhead and improves the isoefficiency to O (p 1:5 log 3 p).
Reference: [37] <author> P. Sadayappan and Sailesh K. Rao. </author> <title> Communication Reduction for Distributed Sparse Matrix Factorization on a Processors Mesh. </title> <booktitle> In Supercomputing '89 Proceedings, </booktitle> <pages> pages 371-379, </pages> <year> 1989. </year>
Reference: [38] <author> Robert Schreiber. </author> <title> Scalability of sparse direct solvers. </title> <type> Technical Report RIACS TR 92.13, </type> <institution> NASA Ames Research Center, Moffet Field, </institution> <address> CA, </address> <month> May </month> <year> 1992. </year> <note> Also appears in A. </note> <editor> George, John R. Gilbert, and J. W.-H. Liu, editors, </editor> <title> Sparse Matrix Computations: Graph Theory Issues and Algorithms (An IMA Workshop Volume). </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1993. </year>
Reference-contexts: It is well known that dense matrix factorization can be implemented efficiently on distributed-memory parallel computers [4, 27, 7, 22]. However, despite inherent parallelism in sparse sparse direct methods, not much success has been achieved to date in developing their scalable parallel formulations <ref> [15, 38] </ref>, and for several years, it has been a challenge to implement efficient sparse linear system solvers using direct methods on even moderately parallel computers. <p> In <ref> [38] </ref>, Schreiber concludes that it is not yet clear whether sparse direct solvers can be made competitive at all for highly (p 256) and massively (p 4096) parallel computers. A parallel formulation for sparse matrix factorization can be easily obtained by simply distributing rows to different processors [8].
Reference: [39] <author> Chunguang Sun. </author> <title> Efficient parallel solutions of large sparse SPD systems on distributed-memory multiprocessors. </title> <type> Technical report, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1993. </year>
Reference: [40] <author> Sesh Venugopal and Vijay K. Naik. </author> <title> Effects of partitioning and scheduling sparse matrix factorization on communication and load balance. </title> <booktitle> In Supercomputing '91 Proceedings, </booktitle> <pages> pages 866-875, </pages> <year> 1991. </year>
Reference: [41] <author> M. Yannakakis. </author> <title> Computing the minimum fill-in is NP-complete. </title> <journal> SIAM J. Algebraic Discrete Methods, </journal> <volume> 2 </volume> <pages> 77-79, </pages> <year> 1981. </year>
Reference-contexts: More precisely, we can choose a permutation matrix P such that the Cholesky factors 2 of P AP T have minimal fill-in. The problem of finding the best ordering for M that minimizes the amount of fill-in is NP-complete <ref> [41] </ref>, therefore a number of heuristic algorithms for ordering have been developed. In particular, minimum degree ordering [9, 14, 10] is found to have low fill-in. For a given ordering of a matrix, there exists a corresponding elimination tree. Each node in this tree is a column of the matrix.
References-found: 41

