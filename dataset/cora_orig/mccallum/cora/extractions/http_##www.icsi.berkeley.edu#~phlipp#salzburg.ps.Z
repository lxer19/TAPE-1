URL: http://www.icsi.berkeley.edu/~phlipp/salzburg.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/~phlipp/phlipp.publ.html
Root-URL: http://www.icsi.berkeley.edu
Title: Modula-2* and its Compilation a programming language for writing highly parallel programs in a machine-independent,
Author: Michael Philippsen and Walter F. Tichy 
Note: Modula-2*, an extension of Modula-2, is  
Affiliation: Universitat Karlsruhe  
Abstract: email: philippsen@ira.uka.de This paper appeared in: First International Conference of the Austrian Center for Parallel Computation, Salzburg, Austria, 1991, pages 169-183. Springer Verlag, Lecture Notes in Computer Science 591, 1992 Abstract This article briefly describes Modula-2* and discusses its major advantages over the data-parallel programming model. We also present the principles of translating Modula-2* programs to MIMD and SIMD machines and discuss the lessons learned from our first compiler, targeting the Connection Machine. We conclude with important architectural principles required of parallel computers to allow for efficient, compiled programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Selim G. Akl. </author> <title> The Design and Analysis of Parallel Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1989. </year>
Reference-contexts: We claim that only minor extensions of existing programming languages are required to express highly parallel programs. Thus, programmers will need only moderate additional training, mainly in the area of parallel algorithms and their analysis. This area, fortunately, is well developed; see for instance textbooks <ref> [1] </ref> and [5]. In compiler technology, however, new techniques must be found to map machine-independent programs to existing architectures, while at the same time parallel machine architecture must evolve to efficiently support the features that are required for problem-oriented programming styles. <p> Most parallel algorithms in textbooks are data-parallel (compare for instance <ref> [1, 5] </ref>). According to Fox [4], more than 80% of the 84 existing, parallel applications he examined fall in the class of synchronous, data-parallel programs. Furthermore, systolic algorithms as well as vector-algorithms are special cases of data-parallel algorithms.
Reference: [2] <institution> American National Standards Institute, Inc., </institution> <address> Washington, D.C. </address> <month> ANSI, </month> <title> Programming Language 9 Fortran Extended (Fortran 90). ANSI X3.198--1992, </title> <year> 1992. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not.
Reference: [3] <author> Henry E. Bal, Jennifer S. Steiner, and Andrew S. Tanenbaum. </author> <title> Programming languages for distributed computing systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 261-322, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP [11, 17, 14, 9, 10, 16] and others <ref> [3] </ref>. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [4] <author> Geoffrey C. Fox. </author> <title> What have we learnt from using real parallel machines to solve real problems? In Proc. </title> <booktitle> of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <volume> volume 2, </volume> <pages> pages 897-955, </pages> <address> Pasadena, CA, 1988. </address> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: Most parallel algorithms in textbooks are data-parallel (compare for instance [1, 5]). According to Fox <ref> [4] </ref>, more than 80% of the 84 existing, parallel applications he examined fall in the class of synchronous, data-parallel programs. Furthermore, systolic algorithms as well as vector-algorithms are special cases of data-parallel algorithms.
Reference: [5] <author> Alan Gibbons and Wojciech Rytter. </author> <title> Efficient Parallel Algorithms. </title> <publisher> Cambridge University Press, </publisher> <year> 1988. </year>
Reference-contexts: We claim that only minor extensions of existing programming languages are required to express highly parallel programs. Thus, programmers will need only moderate additional training, mainly in the area of parallel algorithms and their analysis. This area, fortunately, is well developed; see for instance textbooks [1] and <ref> [5] </ref>. In compiler technology, however, new techniques must be found to map machine-independent programs to existing architectures, while at the same time parallel machine architecture must evolve to efficiently support the features that are required for problem-oriented programming styles. <p> Most parallel algorithms in textbooks are data-parallel (compare for instance <ref> [1, 5] </ref>). According to Fox [4], more than 80% of the 84 existing, parallel applications he examined fall in the class of synchronous, data-parallel programs. Furthermore, systolic algorithms as well as vector-algorithms are special cases of data-parallel algorithms.
Reference: [6] <author> W. Daniel Hillis and Guy L. Steele. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Thus, programs can use SIMD-mode where proper synchronization is difficult, or use MIMD-mode where synchronization is simple or infrequent. The two modes can even be intermixed freely. The data-parallel approach, discussed in <ref> [6] </ref> and exemplified in languages such as *LISP, C*, and MPL is currently quite successful, because it has reduced machine dependence of parallel programs. Data-parallelism extends a synchronous, SIMD model with a global name space, which obviates the need for explicit message passing between processing elements. <p> As an example, consider the computation of all postfix sums of a vector V of length N . The program should place into V [i] the sum of all elements V [i] : : : V [N 1]. A recursive doubling technique as in reference <ref> [6] </ref> computes all postfix sums in O (log N ) time, where N is the length of the vector. where j counts the iterations. The inner forall creates N processes. Note that there is a one-to-one mapping between process numbers and elements of the vector.
Reference: [7] <author> Charles Koelbel and Piyush Mehrotra. </author> <title> Supporting shared data structures on distributed memory architectures. </title> <booktitle> In Proc. of the 2nd ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming, PPOPP, </booktitle> <pages> pages 177-186, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not.
Reference: [8] <author> Ralf Kretzschmar. </author> <title> Ein Modula-2*-Compiler fur die Connection Machine CM-2. </title> <type> Master's thesis, </type> <institution> University of Karlsruhe, Department of Informat-ics, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The extension is small and easy to learn, but provides a programming model that is far more general and machine independent than other proposals. Next, we discuss compilation techniques for targeting MIMD and SIMD machines and report on experience with our first Modula-2* compiler <ref> [8] </ref> for the Connection Machine.
Reference: [9] <author> James McGraw, Stephen Skedzielewski, Stephen Allan, Rod Oldehoeft, John Glauert, Chris Kirkham, Bill Noyce, and Robert Thomas. </author> <title> SISAL Language Reference Manual. </title> <institution> Lawrence Livermore National Laboratory, </institution> <month> March </month> <year> 1985. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not. <p> Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [10] <author> James R. McGraw. </author> <title> The VAL language: Description and analysis. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(1) </volume> <pages> 44-82, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not. <p> Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [11] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not. <p> Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [12] <author> Michael Metcalf and John Reid. </author> <title> Fortran 90 Explained. </title> <publisher> Oxford Science Publications, </publisher> <year> 1990. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not.
Reference: [13] <author> John K. Ousterhout, Donald A. Scelza, and Pradeep S. Sindhu. </author> <title> Medusa: An experiment in distributed operating system structure. </title> <journal> Communications of the ACM, </journal> <volume> 23(2) </volume> <pages> 92-205, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: Co-scheduling of threads in the same forall is necessary to avoid delays inherent in context swaps when the threads communicate. Without co-scheduling, communicating threads may enter a situation where they execute alternatingly or in co-routine fashion instead of in parallel <ref> [13] </ref>. Co-scheduling can be accomplished by increasing the thread priority with the nesting depth of foralls, or by providing special mechanisms for "task forces", i.e., for scheduling groups of threads simultaneously. Obviously, thread creation, termination, and load balancing must be as fast as possible.
Reference: [14] <editor> Prentice Hall, </editor> <address> Englewood Cliffs, New Jersey. </address> <note> INMOS Limited: Occam Programming Manual, </note> <year> 1984. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not. <p> Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [15] <author> M. Rosing, R. Schnabel, and R. Weaver. DINO: </author> <title> Summary and example. </title> <booktitle> In Proc. of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 472-481, </pages> <address> Pasadena, CA, 1988. </address> <publisher> ACM Press, </publisher> <address> New York. </address>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not.
Reference: [16] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> *Lisp Reference Manual, Version 5.0, </note> <year> 1988. </year>
Reference-contexts: Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [17] <institution> Thinking Machines Corporation, Cambridge, Massachusetts. </institution> <note> C* Programming Guide, Version 6.0, </note> <month> November </month> <year> 1990. </year>
Reference-contexts: Variants of both the synchronous and asynchronous form of the forall statement have been introduced by previously proposed languages, such as Blaze, C*, Oc-cam, Sisal, VAL, *LISP <ref> [11, 17, 14, 9, 10, 16] </ref> and others [3]. Note also that vector instructions are simple instances of the synchronous forall. None of the languages mentioned above include both forms of the forall statement, even though both are necessary for writing readable and portable parallel programs.
Reference: [18] <author> Walter F. Tichy. </author> <title> Parallel matrix multiplication on the Connection Machine. </title> <journal> International Journal of High Speed Computing, </journal> <volume> 1(2) </volume> <pages> 247-262, </pages> <year> 1989. </year>
Reference-contexts: Two major approaches to the programming problem can be distinguished: The first is to automatically parallelize sequential software. Although there is overwhelming economic justification for it, this approach will meet with only limited success in the short to medium term (see, for instance, <ref> [18] </ref>). The goal of automatically producing parallel programs can only, if ever, be achieved by program transformations that start with the problem specification and not with a sequential implementation. In a sequential program, too many opportunities for parallelism have been hidden or eliminated.
Reference: [19] <author> U.S. </author> <title> Government, Ada Joint Program Office. ANSI/MIL-Std 1815 A, Reference Manual for the Ada Programming Language, </title> <month> January </month> <year> 1983. </year>
Reference-contexts: We conclude with properties of parallel machine architectures that would improve the efficiency of high-level parallel programs. 2 Related Work Most current programming languages for parallel and highly parallel machines, including *LISP, C*, MPL, VAL, Sisal, Occam, Ada, FORTRAN90, Blaze, Dino, and Kali <ref> [10, 9, 14, 19, 2, 12, 11, 15, 7] </ref> suffer from some or all of the following problems: 1 * Whereas the number of processors of a parallel machine is fixed, the problem size is not.
Reference: [20] <author> Niklaus Wirth. </author> <title> Programming in Modula-2 (Third corrected Edition). </title> <publisher> Springer-Verlag Berlin, </publisher> <address> Hei-delberg, New York, </address> <year> 1985. </year> <month> 10 </month>
Reference-contexts: We take the approach of expressing parallelism explicitly, but in a machine-independent way. In section 2 we analyze the problems that plague most parallel programming languages today. Section 3 then presents Modula-2*, an extension of Modula-2 <ref> [20] </ref>, for the explicit formulation of highly parallel programs. The extension is small and easy to learn, but provides a programming model that is far more general and machine independent than other proposals.
References-found: 20

