URL: http://www.icsi.berkeley.edu/~amnon/Papers/disp.ps
Refering-URL: http://www.icsi.berkeley.edu/~amnon/
Root-URL: http://www.icsi.berkeley.edu
Email: ICSI  
Title: Almost Optimal Dispersers  
Author: Amnon Ta-Shma 
Abstract: A (K; *) disperser graph G = (V 1 ; V 2 ; E) is a bipartite graph with the property that for any subset A V 1 of cardinality K, the neighbors of A cover at least 1 * fraction of the vertices of V 2 . Such graphs have many applications in derandomization. Saks, Srinivasan and Zhou presented an explicit construction of (K = 2 k ; *) disperser graphs G = (V = [2 n ]; W; E) with an almost optimal degree D = poly(n; * 1 ), for every k n (1) . We extend their result for any parameter k n. 
Abstract-found: 1
Intro-found: 1
Reference: [CG88] <author> B. Chor and O. Goldreich. </author> <title> Unbiased bits from sources of weak randomness and probabilistic communication complexity. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(2) </volume> <pages> 230-261, </pages> <year> 1988. </year>
Reference-contexts: The ultimate goal is finding an optimal extractor for all distributions having k min-entropy, which is still open for most cases [Nis96]. However, it turns out that the problem is much simpler when we restrict ourselves to random sources having more structure: Definition 2.4 <ref> [CG88] </ref> (block-wise source) Suppose X is a distribution over f0; 1g n , and is a partition of [1::n] into l consecutive blocks. Define the induced random variable X i which is the result of the random variable X when restricted to the i'th block of .
Reference: [Nis96] <author> N. Nisan. </author> <title> Refining randomness: Why and how. </title> <booktitle> In Annual Conference on Structure in Complexity Theory, </booktitle> <year> 1996. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan <ref> [Nis96] </ref>). The question whether explicit constructions of such graphs do exist attracted much research in the last decade [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96]. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. <p> away from the optimal one which is only log log ( 1 * ) + O (1), and reducing the min-entropy loss to the optimal is an important open problem with many applications (e.g. for the construction of explicit a-expanding graphs and depth 2 super-concentrators, see the survey paper of <ref> [Nis96] </ref>). The paper is constructed as follows: in Section 2 we give definitions and a few preliminaries. In Section 3 we present the new small family of segmentations, in Section 4 we show how to build from it a structure that in particular implies the existence of good dispersers. <p> However, to get a wider picture and a better understanding of the relationships between these different objects we recommend the reader to look at the survey paper of <ref> [Nis96] </ref>. We are going to work with distributions. A probability distribution X over fl, is a function X : fl 7! [0; 1] s.t. a2fl X (a) = 1. We measure the amount of random ness in the distribution by considering the min-entropy. <p> The ultimate goal is finding an optimal extractor for all distributions having k min-entropy, which is still open for most cases <ref> [Nis96] </ref>. However, it turns out that the problem is much simpler when we restrict ourselves to random sources having more structure: Definition 2.4 [CG88] (block-wise source) Suppose X is a distribution over f0; 1g n , and is a partition of [1::n] into l consecutive blocks. <p> It will be very interesting (and useful) to have a direct (and hopefully efficient) construction for mergers. Another open problem is reducing the entropy loss to O (log (n)). As stated earlier this has some beautiful consequences (e.g. for the construction of explicit depth 2 superconcentrators, see <ref> [Nis96] </ref>). It seems that Theorem 2 gives a lead for making progress on this problem, as it supplies an optimal somewhere-random extractor working for very low min-entropies and retrieving a (relatively) large amount of randomness.
Reference: [NZ93] <author> N. Nisan and D. Zuckerman. </author> <title> More deterministic simulation in logspace. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 235-244, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters. <p> Nisan and Zuckerman <ref> [NZ93] </ref> and later Srinivasan and Zuckerman [SZ94] showed how to extract randomness from block-wise sources: Definition 2.5 (a block-wise extractor) A ((z 1 ; : : : ; z l ); *) block-wise extractor is a function E : f0; 1g n fif0; 1g t 7! f0; 1g m that is <p> We say a block-wise extractor is ex plicit, if given ; x; y, E (x; y) can be computed in time polynomial in n + t. Nisan and Zuckerman <ref> [NZ93] </ref> showed a nice block-wise extractor that was later improved by Srinivasan and Zuckerman [SZ94], yielding: Fact 2.1 [SZ94] There is a constant C sz &gt; 1 s.t. for any b; * &gt; 0; l 1, z 0 sz b there is an explicit (; z 0 l ; *) block-wise <p> This idea originates in the work of <ref> [NZ93] </ref>. When taking this as our measure of surprise we can see that if X has high min-entropy, almost all strings x 2 X have many surprising bits. <p> This idea, of try ing all possible segmentations from a small fixed family, comes directly from the [SSZ95] paper, but the implementation here is more direct than in <ref> [NZ93, SSZ95] </ref>, resulting in a simpler and stronger analysis.
Reference: [RTS97] <author> J. Radhakrishnan and A. Ta-Shma. </author> <title> Tight bounds for depth-two superconcentrators. </title> <booktitle> In IEEE Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1997. </year>
Reference-contexts: It is known that any non-trivial disperser must have degree D that is at least D (log (N ) 1 * ) and the size of the right hand side M is at most M O ( KD log ( 1 * ) ) <ref> [RTS97] </ref>. Indeed, if we build such a graph by choosing D independent neighbors for each v 2 V , we get an optimal disperser with tight degree D (up to a multiplicative factor) and maximal number of right hand side vertices (again, up to a multiplicative factor).
Reference: [Sip88] <author> Sipser. Expanders, </author> <title> randomness, or time versus space. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 36, </volume> <year> 1988. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters.
Reference: [SSZ95] <author> M. Saks, A. Srinivasan, and S. Zhou. </author> <title> Explicit dispersers with polylog degree. </title> <booktitle> In Proceedings of the 26th Annual ACM Symposium on the Theory of Computing, ACM, </booktitle> <year> 1995. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters. <p> As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96]. Saks, Srini-vasan and Zhou <ref> [SSZ95] </ref> showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters. <p> Saks, Srinivasan and Zhou <ref> [SSZ95] </ref> showed a disperser construction with degree D = poly (n; * 1 ) for sets of size K = 2 k with k n (1) . Their work was based on earlier results and a new combinatorial construction of a small family of segmentations (that will be described later). <p> We simplify the SSZ combinatorial construction, giving a simpler analysis with better bounds. We use the new family of segmentations to build the disperser graph in much the same way as is done in <ref> [SSZ95] </ref>, but we prove the correctness of the construction using the terminology of [Ta-96], which results in a simpler proof with tighter bounds. <p> In the disperser we presented, however, the right hand size has size M = K=2 polylog (n) . For convenience we measure the log of this loss, i.e., the entropy loss of a disperser is log (K) + log (D) log (M ). The <ref> [SSZ95] </ref> disperser has n (1) entropy loss, while our disperser has poly (log (n); log ( 1 * )) entropy loss. <p> In Section 4 we construct good somewhere random extractors, and combining it with the above lemma we get Theorem 1. 3 A Small Family of Segmentations In this section we strengthen a combinatorial lemma appearing in <ref> [SSZ95] </ref>. Loosely speaking, the lemma claims there is a small family of segmentations of [1::n] into few blocks l, s.t. for any possible way of dividing weight among these n points, there is at least one segmenta tion in the family s.t. any block in the segmentation has high weight. <p> The next lemma is a generalization of the lemma appearing in <ref> [SSZ95] </ref>. The proof presents a somewhat simpler algorithm, with a simpler analysis and better pa rameters. The idea behind the construction is, though, essentially the same as in [SSZ95]. Lemma 3.1 Suppose k 2 l z log (n) w for some positive values k; l; n; z; w. <p> The next lemma is a generalization of the lemma appearing in <ref> [SSZ95] </ref>. The proof presents a somewhat simpler algorithm, with a simpler analysis and better pa rameters. The idea behind the construction is, though, essentially the same as in [SSZ95]. Lemma 3.1 Suppose k 2 l z log (n) w for some positive values k; l; n; z; w. <p> Thus trying all the possible segmentations (and there aren't too many of them) we know one of them will work and give us an almost uniform distribution. This idea, of try ing all possible segmentations from a small fixed family, comes directly from the <ref> [SSZ95] </ref> paper, but the implementation here is more direct than in [NZ93, SSZ95], resulting in a simpler and stronger analysis. <p> This idea, of try ing all possible segmentations from a small fixed family, comes directly from the [SSZ95] paper, but the implementation here is more direct than in <ref> [NZ93, SSZ95] </ref>, resulting in a simpler and stronger analysis.
Reference: [SZ94] <author> A. Srinivasan and D. Zuckerman. </author> <title> Computing with very weak random sources. </title> <booktitle> In Proceedings of the 35th Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters. <p> Nisan and Zuckerman [NZ93] and later Srinivasan and Zuckerman <ref> [SZ94] </ref> showed how to extract randomness from block-wise sources: Definition 2.5 (a block-wise extractor) A ((z 1 ; : : : ; z l ); *) block-wise extractor is a function E : f0; 1g n fif0; 1g t 7! f0; 1g m that is an extractor for the set of <p> We say a block-wise extractor is ex plicit, if given ; x; y, E (x; y) can be computed in time polynomial in n + t. Nisan and Zuckerman [NZ93] showed a nice block-wise extractor that was later improved by Srinivasan and Zuckerman <ref> [SZ94] </ref>, yielding: Fact 2.1 [SZ94] There is a constant C sz &gt; 1 s.t. for any b; * &gt; 0; l 1, z 0 sz b there is an explicit (; z 0 l ; *) block-wise extractor E : f0; 1g n fif0; 1g t 7! f0; 1g m with <p> We say a block-wise extractor is ex plicit, if given ; x; y, E (x; y) can be computed in time polynomial in n + t. Nisan and Zuckerman [NZ93] showed a nice block-wise extractor that was later improved by Srinivasan and Zuckerman <ref> [SZ94] </ref>, yielding: Fact 2.1 [SZ94] There is a constant C sz &gt; 1 s.t. for any b; * &gt; 0; l 1, z 0 sz b there is an explicit (; z 0 l ; *) block-wise extractor E : f0; 1g n fif0; 1g t 7! f0; 1g m with m = (C l <p> And the correctness follows. 4.2 Plugging specific parameters For the basic lemma to work we need good block-wise extractors, and as mentioned in Section 2 good block-wise extractors do exist. Combining the combinatorial construction of Lemma 3.2 (or even Lemma 3.1) and the block-wise extractor of <ref> [SZ94] </ref> (Fact 2.1) with Lemma 4.1, and plugging the parameters l = O (log log (n)) ; t = O (log (n) + log ( 1 * )) and k = polylog (n) log ( 1 * ) we get: Corollary 4.2 There is a (k = poly (log n; log
Reference: [Ta-96] <author> A. Ta-Shma. </author> <title> On extracting randomness from weak random sources. </title> <booktitle> In STOC, </booktitle> <year> 1996. </year>
Reference-contexts: We simplify the SSZ combinatorial construction, giving a simpler analysis with better bounds. We use the new family of segmentations to build the disperser graph in much the same way as is done in [SSZ95], but we prove the correctness of the construction using the terminology of <ref> [Ta-96] </ref>, which results in a simpler proof with tighter bounds. <p> The reason we achieve much smaller entropy loss is connected to the fact that we have good dispersers for any parameter k, and uses the existence of a good extractor presented in <ref> [Ta-96] </ref>. <p> Next, we are going to weaken the notion of an extractor to that of a somewhere random extractor. We start with a definition of a somewhere random source, taken with some changes from <ref> [Ta-96] </ref>: Definition 2.7 [Ta-96] 1 (somewhere random source) B = B 1 ffi : : : ffi B b is a b-block (m; *; ) somewhere random source, if each B i is a random variable over f0; 1g m , and there is a random variable Y over [0::b] s.t.: <p> Next, we are going to weaken the notion of an extractor to that of a somewhere random extractor. We start with a definition of a somewhere random source, taken with some changes from <ref> [Ta-96] </ref>: Definition 2.7 [Ta-96] 1 (somewhere random source) B = B 1 ffi : : : ffi B b is a b-block (m; *; ) somewhere random source, if each B i is a random variable over f0; 1g m , and there is a random variable Y over [0::b] s.t.: 1 The definition <p> : : ffi B b is a b-block (m; *; ) somewhere random source, if each B i is a random variable over f0; 1g m , and there is a random variable Y over [0::b] s.t.: 1 The definition here is a bit different to the one appearing in <ref> [Ta-96] </ref>. There the conditions are P rob (Y = 0) and for any i, d ((B i jY = i); U m ) *. The two definitions are actually equivalent. To see that our definition implies the one in [Ta-96], define Y 0 (x) to be Y (x) if Y (x) <p> definition here is a bit different to the one appearing in <ref> [Ta-96] </ref>. There the conditions are P rob (Y = 0) and for any i, d ((B i jY = i); U m ) *. The two definitions are actually equivalent. To see that our definition implies the one in [Ta-96], define Y 0 (x) to be Y (x) if Y (x) = i and P rob (Y = i) b , and 0 otherwise. * For any i 2 [1::b]: P rob (Y = i) i); U m ) *. We call Y an (*; ) selector for B. <p> The entropy loss in the system is, however, not optimal. 4.3 Composing somewhere random extractors Now we show that if we can obtain log 2 n quasi-random bits then we can use them to further extract all of the remaining min-entropy in the source using the extractor in <ref> [Ta-96] </ref>. <p> And, j j The output contains nl blocks, where the (i; j) out put block is z j Theorem 3 (Implicit in <ref> [Ta-96] </ref>) Suppose E 1 ; E 2 are as above. Then for every safety parameter s &gt; 0, E 1 E 2 is an (k 1 + k 2 + s; 1 + 2 ; O (n ( 1 + 2 s=3 ))- somewhere random extractor. <p> In Theorem 2 we obtained a somewhere random extractor outputting blocks of length polylog (n). In <ref> [Ta-96] </ref> it was shown that with that amount of truly random bits we can extract all of the remaining min-entropy: Fact 4.1 [Ta-96] For every constant fl &lt; 1 , * 2 n fl , and every k = k (n) there is an explicit (k; *) extractor E : f0; <p> In Theorem 2 we obtained a somewhere random extractor outputting blocks of length polylog (n). In <ref> [Ta-96] </ref> it was shown that with that amount of truly random bits we can extract all of the remaining min-entropy: Fact 4.1 [Ta-96] For every constant fl &lt; 1 , * 2 n fl , and every k = k (n) there is an explicit (k; *) extractor E : f0; 1g n fi f0; 1g polylog (n)log ( 1 * ) 7! f0; 1g k . <p> In particular this reduces the problem of finding explicit general extractors, to that of finding explicit extractors for somewhere random sources, for if we can do the later than by our result we can also do the former. Extractors for somewhere random sources are called "mergers" in <ref> [Ta-96] </ref>, where some explicit constructions with non-optimal degree are presented. It will be very interesting (and useful) to have a direct (and hopefully efficient) construction for mergers. Another open problem is reducing the entropy loss to O (log (n)).
Reference: [Zuc90] <author> D. Zuckerman. </author> <title> General weak random sources. </title> <booktitle> In Proceedings of the 31st Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 534-543, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters.
Reference: [Zuc91] <author> D. Zuckerman. </author> <title> Simulating BPP using a general weak random source. </title> <booktitle> In Proceedings of the 32nd Annual IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> pages 79-89, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters.
Reference: [Zuc96] <author> David Zuckerman. </author> <title> Randomness-optimal sampling, extractors, and constructive leader election. </title> <booktitle> In ACM Symposium on Theory of Computing (STOC), </booktitle> <pages> pages 286-295, </pages> <address> Philadelphia, Pennsylvania, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: 1 Introduction A disperser is a sparse graph with strong random-like properties. As such, explicit dispersers have numerous applications in derandomization (many of them appearing in the excellent survey paper by Nisan [Nis96]). The question whether explicit constructions of such graphs do exist attracted much research in the last decade <ref> [Sip88, Zuc90, Zuc91, NZ93, SZ94, SSZ95, Zuc96] </ref>. Saks, Srini-vasan and Zhou [SSZ95] showed an almost optimal disperser construction for certain parameters. In this paper we show how to extend their work to give explicit constructions for the whole range of parameters.
References-found: 11

