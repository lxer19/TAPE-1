URL: http://www.cs.ucsd.edu/~calder/papers/UCSD-CS98-592.ps.Z
Refering-URL: http://www.cs.ucsd.edu/~calder/papers.html
Root-URL: http://www.cs.ucsd.edu
Email: fcalder,pfellerg@cs.ucsd.edu eustace@pa.dec.com  
Title: Value Profiling and Optimization  
Author: Brad Calder Peter Feller Alan Eustace 
Affiliation: Department of Computer Science and Engineering Compaq Computer Corporation University of California, San Diego Western Research Lab  
Abstract: UCSD Technical Report CS98-592, July 1998 Abstract Variables and instructions that have invariant or predictable values at run-time, but cannot be identified as such using compiler analysis, can benefit from value-based compiler optimizations. Value-based optimizations include all optimizations based on a predictable value or range of values for a variable or instruction at run-time. These include constant propagation, code specialization, optimizations assuming the value predictability of an instruction, continuous optimization, and partial evaluation. This paper explores the value behavior found from profiling load instructions and memory locations. We compare the value predictability and invariant behavior of instructions (registers) and variables (memory locations) found from value profiling across different inputs. We use the value profiles to perform code-specialization, showing that value profiles can be used to reduce a program's execution time up to 21%. The ability to accurately and efficiently generate value profiles is also examined using convergent profiling and random sampling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. M. Anderson, L. M. Berc, J. Dean, S. G. Ghemawat, M. R. Henzinger, S-T. A. Leung, R. L. Sites, M. T. Vandevoorde, C. A. Waldspurger, W. E. Weihl, and G. Chrysos. </author> <title> Continous profiling: </title> <booktitle> Where have all the cycles gone? In Proceedings of the Sixteenth ACM Symposium on Operating System Principles, </booktitle> <month> October </month> <year> 1997. </year>
Reference-contexts: In using this technique our ATOM-based unoptimized convergent value profiler has an average of 5 to 10 time slowdown. With an optimized profiler, the overhead should be reduced to below a 5 time slowdown. Using hardware sampling, as in DCPI <ref> [1] </ref>, could potentially reduce this overhead to a few percent. load has its execution broken up into 10 time intervals, and the invariance is kept track of separately for each time interval and then plotted. Each interval in this example represents 10% of a load's execution.
Reference: [2] <author> J. Auslander, M. Philipose, C. Chambers, S.J. Eggers, and B.N. Bershad. </author> <title> Fast, effective dynamic compilation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 149159. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: For these techniques to be effective the compiler can help determine which sections of code to concentrate on for run-time optimization. Existing techniques for dynamic compilation and adaptive execution require the user to identify run-time invariants using user guided annotations <ref> [2, 9, 11, 21, 22] </ref>. One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization. <p> Consel and Noel [9] use partial evaluation techniques to automatically generate templates for run-time code generation, although their approach still requires the user to annotate arguments of the top-level procedures, global variables and a few data structures as run-time constants. Auslander et al. <ref> [2] </ref> proposed a dynamic compilation system that uses a unique form of binding time analysis to generate templates for code sequences that have been identified as semi-invariant. Their approach currently uses user defined annotations to indicate which variables are semi-invariant.
Reference: [3] <author> T.M. Austin, D.N. Pnevmatikatos, and G.S. Sohi. </author> <title> Streamlining data cache access with fast address calculation. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: To make such an architecture worthwhile, an address other than the effective address would have to be used. Two studies <ref> [3, 7] </ref> have suggested predictive techniques for generating fast or approximate memory addresses early in the pipeline.
Reference: [4] <author> T. Autrey and M. Wolfe. </author> <title> Initial results for glacial variable analysis. </title> <booktitle> In 9th International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: Code fragments can then be optimized by partitioning the invariant parts of the program fragment. Knoblock and Ruf [21] used a form of staging analysis and annotations to guide data specialization. Autrey and Wolfe <ref> [4] </ref> have started to investigate a form of staging analysis for automatic identification of semi-invariant variables.
Reference: [5] <author> B. Calder, P. Feller, and A. Eustace. </author> <title> Value profiling. </title> <booktitle> In 30th International Symposium on Microarchitecture, </booktitle> <pages> pages 259269, </pages> <month> December </month> <year> 1997. </year> <month> 32 </month>
Reference-contexts: The goal is to break true data-dependencies by predicting the outcome value of instructions before they are executed, and forwarding these speculated values to instructions which depend on them. We originally proposed and evaluated Value Profiling in <ref> [5] </ref>. In that work, we examined the invariance of parameters, load instructions, and the breakdown of invariance between different instruction types. We showed that the invariance and last value predictability of instructions was similar between different inputs. In addition, we presented two approaches for reducing the time for value profiling. <p> The motivation for this research was to compare the predictability and invariance of load instructions with the vari ables used by those instructions. Our prior research <ref> [5] </ref> focused only on profiling instructions. <p> The problem with a straight forward profiler, as shown in Figure 1, is it could run hundreds of times slower than the original application, especially if all of the instructions are profiled. One solution we proposed in <ref> [5] </ref> was to use an intelligent profiler that realizes the data (invariance and top N values) being profiled is converging to a steady state and then profiling is turned off on an instruction by instruction basis.
Reference: [6] <author> B. Calder and D. Grunwald. </author> <title> Reducing indirect function call overhead in C++ programs. </title> <booktitle> In 1994 ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 397408, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: One method to reduce the penalty of virtual function calls is to create a specialized version of the method, and have the code conditioned on the type of the object used in the virtual function call. Calder and Grunwald <ref> [6] </ref> found that on average 66% of the virtual function calls had only a single destination, and these could benefit from code specialization. H olzle et al. [20] implemented a run-time type feedback system.
Reference: [7] <author> B. Calder, D. Grunwald, and J. Emer. </author> <title> Predictive sequential associative cache. </title> <booktitle> In Proceedings of the Second International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: To make such an architecture worthwhile, an address other than the effective address would have to be used. Two studies <ref> [3, 7] </ref> have suggested predictive techniques for generating fast or approximate memory addresses early in the pipeline.
Reference: [8] <author> B. Calder, D. Grunwald, and B. Zorn. </author> <title> Quantifying behavioral differences between C and C++ programs. </title> <journal> Journal of Programming Languages, </journal> <volume> 2(4):313351, </volume> <year> 1994. </year>
Reference-contexts: There will be one general version of the code, and a special version of the code. The two versions of the code will be conditioned on the semi-invariant variable, to choose which version to execute. Calder and Grunwald <ref> [8] </ref> found that up to 80% of all function calls in C++ languages are made indirectly. These indirect function calls are virtual function calls, and can have multiple branch destinations. They pose a serious performance bottleneck for future processors that try to exploit instruction level parallelism.
Reference: [9] <author> C. Consel and F. Noel. </author> <title> A general approach for run-time specialization and its application to C. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 145156. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: For these techniques to be effective the compiler can help determine which sections of code to concentrate on for run-time optimization. Existing techniques for dynamic compilation and adaptive execution require the user to identify run-time invariants using user guided annotations <ref> [2, 9, 11, 21, 22] </ref>. One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization. <p> Knoblock and Ruf [21] used a form of staging analysis and annotations to guide data specialization. Autrey and Wolfe [4] have started to investigate a form of staging analysis for automatic identification of semi-invariant variables. Consel and Noel <ref> [9] </ref> use partial evaluation techniques to automatically generate templates for run-time code generation, although their approach still requires the user to annotate arguments of the top-level procedures, global variables and a few data structures as run-time constants.
Reference: [10] <author> J. Dean, C. Chambers, and D. Grove. </author> <title> Selective specialization for object-oriented languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 93102. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1995. </year>
Reference-contexts: Using the type feedback information, the compiler can then inline any dynamically dispatched function calls, specializing the dispatch based on the frequently encountered object types. They implemented their system in Self [19], which dynamically compiles or recompiles the code applying the optimization with polymorphic inline caches. Dean et al. <ref> [10, 18] </ref> extend the approach of customization by specializing only those cases where the highest benefit can be achieved. Selective specialization uses a run-time profile to determine exactly where customization would be most beneficial.
Reference: [11] <author> D.R. Engler, W.C. Hsieh, and M.F. Kaashoek. </author> <title> `C: A language for high-level efficient, and machine-independent dynamic code generation. </title> <booktitle> In Thirteenth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 131144. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: For these techniques to be effective the compiler can help determine which sections of code to concentrate on for run-time optimization. Existing techniques for dynamic compilation and adaptive execution require the user to identify run-time invariants using user guided annotations <ref> [2, 9, 11, 21, 22] </ref>. One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization.
Reference: [12] <author> P. Feller. </author> <title> Value profiling for instructions and memory locations. </title> <type> Technical Report CS98-581, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <month> April </month> <year> 1998. </year> <type> MS Thesis. </type>
Reference-contexts: We chose a large TNV table size for comparison, since the goal is to determine the minimum sizes for the steady part and clear part of the table for efficient, but accurate, value profiling. In this section we provide a summary of the results. See <ref> [12] </ref> for the complete results for each program. <p> We use an invariance threshold of 30%. This was shown to filter out the instructions with random behavior in <ref> [12] </ref>. 1. Finding the Top Value (Find-Top). The Find-Top metric shows the percent of time the top value for an instruction in the first profile is equal to one of the values in the steady part of the table in the second profile.
Reference: [13] <author> C. Fu, M.D. Jennings, S.Y. Larin, </author> <title> and T.M. Conte. Software-only value speculation scheduling. </title> <type> Technical report, </type> <institution> Department of Electrical and Computer Engineering, North Carolina State University, </institution> <month> June </month> <year> 1998. </year>
Reference-contexts: They used value profiling to identify the load instructions to speculate, and they used value prediction hardware to predict the value of the speculated load instruction. They then extended this work to provide the value prediction in software with no hardware value prediction support <ref> [13] </ref>. They break the dependency chain started by a load by inserting an add instruction which provides the predicted value.
Reference: [14] <author> C. Fu, M.D. Jennings, S.Y. Larin, </author> <title> and T.M. Conte. Value speculation scheduling for high performance processors. </title> <booktitle> In Eigth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1998. </year>
Reference-contexts: Value profiling and memory disambiguation profiling [26] can be used to identify the loads that will not have conflicting addresses with stores, and loads that have invariant value behavior. These loads will benefit the most from this type of load speculation. Chao-ying et al. <ref> [14] </ref> examined a technique similar to Moudgill and Moreno, where they speculate a load and its dependent instructions, and then re-execute the load in its original place performing fix up code if the values are different.
Reference: [15] <author> F. Gabbay and A. Mendelson. </author> <title> Speculative execution based on value prediction. </title> <type> EE Department TR 1080, </type> <institution> Technion - Israel Institue of Technology, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: Value Profiling can be used to identify the invariance and the top N values or range of a variable. Value Profiling can also be used to identify the predictability of instructions for value prediction and value-based optimization. Value prediction <ref> [15, 23, 24] </ref> enables programs to exceed the limits which are placed upon them by their data-dependencies. The goal is to break true data-dependencies by predicting the outcome value of instructions before they are executed, and forwarding these speculated values to instructions which depend on them. <p> Value profiling can also be used to provide an automated approach for identifying semi-invariant variables for guiding dynamic compilation, adaptive execution, and code specialization. 2.1 Value Prediction The recent publications on value prediction <ref> [15, 23, 24] </ref> in hardware provided motivation for our research into value profiling. Lipasti et al. [23] introduced the term value locality, which describes the likelihood of the recurrence of a previously seen value within a storage location. <p> These results show that there is a high degree of temporal locality in the values produced by instructions. Last Value Prediction (LVP) is implemented in hardware using an N entry value history table <ref> [15] </ref>. The table contains a value field and an optional tag, which would store the identity of the instruction which is mapped to the entry. The PC of the executing instruction is used to hash into that table to retrieve the last value.
Reference: [16] <author> F. Gabbay and A. Mendelson. </author> <booktitle> Can program profiling support value prediction? In 30th International Symposium on Microarchitecture, </booktitle> <pages> pages 270280, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: The PC of the executing instruction is used to hash into that table to retrieve the last value. Several additional value predictor models have been proposed <ref> [16, 28, 30] </ref>. These include stride prediction, context prediction, and several hybrid approaches. Gabbay et al. [16] studied the applicability of program profiling to aid value prediction. His motivation for using profiling information was to classify the instructions tendency to be value predictable. <p> The PC of the executing instruction is used to hash into that table to retrieve the last value. Several additional value predictor models have been proposed [16, 28, 30]. These include stride prediction, context prediction, and several hybrid approaches. Gabbay et al. <ref> [16] </ref> studied the applicability of program profiling to aid value prediction. His motivation for using profiling information was to classify the instructions tendency to be value predictable. The opcodes of instructions found to be predictable were annotated. Then only instructions marked predictable were considered 3 for value prediction.
Reference: [17] <author> D.M. Gallagher, W.Y. Chen, S.A. Mahlke, J.C. Gyllenhaal, and W.W. Hwu. </author> <title> Dynamic memory disambiguation using the memory conflict buffer. </title> <booktitle> In Six International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 183193, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: This increases the prediction accuracy and decreases the conflicts or aliasing in a prediction table. 2.2 Load Speculation and Value Speculation Optimizations The Memory Conflict Buffer (MCB) proposed by Gallagher et al. <ref> [17] </ref> provides a hardware solution with compiler support to allow load instructions to speculatively execute before stores. The compiler inserts a check instruction at the point where the load is known to be non-speculative.
Reference: [18] <author> D. Grove, J. Dean, C. Garret, and C. Chambers. </author> <title> Profile-guided receiver class prediction. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 108123. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1995. </year>
Reference-contexts: Using the type feedback information, the compiler can then inline any dynamically dispatched function calls, specializing the dispatch based on the frequently encountered object types. They implemented their system in Self [19], which dynamically compiles or recompiles the code applying the optimization with polymorphic inline caches. Dean et al. <ref> [10, 18] </ref> extend the approach of customization by specializing only those cases where the highest benefit can be achieved. Selective specialization uses a run-time profile to determine exactly where customization would be most beneficial.
Reference: [19] <author> U. Holzle, C. Chambers, and D. Ungar. </author> <title> Optimizing dynamically-types object-oriented languages with polymorhic inline caches. </title> <booktitle> In ECOOP'91, Fourth European Conference on Object-Oriented Programming, </booktitle> <pages> pages 2138, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: H olzle et al. [20] implemented a run-time type feedback system. Using the type feedback information, the compiler can then inline any dynamically dispatched function calls, specializing the dispatch based on the frequently encountered object types. They implemented their system in Self <ref> [19] </ref>, which dynamically compiles or recompiles the code applying the optimization with polymorphic inline caches. Dean et al. [10, 18] extend the approach of customization by specializing only those cases where the highest benefit can be achieved.
Reference: [20] <author> U. Holzle and D. Ungar. </author> <title> Optimizing dynamically-dispatched calls with run-time type feedback. </title> <booktitle> In Proceedings of the SIGPLAN'93 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 326336. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1994. </year>
Reference-contexts: Calder and Grunwald [6] found that on average 66% of the virtual function calls had only a single destination, and these could benefit from code specialization. H olzle et al. <ref> [20] </ref> implemented a run-time type feedback system. Using the type feedback information, the compiler can then inline any dynamically dispatched function calls, specializing the dispatch based on the frequently encountered object types.
Reference: [21] <author> T.B. Knoblock and E. Ruf. </author> <title> Data specialization. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 215225. </pages> <publisher> ACM, </publisher> <month> January </month> <year> 1996. </year>
Reference-contexts: For these techniques to be effective the compiler can help determine which sections of code to concentrate on for run-time optimization. Existing techniques for dynamic compilation and adaptive execution require the user to identify run-time invariants using user guided annotations <ref> [2, 9, 11, 21, 22] </ref>. One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization. <p> Their approach requires programmers to provide hints to the staging analysis to determine what arguments have semi-invariant behavior. Code fragments can then be optimized by partitioning the invariant parts of the program fragment. Knoblock and Ruf <ref> [21] </ref> used a form of staging analysis and annotations to guide data specialization. Autrey and Wolfe [4] have started to investigate a form of staging analysis for automatic identification of semi-invariant variables.
Reference: [22] <author> P. Lee and M. Leone. </author> <title> Optimizing ml with run-time code generation. </title> <booktitle> In Proceedings of the ACM SIGPLAN '96 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 137148. </pages> <publisher> ACM, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: For these techniques to be effective the compiler can help determine which sections of code to concentrate on for run-time optimization. Existing techniques for dynamic compilation and adaptive execution require the user to identify run-time invariants using user guided annotations <ref> [2, 9, 11, 21, 22] </ref>. One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization. <p> One of the goals of value profiling is to provide an automated approach for identifying semi-invariant variables and to use this to guide run-time optimization. Staging analysis has been proposed by Lee and Leone <ref> [22] </ref> as an effective means for determining which computations can be performed early by the compiler and which optimizations should be performed late or postponed by the compiler for dynamic code generation. Their approach requires programmers to provide hints to the staging analysis to determine what arguments have semi-invariant behavior.
Reference: [23] <author> M.H. Lipasti and J.P. Shen. </author> <title> Exceeding the dataflow limit via value prediction. </title> <booktitle> In 29th International Symposium on Microarchitecture, </booktitle> <pages> pages 226237, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: Value Profiling can be used to identify the invariance and the top N values or range of a variable. Value Profiling can also be used to identify the predictability of instructions for value prediction and value-based optimization. Value prediction <ref> [15, 23, 24] </ref> enables programs to exceed the limits which are placed upon them by their data-dependencies. The goal is to break true data-dependencies by predicting the outcome value of instructions before they are executed, and forwarding these speculated values to instructions which depend on them. <p> Value profiling can also be used to provide an automated approach for identifying semi-invariant variables for guiding dynamic compilation, adaptive execution, and code specialization. 2.1 Value Prediction The recent publications on value prediction <ref> [15, 23, 24] </ref> in hardware provided motivation for our research into value profiling. Lipasti et al. [23] introduced the term value locality, which describes the likelihood of the recurrence of a previously seen value within a storage location. <p> Value profiling can also be used to provide an automated approach for identifying semi-invariant variables for guiding dynamic compilation, adaptive execution, and code specialization. 2.1 Value Prediction The recent publications on value prediction [15, 23, 24] in hardware provided motivation for our research into value profiling. Lipasti et al. <ref> [23] </ref> introduced the term value locality, which describes the likelihood of the recurrence of a previously seen value within a storage location.
Reference: [24] <author> M.H. Lipasti, C.B. Wilkerson, and J.P. Shen. </author> <title> Value locality and load value prediction. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 138147, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Value Profiling can be used to identify the invariance and the top N values or range of a variable. Value Profiling can also be used to identify the predictability of instructions for value prediction and value-based optimization. Value prediction <ref> [15, 23, 24] </ref> enables programs to exceed the limits which are placed upon them by their data-dependencies. The goal is to break true data-dependencies by predicting the outcome value of instructions before they are executed, and forwarding these speculated values to instructions which depend on them. <p> Value profiling can also be used to provide an automated approach for identifying semi-invariant variables for guiding dynamic compilation, adaptive execution, and code specialization. 2.1 Value Prediction The recent publications on value prediction <ref> [15, 23, 24] </ref> in hardware provided motivation for our research into value profiling. Lipasti et al. [23] introduced the term value locality, which describes the likelihood of the recurrence of a previously seen value within a storage location.
Reference: [25] <author> M. Moudgill and J. H. Moreno. </author> <title> Run-time detection and recovery from incorrectly reordered memory operations. </title> <institution> IBM Research Report, </institution> <month> May </month> <year> 1997. </year>
Reference-contexts: The check instruction checks to see if a store wrote to the same address since the speculative load was executed. If the speculation was incorrect, recover code has to be executed. Moudgill and Moreno <ref> [25] </ref> proposed a similar approach but instead of comparing addresses as in the MCB approach, they compare the speculated value and the real value of the load. They speculatively execute the load and its dependent instructions, and they also re-execute the load in its original location.
Reference: [26] <author> G. Reinman, B. Calder, D. Tullsen, G. Tyson, and T. Austin. </author> <title> Profile guided load marking for memory renaming. </title> <type> Technical Report CS98-593, </type> <institution> Department of Computer Science and Engineering, University of California, </institution> <address> San Diego, </address> <month> July </month> <year> 1998. </year>
Reference-contexts: They speculatively execute the load and its dependent instructions, and they also re-execute the load in its original location. They then check the value of the speculative load with the correctly loaded value. If they are different a recovery sequence must be executed. Value profiling and memory disambiguation profiling <ref> [26] </ref> can be used to identify the loads that will not have conflicting addresses with stores, and loads that have invariant value behavior. These loads will benefit the most from this type of load speculation.
Reference: [27] <author> S.E. Richardson. </author> <title> Exploiting trivial and redundant computation. </title> <booktitle> In Proceedings of the Eleventh Symposium on Computer Arithmetic, </booktitle> <year> 1993. </year>
Reference-contexts: Dean et al. [10, 18] extend the approach of customization by specializing only those cases where the highest benefit can be achieved. Selective specialization uses a run-time profile to determine exactly where customization would be most beneficial. Richardson <ref> [27] </ref> studied the potential performance gain due to replacing a complex instruction with trivial operands, with a trivial instruction. He profiled the operands of arithmetic operations looking for long latency calculations, with semi-invariant and optimizable inputs. These long latency calculations could then be specialized based on the optimizable inputs.
Reference: [28] <author> Y. Sazeides and James E. Smith. </author> <title> The predictability of data values. </title> <booktitle> In 30th International Symposium on Microarchi-tecture, </booktitle> <pages> pages 248258, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: The PC of the executing instruction is used to hash into that table to retrieve the last value. Several additional value predictor models have been proposed <ref> [16, 28, 30] </ref>. These include stride prediction, context prediction, and several hybrid approaches. Gabbay et al. [16] studied the applicability of program profiling to aid value prediction. His motivation for using profiling information was to classify the instructions tendency to be value predictable.
Reference: [29] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation, pages 196205. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: We compiled the SPEC benchmark suite under OSF/1 V4.0 operating system using full compiler optimization (-O4 -ifo). Table 1 shows the two data sets we used in gathering results for each program, and the number of instructions executed in millions. We used ATOM <ref> [29] </ref> to instrument the programs and gather the value profiles. The ATOM instrumentation tool has an interface that allows the elements of the program executable, such as instructions, basic blocks, and procedures, to be queried and manipulated.
Reference: [30] <author> K. Wang and M. Franklin. </author> <title> Highly accurate data value prediction using hybrid predictors. </title> <booktitle> In 30th International Symposium on Microarchitecture, </booktitle> <pages> pages 281290, </pages> <month> December </month> <year> 1997. </year> <month> 34 </month>
Reference-contexts: The PC of the executing instruction is used to hash into that table to retrieve the last value. Several additional value predictor models have been proposed <ref> [16, 28, 30] </ref>. These include stride prediction, context prediction, and several hybrid approaches. Gabbay et al. [16] studied the applicability of program profiling to aid value prediction. His motivation for using profiling information was to classify the instructions tendency to be value predictable.
References-found: 30

