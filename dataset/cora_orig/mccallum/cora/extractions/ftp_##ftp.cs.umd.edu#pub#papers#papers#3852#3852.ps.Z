URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3852/3852.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: An Approach to Improving Existing Measurement Frameworks in Software Development Organizations  
Author: Manoel Gomes de Mendon~ca Neto, 
Degree: Doctor of Philosophy, 1997 Dissertation directed by: Professor Victor R. Basili  
Affiliation: Department of Computer Science  
Note: Abstract Title of Dissertation:  
Abstract: Measurement is a key mechanism to characterize, evaluate, and improve software development, management, and maintenance processes. Nowadays, software organizations use metrics for very different purposes. Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Improving data collection and better using the existing data are important problems for software organizations. This dissertation proposes an approach for improving measurement and data use when a large number of diverse metrics are already being collected by a software organization. The approach combines two methods. One looks at an organization's measurement framework in a top-down fashion and the other looks at it in a bottom-up fashion. The top-down method, based on the Goal-Question-Metric (GQM) Paradigm, is used to identify the measurement goals of data users and map them to the met-rics being used by the organization. This allows the measurement practitioners to: (1) identify which metrics are and are not useful to the organization; and (2) check if the goals of data user groups can be satisfied by the data that is being collected by the organization. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Y. Akao, </author> <title> editor. Quality Function Deployment: Integrating Customer Requirements Into Product Design. </title> <publisher> Productivity Press, </publisher> <address> Cambridge MA, </address> <year> 1987. </year>
Reference-contexts: Req. N.1.2 2 x Table 2.2: QFD Matrix 2.2.1 Quality Function Deployment The Quality Function Deployment (QFD) <ref> [1, 76] </ref> is a technique that evolved from the TQM principle of deriving measures from the customer point of view. In QFD, quality requirements are established for each product from the customer point of view. Those requirements are then mapped to metrics to be used to satisfy those customer needs.
Reference: [2] <author> A. J. Albrech and J. E. Gaffney. </author> <title> Software function, source lines of code, and development effort prediction: a software science validation. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 9(6) </volume> <pages> 639-647, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound [43, 50, 83, 114]; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure <ref> [2, 31, 102, 108, 111] </ref>. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [3] <author> L. J. Arthur. </author> <title> Quantum improvements in software system quality. </title> <journal> Communications of the ACM, </journal> <volume> 40(6) </volume> <pages> 47-52, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: It is practically impossible to remove all faults 10 from a software product. Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique [33, 34]. The success histories of TQM in manufacturing industries could not be easily transferred to the software industries <ref> [3, 57] </ref>, not even in Japan [68, 70]. 2.1.2 The Lean Enterprise Management The Lean Enterprise Management (LEM) goal is to build a product using the minimal set of activities and materials needed, eliminating non essential steps and costs. LEM has been used to improve factory output. <p> 1 ); P roc (P 2 ); : : : ; P roc (P N ) ! P rod (X 1 ); P rod (X 2 ); : : : ; P rod (X N ) The PDCA idea of creating process improvement cycles has been adapted to software organizations <ref> [3, 45, 68] </ref>. However, the process improvement cycle is more complex in software industries. Each software product is unique and requires its own process.
Reference: [4] <author> J. Barnard and A. Price. </author> <title> Managing code inspection information. </title> <journal> IEEE Software, </journal> <volume> 11(2) </volume> <pages> 59-69, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Goals are refined in a operational, tractable way, into a set of quantifiable questions. Questions in turn imply a specific set of metrics and data for collection. This paradigm has been used successfully in several organizations (e.g., NASA [15], Motorola [40], HP [58], AT&T <ref> [4] </ref>). following template | defined by Basili and Rombach [17] | is used to define measurement goals: Analyze `object of study' in order to `purpose' with respect to `focus' from the point of view of `point of view'. (2.4) Each of the underlined words above represents a facet, that must be <p> In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways <ref> [4, 8, 15, 46, 58] </ref>. All those instantiations aim to define measurement from scratch. This dissertation will use its own instantiation of the GQM Paradigm. Instead of being tailored to define new MFs from scratch, our "version" is tailored to improve existing MFs. better understand the data user needs.
Reference: [5] <author> K. M. Bartol and D. C. Martin. </author> <title> Management, chapter 7. McGraw Hill Series in Management. </title> <publisher> McGraw Hill, </publisher> <year> 1991. </year>
Reference-contexts: This dissertation will call them general goals to differentiate them from our very specific definition of a `goal'. From the business management point of view, one can classify general goals into two large domains <ref> [5] </ref>: `strategic goals' and `organizational goals'. * Strategic goals are general goals affecting the nature the business in which a firm engages (e.g. a database software company can have as a strategic goal to enter the workstation DBMS market). * Organizational goals are general goals affecting the way that parts of
Reference: [6] <author> V. R. Basili. </author> <title> The Experience Factory: </title> <booktitle> Can it make a 5 ? In Proceedings of the Seventeenth Annual Software Engineering Workshop, number SEL-92-004 in Software Engineering Laboratory Series, </booktitle> <address> Greenbelt MD, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Unlike the CMM, the QIP does not assume that process improvement is dependent on the maturity of the organization [106]. The QIP starts with a CMM level 5 style of organization, even though it does not have level 5 capability yet <ref> [6] </ref>. The organization is driven by the understanding of its business, products and process problems [14].
Reference: [7] <author> V. R. Basili, G. Caldiera, and H. D. Rombach. </author> <title> The Experience Factory. </title> <booktitle> In Encyclopedia of Software Engineering, </booktitle> <pages> pages 469-476. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year>
Reference-contexts: The difference is that each software product is unique. In software development, one has to learn from one process about another, the quantitative models are less rigorous and more abstract, and the development process has to be tailored to each new product that is developed <ref> [7] </ref>. QIP basic idea is to tailor a process suited to the project needs based of the goals stated for this project.
Reference: [8] <author> V. R. Basili, M. K. Daskalantonakis, and R. H. Yacobellis. </author> <title> Technology transfer at Motorola. </title> <journal> IEEE Software, </journal> <volume> 11(2) </volume> <pages> 70-76, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways <ref> [4, 8, 15, 46, 58] </ref>. All those instantiations aim to define measurement from scratch. This dissertation will use its own instantiation of the GQM Paradigm. Instead of being tailored to define new MFs from scratch, our "version" is tailored to improve existing MFs. better understand the data user needs.
Reference: [9] <author> V. R. Basili and R. W. Reiter Jr. </author> <title> Evaluating automatable measures of software development. </title> <booktitle> In Workshop on Quantitative Software Models, </booktitle> <address> Ki-amesha NY, </address> <month> October </month> <year> 1979. </year> <note> IEEE. </note>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures <ref> [9, 16, 19, 72, 9 100] </ref>; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing [73, 100]. There are few works on the validation of MFs' completeness, leanness, and consistency.
Reference: [10] <author> Victor R. Basili. </author> <title> Models and Metrics for Software Management and Engineering. </title> <publisher> IEEE Tutorial. IEEE Computer Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1980. </year>
Reference-contexts: Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models <ref> [10, 38, 58, 51, 89] </ref>. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs.
Reference: [11] <author> Victor R. Basili. </author> <title> Can we measure software technology: Lessons learned from 8 years trying. </title> <booktitle> In Proc. 10th Annual Software Engineering Workshop, </booktitle> <address> Greenbelt MD, </address> <year> 1985. </year> <month> NASA/GSFC. </month>
Reference-contexts: The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory <ref> [11, 12, 15, 17] </ref>. In its current form the QIP has six essential phases: 1. Characterize the environment this involves understanding a software project and its context qualitatively and quantitatively so that the correct decisions can be made. 2.
Reference: [12] <author> Victor R. Basili. </author> <title> Quantitative evaluation of software engineering methodology. </title> <booktitle> In Proc. of the First Pan Pacific Computer Conference, </booktitle> <address> Melbourne Australia, </address> <month> September </month> <year> 1985. </year>
Reference-contexts: The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory <ref> [11, 12, 15, 17] </ref>. In its current form the QIP has six essential phases: 1. Characterize the environment this involves understanding a software project and its context qualitatively and quantitatively so that the correct decisions can be made. 2.
Reference: [13] <author> Victor R. Basili. </author> <title> Applying the Goal/Question/Metric paradigm in the Experience Factory. </title> <booktitle> In 10th Annual CSR Workshop, </booktitle> <month> October </month> <year> 1993. </year>
Reference-contexts: Both were conceived to measure things from the point of view of customers and users. To the GQM Paradigm, the measurement point of view, object of study, and purpose are input variables. In fact, SQM and QFD can be considered subsets of the GQM Paradigm <ref> [13] </ref>. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways [4, 8, 15, 46, 58]. All those instantiations aim to define measurement from scratch.
Reference: [14] <author> Victor R. Basili. </author> <title> The Experience Factory and its relationship to other quality approaches. </title> <booktitle> Advances in Computers, </booktitle> <volume> 41(1) </volume> <pages> 65-82, </pages> <year> 1995. </year>
Reference-contexts: P rocess (P V ) ! P roduct (V ) (2.1) The ideas of LEM are very useful in software development as software organizations have to learn from one process about another, and the development process has to be tailored to each new product that is developed <ref> [14] </ref>. 2.1.3 The Plan-Do-Check-Act Approach The Plan-Do-Check-Act (PDCA) is a quality improvement process based upon a feedback cycle for optimizing single process production lines. Its based on work by W. A. Shewhart [103] and was made popular and applied effectively to improve Japanese manufacturing after World War II by W. <p> The Quality Improvement Paradigm (QIP) is a long-term, quality-oriented, meta-lifecycle model for software organizations <ref> [14, 18] </ref>. The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory [11, 12, 15, 17]. <p> Inter-project feedback is based on phase 5. It is done postmortem by packaging experiences into models and other forms of structured knowledge that can be reused in the future. The QIP incorporates ideas from several quality improvement approaches used in the manufacturing industries <ref> [14] </ref>: * Its evolutionary nature, based on feedback loops, is similar to the Plan-Do Check-Act Paradigm (PDCA) [44, 103]. * Its goals, feedback mechanisms, and use of measurement allow us to involve everyone in the job of quality assurance. <p> What basically differentiates the QIP from the LEM, TQM and PDCA approaches is that the QIP is tailored to software development, while the others were essentially used to improve the quality of an assembly-line-like production environment <ref> [14] </ref>. The difference is that each software product is unique. In software development, one has to learn from one process about another, the quantitative models are less rigorous and more abstract, and the development process has to be tailored to each new product that is developed [7]. <p> The QIP starts with a CMM level 5 style of organization, even though it does not have level 5 capability yet [6]. The organization is driven by the understanding of its business, products and process problems <ref> [14] </ref>. It learns from its own business, not from an external generic process model. 2.1.6 Mapping our Improvement Approach to QIP The QIP can been seen as a framework for applying the scientific method to software organizations. Our experimental work can be mapped to the QIP.
Reference: [15] <author> Victor R. Basili and S. Green. </author> <title> Software process evolution at the SEL. </title> <journal> IEEE Software, </journal> <volume> 11(4) </volume> <pages> 58-66, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory <ref> [11, 12, 15, 17] </ref>. In its current form the QIP has six essential phases: 1. Characterize the environment this involves understanding a software project and its context qualitatively and quantitatively so that the correct decisions can be made. 2. <p> Goals are refined in a operational, tractable way, into a set of quantifiable questions. Questions in turn imply a specific set of metrics and data for collection. This paradigm has been used successfully in several organizations (e.g., NASA <ref> [15] </ref>, Motorola [40], HP [58], AT&T [4]). following template | defined by Basili and Rombach [17] | is used to define measurement goals: Analyze `object of study' in order to `purpose' with respect to `focus' from the point of view of `point of view'. (2.4) Each of the underlined words above <p> In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways <ref> [4, 8, 15, 46, 58] </ref>. All those instantiations aim to define measurement from scratch. This dissertation will use its own instantiation of the GQM Paradigm. Instead of being tailored to define new MFs from scratch, our "version" is tailored to improve existing MFs. better understand the data user needs.
Reference: [16] <author> Victor R. Basili and D. H. Hutchens. </author> <title> An empirical study of a syntactic complexity family. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 9(6) </volume> <pages> 664-672, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures <ref> [9, 16, 19, 72, 9 100] </ref>; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing [73, 100]. There are few works on the validation of MFs' completeness, leanness, and consistency.
Reference: [17] <author> Victor R. Basili and H. D. Rombach. </author> <title> The TAME project: Towards improvement-oriented software environments. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 14(6) </volume> <pages> 758-773, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models [10, 38, 58, 51, 89]. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement <ref> [17, 40, 88] </ref>. However, very little attention has been given to the problem of improving existing measurement programs. This dissertation proposes an approach for improving measurement and data use when a large number of diverse metrics are already being collected by a software organization. The approach combines two methods. <p> The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory <ref> [11, 12, 15, 17] </ref>. In its current form the QIP has six essential phases: 1. Characterize the environment this involves understanding a software project and its context qualitatively and quantitatively so that the correct decisions can be made. 2. <p> In this way, the SQM user just selects the factors and criteria of interest to define which metrics will be used to assess the delivered product. Figure 2.2 shows an example of a SQM structure adapted from [28]. 2.2.3 The Goal-Question-Metric Paradigm The Goal-Question-Metric Paradigm was proposed by Basili <ref> [20, 17] </ref> as a means of measuring software in a purposeful way. The GQM paradigm first step is to 17 define measurement goals tailored to the specific needs of an organization. Goals are refined in a operational, tractable way, into a set of quantifiable questions. <p> Questions in turn imply a specific set of metrics and data for collection. This paradigm has been used successfully in several organizations (e.g., NASA [15], Motorola [40], HP [58], AT&T [4]). following template | defined by Basili and Rombach <ref> [17] </ref> | is used to define measurement goals: Analyze `object of study' in order to `purpose' with respect to `focus' from the point of view of `point of view'. (2.4) Each of the underlined words above represents a facet, that must be considered in measurement planning. <p> To the GQM Paradigm, the measurement point of view, object of study, and purpose are input variables. In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm <ref> [20, 17] </ref> to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways [4, 8, 15, 46, 58]. All those instantiations aim to define measurement from scratch.
Reference: [18] <author> Victor R. Basili and R. W. Selby. </author> <title> Paradigms for experimentation and empirical studies in software engineering. Reliability Engineering and System Safety, </title> <booktitle> 32 </booktitle> <pages> 171-191, </pages> <year> 1991. </year>
Reference-contexts: However, the process improvement cycle is more complex in software industries. Each software product is unique and requires its own process. In software, each improvement cycle has to build a "new" process tailored from previous software development experiences <ref> [18] </ref>. 2.1.4 The SEI Capability Maturity Model The Capability Maturity Model (CMM) [63, 90] is a quality improvement approach that was specifically tailored to Software Development. CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby [39]. <p> The Quality Improvement Paradigm (QIP) is a long-term, quality-oriented, meta-lifecycle model for software organizations <ref> [14, 18] </ref>. The QIP promotes understanding, assessing, and packaging of software development experiences as the means to improve software quality. QIP Paradigm: the intra-project monitor-control cycle, and the inter-project (corporate) learn-improve cycle. 13 The QIP evolved from the lessons learned in the NASA Software Engineering Laboratory [11, 12, 15, 17].
Reference: [19] <author> Victor R. Basili, R. W. Selby, and T. Y. Phillips. </author> <title> Metric analysis and validation across FORTRAN projects. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 9(6) </volume> <pages> 652-663, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures <ref> [9, 16, 19, 72, 9 100] </ref>; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing [73, 100]. There are few works on the validation of MFs' completeness, leanness, and consistency.
Reference: [20] <author> Victor R. Basili and D. M. Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 10(6) </volume> <pages> 728-738, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: In this way, the SQM user just selects the factors and criteria of interest to define which metrics will be used to assess the delivered product. Figure 2.2 shows an example of a SQM structure adapted from [28]. 2.2.3 The Goal-Question-Metric Paradigm The Goal-Question-Metric Paradigm was proposed by Basili <ref> [20, 17] </ref> as a means of measuring software in a purposeful way. The GQM paradigm first step is to 17 define measurement goals tailored to the specific needs of an organization. Goals are refined in a operational, tractable way, into a set of quantifiable questions. <p> To the GQM Paradigm, the measurement point of view, object of study, and purpose are input variables. In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm <ref> [20, 17] </ref> to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways [4, 8, 15, 46, 58]. All those instantiations aim to define measurement from scratch.
Reference: [21] <author> I. S. Bhandari. </author> <title> Attribute focusing: Machine-assisted knowledge discovery applied to software production process control. </title> <journal> Knowledge Acquisition Journal, </journal> <volume> 6(3) </volume> <pages> 271-294, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets [29, 32, 92, 101, 104, 107]. Our bottom-up analyses use Attribute Focusing <ref> [21] </ref> - 44 a data mining technique to extract unexpected and useful information directly from the MF database. 3.3.1 The AF-based Method The aim of the AF-based (bottom-up) method is to establish procedures to effectively apply the AF technique maximizing knowledge discovery and minimizing discovery cost.
Reference: [22] <author> I. S. Bhandari, E. Colet, J. Parker, Z. Pines, R. Pratap, and K. Ramanujam. </author> <title> Advanced scout: Data mining and knowledge discovery in the NBA data. Data Mining and Knowledge Discovery, </title> <booktitle> 1(1) </booktitle> <pages> 121-125, </pages> <month> January </month> <year> 1997. </year> <month> 173 </month>
Reference-contexts: Attribute Focusing (AF) has been used in several different applications including software process measurement [24, 23, 26], customer satisfaction [25], and sports <ref> [22] </ref> data analyses. The AF technique searches an attribute-value (measurement) database for interesting facts. An interesting fact is characterized by the deviation of attribute values from some expected distribution or by an unexpected correlation between values of a set of attributes. <p> The convergence to optimal length associations is therefore essentially a convergence towards the most informative and parsimonious associations. A deeper discussion of these concepts can be found in <ref> [22, 37] </ref>. Although the strength and optimal length of an association is a statistical measure of interestingness, it may not be a complete measure of practical utility. A strong association is useful only if it is unexpected or previously unknown.
Reference: [23] <author> I. S. Bhandari, M. J. Halliday, J. Chaar, R. Chillarege, K. Jones, J. S. Atkinson, C. Lepori-Costello, P. Y. Jasper, E.D. Tarver, C. C. Lewis, and M. Yonezawa. </author> <title> In-process improvement through defect data interpretation. </title> <journal> IBM Systems Journal, </journal> <volume> 33(1), </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: Attribute Focusing (AF) has been used in several different applications including software process measurement <ref> [24, 23, 26] </ref>, customer satisfaction [25], and sports [22] data analyses. The AF technique searches an attribute-value (measurement) database for interesting facts. An interesting fact is characterized by the deviation of attribute values from some expected distribution or by an unexpected correlation between values of a set of attributes.
Reference: [24] <author> I. S. Bhandari, M. J. Halliday, E. Tarver, D. Brown, J. Chaar, and R. Chillarege. </author> <title> A case study of software process improvement during development. </title> <journal> IEEE Transactions on Software Eng., </journal> 19(12) 1157-1170, De-cember 1993. 
Reference-contexts: Attribute Focusing (AF) has been used in several different applications including software process measurement <ref> [24, 23, 26] </ref>, customer satisfaction [25], and sports [22] data analyses. The AF technique searches an attribute-value (measurement) database for interesting facts. An interesting fact is characterized by the deviation of attribute values from some expected distribution or by an unexpected correlation between values of a set of attributes.
Reference: [25] <author> I. S. Bhandari, M. G. Mendon~ca, and J. Dawson. </author> <title> On the use of machine-assisted knowledge discovery to analyze and reengineer measurement frameworks. </title> <booktitle> In Proc. of CASCON'95, Toronto ON, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Attribute Focusing (AF) has been used in several different applications including software process measurement [24, 23, 26], customer satisfaction <ref> [25] </ref>, and sports [22] data analyses. The AF technique searches an attribute-value (measurement) database for interesting facts. An interesting fact is characterized by the deviation of attribute values from some expected distribution or by an unexpected correlation between values of a set of attributes. <p> The diagrams are sorted by interestingness level | a numeric value calculated to quantify how interesting each diagram might be to an expert. The ordered diagrams are presented to the experts. Knowledge discovery takes place when the experts address the questions raised by the diagrams. 29 ucts <ref> [25] </ref>. Let us call it "Product Class X." This particular diagram has two attributes: "Overall Satisfaction" and "Customer Involvement in the Decision to Purchase the Product." The satisfaction level by customer involvement in purchase is shown by bar patterns in the diagram.
Reference: [26] <author> I. S. Bhandari, B. Ray, M. Y. Wong, D. Choi, A. Watanabe, R. Chillarege, M. Halliday, A. Dooley, and J. Chaar. </author> <title> An inference structure for process feedback: Technique and implementation. </title> <journal> Software Quality Journal, </journal> <volume> 3(3) </volume> <pages> 167-189, </pages> <year> 1994. </year>
Reference-contexts: Attribute Focusing (AF) has been used in several different applications including software process measurement <ref> [24, 23, 26] </ref>, customer satisfaction [25], and sports [22] data analyses. The AF technique searches an attribute-value (measurement) database for interesting facts. An interesting fact is characterized by the deviation of attribute values from some expected distribution or by an unexpected correlation between values of a set of attributes.
Reference: [27] <author> B. W. </author> <title> Boehm. </title> <journal> Software engineering economics. IEEE Transactions on Software Eng., </journal> <volume> 10(1) </volume> <pages> 4-21, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy <ref> [27, 32, 71, 109] </ref>. <p> These metrics uses a function to determine an attribute value from other attribute values. They are commonly used in software engineering resource and prediction models ([10] part II and [38] chapter 6). For example, Boehm's Cost Constructive Model <ref> [27] </ref> (a well known effort prediction model) uses the following relation to measure the attribute "Effort:" E = 2:4 fi LOC 1:05 In this case, the measurement model is the "E" function and the value domain is the real numbers in ratio scale. 2.3.2 Expressing Attributes As defined in Section 1.6,
Reference: [28] <author> B. W. Boehm, J. R. Brown, and M. Lipow. </author> <title> Quantitative evaluation of software quality. </title> <booktitle> In 2nd International Conference on Software Engineering, </booktitle> <pages> pages 592-605, </pages> <address> San Francisco CA, </address> <month> October </month> <year> 1976. </year> <note> IEEE & ACM. </note>
Reference-contexts: The right side of the matrix map each of those requirements to the measurable attributes that will be used to evaluate them. 2.2.2 Software Quality Metrics Software Quality Metrics (SQM) was developed to allow the customer to assess the product being developed by a contractor <ref> [28, 81] </ref>. In SQM, a set of quality factors is defined for the final product. Those factors are refined into a set of criteria (attributes), which are mapped to a set of pre-defined metrics. <p> In this way, the SQM user just selects the factors and criteria of interest to define which metrics will be used to assess the delivered product. Figure 2.2 shows an example of a SQM structure adapted from <ref> [28] </ref>. 2.2.3 The Goal-Question-Metric Paradigm The Goal-Question-Metric Paradigm was proposed by Basili [20, 17] as a means of measuring software in a purposeful way. The GQM paradigm first step is to 17 define measurement goals tailored to the specific needs of an organization.
Reference: [29] <author> Lionel C. Briand, Victor R. Basili, and Christopher Hetmanski. </author> <title> Developing interpretable models with optimized set reduction for identifying high-risk software components. </title> <journal> IEEE Transactions on Software Eng., </journal> 19(11) 1028-1044, November 1993. 
Reference-contexts: Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees [92, 101, 104, 107] and Optimized Set Reduction <ref> [29, 32] </ref> have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [30] <author> Lionel C. Briand, Victor R. Basili, and Sandro Morasca. </author> <title> Goal-driven definition of product metrics based on properties. </title> <type> Technical Report CS-TR 3346 / UMIACS-TR 94-106, </type> <institution> University of Maryland, College Park MD, </institution> <year> 1994. </year>
Reference-contexts: One natural extension to the current AF-based method is to use assumptions to help detect interesting data associations. An assumption is a statement believed to be true about the relationship between attributes of interest <ref> [30] </ref>. Assumptions can be used as one of the inputs to the functions that calculates the interestingness of data associations. The more an association deviates from an assumption the more interesting it is.
Reference: [31] <author> Lionel C. Briand, Victor R. Basili, and Sandro Morasca. </author> <title> Property-based software engineering measurement. </title> <type> Technical Report CS-TR 3368 / UMIACS-TR 94-75, </type> <institution> University of Maryland, College Park MD, </institution> <year> 1994. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound [43, 50, 83, 114]; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure <ref> [2, 31, 102, 108, 111] </ref>. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. <p> In such cases, a formal description should be used to describe this attribute. This dissertation endorses the use of property-based approaches <ref> [31] </ref> to formally define an attribute. The property-based attribute definition works by stating as axioms the properties that the metrics used to measure the attribute have to satisfy.
Reference: [32] <author> Lionel C. Briand, Victor R. Basili, and W. M. Thomas. </author> <title> A pattern recognition approach for software engineering data analysis. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 18(11) </volume> <pages> 931-942, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy <ref> [27, 32, 71, 109] </ref>. <p> Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees [92, 101, 104, 107] and Optimized Set Reduction <ref> [29, 32] </ref> have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [33] <author> F. P. Brooks. </author> <title> The Mythical Man-Month: </title> <booktitle> Essays on Software Engineering. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading MA, 2nd edition, </address> <month> July </month> <year> 1978. </year> <month> 174 </month>
Reference-contexts: It is difficult to define and evaluate the quality of software products. It is difficult capture software customer needs. It is practically impossible to remove all faults 10 from a software product. Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique <ref> [33, 34] </ref>. <p> The cost and quality of software products are associated with its development process as opposed as to its production 1 process [93]. Software is an abstract and complex product and software development is a human intensive process <ref> [34, 33] </ref>. One of the key ideas behind our approach is that, in software organizations, measurement should be defined in top-down goal-oriented fashion.
Reference: [34] <author> F. P. Brooks. </author> <title> No silver bullet: Essence and accidents of software engineer-ing. </title> <journal> IEEE Computer, </journal> <volume> 20(10):19, </volume> <year> 1987. </year>
Reference-contexts: It is difficult to define and evaluate the quality of software products. It is difficult capture software customer needs. It is practically impossible to remove all faults 10 from a software product. Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique <ref> [33, 34] </ref>. <p> The cost and quality of software products are associated with its development process as opposed as to its production 1 process [93]. Software is an abstract and complex product and software development is a human intensive process <ref> [34, 33] </ref>. One of the key ideas behind our approach is that, in software organizations, measurement should be defined in top-down goal-oriented fashion.
Reference: [35] <author> B. G. Buchanan and T. M. Mitchell. </author> <title> Model-directed learning of production rules. In Pattern Directed Inference Systems. </title> <publisher> Academic Press, </publisher> <year> 1978. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms <ref> [35, 41, 85] </ref>, neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [36] <author> J. G. Carbonell, R. S. Michalski, and T. M Mitchell. </author> <title> An overview of machine learning. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M Mitchell, editors, </editor> <booktitle> Machine Learning, an Artificial Intelligence Approach, </booktitle> <volume> volume 1, </volume> <pages> pages 3-24. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo CA, </address> <year> 1983. </year>
Reference-contexts: It codes these facts as "coded observations" of the environment. These coded observations are fed into a machine learning mechanism to produce a model of the environment. The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment <ref> [36, 65] </ref>. A broad range of machine learning approaches can be fit into the above framework.
Reference: [37] <author> E. Colet and I. S. Bhandari. </author> <title> Statistical issues in the application of data mining to the NBA using attribute focusing. </title> <booktitle> In Proceedings of the 1997 Joint Statistical Meetings, </booktitle> <address> Anahein CA, </address> <month> August </month> <year> 1997. </year> <journal> American Statistical Association. </journal>
Reference-contexts: The convergence to optimal length associations is therefore essentially a convergence towards the most informative and parsimonious associations. A deeper discussion of these concepts can be found in <ref> [22, 37] </ref>. Although the strength and optimal length of an association is a statistical measure of interestingness, it may not be a complete measure of practical utility. A strong association is useful only if it is unexpected or previously unknown.
Reference: [38] <author> S. D. Conte, H. E. Dunsmore, and V. Y. Shen. </author> <title> Software Engineering Metrics and Models. </title> <publisher> The Benjamin/Cummings Publishing Company Inc., </publisher> <address> Menlo Park CA, </address> <year> 1986. </year>
Reference-contexts: Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models <ref> [10, 38, 58, 51, 89] </ref>. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs. <p> In the other end of the spectrum, 21 there are the indirect metrics. These metrics uses a function to determine an attribute value from other attribute values. They are commonly used in software engineering resource and prediction models ([10] part II and <ref> [38] </ref> chapter 6).
Reference: [39] <author> P. B. Crosby. </author> <title> Quality is Free: the Art of Making Quality Certain. </title> <address> New American Library, NY, </address> <year> 1980. </year>
Reference-contexts: CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby <ref> [39] </ref>. The idea of using a software maturity model was developed by Radice while at IBM [95] and was made popular by Humprey at the Software Engineering Institute (SEI) [66]. CMM uses a five-level process maturity model to improve quality (Table 2.1).
Reference: [40] <author> M. K. Daskalantonakis. </author> <title> A practical view of software measurement and implementation experiences within Motorola. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 18(11) </volume> <pages> 998-1010, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models [10, 38, 58, 51, 89]. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement <ref> [17, 40, 88] </ref>. However, very little attention has been given to the problem of improving existing measurement programs. This dissertation proposes an approach for improving measurement and data use when a large number of diverse metrics are already being collected by a software organization. The approach combines two methods. <p> There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs <ref> [40, 60, 91, 96, 110] </ref>. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs. <p> Goals are refined in a operational, tractable way, into a set of quantifiable questions. Questions in turn imply a specific set of metrics and data for collection. This paradigm has been used successfully in several organizations (e.g., NASA [15], Motorola <ref> [40] </ref>, HP [58], AT&T [4]). following template | defined by Basili and Rombach [17] | is used to define measurement goals: Analyze `object of study' in order to `purpose' with respect to `focus' from the point of view of `point of view'. (2.4) Each of the underlined words above represents a
Reference: [41] <author> G. DeJong and R. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms <ref> [35, 41, 85] </ref>, neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [42] <author> Tom DeMarco. </author> <title> Why Does Software Cost So Much ?, chapter 2: </title> <booktitle> Mad About Measurement, </booktitle> <pages> pages 11-25. </pages> <publisher> Dorset Housing Publishing, </publisher> <year> 1995. </year>
Reference-contexts: A MF is complete when it measures everything that its users need to achieve their goals. A MF is lean when it measures what is needed and nothing else (metrics cost money to collect <ref> [42] </ref>). A MF is consistent when its metrics are consistent with the user goals. This means that: (1) the metrics scale and range of values are suitable for the user needs; and (2) the metrics can be applied when and where they are needed by the users.
Reference: [43] <author> R. A. DeMillo and R. J. Lipton. </author> <title> Software project forecasting. </title> <editor> In A. J. Perlis, F. G. Sayard, and M. Shaw, editors, </editor> <title> Software Metrics. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1981. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound <ref> [43, 50, 83, 114] </ref>; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure [2, 31, 102, 108, 111]. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [44] <author> W. E. </author> <title> Deming. Out of The Crisis. MIT Center for Advanced Engineering Study. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference-contexts: The approaches to achieve TQM vary greatly in practice. In general, however, they seek to achieve total quality of a product by involving all members of the production process in the improvement effort. TQM was developed in Japan based on the ideas of W. E. Deming <ref> [44] </ref> and J.M. Juran [69]. The principles of TQM were successfully applied in industries for mass production, such as automobile and consumer electronics industries. In those industries, the concept of total customer satisfaction was translated in terms of producing parts and products with zero defect. <p> Its based on work by W. A. Shewhart [103] and was made popular and applied effectively to improve Japanese manufacturing after World War II by W. E. Demming <ref> [44] </ref>. The approach is defined as four basic steps: Plan: Develop a plan for improving the existing production process. Set up quality targets (using measurable criteria) and methods to achieve the targets. Do: Carry out the plan complying with development standards and quality guidelines. <p> The QIP incorporates ideas from several quality improvement approaches used in the manufacturing industries [14]: * Its evolutionary nature, based on feedback loops, is similar to the Plan-Do Check-Act Paradigm (PDCA) <ref> [44, 103] </ref>. * Its goals, feedback mechanisms, and use of measurement allow us to involve everyone in the job of quality assurance.
Reference: [45] <author> R. Dion. </author> <title> Process improvement and the corporate balance sheet. </title> <journal> IEEE Software, </journal> <volume> 10(4) </volume> <pages> 28-35, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: 1 ); P roc (P 2 ); : : : ; P roc (P N ) ! P rod (X 1 ); P rod (X 2 ); : : : ; P rod (X N ) The PDCA idea of creating process improvement cycles has been adapted to software organizations <ref> [3, 45, 68] </ref>. However, the process improvement cycle is more complex in software industries. Each software product is unique and requires its own process.
Reference: [46] <author> K. El Eman, N. Moukheiber, and N. H. Madhavji. </author> <title> An empirical evaluation of the G/Q/M method. </title> <booktitle> In Proceedings of CASCON 93, </booktitle> <pages> pages 265-289, </pages> <institution> Toronto ON, </institution> <month> November </month> <year> 1993. </year> <institution> IBM Canada Ltd. </institution> <month> 175 </month>
Reference-contexts: In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways <ref> [4, 8, 15, 46, 58] </ref>. All those instantiations aim to define measurement from scratch. This dissertation will use its own instantiation of the GQM Paradigm. Instead of being tailored to define new MFs from scratch, our "version" is tailored to improve existing MFs. better understand the data user needs.
Reference: [47] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> The KDD process for extracting useful knowledge from volumes of data. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 27-34, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: Formally, data mining is defined as the process of inducing previously unknown, and 28 potentially useful, information from databases [54]. Figure 2.6 shows a data mining framework (adapted from <ref> [47] </ref>). Although the framework for data mining and machine learning may seem similar, there is an important distinction. The database is designed for purposes others than data mining.
Reference: [48] <author> U. Fayyad and R Uthurusamy. </author> <title> Data mining and knowledge discovery in databases. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 24-26, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: The research area that studies machine learning systems that draw "coded observations" directly from a database is called `data mining' <ref> [48] </ref>. Formally, data mining is defined as the process of inducing previously unknown, and 28 potentially useful, information from databases [54]. Figure 2.6 shows a data mining framework (adapted from [47]). Although the framework for data mining and machine learning may seem similar, there is an important distinction.
Reference: [49] <author> A. V. Feigenbaun. </author> <title> Total Quality Control. </title> <publisher> McGraw Hill, </publisher> <address> NY, 40th anniversary edition, </address> <year> 1991. </year>
Reference-contexts: examines some of the organizational approaches used to improve quality in various types of business. 2.1.1 Total Quality Management The goal of Total Quality Control (TQM) is to generate institutional commitment to success through customer satisfaction | the term was coined to describe the Japanese management style to quality improvement <ref> [49] </ref>. The approaches to achieve TQM vary greatly in practice. In general, however, they seek to achieve total quality of a product by involving all members of the production process in the improvement effort. TQM was developed in Japan based on the ideas of W. E. Deming [44] and J.M. <p> One can use QIP to implement a Total Quality Management (TQM) philosophy in a software organiza tion <ref> [49] </ref>. * Its approach to tailoring the development process as an optimum set of available sub-processes is similar to Lean Enterprise Management (LEM) [112]. Both have the idea of meeting the particular goals of a project using the minimum set of essential steps.
Reference: [50] <author> N. E. Fenton and A. Melton. </author> <title> Deriving structurally based software measures. </title> <journal> J. Syst. Software, </journal> <volume> 12(3) </volume> <pages> 177-187, </pages> <year> 1990. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound <ref> [43, 50, 83, 114] </ref>; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure [2, 31, 102, 108, 111]. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [51] <author> Norman E. Fenton. </author> <title> Software Metrics: A Rigorous Approach. </title> <publisher> Chapman Hall, </publisher> <year> 1991. </year>
Reference-contexts: Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models <ref> [10, 38, 58, 51, 89] </ref>. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs. <p> Examples of attributes used in software organizations are: complexity, size, coupling, and cohesion of software code, type and seriousness of a defect, experience and capability of programmers. Although the differentiation between "metrics" and "attributes" was made popular by Fenton in his 1991 software metrics book <ref> [51] </ref>, it was first proposed by Rubey and Hartwick in a seminal 1968 work [98], on their own words: "... attribute is a precise statement of a specific software characteristic. ... a metric was developed for quantitative measurement of each quality attribute.
Reference: [52] <author> Norman E. Fenton. </author> <title> Software measurement: A necessary scientific basis. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 20(3), </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: The terminology adopted here was adapted from the data mining terminology proposed by Klosgen and Zytkow [75] and the software engineering measurement terminology proposed by Fenton <ref> [52] </ref>. During this section (and the rest of this dissertation), boldface font is used when new terms are defined. We define application domain as the real or abstract system a software organization wants to analyze using a MF.
Reference: [53] <author> L. Finkelstein and M. S. Leaning. </author> <title> A review of the fundamental concepts of measurement. </title> <booktitle> Measurement, </booktitle> <volume> 2(1), </volume> <month> January </month> <year> 1984. </year>
Reference-contexts: This definition recognizes the fact that a measure is a mapping from the empirical (real) world to a formal (mathematical) world <ref> [53, 62, 73, 97] </ref>. A metric is characterized by a measurement model, a value domain, and a scale. In order to describe a metric, one has to identify all those components.
Reference: [54] <author> W. J. Frawley, G. Piatetsky-Shapiro, and C. J. Matheus. </author> <title> Knowledge discovery in databases: An overview. </title> <journal> AI Magazine, </journal> <pages> pages 57-70, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: It is usually higher-level information, that is not necessarily true, but it is believed to be true due to the contents of the database. If this information is also interesting and previously unknown, it called discovered knowledge or new knowledge <ref> [54] </ref>. The process of discovering new knowledge is referred to as inductive learning [64]. The automation of inductive learning processes has been researched in an artificial intelligence area called machine learning [99]. A machine learning system does not interact directly with its environment. <p> The research area that studies machine learning systems that draw "coded observations" directly from a database is called `data mining' [48]. Formally, data mining is defined as the process of inducing previously unknown, and 28 potentially useful, information from databases <ref> [54] </ref>. Figure 2.6 shows a data mining framework (adapted from [47]). Although the framework for data mining and machine learning may seem similar, there is an important distinction. The database is designed for purposes others than data mining.
Reference: [55] <author> T. </author> <title> Gilb. </title> <booktitle> Principles of Software Engineering Management. </booktitle> <publisher> Addison Wesley, </publisher> <year> 1987. </year>
Reference-contexts: Software is an abstract and complex product and software development is a human intensive process [34, 33]. One of the key ideas behind our approach is that, in software organizations, measurement should be defined in top-down goal-oriented fashion. Gilb put it better when he said <ref> [55] </ref>: "Projects without clear goals will not achieve their goals clearly." A variety of goal-oriented measurement paradigms have appeared in the literature: the Quality Function Deployment [76] (QFD) ; the Software Quality Metrics [81] (SQM); and the Goal Question Metric (GQM) paradigm are some of them. 1 Software production corresponds to
Reference: [56] <author> C. Glymour, D. Madigan, D. Pregibon, and P. Smyth. </author> <title> Statistical inference and data mining. </title> <journal> Communications of the ACM, </journal> <volume> 39(11) </volume> <pages> 35-41, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: It is geared towards "discovering" information. One should use statistics to further analyze the facts discovered using this method. A good discussion on how statistics should be combined with data mining is found in <ref> [56] </ref>. The AF tool used in the case study was designed to mine nominal and ordinal data. The tool has limitations in exploring interval and ratio data. Numeric-valued attributes have to be mapped into discrete ranges of values before they can be used in an AF-analysis.
Reference: [57] <author> P. Goodman. </author> <title> The practical implementation of process improvement initiatives. </title> <editor> In N. Fenton, R. Whitty, and Y. Iizuka, editors, </editor> <title> Software Quality Assurance and Measurement: A Worldwide Perspective. </title> <publisher> International Thomson Computer Press, </publisher> <year> 1995. </year>
Reference-contexts: It is practically impossible to remove all faults 10 from a software product. Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique [33, 34]. The success histories of TQM in manufacturing industries could not be easily transferred to the software industries <ref> [3, 57] </ref>, not even in Japan [68, 70]. 2.1.2 The Lean Enterprise Management The Lean Enterprise Management (LEM) goal is to build a product using the minimal set of activities and materials needed, eliminating non essential steps and costs. LEM has been used to improve factory output.
Reference: [58] <author> R. B. Grady. </author> <title> Practical Software Metrics for Project Management and Process Improvement, chapter 3. Hewlett-Packard Professional Books, </title> <year> 1992. </year>
Reference-contexts: Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models <ref> [10, 38, 58, 51, 89] </ref>. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs. <p> Goals are refined in a operational, tractable way, into a set of quantifiable questions. Questions in turn imply a specific set of metrics and data for collection. This paradigm has been used successfully in several organizations (e.g., NASA [15], Motorola [40], HP <ref> [58] </ref>, AT&T [4]). following template | defined by Basili and Rombach [17] | is used to define measurement goals: Analyze `object of study' in order to `purpose' with respect to `focus' from the point of view of `point of view'. (2.4) Each of the underlined words above represents a facet, that <p> In fact, SQM and QFD can be considered subsets of the GQM Paradigm [13]. This dissertation adopts the Goal-Question-Metric Paradigm [20, 17] to improve MFs in a top-down fashion. 2.3 Instantiating the GQM Paradigm The GQM is a general paradigm that has been instantiated in several different ways <ref> [4, 8, 15, 46, 58] </ref>. All those instantiations aim to define measurement from scratch. This dissertation will use its own instantiation of the GQM Paradigm. Instead of being tailored to define new MFs from scratch, our "version" is tailored to improve existing MFs. better understand the data user needs.
Reference: [59] <author> R. B. Grady. </author> <title> Practical Software Metrics for Project Management and Process Improvement, chapter 1. Hewlett-Packard Professional Books, </title> <year> 1992. </year>
Reference-contexts: The improved MF and a packaged set of improvement processes is the final result of using this paradigm. 2.2 Looking at Measurement Frameworks in a Top-down Fashion Measuring for software quality and cost improvement is not a simple task <ref> [59] </ref>. The cost and quality of software products are associated with its development process as opposed as to its production 1 process [93]. Software is an abstract and complex product and software development is a human intensive process [34, 33].
Reference: [60] <author> Robert B. Grady. </author> <title> Successfully applying software metrics. </title> <journal> IEEE Computer, </journal> <volume> 27(9) </volume> <pages> 18-25, </pages> <month> September </month> <year> 1994. </year> <month> 176 </month>
Reference-contexts: There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs <ref> [40, 60, 91, 96, 110] </ref>. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs.
Reference: [61] <author> Tracy Hall and Norman Fenton. </author> <title> Implementing effective software metrics programs. </title> <journal> IEEE Software, </journal> <volume> 14(2) </volume> <pages> 55-65, </pages> <month> March </month> <year> 1997. </year>
Reference-contexts: Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models [10, 38, 58, 51, 89]. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations <ref> [61] </ref>, most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs. This dissertation proposes an approach for improving measurement and data use when a large number of diverse metrics are already being collected by a software organization. <p> There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs [40, 60, 91, 96, 110]. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs <ref> [61, 88] </ref>. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs. Little attention has been given to the problem of improving the completeness, leanness, and consistency of existing operational MFs.
Reference: [62] <author> W. Harrison. </author> <title> Software measurement: A decision support approach. </title> <booktitle> In Advances in Computers, </booktitle> <volume> volume 39, </volume> <pages> pages 51-105. </pages> <publisher> Academic Press Inc., </publisher> <year> 1994. </year>
Reference-contexts: This definition recognizes the fact that a measure is a mapping from the empirical (real) world to a formal (mathematical) world <ref> [53, 62, 73, 97] </ref>. A metric is characterized by a measurement model, a value domain, and a scale. In order to describe a metric, one has to identify all those components.
Reference: [63] <author> J. Hersleb, D. Zubrow, D. Goldenson, W. Hayes, and M. Paulk. </author> <title> Quality improvement and the capability maturity model. </title> <journal> Communications of the ACM, </journal> <volume> 40(6) </volume> <pages> 31-40, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Each software product is unique and requires its own process. In software, each improvement cycle has to build a "new" process tailored from previous software development experiences [18]. 2.1.4 The SEI Capability Maturity Model The Capability Maturity Model (CMM) <ref> [63, 90] </ref> is a quality improvement approach that was specifically tailored to Software Development. CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby [39].
Reference: [64] <author> J. H. Holland, K. J. Holyoak, R. E. Nisbett, and P. R. Thagard. </author> <title> Induction: Processes of Inference, Learning and Discovery. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1986. </year>
Reference-contexts: If this information is also interesting and previously unknown, it called discovered knowledge or new knowledge [54]. The process of discovering new knowledge is referred to as inductive learning <ref> [64] </ref>. The automation of inductive learning processes has been researched in an artificial intelligence area called machine learning [99]. A machine learning system does not interact directly with its environment. It uses "coded observations" of this environment to learn about it. Figure 2.5 depicts the machine leaning process. <p> A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms [35, 41, 85], neural network algorithms [79, 86], and genetic algorithms <ref> [64] </ref>, although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment. The environment model is represented in a neural network as patterns of interactions between the simple computational elements.
Reference: [65] <author> M. Holsheimer and A. P. J. Siebes. </author> <title> Data mining: the search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> CWI Department of Algorithms and Architecture, </institution> <address> Amsterdam, The Netherlands, </address> <year> 1994. </year>
Reference-contexts: The bottom-up analysis aims explore a MF database to infer new useful information (knowledge) about the application domain and the MF itself. 2.4.1 Machine Learning Information can be inferred from a database by deduction or induction <ref> [65] </ref>. Deduction infers information that is a logical consequence of the information in the database. This information is always true provided that the database contents 26 are true. Induction infers information by generalization of the information contained in the database. <p> It codes these facts as "coded observations" of the environment. These coded observations are fed into a machine learning mechanism to produce a model of the environment. The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment <ref> [36, 65] </ref>. A broad range of machine learning approaches can be fit into the above framework. <p> The database is designed for purposes others than data mining. The representation of entities and attributes in the database has been chosen to meet the needs of the applications that use it rather than the needs of data mining <ref> [65] </ref>. In our case, the database is designed to meet the needs of the measurement framework as stated by the software organization's goals. This means that the data is not organized in a way that will facilitate machine learning.
Reference: [66] <author> W. S. Humphrey. </author> <title> Managing the Software Process. SEI Series in Software Engineering. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1989. </year>
Reference-contexts: CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby [39]. The idea of using a software maturity model was developed by Radice while at IBM [95] and was made popular by Humprey at the Software Engineering Institute (SEI) <ref> [66] </ref>. CMM uses a five-level process maturity model to improve quality (Table 2.1). A maturity level is defined based on repeated assessment of an organization's capability in key process areas. Improvement is achieved by action plans for processes that had a poor assessment result.
Reference: [67] <institution> IBM Data Management Solutions. IBM's data mining technology. White Paper, </institution> <year> 1996. </year>
Reference-contexts: This means that the data is not organized in a way that will facilitate machine learning. In particular, there might be irrelevant, missing, noisy, and uncertain data in the database. There are two basic types of data mining operations (see <ref> [67] </ref> for a more detailed classification): (1) one can data mine to create predictive and classification models to forecast the future (predictive data mining); or (2) one can data mine to discover interesting facts in a database (forensic data mining).
Reference: [68] <author> Y. Iizuka. </author> <title> A new paradigm for software quality: the turning point for the japanese software. </title> <editor> In N. Fenton, R. Whitty, and Y. Iizuka, editors, </editor> <title> Software Quality Assurance and Measurement: A Worldwide Perspective. </title> <publisher> International Thomson Computer Press, </publisher> <year> 1995. </year>
Reference-contexts: Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique [33, 34]. The success histories of TQM in manufacturing industries could not be easily transferred to the software industries [3, 57], not even in Japan <ref> [68, 70] </ref>. 2.1.2 The Lean Enterprise Management The Lean Enterprise Management (LEM) goal is to build a product using the minimal set of activities and materials needed, eliminating non essential steps and costs. LEM has been used to improve factory output. <p> 1 ); P roc (P 2 ); : : : ; P roc (P N ) ! P rod (X 1 ); P rod (X 2 ); : : : ; P rod (X N ) The PDCA idea of creating process improvement cycles has been adapted to software organizations <ref> [3, 45, 68] </ref>. However, the process improvement cycle is more complex in software industries. Each software product is unique and requires its own process.
Reference: [69] <author> J. M. Juran. </author> <title> Juran on Planning for Quality. </title> <publisher> Free Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: In general, however, they seek to achieve total quality of a product by involving all members of the production process in the improvement effort. TQM was developed in Japan based on the ideas of W. E. Deming [44] and J.M. Juran <ref> [69] </ref>. The principles of TQM were successfully applied in industries for mass production, such as automobile and consumer electronics industries. In those industries, the concept of total customer satisfaction was translated in terms of producing parts and products with zero defect.
Reference: [70] <author> Y. Kaneko, Y. Kadota, and S.Ohba. </author> <title> Behaviour analysis makes the company mature. </title> <editor> In N. Fenton, R. Whitty, and Y. Iizuka, editors, </editor> <title> Software Quality Assurance and Measurement: A Worldwide Perspective. </title> <publisher> International Thomson Computer Press, </publisher> <year> 1995. </year>
Reference-contexts: Those difficulties are added to the fact that each soft-ware product is complex, abstract, and unique [33, 34]. The success histories of TQM in manufacturing industries could not be easily transferred to the software industries [3, 57], not even in Japan <ref> [68, 70] </ref>. 2.1.2 The Lean Enterprise Management The Lean Enterprise Management (LEM) goal is to build a product using the minimal set of activities and materials needed, eliminating non essential steps and costs. LEM has been used to improve factory output.
Reference: [71] <author> C. F. Kemerer. </author> <title> An empirical validation of software cost estimation models. </title> <journal> Communications of the ACM, </journal> <volume> 30(5) </volume> <pages> 416-429, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy <ref> [27, 32, 71, 109] </ref>.
Reference: [72] <author> T. M. Khoshgoftaar, E. B. Allen, and D. L. Lanning. </author> <title> An information theory-based approach to quantify the contribution of a software metric. </title> <journal> Journal Systems and Software, </journal> <volume> 36(2) </volume> <pages> 103-113, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures <ref> [9, 16, 19, 72, 9 100] </ref>; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing [73, 100]. There are few works on the validation of MFs' completeness, leanness, and consistency.
Reference: [73] <author> Barbara Kitchenham, Shari L. Pfleeger, and Norman E. Fenton. </author> <title> Towards a framework for software measurement validation. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 21(12) </volume> <pages> 929-944, </pages> <year> 1995. </year> <month> 177 </month>
Reference-contexts: Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures [9, 16, 19, 72, 9 100]; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing <ref> [73, 100] </ref>. There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs [40, 60, 91, 96, 110]. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. <p> This definition recognizes the fact that a measure is a mapping from the empirical (real) world to a formal (mathematical) world <ref> [53, 62, 73, 97] </ref>. A metric is characterized by a measurement model, a value domain, and a scale. In order to describe a metric, one has to identify all those components.
Reference: [74] <author> Barbara Kitchenham, Lesley Pickard, and Shari L. Pfleeger. </author> <title> Case studies for method and tool evaluation. </title> <journal> IEEE Software, </journal> <volume> 12(4) </volume> <pages> 52-62, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: The use of a controlled large scale experiment was discarded because it was impractical. Several industrial scale measurement frameworks in similar settings would be needed to do that. The chosen validation method was to execute a case study <ref> [74] </ref> in which the approach was to be applied to a real industrial measurement framework and its results compared with the existing ad-hoc process to improve this measurement framework. 1.4 Experimental Platform We applied our approach to improve the Customer Satisfaction (CUSTSAT) Measurement Framework at the IBM Toronto Laboratory.
Reference: [75] <author> W. Klosgen and J. M. _ Zytkow. </author> <title> Knowledge discovery in databases terminology. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996. </year>
Reference-contexts: The terminology adopted here was adapted from the data mining terminology proposed by Klosgen and Zytkow <ref> [75] </ref> and the software engineering measurement terminology proposed by Fenton [52]. During this section (and the rest of this dissertation), boldface font is used when new terms are defined. We define application domain as the real or abstract system a software organization wants to analyze using a MF.
Reference: [76] <author> M. Kogure and Y. Akao. </author> <title> Quality function deployment and CWQC in Japan. </title> <booktitle> Quality Progress, </booktitle> <pages> pages 25-29, </pages> <month> October </month> <year> 1983. </year>
Reference-contexts: Gilb put it better when he said [55]: "Projects without clear goals will not achieve their goals clearly." A variety of goal-oriented measurement paradigms have appeared in the literature: the Quality Function Deployment <ref> [76] </ref> (QFD) ; the Software Quality Metrics [81] (SQM); and the Goal Question Metric (GQM) paradigm are some of them. 1 Software production corresponds to the act of recording a software product and its installation procedures into the storage media to be shipped to a customer. 16 Required Quality Quality Attributes <p> Req. N.1.2 2 x Table 2.2: QFD Matrix 2.2.1 Quality Function Deployment The Quality Function Deployment (QFD) <ref> [1, 76] </ref> is a technique that evolved from the TQM principle of deriving measures from the customer point of view. In QFD, quality requirements are established for each product from the customer point of view. Those requirements are then mapped to metrics to be used to satisfy those customer needs. <p> Those requirements are then mapped to metrics to be used to satisfy those customer needs. This mapping is done using "House of Quality" matrices. Table 2.2 shows an abstract example of such matrix (adapted from <ref> [76] </ref>). The left side of the matrix has the captured customer requirements. Those requirements are refined top-down - "deployed" in more detailed ones. The detailed requirements are ranked by importance (in this cases from 1 to 3) from the customer point of view.
Reference: [77] <author> R. Likert. </author> <title> The Human Organization: Its Management and Value. </title> <publisher> McGraw Hill, </publisher> <address> NY, </address> <year> 1967. </year>
Reference-contexts: CMM is based on the idea of quality management maturity models developed by Likert <ref> [77] </ref> and Crosby [39]. The idea of using a software maturity model was developed by Radice while at IBM [95] and was made popular by Humprey at the Software Engineering Institute (SEI) [66]. CMM uses a five-level process maturity model to improve quality (Table 2.1).
Reference: [78] <author> Y. S. Lincoln and E. G. Guba. Naturalistic Inquiry. Sage, </author> <year> 1985. </year>
Reference-contexts: A structured interview is one in which the questions are in the hands of the interviewer and the response rests with the interviewee (as opposed to an unstructured interview in which the interviewer simply raises topics for discussion and the interviewee provides both the relevant questions and the answers) <ref> [78] </ref>. Structured interviews are used to capture the descriptions of user groups, attributes, and data uses, as those MF components are usually not documented and can only be obtained by interviewing data managers and data users. <p> The three chosen groups are associated with the database product development at the laboratory: 1. the DB customer service and support group. 2. the DB information development (documentation) group. 3. the DB usability group. We used structured interviews <ref> [78] </ref> to build GQM structures for these groups. We interviewed a senior representative of each group. All the material for the interview was prepared beforehand.
Reference: [79] <author> R. P. Lippmann. </author> <title> An introduction to computing with neural nets. </title> <journal> IEEE ASSP Magazine, </journal> <pages> pages 4-22, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms [35, 41, 85], neural network algorithms <ref> [79, 86] </ref>, and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment. <p> The environment model is represented in a neural network as patterns of interactions between the simple computational elements. The learning algorithm works by adjusting the weights and thresholds of the network connections. Knowledge is implicitly stored in the network itself as a vast number of connections and weights <ref> [79] </ref>. * Decision trees also use a set of training instances. The learning algorithm builds a classification tree as the environment model. The tree classifies examples among a finite number of classes.
Reference: [80] <author> T. J. McCabe. </author> <title> A complexity measure. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 2(4) </volume> <pages> 308-320, </pages> <month> April </month> <year> 1976. </year>
Reference-contexts: (9P ) (9Q) ((M (P ) = M (Q)) ^ (M (R; P ) 6= M (R; Q))), where M (P ) means measured complexity of "P ," and "R; P " means concatenation of program "R" and "P ." In her paper [111], Weyuker showed that the cyclomatic number <ref> [80] </ref> | a very common code complexity metric | does not satisfy this property. The property-based approach can be used to describe attributes, because it isolates the attribute definition from the metric definition.
Reference: [81] <author> J. A. McCall, P. K. Richards, and G. F. Walters. </author> <title> Factors in software quality. </title> <type> Technical Report TR-77369, </type> <institution> RADC, </institution> <year> 1977. </year>
Reference-contexts: Gilb put it better when he said [55]: "Projects without clear goals will not achieve their goals clearly." A variety of goal-oriented measurement paradigms have appeared in the literature: the Quality Function Deployment [76] (QFD) ; the Software Quality Metrics <ref> [81] </ref> (SQM); and the Goal Question Metric (GQM) paradigm are some of them. 1 Software production corresponds to the act of recording a software product and its installation procedures into the storage media to be shipped to a customer. 16 Required Quality Quality Attributes Primary Secondary Tertiary Importance A1 A2 A3 <p> The right side of the matrix map each of those requirements to the measurable attributes that will be used to evaluate them. 2.2.2 Software Quality Metrics Software Quality Metrics (SQM) was developed to allow the customer to assess the product being developed by a contractor <ref> [28, 81] </ref>. In SQM, a set of quality factors is defined for the final product. Those factors are refined into a set of criteria (attributes), which are mapped to a set of pre-defined metrics.
Reference: [82] <author> F. McGarry. </author> <title> Top-down x bottom-up process improvement. </title> <journal> IEEE Software, </journal> <volume> 11(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The Experimental Software Engineering Group at Maryland has adopted the QIP because we believe that an organization should focus on the specific problems they want to solve <ref> [82] </ref>. Unlike the CMM, the QIP does not assume that process improvement is dependent on the maturity of the organization [106]. The QIP starts with a CMM level 5 style of organization, even though it does not have level 5 capability yet [6].
Reference: [83] <author> A. Melton, D. Gustafson, J. Bieman, and A. Baker. </author> <title> A mathematical perspective for software measures research. </title> <journal> J. of Software Eng., </journal> <volume> 5(5) </volume> <pages> 246-254, </pages> <year> 1990. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound <ref> [43, 50, 83, 114] </ref>; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure [2, 31, 102, 108, 111]. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [84] <author> T. M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18(2) </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms <ref> [84] </ref>, decision tree algorithms [87, 94], explanation-based algorithms [35, 41, 85], neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [85] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms <ref> [35, 41, 85] </ref>, neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [86] <author> B. Muller and J. Reinhardt. </author> <title> Neural Networks, An Introduction. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1991. </year>
Reference-contexts: A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms [87, 94], explanation-based algorithms [35, 41, 85], neural network algorithms <ref> [79, 86] </ref>, and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [87] <author> S. Murthy, S. Kasif, S. Salzberg, and R. Beigel. </author> <title> OC1: Randomized induction of oblique decision trees. </title> <booktitle> In Proceedings of the 11th National Conference on Artificial Intelligence, </booktitle> <pages> pages 322-327, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms <ref> [87, 94] </ref>, explanation-based algorithms [35, 41, 85], neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment.
Reference: [88] <author> Raymond J. Offen and Ross Jeffery. </author> <title> Establishing software measurement programs. </title> <journal> IEEE Software, </journal> <volume> 14(2) </volume> <pages> 45-53, </pages> <month> March </month> <year> 1997. </year> <month> 178 </month>
Reference-contexts: Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models [10, 38, 58, 51, 89]. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement <ref> [17, 40, 88] </ref>. However, very little attention has been given to the problem of improving existing measurement programs. This dissertation proposes an approach for improving measurement and data use when a large number of diverse metrics are already being collected by a software organization. The approach combines two methods. <p> There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs [40, 60, 91, 96, 110]. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs <ref> [61, 88] </ref>. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs. Little attention has been given to the problem of improving the completeness, leanness, and consistency of existing operational MFs.
Reference: [89] <author> Paul Oman and Shari L. Pfleeger. </author> <title> Applying Software Metrics. </title> <publisher> IEEE Com--puter Society Press, </publisher> <address> Los Alamitos CA, </address> <year> 1997. </year>
Reference-contexts: Data is collected to describe, monitor, understand, assess, compare, validate, and appraise very diverse attributes related to software processes or products. Much of the research on software engineering measurement has dealt with the definition and validation of software engineering metrics and models <ref> [10, 38, 58, 51, 89] </ref>. Several works have also dealt with the problems of planning and implementing measurement programs in software organizations [61], most notably on goal-oriented measurement [17, 40, 88]. However, very little attention has been given to the problem of improving existing measurement programs.
Reference: [90] <author> M. C. Paulk, B. Curtis, M. B. Chrissis, and C. V. Weber. </author> <title> Capability maturity model, version 1.1. </title> <journal> IEEE Software, </journal> <volume> 10(4), </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: Each software product is unique and requires its own process. In software, each improvement cycle has to build a "new" process tailored from previous software development experiences [18]. 2.1.4 The SEI Capability Maturity Model The Capability Maturity Model (CMM) <ref> [63, 90] </ref> is a quality improvement approach that was specifically tailored to Software Development. CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby [39]. <p> They are built upon understanding the relationships between the historical projects and products and the goals for the new P roduct (V ). 15 In terms of being tailored to quality improvement of software organizations, only the CMM <ref> [90] </ref> can be compared to the QIP. The Experimental Software Engineering Group at Maryland has adopted the QIP because we believe that an organization should focus on the specific problems they want to solve [82].
Reference: [91] <author> Shari L. Pfleeger. </author> <title> Lessons learned in building a corporate metrics program. </title> <journal> IEEE Software, </journal> <volume> 10(3) </volume> <pages> 67-74, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs <ref> [40, 60, 91, 96, 110] </ref>. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs.
Reference: [92] <author> H. Potier, J. Albin, R. Ferreol, and A. Bilodeau. </author> <title> Experiments with computer complexity and reliability. </title> <booktitle> In Proceedings of the 6th International Conference on Software Engineering, </booktitle> <pages> pages 94-103, </pages> <address> Tokyo, Japan, Septem-ber 1982. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees <ref> [92, 101, 104, 107] </ref> and Optimized Set Reduction [29, 32] have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [93] <author> R. S. Pressman. </author> <title> Software Engineering: A Practitioner's Approach, chapter 1. </title> <publisher> McGraw Hill Inc., 3rd edition, </publisher> <year> 1992. </year>
Reference-contexts: The cost and quality of software products are associated with its development process as opposed as to its production 1 process <ref> [93] </ref>. Software is an abstract and complex product and software development is a human intensive process [34, 33]. One of the key ideas behind our approach is that, in software organizations, measurement should be defined in top-down goal-oriented fashion.
Reference: [94] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The model can then be used to derive unknown and interesting information (i.e. new knowledge) about the environment [36, 65]. A broad range of machine learning approaches can be fit into the above framework. Candidate elimination algorithms [84], decision tree algorithms <ref> [87, 94] </ref>, explanation-based algorithms [35, 41, 85], neural network algorithms [79, 86], and genetic algorithms [64], although very different, all fit this very general framework. 27 Let's consider two extreme examples: * Neural networks use training sets which are coded observations of the environment. <p> The learning algorithm builds a classification tree as the environment model. The tree classifies examples among a finite number of classes. Nodes of the tree are labeled with attribute names, the edges are labeled with possible values for these attributes, and the leaves are labeled with the different classes <ref> [94] </ref>. Some machine learning techniques such as decision tress represent knowledge in an interpretable symbolic format. Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99].
Reference: [95] <author> R. Radice, A. J. Harding, P. E. Munnis, and R. W. Phillips. </author> <title> A programming process study. </title> <journal> IBM Systems Journal, </journal> <volume> 24(2), </volume> <year> 1985. </year>
Reference-contexts: CMM is based on the idea of quality management maturity models developed by Likert [77] and Crosby [39]. The idea of using a software maturity model was developed by Radice while at IBM <ref> [95] </ref> and was made popular by Humprey at the Software Engineering Institute (SEI) [66]. CMM uses a five-level process maturity model to improve quality (Table 2.1). A maturity level is defined based on repeated assessment of an organization's capability in key process areas.
Reference: [96] <author> S. Rifkin and C. Cox. </author> <title> Measurement in practice. </title> <type> Technical Report CMU/SEI-91-TR-16, </type> <institution> SEI, </institution> <year> 1991. </year>
Reference-contexts: There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs <ref> [40, 60, 91, 96, 110] </ref>. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs.
Reference: [97] <author> F. S. Roberts. </author> <title> Measurement Theory with Applications to Decision Making, Utility, and the Social Sciences, chapter 1. </title> <publisher> Addison Wesley Inc., </publisher> <year> 1979. </year>
Reference-contexts: This definition recognizes the fact that a measure is a mapping from the empirical (real) world to a formal (mathematical) world <ref> [53, 62, 73, 97] </ref>. A metric is characterized by a measurement model, a value domain, and a scale. In order to describe a metric, one has to identify all those components.
Reference: [98] <author> R. J. Rubey and R. D. Hartwick. </author> <title> Quantitative measurement of program quality. </title> <booktitle> In Proceedings of the 23rd ACM National Conference, </booktitle> <pages> pages 671-677, </pages> <address> Princeton NJ, 1968. </address> <publisher> Brandon/Systems Press. </publisher>
Reference-contexts: Although the differentiation between "metrics" and "attributes" was made popular by Fenton in his 1991 software metrics book [51], it was first proposed by Rubey and Hartwick in a seminal 1968 work <ref> [98] </ref>, on their own words: "... attribute is a precise statement of a specific software characteristic. ... a metric was developed for quantitative measurement of each quality attribute.
Reference: [99] <author> J. C. Schilimmer and P. Langley. </author> <title> Machine learning. </title> <editor> In S. C. Shapiro, editor, </editor> <booktitle> Encyclopedia of Artificial Intelligence, </booktitle> <volume> volume 1, </volume> <pages> pages 785-805. </pages> <publisher> John Wiley & Sons, </publisher> <year> 1992. </year>
Reference-contexts: If this information is also interesting and previously unknown, it called discovered knowledge or new knowledge [54]. The process of discovering new knowledge is referred to as inductive learning [64]. The automation of inductive learning processes has been researched in an artificial intelligence area called machine learning <ref> [99] </ref>. A machine learning system does not interact directly with its environment. It uses "coded observations" of this environment to learn about it. Figure 2.5 depicts the machine leaning process. It samples facts from the environment we want to model. It codes these facts as "coded observations" of the environment. <p> Some machine learning techniques such as decision tress represent knowledge in an interpretable symbolic format. Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format <ref> [99] </ref>. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format.
Reference: [100] <author> N. F. Schneidewind. </author> <title> Methodology for validating software metrics. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 18(5) </volume> <pages> 410-422, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Empirical validation of direct metrics has been done: (1) to analyze the association between these metrics and important quality measures [9, 16, 19, 72, 9 100]; and, (2) to assess these metrics consistency when they are used by different people to measure the same thing <ref> [73, 100] </ref>. There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs [40, 60, 91, 96, 110]. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88].
Reference: [101] <author> R. Selby and A. H. Porter. </author> <title> Learning from examples: Generation and evaluation of decision trees for software resource analysis. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 14(12) </volume> <pages> 1743-1757, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees <ref> [92, 101, 104, 107] </ref> and Optimized Set Reduction [29, 32] have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [102] <author> M. Shepperd. </author> <title> Algebraic models and metric validation. </title> <editor> In I. Somerville and M. Paul, editors, </editor> <booktitle> Formal Aspects of Measurement, Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1992. </year> <month> 179 </month>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound [43, 50, 83, 114]; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure <ref> [2, 31, 102, 108, 111] </ref>. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [103] <author> W. A. Shewhart. </author> <title> Economic Control of Quality of Manufactured Product. </title> <address> D. </address> <publisher> Van Nostrand Company, Inc., </publisher> <address> New York, </address> <year> 1931. </year>
Reference-contexts: Its based on work by W. A. Shewhart <ref> [103] </ref> and was made popular and applied effectively to improve Japanese manufacturing after World War II by W. E. Demming [44]. The approach is defined as four basic steps: Plan: Develop a plan for improving the existing production process. <p> The QIP incorporates ideas from several quality improvement approaches used in the manufacturing industries [14]: * Its evolutionary nature, based on feedback loops, is similar to the Plan-Do Check-Act Paradigm (PDCA) <ref> [44, 103] </ref>. * Its goals, feedback mechanisms, and use of measurement allow us to involve everyone in the job of quality assurance.
Reference: [104] <author> K. Srinivasan and D. Fisher. </author> <title> Machine learning approaches to estimating software development effort. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 21(2) </volume> <pages> 126-137, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees <ref> [92, 101, 104, 107] </ref> and Optimized Set Reduction [29, 32] have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [105] <author> S. S. Stevens. </author> <title> On the theory of scales of measurement. </title> <journal> Science, </journal> <volume> 103 </volume> <pages> 677-680, </pages> <year> 1946. </year>
Reference-contexts: The value domain is not enough to define what operations can be executed over these values. A measurement scale is needed to define what operations are admissible over a value domain. The work on scales was pioneered by Stevens <ref> [105] </ref>, the following four scale types make up his original classification: * Nominal scale defines representations that can be used to classify entities into categories based on their attribute values.
Reference: [106] <author> M. Thomas. </author> <title> Top-down x bottom-up process improvement. </title> <journal> IEEE Software, </journal> <volume> 11(4), </volume> <month> April </month> <year> 1994. </year>
Reference-contexts: The Experimental Software Engineering Group at Maryland has adopted the QIP because we believe that an organization should focus on the specific problems they want to solve [82]. Unlike the CMM, the QIP does not assume that process improvement is dependent on the maturity of the organization <ref> [106] </ref>. The QIP starts with a CMM level 5 style of organization, even though it does not have level 5 capability yet [6]. The organization is driven by the understanding of its business, products and process problems [14].
Reference: [107] <author> J. Tian, J. Henshaw, and I. Burrows. </author> <title> Analysis of factors affecting infield product quality using tree-based predictive modeling. </title> <booktitle> In Proc. IBM Software Development Conference, </booktitle> <address> San Jose, California, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Other techniques such as neural networks represent knowledge implicitly in a non-interpretable format [99]. Most of the reported uses of machine learning techniques in software engineering use techniques that represents knowledge in symbolic interpretable format. Techniques such as Classification Trees <ref> [92, 101, 104, 107] </ref> and Optimized Set Reduction [29, 32] have been used more frequently than neural networks to build predictive and classification models for software organizations. 2.4.2 Data Mining The bottom-up analysis aims to extract knowledge directly from the MF database. <p> The bottom-up analyses are aimed at discovering new and useful information in the existing data, thus improving data awareness and data usage. The literature has many examples of the use of machine learning techniques to extract knowledge (new and useful information) from software engineering data sets <ref> [29, 32, 92, 101, 104, 107] </ref>.
Reference: [108] <author> J. Tian and M. Zelkowitz. </author> <title> A formal program complexity model and its application. </title> <journal> J. Syst. Software, </journal> <volume> 17 </volume> <pages> 253-266, </pages> <year> 1992. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound [43, 50, 83, 114]; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure <ref> [2, 31, 102, 108, 111] </ref>. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
Reference: [109] <author> C. E. Walston and C. P. Felix. </author> <title> A method of programming measurement and estimation. </title> <journal> IBM Systems Journal, </journal> <volume> 16(1) </volume> <pages> 54-73, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: Empirical validations of predictive models have been done to validate these models' precision and accuracy <ref> [27, 32, 71, 109] </ref>.
Reference: [110] <author> Edward F. Weller. </author> <title> Using metrics to manage software projects. </title> <journal> IEEE Computer, </journal> <volume> 27(9) </volume> <pages> 27-33, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: There are few works on the validation of MFs' completeness, leanness, and consistency. These three issues have traditionally been addressed in practitioner's examples of successful MFs <ref> [40, 60, 91, 96, 110] </ref>. Only recently, methodologies have been proposed to build complete, lean, and consistent MFs [61, 88]. Most of these works recognize that measurement should be executed in a top-down goal-oriented way, but they only address the problem of defining lean, complete, and consistent MFs.
Reference: [111] <author> E. J. Weyuker. </author> <title> Evaluating software complexity measures. </title> <journal> IEEE Transactions on Software Eng., </journal> <volume> 14(9) </volume> <pages> 1357-1365, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound [43, 50, 83, 114]; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure <ref> [2, 31, 102, 108, 111] </ref>. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109]. <p> The property-based attribute definition works by stating as axioms the properties that the metrics used to measure the attribute have to satisfy. Consider the following axiom stated by Weyuker as a property of the attribute "code complexity" <ref> [111] </ref>: The concatenation of a program body "R" with two different programs bodies "P" and "Q" can affect complexity in different ways. <p> "Q." This can be formally stated as: (9P ) (9Q) ((M (P ) = M (Q)) ^ (M (R; P ) 6= M (R; Q))), where M (P ) means measured complexity of "P ," and "R; P " means concatenation of program "R" and "P ." In her paper <ref> [111] </ref>, Weyuker showed that the cyclomatic number [80] | a very common code complexity metric | does not satisfy this property. The property-based approach can be used to describe attributes, because it isolates the attribute definition from the metric definition.
Reference: [112] <author> J. P. Womack, D. T. Jones, and D. Roos. </author> <title> The Machine That Changed the World: Based on the MIT 5-Million Dollars 5-year Study on the Future of the Automobile. </title> <publisher> Rawson Associates, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: Requiring soundness, completeness, leaness, and consistency of measurement frameworks is not new a new idea in software measurement. In a seminal 1976 work, Boehm, et al. <ref> [112] </ref>, wrote: "Our ... approach were as follow: 1. Determine a set of characteristics which are important ... and reasonably exhaustive and non-overlapping. ... 3. Investigate the characteristics and associated metrics to determine their correlation with software quality ... 4. <p> LEM has been used to improve factory output. Womack, et al. <ref> [112] </ref>, have written a book on the application of LEM to automotive industries. LEM basic idea is to tailor a process suited to the product needs. <p> One can use QIP to implement a Total Quality Management (TQM) philosophy in a software organiza tion [49]. * Its approach to tailoring the development process as an optimum set of available sub-processes is similar to Lean Enterprise Management (LEM) <ref> [112] </ref>. Both have the idea of meeting the particular goals of a project using the minimum set of essential steps.
Reference: [113] <author> Marvin V. Zelkowitz and Dolores Wallace. </author> <title> Experimental models for validating computer technology. </title> <journal> IEEE Computer, </journal> <note> to appear. </note>
Reference-contexts: For these reasons, experimental was chosen over analytical validation. 1.3.2 Why a Case Study ? There are several experimental methodologies to validate new software technologies <ref> [113] </ref>. In the case of our approach, one might consider executing a small replicated experiment in an artificial setting, a controlled large scale experiment, or a case study in an industrial setting.
Reference: [114] <author> H. Zuse. </author> <title> Software Complexity: Measures and Methods. </title> <address> deGruyter, </address> <year> 1990. </year> <month> 180 </month>
Reference-contexts: Metrics have been validated in very different ways. Analytical validations have been done: (1) to analyze if a metric is theoretically sound <ref> [43, 50, 83, 114] </ref>; or (2) to verify if a metric fulfills the properties that are associated with the attribute it is supposed to measure [2, 31, 102, 108, 111]. Empirical validations of predictive models have been done to validate these models' precision and accuracy [27, 32, 71, 109].
References-found: 114

