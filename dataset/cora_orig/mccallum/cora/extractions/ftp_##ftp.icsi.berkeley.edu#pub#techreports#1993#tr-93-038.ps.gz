URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1993/tr-93-038.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1993.html
Root-URL: http://www.icsi.berkeley.edu
Title: Exploitation of Structured Gating Connections for the Normalization of a Visual Pattern  
Phone: 1-510-642-4274 FAX 1-510-643-7684  
Author: Alessandro Sperduti 
Note: Paper to be presented at the International Joint Conference on Neural Networks, Oct. 1993, Nagoya, Japan.  
Address: I 1947 Center Street Suite 600 Berkeley, California 94704  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  
Pubnum: TR-93-038  
Abstract: Structured gating connections can be useful to reduce the complexity of networks with a high number of inputs. An example of their application to the normalization of a visual pattern with respect to scale and position is presented. The use of gating connections allows us to have a linear number of connections in the number of pixels. The connections are also very localized. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Durbin and D. E. Rumelhart. </author> <title> Product units: a computationally powerful and biologically plausible extension to backpropagation networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1:133, </address> <year> 1989. </year>
Reference-contexts: 1 Introduction Usually, networks facing tasks involving high dimensional data need a large amount of connections and long training periods. In this paper we present an example of a neural network exploiting structured gating connections (see <ref> [1] </ref>.) In particular, we show how gating connections of enabling type can reduce the number of connections in a network, provided that the desired function can be realized or approximated by the composition of simplier basic functions.
Reference: [2] <author> K. Fukushima, S. Miyake, and I. Takayuki. Noecognitron: </author> <title> a neural network model for a mechanism of visual pattern recognition. </title> <journal> IEEE Transaction on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 826-834, </pages> <year> 1983. </year>
Reference-contexts: Here we give an example of how structured gating connections can be used in the normalization of a visual pattern with respect to scale and position. The structure of several neural networks for visual pattern recognition is inspired after the Neocognitron network <ref> [2] </ref>, where several layers of neurons are connected in cascade by relatively localized but dense connections (see [5] for a shift-invariant network.) In this paper, we show how gating connections can lead to high performance in generalization since they allow the embedding of a priori knowledge in a neural network with
Reference: [3] <author> R. A. Jacobs, M. I. Jordan, and A. G. Barto. </author> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <type> Technical Report COINS 90-27, </type> <institution> Dept. Comput. and Inform. Sci., Univ. </institution> <address> Mass., </address> <year> 1990. </year>
Reference: [4] <author> B. Olshausen, C. Anderson, and D. Van Essen. </author> <title> A neural model of visual attention and invariant pattern recognition. </title> <type> Technical Report CNS Memo 18, </type> <institution> California Institute of Technology Computation and Neural Systems Program, </institution> <year> 1992. </year>
Reference: [5] <author> W. Zhang, A. Hasegawa, K. Itoh, and Y. Ichioka. </author> <title> Error back propagation with minimum-entropy weights: A technique for better generalization of 2-d shift-invariant nns. </title> <booktitle> In International Joint Conference on Neural Networks, </booktitle> <pages> pages 219-224, </pages> <address> 1991. Seattle. </address> <month> 9 </month>
Reference-contexts: The structure of several neural networks for visual pattern recognition is inspired after the Neocognitron network [2], where several layers of neurons are connected in cascade by relatively localized but dense connections (see <ref> [5] </ref> for a shift-invariant network.) In this paper, we show how gating connections can lead to high performance in generalization since they allow the embedding of a priori knowledge in a neural network with a few units and sparse connections. 2 The Task The task we have chosen consists in scale
References-found: 5

