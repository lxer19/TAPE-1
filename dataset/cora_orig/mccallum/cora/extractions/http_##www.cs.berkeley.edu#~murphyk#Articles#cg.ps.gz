URL: http://www.cs.berkeley.edu/~murphyk/Articles/cg.ps.gz
Refering-URL: http://www.cs.berkeley.edu/~murphyk/publ.html
Root-URL: 
Title: Inference and Learning in Hybrid Bayesian Networks  
Author: Kevin P. Murphy 
Address: Berkeley, California 94720  
Affiliation: Computer Science Division (EECS) University of California  
Date: January 1998  
Pubnum: Report No. UCB/CSD-98-990  
Abstract-found: 0
Intro-found: 1
Reference: [AA96] <author> S. Alag and A. Agogino. </author> <title> Inference using message propogation and topology transformation in vector Gaussian continuous networks. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <year> 1996. </year>
Reference-contexts: Similar results have been derived for directed trees <ref> [Pea88, AA96, DM95] </ref> but some of the details have still to be worked out. The algorithms for directed trees involve more complicated equations, since we must work out by hand the form of various expressions involving products and integrals of multidimensioanl Gaussians.
Reference: [BC94] <author> P. Baldi and Y. Chauvin. </author> <title> A smooth learning algorithm for hidden Markov models. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 305-316, </pages> <year> 1994. </year>
Reference-contexts: However, it is easy to convert them to incremental (online) versions, which update the parameters after seeing a subset of the training set (see e.g., [NH93] for incremental EM and <ref> [BC94] </ref> for incremental gradient descent). 12 6.2.1 EM The basic idea of the Expectation Maximization (EM) algorithm is to "fill in" the missing values with their expected values (expectation w.r.t. the current set of parameters), and to use these Expected Sufficient Statistics (ESS) when computing the MLE.
Reference: [BKRK97] <author> J. Binder, D. Koller, S. J. Russell, and K. </author> <title> Kanazawa. Adaptive probabilistic networks with hidden variables. </title> <booktitle> Machine Learning, </booktitle> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: be justified both from a theoretical on-line learning perspective [BKS97] or from a more classical statistical perspective [RW84].) 13 6.2.2 Gradient descent It is possible to compute an expression for the gradient of the log-likelihood (e.g., [XJ96] gives the derivatives w.r.t. ~ and for the mixture of Gaussians model, and <ref> [BKRK97] </ref> gives the derivative for discrete BNs), and hence to use gradient-based learning methods. The hard part is maintaining the constraints on the parameters.
Reference: [BKS97] <author> E. Bauer, D. Koller, and Y. Singer. </author> <title> Batch and on-line parameter estimation in Bayesian networks. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <year> 1997. </year>
Reference-contexts: There are many variations on EM (see e.g., [MK97]), some of which may be applicable in this context. In particular, it would be interesting to know if the simple, but powerful, speedup technique in <ref> [BKS97] </ref>, which was developed in the context of discrete Bayes nets, is useful in the present case. (This technique is as follows: if ~ t is the vector of parameters at iteration t, and ~ t+1 = U ( ~ t ) are the updated parameters suggested by the M step <p> This rule is somewhat counterintuitive because it involves adding a negative fraction of the old values to the new values, but can be justified both from a theoretical on-line learning perspective <ref> [BKS97] </ref> or from a more classical statistical perspective [RW84].) 13 6.2.2 Gradient descent It is possible to compute an expression for the gradient of the log-likelihood (e.g., [XJ96] gives the derivatives w.r.t. ~ and for the mixture of Gaussians model, and [BKRK97] gives the derivative for discrete BNs), and hence to
Reference: [BSF88] <author> Y. Bar-Shalom and T. Fortmann. </author> <title> Tracking and data association. </title> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: The task of computing the probability of the hidden state given all the observations, Pr (Q t jy 0 ; : : : ; y n ), is called smoothing, and the classical algorithm for it was invented by Rauch. See <ref> [BSF88, BSL93] </ref> for details. The Kalman filter was developed for tracking point-like objects, such as planes and missiles. It is reasonable to represent the state (e.g., position and velocity) of a missile with a single node, Q t .
Reference: [BSL93] <author> Y. Bar-Shalom and X. Li. </author> <title> Estimation and Tracking: </title> <booktitle> Principles, Techniques and Software. </booktitle> <address> Artech House, </address> <year> 1993. </year>
Reference-contexts: The task of computing the probability of the hidden state given all the observations, Pr (Q t jy 0 ; : : : ; y n ), is called smoothing, and the classical algorithm for it was invented by Rauch. See <ref> [BSF88, BSL93] </ref> for details. The Kalman filter was developed for tracking point-like objects, such as planes and missiles. It is reasonable to represent the state (e.g., position and velocity) of a missile with a single node, Q t . <p> If we don't do strong triangulation, the number of mixture components becomes exponential in the length of the sequence. The standard approach (see e.g., <ref> [TSM85, BSL93, Kim94, WH97] </ref>) is to "collapse" the mixture into k components.
Reference: [Bun94] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> J. of AI Research, </journal> <pages> pages 159-225, </pages> <year> 1994. </year>
Reference-contexts: Further, the only terms in the joint distribution that depend on fi i involve X i and its parents, so we just need to compute the sufficient statistics for each family. Discrete, linear Gaussian and mixtures of linear Gaussian distributions are all in the exponential family <ref> [DeG70, Bun94, Lau96] </ref>; hence the size of the sufficient statistics we need to keep is equal to the size of the parameter vector (and independent of N ). 6.1 Fully observable case 6.1.1 Discrete case If X i is a discrete variable, the parameter vector is fi i = ( ijk
Reference: [CF91] <author> K. C. Chang and R. M. Fung. </author> <title> Symbolic probabalistic inference with continuous variables. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <pages> pages 77-85, </pages> <year> 1991. </year> <month> 14 </month>
Reference-contexts: There are two solutions to this. One is to work out what the virgin potentials should be by traversing the graph in topological order, c.f. <ref> [CF91] </ref>. The other is to represent the initial potentials using canonical characteristics, and then, at the end of the initial pass, compute the moment form. We adopt the latter approach.
Reference: [CF95] <author> K. C. Chang and R. M. Fung. </author> <title> Symbolic probabilistic inference with both discrete and continuous variables. </title> <journal> IEEE Trans. on Systems, Man, and Cybernetics, </journal> <volume> 25(6) </volume> <pages> 910-917, </pages> <year> 1995. </year>
Reference-contexts: In contrast, for the undirected case, it suffices to show how to perform several basic operations (multiplication, division, marginalization, evidence substitution, and initialization), and we can let the computer figure out the rest. The simplest algorithm for hybrid networks is probably SPI <ref> [CF95] </ref>; unfortunately, it is not suitable for our purposes, since we need to compute the marginal on every family for learning, and SPI takes N passes to compute N marginals. 5.1 Pure Gaussian case In the discrete case, the potential over a clique can be represented as a table.
Reference: [DeG70] <author> M. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <year> 1970. </year>
Reference-contexts: Further, the only terms in the joint distribution that depend on fi i involve X i and its parents, so we just need to compute the sufficient statistics for each family. Discrete, linear Gaussian and mixtures of linear Gaussian distributions are all in the exponential family <ref> [DeG70, Bun94, Lau96] </ref>; hence the size of the sufficient statistics we need to keep is equal to the size of the parameter vector (and independent of N ). 6.1 Fully observable case 6.1.1 Discrete case If X i is a discrete variable, the parameter vector is fi i = ( ijk <p> It is simpler to associate a prior with the MVG distribution N (~ F ; F ) on the family F , rather than with the parameters (~ X ; X ; B X ) of the node itself. A suitable prior is the Normal-Wishart <ref> [GH94, DeG70] </ref>. This can be important since it takes a lot of data to ensure b is positive definite.
Reference: [DM95] <author> E. Driver and D. Morrel. </author> <title> Implementation of continuous Bayesian networks usings sums of weighted Gaussians. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <pages> pages 134-140, </pages> <year> 1995. </year>
Reference-contexts: Similar results have been derived for directed trees <ref> [Pea88, AA96, DM95] </ref> but some of the details have still to be worked out. The algorithms for directed trees involve more complicated equations, since we must work out by hand the form of various expressions involving products and integrals of multidimensioanl Gaussians.
Reference: [DW91] <author> Thomas L. Dean and Michael P. Wellman. </author> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: i=1 2 i j 1 Using Dawid's theorem, which states that X ? Y jZ if the joint density can be factored as f X;Y;Z (x; y; z) = g (x; z)h (y; z) we prove Equation 5. 4 Example of hybrid DBNs: switching Kalman filters A Dynamic Bayesian Network <ref> [DW91, Gha97] </ref> is a BN used to model a temporal stochastic process. It can be be created by specifying the network (structure and parameters) for two consecutive "time slices", and then "unrolling" it into a static network of the required size.
Reference: [Edw95] <author> D. Edwards. </author> <title> Introduction to graphical modelling. </title> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: Gaussian graphical models In this section we will show that X i ? X j j (the rest) () K ij = 0 (5) where K = 1 is the inverse covariance matrix (also called the precision matrix) of the joint distribution, and "the rest" means all the other nodes <ref> [Whi90, Edw95] </ref>.
Reference: [GH94] <author> D. Geiger and D. Heckerman. </author> <title> Learning Gaussian networks. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <volume> volume 10, </volume> <pages> pages 235-243, </pages> <year> 1994. </year>
Reference-contexts: It is simpler to associate a prior with the MVG distribution N (~ F ; F ) on the family F , rather than with the parameters (~ X ; X ; B X ) of the node itself. A suitable prior is the Normal-Wishart <ref> [GH94, DeG70] </ref>. This can be important since it takes a lot of data to ensure b is positive definite.
Reference: [GH96] <author> Z. Ghahramani and G. Hinton. </author> <title> Switching state-space models. </title> <type> Technical Report CRG-TR-96-3, </type> <institution> Dept. Comp. Sci., Univ. Toronto, </institution> <year> 1996. </year>
Reference-contexts: It is not clear how the "collapsing" or weak marginalization approximations affect learning, especially in the context of DBNs where we want to use all the evidence ("future" as well as past), as is traditional in off-line learning. The approach taken in <ref> [GH96] </ref> is to maximize an exact lower bound on the likelihood, produced by considering a tractable approximation to the original structure.
Reference: [Gha97] <author> Z. Gharamani. </author> <title> Learning dynamic bayesian networks. </title> <editor> In C.L. Giles and M. Gori, editors, </editor> <booktitle> Adaptive Processing of Temporal Information . Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year> <note> To appear. </note>
Reference-contexts: i=1 2 i j 1 Using Dawid's theorem, which states that X ? Y jZ if the joint density can be factored as f X;Y;Z (x; y; z) = g (x; z)h (y; z) we prove Equation 5. 4 Example of hybrid DBNs: switching Kalman filters A Dynamic Bayesian Network <ref> [DW91, Gha97] </ref> is a BN used to model a temporal stochastic process. It can be be created by specifying the network (structure and parameters) for two consecutive "time slices", and then "unrolling" it into a static network of the required size.
Reference: [Jen96] <author> F. V. Jensen. </author> <title> An Introduction to Bayesian Networks. </title> <publisher> UCL Press, </publisher> <address> London, England, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction We discuss Bayesian networks (BNs <ref> [Jen96] </ref>) in which each node is either discrete or continuous, scalar or vector-valued, and in which the joint distribution over all the nodes is Conditional Gaussian (CG) [LW89, Lau92] i.e., for each instantiation i of the discrete nodes Y, the distribution over the continuous nodes X has the form f (xjY
Reference: [JJD94] <author> F. V. Jensen, F. Jensen, and S. L. Dittmer. </author> <title> From influence diagrams to junction trees. </title> <booktitle> In Proc. of the Conf. on Uncertainty in AI, </booktitle> <year> 1994. </year>
Reference: [Kim94] <author> C-J. Kim. </author> <title> Dynamic linear models with Markov-switching. </title> <journal> J. of Econometrics, </journal> <volume> 60 </volume> <pages> 1-22, </pages> <year> 1994. </year>
Reference-contexts: If we don't do strong triangulation, the number of mixture components becomes exponential in the length of the sequence. The standard approach (see e.g., <ref> [TSM85, BSL93, Kim94, WH97] </ref>) is to "collapse" the mixture into k components.
Reference: [Lau92] <author> S. L. Lauritzen. </author> <title> Propagation of probabilities, means and variances in mixed graphical association models. </title> <journal> J. of the Am. Stat. Assoc., </journal> <volume> 87(420) </volume> <pages> 1098-1108, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: 1 Introduction We discuss Bayesian networks (BNs [Jen96]) in which each node is either discrete or continuous, scalar or vector-valued, and in which the joint distribution over all the nodes is Conditional Gaussian (CG) <ref> [LW89, Lau92] </ref> i.e., for each instantiation i of the discrete nodes Y, the distribution over the continuous nodes X has the form f (xjY = i) = N (x; ~(i); (i)), where N () represents a multivariate Gaussian (MVG) or Normal density. (Note that discrete nodes cannot have continuous parents in <p> = H [D t ]Q t + J [D t ]V t We briefly discuss the computational issues involved in performing inference in hybrid DBNs in Section 5.2. 6 5 Inference We shall discuss how to perform inference in hybrid networks using a variation of the Lauritzen and Spiegel-halter method <ref> [LS88, LW89, Lau92, Ole93, Lau96] </ref>, which works on undirected trees. Similar results have been derived for directed trees [Pea88, AA96, DM95] but some of the details have still to be worked out. <p> However, this requires a matrix inversion, which is slow and can introduce loss of precision. We would therefore like to work exclusively in moment form. In what follows below, we derive the equations for the moment form, but, for completeness, also state the results in <ref> [Lau92] </ref> for the canonical case. 5.1.1 Initialization In the Lauritzen and Spiegelhalter algorithm, each clique potential is initialized to be the product of the conditional distributions of all the nodes that have been assigned to that clique. (Each node is assigned to exactly one clique, which must contain its family.) After <p> Hence we set the canonical characteristics to g = 1 n log (2) 1 h = 1 ~ B T B 1 B T This generalizes the result in <ref> [Lau92] </ref> to the vector case. <p> [g + ( x T y T ) h X 2 ( x T y T ) K XX K XY y = exp [ g + h T 2 y T K Y Y y + x T (h X K XY y) 1 This generalizes the equation in <ref> [Lau92] </ref> to the vector case. We can compute the analogous result for moment characteristics as follows.
Reference: [Lau96] <author> S. Lauritzen. </author> <title> Graphical Models. </title> <address> OUP, </address> <year> 1996. </year>
Reference-contexts: = H [D t ]Q t + J [D t ]V t We briefly discuss the computational issues involved in performing inference in hybrid DBNs in Section 5.2. 6 5 Inference We shall discuss how to perform inference in hybrid networks using a variation of the Lauritzen and Spiegel-halter method <ref> [LS88, LW89, Lau92, Ole93, Lau96] </ref>, which works on undirected trees. Similar results have been derived for directed trees [Pea88, AA96, DM95] but some of the details have still to be worked out. <p> j) ^ ~(i) T (~(i; j) ^ ~(i)p (i; j)= ^p (i) These will give the "correct" mean and variance: Pr (I = i) = ^p (i) = j Var [YjI = i] = E [Var [YjI; J]ji = i] + Var [E [YjI; J]ji = i] h T Lauritzen <ref> [Lau96] </ref> shows that this is the best approximation (in the KL sense) if k = 1. 6 Learning In this section, we discuss how to find the Maximum Likelihood Estimates (MLEs) of the parameters associated with each node. <p> Further, the only terms in the joint distribution that depend on fi i involve X i and its parents, so we just need to compute the sufficient statistics for each family. Discrete, linear Gaussian and mixtures of linear Gaussian distributions are all in the exponential family <ref> [DeG70, Bun94, Lau96] </ref>; hence the size of the sufficient statistics we need to keep is equal to the size of the parameter vector (and independent of N ). 6.1 Fully observable case 6.1.1 Discrete case If X i is a discrete variable, the parameter vector is fi i = ( ijk <p> = X 1 + 12 1 and 22 21 : (10) Hence the local parameters for the node are given by B = Y Z 1 ~ = ~ Y B~ Z B can then be broken up into its individual blocks, one for each parent. 6.1.3 Hybrid case In <ref> [Lau96] </ref>, it is shown that the sample mean, given a discrete instantiation i, has a N (~(i); n (i) 1 (i)) distribution, where n (i) is the number of times this discrete instantiation has been seen (we assume n (i) &gt; 0).
Reference: [LS88] <author> S. L. Lauritzen and D. J. Spiegelhalter. </author> <title> Local computations with probabilities on graphical structures and their applicatins to expert systems. </title> <journal> J. R. Stat. Soc. B, </journal> <volume> B(50):127-224, </volume> <year> 1988. </year>
Reference-contexts: = H [D t ]Q t + J [D t ]V t We briefly discuss the computational issues involved in performing inference in hybrid DBNs in Section 5.2. 6 5 Inference We shall discuss how to perform inference in hybrid networks using a variation of the Lauritzen and Spiegel-halter method <ref> [LS88, LW89, Lau92, Ole93, Lau96] </ref>, which works on undirected trees. Similar results have been derived for directed trees [Pea88, AA96, DM95] but some of the details have still to be worked out.
Reference: [LW89] <author> S. L. Lauritzen and N. Wermuth. </author> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics, </journal> <volume> 17 </volume> <pages> 31-57, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction We discuss Bayesian networks (BNs [Jen96]) in which each node is either discrete or continuous, scalar or vector-valued, and in which the joint distribution over all the nodes is Conditional Gaussian (CG) <ref> [LW89, Lau92] </ref> i.e., for each instantiation i of the discrete nodes Y, the distribution over the continuous nodes X has the form f (xjY = i) = N (x; ~(i); (i)), where N () represents a multivariate Gaussian (MVG) or Normal density. (Note that discrete nodes cannot have continuous parents in <p> = H [D t ]Q t + J [D t ]V t We briefly discuss the computational issues involved in performing inference in hybrid DBNs in Section 5.2. 6 5 Inference We shall discuss how to perform inference in hybrid networks using a variation of the Lauritzen and Spiegel-halter method <ref> [LS88, LW89, Lau92, Ole93, Lau96] </ref>, which works on undirected trees. Similar results have been derived for directed trees [Pea88, AA96, DM95] but some of the details have still to be worked out.
Reference: [MK97] <author> G. J. McLachlan and T. Krishnan. </author> <title> The EM Algorithm and Extensions. </title> <publisher> Wiley, </publisher> <year> 1997. </year>
Reference-contexts: Update the ESS for each family. (d) Compute the MLE of the parameters for each family given the ESS. 3. Until L converges. Steps 2 (c)i and 2 (c)ii can be computed using the inference algorithms we discussed earlier. There are many variations on EM (see e.g., <ref> [MK97] </ref>), some of which may be applicable in this context.
Reference: [Mur97] <author> K. P. Murphy. </author> <title> Comparing EM to gradient-based methods for learning parameters in discrete Bayes nets. </title> <institution> U.C. Berkeley, Dept. Comp. Sci, </institution> <year> 1997. </year>
Reference-contexts: One way is to start out with a valid initial guess, and then to take sufficiently small steps in parameter space. However, this can lead to slow convergence. Indeed, experiments with the mixture of Gaussians model [XJ96] and discrete Bayes nets <ref> [Mur97] </ref> indicate that EM, while technically a first order method, often does better than nominally faster gradient-based methods, such as conjugate gradient or quasi-Newton.
Reference: [NH93] <author> R. M. Neal and G. E. Hinton. </author> <title> A new view of the EM algorithm that justifies incremental and other variants. </title> <journal> Biometrika, </journal> <note> 1993. submitted. </note>
Reference-contexts: However, it is easy to convert them to incremental (online) versions, which update the parameters after seeing a subset of the training set (see e.g., <ref> [NH93] </ref> for incremental EM and [BC94] for incremental gradient descent). 12 6.2.1 EM The basic idea of the Expectation Maximization (EM) algorithm is to "fill in" the missing values with their expected values (expectation w.r.t. the current set of parameters), and to use these Expected Sufficient Statistics (ESS) when computing the
Reference: [Ole93] <author> K. G. Olesen. </author> <title> Causal probabilistic networks with both discrete and continuous variables. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 3(15), </volume> <year> 1993. </year>
Reference-contexts: = H [D t ]Q t + J [D t ]V t We briefly discuss the computational issues involved in performing inference in hybrid DBNs in Section 5.2. 6 5 Inference We shall discuss how to perform inference in hybrid networks using a variation of the Lauritzen and Spiegel-halter method <ref> [LS88, LW89, Lau92, Ole93, Lau96] </ref>, which works on undirected trees. Similar results have been derived for directed trees [Pea88, AA96, DM95] but some of the details have still to be worked out.
Reference: [Pea88] <author> J. Pearl. </author> <title> Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Similar results have been derived for directed trees <ref> [Pea88, AA96, DM95] </ref> but some of the details have still to be worked out. The algorithms for directed trees involve more complicated equations, since we must work out by hand the form of various expressions involving products and integrals of multidimensioanl Gaussians.
Reference: [RG97] <author> S. Roweis and Z. Ghahramani. </author> <title> A unifying review of linear gaussian models. </title> <note> Submitted, </note> <month> August </month> <year> 1997. </year>
Reference-contexts: A related review article is <ref> [RG97] </ref>. We start by discussing how to represent the conditional probability distribution of each node, and the joint distribution this encodes.
Reference: [RW84] <author> R. A. Redner and H. F. Walker. </author> <title> Mixture densities, maximum likelihood, and the EM algorithm. </title> <journal> SIAM Review, </journal> <volume> 26 </volume> <pages> 195-239, </pages> <year> 1984. </year> <month> 15 </month>
Reference-contexts: This rule is somewhat counterintuitive because it involves adding a negative fraction of the old values to the new values, but can be justified both from a theoretical on-line learning perspective [BKS97] or from a more classical statistical perspective <ref> [RW84] </ref>.) 13 6.2.2 Gradient descent It is possible to compute an expression for the gradient of the log-likelihood (e.g., [XJ96] gives the derivatives w.r.t. ~ and for the mixture of Gaussians model, and [BKRK97] gives the derivative for discrete BNs), and hence to use gradient-based learning methods.
Reference: [SK89] <author> R. Shachter and C. R. Kenley. </author> <title> Gaussian influence diagrams. </title> <journal> Managment Science, </journal> <volume> 35(5) </volume> <pages> 527-550, </pages> <year> 1989. </year>
Reference-contexts: We now show how to to compute these parameters as a function of the the local parameters of each node. 3.1 Scalar case We start by considering the scalar case, as in <ref> [SK89] </ref>. First we compute and then ~. Construct a diagonal matrix containing the variances of each node, D = diag ( 2 i ), and another containing the standard deviations, S = diag ( i ).
Reference: [TSM85] <author> D. M. Titterington, A. F. M. Smith, and U. E. Makov. </author> <title> Statistical analysis of finite mixture distributions. </title> <publisher> Wiley, </publisher> <year> 1985. </year>
Reference-contexts: If we don't do strong triangulation, the number of mixture components becomes exponential in the length of the sequence. The standard approach (see e.g., <ref> [TSM85, BSL93, Kim94, WH97] </ref>) is to "collapse" the mixture into k components.
Reference: [WH97] <author> Mike West and Jeff Harrison. </author> <title> Bayesian forecasting and dynamic models. </title> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: If we don't do strong triangulation, the number of mixture components becomes exponential in the length of the sequence. The standard approach (see e.g., <ref> [TSM85, BSL93, Kim94, WH97] </ref>) is to "collapse" the mixture into k components.
Reference: [Whi90] <author> J. Whittaker. </author> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: Gaussian graphical models In this section we will show that X i ? X j j (the rest) () K ij = 0 (5) where K = 1 is the inverse covariance matrix (also called the precision matrix) of the joint distribution, and "the rest" means all the other nodes <ref> [Whi90, Edw95] </ref>.
Reference: [XJ96] <author> L. Xu and M. I. Jordan. </author> <title> On convergence properties of the EM algorithm for Gaussian mixtures. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1996. </year> <month> 16 </month>
Reference-contexts: adding a negative fraction of the old values to the new values, but can be justified both from a theoretical on-line learning perspective [BKS97] or from a more classical statistical perspective [RW84].) 13 6.2.2 Gradient descent It is possible to compute an expression for the gradient of the log-likelihood (e.g., <ref> [XJ96] </ref> gives the derivatives w.r.t. ~ and for the mixture of Gaussians model, and [BKRK97] gives the derivative for discrete BNs), and hence to use gradient-based learning methods. The hard part is maintaining the constraints on the parameters. <p> One way is to start out with a valid initial guess, and then to take sufficiently small steps in parameter space. However, this can lead to slow convergence. Indeed, experiments with the mixture of Gaussians model <ref> [XJ96] </ref> and discrete Bayes nets [Mur97] indicate that EM, while technically a first order method, often does better than nominally faster gradient-based methods, such as conjugate gradient or quasi-Newton.
References-found: 35

