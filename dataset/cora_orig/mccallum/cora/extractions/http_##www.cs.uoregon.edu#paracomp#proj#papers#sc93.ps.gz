URL: http://www.cs.uoregon.edu/paracomp/proj/papers/sc93.ps.gz
Refering-URL: http://www.cs.uoregon.edu/paracomp/proj/tau/papers.html
Root-URL: http://www.cs.uoregon.edu
Email: Francois.Bodin@cs.irisa.fr fbeckman,gannon,yangg@cs.indiana.edu fkesavans,malony,mohrg@cs.uoregon.edu  
Title: Implementing a Parallel C++ Runtime System for Scalable Parallel Systems 1  
Author: F. Bodin P. Beckman, D. Gannon, S. Yang S. Kesavan, A. Malony, B. Mohr 
Address: France Bloomington, Indiana 47405 Eugene, Oregon 97403  
Affiliation: Irisa Dept. of Comp. Sci. Dept. of Comp. and Info. Sci. University of Rennes Indiana University University of Oregon Rennes,  
Date: November 15-19, 1993.  
Note: To appear in: Proceedings of the Supercomputing '93 Conference, Portland, Oregon,  
Abstract: pC++ is a language extension to C++ designed to allow programmers to compose "concurrent aggregate" collection classes which can be aligned and distributed over the memory hierarchy of a parallel machine in a manner modeled on the High Performance Fortran Forum (HPFF) directives for Fortran 90. pC++ allows the user to write portable and efficient code which will run on a wide range of scalable parallel computer systems. The first version of the compiler is a preprocessor which generates Single Program Multiple Data (SPMD) C++ code. Currently, it runs on the Thinking Machines CM-5, the Intel Paragon, the BBN TC2000, the Kendall Square Research KSR-1, and the Sequent Symmetry. In this paper we describe the implementation of the runtime system, which provides the concurrency and communication primitives between objects in a distributed collection. To illustrate the behavior of the runtime system we include a description and performance results on four benchmark programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> BBN Advanced Computer Inc., </institution> <address> Cambridge, MA. </address> <booktitle> Inside the TC2000, </booktitle> <year> 1989. </year>
Reference-contexts: The processor objects then exchange local element addresses to build the full collection element table. * Barrier synchronization: The barrier implementation is chosen from optimized hard ware/software mechanisms on the target system. 3.2.2 The BBN TC2000 The BBN TC2000 <ref> [1] </ref> is a scalable multiprocessor architecture which can support up to 512 computational nodes. The nodes are interconnected by a variant of a multistage cube network referred to as the butterfly switch.
Reference: [2] <author> D. Hensgen, R. Finkel, and U. Manber. </author> <title> Two Algorithms for Barrier Synchronization. </title> <journal> Int'l. Journal of Parallel Programming, </journal> <volume> 17(1) </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: Currently, the TC2000 pC++ runtime system allocates collection elements in the "owner's" node memory with a write-through caching strategy. The TC2000 does not have special barrier synchronization hardware. Instead, we implemented the logarithmic barrier algorithm described in <ref> [2] </ref>. Our implementation requires approximately 70 microseconds to synchronize 32 nodes. This time scales as the log of the number of processors. 3.2.3 The Sequent Symmetry The Sequent Symmetry [3] is a bus-based, shared memory multiprocessor that can be configured with up to 30 processors.
Reference: [3] <institution> Sequent Computer Systems, Inc. Symmetry Multiprocessor Architecture Overview, </institution> <year> 1992. </year>
Reference-contexts: The TC2000 does not have special barrier synchronization hardware. Instead, we implemented the logarithmic barrier algorithm described in [2]. Our implementation requires approximately 70 microseconds to synchronize 32 nodes. This time scales as the log of the number of processors. 3.2.3 The Sequent Symmetry The Sequent Symmetry <ref> [3] </ref> is a bus-based, shared memory multiprocessor that can be configured with up to 30 processors. The Symmetry architecture provides hardware cache consistency through a copy-back policy and user-accessible hardware locking mechanisms for synchronization.
Reference: [4] <author> S. Frank, H. Burkhardt III, J. Rothnie, </author> <title> The KSR1: Bridging the Gap Between Shared Memory and MPPs, </title> <booktitle> Proc. </booktitle> <address> Compcon'93, San Francisco, </address> <year> 1993, </year> <pages> pp. 285-294. </pages>
Reference-contexts: The memory is physically distributed on the nodes and organized as a hardware-coherent distributed cache <ref> [4] </ref>. The machine can scale to 1088 nodes, in clusters of 32. Nodes in a cluster are interconnected with a pipelined slotted ring. Clusters are connected by a higher-level ring. Each node has a superscalar 64-bit custom processor, a 0.5 Mbyte local sub-cache, and 32 Mbyte local cache memory.
Reference: [5] <author> T. von Eiken, D. Culler, S. Goldstein, K. Schauser, </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation, </title> <booktitle> Proc. 19th Int'l Symp. on Computer Architecture, </booktitle> <address> Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Each processor is a Sparc CPU together with four vector units. (In the experiments described here the vector units are not used.) The basic communication library is CMMD 3.0 Beta which is based on a re-implementation of the Berkeley CMAM Active Message layer <ref> [5, 6] </ref>. The active message layer provides very good support for short messages that consist of a pointer to a local function to execute and three words of argument.
Reference: [6] <author> D. Culler, T. von Eiken, </author> <title> CMAM Introduction to CM-5 Active Message communication layer, man page, CMAM distribution. </title>
Reference-contexts: Each processor is a Sparc CPU together with four vector units. (In the experiments described here the vector units are not used.) The basic communication library is CMMD 3.0 Beta which is based on a re-implementation of the Berkeley CMAM Active Message layer <ref> [5, 6] </ref>. The active message layer provides very good support for short messages that consist of a pointer to a local function to execute and three words of argument.
Reference: [7] <author> A. Chien and W. Dally. </author> <title> Concurrent Aggregates (CA), </title> <booktitle> Proceedings of the Second ACM Sigplan Symposium on Principles & Practice of Parallel Programming, </booktitle> <address> Seattle, Washington, </address> <month> March, </month> <year> 1990. </year>
Reference-contexts: To illustrate the behavior of the runtime system we include performance results for four benchmark programs. 2 A Brief Introduction to pC++ The basic concept behind pC++ is the notion of a distributed collection, which is a type of concurrent aggregate "container class" <ref> [7, 9] </ref>. More specifically, a collection is a structured set of objects distributed across the processing elements of the computer.
Reference: [8] <author> High Performance Fortran Forum, </author> <title> Draft High Performance Fortran Language Specification, </title> <note> Version 1.0, 1993. Available from titan.cs.rice.edu by ftp. </note>
Reference-contexts: The distribution describes an abstract coordinate system that will be distributed over the available "processor objects" of the target by the runtime system. (In HPF <ref> [8] </ref>, the term template is used to refer to the coordi-nate system. We will avoid this so that there will be no confusion with the C++ keyword template.) * A function object called the Alignment. This function maps collection elements to the abstract coordinate system of the Distribution object.
Reference: [9] <author> J. K. Lee, </author> <title> Object Oriented Parallel Programming Paradigms and Environments For Supercomputers, </title> <type> Ph.D. Thesis, </type> <institution> Indiana University, Bloomington, Indi-ana, </institution> <month> June </month> <year> 1992. </year>
Reference-contexts: To illustrate the behavior of the runtime system we include performance results for four benchmark programs. 2 A Brief Introduction to pC++ The basic concept behind pC++ is the notion of a distributed collection, which is a type of concurrent aggregate "container class" <ref> [7, 9] </ref>. More specifically, a collection is a structured set of objects distributed across the processing elements of the computer.
Reference: [10] <author> J. K. Lee and D. Gannon, </author> <title> Object Oriented Parallel Programming: Experiments and Results Proc. Supercomputing 91, </title> <journal> IEEE Computer Society and ACM SIGARCH, </journal> <year> 1991, </year> <pages> pp. 273-282. </pages>
Reference-contexts: This function maps collection elements to the abstract coordinate system of the Distribution object. The pC++ language has a library of standard collection classes that may be used (or subclassed) by the programmer <ref> [10, 11, 12, 13] </ref>. This includes collection classes such as DistributedArray, Distributed-Matrix, DistributedVector, and DistributedGrid. To illustrate the points above, consider the problem of creating a distributed 5 by 5 matrix of floating point numbers. We begin by building a Distribution.
Reference: [11] <author> D. Gannon and J. K. Lee, </author> <title> Object Oriented Parallelism: </title> <booktitle> pC++ Ideas and Experiments Proc. 1991 Japan Society for Parallel Processing, </booktitle> <pages> pp. 13-23. </pages>
Reference-contexts: This function maps collection elements to the abstract coordinate system of the Distribution object. The pC++ language has a library of standard collection classes that may be used (or subclassed) by the programmer <ref> [10, 11, 12, 13] </ref>. This includes collection classes such as DistributedArray, Distributed-Matrix, DistributedVector, and DistributedGrid. To illustrate the points above, consider the problem of creating a distributed 5 by 5 matrix of floating point numbers. We begin by building a Distribution.
Reference: [12] <author> D. Gannon, J. K. Lee, </author> <title> On Using Object Oriented Parallel Programming to Build Distributed Algebraic Abstractions, </title> <booktitle> Proc. </booktitle> <address> CONPAR/VAPP, Lyon, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: This function maps collection elements to the abstract coordinate system of the Distribution object. The pC++ language has a library of standard collection classes that may be used (or subclassed) by the programmer <ref> [10, 11, 12, 13] </ref>. This includes collection classes such as DistributedArray, Distributed-Matrix, DistributedVector, and DistributedGrid. To illustrate the points above, consider the problem of creating a distributed 5 by 5 matrix of floating point numbers. We begin by building a Distribution.
Reference: [13] <author> D. Gannon, </author> <title> Libraries and Tools for Object Parallel Programming, </title> <booktitle> Proc. CNRS-NSF Workshop on Environments and Tools For Parallel Scientific Computing, 1992, </booktitle> <address> St. Hilaire du Touvet, France. Elsevier, </address> <booktitle> Advances in Parallel Computing, </booktitle> <address> Vol.6. </address>
Reference-contexts: This function maps collection elements to the abstract coordinate system of the Distribution object. The pC++ language has a library of standard collection classes that may be used (or subclassed) by the programmer <ref> [10, 11, 12, 13] </ref>. This includes collection classes such as DistributedArray, Distributed-Matrix, DistributedVector, and DistributedGrid. To illustrate the points above, consider the problem of creating a distributed 5 by 5 matrix of floating point numbers. We begin by building a Distribution.
Reference: [14] <author> K. Li, </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors, </title> <type> Ph.D. Thesis, </type> <institution> Yale University, </institution> <year> 1986. </year>
Reference-contexts: In SVM, the message passing system is used to move pages from one processor to another in the same way a standard VM system moves pages from memory to disk. Though these systems are still experimental <ref> [14, 15] </ref>, they show great promise for providing a support environment for shared memory parallel programming models. Because pC++ is based on a shared name space of collection elements, we use SVM techniques to build the runtime system for the Intel Paragon and the Thinking Machines CM-5.
Reference: [15] <author> Z. Lahjomri and T. Priol, KOAN: </author> <title> a Shared Virtual Memory for the iPSC/2 Hypercube, </title> <booktitle> Proc. </booktitle> <address> CON-PAR/VAPP, Lyon, </address> <month> Sept. </month> <year> 1992. </year>
Reference-contexts: In SVM, the message passing system is used to move pages from one processor to another in the same way a standard VM system moves pages from memory to disk. Though these systems are still experimental <ref> [14, 15] </ref>, they show great promise for providing a support environment for shared memory parallel programming models. Because pC++ is based on a shared name space of collection elements, we use SVM techniques to build the runtime system for the Intel Paragon and the Thinking Machines CM-5. <p> Because pC++ is based on a shared name space of collection elements, we use SVM techniques to build the runtime system for the Intel Paragon and the Thinking Machines CM-5. More specifically, our model is based on ideas from Koan <ref> [15] </ref>. The basic idea is that each collection element has a manager and a owner. The owner of the element is the processor object that contains the element in its local collection.
Reference: [16] <author> M. Lemke and D. Quinlan, </author> <title> a Parallel C++ Array Class Library for Architecture-Independent Development of Numerical Software, </title> <booktitle> Proc. OON-SKI Object Oriented Numerics Conf., </booktitle> <pages> pp. 268-269, </pages> <address> Sun River, Oregon, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Note that the TEClass mechanisms provide a simple and direct way to "wrap" message passing SPMD style C++ routines inside a standard sequential main program. In this way, many of the special libraries of parallel C++ code already designed can be easily integrated into this model <ref> [16, 17] </ref>. 2.2 Collections and Templates The C++ language provides the templates mechanism to build parameterized classes. Collections are very similar to templates defined over TEClasses.
Reference: [17] <author> J. Dongarra, R. Pozo, D. Walker, </author> <title> An Object Oriented Design for High Performance Linear Algebra on Distributed Memory Architectures, </title> <booktitle> Proc. OON-SKI Object Oriented Numerics Conf., </booktitle> <pages> pp. 257-264, </pages> <address> Sun River, Oregon, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: Note that the TEClass mechanisms provide a simple and direct way to "wrap" message passing SPMD style C++ routines inside a standard sequential main program. In this way, many of the special libraries of parallel C++ code already designed can be easily integrated into this model <ref> [16, 17] </ref>. 2.2 Collections and Templates The C++ language provides the templates mechanism to build parameterized classes. Collections are very similar to templates defined over TEClasses.
References-found: 17

