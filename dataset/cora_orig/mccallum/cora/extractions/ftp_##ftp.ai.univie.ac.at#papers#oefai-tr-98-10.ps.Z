URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-98-10.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+98-10
Root-URL: 
Email: fstefan,bernhardg@ai.univie.ac.at  Christoph.Helma@univie.ac.at  
Title: Stochastic Propositionalization of Non-Determinate Background Knowledge  
Author: Stefan Kramer and Bernhard Pfahringer Christoph Helma 
Address: Schottengasse 3, A-1010 Vienna, Austria  Borschkegasse 8a, A-1090 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  Institute for Tumor Biology Cancer Research University of Vienna,  
Abstract: Both propositional and relational learning algorithms require a good representation to perform well in practice. Usually such a representation is either engineered manually by domain experts or derived automatically by means of so-called constructive induction. Inductive Logic Programming (ILP) algorithms put a somewhat less burden on the data engineering effort as they allow for a structured, relational representation of background knowledge. In chemical and engineering domains, a common representational device for graph-like structures are so-called non-determinate relations. Manually engineered features in such domains typically test for or count occurrences of specific substructures having specific properties. However, representations containing non-determinate relations pose a serious efficiency problem for most standard ILP algorithms. Therefore, we have devised a stochastic algorithm to automatically derive features from non-determinate background knowledge. The algorithm conducts a top-down search for first-order clauses, where each clause represents a binary feature. These features are used instead of the non-determinate relations in a subsequent induction step. In contrast to comparable algorithms search is not class-blind and there are no arbitrary size restrictions imposed on candidate clauses. An empirical investigation in three chemical domains supports the validity and usefulness of the proposed algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> H. Blockeel and L. DeRaedt. </author> <title> Top-down induction of logical decision trees. </title> <type> Technical Report Report CW 247, </type> <institution> Katholieke Universiteit Leuven, Belgium, </institution> <year> 1997. </year>
Reference: <author> A. Blum. </author> <title> Learning boolean functions in an infinite attribute space. </title> <journal> Machine Learning, </journal> <volume> 9(4), </volume> <year> 1992. </year>
Reference-contexts: Currently, there exists no fully worked-out theory of propositionalization in general and of the stochastic approach in particular. For the set-valued feature approach, (Cohen 1996) employed Blum's infinite attribute model <ref> (Blum 1992) </ref> as a theoretical basis. We conjecture that this model could serve as a basis for certain other types of propositionalization as well.
Reference: <editor> W.W. Cohen. Pac-learning nondeterminate clauses. </editor> <booktitle> In Proc. Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <year> 1994. </year>
Reference-contexts: In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ((Lavrac and Dzeroski 1994), <ref> (Cohen 1994) </ref>, (Cohen 1996)). But even if an equivalent transformation exists theoretically, its size (measured as the number of propositional features generated) is usually huge in most interesting application domains. Thus, any subsequent propositional learner will need to employ some form of feature subset selection. <p> DINUS (Lavrac and Dzeroski 1994) weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS. Cohen <ref> (Cohen 1994) </ref> introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches (Silverstein and Pazzani 1991) of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche.
Reference: <author> W.W. Cohen. </author> <title> Learning trees and rules with set-valued features. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <pages> pages 709-716, </pages> <year> 1996. </year>
Reference-contexts: In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ((Lavrac and Dzeroski 1994), (Cohen 1994), <ref> (Cohen 1996) </ref>). But even if an equivalent transformation exists theoretically, its size (measured as the number of propositional features generated) is usually huge in most interesting application domains. Thus, any subsequent propositional learner will need to employ some form of feature subset selection. <p> The necessary changes would include a built-in bias towards clauses with a high coverage of positive examples and the addition of a noise-handling mechanism. Currently, there exists no fully worked-out theory of propositionalization in general and of the stochastic approach in particular. For the set-valued feature approach, <ref> (Cohen 1996) </ref> employed Blum's infinite attribute model (Blum 1992) as a theoretical basis. We conjecture that this model could serve as a basis for certain other types of propositionalization as well.
Reference: <author> D.J. Cook and L.B. Holder. </author> <title> Substructure discovery using minimum description length and background knowledge. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1 </volume> <pages> 231-255, </pages> <year> 1994. </year>
Reference-contexts: Interestingly, STILL is also a stochastic algorithm, but stochastic search is applied in a totally different manner in STILL, namely in the matching phase between features and examples. STILL also uses a totally different representation, which effectively yields black-box classifiers instead of intelligible features. SUBDUE <ref> (Cook and Holder 1994) </ref> is an MDL-based algorithm for substructure discovery in graphs. It differs from SP in that its search is class-blind, and that it basically performs a beam search (the current clauses are the parent clauses of the next generation).
Reference: <author> S. Dzeroski and B. Kompare, </author> <year> 1995. </year> <type> Personal Communication. </type>
Reference-contexts: We used the dataset containing 188 instances (compounds) and performed 10-fold cross-validation. In the second domain, we are trying to predict the half-rate of surface water aerobic aqueous biodegradation in hours <ref> (Dzeroski and Kompare 1995) </ref>. The class to learn is whether this quantity exceeds a certain threshold. The dataset contains 62 chemicals, and we performed 6-fold cross-validation in our tests.
Reference: <author> P. Geibel and F. Wysotzki. </author> <title> Relational learning with decision trees. </title> <booktitle> In Proc. Twelfth European Conference on Artificial Intelligence (ECAI-96), </booktitle> <pages> pages 428-432, </pages> <year> 1996. </year>
Reference-contexts: Cohen (Co-hen 1996) introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge. Geibel and Wysotzki <ref> (Geibel and Wysotzki 1996) </ref> propose a method for feature construction in a graph-based representation. The features are obtained through fixed-length paths in the neighborhood of a node in the graph. The constructed features are either "context-dependent node attributes of depth n" or "context dependent edge attributes of depth n".
Reference: <author> A. Giordana, L. Saitta, and F. Zini. </author> <title> Learning disjunctive concepts by means of genetic algorithms. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 96-104, </pages> <year> 1994. </year>
Reference-contexts: The overall approach would not work, if the clauses in the population were the same extensionally. In other words, there would be no "division of labour" among the clauses (C3). (This is also the motivation for the universal suffrage selection algorithm presented in <ref> (Giordana et al. 1994) </ref>.) We took a simple extension-driven approach to solve this problem: the algorithm only considers those refinements that yield clauses with an extension different from the extensions of clauses in the current population. The extension only has to differ in one example to make it different. <p> For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge. In contrast to SP, REGAL <ref> (Giordana et al. 1994) </ref> is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning.
Reference: <author> R.D. King and A. Srinivasan. </author> <title> Prediction of rodent carcinogenicity bioassays from molecular structure using inductive logic programming. Environmental Health Perspectives, </title> <year> 1997. </year>
Reference-contexts: The class to learn is whether this quantity exceeds a certain threshold. The dataset contains 62 chemicals, and we performed 6-fold cross-validation in our tests. The third database, provided by King and Srinivasan <ref> (King and Srinivasan 1997) </ref>, contains information about the carcinogenicity of 330 compounds, as classified by the National Institute of Environmental Health Sciences (NIEHS). The NIEHS has classified these chemicals as non-carcinogenic, equivocal and carcinogenic. Here, we performed 5-fold cross-validation. <p> The entries for 'Progol f. BK' are the results by Progol with the full available background knowledge, including global features describing the molecule such as the molecular weight. The results for mutagenicity and for carcinogenicity have been taken from (Srinivasan et al. 1995) and <ref> (King and Srinivasan 1997) </ref>, respectively. In general, the reason for the performance of Pro-golFC is that it heavily relies on the quality of the Pro-gol theories induced from relational background knowledge only. As can be seen in Table 1 as well (in the entries for 'Progol r.
Reference: <author> M. Kovacic. MILP: </author> <title> a stochastic approach to Inductive Logic Programming. </title> <booktitle> In Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pages 123-138, </pages> <year> 1994. </year>
Reference-contexts: In contrast to SP, REGAL (Giordana et al. 1994) is a concept learning algorithm. It is a full-fledged genetic algorithm. REGAL's universal suffrage selection algorithm is the first extension-driven approach to stochastic search in machine learning. MILP <ref> (Kovacic 1994) </ref> is an ILP algorithm that performs stochastic search for single clauses to overcome the myopic behavior of greedy search. The outer loop of the algorithm employs the more conventional separate-and-conquer strategy.
Reference: <author> S. Kramer, B. Pfahringer, and C. Helma. </author> <title> Mining for causes of cancer: Machine learning experiments at various levels of detail. </title> <booktitle> In Proceedings of the Third International Conference on Knowledge Discovery and Data Mining (KDD-97), </booktitle> <address> Menlo Park, CA, 1997. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The NIEHS has classified these chemicals as non-carcinogenic, equivocal and carcinogenic. Here, we performed 5-fold cross-validation. Note that the results here cannot be compared with those in <ref> (Kramer et al. 1997) </ref>, because we use all 330 compounds including those of the first round of the so-called predictive toxicology evaluation (PTE-1), and furthermore we apply the algorithms not to a three-class problem, but to a two-class problem (equivocals are counted as non-carcinogenic). Mutag. Acc. Mean () Biod. Acc.
Reference: <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming. </title> <publisher> Ellis Horwood, </publisher> <address> Chichester, UK, </address> <year> 1994. </year>
Reference-contexts: Introduction A very large number of algorithms require a propositional representation, whereas many real-world learning problems are essentially relational. To bridge this gap, various researchers (e.g., <ref> (Lavrac and Dzeroski 1994) </ref>) have proposed a transformational approach. This type of transformation is called propositionaliza-tion. Propositionalization is also a necessity for types of background knowledge that even full-fledged ILP learners cannot handle efficiently, namely background knowledge containing non-determinate literals. <p> Features are constructed automatically, and yet are of a quality comparable to the one of expert-provided features. Related Work In this section we briefly review related work on propo-sitionalization and stochastic search in machine learning and Inductive Logic Programming. LINUS <ref> (Lavrac and Dzeroski 1994) </ref> was the first system to transform a relational representation into a propositional representation. The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. This implies that no recursion is allowed, and that no new variables may be introduced. <p> LINUS <ref> (Lavrac and Dzeroski 1994) </ref> was the first system to transform a relational representation into a propositional representation. The hypothesis language of LINUS is restricted to function-free constrained DHDB (deductive hierarchical database) clauses. This implies that no recursion is allowed, and that no new variables may be introduced. DINUS (Lavrac and Dzeroski 1994) weakens the language bias of LINUS so that the system can learn clauses with a restricted form of new variables, namely determinate variables. This allows for the same transformation approach as the one taken in LINUS.
Reference: <author> S. Muggleton. </author> <title> Inverse Entailment and Progol. </title> <journal> New Generation Computing, </journal> <volume> 13 </volume> <pages> 245-286, </pages> <year> 1995. </year>
Reference-contexts: In the experiments, we compared SP with the currently only other general method for propositionaliza-tion of non-determinate background knowledge that has been presented in (Srinivasan and King 1996). This method constructs features based on hypotheses returned by Progol <ref> (Muggleton 1995) </ref>. For each clause, each input-output connected subset of literals is used to define a feature. In the following, we will refer to that method as ProgolFC (Progol feature construction). After propositionalization through ProgolFC, we applied C4.5. <p> This method is restricted to graphs, and using fixed-length paths obviously becomes prohibitive for large n. Srinivasan and King (Srinivasan and King 1996) presented a method for feature construction based on hypotheses returned by Progol <ref> (Muggleton 1995) </ref>. For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge. In contrast to SP, REGAL (Giordana et al. 1994) is a concept learning algorithm.
Reference: <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: Second, no negation is used in the clauses, since it often is detrimental to comprehensibility. Experimental Results In the following experiments, we applied C4.5 (Quinlan 1993) in the learning step after propositionalization. Family Domain We conducted experiments in the family domain <ref> (Quinlan 1990) </ref> with the relations son (A,B) and niece (A,B). Although the family domain is determinate, it helped us see which things work and which do not.
Reference: <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: Second, no negation is used in the clauses, since it often is detrimental to comprehensibility. Experimental Results In the following experiments, we applied C4.5 <ref> (Quinlan 1993) </ref> in the learning step after propositionalization. Family Domain We conducted experiments in the family domain (Quinlan 1990) with the relations son (A,B) and niece (A,B). Although the family domain is determinate, it helped us see which things work and which do not.
Reference: <author> J.R. Quinlan. </author> <title> The minimum description length principle and categorical theories. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <address> San Mateo, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In (2), poss (i) is the number of possible refinements when selecting literal l i . The coding cost of the errors (3) uses an encoding of selections of subsets of a set that is due to Cameron-Jones <ref> (Quinlan 1994) </ref>.
Reference: <author> J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: The definition of the fitness of a clause is shown in Description Length (MDL) principle <ref> (Rissanen 1978) </ref>. This principle allows taking into account a model's accuracy and its complexity simultaneously. The key idea is to measure the joint cost of coding a model (in our case a clause) and of coding the data in terms of that model.
Reference: <author> M. Sebag and C. </author> <title> Rouveirol. Tractable induction and classification in first order logic via stochastic matching. </title> <booktitle> In Proc. Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97), </booktitle> <pages> pages 888-893, </pages> <address> San Mateo, CA, 1997. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The outer loop of the algorithm employs the more conventional separate-and-conquer strategy. In principle, SP is capable of exploring features of arbitrary length, but in practice the constraints are strong enough to keep search focused. The only other algorithm capable of efficiently exploring arbitrary-length features is STILL <ref> (Sebag and Rouveirol 1997) </ref>. Interestingly, STILL is also a stochastic algorithm, but stochastic search is applied in a totally different manner in STILL, namely in the matching phase between features and examples. STILL also uses a totally different representation, which effectively yields black-box classifiers instead of intelligible features.
Reference: <author> G. Silverstein and M.J. Pazzani. </author> <title> Relational cliches: Constraining constructive induction during relational learning. In L.A. </title> <editor> Birnbaum and G.C. Collins, editors, </editor> <booktitle> Machine Learning: Proceedings of the Eighth International Workshop (ML91), </booktitle> <pages> pages 203-207, </pages> <address> San Mateo, CA, 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: On the other hand, complete search needs some arbitrary length restriction on clauses to make induction practical due to the extremely large search spaces encountered otherwise. Current solutions either rely on manually engineered features or employ solutions based on look-ahead, for instance by so-called relational cliches <ref> (Silverstein and Pazzani 1991) </ref>. In general, full equivalence between the original and the transformed problem can only be achieved for certain subsets of first-order logic and certain types of background knowledge ((Lavrac and Dzeroski 1994), (Cohen 1994), (Cohen 1996)). <p> This allows for the same transformation approach as the one taken in LINUS. Cohen (Cohen 1994) introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches <ref> (Silverstein and Pazzani 1991) </ref> of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney (Turney 1995) described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation.
Reference: <author> A. Srinivasan and R.D. King. </author> <title> Feature construction with Inductive Logic Programming: a study of quantitative predictions of chemical activity aided by structural attributes. </title> <booktitle> In Proceedings of the 6th International Workshop on Inductive Logic Programming (ILP-96), </booktitle> <year> 1996. </year>
Reference-contexts: The bonds are defined as relations between atoms, and also have types (according to QUANTA). In the experiments, we compared SP with the currently only other general method for propositionaliza-tion of non-determinate background knowledge that has been presented in <ref> (Srinivasan and King 1996) </ref>. This method constructs features based on hypotheses returned by Progol (Muggleton 1995). For each clause, each input-output connected subset of literals is used to define a feature. In the following, we will refer to that method as ProgolFC (Progol feature construction). <p> The constructed features are either "context-dependent node attributes of depth n" or "context dependent edge attributes of depth n". This method is restricted to graphs, and using fixed-length paths obviously becomes prohibitive for large n. Srinivasan and King <ref> (Srinivasan and King 1996) </ref> presented a method for feature construction based on hypotheses returned by Progol (Muggleton 1995). For each clause, each input-output connected subset of literals is used to define a feature. In contrast to all previously discussed methods, this method works for all types of background knowledge.
Reference: <author> A. Srinivasan, S. Muggleton, R.D. King, and M. Sternberg. Mutagenesis: </author> <title> ILP experiments in a non-determinate biological domain. </title> <booktitle> In Proceedings of the Fourth International Workshop on Inductive Logic Programming (ILP-94), GMD-Studien Nr. </booktitle> <volume> 237, </volume> <pages> pages 217-232, </pages> <year> 1994. </year>
Reference-contexts: Experiments in Three Chemical Domains We conducted experiments in three chemical domains to validate the usefulness of the approach. In the first domain, the goal is to predict whether or not a chemical is mutagenic, i.e., if it is harmful to DNA. (For details see <ref> (Srinivasan et al. 1994) </ref> and (Srinivasan et al. 1995)). We used the dataset containing 188 instances (compounds) and performed 10-fold cross-validation. In the second domain, we are trying to predict the half-rate of surface water aerobic aqueous biodegradation in hours (Dzeroski and Kompare 1995).
Reference: <author> A. Srinivasan, S. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by Inductive Logic Programming systems. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 199-230. </pages> <address> Katholieke Uni-versiteit Leuven, </address> <year> 1995. </year>
Reference-contexts: In the first domain, the goal is to predict whether or not a chemical is mutagenic, i.e., if it is harmful to DNA. (For details see (Srinivasan et al. 1994) and <ref> (Srinivasan et al. 1995) </ref>). We used the dataset containing 188 instances (compounds) and performed 10-fold cross-validation. In the second domain, we are trying to predict the half-rate of surface water aerobic aqueous biodegradation in hours (Dzeroski and Kompare 1995). <p> The entries for 'Progol f. BK' are the results by Progol with the full available background knowledge, including global features describing the molecule such as the molecular weight. The results for mutagenicity and for carcinogenicity have been taken from <ref> (Srinivasan et al. 1995) </ref> and (King and Srinivasan 1997), respectively. In general, the reason for the performance of Pro-golFC is that it heavily relies on the quality of the Pro-gol theories induced from relational background knowledge only.
Reference: <author> P. Turney. </author> <title> Low size-complexity Inductive Logic Programming: the East-West challenge considered as a problem in cost-sensitive classification. </title> <booktitle> In Proceedings of the 5th International Workshop on Inductive Logic Programming (ILP-95), </booktitle> <pages> pages 247-263. </pages> <institution> Katholieke Universiteit Leuven, </institution> <year> 1995. </year>
Reference-contexts: Cohen (Cohen 1994) introduces a new restriction on non-determinate free variables called "locality constraint". This can be thought of in terms of schemata or cliches (Silverstein and Pazzani 1991) of literals. Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney <ref> (Turney 1995) </ref> described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia (Zucker and Ganascia 1996) proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example".
Reference: <author> J.D. Zucker and J.G. Ganascia. </author> <title> Representation changes for efficient learning in structural domains. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 543-551, </pages> <year> 1996. </year>
Reference-contexts: Newly introduced non-determinate variables may only be reused in literals within the same instantiated schema or cliche. Turney (Turney 1995) described a special purpose program for translating the "trains" problem of the East-West challenge into a propositional representation. Zucker and Ganascia <ref> (Zucker and Ganascia 1996) </ref> proposed to decompose structured examples into several learning examples, which are descriptions of parts of what they call the "natural example". Cohen (Co-hen 1996) introduced the notion of "set-valued features", which can be used to transform certain types of background knowledge.
References-found: 24

