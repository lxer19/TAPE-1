URL: http://theory.stanford.edu/~dabo/papers/grouprep.ps.gz
Refering-URL: http://theory.stanford.edu/~dabo/publications.html
Root-URL: 
Email: dabo@cs.princeton.edu  
Title: Learning Using Group Representations (Extended Abstract)  
Author: Dan Boneh 
Note: Supported by NSF grant CCR-8858788  
Address: Princeton, NJ 08544  
Affiliation: Department of Computer Science Princeton University  
Abstract: We consider the problem of learning functions over a fixed distribution. An algorithm by Kushilevitz and Mansour [7] learns any boolean function over f0; 1g n in time polynomial in the L 1 -norm of the Fourier transform of the function. We show that the KM-algorithm is a special case of a more general class of learning algorithms. This is achieved by extending their ideas using representations of finite groups. We introduce some new classes of functions which can be learned using this generalized KM algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Blum A., Furst M., Jackson J., Kearns M., Mansour Y., and Rudich S. </author> <title> Weakly learning DNF 10 and characterizing statistical query learning using fourier analysis. </title> <booktitle> In Proc. 26th Annu. ACM Sympos. Theory Comput. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries. <p> Theorem 3.3 tells us that B G is an orthonormal set in W. By theorem 3.1, the number of functions in B G is jGj. Hence, B G is an orthonormal basis of W. Example The orthonormal basis generated by the representation of Q is: <ref> [1; 1; 1; 1; 1; 1; 1; 1] </ref> 2 [1; 1; 0; 0; 0; 0; i; i] p [1; 1; 1; 1; 1; 1; 1; 1] 2 [0; 0; 1; 1; i; i; 0; 0] p where each vector is a function b (x) whose values are [b (1); b (1); <p> By theorem 3.1, the number of functions in B G is jGj. Hence, B G is an orthonormal basis of W. Example The orthonormal basis generated by the representation of Q is: [1; 1; 1; 1; 1; 1; 1; 1] 2 <ref> [1; 1; 0; 0; 0; 0; i; i] </ref> p [1; 1; 1; 1; 1; 1; 1; 1] 2 [0; 0; 1; 1; i; i; 0; 0] p where each vector is a function b (x) whose values are [b (1); b (1); b (i); b (i); b (j); b (j); b <p> Hence, B G is an orthonormal basis of W. Example The orthonormal basis generated by the representation of Q is: <ref> [1; 1; 1; 1; 1; 1; 1; 1] </ref> 2 [1; 1; 0; 0; 0; 0; i; i] p [1; 1; 1; 1; 1; 1; 1; 1] 2 [0; 0; 1; 1; i; i; 0; 0] p where each vector is a function b (x) whose values are [b (1); b (1); b (i); b (i); b (j); b (j); b (k); b (k)]. <p> Hence, B G is an orthonormal basis of W. Example The orthonormal basis generated by the representation of Q is: [1; 1; 1; 1; 1; 1; 1; 1] 2 [1; 1; 0; 0; 0; 0; i; i] p [1; 1; 1; 1; 1; 1; 1; 1] 2 <ref> [0; 0; 1; 1; i; i; 0; 0] </ref> p where each vector is a function b (x) whose values are [b (1); b (1); b (i); b (i); b (j); b (j); b (k); b (k)]. <p> It would be interesting to determine whether the results in [2] regarding decision trees generalize to these settings. We mention a few more applications of these methods. The results for weak learning of polynomial size DNF reported in <ref> [1] </ref> may be extended to arbitrary groups of the form G = H n for some fixed group H. The implication is that it is possible to generate many different weak approximations for DNF formulae by varying the group H. Of course, all these different approximations are correlated.
Reference: [2] <author> M. Bellare. </author> <title> A technique for upper bounding the spectral norm with applications to learning. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 62-70. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries. <p> The proof of this theorem is given in the appendix. This result generalizes the result in [7] to trees with arbitrary group operations. It would be interesting to determine whether the results in <ref> [2] </ref> regarding decision trees generalize to these settings. We mention a few more applications of these methods. The results for weak learning of polynomial size DNF reported in [1] may be extended to arbitrary groups of the form G = H n for some fixed group H.
Reference: [3] <author> A.H. Clifford. </author> <title> Representations induced in an invariant subgroup. </title> <journal> Ann. of Math., </journal> <volume> 38(2) </volume> <pages> 533-550, </pages> <year> 1937. </year>
Reference-contexts: The product tower of G plays an important role in the algorithm. It is worth noting that the algorithm works for any p-group G by using a principle composition series 5 of the group instead of a product tower. This requires an application of Clifford theory <ref> [3] </ref> and will not be presented in this abstract. First we describe how to implement step 3 of the algorithm. Given a representation we wish to find all the entries (k; l) of the matrix ^ f such that j ( ^ f ) k;l j &gt; fi.
Reference: [4] <author> J. Dixon. </author> <title> Computing irreducible representations of groups. </title> <journal> Math. Comp., </journal> <volume> 24 </volume> <pages> 707-712, </pages> <year> 1970. </year>
Reference-contexts: It is worth noting that the generation of arbitrary 2-groups of a given size is quite easy. It can be carried out by picking an arbitrary presentation of a 2-groups. Once the group is determined it is possible to compute its representations using an algorithm due to Dixon <ref> [4] </ref>. Acknowledgments I wish to thank Richard Lipton for helpful discussions on this subject. I am also grateful to Herve Bronnimann for his help in reviewing the manuscript.
Reference: [5] <author> L. Dornhoff. </author> <title> Group Representation Theory, Part A. </title> <publisher> Mercel Dekker, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: There exists a basis of V for which the matrix representing (g) is unitary 1 for all g 2 G. Further more, when G is a p-group 2 there exists a basis of V for which the matrix representing (g) is monomial 3 for all g 2 G. See <ref> [5, p. 77] </ref> for a proof of the second part of the theorem. The matrices representing 5 above are monomial. From now on, whenever we speak of the matrix of a representation we will assume that some basis satisfying the assertion of Theorem 3.2 has been fixed.
Reference: [6] <author> J. Jackson. </author> <title> An efficient membership-query algorithm for learning DNF with respect to the uniform distribution. </title> <booktitle> In Proc. of the 35th Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 42-53. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries. <p> 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory [8, 7, 9, 2, 1, 6]. This approach culminated in a recent result due to Jackson <ref> [6] </ref> showing that polynomial size DNF formula can be learned in polynomial time using membership queries. In an influential paper, Kushilevitz and Mansour [7] presented an algorithm for learning boolean decision trees over the uniform distribution. <p> Clearly the most useful group G will be the one which has the lowest L G (f ). Optionally, instead of picking the best approximation, one can use boosting methods similar to the one used in <ref> [6] </ref>. 6 Conclusions The main result of this paper is to show that the KM algorithm is not restricted to using the orthonormal basis of parity functions. We showed that it can use many different orthonormal bases.
Reference: [7] <author> E. Kushilevitz and Y. Mansour. </author> <title> Learning decision trees using the Fourier spectrum. </title> <booktitle> In Proc. of the 23rd Symposium on Theory of Computing, </booktitle> <pages> pages 455-464. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries. <p> This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries. In an influential paper, Kushilevitz and Mansour <ref> [7] </ref> presented an algorithm for learning boolean decision trees over the uniform distribution. In fact, they prove a stronger result showing that any boolean function on f0; 1g n can be learned in time polynomial in the L 1 norm of its Fourier transform. <p> The algorithm is polynomial if its running time is polynomial in n; 1 * ; log 1 Before we explain the results of this paper, we briefly describe the approach taken in <ref> [7] </ref>. The set of functions f : f0; 1g n ! C forms a vector space over C. We denote this vector space by W n . Clearly the dimension of W n is 2 n . <p> It has a natural inner product defined by: (f; g) = 2 n x2f0;1g n where z denotes the complex conjugate of z. As usual, the norm of a function f 2 W n is k f k 2 = (f; f ). The algorithm in <ref> [7] </ref> learns a function f by learning its projections on an orthonormal basis of W n . The orthonormal basis used is made up of all parity functions on n variables. <p> Define the L 1 norm of a function with respect to P as L P (f ) = P k jc k j. The main theorem in <ref> [7] </ref> is that a function f 2 W n can be learned in time polynomial in L P (f ). Note that L P (f ) is dependent on the choice of basis P . <p> That is, impose a group structure on the domain and use the orthonormal basis associated with the group to learn boolean functions over the domain. 2 Learning functions using an orthonormal basis In this section we summarize some basic definitions taken from <ref> [7] </ref> and extend them to arbitrary orthonormal bases. Let G be some finite domain of size s. Usually G will be taken to be f0; 1g n . Let W be the vector space of all functions f : G ! C. <p> When G = H n for H some fixed group of size 2 k one can assign distinct sets of k variables to encode the copies of H. 5 Applications In this section we discuss some applications for the learning algorithm. We begin by generalizing the results in <ref> [7] </ref> for learning decision trees, to decision trees with arbitrary group operations. Definition 5.1 Let G be some group. A G-Decision Tree, denoted G-DT, is a tree where the leaves are labeled f1; 1g and the internal nodes are labeled with normal subgroups of G. <p> Theorem 5.1 A function f : G ! f1; 1g computed by a GDT with m leaves satisfies L G (f ) m. The proof of this theorem is given in the appendix. This result generalizes the result in <ref> [7] </ref> to trees with arbitrary group operations. It would be interesting to determine whether the results in [2] regarding decision trees generalize to these settings. We mention a few more applications of these methods.
Reference: [8] <author> N. Linial, Y. Mansour, and N. Nisan. </author> <title> Constant depth circuits, Fourier transform, </title> <booktitle> and learnabil-ity. In Proc. of the 31st Symposium on the Foundations of Comp. Sci., </booktitle> <pages> pages 574-579. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries.
Reference: [9] <author> Y. Mansour. </author> <title> An o(n log log n ) learning algorithm for DNF under the uniform distribution. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory, </booktitle> <pages> pages 53-59. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction In recent years Fourier analysis has become a valuable tool in learning theory <ref> [8, 7, 9, 2, 1, 6] </ref>. This approach culminated in a recent result due to Jackson [6] showing that polynomial size DNF formula can be learned in polynomial time using membership queries.
Reference: [10] <author> J. Spencer N. Alon. </author> <title> The probabilistic method. </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: To bound the error we use the standard Chernoff bounds (see <ref> [10, p. 240] </ref>). The number of sample points, m, required to make the error less than fi=2 with probability ffi comes out to be O ((1=fi) 10 ffi ). A similar method can be used to approximate the value of the entries in ^ f for a representation of G.
Reference: [11] <author> J.P. Serre. </author> <title> Linear representations of finite groups. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg, New York, </address> <year> 1977. </year> <month> 11 </month>
Reference-contexts: This will enable us to use these methods for learning functions which are not learnable using the basis P . 3 Group Representations In this section we recall some basic definitions from the theory of linear representations of finite groups. For an introduction to this subject see <ref> [11] </ref>. For a vector space V over C we let GL (V ) denote the group of invertible linear transformations of V under composition. For the remainder of the paper G and H will denote finite groups.
References-found: 11

