URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/cupit2_imacs97.ps.gz
Refering-URL: 
Root-URL: 
Email: (hoppjprechelt@ira.uka.de)  
Title: CuPit-2: A Portable Parallel Programming Language for Artificial Neural Networks  
Author: Holger Hopp, Lutz Prechelt 
Address: 76128 Karlsruhe, Germany  
Affiliation: Universitat Karlsruhe, Fakultat fur Informatik,  
Abstract: CuPit-2 is a programming language specifically designed to express neural network learning algorithms. It provides most of the flexibility of general-purpose languages like C/C ++ , but results in much clearer and more elegant programs due to higher expressiveness, in particular for algorithms that change the network topology dynamically (constructive algorithms, pruning algorithms). Furthermore, CuPit-2 programs can be compiled into efficient code for parallel machines; no changes are required in the source program. This article presents a description of the language constructs and reports performance results for an implementation of CuPit-2 on symmetric multiprocessors (SMPs). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Almassy. </author> <title> Ein Compiler fur CONDELA-III. </title> <type> Master's thesis, </type> <institution> Institut fur praktische Informatik, TU Wien, Austria, </institution> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: CuPit-2 programs can be ported to parallel machines without change, though. Compared to pre-programmed simulators, CuPit-2 is more flexible, while the relative efficiency depends on the application. To obtain similar advantages, various proposals have been made for network description languages <ref> [1, 5, 6, 11] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [2] <author> A. A. Chien. </author> <title> Concurrent Aggregates: Supporting Modularity in Massively-Parallel Programs. </title> <publisher> MIT Press Cambridge, </publisher> <address> Massachusetts, London, England, </address> <year> 1993. </year>
Reference-contexts: Just like simulators, such libraries do not seem to be widely used in neural network research. On a parallel computer things become even worse. A few parallel implementations of simulators exist, but most are very restricted or unreliable. There are also high-level parallel languages (e.g. Concurrent Aggregates <ref> [2] </ref>) in which a hand-implementation would be about as easy as in plain C/C ++ , but they cannot yet be translated into sufficiently efficient parallel code.
Reference: [3] <author> H. Hopp and L. Prechelt. CuPit-2: </author> <title> A parallel language for neural algorithms: Language reference and tutorial. </title> <type> TR 4/97, </type> <institution> Univ. Karlsruhe, Fakultat fur Informatik, Germany, </institution> <month> Mar. </month> <year> 1997. </year>
Reference-contexts: In the remainder of this article we informally describe CuPit-2 language constructs and present performance results obtained with a parallel implementation on symmetric multiprocessor machines (SMPs). LANGUAGE OVERVIEW The programming language CuPit-2 <ref> [3] </ref> is based on the observation that neural algorithms predominantly execute local operations (on nodes or connections), reductions (e. g. sum over all weighted incoming connections) and broadcasts (e. g. apply a global parameter to all nodes).
Reference: [4] <author> M. Jabri, E. Tinker, and L. Leerink. MUME: </author> <title> An environment for multi-net and multi-architectures neural simulation. </title> <type> Technical report, </type> <institution> System Engineering and Design Automation Laboratory, University of Sydney, </institution> <address> NSW 2006, Australia, </address> <year> 1993. </year>
Reference-contexts: In particular, most simulators do not provide good support for algorithms that change the network topology during learning. Hence, neural network researchers often end up with hand-implementations. Somewhat in between these two approaches are libraries such as MUME <ref> [4] </ref> or Sesame [7] that provide neural network building blocks for C/C ++ . Just like simulators, such libraries do not seem to be widely used in neural network research. On a parallel computer things become even worse.
Reference: [5] <author> G. Kock and N. Serbedzija. </author> <title> Artificial neural networks: From compact descriptions to C++. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <year> 1994. </year>
Reference-contexts: CuPit-2 programs can be ported to parallel machines without change, though. Compared to pre-programmed simulators, CuPit-2 is more flexible, while the relative efficiency depends on the application. To obtain similar advantages, various proposals have been made for network description languages <ref> [1, 5, 6, 11] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [6] <author> R. R. Leighton. </author> <title> The Aspirin/MIGRAINES neural network software, user's manual, release v6.0. </title> <type> Technical Report MP-91W00050, </type> <institution> MITRE Corp., </institution> <month> Oct. </month> <year> 1999. </year>
Reference-contexts: CuPit-2 programs can be ported to parallel machines without change, though. Compared to pre-programmed simulators, CuPit-2 is more flexible, while the relative efficiency depends on the application. To obtain similar advantages, various proposals have been made for network description languages <ref> [1, 5, 6, 11] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [7] <author> A. Linden and C. Tietz. </author> <title> Combining multiple neural network paradigms and applications using SESAME. </title> <booktitle> In Proc. of the Intl. Joint Conf. on Neural Networks, </booktitle> <address> Baltimore, </address> <month> June </month> <year> 1992. </year> <note> IEEE. </note>
Reference-contexts: In particular, most simulators do not provide good support for algorithms that change the network topology during learning. Hence, neural network researchers often end up with hand-implementations. Somewhat in between these two approaches are libraries such as MUME [4] or Sesame <ref> [7] </ref> that provide neural network building blocks for C/C ++ . Just like simulators, such libraries do not seem to be widely used in neural network research. On a parallel computer things become even worse. A few parallel implementations of simulators exist, but most are very restricted or unreliable.
Reference: [8] <editor> NeuralWorks Reference Guide, </editor> <publisher> NeuralWare Inc. </publisher> <address> http://www.neuralware.com/. </address>
Reference-contexts: INTRODUCTION For researchers in neural network learning algorithms, there are usually two possibilities when it comes to implementing and running an algorithm. They can either adapt or extend a preprogrammed simulator (such as SNNS [13], Xerion [10], NeuroGraph [12], NeuralWorks <ref> [8] </ref> etc.) or use a general-purpose programming language (such as C or C ++ ) to create a complete implementation by hand. Simulators have more powerful interactive facilities than hand-written implementations but often lack flexibility and extensibility.
Reference: [9] <author> M. Riedmiller and H. Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Intl. Conf. on Neural Networks, </booktitle> <pages> pages 586591, </pages> <address> San Francisco, CA, </address> <month> Apr. </month> <year> 1993. </year>
Reference-contexts: If there is more than one processor per network replicate, node parallelism will be used. Our results show performance variation depending on the size of the net work: Example parallelism is bad for networks larger than the cache, see Figure 2. The figure shows RPROP <ref> [9] </ref> performance expressed in Million Connection Updates Per Second (MCUPS) for a large network (nettalk, 203+120+26 nodes, 27480 connections, 200 patterns). The poor example-parallel performance on the Hyper-SPARC system occurs because this network does not fit in the 256KB local processor cache.
Reference: [10] <author> D. van Camp. </author> <title> A users guide for the Xerion neural network simulator version 3.1. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Toronto, Toronto, Canada, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: INTRODUCTION For researchers in neural network learning algorithms, there are usually two possibilities when it comes to implementing and running an algorithm. They can either adapt or extend a preprogrammed simulator (such as SNNS [13], Xerion <ref> [10] </ref>, NeuroGraph [12], NeuralWorks [8] etc.) or use a general-purpose programming language (such as C or C ++ ) to create a complete implementation by hand. Simulators have more powerful interactive facilities than hand-written implementations but often lack flexibility and extensibility.
Reference: [11] <author> A. Weitzenfeld. </author> <booktitle> NSL neural simulation language. In Proc. of the Intl. Workshop on ANN, number 930 in LNCS, </booktitle> <pages> pages 683688, </pages> <address> Malaga-Torremolinos, Spain, June 1995. </address> <publisher> Springer. </publisher>
Reference-contexts: CuPit-2 programs can be ported to parallel machines without change, though. Compared to pre-programmed simulators, CuPit-2 is more flexible, while the relative efficiency depends on the application. To obtain similar advantages, various proposals have been made for network description languages <ref> [1, 5, 6, 11] </ref>. Most of these cover only static network topologies and are not full programming languages, thus still exhibit most of the problems of hand-written implementations.
Reference: [12] <author> P. Wilke and C. Jacob. </author> <title> The NeuroGraph neural network simulator. </title> <booktitle> In Proceedings of MASCOTS'93, </booktitle> <address> San Diego, CA, </address> <year> 1993. </year>
Reference-contexts: INTRODUCTION For researchers in neural network learning algorithms, there are usually two possibilities when it comes to implementing and running an algorithm. They can either adapt or extend a preprogrammed simulator (such as SNNS [13], Xerion [10], NeuroGraph <ref> [12] </ref>, NeuralWorks [8] etc.) or use a general-purpose programming language (such as C or C ++ ) to create a complete implementation by hand. Simulators have more powerful interactive facilities than hand-written implementations but often lack flexibility and extensibility.
Reference: [13] <author> A. Zell, G. Mamier, et al. </author> <title> SNNS User Manual, Version 4.1. </title> <type> Technical Report 6/95, </type> <institution> Institut fur parallele und verteilte Hochstleistungsrechner, Universitat Stuttgart, Germany, </institution> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: INTRODUCTION For researchers in neural network learning algorithms, there are usually two possibilities when it comes to implementing and running an algorithm. They can either adapt or extend a preprogrammed simulator (such as SNNS <ref> [13] </ref>, Xerion [10], NeuroGraph [12], NeuralWorks [8] etc.) or use a general-purpose programming language (such as C or C ++ ) to create a complete implementation by hand. Simulators have more powerful interactive facilities than hand-written implementations but often lack flexibility and extensibility. <p> REPLICATE ME INTO 3, which triplicates the node and all its connections), node self-deletion (REPLICATE ME INTO 0), and node deletion using negative arguments to EXTEND. Furthermore, a powerful library provides operations for input and output of complete networks using SNNS <ref> [13] </ref> style network files. IMPLEMENTATIONS AND EXPERIMENTAL RESULTS We have implemented prototype compilers for the massively parallel MasPar MP-1/MP-2, for sequential computers, and for symmetric multiprocessors (SMP). We focus on the sequential and SMP compilers here. <p> We focus on the sequential and SMP compilers here. For simple feed-forward algorithms Example parallelism Node parallelism Example parallelism Node parallelism (backprop, rprop) the performance of sequential code is a little better than the SNNS simulator <ref> [13] </ref>. CuPit-2 is about 10% to 100% faster than SNNS on Sun SuperSPARC or HyperSPARC, and about the same speed (5%) on DEC Alpha systems. The performance gain increases for algorithms performing connection pruning.
References-found: 13

