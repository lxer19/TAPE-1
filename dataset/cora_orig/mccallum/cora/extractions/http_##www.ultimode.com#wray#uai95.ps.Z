URL: http://www.ultimode.com/wray/uai95.ps.Z
Refering-URL: http://WWW.Ultimode.com/~wray/refs.html
Root-URL: 
Email: wray@kronos.arc.nasa.gov  
Title: Chain graphs for learning  
Author: Wray L. Buntine 
Note: discussed in the conclusion.  
Address: Mail Stop 269-2 Moffett Field, CA 94035-1000, USA  
Affiliation: RIACS at NASA Ames Research Center  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: <author> Andersson, S., Madigan, D., & Perlman, M. </author> <year> (1994). </year> <title> On the Markov equivalence of chain graphs, undirected graphs, and acyclic digraphs. </title> <type> Technical report #281, </type> <institution> Department of Statistics, University of Washington, </institution> <address> Seattle, WA. </address>
Reference: <author> Besag, J., York, J., & Mollie, A. </author> <year> (1991). </year> <title> Bayesian image restoration with two applications in spatial statistics. </title> <journal> Ann. Inst. Statist. Math., </journal> <volume> 43 (1), </volume> <pages> 1-59. </pages>
Reference: <author> Buntine, W. </author> <year> (1994). </year> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 159-225. </pages>
Reference-contexts: A chain graph is then introduced as a hierarchical combination of conditional networks, in Section 5. Several examples of learning systems are then given. Finally, a graphical construct for modeling samples and learning, plates <ref> (Buntine, 1994) </ref>, is reviewed. 2 Directed networks A Bayesian or directed network consists of a set of variables X and a directed graph defined on it consisting of a node corresponding to each variable and a set of directed arcs. <p> The conditional distributions computed from the above are identical: p (DisjAge; Occ; Clim; Symp) = p (DisjAge; Occ; Clim) p (SympjAge; Dis) P Dis p (DisjAge; Occ; Clim) p (SympjAge; Dis) : More generally, conditional networks can be simplified sometimes <ref> (Buntine, 1994) </ref>. The following simple lemma applies to conditional Bayesian networks and is derived directly from Equation (1). Lemma 2 Given a directed network G with some nodes shaded representing a conditional probability distribution. <p> Also, the global Markov property implies that for x 2 U i , x??(U i [parents (U i )) j (neigbors (x)[parents (x)). Using a proof like that for <ref> (Buntine, 1994, Theorem 2.1) </ref>, Equation (4) follows. 2 A number of interesting properties can be derived from the global Markov property for chain graphs, their equivalence, and their relationship with Bayesian networks (Frydenberg, 1990; Andersson, Madigan, & Perlman, 1994). <p> As a notational device to represent a sample| a group of like variables whose conditional distributions are independent and identical|plates are used on a chain graph <ref> (Buntine, 1994) </ref>. By defining various operations on chain graphs with plates, such as conditioning and differentiation, useful algorithms can be pieced together for standard statistical procedures such as maximum likelihood or maximum a posteriori calculations, or the expectation maximization algorithm.
Reference: <author> Dawid, A. </author> <year> (1979). </year> <title> Conditional independence in statistical theory. </title> <journal> SIAM Journal on Computing, </journal> <volume> 41, </volume> <pages> 1-31. </pages>
Reference-contexts: The lemma below shows that this definition is equivalent to a definition based in independence statements (Lauritzen, Dawid, Larsen, & Leimer, 1990), related to (Pearl, 1988). The independence notation is due to <ref> (Dawid, 1979) </ref>. Definition 1 A is independent of B given C, denoted A??BjC, when p (A [ BjC) = p (AjC)p (BjC) for all instantiations of the variables A; B; C. The following definitions are used here.
Reference: <author> Dawid, A., & Lauritzen, S. </author> <year> (1993). </year> <title> Hyper Markov laws in the statistical analysis of decomposable graphical models. </title> <journal> Annals of Statistics, </journal> <volume> 21 (3), </volume> <pages> 1272-1317. </pages>
Reference: <author> Dean, T., & Wellman, M. </author> <year> (1991). </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: Efforts to date have largely focused on first-order probabilistic inference, for instance found in expert systems and diagnosis (Heckerman, Mamdani, & Wellman, 1995; Spiegelhal-ter, Dawid, Lauritzen, & Cowell, 1993), and planning <ref> (Dean & Wellman, 1991) </ref>. For instance, given a set of observations about a patient, what are the posterior probabilities for different diseases? Should an additional expensive diagnostic test be performed on the patient? This paper presents a representation for extending probabilistic networks to handle second-order or statistical problems.
Reference: <author> Duda, R., & Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley, </publisher> <address> New York. </address>
Reference-contexts: Boltzmann machines traditionally only involve quadratic terms. The correspondence between stochastic neural networks and probabilistic networks is not exact. 6.3 Bayesian classifiers Bayesian classifiers <ref> (Duda & Hart, 1973) </ref> are a broad family of supervised learning systems. Bayesian networks offer a rich representation for designing many different kinds of Bayesian classifiers, for instance illustrated with the Bayesian conditional trees of (Geiger, 1992).
Reference: <author> Frydenberg, M. </author> <year> (1990). </year> <title> The chain graph Markov property. </title> <journal> Scandinavian Journal of Statistics, </journal> <volume> 17, </volume> <pages> 333-353. </pages>
Reference-contexts: In this paper, I define a chain graph as a hierarchical combination of directed (Bayesian) and undirected (Markov) networks. This definition extends the notion of block recursive models used in (Wermuth & Lauritzen, 1989; Htjsgaard & Thiesson, 1995) and analyzed in <ref> (Frydenberg, 1990, Theorem 4.1) </ref> by allowing blocks to include directed networks as well as undirected networks. This definition allows the complex independence properties and functional form of a chain graph (Frydenberg, 1990) to be read off from knowledge of the simpler corresponding properties for directed and undirected networks. <p> This definition allows the complex independence properties and functional form of a chain graph <ref> (Frydenberg, 1990) </ref> to be read off from knowledge of the simpler corresponding properties for directed and undirected networks. <p> This definition also allows chain graphs to have embedded deterministic nodes|as commonly used in learning for neural networks, generalized linear models, and basis functions| and thus extends the general applicability of chain graphs. Previous constructions relied on positivity constraints <ref> (Frydenberg, 1990) </ref> so did not allow determinism, or used limit theorems (Htjsgaard & Thies-son, 1995) to allow some determinism. This framework for modeling chain graphs, and the general interpretation theorem, Theorem 2, are the major technical contribution of this paper.
Reference: <author> Geiger, D. </author> <year> (1992). </year> <title> An entropy-based learning algorithm of Bayesian conditional trees. </title> <editor> In Dubois, D., Wellman, M., D'Ambrosio, B., & Smets, P. (Eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence: Proceedings of the Eight Conference, </booktitle> <pages> pp. </pages> <address> 92-97 Stanford, CA. </address>
Reference-contexts: Bayesian networks offer a rich representation for designing many different kinds of Bayesian classifiers, for instance illustrated with the Bayesian conditional trees of <ref> (Geiger, 1992) </ref>. Chain graphs offer a richer family again of Bayesian classifiers, and a nice framework for their elicitation. During elicitation we can interpret the directed arcs in the "true" 1 chain graph as being causal connections, and undirected arcs as being associational connections.
Reference: <author> Geman, D. </author> <year> (1990). </year> <title> Random fields and inverse prob lems in imaging. </title> <editor> In Hennequin, P. (Ed.), Ecole d' Ete de Probabilites de Saint-Flour XVIII - 1988. </editor> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address> <booktitle> In Lecture Notes in Mathematics, </booktitle> <volume> Volume 1427. </volume>
Reference: <author> Gilks, W., Thomas, A., & Spiegelhalter, D. </author> <year> (1993). </year> <title> A language and program for complex Bayesian modelling. </title> <journal> The Statistician, </journal> <volume> 43, </volume> <pages> 169-178. </pages>
Reference: <author> Heckerman, D. </author> <year> (1995). </year> <title> Bayesian networks for knowledge representation and learning. </title> <editor> In Fayyad, U. M., Piatetsky-Shapiro, G., Smyth, P., & Uthurasamy, R. S. (Eds.), </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: Whereas, an introduction to learning of Bayesian networks can be found in <ref> (Heckerman, 1995) </ref>. This paper uses chain graphs (Lauritzen & Wermuth, 1989) as a general probabilistic network model. Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models.
Reference: <author> Heckerman, D., Mamdani, A., & Wellman, M. </author> <year> (1995). </year> <title> Real-world applications of Bayesian networks: Introduction. </title> <journal> Communications of the ACM, </journal> <volume> 38 (3). </volume>
Reference-contexts: Whereas, an introduction to learning of Bayesian networks can be found in <ref> (Heckerman, 1995) </ref>. This paper uses chain graphs (Lauritzen & Wermuth, 1989) as a general probabilistic network model. Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models.
Reference: <author> Hertz, J., Krogh, A., & Palmer, R. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: An application of this lemma to the chain graph in all the information conveyed by the original graph. 6.2 Stochastic neural networks Stochastic networks form the basis of the stochastic Boltzmann machine, and the Hopfield network <ref> (Hertz, Krogh, & Palmer, 1991) </ref>, which both have relationships to graphical models (Neal, 1992). A stochastic network corresponds to an undirected network with hidden variables, except interactions involve quadratic terms at most. A simple configuration is given in Figure 6. <p> A stochastic network corresponds to an undirected network with hidden variables, except interactions involve quadratic terms at most. A simple configuration is given in Figure 6. On the left is a representation of a network for a Boltzmann using the notation in <ref> (Hertz et al., 1991) </ref>. The input variables are x 1 , x 2 , x 3 and x 4 and the output variable is o. There is one hidden variable h 1 marked in black.
Reference: <author> Htjsgaard, S., & Thiesson, B. </author> <year> (1995). </year> <title> BIFROST | Block recursive models Induced From Relevant knowledge, Observations, and Statistical Techniques. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 (2), </volume> <pages> 155-175. </pages>
Reference-contexts: Previous constructions relied on positivity constraints (Frydenberg, 1990) so did not allow determinism, or used limit theorems <ref> (Htjsgaard & Thies-son, 1995) </ref> to allow some determinism. This framework for modeling chain graphs, and the general interpretation theorem, Theorem 2, are the major technical contribution of this paper. <p> Informally, a chain graph over variables X with component subgraphs given by the set T is interpreted first as the component factorization (compared to a block factorization <ref> (Htjsgaard & Thiesson, 1995) </ref>) given by: p (X) = t2T p (t jparents (t )) ; (5) where parents (A) = [ parents (a) A ; and likewise for ancestors. <p> The language of chain graphs allow associations to be represented in a model in those situations where causality is perhaps difficult to interpret. This is illustrated by <ref> (Htjsgaard & Thiesson, 1995) </ref>. <p> A sample chain graph consistent with this prior knowledge is given in Figure 7 (this simplifies the situation presented in <ref> (Htjsgaard & Thiesson, 1995) </ref>). 7 Chain graphs with plates To represent data analysis problems within a network language such as chain graphs, some additions are 1 The terms "true model", "causal", and "associations" used here are controversial so consider them to be fictions introduced for the purposes of elicitation. predict c
Reference: <author> Lauritzen, S., & Wermuth, N. </author> <year> (1989). </year> <title> Graphical models for associations between variables, some of which are qualitative and some quantitative. </title> <journal> Annals of Statistics, </journal> <volume> 17, </volume> <pages> 31-57. </pages>
Reference-contexts: Whereas, an introduction to learning of Bayesian networks can be found in (Heckerman, 1995). This paper uses chain graphs <ref> (Lauritzen & Wermuth, 1989) </ref> as a general probabilistic network model. Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models. <p> Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models. Lauritzen and Wermuth demonstrated that chain graphs are a powerful tool for modeling statistical analysis, research hypotheses, and hence learning <ref> (Wermuth & Lauritzen, 1989) </ref>. Chain graphs when augmented with deterministic nodes can represent many well known models as a special case including generalized linear models, various forms of clustering, feed-forward neural networks and various stochastic neural networks. <p> It is shown below that a chain graph can be represented as a hierarchical combination of conditional networks. A chain graph is first broken up into components as follows. The chain components are the standard definition used <ref> (Lauritzen & Wermuth, 1989) </ref>. Definition 6 Given a chain graph G over some variables X. The chain components are the coarsest mutually exclusive and exhaustive partition of X where the set of subgraphs induced by the partition are connected and undirected. <p> Let chain-components (A) denote all nodes in the same chain component as at least one variable in A. The chain components are unique and are found by removing the directed arcs from the graph G and identifying the connected components on the resulting graph <ref> (Lauritzen & Wermuth, 1989) </ref>. Definition 7 Given a chain graph G over some variables X.
Reference: <author> Lauritzen, S. </author> <year> (1995). </year> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 (2), </volume> <pages> 191-201. </pages>
Reference: <author> Lauritzen, S., Dawid, A., Larsen, B., & Leimer, H.- G. </author> <year> (1990). </year> <title> Independence properties of directed Markov fields. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 491-505. </pages>
Reference-contexts: The lemma below shows that this definition is equivalent to a definition based in independence statements <ref> (Lauritzen, Dawid, Larsen, & Leimer, 1990) </ref>, related to (Pearl, 1988). The independence notation is due to (Dawid, 1979). Definition 1 A is independent of B given C, denoted A??BjC, when p (A [ BjC) = p (AjC)p (BjC) for all instantiations of the variables A; B; C.
Reference: <author> Lauritzen, S., & Spiegelhalter, D. </author> <year> (1988). </year> <title> Local computations with probabilities on graphical structures and their application to expert systems (with discussion). </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 50 (2), </volume> <pages> 240-265. </pages>
Reference-contexts: Second order problems are mainly concerned with building or improving a probabilistic network from a database of cases. Second order fl Current address: Heuristicrats Research, Inc., 1678 Shattuck Avenue, Suite 310, Berkeley, CA 94709-1631, wray@Heuristicrat.COM inference on probabilistic networks was first suggested by Lauritzen and Spiegelhalter <ref> (Lauritzen & Spiegel-halter, 1988) </ref>, and has subsequently been developed by several groups (Gilks, Thomas, & Spiegelhalter, 1993; Dawid & Lauritzen, 1993; Buntine, 1994; Shachter, Eddy, & Hasselblad, 1990). Whereas, an introduction to learning of Bayesian networks can be found in (Heckerman, 1995).
Reference: <author> Neal, R. </author> <year> (1992). </year> <title> Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56, </volume> <pages> 71-113. </pages>
Reference-contexts: An application of this lemma to the chain graph in all the information conveyed by the original graph. 6.2 Stochastic neural networks Stochastic networks form the basis of the stochastic Boltzmann machine, and the Hopfield network (Hertz, Krogh, & Palmer, 1991), which both have relationships to graphical models <ref> (Neal, 1992) </ref>. A stochastic network corresponds to an undirected network with hidden variables, except interactions involve quadratic terms at most. A simple configuration is given in Figure 6. On the left is a representation of a network for a Boltzmann using the notation in (Hertz et al., 1991).
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The lemma below shows that this definition is equivalent to a definition based in independence statements (Lauritzen, Dawid, Larsen, & Leimer, 1990), related to <ref> (Pearl, 1988) </ref>. The independence notation is due to (Dawid, 1979). Definition 1 A is independent of B given C, denoted A??BjC, when p (A [ BjC) = p (AjC)p (BjC) for all instantiations of the variables A; B; C. The following definitions are used here. <p> The particular independence statements are based on set separation in the moralized graph, which is equivalent to another condition known as d-separation <ref> (Pearl, 1988) </ref>: Definition 3 The distribution p (X) satisfies the directed global Markov property relative to the directed graph G if A??BjS when S separates A and B in the graph H m where H is the subgraph of G restricted to ancestors (A [ B [ S).
Reference: <author> Ripley, B. </author> <year> (1994). </year> <title> Network methods in statistics. </title> <editor> In Kelly, F. (Ed.), </editor> <title> Probability, </title> <journal> Statistics and Optimization, </journal> <pages> pp. 241-253. </pages> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Shachter, R. </author> <year> (1986). </year> <title> Evaluating influence diagrams. </title> <journal> Operations Research, </journal> <volume> 34 (6), </volume> <pages> 871-882. </pages>
Reference-contexts: The expressiveness of chain graphs is illustrated in Section 6 where a number of models are represented. Decision theoretic constructs could also be used to represent the decisions and utilities of a problem <ref> (Shachter, 1986) </ref>, although this is not done here. In this paper, I define a chain graph as a hierarchical combination of directed (Bayesian) and undirected (Markov) networks.
Reference: <author> Shachter, R. </author> <year> (1990). </year> <title> An ordered examination of influence diagrams. </title> <journal> Networks, </journal> <volume> 20, </volume> <pages> 535-563. </pages>
Reference-contexts: Here the five sigmoid units of the network are modeled with deterministic nodes. A deterministic node has double circles to indicate it is a deterministic function of its inputs. The analysis of deterministic nodes in Bayesian networks and, more generally, in influence diagrams is considered by <ref> (Shachter, 1990) </ref>. The network outputs m 1 and m 2 represent the mean of a bivariate Gaussian. To analyze these nodes, we need to extend the usual definition of a parent and a child for a graph.
Reference: <author> Shachter, R., Eddy, D., & Hasselblad, V. </author> <year> (1990). </year> <title> An influence diagram approach to medical technology assessment. </title> <editor> In Oliver, R., & Smith, J. (Eds.), </editor> <title> Influence Diagrams, Belief Nets and Decision Analysis, </title> <journal> pp. </journal> <pages> 321-350. </pages> <publisher> Wiley. </publisher>
Reference-contexts: Here the five sigmoid units of the network are modeled with deterministic nodes. A deterministic node has double circles to indicate it is a deterministic function of its inputs. The analysis of deterministic nodes in Bayesian networks and, more generally, in influence diagrams is considered by <ref> (Shachter, 1990) </ref>. The network outputs m 1 and m 2 represent the mean of a bivariate Gaussian. To analyze these nodes, we need to extend the usual definition of a parent and a child for a graph.
Reference: <author> Shachter, R., & Heckerman, D. </author> <year> (1987). </year> <title> Thinking backwards for knowledge acquisition. </title> <journal> AI Magazine, </journal> <volume> 8 (Fall), </volume> <pages> 55-61. </pages>
Reference-contexts: Conditional networks are represented by introducing shaded variables in the graph. Shaded variables are assumed to have their values known, so the probability defined by the network is now conditional on the shaded variables. Figure 1 shows two conditional versions of a simple medical problem <ref> (Shachter & Heckerman, 1987) </ref>.
Reference: <author> Spiegelhalter, D., Dawid, A., Lauritzen, S., & Cowell, R. </author> <year> (1993). </year> <title> Bayesian analysis in expert systems. </title> <journal> Statistical Science, </journal> <volume> 8 (3), </volume> <pages> 219-283. </pages>
Reference: <author> Verma, T., & Pearl, J. </author> <year> (1990). </year> <title> Equivalence and synthesis of causal models. </title> <editor> In Bonissone, P. (Ed.), </editor> <booktitle> Proceedings of the Sixth Conference on Uncertainty in Artificial Intelligence Cambridge, </booktitle> <address> Mas-sachusetts. </address>
Reference-contexts: For instance, a chain graph is a convenient representation for the class of equivalent Bayesian networks <ref> (Verma & Pearl, 1990) </ref>. 6 Examples of chain graphs for learning This section presents a number of models using chain graphs, to illustrate their generality. 6.1 Feed-forward networks and deterministic nodes The preceding definitions of a chain graph have been carefully set up to allow nodes to represent deterministic variables.
Reference: <author> Wermuth, N., & Lauritzen, S. </author> <year> (1989). </year> <title> On substantive research hypotheses, conditional independence graphs and graphical chain models. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 51 (3). </volume>
Reference-contexts: Whereas, an introduction to learning of Bayesian networks can be found in (Heckerman, 1995). This paper uses chain graphs <ref> (Lauritzen & Wermuth, 1989) </ref> as a general probabilistic network model. Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models. <p> Chain graphs mix undirected and directed graphs (or networks) to give a probabilistic representation that includes Markov random fields and various Markov models. Lauritzen and Wermuth demonstrated that chain graphs are a powerful tool for modeling statistical analysis, research hypotheses, and hence learning <ref> (Wermuth & Lauritzen, 1989) </ref>. Chain graphs when augmented with deterministic nodes can represent many well known models as a special case including generalized linear models, various forms of clustering, feed-forward neural networks and various stochastic neural networks. <p> It is shown below that a chain graph can be represented as a hierarchical combination of conditional networks. A chain graph is first broken up into components as follows. The chain components are the standard definition used <ref> (Lauritzen & Wermuth, 1989) </ref>. Definition 6 Given a chain graph G over some variables X. The chain components are the coarsest mutually exclusive and exhaustive partition of X where the set of subgraphs induced by the partition are connected and undirected. <p> Let chain-components (A) denote all nodes in the same chain component as at least one variable in A. The chain components are unique and are found by removing the directed arcs from the graph G and identifying the connected components on the resulting graph <ref> (Lauritzen & Wermuth, 1989) </ref>. Definition 7 Given a chain graph G over some variables X.
Reference: <author> Whittaker, J. </author> <year> (1990). </year> <title> Graphical Models in Applied Multivariate Statistics. </title> <publisher> Wiley. </publisher>
References-found: 30

