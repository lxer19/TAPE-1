URL: http://cosmos.imag.fr/RESEAUX/malek/malek.iccbr95.ps.gz
Refering-URL: http://cosmos.imag.fr/RESEAUX/malek/Publications.html
Root-URL: 
Email: Email: Maria.Malek@imag.fr  
Title: A Connectonist Indexing Approach for CBR Systems  
Author: Maria Malek 
Keyword: Key Words: Indexing, Incremental, Neural Network, Prototype.  
Address: bat. Lifia, 46 ave Felix Viallet, 38031 Grenoble, France  
Affiliation: TIMC-LIFIA  
Abstract: An important factor that plays a major role in determining the performances of a CBR system is the complexity and the accuracy of the case retrieval phase. Both flat memory and inductive approaches suffer from serious drawbacks. In the first approach, the search time becomes considerable when dealing with large scale memory base, while in the second one the modifications of the case memory becomes very complex because of its sophisticated architecture. In this paper, we show how we can construct a simple efficient indexing system structure. We construct a case hierarchy with two levels of memory: the lower level contains cases organised into groups of similar cases, while the upper level contains prototypes, each of which represents one group of cases. The construction of prototypes is made by using an incremental prototype-based network. This upper level parallel memory is used as an indexing system during the retrieval phase. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Aamodt and E. </author> <title> Plaza. Case-based reasoning: Foundational issues, methodological variations, and system approaches. </title> <journal> AICOM, </journal> <volume> 7(1), </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: Given a new input problem, the case retriever identifies the most appropriate cases (in some similarity sense) in the case base and presents them to the case adapter. The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases <ref> [1] </ref> [5] [14]. It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased. Hence, a particular attention must be paid to the design and the implementation of this phase.
Reference: 2. <author> E. Alpaydin. </author> <title> Gal : Networks that grow when they learn and shrink when they forget. </title> <type> Technical Report TR 91-032, </type> <institution> International Computer Science Institute, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The first implemented model using this method was described in [13] commercially known as the Nestor Learning System (NLS). An incremental prototype-based neural network which is based on a " Grow and Learn" (Gal) algorithm is also described in <ref> [2] </ref>. In this section, we describe the incremental prototype-based neural network model (ARN2) proposed in [3]. Figure 1 shows the architecture of the model. The network is composed of three layers: 1. The input layer that contains one unit for each attribute 2.
Reference: 3. <author> A. Azcarraza and A. Giacometti. </author> <title> A prototype-based incremental network model for classification task. </title> <booktitle> In Fourth International Conference on Neural Networks and Their Applications, </booktitle> <address> Nimes, France, </address> <year> 1991. </year>
Reference-contexts: An incremental prototype-based neural network which is based on a " Grow and Learn" (Gal) algorithm is also described in [2]. In this section, we describe the incremental prototype-based neural network model (ARN2) proposed in <ref> [3] </ref>. Figure 1 shows the architecture of the model. The network is composed of three layers: 1. The input layer that contains one unit for each attribute 2. The output layer that contains one unit for each class 3.
Reference: 4. <author> S.K. Bamberger and K. Goos. </author> <title> Integration of case-based reasoning and inductive learning methods. </title> <booktitle> In First European Workshop on CBR, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [12] have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration <ref> [4] </ref> [9]. This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the complex architecture of the used structure [17]. In this paper, we propose a simple and efficient indexing system. We construct a case hierarchy of two levels of memory.
Reference: 5. <author> R. Barletta. </author> <title> An introduction to case-based reasoning. </title> <journal> AI Expert, </journal> <month> August </month> <year> 1991. </year>
Reference-contexts: Given a new input problem, the case retriever identifies the most appropriate cases (in some similarity sense) in the case base and presents them to the case adapter. The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases [1] <ref> [5] </ref> [14]. It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased. Hence, a particular attention must be paid to the design and the implementation of this phase. <p> They aim to construct inductively a model by generalizing from specific examples. These methods have two advantages <ref> [5] </ref>: 1. They can automatically analyze the cases to determine the best features for distinguishing them 2.
Reference: 6. <author> A. Giacometti. Modeles Hybrides de l'Expertise. </author> <type> PhD thesis, </type> <institution> Telecom- Paris, </institution> <year> 1992. </year>
Reference-contexts: The example is then represented again to the network. However, experience shows that the learning procedure may cause the creation of a considerable number of prototype units near the boundaries of classes in the input space <ref> [6] </ref>. This situation can be avoided by adding a new criterion to the differentiation process. This has been achieved by introducing an uncertain region between two classes in which differentiation is not allowed [6]. This region presents vectors that cause nearly the same activation to both prototypes. <p> creation of a considerable number of prototype units near the boundaries of classes in the input space <ref> [6] </ref>. This situation can be avoided by adding a new criterion to the differentiation process. This has been achieved by introducing an uncertain region between two classes in which differentiation is not allowed [6]. This region presents vectors that cause nearly the same activation to both prototypes. As a result, the network is unable to learn new cases that falls into this region (we call these cases: boundary cases) (Fig. 2).
Reference: 7. <author> J. Kolodner. </author> <title> Case-Based Reasoning. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1993. </year>
Reference-contexts: In addition, the retrieval time is decreased because of using a parallel memory that contains a few number of prototypes. In fact, using a simple memory hierarchy (two levels of memory) in stead of a shared-neural networks <ref> [7] </ref> simplifies learning new cases (incrementality); this is achieved by modifying one neural unit (prototype) and it is much simpler than the method used in ID5R for example. We plan to study soon the effect of choosing a similarity measure on the retrieval accuracy.
Reference: 8. <author> M. Malek and V. Rialle. </author> <title> A case-based reasoning system applied to neuropathy diagnosis. </title> <booktitle> In Second European Workshop, EWCBR-94, Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: We describe in the next, the memory organisation, how learning and retrieval processes are performed and we compare this system to two widely used inductive indexing approaches which are C4.5 and ID5R. 3.1 Memory Organisation The idea is to construct a simple indexing system that contains two levels of memory <ref> [8] </ref>: 1. Memory that contains prototypical cases 2. Memory that contains instances or real cases The prototype memory is used during retrieval phase as an indexing system in order to decrease retrieval time. The first memory level is the hidden layer of the incremental prototype-based network described in previous section.
Reference: 9. <author> M. Manago, K. Althoff, E. Auriol, R. Traphoner, S. Wess, N. Conruyt, and F. Maurer. </author> <title> Induction and reasoning from cases. </title> <booktitle> In First European Workshop on CBR, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: The study of some inductive approaches like ID3/C4 [12] have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration [4] <ref> [9] </ref>. This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the complex architecture of the used structure [17]. In this paper, we propose a simple and efficient indexing system. We construct a case hierarchy of two levels of memory.
Reference: 10. <author> P. Myllymaki and H.Tirri. </author> <title> Massively parallel case-based reasoning with probabilistic similarity metrics. </title> <booktitle> In First European Workshop on CBR, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: In [15], a neural network model for case-filtering is proposed, a feed forward network is designed such that the inputs are factors from instances in a library of cases and factors from a new observed situation. The filter is designed to select relevant cases from the library. In <ref> [10] </ref> Authors propose another approach where case-based reasoning can be implemented on a connectionist network architecture. The method is based on implementing Pearl's probability propagation as a 3-layer hierarchical network.
Reference: 11. <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> (1):81- 106, </volume> <year> 1986. </year>
Reference-contexts: It is a descendent of the ID3 algorithm <ref> [11] </ref>. Each leaf in the tree indicates a class (or a prototype). A decision node specifies some test to be carried on a single attribute value.
Reference: 12. <author> J.R. Quinlan. </author> <title> C4.5. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Building such a hierarchy is time consuming, but it can be used very efficiently during the inference process. Now, adding cases is a complex operation and it is hard to keep the network optimal as cases are added. The study of some inductive approaches like ID3/C4 <ref> [12] </ref> have shown that the costs of altering large case bases are very high and lead sometimes to a re-compilation of the complete case base after each alteration [4] [9]. <p> These methods have two advantages [5]: 1. They can automatically analyze the cases to determine the best features for distinguishing them 2. The cases are organized for retrieval into a hierarchical structure that increases the retrieval time by only the log of the number of cases C4.5 <ref> [12] </ref> is an inductive method that generates a decision tree classifier from an initial set of cases. It is a descendent of the ID3 algorithm [11]. Each leaf in the tree indicates a class (or a prototype). <p> A decision tree can be used to classify a case by starting at the root of the tree and moving through it until a leaf is encountered. The class of the leaf is predicted to be that recorded at the leaf. To construct a decision tree, Quinlan <ref> [12] </ref> has pointed out that selecting an attribute with a lowest E-score is equivalent to selecting an attribute with a highest information gain.
Reference: 13. <author> D.L. Reilly, L.N. Cooper, and C. Elbaum. </author> <title> A neural model for category learning. </title> <journal> Biological Cyber-metics, </journal> (45):35-41, 1982. 
Reference-contexts: Each hidden network unit represents a prototype. These prototype units are grouped into their corresponding classes, each class identified by a category label. The first implemented model using this method was described in <ref> [13] </ref> commercially known as the Nestor Learning System (NLS). An incremental prototype-based neural network which is based on a " Grow and Learn" (Gal) algorithm is also described in [2]. In this section, we describe the incremental prototype-based neural network model (ARN2) proposed in [3].
Reference: 14. <editor> C.K. Riesbeck and R.C. Schank. </editor> <title> Inside Case-Based Reasoning. </title> <publisher> Lawrence Erlbaum Associates, publishers, </publisher> <year> 1989. </year>
Reference-contexts: Given a new input problem, the case retriever identifies the most appropriate cases (in some similarity sense) in the case base and presents them to the case adapter. The case adapter then examines the retrieved cases and tries to solve the new problem by adapting these cases [1] [5] <ref> [14] </ref>. It is obvious that more the retrieval phase is efficient more the overall performances of the CBR system is increased. Hence, a particular attention must be paid to the design and the implementation of this phase. Indexing and memory organisation are the main issues for efficient retrieval of cases.
Reference: 15. <author> P. Thrift. </author> <title> A neural network model for case-based reasoning. </title> <booktitle> In Proceddings of the DARPA Case-Based Reasoning Workshop, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Some neural network architectures for CBR systems have been proposed. In <ref> [15] </ref>, a neural network model for case-filtering is proposed, a feed forward network is designed such that the inputs are factors from instances in a library of cases and factors from a new observed situation. The filter is designed to select relevant cases from the library.
Reference: 16. <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S Dzroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michal-ski, T. Mitchell, P. Pachowicz, Y. Reich H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang. </author> <title> The monk's problems a performance comparison of different learning algorihms. </title> <type> Technical Report CMU-CS-91-197, </type> <institution> Carnegie Mellon University, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The algorithm allows at each moment to restructure the whole tree and to expand it if possible. In order to compare our indexing system to C4.5 and ID5R methods we have chosen the MONK's problems that were used to compare different learning algorithm performances <ref> [16] </ref>.
Reference: 17. <author> P.E. Utgoff. </author> <title> Incremental induction of decision trees. </title> <journal> Machine Learning, </journal> (4):161-186, 1989. This article was processed using the L a T E X macro package with LLNCS style 
Reference-contexts: This shows that using inductive approaches makes retrieval more efficient but it considerably complicates the learning phase because of the complex architecture of the used structure <ref> [17] </ref>. In this paper, we propose a simple and efficient indexing system. We construct a case hierarchy of two levels of memory. The lower level contains cases organised into groups of similar cases. The upper level contains prototypes, each of which represents a group of cases. <p> C4.5 is not incremental; this means that one needs to build a new decision tree to learn an additional case. ID5R is an incremental method to build a decision tree <ref> [17] </ref>. The idea is to maintain sufficient information to compute the E-score for an attribute at a node, making it possible to change the test attribute at a node to one with the lowest E-score. <p> The generalisation accuracies registered by PBIS are better than the ones registered by ID5R,for M1 and M2 problem. ID5R and PBIS are both incremental. Adding a new case in ID5R is a complicated operation because it leads to a whole restructuration of the generated tree <ref> [17] </ref>, whereas in PBIS, this is achieved by simply adding the treated case to the suitable memory zone and by modifying only one unit in the neural network (assimilation, accommodation or differentiation of a prototype). 4 Related Works A natural relationship seems to exist between CBR and NN models where similar
References-found: 17

