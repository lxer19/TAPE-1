URL: http://www.cs.colostate.edu/~vision/ps/iuw94-coreg-wnums.ps.gz
Refering-URL: http://www.cs.colostate.edu/~vision/html/publications.html
Root-URL: 
Email: schwicke/ross@cs.colostate.edu  
Title: Object to Multisensor Coregistration with Eight Degrees of Freedom  
Author: Anthony N. A. Schwickerath and J. Ross Beveridge 
Affiliation: Computer Science Department Colorado State University  
Abstract: A new least-squares procedure is presented which fuses data from a standard perspective sensor (CCD camera, FLIR sensor, etc.) and a range sensor (LADAR) based upon corresponding features identified on a 3D object model and in each image. The algorithm solves for both the pose estimate of the object relative to the sensors and the registration between sensors. This model-based coregistration process is being developed to support future work recognizing modeled 3D objects in scenes imaged by both optical (FLIR and CCD) and range (LADAR) sensors. Coregistration results are presented for both synthetic and real world tests. The algorithm requires an initial pose and sensor registration estimate. Tests on controlled synthetic data show it is robust with respect to substantial errors in initial translation and orientation errors up to roughly 45 degrees.
Abstract-found: 1
Intro-found: 1
Reference: [ Beveridge and Riseman, 1992 ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Hybrid Weak-Perspective and Full-Perspective Matching. </title> <booktitle> In Proceedings: IEEE 1992 Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 432 - 438. </pages> <publisher> IEEE Computer Society, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Using work of Kumar [ Kumar, 1989; Kumar and Hanson, 1994 ] , this approach generalizes to matching subject to perspective given approximate estimates of initial object pose <ref> [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] </ref> . The coregistration work presented here is the next logical step in a generalization of our optimal matching work to multisensor problems. <p> The Levenberg-Marquardt [ Press et al., 1988 ] method, has been found to be robust in our past single sensor pose work <ref> [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] </ref> , and it is used here to find the optimal coregistration parameters. It is trivial to modify this formulation to handle point rather than line segment features in the CCD data.
Reference: [ Beveridge and Riseman, 1994 ] <author> J. Ross Beveridge and Edward M. Riseman. </author> <title> Optimal Geometric Model Matching Under Full 3D Perspective. </title> <booktitle> In Second CAD-Based Vision Workshop, </booktitle> <pages> pages 54 - 63. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> February </month> <year> 1994. </year> <note> (Submitted to CVGIP-IU). </note>
Reference-contexts: Using work of Kumar [ Kumar, 1989; Kumar and Hanson, 1994 ] , this approach generalizes to matching subject to perspective given approximate estimates of initial object pose <ref> [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] </ref> . The coregistration work presented here is the next logical step in a generalization of our optimal matching work to multisensor problems. <p> The Levenberg-Marquardt [ Press et al., 1988 ] method, has been found to be robust in our past single sensor pose work <ref> [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] </ref> , and it is used here to find the optimal coregistration parameters. It is trivial to modify this formulation to handle point rather than line segment features in the CCD data.
Reference: [ Beveridge et al., 1990 ] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Combinatorial Optimization Applied to Variable Scale 2D Model Matching. </title> <booktitle> In Proceedings of the IEEE International Conference on Pattern Recognition 1990, Atlantic City, </booktitle> <pages> pages 18 - 23. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1990. </year>
Reference: [ Beveridge et al., 1991 ] <author> J. Ross Beveridge, Rich Weiss, and Edward M. Riseman. </author> <title> Optimization of 2-Dimensional Model Matching. </title> <editor> In Hatem Nasr, editor, </editor> <booktitle> Selected Papers on Automatic Object Recognition (originally appeared in DARPA Image Understanding Workshop, 1989), SPIE Milestone Series. SPIE, </booktitle> <address> Bellingham, WA, </address> <year> 1991. </year>
Reference: [ Beveridge et al., 1994a ] <author> J. Ross Beveridge, Allen Han-son, </author> <title> and Durga Panda. </title> <institution> RSTA Research of the Col-orado State, University of Massachusetts and Alliant Techsystems Team. </institution> <note> In Proceedings: Image Understanding Workshop, page (to appear). Morgan Kauf-mann, </note> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction The work presented here is part of a larger project developing new ways of identifying modeled 3D objects in range, IR and color data <ref> [ Beveridge et al., 1994a ] </ref> . The project task domain is the recognition of military vehicles from an Unmanned Ground Vehicle performing a Reconnaissance, Surveillance and Target Acquisition (RSTA) task. <p> The initial coregistration estimate, shown in Figure 4a, places the M60 roughly 50 meters too high in the scene. The CCD to LADAR sensor registration is off by nearly 15 pixels. Using the LADAR hypothesis generation techniques described in <ref> [ Beveridge et al., 1994a ] </ref> , we expect much better initial coregistration estimates.
Reference: [ Beveridge et al., 1994b ] <author> J. Ross Beveridge, Durga P. Panda, and Theodore Yachik. </author> <title> November 1993 Fort Carson RSTA Data Collection Final Report. </title> <type> Technical Report CSS-94-118, </type> <institution> Colorado State University, </institution> <address> Fort Collins, CO, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Note that determining the occluded portion is a simple matter of testing what portion of the contour lies over the dark grey foreground area. The same contour is shown over the FLIR data in Figure 2d. Given precise coregistration, the lack of a lower 1 See <ref> [ Beveridge et al., 1994b ] </ref> for a description of the Fort Carson data. contour for the tank in the FLIR is explained by the occlusion evident in the LADAR.
Reference: [ Beveridge, 1993 ] <author> J. Ross Beveridge. </author> <title> Local Search Algorithms for Geometric Obejct Recognition: Optimal Correspondence and Pose. </title> <type> PhD thesis, </type> <institution> University of Massachuesetts at Amherst, </institution> <month> May </month> <year> 1993. </year>
Reference: [ Bolles et al., 1993 ] <author> Robert C. Bolles, H. Harlyn Baker, and Marsha Jo Hannah. </author> <title> The JISCT Stereo Evaluation. </title> <booktitle> In Proceedings: Image Understanding Workshop, </booktitle> <address> Los Altos, CA, </address> <month> April </month> <year> 1993. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, coregistration allows constraints which involve sophisticated dependencies. To illustrate this last point, consider the problem of occlusion and how detection in LADAR can permit proper handling in coregistered FLIR or CCD data. Surface occlusion is potentially determined using stereo <ref> [ Bolles et al., 1993 ] </ref> or active vision [ Kutulakos and Dyer, 1994 ] . However, it is essentially impossible to detect from a single IR or color image.
Reference: [ Eason and Gonzalez, 1992 ] <author> R. O. Eason and R. C. Gon-zalez. </author> <title> Least-Squares Fusion of Multisensory Data. </title> <editor> In Mongi A. Abidi and Rafael C. Gonzalez, editors, </editor> <booktitle> Data Fusion in Robotics and Machine Intelligence, chapter 9. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference: [ Goss et al., 1994 ] <author> Michael E. Goss, J. Ross Beveridge, Mark Stevens, and Aaron Fuegi. </author> <title> Visualization and Verification of Automatic Target Recognition Results Using Combined Range and Optical Imagery. </title> <booktitle> In Proceedings: Image Understanding Workshop, page (to appear), </booktitle> <address> Los Altos, CA, </address> <month> November </month> <year> 1994. </year> <title> ARPA, </title> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: " 0 500 10 3.6 5.42e-10 Table 3: Test I Results 5.4 Demonstration on Real Data Four corresponding points on a 3D M60 object model, in the Fort Carson nov31553c color image, and in the nov31205l LADAR image were hand picked using the Rangeview program described elsewhere in these proceedings <ref> [ Goss et al., 1994 ] </ref> . The four pairs of corresponding points were used to generate six corresponding pairs of line segments for the model-to-CCD error term. These corresponding features, intrinsic camera parameters, and an initial coregistration estimate were passed to the coregistration algorithm.
Reference: [ Grimson, 1990 ] <author> W. Eric L. </author> <title> Grimson. Object Recognition by Computer: The Role of Geometric Constraints. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ Lowe and Binford, 1985 ] and later demonstrated object tracking under perspective projection [ Lowe, 1991 ] . Hutten-locher [ Huttenlocher and Ullman, 1990 ] demonstrated alignment based recognition under orthographic projection. Grimson's work <ref> [ Grimson, 1990 ] </ref> on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ Hebert et al., 1990 ] <author> Martial Hebert, Takeo Kanade, and InSo Kweon. </author> <title> 3-D Vision Techniques for Autonomous Vehicles. </title> <editor> In Ramesh C. Jain and Anil K. Jain, editors, </editor> <title> Analysis and Interpretation of Range Images, chapter 7. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: We believe this dependence upon scene geometry implies that known sensor and object geometry should be used to constrain and adapt both the object pose and sensor-to-sensor co-ordinate transformations as part of object recognition. Others have worked on problems similar to the coregis-tration problem discussed in this paper. Herbert <ref> [ Hebert et al., 1990 ] </ref> presents a clean least-squares mechanism for computing the rigid transform between a range and color CCD sensor based upon corresponding image points in the two sensor images.
Reference: [ Hel-Or and Werman, 1993 ] <author> Y. Hel-Or and M. Werman. </author> <title> Absolute Orientation from Uncertain Data: A Unified Approach. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 77 - 82. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: However, this work does not recover explicitly the associated pose of the 3D points relative to the sensors. Both Eason [ Eason and Gonza-lez, 1992 ] and Hel-Or <ref> [ Hel-Or and Werman, 1993 ] </ref> develop least-squares multisensor pose algorithms. However, these algorithms only solve for the six degree of freedom pose estimate. They do not support simultaneous adjustment of the sensor registration parameters. More recent work by Hel-Or and Werman [ Y. Hel-Or and M.
Reference: [ Huttenlocher and Ullman, 1990 ] <author> Daniel P. Hutten-locher and Shimon Ullman. </author> <title> Recognizing solid objects by alignment with an image. </title> <journal> Internation Journal of Computer Vision, </journal> <volume> 5(2):195 - 212, </volume> <month> November </month> <year> 1990. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ Lowe and Binford, 1985 ] and later demonstrated object tracking under perspective projection [ Lowe, 1991 ] . Hutten-locher <ref> [ Huttenlocher and Ullman, 1990 ] </ref> demonstrated alignment based recognition under orthographic projection. Grimson's work [ Grimson, 1990 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ Kumar and Hanson, 1994 ] <author> Rakesh Kumar and Allen R. Hanson. </author> <title> Robust Methods for Estimating Pose and a Sensitivity Analysis. CVGIP:Image Understanding, </title> <note> 11:(to appear in November), </note> <year> 1994. </year>
Reference-contexts: The goal of coregistration is the best estimate of the eight coregistration parameters. This best estimate minimizes a squared distance function between corresponding object and image features. The problem setup is an extension of a single optical sensor pose algorithm developed by Kumar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> . Kumar's algorithm iteratively solves the non-linear optimization problem associated with minimizing a sum of point-to-plane distances. Typically the points are endpoints of 3D line segments on the object model. <p> Our own previous work [ Beveridge et al., 1990; Bev-eridge et al., 1991; Beveridge, 1993 ] emphasizes global alignment as a basis for match ranking and optimal matching. Using work of Kumar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> , this approach generalizes to matching subject to perspective given approximate estimates of initial object pose [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] . <p> E tot = w mc E mc + w ml E ml (1) The first term, E mc , measures distance between corresponding CCD and object features. This term is precisely the point-to-plane error criterion defined by Ku-mar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> for computing camera to object pose. The second, E mcl , is simply the sum-of-squared Euclidean distances between corresponding object and range points.
Reference: [ Kumar, 1989 ] <author> Rakesh Kumar. </author> <title> Determination of Camera Location and Orientation. </title> <booktitle> In Proceedings: Image 9 Understanding Workshop, </booktitle> <pages> pages 870 - 881, </pages> <address> Los Altos, CA, </address> <month> June </month> <year> 1989. </year> <title> DARPA, </title> <publisher> Morgan Kaufmann Publishers, Inc. </publisher>
Reference-contexts: The goal of coregistration is the best estimate of the eight coregistration parameters. This best estimate minimizes a squared distance function between corresponding object and image features. The problem setup is an extension of a single optical sensor pose algorithm developed by Kumar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> . Kumar's algorithm iteratively solves the non-linear optimization problem associated with minimizing a sum of point-to-plane distances. Typically the points are endpoints of 3D line segments on the object model. <p> Our own previous work [ Beveridge et al., 1990; Bev-eridge et al., 1991; Beveridge, 1993 ] emphasizes global alignment as a basis for match ranking and optimal matching. Using work of Kumar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> , this approach generalizes to matching subject to perspective given approximate estimates of initial object pose [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] . <p> E tot = w mc E mc + w ml E ml (1) The first term, E mc , measures distance between corresponding CCD and object features. This term is precisely the point-to-plane error criterion defined by Ku-mar <ref> [ Kumar, 1989; Kumar and Hanson, 1994 ] </ref> for computing camera to object pose. The second, E mcl , is simply the sum-of-squared Euclidean distances between corresponding object and range points.
Reference: [ Kutulakos and Dyer, 1994 ] <author> Kiriakos N. Kutulakos and Charles R. Dyer. </author> <title> Occluding Contour Detection Using Affine Invariants and Purposive Viewpoint Control. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 323 - 330. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1994. </year> [ <note> Lowe and Binford, </note> <author> 1985 ] David G. Lowe and Thomas O. Binford. </author> <title> The Recovery of Three-Dimensional Structure from Image Curves. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(3):320 - 325, </volume> <year> 1985. </year>
Reference-contexts: Finally, coregistration allows constraints which involve sophisticated dependencies. To illustrate this last point, consider the problem of occlusion and how detection in LADAR can permit proper handling in coregistered FLIR or CCD data. Surface occlusion is potentially determined using stereo [ Bolles et al., 1993 ] or active vision <ref> [ Kutulakos and Dyer, 1994 ] </ref> . However, it is essentially impossible to detect from a single IR or color image. Thus, using single IR or color images in a RSTA scenario it is impossible to define a match evaluation measure which can distinguish between occlusion and omission.
Reference: [ Lowe, 1991 ] <author> David G. Lowe. </author> <title> Fitting Parameterized Three-Dimensional Models to Images. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(5):441 - 450, </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: For example, Lowe utilized this approach subject to orthographic projection [ Lowe and Binford, 1985 ] and later demonstrated object tracking under perspective projection <ref> [ Lowe, 1991 ] </ref> . Hutten-locher [ Huttenlocher and Ullman, 1990 ] demonstrated alignment based recognition under orthographic projection. Grimson's work [ Grimson, 1990 ] on constraint base matching has emphasized local feature compatibility to prune tree search and thus minimize tests of global alignment.
Reference: [ Magee et al., 1985 ] <author> M. J. Magee, B. A. Boyter, C. H. Chien, and J. K. Aggarwal. </author> <title> Experiments in Intensity Guided Range Sensing Recognition of Three-Dimensional Objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(6):629 - 637, </volume> <month> Novem-ber </month> <year> 1985. </year>
Reference-contexts: Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities. The implied explanation is that relating data from different modalities is more difficult. While Aggarwal <ref> [ Magee et al., 1985 ] </ref> and others [ Stentz and Goto, 1987 ] have examples of successful mixed modality fusion, this is still a young research area. Aggarwal also notes that to properly perform mixed modality sensor fusion, co-ordinate transformations between images need to be adaptively determined.
Reference: [ Press et al., 1988 ] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Note this means the constants in Table 2 are recomputed each time through the loop. Iteration stops when either the amount by which E tot drops between iterations falls below a preset threshold or the total number of iterations exceeds a maximum allowable upper bound. The Levenberg-Marquardt <ref> [ Press et al., 1988 ] </ref> method, has been found to be robust in our past single sensor pose work [ Beveridge and Riseman, 1992; Beveridge and Riseman, 1994 ] , and it is used here to find the optimal coregistration parameters.
Reference: [ Stentz and Goto, 1987 ] <author> A. Stentz and Y. </author> <title> Goto. </title> <booktitle> The CMU Navigational Architecture. In Proceedings: Image Understanding Workshop, </booktitle> <pages> pages 440-446, </pages> <address> Los An-geles, CA, </address> <month> February </month> <year> 1987. </year> <title> ARPA, </title> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Aggarwal notes that past work on sensor fusion emphasized single modality sensors, with comparatively little work on different sensor modalities. The implied explanation is that relating data from different modalities is more difficult. While Aggarwal [ Magee et al., 1985 ] and others <ref> [ Stentz and Goto, 1987 ] </ref> have examples of successful mixed modality fusion, this is still a young research area. Aggarwal also notes that to properly perform mixed modality sensor fusion, co-ordinate transformations between images need to be adaptively determined.
Reference: [ Y. Hel-Or and M. Werman, 1994 ] <author> Y. Hel-Or and M. Werman. </author> <title> Constraint-Fusion for Interpretation of Articulated Objects. </title> <booktitle> In Proceedings: Computer Vision and Pattern Recognition, </booktitle> <pages> pages 39 - 45. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> June </month> <year> 1994. </year> <month> 10 </month>
References-found: 22

