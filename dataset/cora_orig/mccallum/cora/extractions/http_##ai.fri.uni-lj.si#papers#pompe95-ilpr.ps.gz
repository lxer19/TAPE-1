URL: http://ai.fri.uni-lj.si/papers/pompe95-ilpr.ps.gz
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: furos.pompe, igor.kononenkog@ninurta.fer.uni-lj.si  
Phone: tel: +386-61-1768 386, fax: +386-61-264 990  
Title: Linear Space Induction in First Order Logic with RELIEFF  
Author: Uros Pompe, Igor Kononenko 
Address: Trzaska 25, SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of electrical engineering computer science,  
Abstract: Current ILP algorithms typically use variants and extensions of the greedy search. This prevents them to detect significant relationships between the training objects. Instead of myopic impurity functions, we propose the use of the heuristic based on RELIEF for guidance of ILP algorithms. At each step, in our ILP-R system, this heuristic is used to determine a beam of candidate literals. The beam is then used in an exhaustive search for a potentially good conjunction of literals. From the efficiency point of view we introduce interesting declarative bias which enables us to keep the growth of the training set, when introducing new variables, within linear bounds (linear with respect to the clause length). This bias prohibits cross-referencing of variables in variable dependency tree. The resulting system has been tested on various artificial problems. The advantages and deficiencies of our approach are discussed. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Breiman L., Friedman J.H., Olshen R.A. & Stone C.J. </author> <title> (1984) Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: Pompe, I. Kononenko: ILP with RELIEFF 2 two instances. For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval <ref> [0; 1] </ref>. Normalization with n guarantees all weights to be in the interval [1; 1]. The weight W [A] is used as an estimate of the quality of attribute A. The function diff is used also for calculating the distance between instances to find the nearest neighbors. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval [0; 1]. Normalization with n guarantees all weights to be in the interval <ref> [1; 1] </ref>. The weight W [A] is used as an estimate of the quality of attribute A. The function diff is used also for calculating the distance between instances to find the nearest neighbors. The total distance is simply the sum of differences over all attributes. <p> Kononenko (1994) has shown that (1) is highly correlated with gini index <ref> (Breiman et al., 1984) </ref> and that it implicitly uses a kind of normalization for multi valued attributes. (impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency, see (Quinlan, 1986)).
Reference: 2. <author> Cestnik B. & Bratko I. </author> <title> (1991) On estimating probabilities in tree pruning, </title> <booktitle> Proc. European Working Session on Learning, (Porto, </booktitle> <month> March </month> <year> 1991), </year> <editor> Y.Kodratoff (ed.), </editor> <publisher> Springer Verlag. pp.138-150. </publisher>
Reference: 3. <author> Cestnik, B., Kononenko, I. & Bratko, I. </author> <title> (1987) ASSISTANT 86 : A knowledge elicitation tool for sophisticated users. </title> <editor> In: I. Bratko & N. Lavrac (eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference: 4. <author> Dzeroski S. </author> <title> (1991) Handling noise in inductive logic programming, M.SC. </title> <type> Thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering & computer science, Ljubljana, Slovenia. </institution>
Reference: 5. <author> Kira K. & Rendell L. </author> <title> (1992a) A practical approach to feature selection, </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <address> July 1992) D.Sleeman & P.Edwards (eds.), </address> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference: 6. <author> Kira K. & Rendell L. </author> <title> (1992b) The feature selection problem: traditional methods and new algorithm. </title> <booktitle> Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference: 7. <author> Kononenko I. </author> <title> (1994) Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. European Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994), </year> <editor> L. De Raedt & F.Bergadano (eds.), </editor> <publisher> Springer Verlag. (in press) </publisher>
Reference: 8. <author> Kononenko, I. & Bratko, I. </author> <title> (1991) Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80. </pages>
Reference: 9. <author> Kovacic M. </author> <title> (1994) Stochastic inductive logic programming, </title> <type> Ph.D. Thesis, </type> <institution> University of Ljubljana, Faculty of electr. eng. & computer sc., Ljubljana, Slovenia. </institution>
Reference-contexts: While the original RELIEF can deal with discrete and continuous attributes, it can not deal with incomplete data and is limited to two-class problems only. Kononenko <ref> (1994) </ref> developed an extension of RELIEF, called RELIEFF, that improves the original algorithm by estimating probabilities more reliably and extends it to handle the incomplete and multi-class data sets. RELIEFF seems to be a promising heuristic function that may overcome the myopia of current inductive learning algorithms, including ILP algorithms. <p> Kononenko <ref> (1994) </ref> has shown that (1) is highly correlated with gini index (Breiman et al., 1984) and that it implicitly uses a kind of normalization for multi valued attributes. (impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency, see (Quinlan, 1986)). <p> From the global point of view, when using classical impurity functions like information gain, these dependencies are hidden due to the effect of averaging over all training instances, and exactly this makes impurity functions myopic. Kononenko <ref> (1994) </ref> developed an extension of RELIEF, called RELIEFF, that improves the original algorithm by estimating probabilities more reliably (by running the outer loop of RELIEFF over all training instances and by using k-nearest hits/misses instead of only one nearest hit/miss) and extends it to deal with incomplete data (by generalizing the
Reference: 10. <author> Kovacic, M., Lavrac, N., Grobelnik, M., Zupanic, D., Mladenic, D. </author> <title> (1992) Stochastic Search in Inductive Logic Programming. </title> <booktitle> Proceedings of the tenth European Conference on Artificial Intelligence, </booktitle> <address> Vienna, </address> <year> 1992. </year>
Reference: 11. <author> Mladenic D. </author> <title> (1993) Combinatorial optimization in inductive concept learning. </title> <booktitle> Proc. 10th Intern. Conf. on Machine Learning. </booktitle> <address> (Amherst, June 1993), </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 205-211 </pages>
Reference: 12. <editor> Muggleton S. (ed.) </editor> <booktitle> (1992) Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference: 13. <author> U.Pompe, </author> <month> M.Kovacic & I.Kononenko </month> <year> (1993) </year> <month> SFOIL: </month> <title> Stochastic approach to inductive logic programming. </title> <booktitle> Proc. Slovenian Conf. </booktitle> <institution> on Electrical Engineering and Computer Science. Portoroz, Slovenia, </institution> <month> Sept. </month> <year> 1993, </year> <pages> pp 189-192. </pages>
Reference: 14. <author> Quinlan R. </author> <title> (1986) Induction of decision trees, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference: 15. <author> Quinlan R. </author> <title> (1990) Learning logical definitions from relations, </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266. </pages>
Reference: 16. <author> Smyth P., Goodman R.M. & Higgins C. </author> <title> (1990) A hybrid Rule-based Bayesian Classifier, </title> <booktitle> Proc.European Conf. on Artificial Intelligence, </booktitle> <address> Stockholm, </address> <month> August, </month> <year> 1990, </year> <pages> pp. 610-615. </pages> <editor> U. Pompe, I. </editor> <title> Kononenko: ILP with RELIEFF 11 </title>
References-found: 16

