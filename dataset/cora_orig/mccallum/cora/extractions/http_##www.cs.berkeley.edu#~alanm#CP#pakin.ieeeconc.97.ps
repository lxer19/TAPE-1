URL: http://www.cs.berkeley.edu/~alanm/CP/pakin.ieeeconc.97.ps
Refering-URL: http://www.cs.berkeley.edu/~alanm/CP/bib.html
Root-URL: 
Email: fpakin,vijayk,achieng@cs.uiuc.edu  
Title: Fast Messages (FM): Efficient, Portable Communication for Workstation Clusters and Massively-Parallel Processors  
Author: Scott Pakin Vijay Karamcheti Andrew A. Chien 
Keyword: communication, message-passing, massively-parallel processing, workstation clusters, network interface, software messaging layer, flow control, reliable delivery, in-order delivery  
Date: January 27, 1997  
Address: 1304 W. Springfield Avenue Urbana, IL 61801  
Affiliation: Department of Computer Science University of Illinois  
Abstract: Illinois Fast Messages (FM) is a low-level software messaging layer designed to meet the demands of high performance network hardware. It delivers much of the hardware's raw performance to both applications and higher-level messaging layers. FM presents an architectural interface which is both portable and amenable to high-performance implementations on massively-parallel computers and networks of workstations. By providing key services|buffer management and ordered, reliable delivery|FM enables the simplification and streamlining of higher level communication layers. FM also decouples the processor and the network, lending control over scheduling communication processing to software built on top of FM. This, in turn, minimizes communication's impact on local computation performance (e.g. by preserving cached data). We have built several implementations of the FM interface on the Cray T3D and a Myrinet-based workstation cluster. These implementations demonstrate that FM can deliver much of the underlying hardware's performance. On the Cray T3D, FM achieves 4.9 s total overhead and 6.1 s latency for a minimal-sized message, substantially better than other messaging layers on that platform, most notably PVM. T3D FM reaches a peak bandwidth of 112 MB/s out of a hardware limit of 130 MB/s. On a SPARCstation-based subset of our Myrinet cluster, FM achieves 4.4 s overhead and 13.1 s latency for a minimal-sized message, again substantially better than other messaging layers on that platform, most notably the Myrinet API. Its peak bandwidth of 17.5 MB/s is fairly close to the SBus programmed I/O limitation. FM's performance continues to improve with newer networking hardware and advances in our implementation techniques. On a pair of Pentium Pro-based PCs, Myrinet FM reaches a peak bandwidth of 56.3 MB/s with a minimum latency of 11.5 s. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Remzi H. Arpaci, David E. Culler, Arvind Krishnamurthy, Steve G. Steinberg, and Katherine Yelick. </author> <title> Empirical evaluation of the CRAY-T3D: A compiler perspective. </title> <booktitle> In Proceeedings of the International Symposium on Computer Architecture, </booktitle> <pages> pages 320-331, </pages> <month> June </month> <year> 1995. </year> <note> Available from http://www.cs.berkeley.edu/~yelick/arvindk/t3d-isca95.ps. </note>
Reference-contexts: The T3D supports up to 4096 processors and is a shared address space machine with no hardware support for cache coherence. However, because memory access is non-uniform both in mechanism (special addressing mechanisms for remote locations) and performance (4-5 times slower than accesses to remote memory <ref> [1] </ref>), a message-passing programming style can be beneficial for performance as well as portability and relative ease of programming. The key relevant features of the T3D are all implemented by the network interface unit (Figure 1), called the annex or DTB annex.
Reference: [2] <author> Ray Barriuso and Allan Knies. </author> <title> SHMEM User's Guide. Cray Research, </title> <publisher> Inc., </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: of Pull FM is that extra work is required to pull data across the network, increasing the overhead and latency slightly compared to Push FM [29]. 4.3 Performance To place the performance of T3D FM in context, we compare it to two other vendor-supplied communication layers on the T3D: SHMEM <ref> [2] </ref> and PVM [20]. SHMEM is a low-level data movement| not messaging|library that copies data between addresses in the shared address space. PVM is a full-featured messaging layer that does both the end-to-end synchronization and buffer management required for traditional messaging.
Reference: [3] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual memory mapped network interface for the SHRIMP multicomputer. </title> <booktitle> In Proceeding of the International Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year> <note> Available from http://www.cs.princeton.edu/shrimp/papers/isca94.paper.ps. </note>
Reference-contexts: All data transmission is performed at user-level. This approach is exemplified by Hamlyn [8], Cranium [34], U-Net [43], and SHRIMP <ref> [3] </ref>.
Reference: [4] <author> Nanette J. Boden, Danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. </author> <title> Myrinet|a gigabit-per-second local-area network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year> <note> Available from http://www.myri.com/research/publications/Hot.ps. 27 </note>
Reference-contexts: However, higher resolutions, higher frame rates, and less efficient, but faster, encodings are often of interest. Fortunately, a number of new, higher-bandwidth networks have recently hit the market. These include FDDI [18], 100 Base-T Ethernet [22], FibreChannel [46], Myrinet <ref> [4] </ref>, and ATM/SONET [6] and currently run from 100 Mbps to over 600 Mbps, with the ability to scale to Gbps bandwidths and beyond. Not only are these new networks faster, but they are often more reliable. That is, dropped or corrupted data packets occur less frequently. <p> Communication performance delivered by FM is close to the maximum achievable by the underlying hardware. 11 5 Myrinet FM Implementation Our workstation cluster implementation of FM utilizes Myricom's 640 Mbps switched network <ref> [4] </ref> and a collection of Sun SPARCstation workstations [40]. We first discuss the hardware structure of Myrinet. The network exhibits per-hop latencies of around 0.5 s. Packets are wormhole-routed [15], so if an output port is busy, the packet is blocked in place. <p> Less aggressive parallel architectures use commodity microprocessors and add external communication logic (e.g. the TMC CM-5 [42] and Cray T3D [14]). These machines are quite similar to workstation clusters based on, for example, Myricom's Myrinet <ref> [4] </ref> or DEC's Memory Channel [21]. Work on Active Messages [44] bridged the gap between the aggressive integration of communication into the processor and these hybrid systems, showing how much of the communication performance benefit could be reaped on commodity microprocessor based systems.
Reference: [5] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H. T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peter--son, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting systolic and memory communication in iWarp. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 70-81. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1990. </year>
Reference-contexts: We briefly survey efforts in hardware architectures and more recently software interfaces which bring high performance communication to applications. Hardware architects have long pursued high performance for short messages by integrating the network interface with the CPU itself <ref> [5, 16, 25, 45] </ref>. Innovations included interfacing the network to the instruction set and register file, message-field based dispatch, communication driven scheduling, etc. However, none of these innovations persisted into mainstream high performance processors because of their requirement of changes deep in the processor. <p> The original FM 1.1 interface is closely modeled on that of Active Messages (AM) [44], which in turn has as its intellectual antecedents hardware architectures that closely integrated communication and computation in message-driven [16], dataflow [26], and even systolic <ref> [5] </ref> architectures. We briefly describe the key similarities and differences between the FM 1.1 system and Active Messages. Note that the FM 2.0 systems significantly extend the functionality of the interface, as shown in Sidebar C.
Reference: [6] <author> J. Boudec. </author> <title> The Asynchronous Transfer Mode: A tutorial. </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> 24 </volume> <pages> 279-309, </pages> <year> 1992. </year>
Reference-contexts: However, higher resolutions, higher frame rates, and less efficient, but faster, encodings are often of interest. Fortunately, a number of new, higher-bandwidth networks have recently hit the market. These include FDDI [18], 100 Base-T Ethernet [22], FibreChannel [46], Myrinet [4], and ATM/SONET <ref> [6] </ref> and currently run from 100 Mbps to over 600 Mbps, with the ability to scale to Gbps bandwidths and beyond. Not only are these new networks faster, but they are often more reliable. That is, dropped or corrupted data packets occur less frequently.
Reference: [7] <author> Lawrence S. Brakmo and Larry L. Peterson. </author> <title> TCP Vegas: End to end congestion avoidance on a global internet. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 13(8) </volume> <pages> 1465-1480, </pages> <month> October </month> <year> 1995. </year> <note> Available from ftp://ftp.cs.arizona.edu/xkernel/Papers/jsac.ps. </note>
Reference-contexts: Recent efforts driven by academia, industry, and the various gigabit testbeds have pushed towards delivering gigabit performance to the application through standard interfaces and protocols such as TCP/IP. While such efforts have made significant progress <ref> [7, 10] </ref>, high data rates are achieved only for extremely large messages, as exemplified by the CASA gigabit testbed [13]. In practice, however, typical TCP/IP packet size distributions are almost exclusively under 200 bytes of data [24], with a large fraction under 10 bytes [9].
Reference: [8] <author> Greg Buzzard, David Jacobson, Scott Marovich, and John Wilkes. Hamlyn: </author> <title> A high-performance network interface with sender-based memory management. </title> <booktitle> In Proceedings of the IEEE Hot Interconnects Symposium, </booktitle> <month> August </month> <year> 1995. </year> <note> Available from http://www.hpl.hp.com/personal/John Wilkes/papers/ HamlynHotIntIII.pdf. </note>
Reference-contexts: Hence, the operating system is used solely for initializing the network interface, establishing connections with other nodes, mapping network interface control registers and memory buffers into user space, and tearing down connections when communication is complete. All data transmission is performed at user-level. This approach is exemplified by Hamlyn <ref> [8] </ref>, Cranium [34], U-Net [43], and SHRIMP [3]. <p> A number of recent efforts have explored novel low-level communication interfaces. U-Net [43] provides buffer management, but no flow control, implying that data can be lost due to rate mismatch. The low-level interface supporting the Hamlyn <ref> [8] </ref> architecture provides flow control, but no buffer management, implying that higher-level messaging layers must provide their own buffer-management routines to prevent data from being overwritten. FM provides a more complete solution to ensuring message delivery.
Reference: [9] <author> Ramon Caceres. </author> <title> Multiplexing Traffic at the Entrance to Wide-Area Networks. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: In practice, however, typical TCP/IP packet size distributions are almost exclusively under 200 bytes of data [24], with a large fraction under 10 bytes <ref> [9] </ref>. The ability to sustain high data rates on messages of those sizes remains an imposing challenge. An important concurrent trend to the rapid increase in network hardware performance is the rapid increase in microprocessors' computing performance.
Reference: [10] <author> Chran-Ham Chang, Richard Flower, John Forecast, Heather Gray, William R. Hawe, K. K. Ramakr-ishnan, Ashok P. Nadkarni, Uttam N. Shikarpur, and Kathleen M. Wilde. </author> <title> High-performance TCP/IP and UDP/IP networking in DEC OSF/1 for Alpha AXP. </title> <journal> Digital Technical Journal, </journal> <volume> 5(1), </volume> <month> Winter </month> <year> 1993. </year> <note> Available from ftp://ftp.digital.com/pub/Digital/info/DTJ/nw-06-tcp.ps. </note>
Reference-contexts: Recent efforts driven by academia, industry, and the various gigabit testbeds have pushed towards delivering gigabit performance to the application through standard interfaces and protocols such as TCP/IP. While such efforts have made significant progress <ref> [7, 10] </ref>, high data rates are achieved only for extremely large messages, as exemplified by the CASA gigabit testbed [13]. In practice, however, typical TCP/IP packet size distributions are almost exclusively under 200 bytes of data [24], with a large fraction under 10 bytes [9].
Reference: [11] <author> John Chapin, Stephen A. Herrod, Mendel Rosenblum, and Anoop Gupta. </author> <title> Memory system performance of UNIX on CC-NUMA multiprocessors. </title> <booktitle> In Proceedings of SIGMETRICS/PERFORMANCE, </booktitle> <month> May </month> <year> 1995. </year> <note> Available from http://www-flash.stanford.edu/OS/papers/SIGMETRICS95/numa-os.ps.Z. </note>
Reference-contexts: When networks were unreliable, this practice made sense, but modern networks are highly reliable, so such discarding is the major source of data|and therefore, performance|loss. Experience with messaging layers in multicomputers [39], shared memory systems <ref> [11] </ref>, and high speed wide-area networks indicate that cache interference is a critical effect for both communication 2 FM send () and FM send 4 () call FM extract () only when necessary to avoid buffer deadlock. 6 and local computational performance.
Reference: [12] <author> Andrew Chien, Julian Dolby, Bishwaroop Ganguly, Vijay Karamcheti, and Xingbin Zhang. </author> <title> Supporting high level programming with high performance: The illinois concert system. </title> <booktitle> In Proceedings of the Second International Workshop on High-level Parallel Programming Models and Supportive Environments, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: On the other hand, if a messaging layer's guarantees are too strong (i.e. they provide more functionality than is generally needed), the messaging layer's common-case performance may be needlessly degraded. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [12, 28, 29, 30] </ref> have led to the conclusion that a low-level messaging layer should provide the following key guarantees: * Reliable delivery, * Ordered delivery, and * Control over scheduling of communication work (decoupling).
Reference: [13] <author> Bilal Chinoy and Kevin Fall. </author> <title> TCP/IP and HIPPI performance in the CASA gigabit testbed. </title> <booktitle> In Proceedings of the 1994 USENIX Symposium on High-Speed Networking, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: While such efforts have made significant progress [7, 10], high data rates are achieved only for extremely large messages, as exemplified by the CASA gigabit testbed <ref> [13] </ref>. In practice, however, typical TCP/IP packet size distributions are almost exclusively under 200 bytes of data [24], with a large fraction under 10 bytes [9]. The ability to sustain high data rates on messages of those sizes remains an imposing challenge.
Reference: [14] <author> Cray Research, Inc. </author> <title> Cray T3D System Architecture Overview, </title> <month> March </month> <year> 1993. </year>
Reference-contexts: Likewise, applications requiring data security can easily layer encryption atop FM by writing a routine which encrypts data in place when passed a pointer to a memory buffer. 4 Cray T3D FM Implementation Our first implementation of FM was on the Cray T3D <ref> [14] </ref>. This implementation exploits the high-speed interconnect and featureful network interface unit to achieve excellent communication performance. The T3D supports up to 4096 processors and is a shared address space machine with no hardware support for cache coherence. <p> Also, these innovations did not address issues of network and output contention such as Pull FM is designed to tolerate. Less aggressive parallel architectures use commodity microprocessors and add external communication logic (e.g. the TMC CM-5 [42] and Cray T3D <ref> [14] </ref>). These machines are quite similar to workstation clusters based on, for example, Myricom's Myrinet [4] or DEC's Memory Channel [21].
Reference: [15] <author> W. J. Dally and C. Seitz. </author> <title> The torus routing chip. </title> <booktitle> Distributed Computing, </booktitle> <pages> pages 187-196, </pages> <year> 1986. </year>
Reference-contexts: We first discuss the hardware structure of Myrinet. The network exhibits per-hop latencies of around 0.5 s. Packets are wormhole-routed <ref> [15] </ref>, so if an output port is busy, the packet is blocked in place. Packets blocked for greater than 50 ms are dropped.
Reference: [16] <author> William J. Dally, Linda Chao, Andrew Chien, Soha Hassoun, Waldemar Horwat, Jon Kaplan, Paul Song, Brian Totty, and Scott Wills. </author> <title> Architecture of a message-driven processor. </title> <booktitle> In Proceedings of the 14th ACM/IEEE Symposium on Computer Architecture, </booktitle> <pages> pages 189-196. </pages> <publisher> IEEE, </publisher> <month> June </month> <year> 1987. </year>
Reference-contexts: We briefly survey efforts in hardware architectures and more recently software interfaces which bring high performance communication to applications. Hardware architects have long pursued high performance for short messages by integrating the network interface with the CPU itself <ref> [5, 16, 25, 45] </ref>. Innovations included interfacing the network to the instruction set and register file, message-field based dispatch, communication driven scheduling, etc. However, none of these innovations persisted into mainstream high performance processors because of their requirement of changes deep in the processor. <p> The original FM 1.1 interface is closely modeled on that of Active Messages (AM) [44], which in turn has as its intellectual antecedents hardware architectures that closely integrated communication and computation in message-driven <ref> [16] </ref>, dataflow [26], and even systolic [5] architectures. We briefly describe the key similarities and differences between the FM 1.1 system and Active Messages. Note that the FM 2.0 systems significantly extend the functionality of the interface, as shown in Sidebar C.
Reference: [17] <author> Jack J. Dongarra and Tom Dunigan. </author> <title> Message-passing performance of various computers. </title> <type> Technical Report CS-95-299, </type> <institution> University of Tennessee, </institution> <month> August </month> <year> 1995. </year> <note> Available from http://www.netlib.org/ tennessee/ut-cs-95-299.ps. </note>
Reference-contexts: The additional latency that the MPI processing overhead incurs is only 5 s, which is modest compared to many other implementations <ref> [17] </ref>. For comparison, performance data for the IBM SP2's two versions of MPI is also included in Figure 7. "MPICH (SP2)" represents a port of MPICH to the SP2, and "MPIF (SP2)" represents an MPI implementation written and optimized specifically for the SP2.
Reference: [18] <institution> Fiber-distributed data interface (FDDI)|Token ring media access control (MAC). American National Standard for Information Systems ANSI X3.139-1987, </institution> <month> July </month> <year> 1987. </year> <institution> American National Standards Institute. </institution>
Reference-contexts: Compression (e.g. MPEG or MPEG-II) can greatly reduce the bandwidth requirement at some cost in latency. However, higher resolutions, higher frame rates, and less efficient, but faster, encodings are often of interest. Fortunately, a number of new, higher-bandwidth networks have recently hit the market. These include FDDI <ref> [18] </ref>, 100 Base-T Ethernet [22], FibreChannel [46], Myrinet [4], and ATM/SONET [6] and currently run from 100 Mbps to over 600 Mbps, with the ability to scale to Gbps bandwidths and beyond. Not only are these new networks faster, but they are often more reliable.
Reference: [19] <author> G. Fox et al. </author> <title> Solving Problems on Concurrent Processors, volume I and II. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: Messaging is a well-established communication mechanism of wide utility for parallel coordination <ref> [19] </ref>. The FM interface design is similar to Berkeley Active Messages [44] on the CM-5|a model of simplicity and functionality| but with a few critical distinctions detailed in this section. Primarily, FM borrows the notion of message handlers and uses essentially the same API.
Reference: [20] <author> G. Geist and V. Sunderam. </author> <title> The PVM system: Supercomputer level concurrent computation on a heterogeneous network of workstations. </title> <booktitle> In Proceedings of the Sixth Distributed Memory Computers Conference, </booktitle> <pages> pages 258-61, </pages> <year> 1991. </year>
Reference-contexts: is that extra work is required to pull data across the network, increasing the overhead and latency slightly compared to Push FM [29]. 4.3 Performance To place the performance of T3D FM in context, we compare it to two other vendor-supplied communication layers on the T3D: SHMEM [2] and PVM <ref> [20] </ref>. SHMEM is a low-level data movement| not messaging|library that copies data between addresses in the shared address space. PVM is a full-featured messaging layer that does both the end-to-end synchronization and buffer management required for traditional messaging. The PVM implementation was optimized by Cray for use on the T3D.
Reference: [21] <author> Richard B. Gillett. </author> <title> Memory Channel network for PCI. </title> <journal> IEEE Micro, </journal> <volume> 16(1) </volume> <pages> 12-18, </pages> <month> February </month> <year> 1996. </year> <note> Available from http://www.computer.org/pubs/micro/web/m1gil.pdf. 28 </note>
Reference-contexts: Less aggressive parallel architectures use commodity microprocessors and add external communication logic (e.g. the TMC CM-5 [42] and Cray T3D [14]). These machines are quite similar to workstation clusters based on, for example, Myricom's Myrinet [4] or DEC's Memory Channel <ref> [21] </ref>. Work on Active Messages [44] bridged the gap between the aggressive integration of communication into the processor and these hybrid systems, showing how much of the communication performance benefit could be reaped on commodity microprocessor based systems.
Reference: [22] <author> Lee Goldberg. 100Base-T4 transceiver simplifies adapter, repeater, </author> <title> and switch designs. </title> <booktitle> Electronic Design, </booktitle> <pages> pages 155-160, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: MPEG or MPEG-II) can greatly reduce the bandwidth requirement at some cost in latency. However, higher resolutions, higher frame rates, and less efficient, but faster, encodings are often of interest. Fortunately, a number of new, higher-bandwidth networks have recently hit the market. These include FDDI [18], 100 Base-T Ethernet <ref> [22] </ref>, FibreChannel [46], Myrinet [4], and ATM/SONET [6] and currently run from 100 Mbps to over 600 Mbps, with the ability to scale to Gbps bandwidths and beyond. Not only are these new networks faster, but they are often more reliable.
Reference: [23] <author> William Gropp and Ewing Lusk. </author> <title> User's guide for mpich, a portable implementation of MPI. </title> <type> Technical Report ANL/MCS-TM-ANL-96/6, </type> <institution> Argonne National Laboratory, Mathematics and Computer Science Division, </institution> <year> 1996. </year> <note> Available from http://www.mcs.anl.gov/mpi/mpiuserguide/paper.html and ftp://info.mcs.anl.gov/pub/mpi/userguide.ps.Z. </note>
Reference-contexts: MPI is a standard of increasing importance for both parallel and distributed message-passing programs. Sockets are a long-standing standard for building multiprocess and, particularly, client-server applications. We first consider the performance of our implementation of MPI atop FM (MPI-FM). Our implementation is a port of the MPICH <ref> [23] </ref> implementation of MPI. However, we have performed significant restructuring of the code to reduce both buffer copies and control path overhead. Details of the restructuring costs can be found in [32].
Reference: [24] <author> Riccardo Gusella. </author> <title> A measurement study of diskless workstation traffic on Ethernet. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(9), </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: While such efforts have made significant progress [7, 10], high data rates are achieved only for extremely large messages, as exemplified by the CASA gigabit testbed [13]. In practice, however, typical TCP/IP packet size distributions are almost exclusively under 200 bytes of data <ref> [24] </ref>, with a large fraction under 10 bytes [9]. The ability to sustain high data rates on messages of those sizes remains an imposing challenge. An important concurrent trend to the rapid increase in network hardware performance is the rapid increase in microprocessors' computing performance.
Reference: [25] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A tightly-coupled processor-network interface. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 111-122, </pages> <address> Boston, Massachusetts, </address> <month> October </month> <year> 1992. </year> <note> Available from ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-342.ps.gz. </note>
Reference-contexts: We briefly survey efforts in hardware architectures and more recently software interfaces which bring high performance communication to applications. Hardware architects have long pursued high performance for short messages by integrating the network interface with the CPU itself <ref> [5, 16, 25, 45] </ref>. Innovations included interfacing the network to the instruction set and register file, message-field based dispatch, communication driven scheduling, etc. However, none of these innovations persisted into mainstream high performance processors because of their requirement of changes deep in the processor.
Reference: [26] <author> James Hicks, Derek Chiou, Boon Seong Ang, and Arvind. </author> <title> Performance studies of Id on the Monsoon dataflow system. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3) </volume> <pages> 273-300, </pages> <month> July </month> <year> 1993. </year> <note> Available from ftp://csg-ftp.lcs.mit.edu/pub/papers/csgmemo/memo-345-3.ps.gz or http://www.csg.lcs.mit.edu:8001/monsoon/monsoon-performance/monsoon-performance.html. </note>
Reference-contexts: The original FM 1.1 interface is closely modeled on that of Active Messages (AM) [44], which in turn has as its intellectual antecedents hardware architectures that closely integrated communication and computation in message-driven [16], dataflow <ref> [26] </ref>, and even systolic [5] architectures. We briefly describe the key similarities and differences between the FM 1.1 system and Active Messages. Note that the FM 2.0 systems significantly extend the functionality of the interface, as shown in Sidebar C.
Reference: [27] <author> A. Jinzaki, T. Ni'inomi, and S. Kobayashi. </author> <title> Illinois Fast Messages on 1 Gbps Fibre Channel. </title> <booktitle> In the ASPLOS-VII NOW/Cluster Workshop, </booktitle> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year> <note> Available from http://www-csag.cs.uiuc.edu/individual/achien/asplos/posters/fm-fibrechannel.ps. </note>
Reference-contexts: The FM interface is platform-independent and therefore easily adaptable to new architectures. We have implemented FM on two platforms: the Cray T3D and workstation clusters connected by a Myrinet network. Independent from us, a group at Fujitsu implemented FM on FibreChannel <ref> [27] </ref>, which further substantiates the portability of the interface. The base philosophy of FM is described in this section, and the various implementations are detailed in subsequent sections. For each implementation, we explain the flow control and buffering schemes we used to implement sender-receiver decoupling.
Reference: [28] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Software overhead in messaging layers: </title> <booktitle> Where does the time go? In Proceedings of the Sixth Symposium on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/ asplos94.ps. </note>
Reference-contexts: On the other hand, if a messaging layer's guarantees are too strong (i.e. they provide more functionality than is generally needed), the messaging layer's common-case performance may be needlessly degraded. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [12, 28, 29, 30] </ref> have led to the conclusion that a low-level messaging layer should provide the following key guarantees: * Reliable delivery, * Ordered delivery, and * Control over scheduling of communication work (decoupling). <p> Previous studies of communication cost in the CM-5 multicomputer system <ref> [28] </ref> indicate that software overhead for reliability, fault tolerance, and ordering can increase communication cost by over 200%, yet many low-level communication systems simplify their implementation by discarding packets.
Reference: [29] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> A comparison of architectural support for messaging on the TMC CM-5 and the Cray T3D. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/cm5-t3d-messaging.ps. </note>
Reference-contexts: On the other hand, if a messaging layer's guarantees are too strong (i.e. they provide more functionality than is generally needed), the messaging layer's common-case performance may be needlessly degraded. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [12, 28, 29, 30] </ref> have led to the conclusion that a low-level messaging layer should provide the following key guarantees: * Reliable delivery, * Ordered delivery, and * Control over scheduling of communication work (decoupling). <p> While push messaging minimizes latency at low network loads, performance can degrade if there is output contention or if the receiver allows its incoming buffers to fill by not servicing the network often enough <ref> [29] </ref>. If messages arrive at a receiver faster than the receiver's memory can process them or the receiver extracts them, the writes back up into the network, adding to network contention and increasing average latency. <p> The buffer allocation problem is transferred back to the senders, where flow control of ill-behaved senders is easily achieved. One drawback of Pull FM is that extra work is required to pull data across the network, increasing the overhead and latency slightly compared to Push FM <ref> [29] </ref>. 4.3 Performance To place the performance of T3D FM in context, we compare it to two other vendor-supplied communication layers on the T3D: SHMEM [2] and PVM [20]. SHMEM is a low-level data movement| not messaging|library that copies data between addresses in the shared address space. <p> While it may appear that Pull FM is unnecessary|Push FM exhibits superior latency and bandwidth|it performs much better that Push FM in the presence of heavy network traffic, especially when communication patterns are irregular. (See <ref> [29] </ref> for details). The Concert runtime [30], for instance, uses exclusively Pull FM because the communication irregularity inherent to Concert programs makes communication robustness dominate overall performance more than baseline latency or bandwidth.
Reference: [30] <author> Vijay Karamcheti, John Plevyak, and Andrew A. Chien. </author> <title> Runtime mechanisms for efficient dynamic multithreading. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 37 </volume> <pages> 21-40, </pages> <year> 1996. </year>
Reference-contexts: On the other hand, if a messaging layer's guarantees are too strong (i.e. they provide more functionality than is generally needed), the messaging layer's common-case performance may be needlessly degraded. Analysis of the literature and our ongoing studies to support fine-grained parallel computing <ref> [12, 28, 29, 30] </ref> have led to the conclusion that a low-level messaging layer should provide the following key guarantees: * Reliable delivery, * Ordered delivery, and * Control over scheduling of communication work (decoupling). <p> While it may appear that Pull FM is unnecessary|Push FM exhibits superior latency and bandwidth|it performs much better that Push FM in the presence of heavy network traffic, especially when communication patterns are irregular. (See [29] for details). The Concert runtime <ref> [30] </ref>, for instance, uses exclusively Pull FM because the communication irregularity inherent to Concert programs makes communication robustness dominate overall performance more than baseline latency or bandwidth.
Reference: [31] <author> Kimberly K. Keeton, Thomas E. Anderson, and David A. Patterson. </author> <title> LogP quantified: The case for low-overhead local area networks. In Hot Interconnects III: A Symposium on High Performance Interconnects, </title> <note> 1995. Available from http://now.cs.berkeley.edu/Papers/Papers/hotinter95-tcp.ps. </note>
Reference-contexts: This shows that because FM provides important guarantees, higher-level messaging layers can achieve performance approaching FM's. To put the FM Sockets performance in perspective, we compare it to the performance of Myri-com's TCP and UDP implementations. According to third-party measurements <ref> [31] </ref>, Myricom's TCP has an 8-byte one-way latency of 751.0 s and their UDP has an 8-byte one-way latency of 721.5 s. FM Sockets' performance is an order of magnitude better|only 34.8 s for TCP and 33.3 s for UDP.
Reference: [32] <author> Mario Lauria and Andrew Chien. </author> <title> MPI-FM: High performance MPI on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1997. To appear; currently available from http://www-csag.cs.uiuc.edu/papers/mpi-fm.ps. </note>
Reference-contexts: Our implementation is a port of the MPICH [23] implementation of MPI. However, we have performed significant restructuring of the code to reduce both buffer copies and control path overhead. Details of the restructuring costs can be found in <ref> [32] </ref>. The data illustrated in Figure 7 indicates that the high performance of the underlying network hardware is indeed made accessible to higher level messaging layers through FM. <p> While MPI imposes some additional processing overhead|which reduces performance for short messages| we have worked hard to minimize this effect, and near-peak network performance is achieved for messages of modest size|as small as 256 bytes <ref> [32] </ref>. The additional latency that the MPI processing overhead incurs is only 5 s, which is modest compared to many other implementations [17]. <p> Measurements revealed that MPI-FM was spending a significant amount of time doing this data copy. We discovered that by eliminating this copy with even a simple two-piece gather, performance improved significantly <ref> [32] </ref>. Similarly, we found that on the receive side, stripping the message header can be made faster by adding scatter functionality. Flow control between FM and FM Sockets was another problem we observed. <p> MPI-FM adds protocol-specific headers to the beginning of each message. Without gather/scatter built into FM, higher-level messaging layers must copy program data twice: once on the send side to append protocol-specific headers and once on the receive side to strip those headers. These extra copies hurt performance significantly <ref> [32] </ref>. Traditional gather vectors (lists of &lt;length,offset&gt; pairs sent in sequence) remove copies on the send side, but traditional scatter vectors are insufficient for removing copies on the receive side, as the target location of a message is known only when the message header is parsed.
Reference: [33] <author> S. J. Le*er, M. K. McKusick, M. J. Karels, and J. S. Quaterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: The performance continues to improve with newer networking hardware and advances in our implementation techniques. See Sidebar C for our latest performance results. Using two widely-accepted standard interfaces, UNIX sockets <ref> [33] </ref> and the Message Passing Interface (MPI) [35], we demonstrate that FM can be used to build higher level protocols which deliver much of the underlying hardware performance. The key design choices and their ramifications are discussed in detail. <p> A successful low-level messaging layer must enable the construction of efficient higher level messaging software. To evaluate our FM interface design from this perspective, we have built two higher level messaging interfaces atop FM: the Message Passing Interface (MPI) [35] and UNIX sockets <ref> [33] </ref>. MPI is a standard of increasing importance for both parallel and distributed message-passing programs. Sockets are a long-standing standard for building multiprocess and, particularly, client-server applications. We first consider the performance of our implementation of MPI atop FM (MPI-FM).
Reference: [34] <author> Neil R. McKenzie, Kevin Bolding, Carl Ebeling, and Lawrence Snyder. Cranium: </author> <title> An interface for message passing on adaptive packet routing networks. </title> <booktitle> In Proceedings of the 1994 Parallel Computer Routing and Communication Workshop, </booktitle> <month> May </month> <year> 1994. </year> <note> Available from ftp://shrimp.cs.washington.edu/pub/ chaos/docs/cranium-pcrcw.ps.Z. </note>
Reference-contexts: All data transmission is performed at user-level. This approach is exemplified by Hamlyn [8], Cranium <ref> [34] </ref>, U-Net [43], and SHRIMP [3].
Reference: [35] <author> Message Passing Interface Forum. </author> <title> The MPI message passing interface standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, </institution> <month> April </month> <year> 1994. </year> <note> Available from http://www.mcs.anl.gov/ mpi/mpi-report.ps. </note>
Reference-contexts: The performance continues to improve with newer networking hardware and advances in our implementation techniques. See Sidebar C for our latest performance results. Using two widely-accepted standard interfaces, UNIX sockets [33] and the Message Passing Interface (MPI) <ref> [35] </ref>, we demonstrate that FM can be used to build higher level protocols which deliver much of the underlying hardware performance. The key design choices and their ramifications are discussed in detail. <p> A successful low-level messaging layer must enable the construction of efficient higher level messaging software. To evaluate our FM interface design from this perspective, we have built two higher level messaging interfaces atop FM: the Message Passing Interface (MPI) <ref> [35] </ref> and UNIX sockets [33]. MPI is a standard of increasing importance for both parallel and distributed message-passing programs. Sockets are a long-standing standard for building multiprocess and, particularly, client-server applications. We first consider the performance of our implementation of MPI atop FM (MPI-FM).
Reference: [36] <author> R. Metcalfe and D. Boggs. </author> <title> Ethernet: Distributed packet-switching for local computer networks. </title> <journal> Communications of the Association for Computing Machinery, </journal> <volume> 19(7) </volume> <pages> 395-404, </pages> <year> 1976. </year> <month> 29 </month>
Reference-contexts: Today, most local area networks are interconnected via Ethernet <ref> [36] </ref>. Ethernet is a bus interconnect that runs at 10 Mbps in almost all installations. However, 10 Mbps is far too little bandwidth for new, network-intensive applications, such as those utilizing multimedia. For example, even television-sized video displayed with 8 bits/pixel and running at 30 frames/second requires approximately 63 Mbps.
Reference: [37] <author> Scott Pakin, Mario Lauria, and Andrew Chien. </author> <title> High performance messaging on workstations: Illinois Fast Messages (FM) for Myrinet. </title> <booktitle> In Supercomputing, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www-csag.cs.uiuc.edu/papers/myrinet-fm-sc95.ps. </note>
Reference-contexts: Although doing so makes the low-level layer appear fast, it generally has a detrimental affect on actual, application-level performance, because higher-level layers must add the missing functionality, often at a higher price than the lower-level layer would have paid. FM sacrifices a tiny amount of its raw performance <ref> [37] </ref> for a large payoff to the overall performance of the system. By providing generally useful service guarantees such as reliable, in-order delivery, FM saves messaging layers and applications built on top of it from the burden and performance penalty of adding those features.
Reference: [38] <author> Patrick Sobalvarro, Scott Pakin, Andrew Chien, and William Weihl. FM-DCS: </author> <title> An implementation of dynamic coscheduling on a network of workstations. </title> <booktitle> In the ASPLOS-VII NOW/Cluster Workshop, </booktitle> <address> Cam-bridge, Massachusetts, </address> <month> October </month> <year> 1996. </year> <note> Available from http://www-csag.cs.uiuc.edu/individual/ achien/asplos/posters/fm-dcs.ps. </note>
Reference-contexts: A better approach is to integrate communication and scheduling. That is, FM and the operating system should work together to coschedule communicating processes by exploiting local process information and information about message arrival and launch. We have already implemented a version of FM that does exactly that <ref> [38] </ref>, but our implementation is limited to a single communicating thread per node. Ongoing work involves generalizing the algorithm and implementation to support an arbitrary number of threads and processes per node.
Reference: [39] <author> Craig B. Stunkel and W. Kent Fuchs. </author> <title> An analysis of cache performance for a hypercube multicomputer. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(4) </volume> <pages> 421-432, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: When networks were unreliable, this practice made sense, but modern networks are highly reliable, so such discarding is the major source of data|and therefore, performance|loss. Experience with messaging layers in multicomputers <ref> [39] </ref>, shared memory systems [11], and high speed wide-area networks indicate that cache interference is a critical effect for both communication 2 FM send () and FM send 4 () call FM extract () only when necessary to avoid buffer deadlock. 6 and local computational performance.
Reference: [40] <institution> Sun Microsystems Computer Company, Mountain View, </institution> <address> CA. </address> <booktitle> SPARCstation 20 System Architecture, </booktitle> <year> 1995. </year>
Reference-contexts: Communication performance delivered by FM is close to the maximum achievable by the underlying hardware. 11 5 Myrinet FM Implementation Our workstation cluster implementation of FM utilizes Myricom's 640 Mbps switched network [4] and a collection of Sun SPARCstation workstations <ref> [40] </ref>. We first discuss the hardware structure of Myrinet. The network exhibits per-hop latencies of around 0.5 s. Packets are wormhole-routed [15], so if an output port is busy, the packet is blocked in place. Packets blocked for greater than 50 ms are dropped.
Reference: [41] <author> Hiroshi Tezuka, Atsushi Hori, and Yutaka Ishikawa. </author> <title> Design and Implementation of PM: A Communication Library for Workstation Cluster. </title> <booktitle> In JSPP, </booktitle> <year> 1996. </year> <title> (In Japanese). </title>
Reference-contexts: FM provides a more complete solution to ensuring message delivery. By removing the responsibility for flow control and buffer management from higher-level messaging layers, the FM interface permits platform-specific implementations|hence, optimizations|of flow control and buffer management. The Real World Computing Partnership's PM <ref> [41] </ref>, like FM, runs on Myrinet-connected workstations and performs both flow control and buffer management. However, the PM implementation uses an optimistic flow control mechanism and variable-sized packets. We are currently exploring opportunities to make a detailed comparison. 9 Summary Illinois Fast Messages (FM) is a high-performance, low-level messaging layer.
Reference: [42] <institution> Thinking Machines Corporation, </institution> <address> 245 First Street, Cambridge, MA 02154-1264. </address> <booktitle> The Connection Machine CM-5 Technical Summary, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Also, these innovations did not address issues of network and output contention such as Pull FM is designed to tolerate. Less aggressive parallel architectures use commodity microprocessors and add external communication logic (e.g. the TMC CM-5 <ref> [42] </ref> and Cray T3D [14]). These machines are quite similar to workstation clusters based on, for example, Myricom's Myrinet [4] or DEC's Memory Channel [21].
Reference: [43] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year> <note> Available from http://www.cs.cornell.edu/Info/Projects/ ATM/sosp.ps. </note>
Reference-contexts: All data transmission is performed at user-level. This approach is exemplified by Hamlyn [8], Cranium [34], U-Net <ref> [43] </ref>, and SHRIMP [3]. <p> FM's efficient portability across a range of parallel systems|the Cray T3D (passive interface and remote memory access) and Myrinet-based clusters (communication coprocessor and private memory)|demonstrate FM's portability. A number of recent efforts have explored novel low-level communication interfaces. U-Net <ref> [43] </ref> provides buffer management, but no flow control, implying that data can be lost due to rate mismatch. The low-level interface supporting the Hamlyn [8] architecture provides flow control, but no buffer management, implying that higher-level messaging layers must provide their own buffer-management routines to prevent data from being overwritten.
Reference: [44] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <year> 1992. </year> <note> Available from http://www.cs.cornell.edu/ Info/Projects/CAM/isca92.ps. </note>
Reference-contexts: This allows communication to be truly one-sided. That is, a computation can control when it processes communications and the network can make progress in the face of this deferral. While FM is certainly not the only approach to delivering high-performance communication (See Sidebar B or <ref> [44] </ref>), FM's guarantees define a distinct design point of demonstrated utility. FM implementations are currently available for the Cray T3D MPP and Myrinet-based workstation clusters. Both implementations achieve high performance. <p> Messaging is a well-established communication mechanism of wide utility for parallel coordination [19]. The FM interface design is similar to Berkeley Active Messages <ref> [44] </ref> on the CM-5|a model of simplicity and functionality| but with a few critical distinctions detailed in this section. Primarily, FM borrows the notion of message handlers and uses essentially the same API. However, FM expands upon Active Messages by imposing stronger guarantees on the underlying communication. <p> Less aggressive parallel architectures use commodity microprocessors and add external communication logic (e.g. the TMC CM-5 [42] and Cray T3D [14]). These machines are quite similar to workstation clusters based on, for example, Myricom's Myrinet [4] or DEC's Memory Channel [21]. Work on Active Messages <ref> [44] </ref> bridged the gap between the aggressive integration of communication into the processor and these hybrid systems, showing how much of the communication performance benefit could be reaped on commodity microprocessor based systems. <p> For clarity, the figure shows only one. 22 23 B Comparing Fast Messages and Active Messages Fast Messages builds on the work of others in the area. The original FM 1.1 interface is closely modeled on that of Active Messages (AM) <ref> [44] </ref>, which in turn has as its intellectual antecedents hardware architectures that closely integrated communication and computation in message-driven [16], dataflow [26], and even systolic [5] architectures. We briefly describe the key similarities and differences between the FM 1.1 system and Active Messages.
Reference: [45] <author> Colin Whitby-Strevens. </author> <title> The transputer. </title> <booktitle> In Proceedings of 12th International Symposium on Computer Architecture, </booktitle> <year> 1985. </year>
Reference-contexts: We briefly survey efforts in hardware architectures and more recently software interfaces which bring high performance communication to applications. Hardware architects have long pursued high performance for short messages by integrating the network interface with the CPU itself <ref> [5, 16, 25, 45] </ref>. Innovations included interfacing the network to the instruction set and register file, message-field based dispatch, communication driven scheduling, etc. However, none of these innovations persisted into mainstream high performance processors because of their requirement of changes deep in the processor.
Reference: [46] <institution> X3T11 Technical Committee, Project 1119D. Fibre Channel Physical and Signalling Interface-3 (FC-PH-3), Revision 8.3. American National Standards Institute, </institution> <month> January </month> <year> 1996. </year> <title> Working draft proposed American National Standard for Information Systems. </title> <note> Available from http://www.network.com/~ftp/FC/PH/FCPH3 83.ps. 30 </note>
Reference-contexts: However, higher resolutions, higher frame rates, and less efficient, but faster, encodings are often of interest. Fortunately, a number of new, higher-bandwidth networks have recently hit the market. These include FDDI [18], 100 Base-T Ethernet [22], FibreChannel <ref> [46] </ref>, Myrinet [4], and ATM/SONET [6] and currently run from 100 Mbps to over 600 Mbps, with the ability to scale to Gbps bandwidths and beyond. Not only are these new networks faster, but they are often more reliable. That is, dropped or corrupted data packets occur less frequently.
References-found: 46

