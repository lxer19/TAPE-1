URL: ftp://ftp.cs.virginia.edu/pub/techreports/IPC-92-02.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Access Ordering Algorithms for a Single Module Memory  
Author: Steven A. Moyer 
Date: Revised: December 18, 1992  
Pubnum: IPC-TR-92-002  
Abstract-found: 0
Intro-found: 1
Reference: [BeDa91] <author> Benitez-M, Davidson-J, </author> <title> Code Generation for Streaming: an Access/Execute Mechanism, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV, </address> <year> 1991, </year> <pages> pp. 132-141. </pages>
Reference-contexts: Since vectorizable loops contain no loop-carried dependencies, excepting ignorable input dependence and self-antidependence cycles [Wolf89], reordering accesses within an unrolled loop is simplified. Note that recurrence relations can often be eliminated through streaming optimizations <ref> [BeDa91] </ref>, so that algorithms developed here are actually applicable to a superset of the vectorizable loops. i" y i ax i y i + 6 1.4 Memory Device Types For stream-oriented computations, access ordering reorders references within an unrolled loop to exploit features of the underlying memory system. <p> Benitez and Davidson <ref> [BeDa91] </ref> describe a technique for detecting streaming opportunities, including those in recurrence relations. Callahan et al [CaCK90] present a technique called scalar replacement that detects redundant accesses to subscripted variables in a loop, often transforming a more 8 complex sequence of references to a vector into a single access stream. <p> Loop-carried input dependence can result from the transformation of a more complex sequence of read accesses to a single read stream. Consider the finite difference approximation to the first derivative Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a regis t i t j k k k k q k q
Reference: [BeRo91] <author> Bernstein-D, Rodeh-M, </author> <title> Global Instruction Scheduling for Superscalar Machines, </title> <booktitle> Proc. SIGPLAN91 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1991, </year> <pages> pp. 241-255. </pages>
Reference-contexts: Essentially, access scheduling techniques attempt to separate the execution of a load/store instruction from the execution of the instruction which consumes/produces its operand, reducing the time the processor spends delayed on memory requests. Bernstein and Rodeh <ref> [BeRo91] </ref> present an algorithm for scheduling intra-loop instructions on superscalar architectures that accommodates load delay.
Reference: [CaCK90] <author> Callahan-D, Carr-S, Kennedy-K, </author> <title> Improving Register Allocation for Subscripted Variables, </title> <booktitle> Proc. SIGPLAN 90 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1990, </year> <pages> pp. 53-65. </pages>
Reference-contexts: Benitez and Davidson [BeDa91] describe a technique for detecting streaming opportunities, including those in recurrence relations. Callahan et al <ref> [CaCK90] </ref> present a technique called scalar replacement that detects redundant accesses to subscripted variables in a loop, often transforming a more 8 complex sequence of references to a vector into a single access stream. <p> Loop-carried input dependence can result from the transformation of a more complex sequence of read accesses to a single read stream. Consider the finite difference approximation to the first derivative Analysis techniques <ref> [BeDa91, CaCK90] </ref> can transform the natural pattern of access to vector to a simple stream requiring one access per iteration; two values of are preloaded prior to entering the loop, and each successive value accessed is carried in a regis t i t j k k k k q k q
Reference: [CaKe89] <author> Carr-S, Kennedy-K, </author> <title> Blocking Linear Algebra Codes for Memory Hierarchies, </title> <booktitle> Proc. of the Fourth SIAM Conference on Parallel Processing for Scientific Computing, </booktitle> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. <p> However many codes generate multiple references to a subset of vector operands and hence can benefit from caching, particularly when implemented using strip-mining and tiling techniques <ref> [CaKe89, Wolf89] </ref>. Thus access ordering and caching should be used together to complement one another, exploiting the full memory hierarchy to maximize memory bandwidth.
Reference: [CaKP91] <author> Callahan-D, Kennedy-K, Porterfield-A, </author> <title> Software Prefetching, </title> <booktitle> Proc. </booktitle> <address> ASPLOS-IV, </address> <year> 1991, </year> <pages> pp. 40-52. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy [KlLe91] and Callahan et al <ref> [CaKP91] </ref> propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. Access ordering and access scheduling are fundamentally different.
Reference: [Inte89] <author> Intel Corporation, </author> <title> i860 64-Bit Microprocessor Hardware Reference Manual, </title> <address> ISBN 1-55512-106-3, </address> <year> 1989. </year>
Reference-contexts: This general system model is representative of uniprocessor systems and single-processor nodes of distributed memory parallel machines. The processor is presumed to implement a non-caching load instruction, ala Intels i860 <ref> [Inte89] </ref>, allowing the sequence of requests observed by the memory system to be controlled via software. For access ordering, all memory references are assumed to be non-caching.
Reference: [KlLe91] <author> Klaiber-A, Levy-H, </author> <title> An Architecture for Software-Controlled Data Prefetching, </title> <booktitle> Proc. 18th Annual Intl. Symp. Comput. Architecture, </booktitle> <year> 1991, </year> <pages> pp. 43-53. </pages>
Reference-contexts: Weiss and Smith [WeSm90] present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling. Klaiber and Levy <ref> [KlLe91] </ref> and Callahan et al [CaKP91] propose the use of fetch instructions to preload data into cache; compiler techniques are developed for inserting fetch instructions into the normal instruction stream. Access ordering and access scheduling are fundamentally different.
Reference: [Lam88] <author> Lam-M, </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proc. SIGPLAN88 Conf. Prog. Lang. Design and Implementation, </booktitle> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Bernstein and Rodeh [BeRo91] present an algorithm for scheduling intra-loop instructions on superscalar architectures that accommodates load delay. Lam <ref> [Lam88] </ref> presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration.
Reference: [LaRW91] <author> Lam-M, Rothberg-E, Wolf-M, </author> <title> The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Fourth International Conf. on Arch. Support for Prog. Langs. and Operating Systems, </booktitle> <year> 1991, </year> <pages> pp. 63-74. </pages>
Reference-contexts: For scientific computations, vectors are normally too large to cache. Iteration space tiling [CaKe89, Wolf89] can partition problems into cache-size blocks, however tiling often creates cache conicts <ref> [LaRW91] </ref> and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. Finally, caching may actually reduce the effective memory bandwidth achieved by a computation by fetching extraneous data 1. <p> Finally, caching may actually reduce the effective memory bandwidth achieved by a computation by fetching extraneous data 1. Both superscalar and VLIW architectures incorporate concurrent functional units and thus place similar demands on the memory system. 2 for non-unit strides. Thus, as noted by Lam et al <ref> [LaRW91] </ref>, while data caches have been demonstrated to be effective for general-purpose applications..., their effectiveness for numerical code has not been established. Access ordering [Moye92] is a compiler technology that addresses the memory bandwidth problem for scalar processors executing scientific codes. <p> Note that to insure pseudo vector register elements do not conict in cache, vector storage must not exceed cache capacity for a direct-mapped cache or cache capacity for an n-way set-associative cache <ref> [LaRW91] </ref>. Within a loop, vector operands are loaded into the pseudo vector registers, arithmetic operations are performed on vector register data, and vector register results are stored back to the appropriate vector elements in memory. <p> To illustrate, consider implementing the matrix-vector multiply operation where A and B are matrices and and are vectors. size is dependent on cache size and structure <ref> [LaRW91] </ref>. Elements of are preloaded into cache memory at the appropriate loop level. Access ordering is then applied to the inner loop as elements of A and B are not reused and hence referenced via non-caching loads. <p> Lam et al <ref> [LaRW91] </ref> analyze a technique that eliminates cache conicts by copying data to be cached into a contiguous address space. Note that in applying this copy optimization, non-unit stride vectors can be referenced via non-caching loads to reduce extraneous data movement and wasted cache space.
Reference: [Lee90] <author> Lee-K, </author> <title> On the Floating-Point Performance of the i860 Microprocessor, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-090-019, </institution> <year> 1990. </year>
Reference-contexts: Extensive tests of systems constructed from one such processor, Intels i860, show that as a result of insufficient bandwidth, the average performance of hand optimized scientific kernels is only 1/5 peak processor rate; for compiler generated code average performance is an order of magnitude below peak performance <ref> [Lee90, Moye91] </ref>. The majority of improvement in hand-coded routines over compiler generated code results from tailoring accesses to memory system performance characteristics.
Reference: [Lee91] <author> Lee-K, </author> <title> Achieving High Performance on the i860 Microprocessor with Naspack Subroutines, </title> <institution> NASA Ames Research Center, NAS Systems Division, RNR-091-029, </institution> <year> 1991. </year>
Reference-contexts: However, loop unrolling creates register pressure and has traditionally been limited by register resources. Lee <ref> [Lee91] </ref> presents a technique that employs cache memory to mimic a set of vector registers, effectively increasing register file size for vector computations.
Reference: [Mcma90] <author> McMahon-F, </author> <title> FORTRAN Kernels: </title> <type> MFLOPS, </type> <institution> Lawrence Livermore National Laboratory, </institution> <note> Version MF443. </note>
Reference-contexts: For all computations, the depth of loop unrolling is 4. The daxpy computation is the double-precision version of the axpy computation discussed earlier. Similarly dvaxpy is the double-precision version of the vaxpy (vector axpy) computation The remaining computations in Table 2 are selections from the Livermore Loops <ref> [Mcma90] </ref>, with all vectors defined as double-precision. Access ordering improves performance over the natural access sequence for the given computations from 102% to 149%. Note that for LL-24 only a single vector is referenced so that no reordering is performed.
Reference: [Moye91] <author> Moyer-S, </author> <title> Performance of the iPSC/860 Node Architecture, </title> <institution> University of Virginia, IPC-TR-91-007, </institution> <year> 1991. </year>
Reference-contexts: Extensive tests of systems constructed from one such processor, Intels i860, show that as a result of insufficient bandwidth, the average performance of hand optimized scientific kernels is only 1/5 peak processor rate; for compiler generated code average performance is an order of magnitude below peak performance <ref> [Lee90, Moye91] </ref>. The majority of improvement in hand-coded routines over compiler generated code results from tailoring accesses to memory system performance characteristics. <p> Thus, results represent maximum achievable bandwidth. The parameters of the single-module memory are defined in Table 1; sizes are in bytes and times are in nanoseconds. These parameters are representative of the node memory system for the Intel IPSC/860, as detailed in <ref> [Moye91] </ref>. T avg T tot t i S = BW 3 t i S T tot 28 Table 2 presents simulation results comparing effective bandwidth achieved by the natural versus ordered access sequence for a range of scientific kernels. For all computations, the depth of loop unrolling is 4.
Reference: [Moye92] <author> Moyer-S, </author> <title> Access Ordering and Effective Memory Bandwidth, </title> <type> Ph.D. </type> <institution> Dissertation in progress, Computer Science Department, University of Virginia. </institution>
Reference-contexts: Thus, as noted by Lam et al [LaRW91], while data caches have been demonstrated to be effective for general-purpose applications..., their effectiveness for numerical code has not been established. Access ordering <ref> [Moye92] </ref> is a compiler technology that addresses the memory bandwidth problem for scalar processors executing scientific codes. Access ordering is a loop optimization that reorders non-caching accesses to better utilize memory system resources. <p> The following sections provide the minimal level of context necessary to characterize the contributions of this work; a more complete survey of all relevant topics can be found in <ref> [Moye92] </ref>. 2.1 Stream Detection Access ordering algorithms derived in this report presuppose the existence of compiler techniques to detect stream-oriented computations. Benitez and Davidson [BeDa91] describe a technique for detecting streaming opportunities, including those in recurrence relations. <p> Relaxation of this restriction for applying ordering algorithms to the set of vectorizable loops is discussed in section 6. Many loops can be transformed to adhere to the stream interaction restriction, if optimal ordering is desired. A number of these transformations are discussed in <ref> [Moye92] </ref>. 3.5 MAP Dependence Relations Access ordering alters the sequence of instructions that access memory. In performing this reordering, dependence relations must be maintained. As discussed below, the stream interaction restriction limits the types of dependencies that can exist between accesses from different streams. <p> In particular, single module results presented here are used as the basis for deriving access ordering algorithms and performance predictors for parallel memory systems <ref> [Moye92] </ref>. 38 Appendix A Intermix Sequences A.1 Derivation of The function is the average page miss count in performing each set of c write accesses in the intermix sequence , where and specify a read-modify-write operation; i.e. .
Reference: [Quin91] <author> Quinnell-R, </author> <title> High-speed DRAMs, </title> <type> EDN, </type> <month> May 23, </month> <year> 1991, </year> <pages> pp. 106-116. </pages>
Reference-contexts: Thus, the effective bandwidth is sensitive to the sequence of requests. Nearly all DRAMs currently manufactured implement a form of page-mode operation <ref> [Quin91] </ref>. the vaxpy, vector axpy, computation For modest size vectors, elements , , and are likely to reside in different pages, so that alternating accesses to each incurs the page miss overhead; memory references likely to page miss are highlighted in Figure 2. 1.
Reference: [WeSm90] <author> Weiss-S, Smith-J, </author> <title> A Study of Scalar Compilation Techniques for Pipelined Supercomputers, </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16, 3, </volume> <year> 1990, </year> <pages> pp. 223-245. </pages>
Reference-contexts: Lam [Lam88] presents a technique referred to as software pipelining that structures code such that a given loop iteration loads the data for a later iteration, stores results from a previous iteration, and performs computation for the current iteration. Weiss and Smith <ref> [WeSm90] </ref> present a comprehensive study in which they classify and evaluate software pipelining techniques implemented in conjunction with loop unrolling.
Reference: [Wolf89] <author> Wolfe-M, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: In general purpose scalar computing, the addition of cache memory is often a sufficient solution to the memory latency and bandwidth problems given the spatial and temporal locality of reference exhibited by most codes. For scientific computations, vectors are normally too large to cache. Iteration space tiling <ref> [CaKe89, Wolf89] </ref> can partition problems into cache-size blocks, however tiling often creates cache conicts [LaRW91] and the technique is difficult to automate. Furthermore, only a subset of the vectors accessed will generally be reused and hence benefit from caching. <p> In this report, the computation domain for which access ordering algorithms are developed is further restricted to the class of vectorizable loops. Since vectorizable loops contain no loop-carried dependencies, excepting ignorable input dependence and self-antidependence cycles <ref> [Wolf89] </ref>, reordering accesses within an unrolled loop is simplified. <p> Finally, as stream-oriented computations reference vector operands, well known vectorization techniques are applicable, such as those described by Wolfe <ref> [Wolf89] </ref>. 2.2 Access Scheduling Techniques Access ordering is a compilation technique for maximizing effective memory bandwidth. Previous work has focused on reducing load/store interlock delay by overlapping computation with memory latency, referred to here as access scheduling. <p> A dependence relation between two accesses from the same instance of a loop iteration is said to be loop-independent, while a dependence between accesses from different instances is said to be loop-carried. A detailed treatment of dependence analysis can be found in <ref> [Wolf89] </ref>. t i . v t j . v t j k t i t j 14 3.5.1 Output and Input Dependence Output and input dependence can not exist as a result of the stream interaction restriction; two streams of the same mode have a non-intersecting address space. <p> However many codes generate multiple references to a subset of vector operands and hence can benefit from caching, particularly when implemented using strip-mining and tiling techniques <ref> [CaKe89, Wolf89] </ref>. Thus access ordering and caching should be used together to complement one another, exploiting the full memory hierarchy to maximize memory bandwidth. <p> 6.4.3 Access Ordering and Vectorizable Computations A vectorizable loop is one with no multi-statement dependence cycles and only self-dependence cycles that are ignorable or represent known reduction or recurrence operations for which vector instructions exist; in testing if a loop is vectorizable, input dependence is ignored for non-volatile memory locations <ref> [Wolf89] </ref>.
References-found: 17

