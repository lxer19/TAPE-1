URL: ftp://zeus.cs.gsu.edu/pub/volshevsky/papers/pppc3.ps
Refering-URL: http://www.cs.gsu.edu/~matvro/papers.html
Root-URL: http://www.cs.gsu.edu
Title: Pivoting and Backward Stability of Fast Algorithms for Solving Cauchy Linear Equations Vandermonde and Chebyshev-Vandermonde
Author: T. Boros T. Kailath and V. Olshevsky 
Keyword: Key words: Displacement structure, Cauchy matrix, Vandermonde matrix, fast algorithms, pivoting, rounding error analysis, backward stability, total positivity.  
Web: 65F05, 65L20, 15A09, 15A23  
Note: It is shown that  AMS subject classification:  
Abstract: Three fast O(n 2 ) algorithms for solving Cauchy linear systems of equations are proposed. A rounding error analysis indicates that the backward stability of these new Cauchy solvers is similar to that of Gaussian elimination, thus suggesting to employ various pivoting techniques to achieve a favorable backward stability. It is shown that Cauchy structure allows one to achieve in O(n 2 ) operations partial pivoting ordering of the rows and several other orderings in advance, without actually performing the elimination. The analysis also shows that for the important class of totally positive Cauchy matrices it is advantageous to avoid pivoting, which yields a remarkable backward stability of the suggested algorithms. The analytical results are illustrated by computed examples. 
Abstract-found: 1
Intro-found: 1
Reference: [BKO94] <author> T.Boros, T.Kailath and V.Olshevsky, </author> <title> Fast algorithms for solving Cauchy linear systems, </title> <institution> Stan-ford Information Systems Laboratory report, </institution> <year> 1994. </year>
Reference-contexts: for j=k+1:n end for k=1:n-1 end for k = n-1:-1:1 a (j) = a (j) /(y (k)-y (j)); tmp = 0; tmp = tmp + a (j); a (k) = a (k) -tmp; 2 Algorithm 3.3 has lower complexity and better error bounds than its earlier variant called Cauchy-2 in <ref> [BKO94] </ref>. 8 a (j) = a (j) * ( x (k) - y (j) ); end return The reader should note that the multiplication of the central factor of U 1 k by a vector is performed by accumulation of the inner product from the last to the first entry; this
Reference: [BKO95] <author> T.Boros, T.Kailath and V.Olshevsky, </author> <title> A Fast Bjorck-Pereyra-type algorithm for Cauchy linear systems, </title> <note> submitted for publication, </note> <year> 1995. </year>
Reference-contexts: They are encountered in many applied problems related to polynomial and rational function computations. Vandermonde and Cauchy matrices have many similar properties, among them one could mention the existence of explicit formulas for their determinants and inverses, see, e.g., <ref> [BKO95] </ref> and references therein. <p> Here u denotes the unit roundoff in the standard model of floating point arithmetic, and the operations of comparison, and of taking the absolute value of a matrix, are understood in a componentwise sense. It was further shown in <ref> [BKO95] </ref> that under the condition (1.4) the BP algorithm is not only forward, but also backward stable, jV j 12n 2 uV (x 1:n ) + O (u 2 ): (1.6) Here the computed solution ^a is the exact solution of a nearby system (V (x 1:n ) + V )^a <p> Such a high accuracy motivated several authors to extend the BP algorithm to several other classes of matrices, see, e.g., [TG81], [Hig88], [RO91]. All these generalizations were developed for Vandermonde related structures. In a recent paper <ref> [BKO95] </ref> a similar decomposition C 1 (x 1:n ; y 1:n ) = U 1 n1 L 1 1 ; (1.7) was written down for Cauchy matrices, thus leading to a Bjorck-Pereyra-type algorithm which will be referred to as the BKO algorithm. <p> This algorithm requires O (n 2 ) operations and O (n) locations of memory. It was further shown in <ref> [BKO95] </ref> that the following configuration of the nodes, y n &lt; ::: &lt; y 1 &lt; x 1 &lt; ::: &lt; x n ; (1.8) is an appropriate analog of (1.4) for Cauchy matrices, allowing to prove that the error bounds associated with the BKO algorithm are entirely similar to (1.5) <p> The bound (1.9) indicates that also in such "difficult" cases the BKO algorithm can produce, for special right hand sides, all possible relative precision, see, e.g., <ref> [BKO95] </ref> for a discussion and numerical illustrations. Limitations for non-totally-positive matrices. Of course, reordering of the nodes fx k g and fy k g is equivalent to row and column permutation of C (x 1:n ; y 1:n ), respectively. <p> Therefore it improves the stability properties of the Gaussian elimination procedure, for which the backward error bound involves the product j ^ Lj j ^ U j (computed factors). In contrast, an examination of the error analysis of the BKO algorithm in <ref> [BKO95] </ref> indicates that the corresponding backward bound involves the quantity jL 1 j : : : jL n1 j jU n1 j : : : jU 1 j; (1.12) which because of a non-cancellation property can be much higher than the more attractive quantity jLjjU j. <p> Gaussian elimination with partial pivoting. (Slow O (n 3 ) algorithm, requiring O (n 2 ) storage.) We refer to <ref> [BKO95] </ref> for the discussion and computed examples related to the important case of totally positive Cauchy matrices, and restrict ourselves here to the generic case in which the two sets fx k g and fy k g cannot be separated from each other, so that they cannot be reordered to achieve <p> The PPP technique also improves the numerical performance of the BKO algorithm, however for this algorithm the results are not as favorable as for other algorithms (see, e.g., introduction for the explanation of this phenomena, and <ref> [BKO95] </ref> for the discussion on extremely high accuracy of this algorithm for totally positive Cauchy matrices). <p> ) T = diagfu 0 (y 1 ) v (y 1 ) ; : : : ; u 0 (y n ) v (y n )g: Now, substituting the B fP;Q;u;vg V Q (x 1:n ) T obtained from the last equation back into (7.10) yields (7.2). 8 Conclusion In <ref> [BKO95] </ref> we developed a fast O (n 2 ) Cauchy solver BKO, and proved that for the important class of totally positive coefficient matrices it yields pleasantly small forward, backward and residual errors.
Reference: [BP70] <author> A.Bjorck and V.Pereyra, </author> <title> Solution of Vandermonde Systems of Equations, </title> <journal> Math. Comp., </journal> <volume> 24 (1970), </volume> <pages> 893-903. </pages>
Reference-contexts: Government. y Information Systems Laboratory, Department of Electrical Engineering, Stanford University, Stanford CA 94305-4055, U.S.A. (tibor@isl.stanford.edu, tk@isl.stanford.edu and olshevsk@isl.stanford.edu) 1 general (structure-ignoring) algorithms, say Gaussian elimination with pivoting. At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example <ref> [BP70] </ref>, [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> The Bjorck-Pereyra algorithm for Vandermonde systems. In particular, most of the above mentioned papers were devoted to the analysis of numerical properties and extensions of the now well-known Bjorck-Pereyra algorithm for solving Vandermonde linear systems <ref> [BP70] </ref>, [GVL89].
Reference: [C41] <author> A.L. </author> <title> Cauchy, Memoires sur les fonctions alternees et sur les sommes alternees, </title> <journal> Exercices d'analyse et de phys. math., </journal> <volume> ii (1841), </volume> <pages> 151-159. </pages>
Reference-contexts: Clearly, the determinant of the kfik leading submatrix of R is equal to d 1 :::d k , so the objective of partial pivoting is the successive maximization of the determinants of leading submatrices. This observation, and the well-known formula <ref> [C41] </ref> for the determinant of a Cauchy matrix, imply that partial pivoting on C (x 1:n ; y 1:n ) is equivalent to the successive maximization of the quantities jd i j = j j=1 (x i x j ) j=1 (y i y j ) Q i1 Q i1 j;
Reference: [CF88] <author> T.Chan and D.Foulser, </author> <title> Effectively well-conditioned linear systems, </title> <journal> SIAM J. Sci. Stat. Computation, </journal> <volume> 9 (1988), 963 - 969. </volume>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], <ref> [CF88] </ref>, [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems.
Reference: [CR92] <author> D.Calvetti and L.Reichel, </author> <title> A Chebyshev-Vandermonde solver, </title> <journal> Lin. Alg. Appl., </journal> <volume> 172(1992), </volume> <pages> 219-229. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], <ref> [CR92] </ref>, [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems.
Reference: [CR93] <author> Calvetti, D. and Reichel, L. </author> : <title> Fast inversion of Vandermonde-like matrices involving orthogonal polynomials, </title> <journal> BIT, </journal> <year> 1993. </year> <month> 21 </month>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], <ref> [CR93] </ref>, [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> If P stands for the basis of Chebyshev polynomials (of the first or of the second kind), then V P (x 1:n ) is called a Chebyshev-Vandermonde matrix. Fast O (n 2 ) algorithms for solving Chebyshev-Vandermonde systems were suggested in [Hig88], [HHR89], <ref> [CR93] </ref>, [GO94d]. Here we suggest an alternative, based on the next result.
Reference: [DBP77] <author> C. de Boor and A. Pinkus, </author> <title> Backward error analysis for totally positive linear systems, </title> <journal> Numer. Math., </journal> <volume> 27 (1977), </volume> <pages> 485-490. </pages>
Reference-contexts: Total positivity. There are classes of matrices for which it is advantageous not to pivot. For example, for totally positive matrices, the exact triangular factors have only positive entries. C.De Boor and A.Pinkus pointed out in <ref> [DBP77] </ref> that if the entries of the computed factors, ^ L and ^ U remain nonnegative, than the backward error of Gaussian elimination without pivoting is pleasantly small, jRj 3fl n R; (1.16) see also p.176 in [Hig96].
Reference: [GK50] <author> F.R.Gantmacher and M.G.Krein, </author> <title> Oscillatory matrices and kernels, and small vibrations of mechanical systems, second edition, </title> <booktitle> (in Russian), </booktitle> <address> GITTL, Moscow, </address> <year> 1950. </year> <title> German translation : Oszillationsmatrizen, </title> <editor> Oszillationskerne und kleine Schwingungen mechanischer Systeme, </editor> <publisher> Berlin, Akademie Verlag, </publisher> <year> 1960. </year>
Reference-contexts: (1.3) of the BKO algorithm, the sparsity pattern of the factors (1.13) immediately implies the equality jLjjU j = jL 1 j ::: jL n1 jjU n1 j ::: jU 1 j; 1 Totally positive matrices are those for which the determinant of every submatrix is positive, see the monographs <ref> [GK50] </ref> and [K72]. resulting in pleasing backward bounds of the form jCj d n ujLjjU j + O (u 2 ); (1.14) for the new algorithms. Here the computed solution ^a is the exact solution of a nearby system (C (x 1:n ; y 1:n )+ C)^a = f . <p> positive matrices If we assume that y n &lt; ::: &lt; y 1 &lt; x 1 &lt; ::: &lt; x n ; (5.1) i.e. the matrix C (x 1:n ; y 1:n ) is totally positive, so that all the entries of the exact factors L and U are positive <ref> [GK50] </ref>. In this case theorems 4.1 and 4.2 imply that the algorithms 2.5 and 3.3 produce a favorable small backward error.
Reference: [GK90] <author> I.Gohberg and I.Koltracht, </author> <title> On the inversion of Cauchy matrices, In Signal processing, Scattering and Operator Theory, and Numerical methods. </title> <booktitle> Proc of the MTNS-89 (M.A.Kaashoek, </booktitle> <editor> J.H.van Schuppen and A.C.M.Ran, eds), </editor> <address> 381-392, </address> <publisher> Birkhauser, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see <ref> [GK90] </ref>, [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. In particular, most of the above mentioned papers were devoted to the analysis of numerical properties and extensions of the now well-known Bjorck-Pereyra algorithm for solving Vandermonde linear systems [BP70], [GVL89].
Reference: [GK93] <author> I.Gohberg and I.Koltracht, </author> <title> Mixed, componentwise and structured condition numbers, </title> <journal> SIAM J. Matrix Anal., </journal> <volume> 14(1993), </volume> <pages> 688-704. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], <ref> [GK93] </ref>). The Bjorck-Pereyra algorithm for Vandermonde systems. In particular, most of the above mentioned papers were devoted to the analysis of numerical properties and extensions of the now well-known Bjorck-Pereyra algorithm for solving Vandermonde linear systems [BP70], [GVL89]. <p> The use of explicit inversion formula also yields high accuracy, predicted by the analysis of <ref> [GK93] </ref>, and apparently, this is the only algorithm whose accuracy does not depend upon the ordering of the nodes. 15 At the same time, comparison of the data in Tables 1 and 2 as well as in other examples indicates that the use of the PPP technique prevents the INV algorithm
Reference: [GKO95] <author> I.Gohberg, T.Kailath and V.Olshevsky, </author> <title> Fast Gaussian elimination with partial pivoting for matrices with displacement structure, </title> <journal> Math. of Comp., </journal> <volume> 64 (1995), </volume> <pages> 1557-1576. </pages>
Reference-contexts: The next lemma from <ref> [GKO95] </ref>, [KO95b] provides one particular form for the generator recur sion. Lemma 2.2 Let R 1 = l 1 R 22 satisfy the displacement equation (2.2) with triangular F 1 ; A 1 partitioned as in (2.5). <p> Direct generator recursion. In fact the algorithm 2.3 is valid for the more general class of Cauchy-like matrices, see, e.g., <ref> [GKO95] </ref> [KO95b] for details and applications. However for the special case of ordinary Cauchy matrices we can exploit the fact that the corresponding displacement rank is equal to one, to formulate a more specific version of the generalized Schur algorithm, based on the next Lemma. <p> The algorithm 2.3 is a special case of the more general GKO algorithm <ref> [GKO95] </ref>, which is applicable to the wider class of Cauchy-like matrices. A normwise rounding error analysis for the GKO algorithm appeared in [SB95]. <p> Along with the usual factor kLkkU k ( cf. with (1.15)) the backward error bound of [SB95] involves also a so-called generator growth factor of the form k diag ( jg k j jb k j (k) (k) ) k (4.1) In the context of <ref> [GKO95] </ref>, [SB95] the quantities g (k) (k) k were vectors of size equal to the displacement rank of R; so if the quantity in (4.1) is large, then the backward stability of of the GKO algorithm could be less favorable than that of Gaussian elimination. <p> Here we should note that the partial pivoting technique can be directly incorporated into the generalized Schur algorithms 2.3 and 2.5, see, e.g., <ref> [GKO95] </ref>. However, the corresponding ordering of fx k g can also be computed in advance in O (n 2 ) flops. <p> 2:0 10 1 1.3 5 3:8 10 4 2:6 10 1 1.3 7 2:5 10 5 2:3 10 1 1.4 9 1:0 10 6 6:8 10 3 1:2 10 4 We note, however, that the motivation for introducing the Gu's pivoting technique was given in [Gu95], where an application of <ref> [GKO95] </ref> with displacement rank 2 or higher was discussed. 17 6.3 Example 3. A transposed system However, an immediate question is what will happen for a transposed to the matrix in Example 2 (clearly the transposed Cauchy matrix is a Cauchy matrix itself).
Reference: [GO94a] <author> I.Gohberg and V.Olshevsky, </author> <title> Fast state space algorithms for matrix Nehari and Nehari-Takagi interpolation problems, Integral Equations and Operator Theory, </title> <journal> 20, </journal> <volume> No. 1 (1994), 44 - 83. </volume>
Reference: [GO94b] <author> I.Gohberg and V.Olshevsky, </author> <title> Fast algorithms with preprocessing for matrix-vector multiplication problems, </title> <journal> Journal of Complexity, </journal> <volume> 10 (1994), </volume> <pages> 411-427. </pages>
Reference: [GO94c] <author> I.Gohberg and V.Olshevsky, </author> <title> Complexity of multiplication with vectors for structured matrices, </title> <journal> Linear Algebra Appl., </journal> <volume> 202, </volume> <year> 1994, </year> <pages> 163 - 192. </pages>
Reference: [GO94d] <author> I.Gohberg and V.Olshevsky, </author> <title> Fast inversion of Chebyshev-Vandermonde matrices, </title> <journal> Numer. Math., </journal> <volume> 67, No. 1 (1994), 71 - 92. </volume>
Reference-contexts: If P stands for the basis of Chebyshev polynomials (of the first or of the second kind), then V P (x 1:n ) is called a Chebyshev-Vandermonde matrix. Fast O (n 2 ) algorithms for solving Chebyshev-Vandermonde systems were suggested in [Hig88], [HHR89], [CR93], <ref> [GO94d] </ref>. Here we suggest an alternative, based on the next result.
Reference: [GVL89] <author> G. Golub and C, Van Loan, </author> <title> Matrix Computations, </title> <note> second edition, </note> <author> John Hopkins U. P., Baltimore, </author> <year> 1989. </year>
Reference-contexts: The Bjorck-Pereyra algorithm for Vandermonde systems. In particular, most of the above mentioned papers were devoted to the analysis of numerical properties and extensions of the now well-known Bjorck-Pereyra algorithm for solving Vandermonde linear systems [BP70], <ref> [GVL89] </ref>.
Reference: [Gu95] <author> Ming Gu, </author> <title> Stable and efficient algorithms for structured linear systems, </title> <type> preprint, </type> <year> 1995. </year>
Reference-contexts: There is a variety of possible pivoting techniques that can be incorporated into the new algorithms without increasing their O (n 2 ) complexity, including partial row and partial column pivoting, Gu's pivoting (a variation of complete pivoting for Cauchy-like matrices <ref> [Gu95] </ref>), and others. Total positivity. There are classes of matrices for which it is advantageous not to pivot. For example, for totally positive matrices, the exact triangular factors have only positive entries. <p> Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper <ref> [Gu95] </ref> a variation of complete pivoting was suggested for the more general Cauchy like matrices. In the context of [Gu95] the corresponding displacement rank is 2 or higher. <p> Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper <ref> [Gu95] </ref> a variation of complete pivoting was suggested for the more general Cauchy like matrices. In the context of [Gu95] the corresponding displacement rank is 2 or higher. For the ordinary Cauchy matrices C (x 1:n ; y 1:n ) (displacement rank = 1), Gu's pivoting can be described as follows. <p> 1.0 3 7:2 10 2 2:0 10 1 1.3 5 3:8 10 4 2:6 10 1 1.3 7 2:5 10 5 2:3 10 1 1.4 9 1:0 10 6 6:8 10 3 1:2 10 4 We note, however, that the motivation for introducing the Gu's pivoting technique was given in <ref> [Gu95] </ref>, where an application of [GKO95] with displacement rank 2 or higher was discussed. 17 6.3 Example 3. A transposed system However, an immediate question is what will happen for a transposed to the matrix in Example 2 (clearly the transposed Cauchy matrix is a Cauchy matrix itself). <p> It is further shown that the row ordering of partial pivoting and of the Gu's pivoting <ref> [Gu95] </ref> can be achieved in advance, without actually performing elimination, and fast O (n 2 ) algorithms for these purposes are suggested.
Reference: [Hig87] <author> N.J.Higham, </author> <title> Error analysis of the Bjorck-Pereyra algorithms for solving Vandermonde systems, </title> <journal> Numer. Math., </journal> <volume> 50 (1987), 613 - 632. </volume>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], <ref> [Hig87] </ref>, [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> Moreover, the algorithm requires only O (n) locations of memory. Remarkable accuracy for monotonically ordered nodes. It turns out that along with dramatic speed-up and savings in storage, the BP algorithm often produces a surprisingly high relative accuracy in the computed solution. N.J.Higham analyzed in <ref> [Hig87] </ref> the numerical performance of the BP algorithm and identified a class of Vandermonde matrices, viz., those for which the nodes can be strictly orderded, 0 &lt; x 1 &lt; x 2 &lt; ::: &lt; x n ; (1.4) with a favorable forward error bound, ja ^aj 5nuV (x 1:n )
Reference: [Hig88] <author> N.J.Higham, </author> <title> Fast solution of Vandermonde-like systems, involving orthogonal polynomials, </title> <journal> IMA J. Numer. Anal., </journal> <volume> 8 (1988), </volume> <pages> 473-486. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], <ref> [Hig88] </ref>, [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> Such a high accuracy motivated several authors to extend the BP algorithm to several other classes of matrices, see, e.g., [TG81], <ref> [Hig88] </ref>, [RO91]. All these generalizations were developed for Vandermonde related structures. <p> If P stands for the basis of Chebyshev polynomials (of the first or of the second kind), then V P (x 1:n ) is called a Chebyshev-Vandermonde matrix. Fast O (n 2 ) algorithms for solving Chebyshev-Vandermonde systems were suggested in <ref> [Hig88] </ref>, [HHR89], [CR93], [GO94d]. Here we suggest an alternative, based on the next result.
Reference: [Hig90] <author> N.J.Higham, </author> <title> Stability analysis of algorithms for solving confluent Vandermonde-like systems, </title> <journal> SIAM J. Matrix Anal., </journal> <volume> 11(1) (1990), </volume> <pages> 23-41. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], <ref> [Hig90] </ref>, [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> i; for j = i:n if dist&lt;aux (j) m = j; dist = aux (j); end end x = swap (x,i,m); aux (m) = aux (i); end return A similar row reordering technique for Vandermonde matrices (and a fast O (n 2 ) algorithm for achieving it) was proposed in <ref> [Hig90] </ref>, and in [R90] it was called Leja ordering. Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper [Gu95] a variation of complete pivoting was suggested for the more general Cauchy like matrices. <p> (i); end return A similar row reordering technique for Vandermonde matrices (and a fast O (n 2 ) algorithm for achieving it) was proposed in <ref> [Hig90] </ref>, and in [R90] it was called Leja ordering. Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper [Gu95] a variation of complete pivoting was suggested for the more general Cauchy like matrices. In the context of [Gu95] the corresponding displacement rank is 2 or higher.
Reference: [Hig96] <author> N.J.Higham, </author> <title> Accuracy and Stability of Numerical Algorithms, </title> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: These bounds are similar to the well-known bounds for Gaussian elimination, jRj 2fl n j ^ Ljj ^ U j; where fl n = nu ; (1.15) see, e.g., p. 175 in <ref> [Hig96] </ref>. Here ^ L and ^ U denote the computed triangular factors. Different pivoting techniques. <p> C.De Boor and A.Pinkus pointed out in [DBP77] that if the entries of the computed factors, ^ L and ^ U remain nonnegative, than the backward error of Gaussian elimination without pivoting is pleasantly small, jRj 3fl n R; (1.16) see also p.176 in <ref> [Hig96] </ref>. <p> Error in the computed solution. Standard error analysis, see, e.g., p. 154 in <ref> [Hig96] </ref>, for solving a linear system ^ L ^ D ^ U a = f by forward and backsubstitution yields that the computed solution ^a satisfies ( ^ L + ^ L)( ^ D + ^ D)( ^ U + ^ U )^a = f; where j ^ Lj fl n <p> To prove (4.8) we shall use the following easily verified fact (see, e.g., <ref> [Hig96] </ref>, p.80) : let jX k j ffijX k j for k = 1; 2; :::; m, then j k=1 m Y X k j ((1 + ffi) m 1) k=1 This and (4.14) imply that jLj = j (L 1 + L 1 ) ::: (L n1 + L n1 <p> errors r i = kf k 2 and the backward errors b i = min f kAk 2 kAk 2 : (A + A)^a i = f g ) using the formula b i = kCk 2 k^a i k 2 a result probably first shown by Wilkinson, see, e.g., <ref> [Hig96] </ref>. The tables below display also the condition number 2 (C) of the coefficient matrix, norms for the solution k^a d k 2 and the right-hand side kf k 2 as well as some other useful information. 6.1 Example 1.
Reference: [HHR89] <author> G.Heinig, W.Hoppe and K.Rost, </author> <title> Structured matrices in interpolation and approximation problems, </title> <journal> Wissenschaftl. Zeitschrift der TU Karl-Marx-Stadt 31, </journal> <volume> 2 (1989), 196 - 202. </volume>
Reference-contexts: If P stands for the basis of Chebyshev polynomials (of the first or of the second kind), then V P (x 1:n ) is called a Chebyshev-Vandermonde matrix. Fast O (n 2 ) algorithms for solving Chebyshev-Vandermonde systems were suggested in [Hig88], <ref> [HHR89] </ref>, [CR93], [GO94d]. Here we suggest an alternative, based on the next result.
Reference: [HR84] <editor> G.Heinig and K.Rost Algebraic methods for Toeplitz-like matrices and operators, </editor> <booktitle> Operator Theory, </booktitle> <volume> vol. </volume> <pages> 13, </pages> <address> 1984, </address> <publisher> Birkauser Verlag, Basel. </publisher>
Reference: [K72] <editor> S.Karlin, Total Positivity, </editor> <volume> vol 1, </volume> <publisher> Stanford University Press, Stanford, </publisher> <year> 1972. </year>
Reference-contexts: the BKO algorithm, the sparsity pattern of the factors (1.13) immediately implies the equality jLjjU j = jL 1 j ::: jL n1 jjU n1 j ::: jU 1 j; 1 Totally positive matrices are those for which the determinant of every submatrix is positive, see the monographs [GK50] and <ref> [K72] </ref>. resulting in pleasing backward bounds of the form jCj d n ujLjjU j + O (u 2 ); (1.14) for the new algorithms. Here the computed solution ^a is the exact solution of a nearby system (C (x 1:n ; y 1:n )+ C)^a = f .
Reference: [KKM79] <author> T.Kailath, S.Kung and M.Morf, </author> <title> Displacement ranks of matrices and linear equations, </title> <journal> J. Math. Anal. and Appl., </journal> <volume> 68 (1979), </volume> <pages> 395-407. </pages>
Reference-contexts: An algorithm with O (n) storage is described next. 3 Exploiting quasi-Cauchy structure The concept of displacement structure was initiated by the paper <ref> [KKM79] </ref>, where it was first applied to study Toeplitz matrices, using a displacement operator of the form r fZ;Z T g (R) = R Z R Z T , where Z is the lower shift matrix.
Reference: [KO95a] <author> T.Kailath and V.Olshevsky, </author> <title> Displacement structure approach to Chebyshev-Vandermonde and related matrices, Integral Equations and Operator Theory, </title> <booktitle> 22, 1995, </booktitle> <volume> 65 - 92. </volume> <pages> 22 </pages>
Reference: [KO95b] <author> T.Kailath and V.Olshevsky, </author> <title> Bunch-Kaufman pivoting for partially reconstructable Cauchy--like matrices with application to Toeplitz-like linear equations and to boundary rational matrix interpolation problems, </title> <note> 1995, to appear in Linear Algebra and Appl. </note>
Reference-contexts: The next lemma from [GKO95], <ref> [KO95b] </ref> provides one particular form for the generator recur sion. Lemma 2.2 Let R 1 = l 1 R 22 satisfy the displacement equation (2.2) with triangular F 1 ; A 1 partitioned as in (2.5). <p> Direct generator recursion. In fact the algorithm 2.3 is valid for the more general class of Cauchy-like matrices, see, e.g., [GKO95] <ref> [KO95b] </ref> for details and applications. However for the special case of ordinary Cauchy matrices we can exploit the fact that the corresponding displacement rank is equal to one, to formulate a more specific version of the generalized Schur algorithm, based on the next Lemma.
Reference: [KS95] <author> T.Kailath and A.H.Sayed, </author> <title> Displacement structure : Theory and Applications, </title> <journal> SIAM Review, </journal> <volume> 37 No.3 (1995), </volume> <pages> 297-386. </pages>
Reference-contexts: stability of the same form (1.16) is guaranteed for the new fast algorithms without any additional assumptions on the computed triangular factors. 1.3 Outline of the paper It is now well-known that for structured matrices the Gaussian elimination procedure can be speeded-up, leading to the generalized Schur algorithms, see, e.g., <ref> [KS95] </ref>, and the references therein. In the next section we exploit the displacement structure of Cauchy matrices to specify two such algorithms. <p> The displacement rank measures the complexity of R, because all its n 2 entries are described by a smaller number 2ffn entries of its generator fG; Bg. We refer to the recent survey <ref> [KS95] </ref> for more complete information on displacement and further references, and here we restrict ourselves only with Cauchy matrices. The next lemma specifies their displacement structure. <p> 2 ; A 2 g-displacement rank of the Schur complement is less than or equal to ff, so that we can write F 2 R 2 R 2 A 2 = G 2 B T 2 with some G 2 ; B 2 2 C (n1)fiff ; (2.6) see, e.g., <ref> [KS95] </ref> and the references therein, which also show how to run the generator recursion fG 1 ; B 1 g ! fG 2 ; B 2 g. The next lemma from [GKO95], [KO95b] provides one particular form for the generator recur sion. <p> Proceeding with R 2 similarly, after n 1 steps one obtains the whole LU factorization. 5 Algorithms that exploit the displacement structure of a matrix to speed-up the Gaussian elimination procedure are called generalized Schur algorithms, because the classical Schur algorithm [S17] was shown (see, e.g., [LAK86], <ref> [KS95] </ref>) to belong to the class.
Reference: [LAK86] <author> H.Lev-Ari and T.Kailath, </author> <title> Triangular factorization of structured Hermitian matrices, in Operator Theory : Advances and Applications (I.Gohberg. </title> <editor> ed.), </editor> <volume> vol. 18, 301 - 324, </volume> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1986. </year>
Reference-contexts: Proceeding with R 2 similarly, after n 1 steps one obtains the whole LU factorization. 5 Algorithms that exploit the displacement structure of a matrix to speed-up the Gaussian elimination procedure are called generalized Schur algorithms, because the classical Schur algorithm [S17] was shown (see, e.g., <ref> [LAK86] </ref>, [KS95]) to belong to the class. <p> In this section we make a connection with <ref> [LAK86] </ref>, where the fact that Toeplitz matrices belong to the more general class of matrices with r fZ;Z T g -displacement rank 2, was used to introduce the name quasi-Toeplitz for such matrices.
Reference: [R90] <author> L.Reichel, </author> <title> Newton interpolation at Leja points, </title> <journal> BIT, </journal> <volume> 30(1990), 23 - 41. </volume>
Reference-contexts: = i:n if dist&lt;aux (j) m = j; dist = aux (j); end end x = swap (x,i,m); aux (m) = aux (i); end return A similar row reordering technique for Vandermonde matrices (and a fast O (n 2 ) algorithm for achieving it) was proposed in [Hig90], and in <ref> [R90] </ref> it was called Leja ordering. Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper [Gu95] a variation of complete pivoting was suggested for the more general Cauchy like matrices. <p> end return A similar row reordering technique for Vandermonde matrices (and a fast O (n 2 ) algorithm for achieving it) was proposed in [Hig90], and in <ref> [R90] </ref> it was called Leja ordering. Therefore, PPP may also be called rational Leja ordering, by analogy with (polynomial) Leja ordering of [Hig90], [R90]. In a recent paper [Gu95] a variation of complete pivoting was suggested for the more general Cauchy like matrices. In the context of [Gu95] the corresponding displacement rank is 2 or higher.
Reference: [RO91] <author> L. Reichel and G. Opfer, </author> <title> Chebyshev-Vandermonde Systems, </title> <journal> Math. of Comp., </journal> <pages> 57(1991) , 703-721. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], <ref> [RO91] </ref>, [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> Such a high accuracy motivated several authors to extend the BP algorithm to several other classes of matrices, see, e.g., [TG81], [Hig88], <ref> [RO91] </ref>. All these generalizations were developed for Vandermonde related structures.
Reference: [S17] <author> I.Schur, </author> <title> Uber potenzreihen die im Inneren des Einheitskreises beschrankt sind, </title> <journal> Journal fur die Reine und Angewandte Mathematik, </journal> <volume> 147 (1917), 205 - 232. </volume> <booktitle> English translation : in Operator Theory : Advances and Applications (I.Gohberg. </booktitle> <editor> ed.), </editor> <volume> vol. 18, 31 - 88, </volume> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1986. </year>
Reference-contexts: Proceeding with R 2 similarly, after n 1 steps one obtains the whole LU factorization. 5 Algorithms that exploit the displacement structure of a matrix to speed-up the Gaussian elimination procedure are called generalized Schur algorithms, because the classical Schur algorithm <ref> [S17] </ref> was shown (see, e.g., [LAK86], [KS95]) to belong to the class.
Reference: [SB80] <author> J.Stoer and R.Bulirsch, </author> <title> Introduction to Numerical Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1980. </year>
Reference: [SB95] <author> D.Sweet and R.Brent, </author> <title> Error analysis of a partial pivoting method for structured matrices, </title> <booktitle> Advanced Signal processing algorithms, Proc of SPIE-1995, </booktitle> <volume> vol. 2563, </volume> <pages> 266-280. </pages>
Reference-contexts: The algorithm 2.3 is a special case of the more general GKO algorithm [GKO95], which is applicable to the wider class of Cauchy-like matrices. A normwise rounding error analysis for the GKO algorithm appeared in <ref> [SB95] </ref>. Along with the usual factor kLkkU k ( cf. with (1.15)) the backward error bound of [SB95] involves also a so-called generator growth factor of the form k diag ( jg k j jb k j (k) (k) ) k (4.1) In the context of [GKO95], [SB95] the quantities g <p> A normwise rounding error analysis for the GKO algorithm appeared in <ref> [SB95] </ref>. Along with the usual factor kLkkU k ( cf. with (1.15)) the backward error bound of [SB95] involves also a so-called generator growth factor of the form k diag ( jg k j jb k j (k) (k) ) k (4.1) In the context of [GKO95], [SB95] the quantities g (k) (k) k were vectors of size equal to the displacement rank of R; so if the <p> algorithm appeared in <ref> [SB95] </ref>. Along with the usual factor kLkkU k ( cf. with (1.15)) the backward error bound of [SB95] involves also a so-called generator growth factor of the form k diag ( jg k j jb k j (k) (k) ) k (4.1) In the context of [GKO95], [SB95] the quantities g (k) (k) k were vectors of size equal to the displacement rank of R; so if the quantity in (4.1) is large, then the backward stability of of the GKO algorithm could be less favorable than that of Gaussian elimination. <p> More precisely, any row or column reordering that reduces the size of jLjjU j appearing in the bounds (4.2), (4.7) will stabilize the numerical performance of the algorithms 2.5, 3.3. Moreover, the normwise error analysis of <ref> [SB95] </ref> for the algorithm 2.3, reviewed at the beginning of section 4, also indicates that the pivoting will enhance the accuracy of the algorithm 2.3. Here we should note that the partial pivoting technique can be directly incorporated into the generalized Schur algorithms 2.3 and 2.5, see, e.g., [GKO95].
Reference: [TG81] <author> W.Tang and G.Golub, </author> <title> The block decomposition of a Vandermonde matrix and its aplications, </title> <journal> BIT, </journal> <volume> 21 (1981), </volume> <pages> 505-517. </pages>
Reference-contexts: Government. y Information Systems Laboratory, Department of Electrical Engineering, Stanford University, Stanford CA 94305-4055, U.S.A. (tibor@isl.stanford.edu, tk@isl.stanford.edu and olshevsk@isl.stanford.edu) 1 general (structure-ignoring) algorithms, say Gaussian elimination with pivoting. At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], <ref> [TG81] </ref>, [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems. <p> Such a high accuracy motivated several authors to extend the BP algorithm to several other classes of matrices, see, e.g., <ref> [TG81] </ref>, [Hig88], [RO91]. All these generalizations were developed for Vandermonde related structures.
Reference: [Ty94] <author> E.Tyrtyshnikov, </author> <title> How bad are Hankel matrices, </title> <journal> Numer. Math., </journal> <volume> 67, No. 2 (1994), 261 - 269. </volume>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], [V93], <ref> [Ty94] </ref>), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems.
Reference: [V93] <author> J. M. Varah, </author> <title> Errors and Perturbations in Vandermonde Systems, </title> <journal> IMA J. of Numer. Anal., </journal> <volume> 13 (1993), </volume> <pages> 1-12. </pages>
Reference-contexts: At the same time, such favorable numerical properties are much better understood for Vandermonde and related matrices (see, for example [BP70], [TG81], [CF88], [Hig87], [Hig88], [Hig90], [RO91], [CR92], [CR93], <ref> [V93] </ref>, [Ty94]), as compared to the analysis of numerical issues related to Cauchy matrices (see [GK90], [GK93]). The Bjorck-Pereyra algorithm for Vandermonde systems.
Reference: [W68] <author> J.Wilkinson, </author> <title> A priori error analysis of algebraic processes, </title> <journal> Proc. Intern. Congr. Math. </journal> <year> (1966), </year> <pages> pp. 629-639, </pages> <note> Moskow, Mir 1968. Submitted to SIMAX, </note> <month> December </month> <year> 1994. </year> <title> First revision, </title> <month> April </month> <year> 1996. </year> <title> Second revision, </title> <month> December </month> <year> 1996. </year> <month> 23 </month>
Reference-contexts: Here q (n) is a polynomial of small degree in n. Then in accordance with <ref> [W68] </ref> the matrix H will likely lose during the elimination not only its total positivity, but also the weaker property of being positive definite.
References-found: 39

