URL: file://ftp.cs.utexas.edu/pub/mooney/papers/chill-bkchapter-95.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/risto/cs395t-cs/schedule.html
Root-URL: 
Email: (jz6011r@dunix.drake.edu)  (mooney@cs.utexas.edu)  
Phone: 2  
Title: Appears in: Symbolic, Connectionist, and Statistical Approaches to Learning for Natural Language  
Author: John M. Zelle and Raymond J. Mooney 
Affiliation: Department of Computer Sciences, University of Texas,  
Address: IA 50311, USA  Austin TX 78712, USA  
Date: 1996  
Note: Processing, Springer Verlag,  Construction  
Abstract: Comparative Results on Using Inductive Logic Abstract. This paper presents results from recent experiments with Chill, a corpus-based parser acquisition system. Chill treats language acquisition as the learning of search-control rules within a logic program. Unlike many current corpus-based approaches that use statistical learning algorithms, Chill uses techniques from inductive logic programming (ILP) to learn relational representations. Chill is a very flexible system and has been used to learn parsers that produce syntactic parse trees, case-role analyses, and executable database queries. The reported experiments compare Chill's performance to that of a more naive application of ILP to parser acquisition. The results show that ILP techniques, as employed in Chill, are a viable alternative to statistical methods and that the control-rule framework is fundamental to Chill's success.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Berwick. </author> <title> The Acquisition of Syntactic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1985. </year>
Reference-contexts: Treating language acquisition as a control-rule learning problem is not in itself a new idea. Berwick <ref> [1] </ref> used this approach to learn rules for a Marcus-style deterministic parser. When the system came to a parsing impasse, a new rule was created by inferring the correct parsing action and creating a new rule using certain properties of the current parser state as trigger conditions for its application.
Reference: 2. <author> E. Black and et. al. </author> <title> A procedure for quantitatively comparing the syntactic coverage of English grammars. </title> <booktitle> In Proceedings of the Fourth DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 306-311, </pages> <year> 1991. </year>
Reference-contexts: Another accuracy measure, which has been used in evaluating systems that bracket the input sentence into unlabeled constituents, is the proportion of constituents in the parse that do not cross any constituent boundaries in the correct tree <ref> [2] </ref>. We have computed the number of sentences with parses containing no crossing constituents, as well as the proportion of constituents which are non-crossing over all test sentences.
Reference: 3. <author> E. Black, F. Jelineck, J. Lafferty, D. Magerman, R. Mercer, and S. Roukos. </author> <title> Towards history-based grammars: Using richer models for probabilistic parsing. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 31-37, </pages> <address> Columbus, Ohio, </address> <year> 1993. </year>
Reference-contexts: These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees <ref> [3, 12] </ref> and transformations [6], statistical methods dominate. A common thread in all of these approaches is that the acquired knowledge is represented in a propositional form (perhaps with associated probabilities). <p> Using the partial scoring metric, Chill's parses garnered an average accuracy of over 84%. The figures for 0-cross and consistent compare very favorably with those reported in studies of automated bracketing for the ATIS corpus. Brill <ref> (1993) </ref> reports 60% and 91.12%, respectively. Chill scores higher on the percentage of sentences with no crossing violations (64%) and slightly lower (90%) on the total percentage of non-crossing constituents.
Reference: 4. <author> E. Black, J. Lafferty, and S. Roukaos. </author> <title> Development and evaluation of a broad-coverage probabilistic grammar of English-language computer manuals. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 185-192, </pages> <address> Newark, Delaware, </address> <year> 1992. </year>
Reference-contexts: More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars <ref> [4, 23, 7] </ref> or transition networks [17]. These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees [3, 12] and transformations [6], statistical methods dominate.
Reference: 5. <author> Borland International. </author> <title> Turbo Prolog 2.0 Reference Guide. </title> <booktitle> Borland International, </booktitle> <address> Scotts Valley, CA, </address> <year> 1988. </year>
Reference-contexts: Hence, the metric is a true measure of performance in a complete database-query application. line labeled "Geobase" shows the average accuracy of the Geobase system, a natural-language front-end supplied as an example application with a commercial Prolog system (Turbo Prolog 2.0 <ref> [5] </ref>). The curves show that Chill outperforms the existing system when trained on 175 or more examples.
Reference: 6. <author> E. Brill. </author> <title> Automatic grammar induction and parsing free text: A transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 259-265, </pages> <address> Columbus, Ohio, </address> <year> 1993. </year>
Reference-contexts: These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees [3, 12] and transformations <ref> [6] </ref>, statistical methods dominate. A common thread in all of these approaches is that the acquired knowledge is represented in a propositional form (perhaps with associated probabilities). <p> We chose this particular data because it represents realistic input from human-computer interaction, and because it has been used in a number of other studies on automated grammar acquisition <ref> [6, 23] </ref> that can serve as a basis for comparison to Chill. The corpus contains 729 sentences with an average length of 10.3 words. The experiments reported here were performed using "tagged" strings of lexical categories as input rather than words.
Reference: 7. <author> Eugene Charniak and Glenn Carroll. </author> <title> Context-sensitive statistics for improved grammatical language models. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars <ref> [4, 23, 7] </ref> or transition networks [17]. These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees [3, 12] and transformations [6], statistical methods dominate.
Reference: 8. <author> D. Hindle and M. Rooth. </author> <title> Structural ambiguity and lexical relations. </title> <journal> Computational Linguistics, </journal> <volume> 19(1) </volume> <pages> 103-120, </pages> <year> 1993. </year>
Reference-contexts: The empirical approach replaces hand-generated rules with models obtained automatically by training over language corpora. Corpus-based methods may be used to augment the knowledge of a traditional parser, for example by acquiring new case-frames for verbs [13] or acquiring models to resolve lexical or attachment ambiguities <ref> [11, 8] </ref>. More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars [4, 23, 7] or transition networks [17].
Reference: 9. <author> B. Kijsirikul, M. Numao, and M. Shimura. </author> <title> Discrimination-based constructive induction of logic programs. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 44-49, </pages> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem [20] and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ <ref> [9] </ref>. Details of the Chill induction algorithm can be found in [28, 29, 27]. Given our simple example, a control rule that might be learned for the agent operator is op ([X,[Y,det:the]], [the|Z], A, B) :- animate (Y). animate (man). animate (boy). animate (girl) ....
Reference: 10. <author> N. Lavrac and S. Dzeroski, </author> <title> editors. Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Experimental results demonstrate that the bias provided by the parsing framework significantly improves the accuracy of the resulting parsers. 2 Using ILP for Parser Construction 2.1 Introduction to ILP ILP research considers the problem of inducing a first-order, definite-clause logic program from a set of examples and given background knowledge <ref> [10, 22] </ref>. As such, it stands at the intersection of the traditional fields of machine learning and logic programming. As an example ILP task, consider learning the concept of list membership.
Reference: 11. <author> Jill Fain Lehman. </author> <title> Toward the essential nature of satistical knowledge in sense resolution. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: The empirical approach replaces hand-generated rules with models obtained automatically by training over language corpora. Corpus-based methods may be used to augment the knowledge of a traditional parser, for example by acquiring new case-frames for verbs [13] or acquiring models to resolve lexical or attachment ambiguities <ref> [11, 8] </ref>. More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars [4, 23, 7] or transition networks [17].
Reference: 12. <author> David M. Magerman. </author> <title> Natrual Lagnuage Parsing as Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1994. </year>
Reference-contexts: These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees <ref> [3, 12] </ref> and transformations [6], statistical methods dominate. A common thread in all of these approaches is that the acquired knowledge is represented in a propositional form (perhaps with associated probabilities).
Reference: 13. <author> Christopher D. Manning. </author> <title> Automatic acquisition of a large subcategorization dictionary from corpora. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 235-242, </pages> <address> Columbus, Ohio, </address> <year> 1993. </year>
Reference-contexts: The empirical approach replaces hand-generated rules with models obtained automatically by training over language corpora. Corpus-based methods may be used to augment the knowledge of a traditional parser, for example by acquiring new case-frames for verbs <ref> [13] </ref> or acquiring models to resolve lexical or attachment ambiguities [11, 8]. More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora.
Reference: 14. <author> M. Marcus, B. Santorini, and M.A. Marcinkiewicz. </author> <title> Building a large annotated corpus of English: The Penn treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference: 15. <author> J. L. McClelland and A. H. Kawamoto. </author> <title> Mechanisms of sentence processing: Assigning roles to constituents of sentences. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA, </address> <year> 1986. </year>
Reference-contexts: This task involves a corpus of 1475 sentence/case-structure pairs originally presented in <ref> [15] </ref>. The corpus was produced from a set of 19 sentence templates, generating sentences such as "The HUMAN ate the FOOD with the UTENSIL", where the capitalized items are replaced with words of the given category. The sample actually comprises 1390 unique sentences, some of which allow multiple analyses.
Reference: 16. <author> R. Miikkulainen and M. G. Dyer. </author> <title> Natural language processing with modular PDP networks and distributed lexicon. </title> <journal> Cognitive Science, </journal> <volume> 15 </volume> <pages> 343-399, </pages> <year> 1991. </year>
Reference-contexts: since there could be a large number of incorrect representations that they could match. 3.2 Results for Case-Role Mapping In one experiment, Chill and naive ILP were compared on an artificial data set for case-role mapping that has been used to demonstrate certain language processing abilities of artificial neural networks <ref> [16] </ref>. This task involves a corpus of 1475 sentence/case-structure pairs originally presented in [15]. The corpus was produced from a set of 19 sentence templates, generating sentences such as "The HUMAN ate the FOOD with the UTENSIL", where the capitalized items are replaced with words of the given category. <p> This measure can be viewed as an average of the parser's precision and recall for a given sentence. Fig. 2. Chill vs. Naive ILP on Artificial Case-role Task able to learn this simple task very accurately, significantly outperforming the previous neural network approaches <ref> [16] </ref>. The results show that the naive approach performs only slightly worse than Chill for small examples sets, and becomes indistinguishable as the sample grows.
Reference: 17. <author> Scott Miller, Robert Bobrow, Robert Ingria, and Richard Schwartz. </author> <title> Hidden understanding models of natural language. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 25-32, </pages> <year> 1994. </year>
Reference-contexts: More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars [4, 23, 7] or transition networks <ref> [17] </ref>. These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees [3, 12] and transformations [6], statistical methods dominate.
Reference: 18. <author> R. J. Mooney and M. E. Califf. </author> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3 </volume> <pages> 1-24, </pages> <year> 1995. </year>
Reference-contexts: Since the naive approach requires positive and negative examples of the parse/2 concept, we employed a version of the induction algorithm which exploits the output-completeness assumption to learn in the context of implicit negative examples <ref> [31, 18] </ref>. A complete discussion of this technique is beyond the scope of this paper, but the intuition is straightforward. A developing program is evaluated by using it to construct all possible parses that it can generate from a given training sentence.
Reference: 19. <author> S. Muggleton and W. Buntine. </author> <title> Machine invention of first-order predicates by inverting resolution. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 339-352, </pages> <address> Ann Arbor, MI, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: This control rule comprises a definite-clause definition that covers the positive control examples for the operator but not the negative. There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol <ref> [19] </ref> and Golem [20] and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in [28, 29, 27].
Reference: 20. <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 281-297. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem <ref> [20] </ref> and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in [28, 29, 27].
Reference: 21. <author> S. Muggleton, R. King, and M. Sternberg. </author> <title> Protein secondary structure prediction using logic-based machine learning. </title> <journal> Protein Engineering, </journal> <volume> 5(7) </volume> <pages> 647-657, </pages> <year> 1992. </year>
Reference-contexts: ILP methods have successfully induced small programs for sorting and list manipulation [24] as well as produced encouraging results on important applications such as predicting protein secondary structure <ref> [21] </ref>. Our research seeks to apply ILP methods to NLP in a effort to bridge the gap between traditional NLP and empirical approaches. A major advantage using ILP techniques is the resulting flexibility.
Reference: 22. <editor> S. H. Muggleton, editor. </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1992. </year>
Reference-contexts: Experimental results demonstrate that the bias provided by the parsing framework significantly improves the accuracy of the resulting parsers. 2 Using ILP for Parser Construction 2.1 Introduction to ILP ILP research considers the problem of inducing a first-order, definite-clause logic program from a set of examples and given background knowledge <ref> [10, 22] </ref>. As such, it stands at the intersection of the traditional fields of machine learning and logic programming. As an example ILP task, consider learning the concept of list membership.
Reference: 23. <author> F. Periera and Y. Shabes. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In Proceedings of the 30th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 128-135, </pages> <address> Newark, Delaware, </address> <year> 1992. </year>
Reference-contexts: More radical approaches attempt to replace the hand-crafted components altogether, constructing complete parsers directly from suitable corpora. Recent approaches to constructing robust parsers from corpora primarily use statistical and probabilistic methods such as stochastic grammars <ref> [4, 23, 7] </ref> or transition networks [17]. These methods eschew traditional, symbolic parsing in favor of statistical and probabilistic methods. Although several current methods learn some symbolic structures such as decision trees [3, 12] and transformations [6], statistical methods dominate. <p> We chose this particular data because it represents realistic input from human-computer interaction, and because it has been used in a number of other studies on automated grammar acquisition <ref> [6, 23] </ref> that can serve as a basis for comparison to Chill. The corpus contains 729 sentences with an average length of 10.3 words. The experiments reported here were performed using "tagged" strings of lexical categories as input rather than words.
Reference: 24. <author> J. R. Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, </address> <year> 1993. </year>
Reference-contexts: Due to the expressiveness of first-order logic, ILP methods can learn relational and recursive concepts that cannot be represented in the featural languages assumed by most machine-learning algorithms. ILP methods have successfully induced small programs for sorting and list manipulation <ref> [24] </ref> as well as produced encouraging results on important applications such as predicting protein secondary structure [21]. Our research seeks to apply ILP methods to NLP in a effort to bridge the gap between traditional NLP and empirical approaches. A major advantage using ILP techniques is the resulting flexibility.
Reference: 25. <author> J.R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: There is a growing body of research in inductive logic programming which addresses this problem. Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem [20] and top-down methods from systems like Foil <ref> [25] </ref>, and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in [28, 29, 27].
Reference: 26. <author> R. F. Simmons and Y. Yu. </author> <title> The acquisition and use of context dependent grammars for English. </title> <journal> Computational Linguistics, </journal> <volume> 18(4) </volume> <pages> 391-418, </pages> <year> 1992. </year>
Reference-contexts: When the system came to a parsing impasse, a new rule was created by inferring the correct parsing action and creating a new rule using certain properties of the current parser state as trigger conditions for its application. In a similar vein, Simmons and Yu <ref> [26] </ref> controlled a simple shift-reduce parser by storing example contexts consisting of the syntactic categories of a fixed number of stack and input buffer locations. New sentences were parsed by matching the current parse state to the stored examples and performing the action corresponding to the best matching previous context.
Reference: 27. <author> J. M. Zelle. </author> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, TX, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: In machine learning, such approaches are often called feature-vector representations, as each decision context can be specified by a finite vector of atomic values associated with the various features of interest. In contrast, the Chill system <ref> [28, 30, 27] </ref> uses a framework for learning with structured representations. Relational representations have long been a tool of traditional NLP. Virtually all of this work has utilized hand-crafted grammars as suitable methods for automating the construction of relational knowledge bases had not yet been developed. <p> Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem [20] and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in <ref> [28, 29, 27] </ref>. Given our simple example, a control rule that might be learned for the agent operator is op ([X,[Y,det:the]], [the|Z], A, B) :- animate (Y). animate (man). animate (boy). animate (girl) .... Here the system has invented a new predicate to help explain the parsing decisions.
Reference: 28. <author> J. M. Zelle and R. J. Mooney. </author> <title> Learning semantic grammars with constructive inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 817-822, </pages> <address> Washington, D.C., </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In machine learning, such approaches are often called feature-vector representations, as each decision context can be specified by a finite vector of atomic values associated with the various features of interest. In contrast, the Chill system <ref> [28, 30, 27] </ref> uses a framework for learning with structured representations. Relational representations have long been a tool of traditional NLP. Virtually all of this work has utilized hand-crafted grammars as suitable methods for automating the construction of relational knowledge bases had not yet been developed. <p> Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem [20] and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in <ref> [28, 29, 27] </ref>. Given our simple example, a control rule that might be learned for the agent operator is op ([X,[Y,det:the]], [the|Z], A, B) :- animate (Y). animate (man). animate (boy). animate (girl) .... Here the system has invented a new predicate to help explain the parsing decisions.
Reference: 29. <author> J. M. Zelle and R. J. Mooney. </author> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pages 343-351, </pages> <address> New Brunswick, NJ, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: Chill combines elements from bottom-up techniques found in systems such as Cigol [19] and Golem [20] and top-down methods from systems like Foil [25], and is able to invent new predicates in a manner analogous to Champ [9]. Details of the Chill induction algorithm can be found in <ref> [28, 29, 27] </ref>. Given our simple example, a control rule that might be learned for the agent operator is op ([X,[Y,det:the]], [the|Z], A, B) :- animate (Y). animate (man). animate (boy). animate (girl) .... Here the system has invented a new predicate to help explain the parsing decisions.
Reference: 30. <author> J. M. Zelle and R. J. Mooney. </author> <title> Inducing deterministic Prolog parsers from tree-banks: A machine learning approach. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 748-753, </pages> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: In machine learning, such approaches are often called feature-vector representations, as each decision context can be specified by a finite vector of atomic values associated with the various features of interest. In contrast, the Chill system <ref> [28, 30, 27] </ref> uses a framework for learning with structured representations. Relational representations have long been a tool of traditional NLP. Virtually all of this work has utilized hand-crafted grammars as suitable methods for automating the construction of relational knowledge bases had not yet been developed. <p> For example, an operator to reduce a prepositional phrase might look like: op ([S1,S2|Ss], Words, [pp:[S2,S1]|Ss], Words). Our initial experiments used this simple representation of parsing actions <ref> [30] </ref>. However, better results were obtained by making the operators more specific, effectively increasing the number of operators, but reducing the complexity of the control-rule induction task for each operator. The basic idea was to index the operators based on some relevant portion of the parsing context.
Reference: 31. <author> John M. Zelle, Cynthia A. Thompson, Mary Elaine Califf, and Raymond J. Mooney. </author> <title> Inducing logic programs without explicit negative examples. </title> <booktitle> In Proceedings of the Fifth International Workshop on Inductive Logic Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Rather than attempting to generate a set of explicit negative examples, we have developed a technique of quantifying implicit negative examples that effectively overcomes this difficulty <ref> [31] </ref>. The problem of providing appropriate background knowledge has been largely unexplored; in our experiments, we relied on the ability of the ILP algorithm to invent suitable background relations. 3 We use the standard notation hnamei/hnumberi to indicate the name and arity (number of arguments) for a predicate. <p> Since the naive approach requires positive and negative examples of the parse/2 concept, we employed a version of the induction algorithm which exploits the output-completeness assumption to learn in the context of implicit negative examples <ref> [31, 18] </ref>. A complete discussion of this technique is beyond the scope of this paper, but the intuition is straightforward. A developing program is evaluated by using it to construct all possible parses that it can generate from a given training sentence.
References-found: 31

