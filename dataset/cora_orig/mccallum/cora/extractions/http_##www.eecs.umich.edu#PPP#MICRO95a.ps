URL: http://www.eecs.umich.edu/PPP/MICRO95a.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: alexe,davidson@eecs.umich.edu  
Title: Stage Scheduling: A Technique to Reduce the Register Requirements of a Modulo Schedule  
Author: Alexandre E. Eichenberger and Edward S. Davidson 
Keyword: Register-sensitive modulo scheduling, software pipelining, loop scheduling, instruction level parallelism, VLIW, superscalar.  
Address: Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory EECS Department, University of Michigan  
Abstract: Modulo scheduling is an efficient technique for exploiting instruction level parallelism in a variety of loops, resulting in high performance code but increased register requirements. We present a set of low computational complexity stage-scheduling heuristics that reduce the register requirements of a given modulo schedule by shifting operations by multiples of II cycles. Measurements on a benchmark suite of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels shows that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. R. Rau, C. D. Glaeser, and R. L. </author> <title> Picard. Efficient code generation for horizontal architectures: Compiler techniques and architecture support. </title> <publisher> ISCA, </publisher> <pages> pages 131-139, </pages> <year> 1982. </year>
Reference: [2] <author> P. Y. Hsu. </author> <title> Highly Concurrent Scalar Processing. </title> <type> PhD thesis, </type> <institution> U. of Illinois at Urbana-Champaign, </institution> <year> 1986. </year>
Reference: [3] <author> B. R. Rau. </author> <title> Iterative Modulo Scheduling: An algorithm for software pipelining loops. </title> <booktitle> MICRO, </booktitle> <pages> pages 63-74, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: This choice was motivated by the availability of quality code for this machine. Also, the resource requirements of the Cy-dra 5 machine are complex [25], thus stressing the importance of good and robust scheduling algorithms. In particular, the machine configuration is the one used in <ref> [3] </ref>, with 7 functional units (2 memory port units, 2 address generation units, 1 FP adder unit, 1 FP multiplier unit, and 1 branch unit). <p> We present here the register requirements associated with these schedules in our comparisons. For dependence graphs with underlying cycles, the complexity of this scheduler is polynomial. Iterative Modulo Scheduler <ref> [3] </ref>: This modulo scheduler has been designed to deal efficiently with realistic machine models while producing schedules with near optimal steady state throughput. <p> Experimental findings show that this algorithm requires the scheduling of only 59% more operations than does acyclic list 346 scheduling while resulting in schedules that are optimal in II for 96% of loops in their benchmark <ref> [3] </ref>. In its current form, it does not attempt to minimize the register requirements of its schedules; however, we will show that the register requirements of its schedules may be reduced significantly by the simple heuristics of this paper.
Reference: [4] <author> M. Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> PLDI, </booktitle> <pages> pages 318-328, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The scope of modulo scheduling has been widened to a large variety of loops. Loops with conditional statements are handled using hierarchical reduction <ref> [4] </ref> or IF-conversion [5]. Modulo scheduling has also been extended to a large variety of loops with early exits, such as while loops [6][7]. Furthermore, the code expansion due to modulo scheduling can be eliminated by using special hardware, such as rotating register files and support for predicated execution [8].
Reference: [5] <author> N. J. Warter, G. E. Haab, and J. W. Bockhaus. </author> <title> Enhanced Modulo Scheduling for loops with conditional branches. </title> <booktitle> MICRO, </booktitle> <pages> pages 170-179, </pages> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: The scope of modulo scheduling has been widened to a large variety of loops. Loops with conditional statements are handled using hierarchical reduction [4] or IF-conversion <ref> [5] </ref>. Modulo scheduling has also been extended to a large variety of loops with early exits, such as while loops [6][7]. Furthermore, the code expansion due to modulo scheduling can be eliminated by using special hardware, such as rotating register files and support for predicated execution [8].
Reference: [6] <author> M. S. Schlansker, V. Kathail, and S. Anik. </author> <title> Height reduction of control recurrences for ILP processors. </title> <booktitle> MICRO, </booktitle> <pages> pages 40-51, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [7] <author> P. P. Tirumalai, M. Lee, and M. S. Schlansker. </author> <title> Par-allelization of loops with exits on pipelined architectures. </title> <booktitle> Supercomputing '90, </booktitle> <pages> pages 200-212, </pages> <month> Nov. </month> <year> 1990. </year>
Reference: [8] <author> B. R. Rau, M. Lee, P. P. Tirumalai, and M. S. Schlansker. </author> <title> Register allocation for software pipelined loops. </title> <booktitle> PLDI, </booktitle> <pages> pages 283-299, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: Modulo scheduling has also been extended to a large variety of loops with early exits, such as while loops [6][7]. Furthermore, the code expansion due to modulo scheduling can be eliminated by using special hardware, such as rotating register files and support for predicated execution <ref> [8] </ref>. As modulo scheduling increases execution parallelism, register requirements increase because more values are needed to support more concurrent operations. This effect is inherent to parallelism and will be exacerbated by wider machines and higher latency pipelines [9]. <p> A modulo schedule can be logically divided into stages of II cycles each. The number of stages in a schedule is referred to as the Stage Count <ref> [8] </ref>, and defines the maximum number of concurrent iterations in a modulo schedule. Figure 1b also depicts the 4 stages associated with the schedule of Example 1.
Reference: [9] <author> W. Mangione-Smith, S. G. Abraham, and E. S. Davidson. </author> <title> Register requirements of pipelined processors. </title> <booktitle> ICS, </booktitle> <pages> pages 260-271, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: As modulo scheduling increases execution parallelism, register requirements increase because more values are needed to support more concurrent operations. This effect is inherent to parallelism and will be exacerbated by wider machines and higher latency pipelines <ref> [9] </ref>. As a result, developing scheduling techniques that exploit instruction level parallelism while containing the register requirements is crucial to the performance of future machines. Several heuristics have been proposed for register sensitive modulo scheduling. <p> In Figure 1c for example, from the time of the mult operation (time 1) to the time of the add operation (time 5), the row of the add operation is skipped twice, at time 1 and 3. The skip factor is within 1 of the iteration difference used in <ref> [9] </ref> [12].
Reference: [10] <author> R. A. Huff. </author> <title> Lifetime-sensitive modulo scheduling. </title> <booktitle> PLDI, </booktitle> <pages> pages 258-267, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Several heuristics have been proposed for register sensitive modulo scheduling. For example, Huff investigated a heuristic based on a bidirectional slack-scheduling method that schedules operations early or late depending on their number of stretchable input and output flow dependences <ref> [10] </ref>. Llosa et al have recently proposed a heuristic that is based on a bidi 338 rectional slack-scheduling method with a scheduling priority function tailored to minimize the register requirements [11]. Others have proposed (integer) linear programming solutions [12][13][14][15][16][17]. <p> First, the heuristic removes all redundant edges, which has a quadratic computational complexity in the number of edges [17], if the MinDist relation is provided. Computing the MinDist relation <ref> [10] </ref> has a cubic computational complexity in the number of operations. However, this relation may be readily available, as some modulo schedulers use this relation to determine the minimum initiation interval. Then, the heuristic applies the up-propagation transform to each reg-source vertex, i.e. vertices without input register edges. <p> We are unable to provide a comparison with Huff's scheduling algorithm since his machine model differs slightly from ours and his latest scheduler, presented in <ref> [10] </ref>, was not available to us. MinReg Stage Scheduler [15]: This stage sched-uler minimizes MaxLive, the maximum number of live values at any single time in the loop schedule, over all valid modulo schedules that share a given MRT. <p> In this graph, the X-axis represents MaxLive and the Y-axis represents the fraction of loops scheduled. The "Schedule Independent Lower Bound" curve corresponds to <ref> [10] </ref> and is representative of the register requirements of a machine without any resource conflicts. There is a significant gap between the Min-Reg Stage Scheduler curve and the Lower Bound curve which we believe is caused by two factors.
Reference: [11] <author> J. Llosa, M. Valero, and E. Ayguade. </author> <title> Bidirectional scheduling to minimize register requirements. </title> <booktitle> Fifth Workshop on Compilers for Parallel Computers, </booktitle> <pages> pages 534-554, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Llosa et al have recently proposed a heuristic that is based on a bidi 338 rectional slack-scheduling method with a scheduling priority function tailored to minimize the register requirements <ref> [11] </ref>. Others have proposed (integer) linear programming solutions [12][13][14][15][16][17]. These approaches are flexible in that they directly generate a modulo schedule, thus potentially generating schedules with lower register requirements.
Reference: [12] <author> Q. Ning and G. R. Gao. </author> <title> A novel framework of register allocation for software pipelining. </title> <booktitle> POPL, </booktitle> <pages> pages 29-42, </pages> <year> 1993. </year>
Reference-contexts: The skip factor is within 1 of the iteration difference used in [9] <ref> [12] </ref>.
Reference: [13] <author> Dupont de Dinechin. </author> <title> Simplex scheduling: More than lifetime-sensitive instruction scheduling. </title> <type> PACT, </type> <year> 1994. </year>
Reference: [14] <author> R. Govindarajan, E. R. Altman, and G. R. Gao. </author> <title> Minimizing register requirements under resource-constrained rate-optimal software pipelining. </title> <booktitle> MICRO, </booktitle> <pages> pages 85-94, </pages> <month> Nov. </month> <year> 1994. </year>
Reference: [15] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abra-ham. </author> <title> Minimum register requirements for a modulo schedule. </title> <booktitle> MICRO, </booktitle> <pages> pages 75-84, </pages> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: We investigate the performance of these stage scheduling heuristics for a benchmark of 1289 loops from the Perfect Club, SPEC-89, and the Livermore Fortran Kernels. We contrast the performance of these stage scheduling heuristics to the optimal stage scheduler presented in <ref> [15] </ref>. Our experimental findings show that our best heuristic achieves on average 99% of the decrease in register requirements obtained by an optimal stage scheduler over the entire benchmark suite. <p> Using II and time i , we determine the row of the MRT in which operation i is placed, namely row i = time i mod II . We also define the following distance relation between operation i and j <ref> [15] </ref>: rdist i;j distance between the row of operation i to the next row of operation j, possibly in the next instance of the MRT. which may be computed as follows: rdist i;j = (row j row i ) mod II (1) Note that since the MRT of a modulo schedule <p> In general, there is one such equation for each pair of distinct paths between two operations in the underlying dependence graph. As shown in <ref> [15] </ref>, it is sufficient to have one equation per elementary underlying cycle, as multiple paths between operations are composed from elementary underlying cycles in the underlying dependence graph. These equations are referred to as underlying cycle constraints and precisely define the space of feasible stage schedules. <p> We conclude by illustrating the effect of these heuristics on our running example. 4.1 Stage Scheduling Transforms The first transform modifies the additional skip factors associated with cut edges, i.e. edges that belong to acyclic parts of the underlying dependence graph. We proved in <ref> [15] </ref> that the additional skip factors of a dependence graph without underlying cycles can be individually minimized by setting their values to zero. We apply this theorem here to the parts of the underlying dependence graph that are acyclic. <p> scheduling constraints only affect the choice of II, which is unchanged by the stage scheduling heuristics. 343 acyclic transforms only decrease the p i;j , and since the register requirements associated with a stage schedule is the sum of the skip factors of the last-use operations plus other unrelated terms <ref> [15] </ref>, the register requirements cannot increase. This heuristic highlights the advantage of describing a stage schedule using skip factors associated with edges instead of absolute scheduling times associated with operations. <p> We are unable to provide a comparison with Huff's scheduling algorithm since his machine model differs slightly from ours and his latest scheduler, presented in [10], was not available to us. MinReg Stage Scheduler <ref> [15] </ref>: This stage sched-uler minimizes MaxLive, the maximum number of live values at any single time in the loop schedule, over all valid modulo schedules that share a given MRT. The resulting schedule has the lowest achievable register requirements for the given machine, loop, and MRT. <p> The resulting schedule has the lowest achievable register requirements for the given machine, loop, and MRT. For dependence graphs with underlying cycles, the complexity of this scheduler is exponential. MinBuf Stage Scheduler <ref> [15] </ref>: This stage sched-uler minimizes the integer MaxLive over all valid modulo schedules that share a given MRT. The resulting schedule has the lowest achievable buffer requirements for the given machine, loop, and MRT.
Reference: [16] <author> J. Wang, A. Krall, and M.A. Ertl. </author> <title> Decomposed software pipelining with reduced register requirement. </title> <booktitle> In PACT, </booktitle> <month> June </month> <year> 1995. </year>
Reference: [17] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abra-ham. </author> <title> Optimum modulo schedules for minimum register requirements. </title> <booktitle> ICS, </booktitle> <pages> pages 31-40, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: Also, our previous work did not present any experi mental results. This work also builds upon a technique presented in <ref> [17] </ref> that simplifies the dependence graph of a loop iteration. In that work, we explored the entire space of modulo schedules to find a schedule with the highest steady state throughput over all modulo schedules, and the minimum register requirements among such schedules. <p> Without careful inspection, one may conclude that either solution results in the same register requirements, as reducing one lifetime increases the other, since both p 1;2 and p 2;4 are associated with scheduling and register edges. However, by using the redundant edge removal technique presented in <ref> [17] </ref>, we discover that the register edges (load1, add2) and (load1, div3) are redundant, as the register requirements generated by the register edge (load1, div5) cover the register requirements of these two edges. <p> First, the heuristic removes all redundant edges, which has a quadratic computational complexity in the number of edges <ref> [17] </ref>, if the MinDist relation is provided. Computing the MinDist relation [10] has a cubic computational complexity in the number of operations. However, this relation may be readily available, as some modulo schedulers use this relation to determine the minimum initiation interval. <p> This result may be partially explained by the fact that this scheduler local and absolute minimum was surprisingly small <ref> [17] </ref>.
Reference: [18] <author> C. Eisenbeis and D. Windheiser. </author> <title> Optimal software pipelining in presence of resource constraints. </title> <type> PACT, </type> <month> Aug. </month> <year> 1993. </year>
Reference: [19] <author> J. Wang, C. Eisenbeis, M. Jourdan, and B. Su. </author> <title> Decomposed software pipelining: A new perspective and a new approach. In Int. </title> <journal> J. of Parallel Programming, </journal> <volume> volume 22, </volume> <pages> pages 357-379, </pages> <year> 1994. </year>
Reference: [20] <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1974. </year>
Reference-contexts: This heuristic has a linear computational complexity in the number of edges and results in valid stage schedules with no larger register requirements. First, the heuristic detects all cut edges, using a linear-time algorithm that enumerates the bicon-nected components of an undirected graph <ref> [20, pp. 179-187] </ref>, i.e. the underlying cycles of the dependence graph. Cut edges are then simply the edges linking vertices from distinct biconnected components.
Reference: [21] <author> M. Berry et al. </author> <title> The Perfect Club Benchmarks: Effective performance evaluation of supercomputers. </title> <journal> The Int. J. of Supercomputer Appl., </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: as the propagation of the p i;j values may require more than one pass before being reduced for a dependence graph with strongly connected components. 5 Measurements We investigate the register requirements of the integer and floating point register files for a benchmark of loops obtained from the Perfect Club <ref> [21] </ref>, SPEC-89 [22], and the Livermore Fortran Kernels [23]. Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [24].
Reference: [22] <author> J. Uniejewski. </author> <title> SPEC Benchmark Suite: Designed for today's advanced system. </title> <journal> SPEC Newsletter, </journal> <month> Fall </month> <year> 1989. </year>
Reference-contexts: propagation of the p i;j values may require more than one pass before being reduced for a dependence graph with strongly connected components. 5 Measurements We investigate the register requirements of the integer and floating point register files for a benchmark of loops obtained from the Perfect Club [21], SPEC-89 <ref> [22] </ref>, and the Livermore Fortran Kernels [23]. Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [24].
Reference: [23] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels: A computer test of the numerical performance range. </title> <type> TR UCRL-53745, </type> <institution> Lawrence Livermore Nat. Lab., Livermore, California, </institution> <year> 1986. </year>
Reference-contexts: may require more than one pass before being reduced for a dependence graph with strongly connected components. 5 Measurements We investigate the register requirements of the integer and floating point register files for a benchmark of loops obtained from the Perfect Club [21], SPEC-89 [22], and the Livermore Fortran Kernels <ref> [23] </ref>. Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler [24].
Reference: [24] <author> J. C. Dehnert and R. A. Towle. </author> <title> Compiling for the Cydra 5. </title> <booktitle> In The J. of Supercomputing, </booktitle> <volume> volume 7, </volume> <pages> pages 181-227, </pages> <year> 1993. </year>
Reference-contexts: Our input loops consist exclusively of innermost loops with no early exits, no procedure calls, and fewer than 30 basic blocks, as compiled by the Cydra 5 Fortran77 compiler <ref> [24] </ref>. The input to our scheduling algorithms consists of the Fortran77 compiler intermediate representation after load-store elimination, recurrence back-substitution, and IF-conversion.
Reference: [25] <editor> G. R. Beck et al. </editor> <booktitle> The Cydra 5 mini-supercomputer: Architecture and implementation. In The J. of Supercomputing, </booktitle> <volume> volume 7, </volume> <pages> pages 143-180, </pages> <year> 1993. </year> <month> 349 </month>
Reference-contexts: The machine model used in these experiments corresponds to the Cydra 5 machine. This choice was motivated by the availability of quality code for this machine. Also, the resource requirements of the Cy-dra 5 machine are complex <ref> [25] </ref>, thus stressing the importance of good and robust scheduling algorithms. In particular, the machine configuration is the one used in [3], with 7 functional units (2 memory port units, 2 address generation units, 1 FP adder unit, 1 FP multiplier unit, and 1 branch unit).
References-found: 25

