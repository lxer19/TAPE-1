URL: ftp://cns.brown.edu/nin/papers/quyen2.ps.Z
Refering-URL: http://www.math.tau.ac.il/~nin/research.html
Root-URL: 
Title: Classification of Underwater Mammals using Feature Extraction Based on Time-Frequency Analysis and BCM Theory  
Author: Quyen Q. Huynh Leon N Cooper Nathan Intrator Harel Shouval 
Keyword: Classification, Non-linear Feature Extraction, Time/Frequency Analysis, Wavelets.  
Note: To appear: IEEE Transactions On Signal Processing, Special issue on Neural Networks  
Affiliation: Physics Department and Institute for Brain and Neural Systems Brown University  
Abstract: Underwater mammal sound classification is demonstrated using a novel application of wavelet time/frequency decomposition and feature extraction using a BCM unsupervised network. Different feature extraction methods and different wavelet representations are studied. The system achieves outstanding classification performance even when tested with mammal sounds recorded at very different locations (from those used for training). The improved results suggest that nonlinear feature extraction from wavelet representations outperforms different linear choices of basis functions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. R. Coifman and N. Saito, </author> <title> "Constructions of local orthonormal bases for classification and regression," </title> <journal> Comptes Rendus Acad. Sci. Paris, Serie I, </journal> <volume> vol. 319, no. 2, </volume> <pages> pp. 191-196, </pages> <year> 1994. </year>
Reference-contexts: BCM applied to the raw data is performed by extracting 10 features while training on randomly chosen sequential chunks of 512 samples from the 32768 sample raw data. LDB on wavelet packet extracts 10 best discriminant basis functions based on Coifman's algorithm <ref> [1] </ref>. Highest energy corresponds to extracting 10 highest energy coefficients with their location (20 features total) from Daubechies 4 basis. The last row represents classification performance on 10 BCM features extracted from Daubechies 4 basis representation. imal. <p> This suggests a possible combination between these two signal representations in the future. We have also compared two different wavelet representations: the compactly supported wavelet Daubechies 2 4 [16] and the wavelet packet representation with the LDB feature extraction of Coifman and Saito <ref> [1] </ref>. LDB yields the closest results to classification from BCM features. Support for the usefulness of the wavelet representation is provided by comparison to performance of a WFT representation (see Section III-B).
Reference: [2] <author> R. E. Learned and A. S. Willsky, </author> <title> "A wavelet packet approach to transient signal classification," </title> <journal> Appl. Comput. Harmonic Anal., </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 265-278, </pages> <year> 1995. </year>
Reference-contexts: These results are therefore not comparable to results shown in <ref> [2] </ref> where training and testing was done from the same geographical location and possibly same an HUYNH ET AL.: CLASSIFICATION OF UNDERWATER MAMMALS 6 Classification results: Wavelet analysis on 512 dimensions Porpoise Sperm Whale BCM applied on raw signals 32 95 LDB on wavelet packet 98 51 Highest energ. from Daub.
Reference: [3] <author> Q. Huynh, W. Greene, and J. Impagliazzo, </author> <title> "Feature extraction and classification of underwater acoustic signals," in Full Field Inversion Methods in Ocean and Seismo-Acoustics (O. </title> <editor> Di-achok, A. Caiti, P. Gerstoft, and H. Schmidt, </editor> <booktitle> eds.), </booktitle> <pages> pp. 183-188, </pages> <publisher> Kluwer, </publisher> <address> Dordrecht, The Netherlands, </address> <year> 1995. </year>
Reference-contexts: III. Feature Extraction Based on Time-Frequency Analysis and BCM Theory Our previous work <ref> [3] </ref> on using wavelet transforms for feature extraction have shown good results in the classification of marine mammals (dolphins, sperm whales and porpoises).
Reference: [4] <author> R. E. Bellman, </author> <title> Adaptive Control Processes. </title> <publisher> Princeton, </publisher> <address> NJ: </address> <publisher> Princeton University Press, </publisher> <year> 1961. </year>
Reference-contexts: Modern time-frequency techniques (wavelet packet, local trigonometric basis, Gabor expansions) are considered as tools for providing an efficient data representation to transform the original data set to a preliminary feature set. However, classification may be improved if a dimensionality reduction takes place before the classification stage (curse of dimensionality <ref> [4] </ref>). In this case, applying the BCM algorithm to the preliminary feature set (time-frequency transformed data) reveals important clues about the underlying structure of the data.
Reference: [5] <author> N. Intrator and L. N. Cooper, </author> <title> "Objective function formulation of the BCM theory of visual cortical plasticity: Statistical connections, stability conditions," </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 5, </volume> <pages> pp. 3-17, </pages> <year> 1992. </year>
Reference-contexts: Its properties and connection to a lateral inhibition network as well as some related statistical and computational issues are discussed in <ref> [5] </ref>. Under reasonable assumptions, the BCM algorithm (with k BCM neurons) produces k weight vectors which converge iteratively to fixed points corresponding to states of "maximum selectivity." In other words, for a single BCM neuron, the converged weight vector becomes orthogonal to all cluster centers except one.
Reference: [6] <author> N. Intrator and J. I. Gold, </author> <title> "Three-dimensional object recognition of gray level images: The usefulness of distinguishing features," </title> <journal> Neural Computation, </journal> <volume> vol. 5, </volume> <pages> pp. 61-74, </pages> <year> 1993. </year>
Reference: [7] <author> N. Intrator, D. Reisfeld, and Y. Yeshurun, </author> <title> "Face recognition using a hybrid supervised/unsupervised neural network," </title> <journal> Pattern Recognition Letters, </journal> <volume> vol. 17, </volume> <pages> pp. 67-76, </pages> <year> 1996. </year>
Reference: [8] <author> R. R. Coifman and M. Wickerhauser, </author> <title> "Entropy-based algorithms for best basis selection," </title> <journal> IEEE Trans. Info. Theory, </journal> <volume> vol. 38, no. 2, </volume> <pages> pp. 713-719, </pages> <year> 1992. </year>
Reference: [9] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams, </author> <title> "Learning internal representations by error propagation," in Parallel Distributed Processing (D. </title> <editor> E. Rumelhart and J. L. McClelland, eds.), </editor> <volume> vol. 1, </volume> <pages> pp. 318-362, </pages> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Extract k crucial features which are the convolution outputs of the k weight vectors with the transformed unknown data. 5. Present the k features as inputs to a classifier e.g. the back propagation classifier <ref> [9] </ref>. A. Signal description The types of signals explored in this study are the marine mammal sounds namely porpoise and sperm whale which were recorded at a sampling rate of 25 kHz at various locations such as the Gulf of Maine, the Mediterranean and the Caribbean sea.
Reference: [10] <author> P. J. Huber, </author> <title> "Projection pursuit. (with discussion)," </title> <journal> The Annals of Statistics, </journal> <volume> vol. 13, </volume> <pages> pp. 435-475, </pages> <year> 1985. </year>
Reference: [11] <author> J. H. Friedman, </author> <title> "Exploratory projection pursuit," </title> <journal> Journal of the American Statistical Association, </journal> <volume> vol. 82, </volume> <pages> pp. 249-266, </pages> <year> 1987. </year>
Reference: [12] <author> E. L. Bienenstock, L. N. Cooper, and P. W. Munro, </author> <title> "Theory for the development of neuron selectivity: orientation specificity and binocular interaction in visual cortex," </title> <journal> Journal Neuroscience, </journal> <volume> vol. 2, </volume> <pages> pp. 32-48, </pages> <year> 1982. </year>
Reference: [13] <author> E. E. Clothiaux, L. N Cooper, and M. F. Bear, </author> <title> "Synaptic plasticity in visual cortex: Comparison of theory with experiment," </title> <journal> Journal of Neurophysiology, </journal> <volume> vol. 66, </volume> <pages> pp. 1785-1804, </pages> <year> 1991. </year>
Reference: [14] <author> N. Intrator, J. I. Gold, H. H. Bulthoff, and S. Edelman, </author> <title> "Three-dimensional object recognition using an unsupervised neural network: Understanding the distinguishing features," </title> <booktitle> in Proceedings of the 8th Israeli Conference on AICV (Y. </booktitle> <editor> Feldman and A. Bruckstein, </editor> <booktitle> eds.), </booktitle> <pages> pp. 113-123, </pages> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference: [15] <author> N. Intrator, </author> <title> "Feature extraction using an unsupervised neural network," </title> <journal> Neural Computation, </journal> <volume> vol. 4, </volume> <pages> pp. 98-108, 92. </pages>
Reference: [16] <author> I. Daubechies, </author> <title> "Orthonormal bases of compactly supported wavelets," </title> <journal> Comm. Pure and Appl. Math., </journal> <volume> vol. XLI, </volume> <pages> pp. 909-996, </pages> <year> 1988. </year>
Reference-contexts: B. Projections on Wavelet Space As a first step in our approach, we choose to project each of the sound vectors on an orthonormal wavelet basis. Since the sound files are sequences of discrete numbers, we adopt the compactly supported wavelets Daubechies 4 <ref> [16] </ref>, which are based on discrete-time filter banks. Let f = ff k g K1 be the discrete version of the input signal f (t) of length K = 2 n . <p> This suggests a possible combination between these two signal representations in the future. We have also compared two different wavelet representations: the compactly supported wavelet Daubechies 2 4 <ref> [16] </ref> and the wavelet packet representation with the LDB feature extraction of Coifman and Saito [1]. LDB yields the closest results to classification from BCM features. Support for the usefulness of the wavelet representation is provided by comparison to performance of a WFT representation (see Section III-B).
Reference: [17] <author> R. O. Duda and P. E. Hart, </author> <title> Pattern Classification and Scene Analysis. </title> <address> New York: </address> <publisher> John Wiley, </publisher> <year> 1973. </year>
Reference-contexts: For a baseline comparison, we also extracted the first 10 principal components (PCA) from the same data. PCA is much used in signal processing as it is very simple to apply and extracts second order statistics which is sufficient for many applications <ref> [17] </ref>. A feed-forward neural network with 10 input nodes was used as a classifier. The architecture of the network consisted of one hidden layer with 8 nodes and one output node. The network was trained to over ninety percent correct classification on the training data.
Reference: [18] <author> J. Buckheit and D. L. Donoho, </author> <title> "Improved linear discrimination using time-frequency dictionaries," </title> <type> Technical Report, </type> <institution> Stanford University, </institution> <year> 1995. </year>
Reference-contexts: Table II presents classification results from the more conventional way of extracting features from this data, a method that allows comparison with (LDB) <ref> [18] </ref>. The first raw represents results of feature extraction taken directly from the raw signal, namely choosing randomly 512 consecutive measurements from the raw signal and using them as input to the BCM feature extraction. <p> We have also demonstrated the ability of this method to extract features from the huge full-signal wavelet representation. This is a unique feature which can not be performed by linear discrimination <ref> [18] </ref>. Classification based on this feature extraction achieved outstanding results on test data that was recorded at the same location as well as data based on a mixture of locations. Acknowledgments The best discriminant basis software was kindly supplied by R. Coifman.
Reference: [19] <author> R. A. Fisher, </author> <title> "The use of multiple measurements in taxonomic problems," </title> <journal> Annals of Eugenics, </journal> <volume> vol. 7, </volume> <pages> pp. 179-188, </pages> <year> 1936. </year>
Reference-contexts: Rather than looking for the projections that minimize the ratio of the within-class distance vs. the between-class distance (as is done in discriminant analysis) <ref> [19] </ref>, BCM looks for a direction that is 2 The third raw represents classification from the 10 highest energy coefficients of the wavelet representation.
References-found: 19

