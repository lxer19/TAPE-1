URL: http://www.cs.umd.edu/users/edjlali/papers/jsp.ps
Refering-URL: http://www.cs.umd.edu/users/edjlali/papers/jsp.html
Root-URL: 
Email: fedjlali,gagan,als,humphrie,saltzg@cs.umd.edu  
Title: Runtime and Compiler Support for Programming in Adaptive Parallel Environments 1  
Author: Guy Edjlali, Gagan Agrawal, Alan Sussman, Jim Humphries, and Joel Saltz 
Address: College Park, MD 20742, USA  
Affiliation: UMIACS and Dept. of Computer Science University of Maryland  
Abstract: For better utilization of computing resources, it is important to consider parallel programming environments in which the number of available processors varies at runtime. In this paper, we discuss runtime support for data parallel programming in such an adaptive environment. Executing programs in an adaptive environment requires redistributing data when the number of processors changes, and also requires determining new loop bounds and communication patterns for the new set of processors. We have developed a runtime library to provide this support. We discuss how the runtime library can be used by compilers of HPF-like languages to generate code for an adaptive environment. We present performance results for a Navier-Stokes solver and a multigrid template run on a network of workstations and an IBM SP-2. Our experiments show that if the number of processors is not varied frequently, the cost of data redistribution is not significant compared to the time required for the actual computation. Overall, our work establishes the feasibility of compiling HPF for a network of non-dedicated workstations, which are likely to be an important resource for parallel programming in the future.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Compiler and runtime support for structured and block structured applications. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 578-587. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism. <p> Incorporating adaptive parallelism in compilation systems in which parallelism is detected automatically [11] is beyond the scope of this paper. In previous work, we successfully integrated the Multiblock PARTI library with a prototype Fortran90D/HPF compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. Routines provided by the library were inserted for analyzing work partitioning and communication at runtime, whenever compile-time analysis was inadequate. This implementation can be extended to use Adaptive Multiblock PARTI and compile HPF programs for adaptive execution. <p> This approach was successfully used in the prototype HPF/Fortran90D compiler that used the Multiblock PARTI runtime library. Our previous experiments have shown that saving schedules in hash tables and searching for existing schedules results in less than 10% overhead, as compared to a hand implementation that reuses schedules optimally <ref> [1] </ref>. This approach easily extends to programs which include remapping. One of the parameters to the schedule call is the Distributed Array Descriptor (DAD). After remapping, a call for building a new DAD for each distributed array is inserted by the compiler.
Reference: [2] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> Efficient runtime support for parallelizing block structured applications. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 158-167. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: The runtime library has been developed on top of an existing runtime library for structured and block structured applications. This library is called Multiblock PARTI <ref> [2, 20] </ref>, since it was initially used to parallelize multiblock applications. We have developed our runtime support for adaptive parallelism on top of Multiblock PARTI because this runtime library provides much of the runtime support required for forall loops and array expressions in data parallel languages like HPF.
Reference: [3] <author> Gagan Agrawal, Alan Sussman, and Joel Saltz. </author> <title> An integrated runtime and compile-time approach for parallelizing structured and block structured applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> 1995. To appear. Also available as University of Maryland Technical Report CS-TR-3143 and UMIACS-TR-93-94. </note>
Reference-contexts: This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism. <p> Incorporating adaptive parallelism in compilation systems in which parallelism is detected automatically [11] is beyond the scope of this paper. In previous work, we successfully integrated the Multiblock PARTI library with a prototype Fortran90D/HPF compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. Routines provided by the library were inserted for analyzing work partitioning and communication at runtime, whenever compile-time analysis was inadequate. This implementation can be extended to use Adaptive Multiblock PARTI and compile HPF programs for adaptive execution.
Reference: [4] <author> R. Bjornson. </author> <title> Linda on Distributed Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <year> 1991. </year>
Reference-contexts: Each of these tasks can be migrated from one machine to another, but again, there is no way of achieving load-balance when a parallel program needs to be executed on a smaller number of processors. Piranha [8] is a system developed on top of Linda <ref> [4] </ref>. In this system, the application programmer has to write functions for adapting to a change in the number of available processors. Programs written in this system use a master-slave model and the master coordinates relocation of slaves.
Reference: [5] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: To the best of our knowledge, all existing work on compiling data parallel applications assumes that the number of processors available for execution does not vary at runtime <ref> [5, 11, 22] </ref>. If the number of processors varies at runtime, runtime routines need to be inserted for determining work partitioning and communication during the execution of the program. We have developed a runtime library for developing data parallel applications for execution in an adaptive environment. <p> This library was also integrated with the HPF/Fortran90D compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. We discuss the functionality of the existing library and then present the extensions that were implemented to support adaptive parallelism. <p> Incorporating adaptive parallelism in compilation systems in which parallelism is detected automatically [11] is beyond the scope of this paper. In previous work, we successfully integrated the Multiblock PARTI library with a prototype Fortran90D/HPF compiler developed at Syracuse University <ref> [1, 3, 5] </ref>. Routines provided by the library were inserted for analyzing work partitioning and communication at runtime, whenever compile-time analysis was inadequate. This implementation can be extended to use Adaptive Multiblock PARTI and compile HPF programs for adaptive execution. <p> There is no mechanism available for changing the number of processors at runtime. Most of the existing work on compiling data parallel languages for distributed memory machines assumes a model in which the number of processors is statically known at compile-time <ref> [5, 11, 22] </ref>. Therefore, several components of our runtime library are also useful for compiling HPF programs in which a processor arrangement has been specified using the intrinsic function Number of Processors.
Reference: [6] <author> Jeremy Casas, Ravi Konuru, Steve W. Otto, Robert Prouty, and Jonathan Walpole. </author> <title> Adaptive load migration systems for PVM. </title> <booktitle> In Proceedings Supercomputing '94, </booktitle> <pages> pages 390-399. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: Several researchers have addressed the problem of using an adaptive environment for executing parallel programs. However, most of these consider a task parallel model or a master-slave model. In a version of PVM called Migratable PVM (MPVM) <ref> [6] </ref>, a process or a task running on a machine can be migrated to other machines or processors. However, MPVM does not provide any mechanism for redistribution of data across the remaining processors when a data parallel program has to be withdrawn from one of the processors.
Reference: [7] <author> Al Geist, A. Beguelin, J. Dongarra, W. Jiang, R. Manchek, and V. Sunderam. </author> <title> PVM 3 user's guide and reference manual. </title> <type> Technical Report ORNL/TM-12187, </type> <institution> Oak Ridge National Laboratory, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: The Multiblock PARTI library is currently implemented on the Intel iPSC/860 and Paragon, the Thinking Machines CM-5, the IBM SP1/2 and the PVM message passing environment for a network of workstations <ref> [7] </ref>. The design of the library is architecture independent and therefore it can be easily ported to any distributed memory parallel machine or any environment that supports message passing (e.g. Express).
Reference: [8] <author> David Gelernter and David Kaminsky. </author> <title> Supercomputing out of recycled garbage: Preliminary experience with Piranha. </title> <booktitle> In Proceedings of the Sixth International Conference on Supercomputing, </booktitle> <pages> pages 417-427. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: This system provides light-weight user level tasks. Each of these tasks can be migrated from one machine to another, but again, there is no way of achieving load-balance when a parallel program needs to be executed on a smaller number of processors. Piranha <ref> [8] </ref> is a system developed on top of Linda [4]. In this system, the application programmer has to write functions for adapting to a change in the number of available processors. Programs written in this system use a master-slave model and the master coordinates relocation of slaves.
Reference: [9] <author> Michael Gerndt. </author> <title> Updating distributed variables in local computations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(3) </volume> <pages> 171-193, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Communication resulting from loops in many real codes has 8 much simpler features that make it easier and less time-consuming to analyze. For example, in many loops in mesh-based codes, only ghost (or overlap) cells <ref> [9] </ref> need to be filled along certain dimension (s). If the data distribution is not known at compile-time, the analysis for communication can be much simpler if it is known that only overlap cells need to be filled.
Reference: [10] <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: We refer to such communication as a regular section move <ref> [10] </ref>. The library includes a regular section move routine, Regular Section Move Sched, that can analyze the communication associated with a copy from a right hand side array to left hand side array when data distribution, loop bounds and/or strides are not known at compile-time.
Reference: [11] <author> Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: To the best of our knowledge, all existing work on compiling data parallel applications assumes that the number of processors available for execution does not vary at runtime <ref> [5, 11, 22] </ref>. If the number of processors varies at runtime, runtime routines need to be inserted for determining work partitioning and communication during the execution of the program. We have developed a runtime library for developing data parallel applications for execution in an adaptive environment. <p> The final form of support provided by the Multiblock PARTI library is to distribute loop iterations and transform global distributed arrays references into local references. In distributed memory compilation, the owner computes rule is often used for distributing loop iterations <ref> [11] </ref>. Owner computes means that a particular loop iteration is executed by the processor owning the left-hand side array element written into during that iteration. <p> Incorporating adaptive parallelism in compilation systems in which parallelism is detected automatically <ref> [11] </ref> is beyond the scope of this paper. In previous work, we successfully integrated the Multiblock PARTI library with a prototype Fortran90D/HPF compiler developed at Syracuse University [1, 3, 5]. Routines provided by the library were inserted for analyzing work partitioning and communication at runtime, whenever compile-time analysis was inadequate. <p> There is no mechanism available for changing the number of processors at runtime. Most of the existing work on compiling data parallel languages for distributed memory machines assumes a model in which the number of processors is statically known at compile-time <ref> [5, 11, 22] </ref>. Therefore, several components of our runtime library are also useful for compiling HPF programs in which a processor arrangement has been specified using the intrinsic function Number of Processors.
Reference: [12] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: A commonly used model for developing parallel applications is the data parallel programming model, in which parallelism is achieved by dividing large data sets between processors and having each processor work only on its local data. High Performance Fortran (HPF) <ref> [12] </ref>, a language proposed by a consortium from industry and academia and being adopted by a number of vendors, targets the data parallel programming model. <p> More Computation involving A & B .. ... Enddo parallelism is achieved by partitioning data structures (typically arrays) between processors. This model is frequently used for scientific and engineering applications, and most of the existing work on developing languages and compilers for programming parallel machines uses the SPMD model <ref> [12] </ref>. An example of a simple data parallel program that can be easily transformed into a parallel program that can be executed in SPMD mode is shown in Figure 1. <p> To illustrate the functionality of the runtime routines for communication analysis, consider a single statement forall loop as specified in HPF. This is a parallel loop in which loop bounds and strides associated with any loop variable cannot be functions of any other loop variable <ref> [12] </ref>. <p> They may be specified by the programmer in the form of a directive, or they may be determined automatically by the compiler. For the data parallel language HPF, parallelism can only be explicitly specified through certain constructs (e.g.. forall statement, forall construct, independent statement <ref> [12] </ref>). Inside any of these constructs, the only functions that can be called are those explicitly marked as pure functions. Thus it is simple to determine, solely from the syntax, what points in the program are not inside any data parallel loop and therefore can be remap points.
Reference: [13] <author> Message Passing Interface Forum. </author> <title> MPI: A message-passing interface standard. </title> <type> Technical Report CS-94-230, </type> <institution> Computer Science Dept., University of Tennessee, </institution> <month> April </month> <year> 1994. </year> <journal> Also appears in the International Journal of Supercomputer Applications, </journal> <volume> Volume 8, Number 3/4, </volume> <year> 1994. </year>
Reference-contexts: The mapping between physical processor numbers and logical processor numbers, for active processors, is updated at remap points. The use of a logical processor numbering is similar in concept to the scheme used for processor groups in the Message Passing Interface Standard (MPI) <ref> [13] </ref>. Information about data distributions is available at each processor in the Distributed Array Descriptors (DADs). However, DADs only store the total size in each dimension for each distributed array. The exact part of the distributed array owned by an active processor can be determined using the logical processor number.
Reference: [14] <author> M.Litzkow and M.Solomon. </author> <title> Supporting checkpointing and process migration outside the Unix kernel. </title> <booktitle> Usenix Winter Conference, </booktitle> <year> 1992. </year>
Reference-contexts: Condor <ref> [14] </ref> is a system that supports transparent migration of a process (through check 19 pointing) from one workstation to another. It also performs detection to determine if the user of the workstation on which a process is being executed has returned, and also looks out for other idle workstations.
Reference: [15] <author> Vijay K. Naik, Sanjeev Setia, and Mark Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 824-833. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: A more attractive model would be one in which a particular parallel program could use a large number of processors when no other job is waiting for resources, and use a smaller number of processors when other jobs need resources. Setia et al. <ref> [15, 19] </ref> have shown that such a dynamic scheduling policy results in better utilization of the available processors. There has been an increasing trend toward using a network of workstations for parallel execution of programs.
Reference: [16] <author> N.Nedeljkovic and M.J.Quinn. </author> <title> Data-parallel programming on a network of heterogeneous workstations. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(4), </volume> <year> 1993. </year>
Reference-contexts: Programs written in this system use a master-slave model and the master coordinates relocation of slaves. There is no clear way of writing data parallel applications for adaptive execution in all these systems. Data Parallel C and its compilation system <ref> [16] </ref> have been designed for load balancing on a network of heterogeneous machines. The system requires continuous monitoring of the progress of the programs executing on each machine. Experimental results have shown that this involves a significant overhead, even when no load balancing is required [16]. 7 Conclusions and Future Work <p> C and its compilation system <ref> [16] </ref> have been designed for load balancing on a network of heterogeneous machines. The system requires continuous monitoring of the progress of the programs executing on each machine. Experimental results have shown that this involves a significant overhead, even when no load balancing is required [16]. 7 Conclusions and Future Work In this paper we have addressed the problem of developing applications for execution in an adaptive parallel programming environment, meaning an environment in which the number of processors available varies at runtime.
Reference: [17] <author> Andrea Overman and John Van Rosendale. </author> <title> Mapping robust parallel multigrid algorithms to scalable memory architectures. </title> <booktitle> In Proceedings of 1993 Copper Mountain Conference on Multigrid Methods, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: would be useful for implementing these directives in an HPF compiler. 5 Experimental Results To study the performance of the runtime routines and to determine the feasibility of using an adaptive environment for data parallel programming, we have experimented with a multiblock Navier-Stokes solver template [21] and a multigrid template <ref> [17] </ref>. The multiblock template was extracted from a computational fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D). The sequential Fortran77 code was developed by Vatsa et al. at NASA Langley Research Center, and consists of nearly 18,000 lines of code.
Reference: [18] <author> R.Konuru, J.Casa, R.Prouty, and J.Walpole. </author> <title> A user-level process package for PVM. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 48-55. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: Alternatively, the user of the individual workstations can change a variable to let the system know whether or not he/she want their workstation to be used for parallel programs. Our model for adaptive parallel programming is closest to the one presented by Konuru et al. <ref> [18] </ref>. They also consider data parallel programming in an adaptive environment, including a network of heterogeneous workstations. The main difference in their approach is that the responsibility for data repartitioning is given to the application programmer. <p> However, MPVM does not provide any mechanism for redistribution of data across the remaining processors when a data parallel program has to be withdrawn from one of the processors. Another system called User Level Processes (ULP) <ref> [18] </ref> has also been developed. This system provides light-weight user level tasks. Each of these tasks can be migrated from one machine to another, but again, there is no way of achieving load-balance when a parallel program needs to be executed on a smaller number of processors.
Reference: [19] <author> Sanjeev Setia. </author> <title> Scheduling on Multiprogrammed Distributed Memory Parallel Machines. </title> <type> PhD thesis, </type> <institution> University of Maryland, </institution> <month> Aug </month> <year> 1993. </year>
Reference-contexts: A more attractive model would be one in which a particular parallel program could use a large number of processors when no other job is waiting for resources, and use a smaller number of processors when other jobs need resources. Setia et al. <ref> [15, 19] </ref> have shown that such a dynamic scheduling policy results in better utilization of the available processors. There has been an increasing trend toward using a network of workstations for parallel execution of programs.
Reference: [20] <author> Alan Sussman, Gagan Agrawal, and Joel Saltz. </author> <title> A manual for the multiblock PARTI runtime primitives, revision 4.1. </title> <institution> Technical Report CS-TR-3070.1 and UMIACS-TR-93-36.1, University of Mary-land, Department of Computer Science and UMIACS, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: The runtime library has been developed on top of an existing runtime library for structured and block structured applications. This library is called Multiblock PARTI <ref> [2, 20] </ref>, since it was initially used to parallelize multiblock applications. We have developed our runtime support for adaptive parallelism on top of Multiblock PARTI because this runtime library provides much of the runtime support required for forall loops and array expressions in data parallel languages like HPF.
Reference: [21] <author> V.N. Vatsa, M.D. Sanetrik, and E.B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based multiblock flow solver; AIAA-93-0677. </title> <booktitle> In Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year>
Reference-contexts: Our redistribution routines would be useful for implementing these directives in an HPF compiler. 5 Experimental Results To study the performance of the runtime routines and to determine the feasibility of using an adaptive environment for data parallel programming, we have experimented with a multiblock Navier-Stokes solver template <ref> [21] </ref> and a multigrid template [17]. The multiblock template was extracted from a computational fluid dynamics application that solves the thin-layer Navier-Stokes equations over a 3D surface (multiblock TLNS3D).
Reference: [22] <author> Hans P. Zima and Barbara Mary Chapman. </author> <title> Compiling for distributed-memory systems. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2) </volume> <pages> 264-287, </pages> <month> February </month> <year> 1993. </year> <title> In Special Section on Languages and Compilers for Parallel Machines. </title> <type> 22 </type>
Reference-contexts: To the best of our knowledge, all existing work on compiling data parallel applications assumes that the number of processors available for execution does not vary at runtime <ref> [5, 11, 22] </ref>. If the number of processors varies at runtime, runtime routines need to be inserted for determining work partitioning and communication during the execution of the program. We have developed a runtime library for developing data parallel applications for execution in an adaptive environment. <p> There is no mechanism available for changing the number of processors at runtime. Most of the existing work on compiling data parallel languages for distributed memory machines assumes a model in which the number of processors is statically known at compile-time <ref> [5, 11, 22] </ref>. Therefore, several components of our runtime library are also useful for compiling HPF programs in which a processor arrangement has been specified using the intrinsic function Number of Processors.
References-found: 22

