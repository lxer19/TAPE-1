URL: file://ftp.tc.cornell.edu/pub/tech.reports/tr190.ps
Refering-URL: http://www.tc.cornell.edu/~liao/papers.html
Root-URL: http://www.tc.cornell.edu
Title: A new parallel algorithm for global optimization with application to the molecular cluster problem  
Author: Aiping Liao 
Keyword: Key words. global optimization, partially separable structure, Lennard Jones potential function.  
Date: July 25 1994  
Address: Ithaca, NY 14853  
Affiliation: Advanced Computing Research Institute Cornell Theory Center Cornell University  
Abstract: In this paper we present a simple algorithm for global optimization. This algorithm combines random searches with efficient local minimization algorithms. The proposed algorithm begins with an initial "local minimizer." In each iteration, a search direction is generated randomly, along which some points are chosen as the initial points for the local optimization algorithm and several "local minimizers" are obtained. The next iterate is determined by comparing these local minimizers. We will discuss the expected number of iterations for finding a global minimizer with this algorithm. Several variants of the algorithm that take advantage of the partially separable structure are proposed for the Lennard-Jones cluster problem and tested on the IBM SP1 parallel computer. Our numerical results show that our algorithms are promising. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Byrd, T. Derby, E. Eskow, K. Oldenkamp, and R. Schnabel. </author> <title> A new stochastic/perturbation method for large-scale global optimization and its application to water cluster problems. </title> <type> Technical Report cu-cs-652-93, </type> <institution> Department of Computer Science, University of Colorado at Boulder, </institution> <year> 1993. </year>
Reference-contexts: This problem can be characterized as finding the global minimizer of a potential energy function. There are many algorithms for solving this problem (see, for example, Coleman, Shalloway, and Wu [2], Xue [17], Wille [16], and Byrd et al. <ref> [1] </ref> to name a few). <p> This structure has been used in many previous works, such as [3], [9], [14], <ref> [1] </ref>, [13], and [17]. Here we give an approach based on a continuity argument which applies to any problem with such a separability. <p> 12 16 20 No. of Processors parametered function f " (x) := n atom 1 X i=1;j&gt;i where " = f" ij : i = 1; : : : ; n atom 1; j = i + 1; : : : ; n atom 1g and each " ij 2 <ref> [0; 1] </ref>. By the continuity of function f (or a modified version of f truncated from above) the global solutions vary continuously with the parameter ".
Reference: [2] <author> T. F. Coleman, D. Shalloway, and Z. Wu. </author> <title> Isotropic effective energy simulated annealing searches for low energy molecular cluster states. </title> <journal> Computational Optimization and Applications, </journal> <volume> 2 </volume> <pages> 145-170, </pages> <year> 1993. </year>
Reference-contexts: The cluster problem is of interest in many applications in biochemistry and physics. This problem can be characterized as finding the global minimizer of a potential energy function. There are many algorithms for solving this problem (see, for example, Coleman, Shalloway, and Wu <ref> [2] </ref>, Xue [17], Wille [16], and Byrd et al. [1] to name a few). Following Coleman, Shalloway, and Wu [2], we will consider a special potential function called the Lennard-Jones potential function: f (x) = n atom X i=1;j&gt;i where n atom is the number of atoms whose locations are x <p> This problem can be characterized as finding the global minimizer of a potential energy function. There are many algorithms for solving this problem (see, for example, Coleman, Shalloway, and Wu <ref> [2] </ref>, Xue [17], Wille [16], and Byrd et al. [1] to name a few). Following Coleman, Shalloway, and Wu [2], we will consider a special potential function called the Lennard-Jones potential function: f (x) = n atom X i=1;j&gt;i where n atom is the number of atoms whose locations are x j 2 R 3 , j = 1; : : : ; n atom and x 2 R n <p> The results are presented in Table 1. The algorithm terminates if an energy within 10 4 of the lowest energy cited in <ref> [2] </ref> is found. nproc is number of processors. nproc can be any number less than 262; 144. The system will automatically load these processors into the physical machines. <p> We note that Algorithm 4 can be strengthened by either adding in the new atom more slowly or taking more inner iterations with Algorithm 3 to get more accurate intermediate solutions. 16 Table 2: How far the full-range algorithm can go. n atom f <ref> [2] </ref> f /time (in sec)/nproc 4 -6.000000e+00 -6.000000e+00/2.35/10 6 -1.271206e+01 -1.271206e+01/5.00/10 8 -1.982149e+01 -1.982149e+01/9.67/10 10 -2.842254e+01 -2.842253e+01/12.97/10 12 -3.796761e+01 -3.796760e+01/18.89/10 14 -4.784517e+01 -4.784516e+01/30.71/10 16 -5.681575e+01 -5.681574e+01/34.46/10 17 Table 3: the results of the single-range algorithm. n atom f [2] f /time (in sec)/nproc iter 4 -6.000000e+00 -6.000000e+00/2.01/10 15 6 -1.271206e+01 -1.271206e+01/3.75/10 <p> Table 2: How far the full-range algorithm can go. n atom f <ref> [2] </ref> f /time (in sec)/nproc 4 -6.000000e+00 -6.000000e+00/2.35/10 6 -1.271206e+01 -1.271206e+01/5.00/10 8 -1.982149e+01 -1.982149e+01/9.67/10 10 -2.842254e+01 -2.842253e+01/12.97/10 12 -3.796761e+01 -3.796760e+01/18.89/10 14 -4.784517e+01 -4.784516e+01/30.71/10 16 -5.681575e+01 -5.681574e+01/34.46/10 17 Table 3: the results of the single-range algorithm. n atom f [2] f /time (in sec)/nproc iter 4 -6.000000e+00 -6.000000e+00/2.01/10 15 6 -1.271206e+01 -1.271206e+01/3.75/10 15 8 -1.982149e+01 -1.982149e+01/6.84/10 15 10 -2.842254e+01 -2.842253e+01/11.53/10 15 12 -3.796761e+01 -3.796760e+01/17.08/10 15 14 -4.784517e+01 -4.784516e+01/26.33/10 15 16 -5.681575e+01 -5.681574e+01/31.05/10 15 18 -6.653097e+01 -6.653095e+01/38.03/10 15 20 -7.717707e+01 -7.717704e+01/53.32/10 15 22 -8.680981e+01 -8.680978e+01/64.31/10 15 24 -9.734884e+01 -9.734882e+01/77.81/10 20 26
Reference: [3] <author> T. F. Coleman, D. Shalloway, and Z. Wu. </author> <title> A parallel build-up algorithm for global energy minimizations of molecular clusters using effective energy simulated annealing. </title> <type> Technical Report CTC93TR130, </type> <institution> ACRI, Cornell University, </institution> <year> 1993. </year>
Reference-contexts: This structure has been used in many previous works, such as <ref> [3] </ref>, [9], [14], [1], [13], and [17]. Here we give an approach based on a continuity argument which applies to any problem with such a separability.
Reference: [4] <author> J. E. Dennis, Jr. and R. B. Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: Examples are, structural optimization, engineering design, allocation and location problems, and protein folding. There are many efficient algorithms for finding a local minimizer of f , e.g., Newton's method, the quasi-Newton method, and the trust region method (see, for example, Fletcher [5], Dennis and Schnabel <ref> [4] </ref>, and Gill, Murray and Wright [7]). However, these local algorithms make use of local information and cannot predict the behavior of the objective function far away from the current iterate. Once the iterate falls into an absorbing neighborhood of a local (but not global) minimizer it cannot escape.
Reference: [5] <author> R. Fletcher. </author> <title> Practical Methods of Optimization. </title> <publisher> John Wiley & Sons, Ltd, </publisher> <year> 1981. </year>
Reference-contexts: Examples are, structural optimization, engineering design, allocation and location problems, and protein folding. There are many efficient algorithms for finding a local minimizer of f , e.g., Newton's method, the quasi-Newton method, and the trust region method (see, for example, Fletcher <ref> [5] </ref>, Dennis and Schnabel [4], and Gill, Murray and Wright [7]). However, these local algorithms make use of local information and cannot predict the behavior of the objective function far away from the current iterate.
Reference: [6] <author> C. Floudas and P. Pardalos. </author> <title> A Collection of Test Problems for Constrained Global Optimization Algorithms. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: 12 -3.796761e+01 -3.796760e+01/17.08/10 15 14 -4.784517e+01 -4.784516e+01/26.33/10 15 16 -5.681575e+01 -5.681574e+01/31.05/10 15 18 -6.653097e+01 -6.653095e+01/38.03/10 15 20 -7.717707e+01 -7.717704e+01/53.32/10 15 22 -8.680981e+01 -8.680978e+01/64.31/10 15 24 -9.734884e+01 -9.734882e+01/77.81/10 20 26 -1.083156e+02 -1.083156e+02/126.20/10 20 18 Table 4: Numerical results for constrained problems problem n f bestknown (<ref> [6] </ref>) f opt x opt 4.3 [6] 4 -4.5142 -4.5142 (4=3; 4; 0; 0) T 4.5 [6] 6 -11.96 -13.4019 (1=6; 2; 4; 1=2; 0; 2) T 4 Concluding remarks We have proposed a simple algorithm for global optimization. The expected number of iterations is considered and parallel implementations are discussed. <p> -5.681574e+01/31.05/10 15 18 -6.653097e+01 -6.653095e+01/38.03/10 15 20 -7.717707e+01 -7.717704e+01/53.32/10 15 22 -8.680981e+01 -8.680978e+01/64.31/10 15 24 -9.734884e+01 -9.734882e+01/77.81/10 20 26 -1.083156e+02 -1.083156e+02/126.20/10 20 18 Table 4: Numerical results for constrained problems problem n f bestknown (<ref> [6] </ref>) f opt x opt 4.3 [6] 4 -4.5142 -4.5142 (4=3; 4; 0; 0) T 4.5 [6] 6 -11.96 -13.4019 (1=6; 2; 4; 1=2; 0; 2) T 4 Concluding remarks We have proposed a simple algorithm for global optimization. The expected number of iterations is considered and parallel implementations are discussed. Some variants of the basic algorithm are described and discussed. <p> To highlight the behavior of the algorithm for constrained problem we solved 3 simple problems (problems 4.3, 4.4, and 4.5) of Floudas and Pardalos <ref> [6] </ref>. The results are presented in Table 4. All runs were performed on one RS6000 and 150 iterations were carried out for each run.
Reference: [7] <author> P. Gill, W. Murray, and M. Wright. </author> <title> Practical Optimization. </title> <publisher> Academic Press, </publisher> <year> 1981. </year>
Reference-contexts: There are many efficient algorithms for finding a local minimizer of f , e.g., Newton's method, the quasi-Newton method, and the trust region method (see, for example, Fletcher [5], Dennis and Schnabel [4], and Gill, Murray and Wright <ref> [7] </ref>). However, these local algorithms make use of local information and cannot predict the behavior of the objective function far away from the current iterate. Once the iterate falls into an absorbing neighborhood of a local (but not global) minimizer it cannot escape.
Reference: [8] <author> A. Griewank and Ph.L. Toint. </author> <title> On the constrained optimization of partially separable objective functions. </title> <editor> In M. J. D. Powell, editor, </editor> <booktitle> Nonlinear Optimization, </booktitle> <pages> pages 301-312, </pages> <address> London, 1982. </address> <publisher> Academic Press, London. </publisher>
Reference-contexts: However we note all these methods are applicable to any problems with the partially separable structure: f (x) = i=1 where each of the ne element functions f i depends only on a few variables. Since every function f with a sparse Hessian is partially separable (Griewank and Toint <ref> [8] </ref>), our methods have a broad application domain. The cluster problem is of interest in many applications in biochemistry and physics. This problem can be characterized as finding the global minimizer of a potential energy function.
Reference: [9] <author> B. Hingerty, S. Figueroa, T. Hayden, and S. Broyd. </author> <title> Prediction of DNA structure from sequence: a build-up technique. </title> <journal> Biopolymers, </journal> <volume> 28 </volume> <pages> 1195-1222, </pages> <year> 1989. </year>
Reference-contexts: This structure has been used in many previous works, such as [3], <ref> [9] </ref>, [14], [1], [13], and [17]. Here we give an approach based on a continuity argument which applies to any problem with such a separability.
Reference: [10] <author> J. Kirkpatrick, Jr. C. D. Gellat, and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year> <month> 21 </month>
Reference-contexts: This is the central philosophy of our method and this philosophy is incorporated into the probability P 1 defined later which characterizes our algorithm. This probability P 1 distincts our method from others as we note that the simulated annealing is based on the Boltzmann distribution <ref> [10] </ref> and the classical Monte Carlo method is based on the uniform distribution. The basic idea of our algorithm is that we take a "local minimizer" as the current iterate and cover it with a "search region." This search region can be, for example, a large ball.
Reference: [11] <author> D. C. Liu and J. Nocedal. </author> <title> On the limited memory bfgs method for large scale optimization. </title> <journal> Math. Program., </journal> <volume> 45 </volume> <pages> 503-528, </pages> <year> 1989. </year>
Reference-contexts: Theoretically, if the number of processors is sufficiently large, the algorithm can find the global minimizer in a few iterations and thus the time spent on the local minimization will be the dominant one. If we use the limited memory BFGS method (see, for example, [12] and <ref> [11] </ref>), instead of the standard BFGS method, the wall-clock time for the calculations for problems with n atom 25 can be reduced by almost half.
Reference: [12] <author> J. Nocedal. </author> <title> Updating quasi-Newton matrices with limited storage. </title> <journal> Math. Comput., </journal> <volume> 35 </volume> <pages> 773-782, </pages> <year> 1980. </year>
Reference-contexts: Theoretically, if the number of processors is sufficiently large, the algorithm can find the global minimizer in a few iterations and thus the time spent on the local minimization will be the dominant one. If we use the limited memory BFGS method (see, for example, <ref> [12] </ref> and [11]), instead of the standard BFGS method, the wall-clock time for the calculations for problems with n atom 25 can be reduced by almost half.
Reference: [13] <author> J. A. Northby. </author> <title> Structure and binding of Lennard-Jones clusters: 13 n 147. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 87 </volume> <pages> 6166-6178, </pages> <year> 1987. </year>
Reference-contexts: This structure has been used in many previous works, such as [3], [9], [14], [1], <ref> [13] </ref>, and [17]. Here we give an approach based on a continuity argument which applies to any problem with such a separability.
Reference: [14] <author> M. Pincus, R. Klausner, and H. Scheraga. </author> <title> Calculation of the three-dimensional structure of the membrane-bound portion of melittin from its amino acid sequence. </title> <booktitle> Proceedings of National Academy of Science, USA, </booktitle> <volume> 79 </volume> <pages> 5107-5110, </pages> <year> 1982. </year>
Reference-contexts: This structure has been used in many previous works, such as [3], [9], <ref> [14] </ref>, [1], [13], and [17]. Here we give an approach based on a continuity argument which applies to any problem with such a separability.
Reference: [15] <author> S. Vavasis. </author> <title> Open problems. </title> <journal> J. Global Optimization, </journal> <volume> 4 </volume> <pages> 343-344, </pages> <year> 1994. </year>
Reference-contexts: A more challenge problem is proposed by Vavasis <ref> [15] </ref>. 1 This value is obviously wrong since it does not fit the associated x fl . 19 Acknowledgment: I would like to thank Professor Thomas F. Coleman and Dr. S. Chinchalkar for many discussions relating to this work and for their helpful comments and suggestions on the manuscript.
Reference: [16] <author> L. T. Wille. </author> <title> Minimum-energy configuration of atom clusters: New results obtained by simulated annealing. </title> <journal> Chemical Physics Letters, </journal> <volume> 133 </volume> <pages> 405-410, </pages> <year> 1987. </year>
Reference-contexts: The cluster problem is of interest in many applications in biochemistry and physics. This problem can be characterized as finding the global minimizer of a potential energy function. There are many algorithms for solving this problem (see, for example, Coleman, Shalloway, and Wu [2], Xue [17], Wille <ref> [16] </ref>, and Byrd et al. [1] to name a few).
Reference: [17] <author> G. Xue. </author> <title> Improvement on the Northby Algorithm for Molecular Conformation: Better Solutions. </title> <journal> J. Global Optimization, </journal> <volume> 4 </volume> <pages> 425-440, </pages> <year> 1994. </year> <month> 22 </month>
Reference-contexts: The cluster problem is of interest in many applications in biochemistry and physics. This problem can be characterized as finding the global minimizer of a potential energy function. There are many algorithms for solving this problem (see, for example, Coleman, Shalloway, and Wu [2], Xue <ref> [17] </ref>, Wille [16], and Byrd et al. [1] to name a few). <p> This structure has been used in many previous works, such as [3], [9], [14], [1], [13], and <ref> [17] </ref>. Here we give an approach based on a continuity argument which applies to any problem with such a separability. <p> The optimal solution x " forms a continuous trajectory and we call it an optimal trajectory. In <ref> [17] </ref> it is observed that "in some cases, the relaxation of a lattice local minimizer with a worse potential function value may lead to a local minimizer with a better potential function value." This observation can be justified with the continuity argument: the minimizer with a worse potential function may be
References-found: 17

