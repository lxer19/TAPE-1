URL: http://iacoma.cs.uiuc.edu/iacoma-papers/false_sharing.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Title: False Sharing and Spatial Locality in Multiprocessor Caches  
Author: Josep Torrellas, Monica S. Lam, and John L. Hennessy 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: The performance of the data cache in shared-memory multiprocessors has been shown to be different from that in uniprocessors. In particular, cache miss rates in multiprocessors do not show the sharp drop typical of uniprocessors when the size of the cache block increases. The resulting high cache miss rate is a cause of concern, since it can significantly limit the performance of multiprocessors. Some researchers have speculated that this effect is due to false sharing, the coherence transactions that result when different processors update different words of the same cache block in an interleaved fashion. While the analysis of six applications in this paper confirms that false sharing has a significant impact on the miss rate, the measurements also show that poor spatial locality among accesses to shared data has an even larger impact. To mitigate false sharing and to enhance spatial locality, we optimize the layout of shared data in cache blocks in a programmer-transparent manner. We show that this approach can reduce the number of misses on shared data by about 10% on average. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. R. Cheriton, H. A. Goosen, and P. D. Boyle, </author> <title> "Multi-Level Shared Caching Techniques for Scalability in VMP-MC," </title> <booktitle> in Proceedings of the 16th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 16-24, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [2] <author> J. Elder, A. Gottlieb, C. K. Kruskal, K. P. McAuliffe, L. Rudolph, M. Snir, P. Teller, and J. Wilson, </author> <title> "Issues Related to MIMD, Shared-Memory Computers: The NYU Ultracomputer Approach," </title> <booktitle> in Proceedings of the 12th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 126-135, </pages> <month> June </month> <year> 1985. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [3] <author> J. R. Goodman and P. J. Woest, </author> <title> "The Wisconsin Multicube: A New Large-Scale Cache-Coherent Multiprocessor," </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 422-431, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [4] <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> "The Directory-Based Cache Coherence Protocol for the DASH Multiprocessor," </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [5] <author> G. Pfister, W. Brantley, D. George, S. Harvey, W. Kleinfelder, K. McAuliffe, E. Melton, A. Norton, and J. Weiss, </author> <title> "The IBM Research Parallel Processor Prototype (RP3): Introduction and Architecture," </title> <booktitle> in Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pp. 764-771, </pages> <year> 1985. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [6] <author> A. W. Wilson, </author> <title> "Hierarchical Cache/Bus Architecture for Shared Memory Multiprocessors," </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 244 32 252, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: 1 Introduction Scalable machines that support a shared-memory paradigm are a promising way of attaining the benefits of large-scale multiprocessing without surrendering programmability <ref> [1, 2, 3, 4, 5, 6] </ref>. An interesting subclass of these machines is the class that provides hardware cache coherence, which makes programming easier, while reducing storage access latencies by caching shared data.
Reference: [7] <author> A. Agarwal and A. Gupta, </author> <title> "Memory-Reference Characteristics of Multiprocessor Applications under MACH," </title> <booktitle> in ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 215-225, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: While these machines can do well on problems with low levels of data sharing, it is unclear how well caches will perform when accesses to shared data occur frequently. The cache performance of shared data is the subject of intense ongoing research. Agarwal and Gupta <ref> [7] </ref> study locality issues in traces of memory references from a four-processor machine and report a high degree of processor interleaving in the accesses to a given shared-memory location. This suggests that shared data can be the source of frequent misses.
Reference: [8] <author> S. J. Eggers and R. H. Katz, </author> <title> "The Effect of Sharing on the Cache and Bus Performance of Parallel Programs," </title> <booktitle> in Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Agarwal and Gupta [7] study locality issues in traces of memory references from a four-processor machine and report a high degree of processor interleaving in the accesses to a given shared-memory location. This suggests that shared data can be the source of frequent misses. Indeed, Eggers and Katz <ref> [8] </ref>, in a simulation of 5 to 12 processors in a bus, show that shared data is responsible for the majority of cache misses and bus cycles. <p> A second issue that motivates the interest in this topic is that the measurements obtained so far on the impact of the block size on the miss rate of shared data show such wide variation <ref> [8] </ref> that they are difficult to generalize. In this paper, we explain the effect of the cache block size on the miss rate as a combination of two well-behaved components: false sharing and spatial locality.
Reference: [9] <author> W. D. Weber and A. Gupta, </author> <title> "Analysis of Cache Invalidation Patterns in Multiprocessors," </title> <booktitle> in Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 243-256, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: While the miss rate in uniprocessors tends to go down with increasing cache block size, the miss rate in multiprocessors can go down or up with larger block sizes. A further understanding of the patterns of data sharing is provided by Weber and Gupta <ref> [9] </ref>, who show that write-shared variables are usually invalidated from caches before being replicated in more than a few different caches.
Reference: [10] <author> R. L. Lee, P. C. Yew, and D. H. Lawrie, </author> <title> "Multiprocessor Cache Design Considerations," </title> <booktitle> in Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 253-262, </pages> <month> June </month> <year> 1987. </year>
Reference-contexts: A further understanding of the patterns of data sharing is provided by Weber and Gupta [9], who show that write-shared variables are usually invalidated from caches before being replicated in more than a few different caches. Finally, in another example of unusual behavior, Lee et al. <ref> [10] </ref> find that the optimal cache block size for data is one or two words long, in contrast to the larger sizes used in uniprocessors [11]. Clearly, given the performance impact of the cache behavior of shared data, a deeper understanding of it is necessary.
Reference: [11] <author> A. J. Smith, </author> <title> "Line (Block) Size Choice for CPU Caches," </title> <journal> in IEEE Trans. on Computers, </journal> <pages> pp. 1063-1075, </pages> <month> September </month> <year> 1987. </year>
Reference-contexts: Finally, in another example of unusual behavior, Lee et al. [10] find that the optimal cache block size for data is one or two words long, in contrast to the larger sizes used in uniprocessors <ref> [11] </ref>. Clearly, given the performance impact of the cache behavior of shared data, a deeper understanding of it is necessary. In this paper, we focus on one parameter that has a major effect on the cache performance of shared data, namely the size of the cache blocks. <p> The large difference in the ratio of shared to total references between optimized and unoptimized code suggests that performance studies of multiprocessor programs must be based on optimized code. 3 Analyzing Sharing Data miss rates in large uniprocessor caches tend to vary predictably as cache blocks increase in size <ref> [11, 27, 28] </ref>. Initially, the miss rate drops quickly as the block size increases; for large blocks, around 32 words, the curve flattens out; eventually, there is a slight reversal of the curve because of misses resulting from conflicts. <p> As the block size increases, a miss produces a higher volume of traffic. If we estimate the traffic caused by shared data as SharedMisses*BlockSize, we produce the plots in Figure 10. The figure includes a curve for uniprocessor data with a finite cache (32 Kbytes) from <ref> [11] </ref> for comparison purposes. From the figure, we see that the block size that minimizes the traffic of shared data in this class of applications is one word, both for 16 and 32 processors.
Reference: [12] <author> A. J. Smith, </author> <title> "Cache Memories," </title> <booktitle> in Computing Surveys, </booktitle> <pages> pp. 473-530, </pages> <month> September </month> <year> 1982. </year>
Reference-contexts: False sharing usually increases with the block size and tends to drive miss rates up with increasing block size. The second component, spatial locality in the data <ref> [12] </ref>, is the property that indicates that the probability of an access to a given memory word is high if neighboring words have been recently accessed. This well-known property produces the opposite effect from false sharing | a reduction in the miss rate as the block size increases.
Reference: [13] <author> F. Irigoin and R. Triolet, </author> <title> "Supernode Partitioning," </title> <booktitle> in Proceedings of the 15th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pp. 319-329, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Further, we do not consider techniques that require changes to the assignment of computation to processors, as in loop interchange or loop tiling <ref> [13, 14] </ref>, since they are only feasible in highly regular codes. Instead, we propose simple, local techniques that optimize the layout of shared data at the cache block level. These techniques are effective enough to eliminate, on average, about 10% of the misses on shared data in the applications.
Reference: [14] <author> M. E. Wolf and M. S. Lam, </author> <title> "A Data Locality Optimizing Algorithm," </title> <booktitle> in Proceedings ACM SIGPLAN 91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Further, we do not consider techniques that require changes to the assignment of computation to processors, as in loop interchange or loop tiling <ref> [13, 14] </ref>, since they are only feasible in highly regular codes. Instead, we propose simple, local techniques that optimize the layout of shared data at the cache block level. These techniques are effective enough to eliminate, on average, about 10% of the misses on shared data in the applications.
Reference: [15] <author> F. J. Carrasco, </author> <title> "A Parallel Maxflow Implementation." </title> <type> CS411 Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1988. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [16] <author> J. A. Dykstal and T. C. Mowry, </author> <title> "MINCUT: Graph Partitioning Using Parallel Simulated Annealing." </title> <type> CS411 Project Report, </type> <institution> Stanford University, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [17] <author> A. Galper, "DWF." </author> <title> CS411 Project Report, </title> <institution> Stanford University, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [18] <author> J. D. McDonald and D. Baganoff, </author> <title> "Vectorization of a Particle Simulation Method for Hypersonic Rarified Flow," </title> <booktitle> in AIAA Thermodynamics, Plasmadynamics and Lasers Conference, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [19] <author> J. Rose, "LocusRoute: </author> <title> A Parallel Global Router for Standard Cells," </title> <booktitle> in Proceedings of the 25th ACM/IEEE Design Automation Conference, </booktitle> <pages> pp. 189-195, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [20] <author> L. Soule and A. Gupta, </author> <title> "Characterization of Parallelism and Deadlocks in Distributed Digital Logic Simulation," </title> <booktitle> in Proceedings of the 26th ACM/IEEE Design Automation Conference, </booktitle> <pages> 33 pp. 81-86, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: This section describes the characteristics of the applications used, presents the simulator models, and evaluates the effect of conventional code optimizations on the frequency of data sharing. 2.1 Application Set and Trace Characteristics The parallel applications studied represent a variety of engineering algorithms <ref> [15, 16, 17, 18, 19, 20] </ref> (Table 1). Csim, Mp3d, and LocusRoute are research tools with between 1000 and 6000 lines of code. The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each.
Reference: [21] <editor> E. Lusk, R. Overbeek, et al., </editor> <title> Portable Programs for Parallel Processors. </title> <publisher> Holt, Rinehart, and Winston, Inc., </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: The other three applications, namely DWF, Maxflow, and Mincut implement several commonly used parallel algorithms and are less than 1000 lines of code each. Each application uses the synchronization and sharing primitives provided by the Argonne National Laboratory macro package <ref> [21] </ref>. The synchronization primitives are locks, barriers, and distributed loop control variables. The applications are in C and written so that they can run on any number of processors. We use code compiled with standard code optimizations. 2 Table 1: Application set characteristics.
Reference: [22] <author> H. Davis, S. Goldschmidt, and J. Hennessy, </author> <title> "Multiprocessing Simulation and Tracing Using Tango," </title> <booktitle> in Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pp. 99-107, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: logic gate simulator. 2.83 DWF Performs string pattern matching. 2.10 Mp3d 3-D particle simulator used in aeronautics. 1.85 LocusRoute Global router for VLSI standard cells. 1.84 Maxflow Determines the maximum flow in a directed graph. 0.26 Mincut Partitions a graph using simulated annealing. 0.01 We trace the applications using Tango <ref> [22] </ref>, a tracing program that simulates a multiprocessor. The traces correspond to 16 and 32 processor runs of the applications. They contain only application virtual address references and range in size from 8 to over 32 million data references.
Reference: [23] <author> F. Baskett, T. Jermoluk, and D. Solomon, </author> <title> "The 4D-MP Graphics Superworkstation: Computing + Graphics = 40 MIPS + 40 MFLOPS and 100,000 Lighted Polygons per Second," </title> <booktitle> in Proceedings of the 33rd IEEE Computer Society International Conference - COMPCON 88, </booktitle> <pages> pp. 468-471, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: We use the ideal architecture to remove dependencies on specific architecture characteristics from our study of shared data. The detailed architecture, used to determine the practical implications of the ideal study, resembles the Silicon Graphics POWER Station 4D/240 <ref> [23] </ref> in memory system bandwidth and latency. Unlike the 4D/240 system, however, the detailed architecture has 16 processors, each of which has one 256 Kbyte direct-mapped data cache. In addition, synchronization accesses use the same bus as regular transactions.
Reference: [24] <author> M. S. Papamarcos and J. H. Patel, </author> <title> "A Low Overhead Coherence Solution for Multiprocessors with Private Cache Memories," </title> <booktitle> in Proceedings of the 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 348-354, </pages> <month> June </month> <year> 1984. </year>
Reference-contexts: Because bus contention would be too high with 32 processors, the detailed architecture is used for 16 processor runs only. Both architectures use the invalidation-based Illinois cache coherence protocol <ref> [24] </ref>.
Reference: [25] <author> J. L. Hennessy and D. A. Patterson, </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: request for ownership on a shared block has the same timing and traffic requirements as a cache miss, we do not distinguish between the two in this paper. 3 2.3 Effect of an Optimizing Compiler on the Frequency of Sharing While code optimizations are known to speed up uniprocessor applications <ref> [25] </ref>, they have an important second effect in multiprocessor code: they increase the frequency of shared data references. This results from the different ways in which optimizations affect data.
Reference: [26] <author> G. Kane, </author> <title> MIPS RISC Architecture. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: To study the effect of an optimizing compiler, we measure, before and after compiler optimization, the fraction of references to data declared shared. The target architecture is the MIPS R2000 processor <ref> [26] </ref>, which has 32 integer registers and 16 double-precision floating point registers. The optimizations applied include global register allocation and other conventional global optimizations. All data in the shared space is declared volatile, and therefore are not register-allocated or optimized.
Reference: [27] <author> M. D. Hill, </author> <title> "Aspects of Cache Memory and Instruction Buffer Performance," </title> <type> Tech. Rep. </type> <institution> UCB/CSD 87/381, University of California, Berkeley, </institution> <month> November </month> <year> 1987. </year>
Reference-contexts: The large difference in the ratio of shared to total references between optimized and unoptimized code suggests that performance studies of multiprocessor programs must be based on optimized code. 3 Analyzing Sharing Data miss rates in large uniprocessor caches tend to vary predictably as cache blocks increase in size <ref> [11, 27, 28] </ref>. Initially, the miss rate drops quickly as the block size increases; for large blocks, around 32 words, the curve flattens out; eventually, there is a slight reversal of the curve because of misses resulting from conflicts.
Reference: [28] <author> S. Przybylski, M. Horowitz, and J. Hennessy, </author> <title> "Performance Tradeoffs in Cache Design," </title> <booktitle> in Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 290-298, </pages> <month> May </month> <year> 1988. </year> <month> 34 </month>
Reference-contexts: The large difference in the ratio of shared to total references between optimized and unoptimized code suggests that performance studies of multiprocessor programs must be based on optimized code. 3 Analyzing Sharing Data miss rates in large uniprocessor caches tend to vary predictably as cache blocks increase in size <ref> [11, 27, 28] </ref>. Initially, the miss rate drops quickly as the block size increases; for large blocks, around 32 words, the curve flattens out; eventually, there is a slight reversal of the curve because of misses resulting from conflicts.
References-found: 28

