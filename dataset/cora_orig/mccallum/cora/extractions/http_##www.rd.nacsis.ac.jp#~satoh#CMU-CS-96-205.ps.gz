URL: http://www.rd.nacsis.ac.jp/~satoh/CMU-CS-96-205.ps.gz
Refering-URL: http://www.rd.nacsis.ac.jp/~satoh/nameit.html
Root-URL: 
Title: Name-It: Association of Face and Name in Video  
Author: Shin'ichi Satoh Takeo Kanade 
Note: This material is based upon work supported by the National Science Foundation under Cooperative Agreement No. IRI-9411299. Any opinions, findings, and conclusions or recommendations expressed in this material are those of the author(s) and do not necessarily reflect the views of the National Science Foundation.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: December 20, 1996  
Pubnum: CMU-CS-96-205  
Abstract: This paper proposes a novel approach to extract meaningful content information from video by collaborative integration of image understanding and natural language processing. As an actual example, we developed a system that associates faces and names in videos, called Name-It, which is given news videos as a knowledge source, then automatically extracts face and name association as content information. The system can infer the name of a given unknown face image, or guess faces which are likely to have the name given to the system. This paper explains the method with several successful matching results which reveal effectiveness in integrating heterogeneous techniques as well as the importance of real content information extraction from video, especially face-name association. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Flickner, H. Sawhney, W. Niblack, J. Ashley, Q. Huand, B. Dom, M. Gorkani, J. Hafner, D. Lee, D. Petkovic, D. Steele, and P. Yanker, </author> <title> "Query by image and video content: </title> <booktitle> The QBIC system," IEEE Computer, </booktitle> <pages> pp. 23-32, </pages> <month> September </month> <year> 1995. </year>
Reference-contexts: Many research efforts are made to achieve this goal. They include image retrieval based on image features including color histogram and texture analysis <ref> [1] </ref>, feature extraction using Karhunen-Loeve (K-L) transform [2], and video structuring based on scene change detection [3].
Reference: [2] <author> A. Pentland, R. W. Picard, and S. Schlaroff, "Photobook: </author> <title> Content-based manipulation of image databases," </title> <journal> International Journal of Computer Vision, </journal> <volume> vol. 18, no. 3, </volume> <pages> pp. 233-254, </pages> <year> 1996. </year>
Reference-contexts: Many research efforts are made to achieve this goal. They include image retrieval based on image features including color histogram and texture analysis [1], feature extraction using Karhunen-Loeve (K-L) transform <ref> [2] </ref>, and video structuring based on scene change detection [3]. These approaches provide successful results to some extent, e.g., by using an image retrieval based on color histograms, a forest image may match well with images having trees, a sea side image may match well with marine images, etc. <p> Finally experimental results are shown to evaluate this method. 2 Related Work Face detection/identification has been researched for a long time, and there are image database systems which can perform face similarity matching; they include MIT Photobook <ref> [2] </ref> and Virage system. These two systems use the eigenvector based method for face similarity matching [4]. It is noteworthy that Photobook applied their method to more than 7,500 images of about 3,000 people and got successful results.
Reference: [3] <author> H. Zhang, C. Low, and S. Smoliar, </author> <title> "Video parsing and indexing of compressed data," </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> vol. 1, </volume> <pages> pp. 89-111, </pages> <year> 1995. </year>
Reference-contexts: Many research efforts are made to achieve this goal. They include image retrieval based on image features including color histogram and texture analysis [1], feature extraction using Karhunen-Loeve (K-L) transform [2], and video structuring based on scene change detection <ref> [3] </ref>. These approaches provide successful results to some extent, e.g., by using an image retrieval based on color histograms, a forest image may match well with images having trees, a sea side image may match well with marine images, etc.
Reference: [4] <author> M. Turk and A. Pentland, </author> <title> "Eigenfaces for recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 71-86, </pages> <year> 1991. </year>
Reference-contexts: These two systems use the eigenvector based method for face similarity matching <ref> [4] </ref>. It is noteworthy that Photobook applied their method to more than 7,500 images of about 3,000 people and got successful results. The results reveal that eigenvector based face similarity matching works well to some extent. Piction system [5] identifies faces within given captioned photos, typically of newspapers. <p> This is based on the well known eigenface method <ref> [4] </ref>. Using this distance, we defined a similarity between two faces. Let face images be two-dimensional N by N arrays of intensity values. These images also can be regarded as vectors of N 2 -dimension.
Reference: [5] <author> R. Chopra and R. K. Srihari, </author> <title> "Control structures for incorporating picture | specific context in image interpretation," </title> <booktitle> in Proceedings of IJCAI '95, </booktitle> <year> 1995. </year>
Reference-contexts: It is noteworthy that Photobook applied their method to more than 7,500 images of about 3,000 people and got successful results. The results reveal that eigenvector based face similarity matching works well to some extent. Piction system <ref> [5] </ref> identifies faces within given captioned photos, typically of newspapers. The system extracts faces from a photo and analyzes captions to get geometric constraints among faces which will appear in the photo, then label each face as each name.
Reference: [6] <author> M. Smith and T. Kanade, </author> <title> "Video skimming for quick browsing based on audio and image characterization," </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-186, School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: They are still far from complete, but it may be promising to properly integrate those techniques to get useful results. We took this strategy to bring face and name association to fruition. We used intensity histogram difference based scene change detection <ref> [6] </ref>, neural-network based face detection, eigenvector method based face identification, and dictionary based name extraction, despite these being still incomplete. <p> This detector can locate front view faces of various sizes with some margin in rotation, though, it takes about 10 seconds with a workstation (MIPS R4400 200MHz) to process a 352 fi 240 image. Thus we first apply an intensity histogram based scene change detector <ref> [6] </ref> to the video to obtain scene change images, then apply the face detector to scene change images. The overall face extraction process is shown as Figure 4. For example, we processed 4.5 hours news video, including about 490,000 frames (30 fps), and obtained 4,318 scene changes.
Reference: [7] <author> H. Rowley, S. Baluja, and T. Kanade, </author> <title> "Human face detection in visual scenes," </title> <type> Tech. Rep. </type> <institution> CMU-CS-95-158, School of Computer Science, Carnegie Mellon University, </institution> <year> 1995. </year>
Reference-contexts: Then we integrate these techniques to collaboratively use image sequences and transcript information of video footage. 4 Preparations 4.1 Face Extraction First we have to extract faces from a given video footage. We use a neural-network 5 based face detector to locate faces in images <ref> [7] </ref>. This detector can locate front view faces of various sizes with some margin in rotation, though, it takes about 10 seconds with a workstation (MIPS R4400 200MHz) to process a 352 fi 240 image.
Reference: [8] <author> A. Guttman, "R-TREES: </author> <title> A dynamic index structure for spatial searching," </title> <booktitle> in SIGMOD, </booktitle> <pages> pp. 47-57, </pages> <year> 1984. </year> <month> 17 </month>
Reference-contexts: To achieve this retrieval, spatial data structure methods like R-tree <ref> [8] </ref> can be used. Roughly speaking, these methods compose tree structures from given M points or (hyper-)rectangles in multidimensional space, and provide spatial retrieval, i.e., they enumerate all data which overlap the given (hyper-)rectangle, with O (log M ) computation.
References-found: 8

