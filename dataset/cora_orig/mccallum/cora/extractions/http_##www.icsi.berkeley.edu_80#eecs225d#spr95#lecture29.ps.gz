URL: http://www.icsi.berkeley.edu:80/eecs225d/spr95/lecture29.ps.gz
Refering-URL: http://www.icsi.berkeley.edu:80/eecs225d/spr95/index.html
Root-URL: http://www.icsi.berkeley.edu
Title: Introduction to Pattern Recognition Feature Extraction As Figure 29.2 shows, feature extraction for ASR starts
Author: Lecturer: Prof. Nelson Morgan Scribe: William Chen - 
Keyword: Feature Extraction (Statistical) Pattern Recognition Sequence Recognition Complete Systems  
Date: Spring 1995  29: April 3, 1995  
Note: EECS225d Audio Signal Processing in Humans and Machines  Lecture  29.2  29.3  
Abstract: 29.1 Where we are now So far, we have covered the basics of audio signal processing. The first portion of the course was devoted to understanding how humans produce and perceive speech. Understanding how nature does this is a science that can lead to useful models and stimulate new technology. The second portion of the course is to apply this knowledge to making machines synthesize and recognize speech. Machine synthesis of speech, or synthesizers, were covered in lectures 23-27 and described the vocoder and glovetalkII systems. This lecture begins machine recognition of speech, or ASR, which will cover the following areas: Ultimately, we want to understand how these different ideas are used in current automatic speech recognition systems. The problem of Automatic Speech Recognition actually belongs to a more general class of problems, known as pattern recognition. The idea is that given some data, how can we extract the relevant information and then classify it [DH73]. The classical pattern recognition view of speech is shown in Figure 29.1. Speech is corrupted by additive and convolutional noise. The role of the feature extractor is to clean the speech up and then represent it with a small number of variables that are useful for discriminating between them. The variables form a feature vector, and the vector can be thought of as embedded in a euclidean space. The classifier then divides this space up according to some criterion and determines where each incoming feature vector belongs. 
Abstract-found: 1
Intro-found: 0
Reference: [DH73] <author> Duda and Hart, </author> <title> Pattern Recognition and Scene Analysis, </title> <publisher> Wiley-Interscience, </publisher> <year> 1973. </year>
Reference-contexts: 29.2 Introduction to Pattern Recognition The problem of Automatic Speech Recognition actually belongs to a more general class of problems, known as pattern recognition. The idea is that given some data, how can we extract the relevant information and then classify it <ref> [DH73] </ref>. The classical pattern recognition view of speech is shown in Figure 29.1. Speech is corrupted by additive and convolutional noise. The role of the feature extractor is to clean the speech up and then represent it with a small number of variables that are useful for discriminating between them.
Reference: [F91] <author> S. Furui, </author> <title> "Speaker-Independent Isolated Word Recognition Based on Emphasized Spectral Dynamics", </title> <booktitle> Proceedings of ICASSP '86, </booktitle> <pages> pp. 1991-1994, </pages> <month> April, </month> <year> 1986. </year>
Reference-contexts: The next few methods show simple ways of providing extra robustness to the features. 29.6.1 Delta Cepstrum Delta Cepstrum looks at the local dynamics of speech <ref> [F91] </ref>. The notion here is that sound perception depends on spectral difference. So the Delta Cepstrum calculates the spectral difference by looking at the current and previous segments of speech to compute the time derivatives of the cepstrum.
Reference: [H54] <author> J. Makhoul, </author> <title> "Linear Prediction: A Tutorial Review", </title> <booktitle> Proceedings of the IEEE, </booktitle> <pages> pp. 561-580, </pages> <month> April, </month> <year> 1975. </year>
Reference: [H90] <author> H. Hermansky, </author> <title> "Perceptual Linear Predictive Analysis of Speech", </title> <journal> The Journal of the Acoustical Society of America, </journal> <pages> pp. 1738-1752, </pages> <month> April, </month> <year> 1990. </year>
Reference-contexts: Advantages: a) Typically less computation than a filter bank or FFT. b) Converts to cepstral coefficients which are roughly orthogonal c) Includes speaker characteristics good for speaker ID. 29.5 Perceptually based Feature Extraction Both Mel Cepstrum and PLP are very similar <ref> [H90] </ref>.
Reference: [HM94] <author> H. Hermansky and N. Morgan, </author> <title> "RASTA Processing of Speech", </title> <journal> IEEE Transactions on Speech and Audio Processing, pp. </journal> <volume> 578-589, vol. 2, no. 4, </volume> <month> October, </month> <year> 1994. </year>
Reference-contexts: RASTA is the online solution to CMS, since CMS is not feasible for real-time implementations <ref> [HM94] </ref>. The procedures for RASTA are shown below in Figure 29.4. 29.6.3 Comparison of RASTA, CMS, and Delta Cepstrum Actually all three procedures are pretty similar. In fact RASTA and Delta Cepstrum are both implemented in a similar fashion as described in Figure 29.4. Note the bandpass filters.
Reference: [SR74] <author> R. Schafer and L. Rabiner, </author> <title> "Digital Representations of Speech Signals", </title> <booktitle> Readings in Speech Recognition, Waibel and Lee, </booktitle> <pages> pp. 49-64, </pages> <year> 1960. </year> <note> 29-6 Lecture 29: April 3, 1995 Lecture 29: April 3, 1995 29-7 29-8 Lecture 29: April 3, 1995 Lecture 29: April 3, 1995 29-9 29-10 Lecture 29: </note> <month> April 3, </month> <year> 1995 </year>
Reference-contexts: vector: * Filter Banks * LPC * Mel Cepstrum and PLP * Delta Cepstrum * Robustness - Cepstral Mean Subtraction and RASTA * Radical Front Ends Auditory Models 29.4 Standard Feature Extraction The first two methods of feature extraction incorporate typical signal and stochastic processing techniques to a speech segment <ref> [SR74] </ref> [M75]. 29.4.1 Filter Banks A filter bank front end divides up a speech segment according to frequency content.
References-found: 6

