URL: http://www.cse.ucsc.edu/~stiliadi/selective.ps
Refering-URL: http://www.cse.ucsc.edu/~stiliadi/projects.html
Root-URL: http://www.cse.ucsc.edu
Title: Selective Victim Caching: A Method to Improve the Performance of Direct Mapped Caches  
Author: Dimitrios Stiliadis Anujan Varma 
Address: Santa Cruz, CA 95064  
Affiliation: Computer Engineering Department University of California  
Abstract: Victim caching was proposed by Jouppi [4] as an approach to improve the miss rate of direct-mapped caches. This approach augments the direct-mapped main cache with a small fully-associate cache, called victim cache, that stores cache blocks evicted from the main cache as a result of replacements. We propose and evaluate an improvement of this scheme, called selective victim caching. In this scheme, incoming blocks into the first-level cache are placed selectively in the main cache or the victim cache by the use of a prediction scheme based on their past history of use. In addition, interchanges of blocks between the main cache and the victim cache are also performed selectively. We show that the scheme results in significant improvements in miss rate as well as the number of interchanges between the two caches, for both small and large caches (4Kbytes - 128 Kbytes). For example, simulations with four instruction traces from the SPEC Release 1 programs showed an average improvement of approximately 20% in miss rate over simple victim caching for a 16K cache with a block size of 32 bytes; the number of blocks interchanged between the main and victim caches reduced by approximately 74%. Implementation of the scheme in an on-chip processor cache is described. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> DECchip 21064-AA Microprocessor Hardware Reference Manual, Digital Equipment Corporation, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: In addition, as the processor cycle-time shrinks, single-cycle access to the cache can often be provided only by a direct-mapped configuration. Indeed, both the instruction and data caches in the DEC Alpha 21064 chip are implemented as direct-mapped to satisfy the 5 ns processor-cycle time <ref> [1] </ref>. The conflicts between memory references in a direct-mapped cache often result in significant increases in miss rate over set-associative caches, particularly when the cache size is small.
Reference: [2] <author> M. D. Hill. </author> <title> Aspects of cache memory and instruction buffer performance, </title> <type> PhD Dissertation, </type> <institution> Computer Science Department, Univ. of Califor-nia, Berkeley, </institution> <month> November </month> <year> 1987. </year>
Reference: [3] <author> M, D. Hill, </author> <title> "A case for direct-mapped caches," </title> <booktitle> IEEE Computer, </booktitle> <month> December </month> <year> 1988, </year> <pages> pp. 25-40. </pages>
Reference-contexts: Realizing the performance potential of these processors requires innovations in the cache memory subsystem. Most of the current-generation single-chip microprocessors employ on-chip L1 (first-level) caches to provide fast access to instructions and data. The design of these on-chips caches involves a fundamental tradeoff between miss-rate and access time <ref> [3, 6] </ref>. A direct-mapped cache results in the lowest access time, but often suffers from high miss rates due to conflicts among memory references. Set-associative caches improve the miss rate at the expense of increasing the access time. <p> Set-associative caches improve the miss rate at the expense of increasing the access time. Hill argued that direct-mapped caches often afford better performance in terms of effective memory-access time over set-associative caches <ref> [3] </ref>. In addition, as the processor cycle-time shrinks, single-cycle access to the cache can often be provided only by a direct-mapped configuration. Indeed, both the instruction and data caches in the DEC Alpha 21064 chip are implemented as direct-mapped to satisfy the 5 ns processor-cycle time [1].
Reference: [4] <author> N.P. Jouppi. </author> <title> "Improving direct-mapped cache performance by the addition of a small fully-associative cache and prefetch buffers," </title> <booktitle> Proc. of the 17th Int. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990, </year> <pages> pp. 364-373, </pages>
Reference-contexts: The conflicts between memory references in a direct-mapped cache often result in significant increases in miss rate over set-associative caches, particularly when the cache size is small. This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting its hit time (cache access time) <ref> [4, 5] </ref>. One such method, known as victim caching, was proposed by Jouppi [4]. In this approach, the main direct-mapped cache is augmented with a small fully-associative cache that is used to store the "victims" of replacements from the main cache. <p> This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting its hit time (cache access time) [4, 5]. One such method, known as victim caching, was proposed by Jouppi <ref> [4] </ref>. In this approach, the main direct-mapped cache is augmented with a small fully-associative cache that is used to store the "victims" of replacements from the main cache. <p> Victim caching improves the effective memory access time by reducing the miss rate as seen by the second level of the memory hierarchy. This reduction in miss rate can be considerable for small caches <ref> [4] </ref>. caching scheme. As the size of the main cache is increased, however, the improvement decreases as a result of the small size of the victim cache.
Reference: [4] <author> J. R. Larus. </author> <title> "Efficient program tracing," </title> <booktitle> IEEE Computer, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 52-61. </pages>
Reference-contexts: The conflicts between memory references in a direct-mapped cache often result in significant increases in miss rate over set-associative caches, particularly when the cache size is small. This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting its hit time (cache access time) <ref> [4, 5] </ref>. One such method, known as victim caching, was proposed by Jouppi [4]. In this approach, the main direct-mapped cache is augmented with a small fully-associative cache that is used to store the "victims" of replacements from the main cache. <p> This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting its hit time (cache access time) [4, 5]. One such method, known as victim caching, was proposed by Jouppi <ref> [4] </ref>. In this approach, the main direct-mapped cache is augmented with a small fully-associative cache that is used to store the "victims" of replacements from the main cache. <p> Victim caching improves the effective memory access time by reducing the miss rate as seen by the second level of the memory hierarchy. This reduction in miss rate can be considerable for small caches <ref> [4] </ref>. caching scheme. As the size of the main cache is increased, however, the improvement decreases as a result of the small size of the victim cache.
Reference: [5] <author> S. McFarling. </author> <title> "Cache replacement with dynamic exclusion," </title> <booktitle> Proc. of the 19th Int. Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992, </year> <pages> pp. 192-200. </pages>
Reference-contexts: The conflicts between memory references in a direct-mapped cache often result in significant increases in miss rate over set-associative caches, particularly when the cache size is small. This motivated researchers to investigate techniques to reduce conflict-misses in a direct-mapped cache without affecting its hit time (cache access time) <ref> [4, 5] </ref>. One such method, known as victim caching, was proposed by Jouppi [4]. In this approach, the main direct-mapped cache is augmented with a small fully-associative cache that is used to store the "victims" of replacements from the main cache. <p> These state bits provide information on the history of the block the last time it was in the main cache. This idea was first proposed by McFarling, who used the history information to exclude certain blocks from a direct-mapped cache, reducing cyclic replacements involving the same block <ref> [5] </ref>. This scheme, called dynamic exclusion, reduces conflict misses in many cases. Its main drawback, however, is that a wrong prediction results in an access to the next level of the memory hierarchy, offsetting some of the performance gain. <p> This is the basic idea behind our scheme. The prediction algorithm is based on the dynamic exclusion algorithm proposed by McFarling <ref> [5] </ref>. The algorithm uses two state bits associated with every cache line, called the hit bit and the sticky bit. The hit bit is logically associated with a L1-cache block as it resides in L2-cache or main memory.
Reference: [6] <author> S. Przybylski, M. Horowitz, and J. Hennessy, </author> <title> "Performance tradeoffs in cache design," </title> <booktitle> Proc. of the 15th Annual Int. Symposium in Computer Architecture, </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 290-298. </pages>
Reference-contexts: Realizing the performance potential of these processors requires innovations in the cache memory subsystem. Most of the current-generation single-chip microprocessors employ on-chip L1 (first-level) caches to provide fast access to instructions and data. The design of these on-chips caches involves a fundamental tradeoff between miss-rate and access time <ref> [3, 6] </ref>. A direct-mapped cache results in the lowest access time, but often suffers from high miss rates due to conflicts among memory references. Set-associative caches improve the miss rate at the expense of increasing the access time.
Reference: [7] <author> A. J. Smith. </author> <title> "Cache memories," </title> <journal> Computing Surveys, </journal> <month> September </month> <year> 1982, </year> <pages> pp. 473-530. </pages>
Reference-contexts: These two factors together have resulted in a rapid increase in the cache-miss penalty in terms of the number of wasted instruction cycles. Influence of the memory hierarchy performance, of cache memory in particular, on program execution is therefore considerably stronger now than it was some time ago <ref> [7] </ref>. This trend is likely to This research is supported by NSF Young Investigator Award MIP-9257103, NSF Grant No. MIP-9111241. continue with the new generation of processors such as the DEC Alpha, Intel Pentium, etc. Realizing the performance potential of these processors requires innovations in the cache memory subsystem.
Reference: [8] <author> D. Stiliadis, A. Varma, </author> <title> "Selective Victim Caching: A Method to Improve the Performance of Direct-Mapped Caches",Tech. </title> <note> Report UCSC-CRL 93- 41,Oct 93. </note>

References-found: 9

