URL: http://ai.iit.nrc.ca/DEIL/cherkauer.ps.Z
Refering-URL: http://ai.iit.nrc.ca/DEIL/abstracts.html
Root-URL: 
Email: E-mail: fcherkaue,shavlikg@cs.wisc.edu  
Phone: Phone: 1-608-262-6613, Fax: 1-608-262-9777,  
Title: Presented at the IJCAI-95 Workshop on Data Engineering for Inductive Learning. Rapidly Estimating the Quality
Author: Kevin J. Cherkauer Jude W. Shavlik 
Keyword: Inductive Learning, Input Representation Selection/Quality Estimation, Empirical Evaluation  
Address: 1210 West Dayton Street, Madison WI 53706, U.S.A.  
Affiliation: Department of Computer Sciences, University of Wisconsin-Madison  
Abstract: The choice of an input representation for machine learning can have a profound impact on the accuracy of the learned model in classifying novel instances. A reliable method of rapidly estimating the value of a representation, independent of the learner, would be a powerful tool in the search for better representations. We introduce a fast representation-quality measure that is more accurate than Rendell and Ragavan's blurring metric in rank ordering input representations for neural networks on two difficult, real-world datasets. This work constitutes a step forward both in representation quality measures and in our understanding of the characteristics that engender good representations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. Almuallim and T.G. Dietterich. </author> <title> Learning boolean concepts in the presence of many irrelevant features. </title> <booktitle> Artif. Intell., </booktitle> <address> 69(1-2):279-305, </address> <year> 1994. </year>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Transparency is a heuristic for estimating representation quality based on the average complexity of the models a typical learner will build under a representation. Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. <ref> [1] </ref>). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. [11, 12, 13, 14, 21]). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
Reference: [2] <author> P.W. Baim. </author> <title> A method for attribute selection in inductive learning systems. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 10(6) </volume> <pages> 888-896, </pages> <year> 1988. </year>
Reference-contexts: Right: the corresponding X 2 statistic. Feature 1 0 1 Feature 2 0 a b X 2 = (a + c)(b + d)(a + b)(c + d) are used as feature-selection algorithms by others <ref> [2, 3] </ref>.) To obtain different representations, we ran RS again with different initial feature pools. The new pools were derived from the original pool by deleting features whose training-set information gains were greater than or equal to various thresholds.
Reference: [3] <author> R. Battiti. </author> <title> Using mutual information for selecting features in supervised neural net learning. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 5(4) </volume> <pages> 537-550, </pages> <year> 1994. </year>
Reference-contexts: Right: the corresponding X 2 statistic. Feature 1 0 1 Feature 2 0 a b X 2 = (a + c)(b + d)(a + b)(c + d) are used as feature-selection algorithms by others <ref> [2, 3] </ref>.) To obtain different representations, we ran RS again with different initial feature pools. The new pools were derived from the original pool by deleting features whose training-set information gains were greater than or equal to various thresholds.
Reference: [4] <author> R. Caruana and D. Freitag. </author> <title> Greedy attribute selection. </title> <booktitle> In Proc. 11th Intl. Conf. on Machine Learning, </booktitle> <pages> pages 28-36, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The most common technique describes each example by a vector of feature-value pairs. However, the particular features used are crucial to the learner's success. Given a large pool of candidate features, there are combi-natorially many subsets that could be chosen, motivating the need for automated selection methods <ref> [4, 5, 6] </ref>. To consider a large number of representations, we need a fast measure of representation quality. This is most important for computationally expensive learning systems such as backpropagation-trained artificial neural networks (ANNs) [19], where a full cross-validation learning experiment may use weeks of computer time. <p> Further work is needed to verify this supposition. John et al. [10] propose that representations should be scored on the basis of models constructed by the actual learning algorithm to be used for production runs, as is done, for example, by Caruana and Freitag <ref> [4] </ref> with ID3 and C4.5 [16]. They believe that learner-independent quality measures cannot be accurate enough. However, this approach is too costly to be applied effectively to the ANNs we use here.
Reference: [5] <author> K.J. Cherkauer and J.W. Shavlik. </author> <title> Protein structure prediction: Selecting salient features from large candidate pools. </title> <booktitle> In Proc. 1st Intl. Conf. on Intelligent Systems for Mol. Bio., </booktitle> <pages> pages 74-82, </pages> <address> Bethesda, MD, 1993. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: The most common technique describes each example by a vector of feature-value pairs. However, the particular features used are crucial to the learner's success. Given a large pool of candidate features, there are combi-natorially many subsets that could be chosen, motivating the need for automated selection methods <ref> [4, 5, 6] </ref>. To consider a large number of representations, we need a fast measure of representation quality. This is most important for computationally expensive learning systems such as backpropagation-trained artificial neural networks (ANNs) [19], where a full cross-validation learning experiment may use weeks of computer time.
Reference: [6] <author> K.J. Cherkauer and J.W. Shavlik. </author> <title> Selecting salient features for machine learning from large candidate pools through parallel decision-tree construction. </title> <editor> In H. Kitano and J.A. Hendler, editors, </editor> <booktitle> Massively Parallel Artificial Intelligence, </booktitle> <pages> pages 102-136. </pages> <publisher> AAAI Press/MIT Press, </publisher> <address> Menlo Park, CA/Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: The most common technique describes each example by a vector of feature-value pairs. However, the particular features used are crucial to the learner's success. Given a large pool of candidate features, there are combi-natorially many subsets that could be chosen, motivating the need for automated selection methods <ref> [4, 5, 6] </ref>. To consider a large number of representations, we need a fast measure of representation quality. This is most important for computationally expensive learning systems such as backpropagation-trained artificial neural networks (ANNs) [19], where a full cross-validation learning experiment may use weeks of computer time.
Reference: [7] <author> M.W. Craven and J.W. Shavlik. </author> <title> Learning to predict reading frames in E. coli DNA sequences. </title> <booktitle> In Proc. 26th Hawaii Intl. Conf. on System Sci., </booktitle> <pages> pages 773-782, </pages> <address> Wailea, HI, 1993. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The DNA data is from Craven and Shavlik <ref> [7] </ref> and consists of 20,000 examples from 30 "sufficiently independent" E. coli genes. Each example is a 61-base window from one gene, and the learner predicts which of six possible reading frames encodes the gene.
Reference: [8] <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proc. 13th Intl. Joint Conf. on Artif. Intell., </booktitle> <pages> pages 1022-1027, </pages> <address> Chambery, Savoie, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Fast representation evaluation is necessary to search many representations for expensive learners. Another issue is extending our transparency measures to continuously valued features. Fortunately, effective methods of discretizing continuous features for decision trees have already been developed <ref> [8, 16] </ref> and can be inserted directly into our measures. 7 Conclusions We discussed transparency as an important factor in representation quality and developed inexpensive, effective ways to measure it.
Reference: [9] <author> R. Holte, L. Acker, and B. Porter. </author> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proc. 11th Intl. Joint Conf. on Artif. Intell., </booktitle> <address> Detroit, MI, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: There are f-choose-d possible order-d feature interactions in a set of f features, so as interactions increase linearly, the search space containing such models grows combinatorially. Disjunctions partition the training data, so there are fewer examples available to learn each disjunct <ref> [9] </ref>, leading to less accurate models.
Reference: [10] <author> G.H. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <booktitle> In Proc. 11th Intl. Conf. on Machine Learning, </booktitle> <pages> pages 121-129, </pages> <address> New Brunswick, NJ, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This suggests that they measure something fundamental about representation quality for a perhaps broad class of learning systems rather than just artifacts of the particular models measured. Further work is needed to verify this supposition. John et al. <ref> [10] </ref> propose that representations should be scored on the basis of models constructed by the actual learning algorithm to be used for production runs, as is done, for example, by Caruana and Freitag [4] with ID3 and C4.5 [16]. They believe that learner-independent quality measures cannot be accurate enough.
Reference: [11] <author> K. Kira and L.A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proc. 10th Nat. Conf. on Artif. Intell., </booktitle> <pages> pages 129-134, </pages> <address> San Jose, CA, 1992. </address> <publisher> AAAI Press/MIT Press. </publisher>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. [1]). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. <ref> [11, 12, 13, 14, 21] </ref>). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. The idea that transparency itself has value (indeed, more value than economy) seldom appears. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
Reference: [12] <author> C.J. Matheus. </author> <title> Feature Construction: An Analytic Framework and an Application to Decision Trees. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <year> 1990. </year>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. [1]). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. <ref> [11, 12, 13, 14, 21] </ref>). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. The idea that transparency itself has value (indeed, more value than economy) seldom appears. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
Reference: [13] <author> G. Pagallo and D. Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. [1]). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. <ref> [11, 12, 13, 14, 21] </ref>). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. The idea that transparency itself has value (indeed, more value than economy) seldom appears. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
Reference: [14] <author> S. Piramuthu and H. Ragavan. </author> <title> Improving connectionist learning with symbolic feature construction. </title> <journal> Connection Sci., </journal> <volume> 4(1) </volume> <pages> 33-43, </pages> <year> 1992. </year>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. [1]). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. <ref> [11, 12, 13, 14, 21] </ref>). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. The idea that transparency itself has value (indeed, more value than economy) seldom appears. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
Reference: [15] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Instead it measures the average value, or information content, of a representation's individual features. This may often correlate with the level of feature interaction needed, but it does not directly measure it. Blurring is equivalent to the (negation of the) average information gain <ref> [15] </ref> of a representation's features with respect to a training set. (We show this in our Appendix.) 3.2 Model-Based Transparency Measures We now introduce a new class of "model-based" transparency measures that sample actual models of the target concept to make their estimates. <p> ID3leaves counts the number of leaves in the tree grown by Quinlan's <ref> [15] </ref> ID3 algorithm: ID3leaves = leaves (ID3) This is the most guided transparency metric we examine, since it bases its estimate on the most intelligently chosen model. 2 Measures based on conjunctive complexity (i.e. leaf depth, or "number of decisions") performed more poorly and so are not included here. 3 4 <p> This is a standard entropy measure, which Quinlan <ref> [15] </ref> calls I (p; n), p and n being R&R's p (y) and p (y), respectively.
Reference: [16] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1992. </year>
Reference-contexts: Further work is needed to verify this supposition. John et al. [10] propose that representations should be scored on the basis of models constructed by the actual learning algorithm to be used for production runs, as is done, for example, by Caruana and Freitag [4] with ID3 and C4.5 <ref> [16] </ref>. They believe that learner-independent quality measures cannot be accurate enough. However, this approach is too costly to be applied effectively to the ANNs we use here. <p> Fast representation evaluation is necessary to search many representations for expensive learners. Another issue is extending our transparency measures to continuously valued features. Fortunately, effective methods of discretizing continuous features for decision trees have already been developed <ref> [8, 16] </ref> and can be inserted directly into our measures. 7 Conclusions We discussed transparency as an important factor in representation quality and developed inexpensive, effective ways to measure it.
Reference: [17] <author> L. Rendell and H. Ragavan. </author> <title> Improving the design of induction methods by analyzing algorithm functionality and data-based concept complexity. </title> <booktitle> In Proc. 13th Intl. Joint Conf. on Artif. Intell., </booktitle> <pages> pages 952-958, </pages> <address> Chambery, Savoie, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this paper we introduce three such estimators, which we call transparency measures. We evaluate the measures empirically using two difficult, real-world datasets, demonstrating that they produce reliable estimates of ANN input representation quality. In addition, these estimates are better than those produced by Rendell and Ragavan's blurring measure <ref> [17] </ref>. 1 Acknowledgments: This work was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and <p> The idea that transparency itself has value (indeed, more value than economy) seldom appears. In the following sections we discuss several different transparency estimators. 2 3.1 Blurring as a Transparency Measure Rendell and Ragavan <ref> [17] </ref> do address the issue of transparency explicitly and present a method for quantifying it in a metric they call blurring. They claim that the less a representation requires the use of feature interactions to produce accurate models, the more transparent it is. <p> This work is a step forward in understanding representation quality and solving the problem of its rapid estimation. Appendix We show here that Rendell and Ragavan's ("R&R") blurring metric <ref> [17] </ref> is equivalent to the average information gain of a representation's features.
Reference: [18] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> New Jersey, </address> <year> 1989. </year>
Reference-contexts: What interests us is the average complexity of the accurate models possible under a representation. This is the representation's transparency, which estimates the complexity of the accurate models we expect typical learners to find. Transparency is related to minimum description length (MDL) <ref> [18] </ref> in that both have biases toward low complexity. MDL is a heuristic for estimating model quality ("smaller encodings are better"). Transparency is a heuristic for estimating representation quality based on the average complexity of the models a typical learner will build under a representation.
Reference: [19] <author> D.E. Rumelhart, G.E. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D.E. Rumelhart and J.L. McClelland, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-363. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: To consider a large number of representations, we need a fast measure of representation quality. This is most important for computationally expensive learning systems such as backpropagation-trained artificial neural networks (ANNs) <ref> [19] </ref>, where a full cross-validation learning experiment may use weeks of computer time. In such cases it is impossible to search many representations without an inexpensive quality estimator. In this paper we introduce three such estimators, which we call transparency measures.
Reference: [20] <author> R.S. Sutton and S.D. Whitehead. </author> <title> Online learning with random representations. </title> <booktitle> In Proc. 10th Intl. Conf. on Machine Learning, </booktitle> <pages> pages 314-321, </pages> <address> Amherst, MA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Moreover, having equalized coverage and economy across representations in Exper. 2, we believe transparency accounts for most of the observed differences in ANN test-set accuracy. 4.3 Experiment 3|Redundant Features ANNs can often use redundant features to advantage (e.g. see <ref> [20] </ref>), an ability generally not attributed to decision trees. Therefore, a transparency measure based on decision trees may be unable to accurately rank a pair of representations R 1 and R 2 to be used by an ANN, where R 2 is just R 1 plus some redundant features.
Reference: [21] <author> J. Wnek and R.S. Michalski. </author> <title> Hypothesis-driven constructive induction in AQ17-HCI: A method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14(2) </volume> <pages> 139-168, </pages> <year> 1994. </year> <month> 10 </month>
Reference-contexts: was supported by ONR Grant N00014-93-1-0998, NSF grant CDA-9024618 (CM-5 use), and a NASA Graduate Student Researcher Program Fellowship held by KJC. 2 Why Transparency? Other work in representation evaluation concentrates mainly on the ability to separate examples of different classes (which we call coverage) and on representation size (economy) <ref> [1, 11, 12, 13, 14, 21] </ref>, the implicit assumption being that these are all one needs to evaluate representation quality. A typical heuristic is, "If two representations have equal coverage, prefer the more economical (smaller) one." We believe this intuition is incomplete. <p> Some authors concerned with representation quality do not address transparency at all, and design feature-selection systems that may even minimize it (e.g. [1]). Others construct new features with the implicit goal of raising transparency, but do not attempt a formal definition of it (e.g. <ref> [11, 12, 13, 14, 21] </ref>). These methods likely do increase transparency, but the authors explain that this is done to (indirectly) reach an explicit goal of economy. The idea that transparency itself has value (indeed, more value than economy) seldom appears. <p> Comparing to Table 2, note that in every case expanding the representations increased the ANN accuracies, even though the added features were poorer than the original ones. This shows that smaller consistent feature sets are not necessarily better as is often claimed <ref> [1, 11, 12, 13, 14, 21] </ref>. Table 3 (right) compares the transparency measures in Exper. 2. Again, all measures performed perfectly on the NIST data. On the DNA data, the three model-based mea Table 3: Left: ground-truth ranking of the ten representations for each dataset in Exper. 2.
References-found: 21

