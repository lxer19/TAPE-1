URL: http://www.cs.nmsu.edu/lldap/download/jicslp/geyer.ps.gz
Refering-URL: http://www.cs.nmsu.edu/lldap/jicslp/gey.html
Root-URL: http://www.cs.nmsu.edu
Email: fEric.Morel, Jacques.Briat,Jacques.Chassin,Claudio.Geyerg@imag.fr  
Phone: Tl +33 76 51 46 63, +33 76 51 45 72, +33 76 51 45 75  
Title: Side-effects in PloSys OR-parallel Prolog on distributed memory machines  
Author: E. Morel, J. Briat, J. Chassin de Kergommeaux, C. Geyer 
Address: LMC-IMAG, B.P. 53, F-38041 Grenoble Cedex 9 France  
Affiliation: Projet CNRS-INRIA APACHE  
Abstract: The aim of the logic programming system PloSys is to offer most Prolog functionalities on distributed memory parallel computers as well as networks of personal computers. The PloSys computational model exploits OR-parallelism. It is designed to implement cut and side-effects while keeping low the number of messages exchanged between processors. It is a multi-sequential model based on copying. Scheduling is centralised as is the management of cut and side-effects. The scheduler keeps up to date the knowledge of the left-most worker in the scope of each cut. A PloSys prototype was implemented and its first results, on IBM SP-1, are promising. Keywords: Logic programming, OR-parallelism, Distributed Memory, Side-effects, Multi-sequential. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. A. M. Ali and R. Karlsson. </author> <title> The Muse Or-Parallel Prolog Model and its Performance. </title> <editor> In Saumya Debray and Manuel Hermenegildo, editors, </editor> <booktitle> Proc. of the 1990 North American Conference on Logic Programming. </booktitle> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Most of these problems are currently taken into account in the definition of a new version of Athapascan. Most benchmarks used in the past to measure OR-parallel logic systems (see for example <ref> [1] </ref>), cannot be used to test PloSys because of their "too" small size: for example, it only takes 110 milliseconds to compute the whole set of solutions to the classical eight queens problem, using wamcc running on a 133 Mhz Pentium processor.
Reference: [2] <author> K.A.M. Ali and R. Karlsson. </author> <title> Full prolog and scheduling or-parallelism in muse. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 19(2) </volume> <pages> 445-475, </pages> <month> december </month> <year> 1990. </year>
Reference-contexts: Most OR-parallel logic programming systems were implemented on shared-memory symmetric multiprocessors <ref> [2, 18] </ref>. Some prototypes were also implemented on distributed memory systems but they do not include the usual Prolog side-effects [3, 4, 15]. <p> Therefore, side-effects executed in such conditions should be "reversible" either by restoring the previous state of computation, or by performing the side-effect in some cache that will be cleared if the side effect is not to be validated. 3.1. Maintenance of the sequential order Solutions adopted in shared-memory systems <ref> [12, 2] </ref> cannot be used in PloSys. These solutions demand the exploration of the Prolog search tree to identify the left-most worker of the tree. This approach would require the use of a large number of messages in a distributed memory architecture. <p> This type of model makes it difficult to implement Prolog cut and side-effects. One solution was proposed to adapt the cavalier cut to this type of model [13]. Muse <ref> [2] </ref> is an OR-parallel logic programming system based as PloSys on copying. However, the Muse system assumes some shared memory between workers, to implement the sharing of choice points. This shared memory area is also used to manage Prolog cut and side-effects.
Reference: [3] <author> L. Araujo and J.J. Ruz. </author> <title> PDP: Prolog Distributed Processor for Independent ANDnOR Parallel Execution of Prolog. </title> <booktitle> In Proc. of ICLP'94, 11th International Conference on Logic Programming, </booktitle> <pages> pages 142-156. </pages> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Most OR-parallel logic programming systems were implemented on shared-memory symmetric multiprocessors [2, 18]. Some prototypes were also implemented on distributed memory systems but they do not include the usual Prolog side-effects <ref> [3, 4, 15] </ref>. The objective of PloSys is to build a "Prolog-like" OR-parallel logic programming system for distributed memory parallel architectures. "Prolog-like" means that PloSys will implement Prolog cut and side-effects.
Reference: [4] <author> V. Benjumea and J.M. Troya. </author> <title> An or parallel prolog model for distributed memory systems. </title> <booktitle> In Programming Language Implementation and Logic Programming, </booktitle> <publisher> LNCS 714. Springer Verlag, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: Most OR-parallel logic programming systems were implemented on shared-memory symmetric multiprocessors [2, 18]. Some prototypes were also implemented on distributed memory systems but they do not include the usual Prolog side-effects <ref> [3, 4, 15] </ref>. The objective of PloSys is to build a "Prolog-like" OR-parallel logic programming system for distributed memory parallel architectures. "Prolog-like" means that PloSys will implement Prolog cut and side-effects. <p> The most important difference with respect to OPERA is the management of side effects, integrated from the design of PloSys. Other differences with OPERA come from different design decisions which favor portability and modularity in PloSys, to make experimentations and evolutions easier. The OR-parallel system of the Malaga university <ref> [4] </ref> is also multi-sequential and implemented on a distributed memory computer. However it does not yet include any side-effect. Several computational models avoid inter-workers communications, where each worker recomputes the entire path leading from the root of the Prolog search tree to its branch.
Reference: [5] <author> J. Briat, M. Favre, C. Geyer, and J. Chassin de Kergommeaux. OPERA: </author> <title> OR-Parallel Prolog System on Supernode. In Kacsuk and Wise [15], </title> <booktitle> chapter 3, </booktitle> <pages> pages 45-63. </pages>
Reference-contexts: Each worker was designed so that the management of the cut and side-effects was kept separated from the inference engine. A PloSys prototype is currently being tested (see section 4.1.). 2.1. Computational model The PloSys computational model is inspired from OPERA <ref> [5] </ref>, without implementing some opti-misations tailored for the Supernode architecture used in OPERA. In PloSys, a Prolog program is executed in parallel by several workers, including each an inference engine as well as an exporter and an importer process. <p> The first unit is dedicated to the distribution of work. The process associated to the work distribution unit collects periodically the state of the workers and distribute work according to a scheduling policy. As in OPERA <ref> [5] </ref>, idle workers search work from "over-loaded" workers whose load is over some pre-defined threshold 2 . <p> Defining a new set of benchmarks, better suited to the performances of modern micro-processors, was undertaken jointly by LMC-IMAG and KFKI and is currently in progress. 10 1996 Compulog Net Meeting on Parallelism and Implementation Technology 5. Related work The PloSys system derives directly from the OPERA system <ref> [5] </ref>. Compared to OPERA, the computational model has been simplified since some OPERA optimisations, targeted to the Supernode parallel architecture used in OPERA, cannot be used in PloSys. This is the case in particular of the possibility for workers to carry on computing while exporting work.
Reference: [6] <author> M. Christaller. Athapascan-0a sur PVM3: </author> <title> definition et mode d'emploi. </title> <institution> Rapport APACHE 11, IMAG, Grenoble, </institution> <month> February </month> <year> 1994. </year> <note> Disponible sous ftp.imag.fr:imag/APACHE/RAPPORTS. </note>
Reference-contexts: A similar situation may occur when the output buffer space of some workers is full. 4. Implementation and preliminary results 4.1. Prolog engine and parallel programming environment A portable PloSys prototype was implemented using the wamcc [10] inference engine and the Atha-pascan <ref> [6] </ref> parallel programming library. The wamcc system includes a compiler which translates Prolog programs into Warren Abstract Machine (WAM) [23] instructions and then translates WAM instructions into C.
Reference: [7] <author> W.F. Clocksin and H. Alshawi. </author> <title> A Method for Efficiently Executing Horn Clause Programs Using Multiple Processors. </title> <journal> New Generation Computing, </journal> <volume> 5 </volume> <pages> 361-376, </pages> <year> 1988. </year>
Reference-contexts: Several computational models avoid inter-workers communications, where each worker recomputes the entire path leading from the root of the Prolog search tree to its branch. Branch allocation to workers is done either by the application of a tree exploration rule by each worker [17], or by a cen-tralised scheduler <ref> [7] </ref>. This type of model makes it difficult to implement Prolog cut and side-effects. One solution was proposed to adapt the cavalier cut to this type of model [13]. Muse [2] is an OR-parallel logic programming system based as PloSys on copying.
Reference: [8] <author> P. Codognet and D. Diaz. WAMCC: </author> <title> Compiling Prolog to C. </title> <booktitle> In Proc. of ICLP'95, 12th International Conference on Logic Programming, </booktitle> <address> Tokyo, Japan, 1995. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: portability: its performances are close to the performances of the compilers generating native code while it only requires a GCC compiler to be ported to a new target processor 4 . wamcc is about 1.6 times faster than emulated SICStus 2.1 and 1.7 times slower than native code SICStus 2.1 <ref> [8] </ref>. PloSys uses also the Athapascan parallel programming library developed within the APACHE research project. In Athapascan, a parallel application is executed by a set of identical virtual processors executing asynchronously.
Reference: [9] <author> J. Chassin de Kergommeaux and P. Codognet. </author> <title> Parallel logic programming systems. </title> <journal> ACM Computing Surveys, </journal> <volume> 26(3) </volume> <pages> 295-336, </pages> <month> september </month> <year> 1994. </year>
Reference-contexts: 1. Introduction Logic programming is one of the best suited paradigms to parallel processing and has given rise to a large number of parallel models of wich several were implemented efficiently <ref> [9] </ref>. By efficiently it is meant here that efficient parallel logic systems execute some programs more rapidly than the most efficient sequential Prolog systems and that, in the worst case (intrinsically sequential programs), parallel execution is not slower than sequential execution.
Reference: [10] <author> D. Diaz. </author> <title> wamcc prolog user's manual. </title> <institution> Rr inria, INRIA-Rocquencourt, Domaine de Voluceau, 78153 Le Chesnay, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: A similar situation may occur when the output buffer space of some workers is full. 4. Implementation and preliminary results 4.1. Prolog engine and parallel programming environment A portable PloSys prototype was implemented using the wamcc <ref> [10] </ref> inference engine and the Atha-pascan [6] parallel programming library. The wamcc system includes a compiler which translates Prolog programs into Warren Abstract Machine (WAM) [23] instructions and then translates WAM instructions into C.
Reference: [11] <author> D. Diaz and P. Codognet. </author> <title> A minimal extension of the wam for clp(fd). </title> <booktitle> In Proc. of ICLP'93, 10th International Conference on Logic Programming, </booktitle> <address> Budapest, Hungary, 1993. </address> <publisher> The MIT Press. </publisher>
Reference-contexts: Another foreseen extension is to combine constraint propagation techniques, already available in the inference engine wamcc <ref> [11] </ref> with OR-parallelism. Such a combination was already proven to be very effective [22]. Acknowledgements All Apache project research reports are available by ftp at ftp.imag.fr in the directory pub/APACHE/RAPPORTS. 1996 Compulog Net Meeting on Parallelism and Implementation Technology 11
Reference: [12] <author> B. Hausman. </author> <title> Pruning and Speculative Work in OR-Parallel Prolog. </title> <type> PhD thesis, </type> <institution> Royal Institute of Technology, Stockholm, Sweden, </institution> <year> 1990. </year> <note> SICS research report D-90-9001. </note>
Reference-contexts: Management of the cut One possible solution is to process sequentially all the branches originating from an OR-node, under the scope of a cut. To avoid the inhibition of parallelism caused by this solution, parallel execution of such "speculative" OR-nodes <ref> [12] </ref> is continued. When a cut is executed, speculative computations are undone; otherwise if the cut is not executed, these computations are validated. <p> Therefore, side-effects executed in such conditions should be "reversible" either by restoring the previous state of computation, or by performing the side-effect in some cache that will be cleared if the side effect is not to be validated. 3.1. Maintenance of the sequential order Solutions adopted in shared-memory systems <ref> [12, 2] </ref> cannot be used in PloSys. These solutions demand the exploration of the Prolog search tree to identify the left-most worker of the tree. This approach would require the use of a large number of messages in a distributed memory architecture. <p> Last, if a worker receives a validation message, it must update the cut levels stored in its choice points as well as its global cut level. It is possible to avoid speculative work by using a "cavalier cut" <ref> [12] </ref> whose execution prunes all other solutions of a predicate. However, interactions with other side effects need to be taken into 1996 Compulog Net Meeting on Parallelism and Implementation Technology 7 account to avoid mixing two different solution flows (see [19] for example). <p> We describe here the main solutions but there remains still a lot of experimental work to be performed. 8 1996 Compulog Net Meeting on Parallelism and Implementation Technology Cut: there exist several possibilities to take into account speculative work in the scheduling <ref> [12] </ref>. The first is to make no distinction between speculative and non speculative work. Scheduling is then easier but this solution may be inefficient if a large number of workers execute speculative work which proves later unuseful. <p> A lot of work was dedicated to the implementation of Prolog cut and side-effects in Aurora <ref> [18, 12] </ref> but all solutions assume the existence of a shared memory such as in a symmetric shared memory multiprocessor including a limited number of processors. The techniques developed for Aurora seem difficult to adapt to the type of parallel systems aimed by PloSys. 6.
Reference: [13] <author> P. Kacsuk. </author> <title> Or-parallel prolog on distributed memory systems. </title> <booktitle> In Proc. of PARLE'94. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: This type of model makes it difficult to implement Prolog cut and side-effects. One solution was proposed to adapt the cavalier cut to this type of model <ref> [13] </ref>. Muse [2] is an OR-parallel logic programming system based as PloSys on copying. However, the Muse system assumes some shared memory between workers, to implement the sharing of choice points. This shared memory area is also used to manage Prolog cut and side-effects.
Reference: [14] <author> P. Kacsuk. </author> <title> Wavefront scheduling in logflow. </title> <booktitle> In Proc. of the Euromicro Workshop on Parallel and Distributed Processing, </booktitle> <pages> pages 503-510. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: In the longer term, the prototype will be entirely distributed. The distributed scheduling algorithm will use some of the techniques developed in the LOGFLOW system <ref> [21, 14] </ref> for which there already exists an animation tool [20] that can be used to tune load-balancing strategies. Another foreseen extension is to combine constraint propagation techniques, already available in the inference engine wamcc [11] with OR-parallelism. Such a combination was already proven to be very effective [22].
Reference: [15] <author> Peter Kacsuk and Michael Wise, </author> <title> editors. Implementations of Distributed Prolog. </title> <publisher> John Wiley and Sons, </publisher> <year> 1992. </year> <booktitle> 12 1996 Compulog Net Meeting on Parallelism and Implementation Technology </booktitle>
Reference-contexts: Most OR-parallel logic programming systems were implemented on shared-memory symmetric multiprocessors [2, 18]. Some prototypes were also implemented on distributed memory systems but they do not include the usual Prolog side-effects <ref> [3, 4, 15] </ref>. The objective of PloSys is to build a "Prolog-like" OR-parallel logic programming system for distributed memory parallel architectures. "Prolog-like" means that PloSys will implement Prolog cut and side-effects.
Reference: [16] <author> S.E. Kannat, E. Morel, J.P. Kitajima, and J. Briat. </author> <title> A platform to study dynamic load balancing functions for parallel logic systems. </title> <booktitle> In IEEE First International Workshop on Parallel Processing, </booktitle> <address> Bangalore - India, </address> <year> 1994. </year>
Reference-contexts: Another research stream, not described in this paper, is concerned with the evaluation of scheduling strategies <ref> [16] </ref>. The organisation of this paper is the following. This introduction is followed by a general description of the PloSys system. The management of PloSys side-effects is described in the next section. The PloSys prototype is then outlined before a comparison with other OR-parallel logic programming systems. 2.
Reference: [17] <author> Z. Lin. </author> <title> A distributed fair polling scheme applied to or-parallel logic programming. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 20(4), </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Several computational models avoid inter-workers communications, where each worker recomputes the entire path leading from the root of the Prolog search tree to its branch. Branch allocation to workers is done either by the application of a tree exploration rule by each worker <ref> [17] </ref>, or by a cen-tralised scheduler [7]. This type of model makes it difficult to implement Prolog cut and side-effects. One solution was proposed to adapt the cavalier cut to this type of model [13]. Muse [2] is an OR-parallel logic programming system based as PloSys on copying.
Reference: [18] <editor> E. Lusk et al. </editor> <title> The Aurora Or-Parallel Prolog System. </title> <editor> In ICOT, editor, </editor> <booktitle> Proc. of the International Conference on Fifth Generation Computer Systems 1988. </booktitle> <publisher> ICOT, </publisher> <year> 1988. </year>
Reference-contexts: Most OR-parallel logic programming systems were implemented on shared-memory symmetric multiprocessors <ref> [2, 18] </ref>. Some prototypes were also implemented on distributed memory systems but they do not include the usual Prolog side-effects [3, 4, 15]. <p> A lot of work was dedicated to the implementation of Prolog cut and side-effects in Aurora <ref> [18, 12] </ref> but all solutions assume the existence of a shared memory such as in a symmetric shared memory multiprocessor including a limited number of processors. The techniques developed for Aurora seem difficult to adapt to the type of parallel systems aimed by PloSys. 6.
Reference: [19] <author> E.L. Lusk, S. Mudambi, Overbeek R, and P. </author> <title> Szeredi. Applications of the aurora parallel pro-log system to computational molecular biology. </title> <booktitle> In Proc. of the JICSLP'92 Post-Conference Joint Workshop on Distributed and Parallel Implementations of Logic Programming Systems, </booktitle> <address> Washington DC, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: However, interactions with other side effects need to be taken into 1996 Compulog Net Meeting on Parallelism and Implementation Technology 7 account to avoid mixing two different solution flows (see <ref> [19] </ref> for example). Another possibility is to prohibit the exportation of choice points under the scope of a cut (see a simple possible solution in section 3.5.). 3.4.
Reference: [20] <author> Zs. Nemeth and P. Kacsuk. </author> <title> Performance measuring facilities in logflow. </title> <booktitle> In Proc. of European School on Parallel Programming Environments, </booktitle> <address> ESPPE'96, Alpe d'Huez, </address> <year> 1996. </year>
Reference-contexts: In the longer term, the prototype will be entirely distributed. The distributed scheduling algorithm will use some of the techniques developed in the LOGFLOW system [21, 14] for which there already exists an animation tool <ref> [20] </ref> that can be used to tune load-balancing strategies. Another foreseen extension is to combine constraint propagation techniques, already available in the inference engine wamcc [11] with OR-parallelism. Such a combination was already proven to be very effective [22].
Reference: [21] <author> Zs. Puskas and P. Kacsuk. </author> <title> Distributed token scheduler for the ws-logflow. </title> <note> submitted to DAP-SYS'96. </note>
Reference-contexts: In the longer term, the prototype will be entirely distributed. The distributed scheduling algorithm will use some of the techniques developed in the LOGFLOW system <ref> [21, 14] </ref> for which there already exists an animation tool [20] that can be used to tune load-balancing strategies. Another foreseen extension is to combine constraint propagation techniques, already available in the inference engine wamcc [11] with OR-parallelism. Such a combination was already proven to be very effective [22].
Reference: [22] <author> P. van Hentenryck. </author> <title> Parallel Constraint Satisfaction in Logic Programming: Preliminary Results of Chip within PEPSys. </title> <booktitle> In Proc. of the Sixth International Conference on Logic Programming, </booktitle> <pages> pages 165-180. </pages> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Another foreseen extension is to combine constraint propagation techniques, already available in the inference engine wamcc [11] with OR-parallelism. Such a combination was already proven to be very effective <ref> [22] </ref>. Acknowledgements All Apache project research reports are available by ftp at ftp.imag.fr in the directory pub/APACHE/RAPPORTS. 1996 Compulog Net Meeting on Parallelism and Implementation Technology 11
Reference: [23] <author> D. H. D. Warren. </author> <title> An Abstract Prolog Instruction Set. </title> <type> Technical Report tn309, </type> <institution> SRI, </institution> <month> October </month> <year> 1983. </year>
Reference-contexts: The inference engine is a sequential Prolog inference engine, as efficient as possible and therefore based on an implementation of the Warren Abstract Machine (WAM) <ref> [23] </ref> which executes compiled Prolog programs. An idle worker obtains work from an active one by developing one or several unexplored choices originating from one or several choice points of this worker. <p> Implementation and preliminary results 4.1. Prolog engine and parallel programming environment A portable PloSys prototype was implemented using the wamcc [10] inference engine and the Atha-pascan [6] parallel programming library. The wamcc system includes a compiler which translates Prolog programs into Warren Abstract Machine (WAM) <ref> [23] </ref> instructions and then translates WAM instructions into C. The main interest of using wamcc is that the inference engine is not "too" big and can thus be "easily" modified, although the amount of modifications required to implement PloSys remains limited.
References-found: 23

