URL: http://www.cs.columbia.edu/~sal/hpapers/ieee98.ps
Refering-URL: http://www.cs.columbia.edu:80/~sal/recent-papers.html
Root-URL: 
Email: &lt;sal,wfan,wenke,andreas,sat@cs.columbia.edu&gt;  &lt;pkc@cs.fit.edu&gt;  
Phone: 212-939-7080  
Title: Agent-based Fraud and Intrusion Detection in Financial Information Systems  
Author: Salvatore J. Stolfo David W. Fan Andreas Prodromidis Wenke Lee and Shelley Tselepis Philip K. Chan 
Note: This research is supported by the Intrusion Detection Program (BAA9603) from DARPA (F30602 96-1-0311), NSF (IRI-96-32225 and CDA-96-25374) and NYSSTF (423115-445). Supported in part by IBM  
Date: November 24, 1997  
Address: New York, NY 10027  Melbourne, FL 32901  
Affiliation: Department of Computer Science Columbia University  Computer Science Florida Institute of Technology  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> P. Chan and S. </author> <title> Stolfo An Extensible Meta-learning Approach for Scalable and Accurate Inductive Learning Ph.D. </title> <type> Thesis, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, New York, </address> <year> 1997 </year>
Reference-contexts: Here, meta-learning serves as the means of "gluing" multiple knowledge sources together. Meta-learning <ref> [1] </ref> is defined as learning of meta-knowledge about learned knowledge. In our work we concentrate on learning from the output of concept learning systems. In this case meta-learning means learning from the predictions of a set of classifiers on common training data. <p> The following is a brief summary of meta-learning results that are pertinent to the fraud detection environment. A detailed result and discussion can be found in <ref> [1] </ref>. * The meta-learning strategies do show a consistent improvement in classification accuracy over any of the base classifiers trained on a subsets of available training data.
Reference: [2] <author> P. Chan and S. Stolfo. </author> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Info. Know. Manag., </booktitle> <pages> pages 314-323, </pages> <year> 1993. </year>
Reference-contexts: An arbiter [5] is learned by some learning algorithm to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. base classifiers and a single arbiter. In the combiner <ref> [2] </ref> strategy, the predictions of the learned base classifiers on the training set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of training examples for the meta-learner.
Reference: [3] <author> P. Chan and S. Stolfo. </author> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> In Proc. Second Intl. Work. on Multistrategy Learning, </booktitle> <pages> pages 150-165, </pages> <year> 1993. </year>
Reference: [4] <author> P. Chan and S. Stolfo. </author> <title> Toward multistrategy parallel and distributed learning in sequence analysis. </title> <booktitle> In Proc. First Intl. Conf. Intel. Sys. Mol. Biol., </booktitle> <pages> pages 65-73, </pages> <year> 1993. </year>
Reference: [5] <author> P. Chan and S. Stolfo. </author> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Know. Disc. Databases, </booktitle> <pages> pages 227-240, </pages> <year> 1993. </year>
Reference-contexts: Here 4 we briefly describe two strategies, the arbiter and combiner. An arbiter <ref> [5] </ref> is learned by some learning algorithm to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions. base classifiers and a single arbiter.
Reference: [6] <author> P. Chan and S. Stolfo. </author> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning, </booktitle> <pages> pages 90-98, </pages> <year> 1995. </year>
Reference: [7] <author> P. Chan and S. Stolfo. </author> <title> Learning arbiter and combiner trees from partitioned data for scaling machine learning. </title> <booktitle> In Proc. Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <pages> pages 39-44, </pages> <year> 1995. </year>
Reference: [8] <author> S. Stolfo, A. Prodromidis, S. Tselepsis, W. Lee, D. Fan and P. Chan JAM: </author> <title> Java Agents for Meta-learning over Distributed Databases. In Prod. </title> <booktitle> Third Intl. Conf. Knowledge Discovery and Data Mining, </booktitle> <year> 1997. </year>
Reference-contexts: An important feature here is that each bank only exchanges its locally computed classifiers, and not their local data. Thus, we are able to demonstrate the principle that it is possible to "share knowledge without disclosing data". 3 The JAM Architecture JAM <ref> [8] </ref> is architected as an agent based system, a distributed computing construct that is designed as an extension of OS environments. It is a distributed meta-learning system that supports the launching of learning and meta-learning agents to distributed database sites.
Reference: [9] <author> S. Stolfo, D. Fan, W. Lee, A. Prodromidis and P. </author> <title> Chan Credit Card Fraud Detection Using Meta-learning: Issues and Initial Results. </title> <booktitle> In Working Notes AAAI-97, </booktitle> <year> 1997 </year>
Reference-contexts: Other problems we addressed include finding the best distribution to use in training sets as well as metrics to select base classifiers to form the best meta-classifiers. The detailed discussion about these last two issues can be found in <ref> [9] </ref>. 10 Bearing these particular goals in mind, we have formed the following learning task. * Data of a full year are partitioned according to months 1 to 12. <p> We display the six best (meta-)classifiers. (A more detailed description of the full set of results can be found in <ref> [9] </ref> and at our project home page.) 11 Classifier Name True Positive Rate False Positive Rate BAYES as meta-learner combining 4 base classifiers (learned over 80% 13% 50%/50% distribution) with highest True Positive Rate RIPPER trained over 50%/50% distribution 80% 16% CART trained over 50%/50% distribution 80% 16% BAYES as meta-learner <p> The next 2 best classifiers are base classifiers: CART and RIPPER each trained on a 50%50% fraud/non-fraud distribution. The TP Rate of 3 base classifiers are identical, but the meta-classifier has the lowest FP Rate. Other findings (detailed in <ref> [9] </ref>) include that 50%/50% fraud distribution in training data will generate classifiers with highest TP rate and relatively low FP rate. This set of experiments provides some information about the relative performance of several inductive learning algorithms and meta-learning strategies.
Reference: [10] <author> W. </author> <title> Cohen Fast Effective Rule Induction. </title> <booktitle> In Twelveth Intl. Conf. on Machine Learning, </booktitle> <pages> pages 115-123, </pages> <year> 1995. </year>
Reference-contexts: Each is applied to every partition above. ID3 and CART is part of IND package [17]. BAYES is a naive Bayesian classifier. RIPPER <ref> [10] </ref> is a rule learning program obtained from William Cohen at ATT. * In this experiment, we have chosen to use the class-combiner meta-learning technique.
Reference: [11] <author> R. </author> <title> Quinlan Induction of Decision Trees. </title> <booktitle> In Machine Learning, </booktitle> <volume> 1 </volume> <pages> 81-106 </pages>
Reference-contexts: fraud/non-fraud in training data: 3 partitions by concatenating f with 2 randomly chosen nf i - 25%/75% fraud/non-fraud in training data: 2 partitions by concatenating f with 3 randomly chosen nf i - 20%/80% the original 42000 records sample data set * Four learning algorithms were available and used: ID3 <ref> [11] </ref>, CART [12], BAYES [13] and RIPPER. Each is applied to every partition above. ID3 and CART is part of IND package [17]. BAYES is a naive Bayesian classifier.
Reference: [12] <author> L. Breiman, J. Friedman, R. Olshen and C. Stone. </author> <title> Classification and Regression Tree, </title> <publisher> Wadsworth, </publisher> <address> Belmont, CA, </address> <year> 1984. </year>
Reference-contexts: training data: 3 partitions by concatenating f with 2 randomly chosen nf i - 25%/75% fraud/non-fraud in training data: 2 partitions by concatenating f with 3 randomly chosen nf i - 20%/80% the original 42000 records sample data set * Four learning algorithms were available and used: ID3 [11], CART <ref> [12] </ref>, BAYES [13] and RIPPER. Each is applied to every partition above. ID3 and CART is part of IND package [17]. BAYES is a naive Bayesian classifier.
Reference: [13] <author> P. Clark and T. Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-285, </pages> <year> 1989. </year>
Reference-contexts: 3 partitions by concatenating f with 2 randomly chosen nf i - 25%/75% fraud/non-fraud in training data: 2 partitions by concatenating f with 3 randomly chosen nf i - 20%/80% the original 42000 records sample data set * Four learning algorithms were available and used: ID3 [11], CART [12], BAYES <ref> [13] </ref> and RIPPER. Each is applied to every partition above. ID3 and CART is part of IND package [17]. BAYES is a naive Bayesian classifier.
Reference: [14] <author> T. M. Mitchell. </author> <title> The need for biases in learning generalizaions. </title> <type> Technical Report CBM-TR-117, </type> <institution> Dept. Comp. Sci., Rutgers Univ., </institution> <year> 1980. </year>
Reference: [15] <author> L. Xu, A. Krzyzak, and C. Suen. </author> <title> Methods of combining multiple classifires and their applications to handwriting recognition. </title> <journal> IEEE Trans. Sys. Man. Cyb., </journal> <volume> 22 </volume> <pages> 418-435, </pages> <year> 1992. </year>
Reference: [16] <author> K. Ali and M. Pazzani. </author> <title> Error Reduction through Learning Multiple Descriptions. </title> <booktitle> In Machine Learning, vol.24, </booktitle> <pages> pages 173-202. </pages>
Reference-contexts: Rate BAYES as meta-learner combining 4 base classifiers (learned over 80% 13% 50%/50% distribution) with highest True Positive Rate RIPPER trained over 50%/50% distribution 80% 16% CART trained over 50%/50% distribution 80% 16% BAYES as meta-learner combining 3 base classifiers (learned over 50%/50% 80% 17% distribution) with least correlated error <ref> [16] </ref> BAYES as meta-learner combining 4 classifiers learned over 50%/50% 76% 13% distribution with least correlated error rate ID3 trained over 50%/50% distribution 74% 23% Table 1: Results of Best Classifiers In Table 1, we show results of the few best performers.
Reference: [17] <author> W. Buntime and R. Caruana. </author> <title> Introduction to IND and Recursive Partitioning, </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Each is applied to every partition above. ID3 and CART is part of IND package <ref> [17] </ref>. BAYES is a naive Bayesian classifier. RIPPER [10] is a rule learning program obtained from William Cohen at ATT. * In this experiment, we have chosen to use the class-combiner meta-learning technique.

References-found: 17

