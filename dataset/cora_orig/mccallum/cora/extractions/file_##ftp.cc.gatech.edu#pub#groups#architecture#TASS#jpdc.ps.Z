URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/jpdc.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/TASS/tass.html
Root-URL: 
Title: A Simulation-based Scalability Study of Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Keyword: Key words: parallel systems, parallel kernels, scalability, execution-driven simulation, performance evaluation, performance metrics.  
Note: In Journal of Parallel and Distributed Computing, Vol  This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, GA 30332-0280.  
Affiliation: College of Computing Georgia Institute of Technology  
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136  
Date: 22(3):411-426, September 1994.  
Abstract: Scalability studies of parallel architectures have used scalar metrics to evaluate their performance. Very often, it is difficult to glean the sources of inefficiency resulting from the mismatch between the algorithmic and architectural requirements using such scalar metrics. Low-level performance studies of the hardware are also inadequate for predicting the scalability of the machine on real applications. We propose a top-down approach to scalability study that alleviates some of these problems. We characterize applications in terms of the frequently occurring kernels, and their interaction with the architecture in terms of overheads in the parallel system. An overhead function is associated with each artifact of the parallel system that limits its scalability. We present a simulation platform called SPASM (Simulator for Parallel Architectural Scalability Measurements) that quantifies these overhead functions. SPASM separates the algorithmic overhead into its components (such as serial and work-imbalance overheads), and interaction overhead into its components (such as latency and contention). Such a separation is novel and has not been addressed in any previous study using real applications. We illustrate the top-down approach by considering a case study in implementing three NAS parallel kernels on two simulated message-passing platforms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Studies undertaken by Kung [16] and Jamieson [13] help identify some of these characteristics from a theoretical perspective, but they do not provide any means of quantifying their effects. Several performance studies address issues such as latency, contention and synchronization. The limits on interconnection network performance <ref> [1, 21] </ref> and the scalability of synchronization primitives supported by the hardware [3, 19] are examples of such studies undertaken over the years.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to achieving Large Scale Computing Capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Performance metrics such as speedup <ref> [2] </ref>, scaled speedup [11], sizeup [28], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems. <p> Parallel system overheads may be broadly classified into a purely algorithmic component (algorithmic overhead), and a component arising from the interaction of the algorithm and the architecture (interaction overhead). The algorithmic overhead is due to the inherent serial part <ref> [2] </ref> and the work-imbalance in the algorithm.
Reference: [3] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Several performance studies address issues such as latency, contention and synchronization. The limits on interconnection network performance [1, 21] and the scalability of synchronization primitives supported by the hardware <ref> [3, 19] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: We illustrate the top-down approach through a case study, implementing three NAS parallel kernels <ref> [4] </ref> on two message-passing platforms (a bus and a binary hypercube) simulated on SPASM. The algorithmic characteristics of these kernels are discussed in Section 4, details of the two architectural platforms are presented in Section 5, and the results of our study are summarized in Section 6. <p> Despite its limitations, we believe that the scalability of an application with respect to an architecture can be captured by studying its kernels, since they represent the computationally intensive phases of an application. Therefore, we have used kernels in this study, in particular the NAS parallel kernels <ref> [4] </ref> that have been derived from a large number of Computational Fluid Dynamics applications. Parallel system overheads may be broadly classified into a purely algorithmic component (algorithmic overhead), and a component arising from the interaction of the algorithm and the architecture (interaction overhead). <p> On the other hand, if a shared memory programming style is used, the communication pattern is not explicit and gets merged with the data access pattern. The Numerical Aerodynamic Simulation (NAS) program at NASA Ames has identified a set of kernels <ref> [4] </ref> that are representative of a number of large scale Computational Fluid Dynamics codes. In this study, we consider three of these kernels for the purposes of illustrating the top-down approach using SPASM. In this section, we identify their characteristics in a message-passing style implementation.
Reference: [5] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: This strategy has considerably lowered the simulation time which is at most a factor of two compared to the real time for the applications considered. This strategy has also been used in other recent simulation studies <ref> [5, 7, 22] </ref>. The novel feature of our simulation study is the separation of overheads in a parallel system. Providing the functionality that is needed for such a separation requires a considerable amount of instrumentation. <p> The novel feature of our simulation study is the separation of overheads in a parallel system. Providing the functionality that is needed for such a separation requires a considerable amount of instrumentation. There exist simulators in the public domain (such as Proteus <ref> [5] </ref>) that provide certain basic functionality. <p> Hence, we have resorted to calculating these time quanta manually and introducing the appropriate instrumentation code in our source programs. These manual measurements may have contributed to the inaccuracies in the estimation. We propose to use the augmentation technique used in other similar simulation studies <ref> [5, 7] </ref> to overcome these inaccuracies. 7 Scalability Implications In this section we illustrate how the top-down approach, in particular the overhead functions defined by the simulator, can be used for drawing conclusions regarding the scalability of a parallel system.
Reference: [6] <author> D. Chen, H. Su, and P. Yew. </author> <title> The Impact of Synchronization and Granularity on Parallel Systems. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [10] and the impact of synchronization and task granularity on parallel 1 The term, parallel system, is used to denote an algorithm-architecture combination. 1 system performance <ref> [6] </ref>. Cypher et al. [9], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. [23] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [7] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: This strategy has considerably lowered the simulation time which is at most a factor of two compared to the real time for the applications considered. This strategy has also been used in other recent simulation studies <ref> [5, 7, 22] </ref>. The novel feature of our simulation study is the separation of overheads in a parallel system. Providing the functionality that is needed for such a separation requires a considerable amount of instrumentation. <p> Hence, we have resorted to calculating these time quanta manually and introducing the appropriate instrumentation code in our source programs. These manual measurements may have contributed to the inaccuracies in the estimation. We propose to use the augmentation technique used in other similar simulation studies <ref> [5, 7] </ref> to overcome these inaccuracies. 7 Scalability Implications In this section we illustrate how the top-down approach, in particular the overhead functions defined by the simulator, can be used for drawing conclusions regarding the scalability of a parallel system.
Reference: [8] <author> Z. Cvetanovic. </author> <title> The effects of problem partitioning, allocation, and granularity on the performance of multiple-processor systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 36(4) </volume> <pages> 421-432, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex 4 interactions between algorithms and architectures. For instance, models developed in <ref> [17, 29, 8] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [29] and [8] develop models for regular iterative algorithms. <p> For instance, models developed in [17, 29, 8] are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [29] and <ref> [8] </ref> develop models for regular iterative algorithms. However, there exist several applications [23] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication.
Reference: [9] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year> <month> 19 </month>
Reference-contexts: Cypher et al. <ref> [9] </ref>, identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. [23] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [10] <author> S. J. Eggers and R. H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, Massachusetts, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance <ref> [10] </ref> and the impact of synchronization and task granularity on parallel 1 The term, parallel system, is used to denote an algorithm-architecture combination. 1 system performance [6]. Cypher et al. [9], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications.
Reference: [11] <author> J. L. Gustafson, G. R. Montry, and R. E. Benner. </author> <title> Development of Parallel Methods for a 1024-node Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup <ref> [11] </ref>, sizeup [28], experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems.
Reference: [12] <institution> Intel Corporation, Oregon. Intel iPSC/2 and iPSC/860 User's Guide, </institution> <year> 1989. </year>
Reference-contexts: Both platforms provide an identical message-passing interface to the programmer. They support blocking and non-blocking modes of message transfer. The semantics of these modes are the same as those available on an iPSC/860 <ref> [12] </ref>. A blocking send blocks the sender until the message has left the sending buffer. Such a send does not necessarily imply that the message has reached the destination processor or even entered the network.
Reference: [13] <author> L. H. Jamieson. </author> <title> Characterizing Parallel Algorithms. </title> <editor> In L. H. Jamieson, D. B. Gannon, and R. J. Douglas, editors, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: An understanding of the interaction between the algorithmic and architectural characteristics of a parallel system can give us such information. Studies undertaken by Kung [16] and Jamieson <ref> [13] </ref> help identify some of these characteristics from a theoretical perspective, but they do not provide any means of quantifying their effects. Several performance studies address issues such as latency, contention and synchronization.
Reference: [14] <author> A. H. Karp and H. P. Flatt. </author> <title> Measuring Parallel processor Performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup [28], experimentally determined serial fraction <ref> [14] </ref>, and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide adequate information needed to understand the reason why an algorithm does not scale well on an architecture.
Reference: [15] <author> V. Kumar and V. N. Rao. </author> <title> Parallel Depth-First Search. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup [28], experimentally determined serial fraction [14], and isoefficiency function <ref> [15] </ref> have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide adequate information needed to understand the reason why an algorithm does not scale well on an architecture.
Reference: [16] <author> H. T. Kung. </author> <title> The Structure of Parallel Algorithms. </title> <booktitle> Advances in Computers, </booktitle> <volume> 19 </volume> <pages> 65-112, </pages> <year> 1980. </year> <title> Edited by Marshall C. </title> <publisher> Yovits and Published by Academic Press, </publisher> <address> New York. </address>
Reference-contexts: An understanding of the interaction between the algorithmic and architectural characteristics of a parallel system can give us such information. Studies undertaken by Kung <ref> [16] </ref> and Jamieson [13] help identify some of these characteristics from a theoretical perspective, but they do not provide any means of quantifying their effects. Several performance studies address issues such as latency, contention and synchronization.
Reference: [17] <author> S. Madala and J. B. Sinclair. </author> <title> Performance of Synchronous Parallel Algorithms with Regular Structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1) </volume> <pages> 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex 4 interactions between algorithms and architectures. For instance, models developed in <ref> [17, 29, 8] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [29] and [8] develop models for regular iterative algorithms. <p> These assumptions restrict their applicability for capturing complex 4 interactions between algorithms and architectures. For instance, models developed in [17, 29, 8] are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair <ref> [17] </ref> confine their studies to synchronous algorithms while [29] and [8] develop models for regular iterative algorithms. However, there exist several applications [23] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.
Reference: [18] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels : A Computer Test of the Numerical Performance Range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: Such abstractions of real applications that capture the main phases of the computation are called kernels. One can go even lower than kernels by abstracting the main loops in the computation (like the Lawrence Livermore loops <ref> [18] </ref>) and evaluating their performance. As one goes lower in the hierarchy, the outcome of the evaluation becomes less realistic.
Reference: [19] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: Several performance studies address issues such as latency, contention and synchronization. The limits on interconnection network performance [1, 21] and the scalability of synchronization primitives supported by the hardware <ref> [3, 19] </ref> are examples of such studies undertaken over the years. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [20] <author> J. H. Patel. </author> <title> Analysis of multiprocessors with private cache memories. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 31(4) </volume> <pages> 296-304, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models. Similarly, several other models make assumptions about architectural characteristics. For instance, the model developed in <ref> [20] </ref> ignores data inconsistency that can arise in a cache-based multiprocessor during the execution of an application and thus does not consider the coherence traffic on the network.
Reference: [21] <author> G. F. Pfister and V. A. Norton. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-34(10):943-948, </volume> <month> October </month> <year> 1985. </year> <month> 20 </month>
Reference-contexts: Studies undertaken by Kung [16] and Jamieson [13] help identify some of these characteristics from a theoretical perspective, but they do not provide any means of quantifying their effects. Several performance studies address issues such as latency, contention and synchronization. The limits on interconnection network performance <ref> [1, 21] </ref> and the scalability of synchronization primitives supported by the hardware [3, 19] are examples of such studies undertaken over the years.
Reference: [22] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: This strategy has considerably lowered the simulation time which is at most a factor of two compared to the real time for the applications considered. This strategy has also been used in other recent simulation studies <ref> [5, 7, 22] </ref>. The novel feature of our simulation study is the separation of overheads in a parallel system. Providing the functionality that is needed for such a separation requires a considerable amount of instrumentation.
Reference: [23] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cypher et al. [9], identify the architectural requirements such as floating point operations, communications, and input/output for message-passing scientific applications. Rothberg et al. <ref> [23] </ref> conduct a similar study towards identifying the cache and memory size requirements for several applications. However, there have been very few attempts at quantifying the effects of algorithmic and architectural interactions in a parallel system. <p> Madala and Sinclair [17] confine their studies to synchronous algorithms while [29] and [8] develop models for regular iterative algorithms. However, there exist several applications <ref> [23] </ref> with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models.
Reference: [24] <author> A. Sivasubramaniam, U. Ramachandran, and H. Venkateswaran. </author> <title> Message-Passing: Computational Model, Programming Paradigm, and Experimental Studies. </title> <type> Technical Report GIT-CC-91/11, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory [25] and message-passing <ref> [24] </ref> platforms. In [27], we illustrate the top-down approach for the scalability study of shared memory systems. <p> Experimentation, simulation, and analytical models are three techniques that have been commonly used for such studies. But each has its own limitations. We adopted the first technique in our earlier work by experimenting with frequently used parallel algorithms on shared memory [25] and message-passing <ref> [24] </ref> platforms. Experimentation is important and useful for scalability studies of existing architectures, but has certain limitations.
Reference: [25] <author> A. Sivasubramaniam, G. Shah, J. Lee, U. Ramachandran, and H. Venkateswaran. </author> <title> Experimental Evaluation of Algorithmic Performance on Two Shared Memory Multiprocessors. In Norihisa Suzuki, editor, </title> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 81-107. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory <ref> [25] </ref> and message-passing [24] platforms. In [27], we illustrate the top-down approach for the scalability study of shared memory systems. <p> Experimentation, simulation, and analytical models are three techniques that have been commonly used for such studies. But each has its own limitations. We adopted the first technique in our earlier work by experimenting with frequently used parallel algorithms on shared memory <ref> [25] </ref> and message-passing [24] platforms. Experimentation is important and useful for scalability studies of existing architectures, but has certain limitations.
Reference: [26] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> Machine Abstractions and Locality Issues in Studying Parallel Systems. </title> <type> Technical Report GIT-CC-93/63, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: In [27], we illustrate the top-down approach for the scalability study of shared memory systems. In this paper, we conduct a similar study for message-passing systems. 2 In a related paper <ref> [26] </ref> we evaluate the use of abstractions for the network and locality in the context of simulating cache-coherent shared memory multiprocessors. We illustrate the top-down approach through a case study, implementing three NAS parallel kernels [4] on two message-passing platforms (a bus and a binary hypercube) simulated on SPASM.
Reference: [27] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory [25] and message-passing [24] platforms. In <ref> [27] </ref>, we illustrate the top-down approach for the scalability study of shared memory systems.
Reference: [28] <author> X-H. Sun and J. L. Gustafson. </author> <title> Towards a better Parallel Performance Metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <year> 1991. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup <ref> [28] </ref>, experimentally determined serial fraction [14], and isoefficiency function [15] have been proposed for quantifying the scalability of parallel systems.
Reference: [29] <author> D. F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. Gehringer. </author> <title> Performance Prediction and Calibration for a Class of Multiprocessors. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 37(11) </volume> <pages> 1353-1365, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex 4 interactions between algorithms and architectures. For instance, models developed in <ref> [17, 29, 8] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while [29] and [8] develop models for regular iterative algorithms. <p> For instance, models developed in [17, 29, 8] are mainly applicable to algorithms with regular communication structures that can be predetermined before execution of the algorithm. Madala and Sinclair [17] confine their studies to synchronous algorithms while <ref> [29] </ref> and [8] develop models for regular iterative algorithms. However, there exist several applications [23] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.

References-found: 29

