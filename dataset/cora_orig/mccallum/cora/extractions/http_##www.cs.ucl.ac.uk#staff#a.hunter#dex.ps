URL: http://www.cs.ucl.ac.uk/staff/a.hunter/dex.ps
Refering-URL: http://www.cs.ucl.ac.uk/staff/A.Hunter/drafts.html
Root-URL: http://www.cs.ucl.ac.uk
Email: Email:a.hunter@cs.ucl.ac.uk  
Title: Context-dependent reasoning with lexical knowledge using default logic  
Author: Anthony Hunter 
Date: August 24, 1998  
Address: Gower Street London WC1E 6BT, UK  
Affiliation: Department of Computer Science University College London  
Abstract: Lexical knowledge is increasingly important in language engineering. However, it is a difficult kind of knowledge to represent and reason with. Existing approaches to formalizing lexical knowledge have used languages with limited expressibility and in particular they have not addressed the context-dependent nature of lexical knowledge. Here we present a framework, based on default logic, called the dex framework, for capturing context-dependent reasoning with lexical knowledge. Default logic is a first-order logic offering a more expressive formali-sation than inheritance hierarchies: (1) First-order formulae about words can be inferred; (2) Preferences over formulae can be based on specificity, reasoning about exceptions, or explicit priorities; (3) Contexts can be reasoned with as first formulae; and (4) Contexts can be derived as default inferences. In the dex framework, the word for which further information is sought called the query word. The context for a query word is derived from further words, such as words in the same sentence as the query word. These further words are used with a form of decision tree called a context classification tree to identify which contexts hold for the query word. We show how we can use these contexts in default logic to identify lexical knowledge about the query word such as synonyms, antinyms, specializations, meronyms, and more sophisticated first-order semantic knowledge. We also show how we can use a standard machine learning algorithm for decision trees to generate context classification trees. 
Abstract-found: 1
Intro-found: 1
Reference: [ADW94] <author> C Apte, F Damerau, and S Weiss. </author> <title> Automated learning of decision rules for text categorization. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 12(3) </volume> <pages> 233-251, </pages> <year> 1994. </year>
Reference-contexts: Elsewhere machine learning techniques have also been successfully applied to text categorization. In particular, rule induction with rule refinement techniques has been been applied to generating text categorization rules using large training and test sets (around 8000 examples) of news articles from the Reuters newswire <ref> [ADW94] </ref>. Given the close relationship between the function of text categorization rules, and context classification trees, the results indicate how the generation of context classification trees could be scaled-up using refinement techniques. Our next goal in developing the dex framework is to support goal-directed reasoning.
Reference: [Ant97] <author> G Antoniou. </author> <title> Non-monotonic Reasoning. </title> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter [Rei80], and good reviews are available (see for example <ref> [Bes89, Bre91, Ant97, BDK97, Sch98] </ref>). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [BCP94] <author> T Briscoe, A Copestake, and V de Paiva, </author> <title> editors. Inheritance, Defaults and the Lexicon. </title> <booktitle> Studies in Natural Language Processing. </booktitle> <address> Cambridge University Prees, </address> <year> 1994. </year>
Reference-contexts: ALso preferences used in the default reasoning are restricted to a form of specificity. There is no facility for preferences based on reasoning about exceptions or for explicit priorities. For discussion of applications for DATR see [EG95], for Laurel see <ref> [BCP94] </ref>, and for PDU see [LBAC95]. There are some proposals to use logics for lexical knoweldge representation and reasoning. An epistemic logic has been used as the basis of a framework for reasoning about context for resolving lexical ambiguity [Buv96].
Reference: [BDK97] <author> G Brewka, J Dix, and K Konolige. </author> <title> Non-monotonic Reasoning: An Overview. </title> <publisher> CLSI Publications, </publisher> <year> 1997. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter [Rei80], and good reviews are available (see for example <ref> [Bes89, Bre91, Ant97, BDK97, Sch98] </ref>). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [Bes89] <author> Ph Besnard. </author> <title> An Introduction to Default Logic. </title> <publisher> Springer, </publisher> <year> 1989. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter [Rei80], and good reviews are available (see for example <ref> [Bes89, Bre91, Ant97, BDK97, Sch98] </ref>). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [BFGM91] <author> R Beckworth, C Fellbaum, D Gross, and G Miller. </author> <title> WordNet: A lexical database organized on psycholinguistic principles. In U Zernik, editor, Lexical Acquisition: Exploiting On-line Resources to Build a Lexicon, </title> <address> pages 211-226. </address> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1991. </year>
Reference-contexts: These are large repositories of information on words with little or no structure beyond providing some attributes for each word. These approaches therefore do not harness context-dependent reasoning, default reasoning, or uncertainty management. Perhaps the most significant example of a general purpose system for lexical knowledge is WordNet <ref> [BFGM91, Mil95] </ref>. This is a semantic network containing lexical knowledge on over 90,000 word senses and it is now found to be an increasingly important resource on synonyms, generalizations, and specializations of words, for language engineering applications, such as information retrieval [Voo94].
Reference: [BM97] <author> E Brill and R Mooney. </author> <title> An overview of empirical natural language processing. </title> <journal> AI Magazine, </journal> <volume> 18(4) </volume> <pages> 13-24, </pages> <year> 1997. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [Bre91] <author> G Brewka. </author> <title> Common-sense Reasoning. </title> <publisher> Cambridge University Press, </publisher> <year> 1991. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter [Rei80], and good reviews are available (see for example <ref> [Bes89, Bre91, Ant97, BDK97, Sch98] </ref>). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [Bri91] <author> T Briscoe. </author> <title> Lexical issues in natural language processing. </title> <editor> In E Klein and F Veltman, editors, </editor> <booktitle> Natural Language and Speech, </booktitle> <pages> pages 39-68. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge such as morphological and grammatical knowledge <ref> [Bri91] </ref>. Lexical knowledge also includes semantic knowledge. Simple forms of semantic knowledge include semantic relations such as synonyms, related terms, antinyms, and specializations for a word [Cru86].
Reference: [Buv96] <author> S Buvac. </author> <title> Resolving lexical ambiguity using a formal theory of context. </title> <editor> In K van Deemter and S Peters, editors, </editor> <booktitle> Semantic Ambiguity and Underspecification, </booktitle> <pages> pages 101-124. </pages> <publisher> CSLI Publications, </publisher> <year> 1996. </year>
Reference-contexts: There are some proposals to use logics for lexical knoweldge representation and reasoning. An epistemic logic has been used as the basis of a framework for reasoning about context for resolving lexical ambiguity <ref> [Buv96] </ref>. Unfortunately, the reasoning is monotonic and so offers no context-dependent default reasoning. A new variant of default logic has been proposed for reasoning about ambiguity according to context [Poe96].
Reference: [CGHH91] <author> K Church, W Gale, P Hanks, and D Hindle. </author> <title> Using statistics in lexical analysis. </title> <editor> In J Pustejovsky and S Bergler, editors, </editor> <title> Lexical Semantics and Knowledge Representation, </title> <booktitle> volume 627 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 115-164. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [Cha96] <author> E Charniak. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [CHS93] <author> J Cussens, A Hunter, and A Srinivasan. </author> <title> Generating explicit orderings for non-monotonic logics. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI'93), </booktitle> <pages> pages 420-425. </pages> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: To address this viability problem, we aim to investigate a number of avenues: (1) Using inductive logic programming ([Mug92, Cus97, Cus98, TM98]) and statistical techniques (such as <ref> [CHS93] </ref>), to generate formulae for a default theory for a domain; (2) Using co-locational data with decision tree learning based on ID3 or C4.5, or co-locational data with unsupervised learning techniques such as in [Yar95]; and (3) Using machine-readable dictionaries and thesauri [NFE90, Mei93].
Reference: [Cop92] <author> A Copestake. </author> <title> The representation of lexical semantic information. </title> <type> Technical report, </type> <institution> University of Sussex, </institution> <year> 1992. </year> <note> CSRP 280. </note>
Reference-contexts: Some machine-readable lexicons have been developed in formal knowledge representation frameworks, supported by logical reasoning. Key formal frameworks are DATR [EG95, EG89], Laurel <ref> [Cop92] </ref>, and persistent default unification (PDU) [LBAC95]. These are forms of inheritance hierarchy that offer an efficient and lucid representation of lexical knowledge. However, they are limited formalisms in terms of expressibility and inferential capability, in particular with respect to context-dependent reasoning, and reasoning with inferences.
Reference: [Cru86] <author> D Cruse. </author> <title> Lexical Semantics. </title> <publisher> Cambridge University Press, </publisher> <year> 1986. </year>
Reference-contexts: Lexical knowledge also includes semantic knowledge. Simple forms of semantic knowledge include semantic relations such as synonyms, related terms, antinyms, and specializations for a word <ref> [Cru86] </ref>. It can also be used to identify meronymic relations, such as engine is part-of a car, and parts-of-speech such as relating actors with actions: For example, for the actor terrorist an appropriate action is terrorism. Particularly important semantic information in parsing delimits what are normally acceptanble combinations of words. <p> consider other semantic relations, including located and made-of, that can hold for a given word. focus (knife) : context (cooking) located (knife; kitchen) focus (hull) : context (ship) made-of (hull,steel) 15 focus (hull) : context (sailing-ship) made-of (hull,wood) We can draw on a richer taxonomy of meronymic relations, in particular <ref> [Cru86, WCH87] </ref>, in order to develop further semantic relations. For example, "member/collection", "portion/mass", "place/area", and "component/integral-object". Semantic information is also important in applying morphological and grammatical rules.
Reference: [Cus97] <author> J Cussens. </author> <title> Parts-of-speech tagging using progol. </title> <booktitle> In Proceedings of the Inductive Logic Programming Conference(ILP'97), </booktitle> <year> 1997. </year> <month> 25 </month>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [Cus98] <author> J Cussens. </author> <title> Using prior probabilities and density estimation for relational classification. </title> <booktitle> In Proceedings of the Inductive Logic Programming Conference(ILP'98), </booktitle> <year> 1998. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [EG89] <author> R Evans and G Gazdar. </author> <title> Inference in DATR. </title> <booktitle> In Proceedings of the 4th Conference of European Chapter of the Association of Computational Linguistics (EACL89), </booktitle> <pages> pages 66-71, </pages> <year> 1989. </year>
Reference-contexts: Some machine-readable lexicons have been developed in formal knowledge representation frameworks, supported by logical reasoning. Key formal frameworks are DATR <ref> [EG95, EG89] </ref>, Laurel [Cop92], and persistent default unification (PDU) [LBAC95]. These are forms of inheritance hierarchy that offer an efficient and lucid representation of lexical knowledge. However, they are limited formalisms in terms of expressibility and inferential capability, in particular with respect to context-dependent reasoning, and reasoning with inferences.
Reference: [EG95] <author> R Evans and G Gazdar. DATR: </author> <title> A language for lexical knowledge representation. </title> <type> Technical report, </type> <institution> University of Sussex, </institution> <year> 1995. </year> <note> CSRP 382. </note>
Reference-contexts: Some machine-readable lexicons have been developed in formal knowledge representation frameworks, supported by logical reasoning. Key formal frameworks are DATR <ref> [EG95, EG89] </ref>, Laurel [Cop92], and persistent default unification (PDU) [LBAC95]. These are forms of inheritance hierarchy that offer an efficient and lucid representation of lexical knowledge. However, they are limited formalisms in terms of expressibility and inferential capability, in particular with respect to context-dependent reasoning, and reasoning with inferences. <p> ALso preferences used in the default reasoning are restricted to a form of specificity. There is no facility for preferences based on reasoning about exceptions or for explicit priorities. For discussion of applications for DATR see <ref> [EG95] </ref>, for Laurel see [BCP94], and for PDU see [LBAC95]. There are some proposals to use logics for lexical knoweldge representation and reasoning. An epistemic logic has been used as the basis of a framework for reasoning about context for resolving lexical ambiguity [Buv96].
Reference: [FA98] <author> C Fillmore and B Atkins. </author> <title> Framenet and lexicongraphic relevance. </title> <booktitle> In Proceedings of the First International Conference on Language Resources and Evaluation, </booktitle> <year> 1998. </year>
Reference-contexts: Moreover, there is no logical reasoning with the relations in the semantic network. A more sophisticated system being developed with a deeper knowledgebase on several thousand wordsenses is FrameNet <ref> [FA98] </ref>. This is a frame-based representation with logical reasoning limited to inheritance, where one frame is an elaboration of another, and composition, where one frame is built-up of other frames as the parts.
Reference: [Fer97] <author> T Fernando. </author> <title> A modal logic non-determinsitic disambiguation. </title> <booktitle> In Proceedings of the Amsterdam Colloquium, </booktitle> <pages> pages 121-126, </pages> <year> 1997. </year>
Reference-contexts: However, there is no mechanism for determining context for a given ambiguous word, and the approach is limited in reasoning about contexts. There are also open questions about the behaviour of the default logic with regard to automated reasoning. Another modal logic for disambiguation has been proposed in <ref> [Fer97] </ref> that does provide default reasoning. This framework is interesting as an abstract framework. But, there open questions with regard to its practical application. Again the reasoning about contexts is limited and there is no implicit or explicit prioritization mechanisms over context-dependent rules for disambigution.
Reference: [Ger95] <author> P Gervas. </author> <title> Logical considerations in the interpretation of presuppositional sentences. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Imperial College, </institution> <year> 1995. </year>
Reference-contexts: In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases [Nie94, LS95, Sch95, NS98]. Other examples, of using default logic in language engineering include for presuppositions <ref> [Mer91, Ger95] </ref>, anaphoric resolution [Qua93], and for reasoning about the notion of "aboutness" for text handling [Hun96]. 4 The dex framework In the dex framework (for default lexical framework), we represent morphological, grammatical, and semantic knowledge using default logic. The system is queried to find information about a word.
Reference: [GPWS96] <author> L Gutherie, J Pustejovsky, Y Wilks, and B Slator. </author> <title> The role of lexicons in natural language processing. </title> <journal> Communications of the ACM, </journal> <volume> 39(1) </volume> <pages> 63-72, </pages> <year> 1996. </year>
Reference-contexts: We also see uncertainty management in the more abstract setting of multiple inheritance in semantic networks. Here there is ambiguity about what should be inherited. 4 The simplest computer-oriented approaches to lexical knowledge are machine-readable lexicons that may be viewed as relational databases (for reviews see <ref> [WSG96, GPWS96] </ref>). These are large repositories of information on words with little or no structure beyond providing some attributes for each word. These approaches therefore do not harness context-dependent reasoning, default reasoning, or uncertainty management.
Reference: [Gre96] <author> G Green. </author> <title> Ambiguity resolution and discourse interpretation. </title> <editor> In K van Deemter and S Peters, editors, </editor> <booktitle> Semantic Ambiguity and Underspecification, </booktitle> <pages> pages 1-26. </pages> <publisher> CSLI Publications, </publisher> <year> 1996. </year>
Reference-contexts: months, and days, we can define one sense of the word sentence as a binary relation under the following constraint: 9x; y; t [judge (x) ^ defendent (y) ^ sentences (x; y) ^ [years (t) _ months (t) _ days (t)] ^ sentence (y; t)] Words are very frequently ambiguous <ref> [Spa86, Hir87, Gut93, Gre96] </ref>. Obvious examples are bank, plane, and train.
Reference: [Gut93] <author> L Gutherie. </author> <title> A note on lexical disambiguation. </title> <editor> In C Souter and E Atwell, editors, </editor> <booktitle> Corpus-based computational linguistics, </booktitle> <pages> pages 227-237. </pages> <address> Rodopi, </address> <year> 1993. </year>
Reference-contexts: months, and days, we can define one sense of the word sentence as a binary relation under the following constraint: 9x; y; t [judge (x) ^ defendent (y) ^ sentences (x; y) ^ [years (t) _ months (t) _ days (t)] ^ sentence (y; t)] Words are very frequently ambiguous <ref> [Spa86, Hir87, Gut93, Gre96] </ref>. Obvious examples are bank, plane, and train.
Reference: [Hir87] <author> G Hirst. </author> <title> Semantic interpretation and the resolution of ambiguity. </title> <booktitle> Studies in Natural Language Processing. </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1987. </year>
Reference-contexts: months, and days, we can define one sense of the word sentence as a binary relation under the following constraint: 9x; y; t [judge (x) ^ defendent (y) ^ sentences (x; y) ^ [years (t) _ months (t) _ days (t)] ^ sentence (y; t)] Words are very frequently ambiguous <ref> [Spa86, Hir87, Gut93, Gre96] </ref>. Obvious examples are bank, plane, and train. <p> A word displays homonymy if it has meanings that have no relation to each for example bank as ground at edge of river, and bank as establishment for custody of money. Managing ambiguity is critical in parsing. In examples, such as the following taken from <ref> [Hir87] </ref>, when parsing left to right, the context starts as predominantly being legal, so when the homonym bar is considered, the legal meaning is adopted, but then when the parsing moves rightwards and the word drink is considered, a contradiction is apparent, forcing a reassessment of the meaning of bar from
Reference: [Hun96] <author> A Hunter. </author> <title> Intelligent text handling using default logic. </title> <booktitle> In Proceedings of the IEEE Conference on Tools with Artificial Intelligence, </booktitle> <pages> pages 34-40. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Other examples, of using default logic in language engineering include for presuppositions [Mer91, Ger95], anaphoric resolution [Qua93], and for reasoning about the notion of "aboutness" for text handling <ref> [Hun96] </ref>. 4 The dex framework In the dex framework (for default lexical framework), we represent morphological, grammatical, and semantic knowledge using default logic. The system is queried to find information about a word.
Reference: [Hun97] <author> A. Hunter. </author> <title> Using default logic for lexical knowledge. In Qualitative and Quantitative Practical Reasoning, </title> <booktitle> volume 1244 of Lecture Notes in Computer Science, </booktitle> <pages> pages 86-95. </pages> <publisher> Springer, </publisher> <year> 1997. </year>
Reference-contexts: In this paper, we use default logic to provide a framework, called the dex framework for context-dependent reasoning with lexical knowledge. This framework is a development of <ref> [Hun97] </ref>. 1 2 Default reasoning with lexical knowledge According to Trask [Tra93], a lexicon within the study of grammar has traditionally been used as a repository of miscellaneous facts, with little in the way of generalizations.
Reference: [LBAC95] <author> A Lascarides, T Briscoe, N Asher, </author> <title> and A Copestake. Order independent and persisitent typed default unification. </title> <journal> Linguistics and Philiosophy, </journal> <volume> 19(1) </volume> <pages> 1-90, </pages> <year> 1995. </year>
Reference-contexts: Some machine-readable lexicons have been developed in formal knowledge representation frameworks, supported by logical reasoning. Key formal frameworks are DATR [EG95, EG89], Laurel [Cop92], and persistent default unification (PDU) <ref> [LBAC95] </ref>. These are forms of inheritance hierarchy that offer an efficient and lucid representation of lexical knowledge. However, they are limited formalisms in terms of expressibility and inferential capability, in particular with respect to context-dependent reasoning, and reasoning with inferences. <p> ALso preferences used in the default reasoning are restricted to a form of specificity. There is no facility for preferences based on reasoning about exceptions or for explicit priorities. For discussion of applications for DATR see [EG95], for Laurel see [BCP94], and for PDU see <ref> [LBAC95] </ref>. There are some proposals to use logics for lexical knoweldge representation and reasoning. An epistemic logic has been used as the basis of a framework for reasoning about context for resolving lexical ambiguity [Buv96]. Unfortunately, the reasoning is monotonic and so offers no context-dependent default reasoning.
Reference: [LCB96] <author> A Lascarides, </author> <title> A Copestake, and T Briscoe. Ambiguity and coherence. </title> <journal> Journal of Semantics, </journal> <volume> 13(1) </volume> <pages> 41-65, </pages> <year> 1996. </year>
Reference-contexts: In particular, we are investigating the use of the XRay query answering system for default logics [Sch95, NS98]. We also want to refine the notion of a source to allow better scoping. To illustrate the need, consider the following extreme example that is a zeugma taken from <ref> [LCB96] </ref>. John banked the money and then the plane. The context of the first occurrence of banked could be described as finance and of the second (implicit) occurrence could be described as aviation.
Reference: [Lib91] <author> M Liberman. </author> <title> The trend towards statistical models in natural language processing. </title> <editor> In E Klein and F Veltman, editors, </editor> <booktitle> Natural Language and Speech, </booktitle> <pages> pages 1-8. </pages> <publisher> Springer, </publisher> <year> 1991. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [LS95] <author> T Linke and T Schaub. </author> <title> Lemma handling in default logic theorem provers. In Symbolic and Qualitative Approaches to Reasoning and Uncertainty, </title> <booktitle> volume 946 of Lecture Notes in Computer Science, </booktitle> <pages> pages 285-292. </pages> <publisher> Springer, </publisher> <year> 1995. </year>
Reference-contexts: It is an efficient representation in terms of space, and it is a well-understood formalism for representing uncertain information, and it has strong theoretical foundations. In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases <ref> [Nie94, LS95, Sch95, NS98] </ref>.
Reference: [Mei93] <author> W Meijs. </author> <title> Exploring lexical knowledge. </title> <editor> In C Souter and E Atwell, editors, </editor> <booktitle> Corpus-based Computational Linguistics, </booktitle> <pages> pages 249-260. </pages> <address> Rodopi, </address> <year> 1993. </year>
Reference-contexts: Cus97, Cus98, TM98]) and statistical techniques (such as [CHS93]), to generate formulae for a default theory for a domain; (2) Using co-locational data with decision tree learning based on ID3 or C4.5, or co-locational data with unsupervised learning techniques such as in [Yar95]; and (3) Using machine-readable dictionaries and thesauri <ref> [NFE90, Mei93] </ref>.
Reference: [Mer91] <author> B Mercer. </author> <title> Presuppositions and default reasoning: A study in lexical pragmatics. </title> <editor> In J Pustejovsky and S Bergler, editors, </editor> <title> Lexical Semantics and Knowledge Representation, </title> <booktitle> volume 627 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 321-340. </pages> <publisher> Springer, </publisher> <year> 1991. </year> <month> 26 </month>
Reference-contexts: In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases [Nie94, LS95, Sch95, NS98]. Other examples, of using default logic in language engineering include for presuppositions <ref> [Mer91, Ger95] </ref>, anaphoric resolution [Qua93], and for reasoning about the notion of "aboutness" for text handling [Hun96]. 4 The dex framework In the dex framework (for default lexical framework), we represent morphological, grammatical, and semantic knowledge using default logic. The system is queried to find information about a word.
Reference: [Mil95] <author> G Miller. </author> <title> WordNet: A lexical database for English. </title> <journal> Communications of the ACM, </journal> <volume> 38(11) </volume> <pages> 39-41, </pages> <year> 1995. </year>
Reference-contexts: These are large repositories of information on words with little or no structure beyond providing some attributes for each word. These approaches therefore do not harness context-dependent reasoning, default reasoning, or uncertainty management. Perhaps the most significant example of a general purpose system for lexical knowledge is WordNet <ref> [BFGM91, Mil95] </ref>. This is a semantic network containing lexical knowledge on over 90,000 word senses and it is now found to be an increasingly important resource on synonyms, generalizations, and specializations of words, for language engineering applications, such as information retrieval [Voo94].
Reference: [Mug92] <author> S Muggleton. </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The ID3 algorithm is a very simple approach to learning classification trees. However, developments, such as in C4.5 [Qui93], include handling missing or noisy data, and avoiding overfitting using pruning of classification trees. More advanced types of machine learning algorithms include 20 ILP for learning relational concepts <ref> [Mug92] </ref>. All these approaches are types of supervised learning which means that the training examples need to be classified prior to learning. This supervision can be a significant cost.
Reference: [NFE90] <author> J Nutter, E Fox, and M Evens. </author> <title> Building a lexicon from machine-readable dictionaries for improved information retrieval. </title> <journal> Literary and Linguistic Computing, </journal> <volume> 5(2) </volume> <pages> 129-137, </pages> <year> 1990. </year>
Reference-contexts: Cus97, Cus98, TM98]) and statistical techniques (such as [CHS93]), to generate formulae for a default theory for a domain; (2) Using co-locational data with decision tree learning based on ID3 or C4.5, or co-locational data with unsupervised learning techniques such as in [Yar95]; and (3) Using machine-readable dictionaries and thesauri <ref> [NFE90, Mei93] </ref>.
Reference: [Nie94] <author> I Niemela. </author> <title> A decision method for non-monotonic reasoning based on autoepistemic reasoning. </title> <booktitle> In Proceedings of the Fourth International Conference Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 473-484. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: It is an efficient representation in terms of space, and it is a well-understood formalism for representing uncertain information, and it has strong theoretical foundations. In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases <ref> [Nie94, LS95, Sch95, NS98] </ref>.
Reference: [NS98] <author> P Nicolas and T Schaub. </author> <title> The XRay system: An implementation platform for local query answering in default logics. </title> <editor> In A Hunter and S Parsons, editors, </editor> <booktitle> Applications of Uncertainty Formalisms, Lecture Notes in Computer Science. </booktitle> <publisher> Springer, </publisher> <year> 1998. </year>
Reference-contexts: It is an efficient representation in terms of space, and it is a well-understood formalism for representing uncertain information, and it has strong theoretical foundations. In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases <ref> [Nie94, LS95, Sch95, NS98] </ref>. <p> For simplicity, we chose Reiter's version of default logic. But, for efficiency, a goal-directed form of default reasoning is more appropriate. In particular, we are investigating the use of the XRay query answering system for default logics <ref> [Sch95, NS98] </ref>. We also want to refine the notion of a source to allow better scoping. To illustrate the need, consider the following extreme example that is a zeugma taken from [LCB96]. John banked the money and then the plane.
Reference: [PB93] <author> J Pustejovsky and B Boguraev. </author> <title> Lexical knowledge and natural language processing. </title> <journal> Artificial Intelligence, </journal> <volume> 63 </volume> <pages> 193-223, </pages> <year> 1993. </year>
Reference-contexts: As with WordNet there is no explicit machinery for determining in which context a particular wordsense should be used, A richer knowledge representation and reasoning framework is the generative lexicon <ref> [PB93, Pus95] </ref>. <p> The generative lexicon framework does offer context-dependent reasoning | though predominantly this is classical | so for a word disambiguation different wordsenses may be determined using the extra constraints in the qualia structure. Consider the qualia structure for the noun car (examples taken from <ref> [PB93] </ref>): 1 Of course, the notion of a strict synonym is in general artificial, since very few words can be interchanged in any sentence that they appear. 5 car (x) const = fbody; engine; :::g formal = physobj (x) telic = drive (P; x; y) agentive = artifact (x) Consider the <p> Consider for example the following default logic. focus (sentence) ^ context (law) : :context (writing) 9x; y; t [judge (x) ^ defendent (y) ^ sentences (x; y) ^ sentence (y; t)] Also by using the first-order capability of default logic, we can adopt the qualia structure <ref> [PB93] </ref> discussed in section 2 for conceptualizing and structuring lexical knoweldge. For this, we can use 18 the labels const, formal, telic and agentive as meta-level relations.
Reference: [Poe96] <author> M Poesio. </author> <title> Semantic ambiguity and perceived ambiguity. </title> <editor> In K van Deemter and S Peters, editors, </editor> <booktitle> Semantic Ambiguity and Underspecification, </booktitle> <pages> pages 159-202. </pages> <publisher> CSLI Publications, </publisher> <year> 1996. </year>
Reference-contexts: An epistemic logic has been used as the basis of a framework for reasoning about context for resolving lexical ambiguity [Buv96]. Unfortunately, the reasoning is monotonic and so offers no context-dependent default reasoning. A new variant of default logic has been proposed for reasoning about ambiguity according to context <ref> [Poe96] </ref>. However, there is no mechanism for determining context for a given ambiguous word, and the approach is limited in reasoning about contexts. There are also open questions about the behaviour of the default logic with regard to automated reasoning.
Reference: [Pus95] <author> J Pustejovsky. </author> <title> The Generative Lexicon. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: As with WordNet there is no explicit machinery for determining in which context a particular wordsense should be used, A richer knowledge representation and reasoning framework is the generative lexicon <ref> [PB93, Pus95] </ref>.
Reference: [Qua93] <author> J Quantz. </author> <title> A preferential default description logic for disambiguation in natural language processing. </title> <editor> In G Brewka and C Witteveen, editors, </editor> <booktitle> Proceedings of the Dutch-German Workshop on Nonmonotonic Reasoning Techniques and their Applications, </booktitle> <pages> pages 1-13. </pages> <address> RWTH Aachen, </address> <year> 1993. </year>
Reference-contexts: In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases [Nie94, LS95, Sch95, NS98]. Other examples, of using default logic in language engineering include for presuppositions [Mer91, Ger95], anaphoric resolution <ref> [Qua93] </ref>, and for reasoning about the notion of "aboutness" for text handling [Hun96]. 4 The dex framework In the dex framework (for default lexical framework), we represent morphological, grammatical, and semantic knowledge using default logic. The system is queried to find information about a word.
Reference: [Qui86] <author> J Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: However, none of these approaches have been developed for identifying rules for determining contexts. In our work on the dex framework, we have used the ID3 inductive learning algorithm developed by Ross Quinlan <ref> [Qui86] </ref>.
Reference: [Qui93] <author> J Quinlan. </author> <title> C4.5:Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: If a training example contains that word, then "yes" is entered into the corresponding position in the table, and "no" otherwise. The ID3 algorithm is a very simple approach to learning classification trees. However, developments, such as in C4.5 <ref> [Qui93] </ref>, include handling missing or noisy data, and avoiding overfitting using pruning of classification trees. More advanced types of machine learning algorithms include 20 ILP for learning relational concepts [Mug92].
Reference: [Rei80] <author> R Reiter. </author> <title> Default logic. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 81-132, </pages> <year> 1980. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter <ref> [Rei80] </ref>, and good reviews are available (see for example [Bes89, Bre91, Ant97, BDK97, Sch98]). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [Riv87] <author> R Rivest. </author> <title> Learning decision lists. </title> <journal> Machine learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: All these approaches are types of supervised learning which means that the training examples need to be classified prior to learning. This supervision can be a significant cost. An alternative is unsupervised learning such as decision list learning <ref> [Riv87] </ref> which has been applied to learning rules for wordsense disambiguation using co-locational data [Yar95]. 6 A case study In this case study, we used a set of 139 news summaries taken from The Economist newspaper in the period 1996-98.
Reference: [Sch95] <author> T Schaub. </author> <title> A new methodology for query-answering in default logics via structure-oriented theorem proving. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 15 </volume> <pages> 95-165, </pages> <year> 1995. </year>
Reference-contexts: It is an efficient representation in terms of space, and it is a well-understood formalism for representing uncertain information, and it has strong theoretical foundations. In addition, there are prototype implementations of inference engines for default logic that can be used for developing default logic knowledge-bases <ref> [Nie94, LS95, Sch95, NS98] </ref>. <p> For simplicity, we chose Reiter's version of default logic. But, for efficiency, a goal-directed form of default reasoning is more appropriate. In particular, we are investigating the use of the XRay query answering system for default logics <ref> [Sch95, NS98] </ref>. We also want to refine the notion of a source to allow better scoping. To illustrate the need, consider the following extreme example that is a zeugma taken from [LCB96]. John banked the money and then the plane.
Reference: [Sch98] <author> T Schaub. </author> <title> The Automation of Reasoning with Incomplete Information: From Semantic Foundation to Efficient Computation. </title> <publisher> Springer, </publisher> <year> 1998. </year>
Reference-contexts: To address some of the shortcomings of existing approaches, we propose a new alternative for lexical knowledge representation and reasoning, called the dex framework, that is based on default logic. 3 Overview of default logic Default logic was proposed by Reiter [Rei80], and good reviews are available (see for example <ref> [Bes89, Bre91, Ant97, BDK97, Sch98] </ref>). In default logic, knowledge is represented as a default theory, which consists of a set of first-order formulae and a set of default rules for representing default information.
Reference: [Spa86] <author> K Spark-Jones. </author> <title> Synonym and Semantic Classification. Edinburgh Information Technology Series. </title> <publisher> Edinburgh University Press, </publisher> <year> 1986. </year>
Reference-contexts: months, and days, we can define one sense of the word sentence as a binary relation under the following constraint: 9x; y; t [judge (x) ^ defendent (y) ^ sentences (x; y) ^ [years (t) _ months (t) _ days (t)] ^ sentence (y; t)] Words are very frequently ambiguous <ref> [Spa86, Hir87, Gut93, Gre96] </ref>. Obvious examples are bank, plane, and train.
Reference: [Tho97] <author> R Thomason. </author> <note> Non-monotonicity in linguistics. </note> <editor> In J van Benthem and A ter Meulen, editors, </editor> <booktitle> Handbook of Logic and Language, </booktitle> <pages> pages 777-831. </pages> <publisher> Elsevier, </publisher> <year> 1997. </year>
Reference-contexts: Given the uncertainty involved in this form of context-dependent reasoning, it is a form of default knowledge. Viewing lexical knowledge as default knowledge is part of a general trend of viewing linguistic knowledge as default knowledge <ref> [Tho97] </ref>. This need for lexical knowledge raises significant knowledge representation and reasoning questions. Indeed, developing lexical knowledgebases, with automated reasoning technology, has proved to be a complex and as yet unresolved problem.
Reference: [TM98] <author> C Thompson and R Mooney. </author> <title> Semantic lexicon acquisition fo rlearning natural language interfaces. </title> <type> Technical Report TR AI98-273, </type> <institution> AI Lab, University of Texas at Austin, </institution> <year> 1998. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>).
Reference: [Tra93] <author> R Trask. </author> <title> A Dictionary of Grammatical Terms in Linguistics. </title> <publisher> Routledge, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we use default logic to provide a framework, called the dex framework for context-dependent reasoning with lexical knowledge. This framework is a development of [Hun97]. 1 2 Default reasoning with lexical knowledge According to Trask <ref> [Tra93] </ref>, a lexicon within the study of grammar has traditionally been used as a repository of miscellaneous facts, with little in the way of generalizations. This view has shifted and recent theories of language are using lexicons for significant proportions of linguistic knowledge such as morphological and grammatical knowledge [Bri91].
Reference: [Voo94] <author> E Voorhees. </author> <title> Query expansion using lexical-semantic relations. </title> <editor> In W Croft and C van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Seventeenth International ACM-SIGIR Conference on Research and Developement in Information Retrieval, </booktitle> <pages> pages 61-69, </pages> <year> 1994. </year>
Reference-contexts: This is a semantic network containing lexical knowledge on over 90,000 word senses and it is now found to be an increasingly important resource on synonyms, generalizations, and specializations of words, for language engineering applications, such as information retrieval <ref> [Voo94] </ref>. In WordNet, each set of words that are regarded as strict synonyms (i.e. the words can be interchanged in a sentence 1 ) is called a synset.
Reference: [vR79] <author> C van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Cambridge University Press, </publisher> <year> 1979. </year> <month> 27 </month>
Reference-contexts: They normally constitute about 50% of the words in a sentence. For further information, and a useful listing of stop words, see <ref> [vR79] </ref> 9 Set of formulae representing lexical knowledge about the query word Default theory Primary contexts identified Context classification trees Query word Source ? ? by the outer box. <p> The methodology involves taking an item of text, removing the stop words, and then using the remaining words as a training (learning) example. The stop list was similar to that in <ref> [vR79] </ref>. Each item of text implies one or more contexts. Contexts are used as the classifications for the examples. Each attribute in the table of training examples is a word.
Reference: [WCH87] <author> M Winston, R Chaffin, and D Herrman. </author> <title> A taxonomy of part-whole relations. </title> <journal> Cognitive Science, </journal> <volume> 11 </volume> <pages> 417-444, </pages> <year> 1987. </year>
Reference-contexts: consider other semantic relations, including located and made-of, that can hold for a given word. focus (knife) : context (cooking) located (knife; kitchen) focus (hull) : context (ship) made-of (hull,steel) 15 focus (hull) : context (sailing-ship) made-of (hull,wood) We can draw on a richer taxonomy of meronymic relations, in particular <ref> [Cru86, WCH87] </ref>, in order to develop further semantic relations. For example, "member/collection", "portion/mass", "place/area", and "component/integral-object". Semantic information is also important in applying morphological and grammatical rules.
Reference: [WSG96] <author> Y Wilks, B Slator, and L Guthrie. </author> <title> Electric Words: Dictionaries, Computers, and Meanings. </title> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: We also see uncertainty management in the more abstract setting of multiple inheritance in semantic networks. Here there is ambiguity about what should be inherited. 4 The simplest computer-oriented approaches to lexical knowledge are machine-readable lexicons that may be viewed as relational databases (for reviews see <ref> [WSG96, GPWS96] </ref>). These are large repositories of information on words with little or no structure beyond providing some attributes for each word. These approaches therefore do not harness context-dependent reasoning, default reasoning, or uncertainty management.
Reference: [Yar95] <author> D Yarowsky. </author> <title> Unsupervised word sense disambiguation rivaling supervised methods. </title> <booktitle> In Proceedings of the 32nd Annual Meeting of the Association of Computational Lin-gustics, </booktitle> <pages> pages 189-196, </pages> <year> 1995. </year>
Reference-contexts: structures, within default logic are: (1) the facility for context-dependent reasoning that handles uncertainty about the contexts; and (2) first-order reasoning with lexical knowledge. 5 Learning context classification trees There is a general trend to use statistical and machine learning techniques in building grammatical and lexical knowledgebases (see for example <ref> [Lib91, CGHH91, Yar95, Cha96, BM97, Cus97, Cus98, TM98] </ref>). <p> This supervision can be a significant cost. An alternative is unsupervised learning such as decision list learning [Riv87] which has been applied to learning rules for wordsense disambiguation using co-locational data <ref> [Yar95] </ref>. 6 A case study In this case study, we used a set of 139 news summaries taken from The Economist newspaper in the period 1996-98. <p> of avenues: (1) Using inductive logic programming ([Mug92, Cus97, Cus98, TM98]) and statistical techniques (such as [CHS93]), to generate formulae for a default theory for a domain; (2) Using co-locational data with decision tree learning based on ID3 or C4.5, or co-locational data with unsupervised learning techniques such as in <ref> [Yar95] </ref>; and (3) Using machine-readable dictionaries and thesauri [NFE90, Mei93].
References-found: 58

