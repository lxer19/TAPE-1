URL: http://www.cs.brown.edu/people/dgk/Papers/thesis.ps
Refering-URL: http://www.cs.brown.edu/people/dgk/papers.html
Root-URL: http://www.cs.brown.edu/
Title: Constraint Query Algebras  
Author: by Dina Q Goldin 
Degree: 1985 M.S. Brown University, 1987 Thesis Submitted in partial fulfillment of the requirements for the Degree of Doctor of Philosophy in the  
Date: May 1997  
Affiliation: B.A. Yale University,  Department of Computer Science at Brown University  
Abstract-found: 0
Intro-found: 1
Reference: [AHV] <author> S. Abiteboul, R. Hull, V. Vianu. </author> <title> Foundations of Databases. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Indeed, having such languages for ad-hoc database querying is a requirement in today's relational technology (see <ref> [AHV, Kan90, Ull] </ref>. CQLs can be viewed as a specialized form of constraint programming, similar to the way that relational query languages can be viewed as a form of first-order theorem proving. Constraint programming paradigms are inherently declarative, since they implicitly describe computations by specifying how these computations are constrained. <p> Definition 3 The syntax of a Constraint Query Calculus is the union of a relational database query language and formulas in a decidable logical theory. For example: Relational calculus [Cod70] + the theory of real closed fields [Tar, Ren92]; Relational calculus (or even Inflationary Datalog : , <ref> [AHV] </ref> + the theory of dense order with constants [FG77]. Definition 4 The semantics of CQC is based on that of the decidable logical theory, by interpreting database atoms as shorthands for formulas of the theory.
Reference: [AV95] <author> L. Agre, J. S. Vitter. </author> <title> Optimal Interval Management in External Memory. </title> <type> Manuscript, </type> <month> November </month> <year> 1995. </year>
Reference-contexts: Optimal in-core dynamic interval management is one of the basic tools of computational geometry. However, I/O optimal solutions are non-trivial, even for the static case. For the first optimal static solution see [KRVV93] and for an optimal dynamic one see <ref> [AV95] </ref>. 3.2 Algebraic Expressions and Query Optimization The use of algebraic expressions in query optimization is based on two key notions: * Specifying a search space of semantically equivalent relational algebraic expres sions that could have different evaluation costs. 23 * Estimating the cost of performing an operation (for example, natural
Reference: [AHO90] <author> A. Aho. </author> <title> Algorithms for Finding Patterns in Strings. Handbook of TCS., </title> <editor> J. Van Leeuwen editor, </editor> <title> volume A, chapter 5, </title> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: A wide range of algorithms has been developed for internal (i.e., in-core) versions of this question <ref> [AHO90] </ref> for strings over an alphabet or for values over bounded discrete domains. There are particularly elegant linear-time O (n + N ) algorithms (by Knuth-Morris-Pratt and Boyer-Moore) and practical searching utilities for more general patterns instead of query strings Q (e.g., regular patterns in grep).
Reference: [AHU] <author> A.V. Aho, J.E. Hopcroft, J.D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms, </title> <publisher> Addison-Wesley Publishing Co., </publisher> <year> 1974. </year>
Reference-contexts: By fixing the program size and letting the database grow, one can prove that the evaluation can be performed in PTIME or even in LOGSPACE, depending on the constraints considered (for models of efficient algorithms see <ref> [AHU] </ref>. It seems reasonable to limit computations to efficient ones, i.e., PTIME manipulations of the data. <p> We start with an introduction to path expressions [Tar81a, Tar81b]. Let M = (X; E) be a directed multigraph; any path in M can be regarded as a string over the alphabet E. The set of all paths in M forms a regular language (see <ref> [AHU] </ref> for an introduction to regular languages). The path expression over M is a regular expression that defines this regular language; its size is the length of the expression, with all subexpressions unfolded. Lemma 7 Let P be the path expression over M.
Reference: [AFS93] <author> R. Agrawal, C. Faloutsos, A. Swami. </author> <title> Efficient Similarity Search in Sequence Databases. </title> <booktitle> FODO Conf., </booktitle> <address> Evanston, Ill., </address> <month> Oct. </month> <year> 1993 </year>
Reference-contexts: In this paper, we formalize the intuitive notions of exact and approximate similarity between time-series patterns and data. Our definition of similarity extends the distance metric used in <ref> [AFS93, FRM94] </ref> with invariance under a group of transformations. Our main observation is that the resulting, more expressive, set of constraint queries can be supported by a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in [AFS93, FRM94]. 7.1 Problem Definition 7.1.1 Approximate Matching of <p> of similarity extends the distance metric used in <ref> [AFS93, FRM94] </ref> with invariance under a group of transformations. Our main observation is that the resulting, more expressive, set of constraint queries can be supported by a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in [AFS93, FRM94]. 7.1 Problem Definition 7.1.1 Approximate Matching of Time-Series Data Time-series are the principal format of data in many applications, from financial to scientific. Time-series data are sequences of real numbers representing measurements at uniformly-spaced temporal instances. <p> However, any proposal 71 of such linguistic facilities must be supported by indexing (i.e., be implementable with reasonable I/O efficiency) for very large data sets. Examples of recent database research towards this goal include <ref> [AFS93, FRM94, SLR94] </ref>. A most basic problem in this area is First-Occurrence Subsequence Matching, defined as follows: given a query sequence Q of length n and a much longer data sequence S of length N , find the first occurrence of a contiguous subsequence within S that matches Q exactly. <p> The All-Occurrences Approximate Matching problems (either Subsequence or Whole-Sequence) are defined as before, but with "match approximately within tolerance *" instead of "match exactly". For external solutions to the All-Occurrence Whole-Sequence Approximate version, we refer to <ref> [AFS93, SLR94] </ref>; for the All-Occurrence Subsequence Approximate version, we refer to [FRM94]. A further characteristic of time-series data that is used to advantage, is that they have a skewed energy spectrum, to use the terminology borrowed from Discrete Signal Processing [DS]. <p> We show this using a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in <ref> [AFS93, FRM94] </ref>. In Section 2, we provide a semantics for similarity querying where we use the similarity distance between Q and S (defined in Section 7.2.2) as the distance metric. <p> In Section 3, we show that the semantics of Section 2.2 has several desirable properties, such as updateability and well-behaved trails, which allow us to provide efficient 75 implementations for similarity querying, both in the internal and external query set-ting. We first adapt the criteria put forth in <ref> [AFS93, FRM94] </ref> (Section 3.1) and satisfy them using fingerprints of the normal form (Section 3.2). We then argue that fingerprints are incrementally computable (Section 3.3), can be used ala Rabin-Karp [KR87] for internal searching (Section 3.4), and most importantly external indexing (Section 3.5). <p> We now define the fingerprint function F as well as the fingerprint distance function D F . These definitions are similar to the ones used for Approximate Matching in <ref> [AFS93] </ref> and [FRM94]. <p> In addition, we have shown that the fingerprint of <ref> [AFS93] </ref> for time-series approximate matching (without similarity) is also updateable. 7.3.4 Internal Query Representation In this section, we sketch out the internal implementation of similarity queries.
Reference: [AS91] <author> W.G. Aref and H. Samet. </author> <title> Extending a DBMS with Spatial Operations. </title> <booktitle> International Symposium on Large Spatial Databases, </booktitle> <pages> 299-318, </pages> <year> 1991. </year>
Reference-contexts: One of them is the generalized version of the "aggregation" operator, a general operator that expresses statistical operations such as avg, min, count [Klu82]. Some aggregation operators like count are not applicable to infinite relations. On the other hand, new operators like area <ref> [AS91] </ref> (or its generalization, n-dimensional volume [GK94]) occur there quite naturally. [Kup94] describes a general framework, modeled after [Klu82], for adding aggregate operators to relational algebra and calculus.
Reference: [AS80] <author> B. Aspvall, Y. Shiloach. </author> <title> A polynomial time algorithm for solving systems of linear inequalities with two variables per inequality. </title> <journal> SIAM J. Comput., </journal> <volume> 9:4:827-845, </volume> <year> 1980. </year>
Reference: [AGSS86] <author> A.K. Aylamazyan, </author> <title> M.M. Gilula, A.P. Stolboushkin, G.F. Schwartz. Reduction of the Relational Model with Infinite Domain to the Case of Finite Domains. </title> <booktitle> Proc. </booktitle> <institution> USSR Acad. of Science (Doklady), 286:2:308-311, </institution> <year> 1986. </year>
Reference-contexts: The CDB framework has provided a unified view of some previous database research: for example, on the power of constraints for the implicit specification of temporal data [CI89], on relational query safety <ref> [AGSS86] </ref>, on conjunctive queries with inequalities [Klu88] and on extending magic sets [Ram88]. <p> The analogue for the relational model is that relations are finite structures, and queries are supposed to preserve this finiteness. This is a requirement that creates various "safety" problems in relational databases [Cod70, Ull]. The precise analogue in relational databases is the notion of weak safety of <ref> [AGSS86] </ref>. Evaluation of a query corresponds to an instance of a decision problem. Quantifier elimination procedures realize the goal of closed form and use induction on the structure of formulas, which leads to bottom-up evaluation.
Reference: [BNW91] <author> M. Baudinet, M. Niezette, P. Wolper. </author> <title> On the Representation of Infinite Temporal Data and Queries. </title> <booktitle> Proc. 10th ACM PODS, </booktitle> <pages> 280-290, </pages> <year> 1991. </year>
Reference-contexts: This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages [Col80, DVSAGB88, JL87] or constraint query languages <ref> [BNW91, BJM93, KSW90, KKR95] </ref>. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work. <p> Possible extensions involve more powerful similarity queries and different distance functions; also indexing of time-series data that is represented using constraints (see <ref> [BNW91, KSW90] </ref>). In addition to implementing the general version of similarity querying, as described in this chapter, we have implemented other versions, by tailoring the internal representation and the corresponding indexing scheme for specialized subsets of similarity queries.
Reference: [BM72] <author> R. Bayer, E. McCreight. </author> <title> Organization of Large Ordered Indexes. </title> <journal> Acta Infor-matica, </journal> <volume> 1 </volume> <pages> 173-189, </pages> <year> 1972. </year> <month> 102 </month>
Reference-contexts: I/O efficient (i.e., logarithmic or constant) use of secondary storage is an additional requirement, beyond low data complexity or strong polynomiality of operations, whose satisfaction greatly contributes to relational technology. B-trees (and their variants B + -trees) are examples of important data structures for implementing relational databases (see <ref> [BM72] </ref>). Let each secondary memory access transmit B units of data, let r be a relation with N tuples, and let us have a B + - tree on the attribute x of r. The space used in this case is O (N=B).
Reference: [Bor81] <author> A.H. Borning. </author> <title> The Programming Language Aspects of ThingLab, A Constraint-Oriented Simulation Laboratory. </title> <journal> ACM TOPLAS 3:4:353-387, </journal> <year> 1981. </year>
Reference-contexts: Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence [Mon74, Mac77, Fre78, Ste80], in graphical-interfaces <ref> [Bor81] </ref>, and in logic programming languages [JL87, DVSAGB88, Col80]. One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP [DVSAGB88, VanH].
Reference: [BJM93] <author> A. Brodsky, J. Jaffar, M.J. Maher. </author> <title> Towards Practical Constraint Databases. </title> <booktitle> Proc. 19th International Conference on Very Large Data Bases, Dublin, Ireland, </booktitle> <pages> 322-331, </pages> <year> 1993. </year>
Reference-contexts: A perfect example is spatiotemporal data, which consists of points in time and/or in space <ref> [BJM93, BLLM95, PVV94, VGVG95, Cho94] </ref>, typical of the applications mentioned at the beginning of the chapter. Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success. <p> As a result, average tuple size, can only be determined by an estimate based on sampling. For several reasons discussed in <ref> [BJM93] </ref>, the sampling techniques used in relational databases are inappropriate in the constraint database setting and need to be reconsidered. Also, the notion of image size is not trivial to generalize. <p> lead to efficient query optimization in constraint databases. 25 3.4 Lazy Evaluation of (Non)Linear Constraint Algebras In this section, we consider an alternate approach to optimizing CQA queries, in particular the positive fragment consisting of the operations project, select, natural-join, union, and rename; this is similar to the approach of <ref> [BJM93] </ref>. We suggest that for (general) linear constraints, as well as for nonlinear constraints, it is worthwhile to leverage the performance on a tuple representation that contains existentially quantified (but not eliminated) variables. We call such variables extraneous, as opposed to essential. <p> This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages [Col80, DVSAGB88, JL87] or constraint query languages <ref> [BNW91, BJM93, KSW90, KKR95] </ref>. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work. <p> This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see <ref> [BJM93, KRVV93] </ref>) and the main motivation for this work.
Reference: [BG96] <author> A. Brodsky, D. Q. Goldin, V. Segal. </author> <title> On Strongly Polynomial Projections in d-monotone Constraint Databases. </title> <booktitle> Workshop on Constraints and Databases, 2st Int'l Conf. on the Principles and Practice of Constraint Programming, </booktitle> <address> Cambridge Massachusetts, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Sections 5-7 represent original work, most of which has already been published <ref> [KG94, GK95, CGK96, GK96, BG96] </ref>.
Reference: [BK95] <author> A. Brodsky and Y. Kornatzky. </author> <title> The LyriC Language: Constraining Objects. </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <address> San Jose, Cal-ifornia, </address> <year> 1995. </year>
Reference: [BLLM95] <author> A. Brodsky, C. Lassez, J-L. Lassez, and M.J. Maher. </author> <title> Separability of Polyhedra for Optimal Filtering of Spatial and Constraint Data. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <address> San Jose, California, </address> <year> 1995. </year>
Reference-contexts: A perfect example is spatiotemporal data, which consists of points in time and/or in space <ref> [BJM93, BLLM95, PVV94, VGVG95, Cho94] </ref>, typical of the applications mentioned at the beginning of the chapter. Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success.
Reference: [CK70] <author> D.R.Chand, S.S. Kapur. </author> <title> An Algorithm for Convex Polytopes. </title> <journal> JACM, </journal> <volume> 17:1:78-86, </volume> <year> 1970. </year>
Reference-contexts: This is done using standard techniques from CAD, summarized below. Each generalized tuple t corresponds to a convex set P t of n-dimensional points, bounded by n-dimensional half-planes. The boundary representation of P t is computed by a convex hull algorithm, such as the "gift wrapping" method in <ref> [CK70] </ref>. B (R) is obtained by unioning together the boundary representations of the individual tuples, a standard Solid Modeling operation. See [FvDFH] for an introduction to polyhedral Solid Modeling, or [MM] for details of the algorithms. These algorithms extend to an arbitrary number of dimensions, as in [PS86]. Step 2.
Reference: [CH82] <author> A. K. Chandra and D. Harel. </author> <title> Structure and Complexity of Relational Queries. </title> <journal> J. Comp. System Sci., </journal> <volume> 25 </volume> <pages> 99-128, </pages> <year> 1982. </year>
Reference-contexts: There has been work on the complexity of constraint database queries ( [KKR95, GST94, PVV95]), concentrating on data complexity (i.e. treating the number of variables k in a query as a constant, see <ref> [CH82, Var82] </ref>). This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving.
Reference: [Cho94] <author> J. Chomicki. </author> <title> Temporal Query Languages: A Survey. Temporal Logic, </title> <booktitle> First International Conference, </booktitle> <editor> editors D.M. Gabbay and H.J. Ohlbach, </editor> <publisher> Springer-Verlag, LNAI 827, </publisher> <pages> 506-534, </pages> <year> 1994. </year>
Reference-contexts: A perfect example is spatiotemporal data, which consists of points in time and/or in space <ref> [BJM93, BLLM95, PVV94, VGVG95, Cho94] </ref>, typical of the applications mentioned at the beginning of the chapter. Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success.
Reference: [CK95] <author> J. Chomicki and G. Kuper. </author> <title> Measuring Infinite Relations. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <pages> 78-85, </pages> <year> 1995. </year>
Reference: [CGK96] <author> J. Chomicki, D. Goldin, G. Kuper. </author> <title> Variable Independence and Aggregation Closure. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <pages> 40-48, </pages> <year> 1996. </year>
Reference-contexts: We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Chapters 2-7 represent original work, most of which has already been published <ref> [KG94, CGK96, GK96, GK95] </ref>. Further work is in progress in the areas of aggregation, monotone constraints, and similarity querying. 11 Chapter 2 Constraint Query Algebras Constraint Query Algebras are a generalization of Relational Algebra for different classes of constraints. <p> In section 6.7, we present some results about inferring variable independence in relational algebra expressions. The extended abstract of the work presented here, written with Gabi Kuper and Jan Chomicki, has appeared in <ref> [CGK96] </ref>. 6.2 Aggregation in Constraint Databases We view a set of variables fx 1 , . . . , x k g as a relation schema and generalized relations over fx 1 , . . . , x k g as instances of this schema. <p> equivalence follows from the following facts: (a) x satisfies some relevant S iff x = t [X] for some t 2 R; (b) given x satisfying some relevant S, f (A S ) = f (G x ). 2 Note: The above proof is different from the original proof in <ref> [CGK96] </ref>, which was due to G. Kuper. To make restricted aggregation practical, we must be able to determine effectively, for a generalized database fR 1 ; : : : ; R n g and a relational expression e hX; f i, whether X and U X are independent in e. <p> In contrast to the previous section, the results in this section are applicable to any constraint language closed under negation and admitting quantifier elimination, not just linear arithmetic constraints. The notation and the inference rules presented below are from <ref> [CGK96] </ref>, due to Jan Chomicki. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Sections 5-7 represent original work, most of which has already been published <ref> [KG94, GK95, CGK96, GK96, BG96] </ref>.
Reference: [CI89] <author> J. Chomicki, T. Imielinski. </author> <title> Relational Specifications of Infinite Query Answers. </title> <booktitle> Proc. ACM SIGMOD, </booktitle> <pages> 174-183, </pages> <year> 1989. </year> <month> 103 </month>
Reference-contexts: The CDB framework has provided a unified view of some previous database research: for example, on the power of constraints for the implicit specification of temporal data <ref> [CI89] </ref>, on relational query safety [AGSS86], on conjunctive queries with inequalities [Klu88] and on extending magic sets [Ram88].
Reference: [Cod70] <author> E.F. Codd. </author> <title> A Relational Model for Large Shared Data Banks. </title> <journal> CACM, </journal> <volume> 13:6:377-387, </volume> <year> 1970. </year>
Reference-contexts: The declarative style of database query languages is an important aspect of database systems, that has been at the core of the relational data model since Codd's pioneering work <ref> [Cod70] </ref> on the declarative relational calculus and its equivalence to the procedural relational algebra. Indeed, having such languages for ad-hoc database querying is a requirement in today's relational technology (see [AHV, Kan90, Ull]. <p> In many cases, the calculi have polynomial time data complexity. This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving. 1.4 Procedural Querying of CDBs The relational data model was pioneered in <ref> [Cod70] </ref>. From that work, follows that Relational Calculus on finite sets can be evaluated bottom-up in closed form. Relational Algebra, an operator-based query paradigm with bottom-up expression evaluation semantics and LOGSPACE data complexity [Var82], is just the query language whose existence was foreseen in [Cod70]. <p> relational data model was pioneered in <ref> [Cod70] </ref>. From that work, follows that Relational Calculus on finite sets can be evaluated bottom-up in closed form. Relational Algebra, an operator-based query paradigm with bottom-up expression evaluation semantics and LOGSPACE data complexity [Var82], is just the query language whose existence was foreseen in [Cod70]. The algebraic querying paradigm is not declarative, since the algebraic expressions represent a `plan' or a `recipe' for evaluating a query. Relational Algebras play a very important role in relational database theory, since they are more useful than the calculi for carrying out query optimization and evaluation. <p> Definition 3 The syntax of a Constraint Query Calculus is the union of a relational database query language and formulas in a decidable logical theory. For example: Relational calculus <ref> [Cod70] </ref> + the theory of real closed fields [Tar, Ren92]; Relational calculus (or even Inflationary Datalog : , [AHV] + the theory of dense order with constants [FG77]. <p> The analogue for the relational model is that relations are finite structures, and queries are supposed to preserve this finiteness. This is a requirement that creates various "safety" problems in relational databases <ref> [Cod70, Ull] </ref>. The precise analogue in relational databases is the notion of weak safety of [AGSS86]. Evaluation of a query corresponds to an instance of a decision problem. Quantifier elimination procedures realize the goal of closed form and use induction on the structure of formulas, which leads to bottom-up evaluation. <p> A canonical database is a finite set of generalized relations. 4.3 The Algebraic Operations We can now introduce the syntax and the semantics of an algebra over canonical relations. As in Codd's relational algebra <ref> [Cod70, Kan90] </ref>, we define basic algebraic operations projection (), selection (&), natural join (1), union ([), difference (), and renaming (%), which map one or more canonical relations to a new one.
Reference: [Col80] <author> A. Colmerauer. </author> <title> An Introduction to Prolog III. </title> <journal> CACM, </journal> <volume> 33:7:69-90, </volume> <year> 1990. </year>
Reference-contexts: Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence [Mon74, Mac77, Fre78, Ste80], in graphical-interfaces [Bor81], and in logic programming languages <ref> [JL87, DVSAGB88, Col80] </ref>. One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP [DVSAGB88, VanH]. <p> One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III <ref> [Col80] </ref>, and in CHIP [DVSAGB88, VanH]. The insight that led to CLP is: the unification mechanism of standard Logic Programming can be regarded as a trivial constraint solver (for equality constraints only). Expressiveness is therefore gained by replacing unification with constraint solving, and allowing constraints in logic programs. <p> We show how various query variations can be expressed and translated into the internal representation of Section 2. This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages <ref> [Col80, DVSAGB88, JL87] </ref> or constraint query languages [BNW91, BJM93, KSW90, KKR95]. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work.
Reference: [Dec92] <author> R. Dechter. </author> <title> From Local to Global Consistency. </title> <journal> Artificial Intelligence, </journal> <volume> 55 </volume> <pages> 87-107, </pages> <year> 1992. </year>
Reference-contexts: This is what happens with relational algebra where projection is restriction and (depending on the representation of sets used) duplicate elimination. A set of constraints is globally consistent <ref> [Fre82, Dec92] </ref> when the projection of the solution set on any subset of the variables can be computed just this way, via restriction. 17 This approach is studied in Chapter 4. The example is an algebra for dense order constraints, which was published in [KG94] in preliminary form. <p> Syntactically, the variables in t 0 have the same bounds and the same constraints as the corresponding variables in t, and all other bounds and constraints are dropped. (Note: Projection is restriction, just as for the standard relational algebra; this corresponds to the notion of global consistency <ref> [Fre82, Dec92] </ref> for canonical tuples). Example 3 Let r = ft 1 ; t 2 g be a canonical relation over variables (A; B; C). We compute (A;C) (r) = ft 0 0 2 g (see Figure 4.1). Selection.
Reference: [DMP91] <author> R. Dechter, I. Meiri, J. Pearl. </author> <title> Temporal Constraint Networks. </title> <journal> Artificial Intelligence, </journal> <volume> 49 </volume> <pages> 61-95, </pages> <year> 1991. </year>
Reference-contexts: Thus, the problem of a strongly polynomial algebra for two-variable linear constraints is open. 18 In Artificial Intelligence, the property of global consistency, also known as decom-posability [Mon74], is desirable because it allows a backtrack-free search [Fre82]. The property has been studied for temporal constraints <ref> [DMP91] </ref> where it was shown that: "a decomposable constraint set equivalent to a given one can be found in time polynomial in the size of the constraints for any number of variables k". (For an extensive treatment of temporal databases using constraint programming see [Kou93]).
Reference: [DVSAGB88] <author> M. Dincbas, P. Van Hentenryck, H. Simonis, A. Aggoun, T. Graf, F. Berthier. </author> <title> The Constraint Logic Programming Language CHIP. </title> <booktitle> Proc. Fifth Generation Computer Systems, </booktitle> <address> Tokyo Japan, </address> <year> 1988. </year>
Reference-contexts: Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence [Mon74, Mac77, Fre78, Ste80], in graphical-interfaces [Bor81], and in logic programming languages <ref> [JL87, DVSAGB88, Col80] </ref>. One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP [DVSAGB88, VanH]. <p> One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP <ref> [DVSAGB88, VanH] </ref>. The insight that led to CLP is: the unification mechanism of standard Logic Programming can be regarded as a trivial constraint solver (for equality constraints only). Expressiveness is therefore gained by replacing unification with constraint solving, and allowing constraints in logic programs. <p> We show how various query variations can be expressed and translated into the internal representation of Section 2. This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages <ref> [Col80, DVSAGB88, JL87] </ref> or constraint query languages [BNW91, BJM93, KSW90, KKR95]. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work.
Reference: [Fal96] <author> C. Faloutsos. </author> <title> Searching Multimedia Databases by Content Kluwer Academic Publishers, </title> <year> 1996 </year>
Reference-contexts: In fact, the decrease is as O (f b ) <ref> [Fal96] </ref>, where: * b = 1 for pink noise, such as musical scores; * b = 2 for brown noise, such as stock movements and exchange rates; * b &gt; 2 for black noise, such as rainfall patterns.
Reference: [FRM94] <author> C. Faloutsos, M. Ranganathan, Y. Manolopoulos. </author> <title> Fast Subsequence Matching in Time-Series Databases. </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <pages> 419-429, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in <ref> [FRM94] </ref>. Chapters 2-7 represent original work, most of which has already been published [KG94, CGK96, GK96, GK95]. <p> In this paper, we formalize the intuitive notions of exact and approximate similarity between time-series patterns and data. Our definition of similarity extends the distance metric used in <ref> [AFS93, FRM94] </ref> with invariance under a group of transformations. Our main observation is that the resulting, more expressive, set of constraint queries can be supported by a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in [AFS93, FRM94]. 7.1 Problem Definition 7.1.1 Approximate Matching of <p> of similarity extends the distance metric used in <ref> [AFS93, FRM94] </ref> with invariance under a group of transformations. Our main observation is that the resulting, more expressive, set of constraint queries can be supported by a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in [AFS93, FRM94]. 7.1 Problem Definition 7.1.1 Approximate Matching of Time-Series Data Time-series are the principal format of data in many applications, from financial to scientific. Time-series data are sequences of real numbers representing measurements at uniformly-spaced temporal instances. <p> However, any proposal 71 of such linguistic facilities must be supported by indexing (i.e., be implementable with reasonable I/O efficiency) for very large data sets. Examples of recent database research towards this goal include <ref> [AFS93, FRM94, SLR94] </ref>. A most basic problem in this area is First-Occurrence Subsequence Matching, defined as follows: given a query sequence Q of length n and a much longer data sequence S of length N , find the first occurrence of a contiguous subsequence within S that matches Q exactly. <p> The All-Occurrences Approximate Matching problems (either Subsequence or Whole-Sequence) are defined as before, but with "match approximately within tolerance *" instead of "match exactly". For external solutions to the All-Occurrence Whole-Sequence Approximate version, we refer to [AFS93, SLR94]; for the All-Occurrence Subsequence Approximate version, we refer to <ref> [FRM94] </ref>. A further characteristic of time-series data that is used to advantage, is that they have a skewed energy spectrum, to use the terminology borrowed from Discrete Signal Processing [DS]. <p> Analogous definitions apply for All-Occurrences Exact Similarity problems (either Subsequence or Whole-Sequence). 74 When T is restricted to be the identity transformation, the various similarity prob-lems become the matching problems of the last section. In this sense, our work is a generalization of the work of <ref> [FRM94] </ref>. This generalization is in the direction of [Jag91], which discusses translation and distortion transformations but does not provide the guarantees of [FRM94] and of our indexing scheme. A general framework for similarity queries is described in [JMM95]. Our work happens to be (an efficiently solvable) special case. <p> In this sense, our work is a generalization of the work of <ref> [FRM94] </ref>. This generalization is in the direction of [Jag91], which discusses translation and distortion transformations but does not provide the guarantees of [FRM94] and of our indexing scheme. A general framework for similarity queries is described in [JMM95]. Our work happens to be (an efficiently solvable) special case. <p> We show this using a new indexing technique, which preserves all the desirable properties of the indexing scheme proposed in <ref> [AFS93, FRM94] </ref>. In Section 2, we provide a semantics for similarity querying where we use the similarity distance between Q and S (defined in Section 7.2.2) as the distance metric. <p> In Section 3, we show that the semantics of Section 2.2 has several desirable properties, such as updateability and well-behaved trails, which allow us to provide efficient 75 implementations for similarity querying, both in the internal and external query set-ting. We first adapt the criteria put forth in <ref> [AFS93, FRM94] </ref> (Section 3.1) and satisfy them using fingerprints of the normal form (Section 3.2). We then argue that fingerprints are incrementally computable (Section 3.3), can be used ala Rabin-Karp [KR87] for internal searching (Section 3.4), and most importantly external indexing (Section 3.5). <p> This is the implementation technology that is needed to support the internal representation of Section 2. Our new indexing technique combines the MBR structure of <ref> [FRM94] </ref> with our internal representation. Many spatial data-structures can be used, for examples varieties of R-trees (see [Sam] for a comprehensive survey of the available external data-structures). In Section 4, we provide a constraint syntax for similarity querying. <p> We now define the fingerprint function F as well as the fingerprint distance function D F . These definitions are similar to the ones used for Approximate Matching in [AFS93] and <ref> [FRM94] </ref>. <p> This is due to a very simple observation: for a real sequence of length m, the there would be m n + 1 indices with 2l reals each. Such space overhead renders indexing less efficient than a direct sequential scan of the data <ref> [FRM94] </ref>. This problem is overcome with the Minimum Bounding Rectangle (MBR) technique, introduced in [FRM94]. This technique significantly reduces the size of the indexing structure, though introducing some false alarms in the process. Our final indexing method consists of combining the MBR technique with the spatial access approach described above. <p> Such space overhead renders indexing less efficient than a direct sequential scan of the data <ref> [FRM94] </ref>. This problem is overcome with the Minimum Bounding Rectangle (MBR) technique, introduced in [FRM94]. This technique significantly reduces the size of the indexing structure, though introducing some false alarms in the process. Our final indexing method consists of combining the MBR technique with the spatial access approach described above. <p> We conclude the discussion of indexing by verifying that our indexing possesses the continuity property. This is a "heuristic" statistical argument, that also applies to <ref> [FRM94] </ref> (where continuity was assumed, but not shown). <p> As with a generalized version of any problem, there is a trade-off between a gain in expressibility and a decrease in performance, though it is not significant. There is some slow-down due to the extra keys in the new indexing scheme (as opposed to <ref> [FRM94] </ref>), as well as due to the additional false alarms generated by our fingerprinting (as compared to the specialized cases without a similarity transformation). We are currently examining these trade-off through performance evaluations; this experimental work is in progress. Some other questions, more theoretical in nature, remain. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in <ref> [FRM94] </ref>. Sections 5-7 represent original work, most of which has already been published [KG94, GK95, CGK96, GK96, BG96].
Reference: [FG77] <author> J. Ferrante, J.R. Geiser. </author> <title> An Efficient Decision Procedure for the Theory of Rational Order. </title> <journal> Theoretical Computer Science, </journal> <volume> 4 </volume> <pages> 227-233, </pages> <year> 1977. </year>
Reference-contexts: For example: Relational calculus [Cod70] + the theory of real closed fields [Tar, Ren92]; Relational calculus (or even Inflationary Datalog : , [AHV] + the theory of dense order with constants <ref> [FG77] </ref>. Definition 4 The semantics of CQC is based on that of the decidable logical theory, by interpreting database atoms as shorthands for formulas of the theory. <p> The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. In Chapter 4, we present the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraint tuples, and define the algebraic operations over this representation. We apply the principle of semantic closure to establish the correctness of the dense-order algebra. We note that this algebra satisfies all the criteria for a good algebra presented in in Chapter 2. <p> Constants, =, , and &lt; are interpreted respectively as elements, equality, the dense order, and the irreflexive dense order of D. For the first-order theory of dense order see <ref> [FG77] </ref> and for its data complexity (with and without recursion) see [KKR95]. Most commonly, dense order constraints are used to represent (multi-dimensional) rectangles, or intersections of rectangles and diagonal hyperplanes. Their expressibility suffices in many spatial database applications. <p> We then highlighted the importance of the syntax-semantics duality on two im-plementational issues in particular: representation of data and implementation of the 95 algebraic operations. In Chapter 4, we presented the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraints, and define the algebraic operations over the data. We established the correctness of the dense-order algebra, by proving the commutativity of its syntactic and semantic interpretations.
Reference: [FvDFH] <author> J.D.Foley, A. van Dam, S.K.Feiner, J.F. </author> <title> Hughes Computer Graphics, </title> <booktitle> Principles and Practice. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: The boundary representation of P t is computed by a convex hull algorithm, such as the "gift wrapping" method in [CK70]. B (R) is obtained by unioning together the boundary representations of the individual tuples, a standard Solid Modeling operation. See <ref> [FvDFH] </ref> for an introduction to polyhedral Solid Modeling, or [MM] for details of the algorithms. These algorithms extend to an arbitrary number of dimensions, as in [PS86]. Step 2. The next step of the algorithm is to create a vertex grid partitioning R of R.
Reference: [Fre78] <author> E. Freuder. </author> <title> Synthesizing Constraint Expressions. </title> <journal> CACM, </journal> <volume> 21:11, </volume> <year> 1978. </year>
Reference-contexts: Programming with constraints as primitives (or constraint programming) is appealing because constraints are the normal language of discourse for many high-level applications. Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence <ref> [Mon74, Mac77, Fre78, Ste80] </ref>, in graphical-interfaces [Bor81], and in logic programming languages [JL87, DVSAGB88, Col80].
Reference: [Fre82] <author> E. Freuder. </author> <title> A sufficient condition for backtrack-free search. </title> <journal> CACM, </journal> <volume> 29:1, </volume> <year> 1982. </year>
Reference-contexts: This is what happens with relational algebra where projection is restriction and (depending on the representation of sets used) duplicate elimination. A set of constraints is globally consistent <ref> [Fre82, Dec92] </ref> when the projection of the solution set on any subset of the variables can be computed just this way, via restriction. 17 This approach is studied in Chapter 4. The example is an algebra for dense order constraints, which was published in [KG94] in preliminary form. <p> Thus, the problem of a strongly polynomial algebra for two-variable linear constraints is open. 18 In Artificial Intelligence, the property of global consistency, also known as decom-posability [Mon74], is desirable because it allows a backtrack-free search <ref> [Fre82] </ref>. <p> Syntactically, the variables in t 0 have the same bounds and the same constraints as the corresponding variables in t, and all other bounds and constraints are dropped. (Note: Projection is restriction, just as for the standard relational algebra; this corresponds to the notion of global consistency <ref> [Fre82, Dec92] </ref> for canonical tuples). Example 3 Let r = ft 1 ; t 2 g be a canonical relation over variables (A; B; C). We compute (A;C) (r) = ft 0 0 2 g (see Figure 4.1). Selection.
Reference: [GK95] <author> D.Q. </author> <title> Goldin and P.C. Kanellakis. On Similarity Queries for Time-Series Data: Constraint Specification and Implementation. </title> <booktitle> Proc. 1st Int'l Conf. on the Principles and Practice of Constraint Programming, </booktitle> <volume> LNCS 976, </volume> <pages> 137-153, </pages> <address> Cassis France, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Chapters 2-7 represent original work, most of which has already been published <ref> [KG94, CGK96, GK96, GK95] </ref>. Further work is in progress in the areas of aggregation, monotone constraints, and similarity querying. 11 Chapter 2 Constraint Query Algebras Constraint Query Algebras are a generalization of Relational Algebra for different classes of constraints. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Sections 5-7 represent original work, most of which has already been published <ref> [KG94, GK95, CGK96, GK96, BG96] </ref>.
Reference: [GK96] <author> D.Q. </author> <title> Goldin and P.C. Kanellakis. Constraint Query Algebras. Constraints Journal, </title> <booktitle> 1st issue (E. Freuder editor), </booktitle> <pages> 1-41, </pages> <year> 1996. </year> <month> 104 </month>
Reference-contexts: We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Chapters 2-7 represent original work, most of which has already been published <ref> [KG94, CGK96, GK96, GK95] </ref>. Further work is in progress in the areas of aggregation, monotone constraints, and similarity querying. 11 Chapter 2 Constraint Query Algebras Constraint Query Algebras are a generalization of Relational Algebra for different classes of constraints. <p> By Lemma 8, the union of the resulting sets is globally consistent. 2 55 Chapter 6 Variable Independence and Aggregation Closure A query language resulting from adding aggregation to a constraint query calculus [KKR95] or algebra <ref> [GK96] </ref> should be well behaved. However, unrestricted use of aggregation may fail to produce a closed language, as shown in [Kup94]. In this section, we approach this problem by restricting the way aggregate operators are used in constraint queries. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Sections 5-7 represent original work, most of which has already been published <ref> [KG94, GK95, CGK96, GK96, BG96] </ref>.
Reference: [GK94] <author> P. Gritzmann and V. Klee. </author> <title> On the Complexity of Some Basic Problems in Computational Convexity: II. Volume and Mixed Volumes. </title> <type> Technical Report TR-94-31, </type> <institution> DIMACS, Rutgers University, </institution> <address> New Brunswick, NJ, </address> <year> 1994. </year>
Reference-contexts: One of them is the generalized version of the "aggregation" operator, a general operator that expresses statistical operations such as avg, min, count [Klu82]. Some aggregation operators like count are not applicable to infinite relations. On the other hand, new operators like area [AS91] (or its generalization, n-dimensional volume <ref> [GK94] </ref>) occur there quite naturally. [Kup94] describes a general framework, modeled after [Klu82], for adding aggregate operators to relational algebra and calculus.
Reference: [GK97] <author> S. Grumbach, G. Kuper. </author> <title> Tractable Query Languages for Geometric and Topological Data. </title> <note> Submitted to ACM PODS, </note> <year> 1997. </year>
Reference-contexts: A well-behaved recursion operation for Constraint Databases should have the standard fixpoint semantics of recursion and the queries should be guaranteed to terminate and to have a finitely representable answer. Some progress in this area has been made very recently <ref> [GK97] </ref>. The operators defined there, the bounded partial fixpoint and the bounded inflationary fixpoint, lead to some new results in expressiveness and complexity of PTIME boolean queries over linear constraint databases.
Reference: [GS95] <author> S. Grumbach, J. Su. </author> <title> Dense Order Constraint Databases. </title> <booktitle> Proc. 14th ACM PODS, </booktitle> <month> May </month> <year> 1995, </year> <pages> 66-77. </pages>
Reference-contexts: Most commonly, dense order constraints are used to represent (multi-dimensional) rectangles, or intersections of rectangles and diagonal hyperplanes. Their expressibility suffices in many spatial database applications. Note that the geometry of dense order constraints is richer than that presented in <ref> [GS95] </ref>; for example, some rectangles with a cut-off corner, though representable with dense order constraints: (3 &lt; x &lt; 5; 1 &lt; y &lt; 4; x &gt; y), was not mentioned there. In this section we present a CQA for dense order constraints.
Reference: [GST94] <author> S. Grumbach, J. Su, C. Tollu. </author> <title> Linear Constraint Query Languages: Expressive Power and Complexity. </title> <booktitle> Proc. Workshop on Finite Model Theory, </booktitle> <address> Indiana, </address> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success. There has been work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>), concentrating on data complexity (i.e. treating the number of variables k in a query as a constant, see [CH82, Var82]). This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving. <p> Thus, constraint databases are a natural generalization of the relational model of data by allowing infinite relations that are finitely representable using constraints. The work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>) has concentrated on data complexity, treating the number of variables k in a query as a constant. In many cases, the calculi have polynomial time data complexity. <p> Note that it is relatively easy to transform a Constraint Query Calculus into an "naive" algebra, one where operations are defined as syntactic manipulations of constraint formulas. For example, with Fourier-Motzkin elimination [Sch] one easily derives a "naive" algebra for linear constraint databases <ref> [GST94] </ref>. In this work, we will show that the "naive" approach is not sufficient to define a 5 good Constraint Query Algebra, i.e. one that preserves the practical advantages of the algebraic querying paradigm. We will provide the syntax and semantics of CQAs, with the goal of preserving these advantages.
Reference: [HN94] <author> D. S. Hochbaum, J. Naor. </author> <title> Simple and Fast Algorithms for Linear and Integer Programs with two Variables per Inequality. </title> <journal> SIAM J. Comput., </journal> <volume> 23:6:1179-1192, </volume> <year> 1994. </year>
Reference-contexts: The existence of an algorithm for linear programming that is strongly polynomial, i.e., where the complexity does not depend on the coefficient sizes, is a major open question. Strongly polynomial bounds have been achieved for the linear programming problem over sets of two-variable linear constraints <ref> [HN94] </ref>; its time complexity is O (mk 2 logm). They present a modification of the Fourier-Motzkin algorithm, pruning away most of the constraints that are generated, while preserving equisatisfiability of the constraint sets. The actual equivalence of the sets is not preserved; indeed, it is not needed to determine feasibility. <p> We consider monotone constraints over the rationals, and show that in most cases, projection is strongly polynomial. The more complex integer case for monotone constraints is analyzed by Hochbaum and Naor <ref> [HN94] </ref>. 2.5 Additional Algebraic Operators Aggregation. To be able to express practical queries, some additional operators are needed, just as for relational query languages. One of them is the generalized version of the "aggregation" operator, a general operator that expresses statistical operations such as avg, min, count [Klu82]. <p> Note that the number of such paths can be exponential in the size of M. We will show that we can prune most of the edges created at each stage of the node elimination algorithm, while preserving network equivalence. This is similar to the approach of <ref> [HN94] </ref>, where the Fourier-Motzkin algorithm is modified to solve the feasibility problem efficiently. There, the equisatisfiability of the constraint sets is preserved at each stage; this condition is necessary to guarantee correctness of the feasibility algorithm.
Reference: [JL87] <author> J. Jaffar, J.L. Lassez. </author> <title> Constraint Logic Programming. </title> <booktitle> Proc. 14th ACM POPL, </booktitle> <pages> 111-119, </pages> <year> 1987. </year>
Reference-contexts: Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence [Mon74, Mac77, Fre78, Ste80], in graphical-interfaces [Bor81], and in logic programming languages <ref> [JL87, DVSAGB88, Col80] </ref>. One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP [DVSAGB88, VanH]. <p> One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) <ref> [JL87] </ref>, in Prolog III [Col80], and in CHIP [DVSAGB88, VanH]. The insight that led to CLP is: the unification mechanism of standard Logic Programming can be regarded as a trivial constraint solver (for equality constraints only). <p> We show how various query variations can be expressed and translated into the internal representation of Section 2. This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages <ref> [Col80, DVSAGB88, JL87] </ref> or constraint query languages [BNW91, BJM93, KSW90, KKR95]. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work.
Reference: [Jag91] <author> H.V. Jagadish. </author> <title> A Retrieval Technique for Similar Shapes. </title> <booktitle> Proc. ACM SIGMOD Conf., </booktitle> <pages> 208-217, </pages> <month> May </month> <year> 1991 </year>
Reference-contexts: In this sense, our work is a generalization of the work of [FRM94]. This generalization is in the direction of <ref> [Jag91] </ref>, which discusses translation and distortion transformations but does not provide the guarantees of [FRM94] and of our indexing scheme. A general framework for similarity queries is described in [JMM95]. Our work happens to be (an efficiently solvable) special case.
Reference: [JMM95] <author> H. V. Jagadish, A. O. Mendelzon, T. Milo. </author> <title> Similarity-Based Queries. </title> <booktitle> Proc. 14th ACM PODS, </booktitle> <year> 1995 </year>
Reference-contexts: In this sense, our work is a generalization of the work of [FRM94]. This generalization is in the direction of [Jag91], which discusses translation and distortion transformations but does not provide the guarantees of [FRM94] and of our indexing scheme. A general framework for similarity queries is described in <ref> [JMM95] </ref>. Our work happens to be (an efficiently solvable) special case. The [JMM95] framework for similarity-based queries has three components: a pattern language P , an approximation language (they refer to it as transformation rule language), and a query language. <p> This generalization is in the direction of [Jag91], which discusses translation and distortion transformations but does not provide the guarantees of [FRM94] and of our indexing scheme. A general framework for similarity queries is described in <ref> [JMM95] </ref>. Our work happens to be (an efficiently solvable) special case. The [JMM95] framework for similarity-based queries has three components: a pattern language P , an approximation language (they refer to it as transformation rule language), and a query language. In our case, P is the set of allowable transformations on the query sequence Q. <p> In our case, the approximations are the distortions in the time-series data (i.e., the jiggling of individual points); the cost is the distance between the original sequence and the distorted one. Note that membership testing in the <ref> [JMM95] </ref> framework is at best exponential; thus this framework is too general for our purposes. 7.1.3 An Overview of Similarity Querying Our main contribution is: A syntax and semantics for similarity queries, that account for approximate matching, scaling and shifting, and that have efficient indexing support.
Reference: [KSW90] <author> F. Kabanza, J-M. Stevenne, P. Wolper. </author> <title> Handling Infinite Temporal Data. </title> <booktitle> Proc. 9th ACM PODS, </booktitle> <pages> 392-403, </pages> <year> 1990. </year>
Reference-contexts: This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages [Col80, DVSAGB88, JL87] or constraint query languages <ref> [BNW91, BJM93, KSW90, KKR95] </ref>. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work. <p> Possible extensions involve more powerful similarity queries and different distance functions; also indexing of time-series data that is represented using constraints (see <ref> [BNW91, KSW90] </ref>). In addition to implementing the general version of similarity querying, as described in this chapter, we have implemented other versions, by tailoring the internal representation and the corresponding indexing scheme for specialized subsets of similarity queries.
Reference: [Kan90] <author> P.C. Kanellakis. </author> <title> Elements of Relational Database Theory. </title> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <editor> (J. van Leeuwen editor), </editor> <volume> volume B, chapter 17, </volume> <pages> 1073-1158, </pages> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: Indeed, having such languages for ad-hoc database querying is a requirement in today's relational technology (see <ref> [AHV, Kan90, Ull] </ref>. CQLs can be viewed as a specialized form of constraint programming, similar to the way that relational query languages can be viewed as a form of first-order theorem proving. Constraint programming paradigms are inherently declarative, since they implicitly describe computations by specifying how these computations are constrained. <p> The positive fragment of Relational Algebra consists of the above operations except Difference. Note that Relational Algebra is equivalent to the domain-independent Relational Calculus for both finite and infinite (i.e., unrestricted) relations <ref> [Kan90] </ref>. 1.6 Principles of Constraint Query Languages As a background for the rest of the work, we present a summary of the framework from [KKR95]. <p> A canonical database is a finite set of generalized relations. 4.3 The Algebraic Operations We can now introduce the syntax and the semantics of an algebra over canonical relations. As in Codd's relational algebra <ref> [Cod70, Kan90] </ref>, we define basic algebraic operations projection (), selection (&), natural join (1), union ([), difference (), and renaming (%), which map one or more canonical relations to a new one. <p> has the following closure property: if r 0 = OP (r 1 ; : : : ; r n ), then (r 0 ) = OP ((r 1 ); : : : ; (r n )), where OP is the operation of the same name in the relational algebra in <ref> [Kan90] </ref>, and is defined in Definition 12. In our definitions of operators, we use the notation ff (r) for the variables of a canonical relation r. Projection. <p> Clearly, variable independence in a tuple is decidable. Also, note that variable independence in a tuple t may be viewed as an embedded join dependency <ref> [Kan90] </ref> holding in the unrestricted relation corresponding to t. Definition 24 We say that X and Y are independent in R if there exists a relation R 0 equivalent to R where X and Y are independent in every (generalized) tuple of R 0 .
Reference: [Kan95] <author> P.C. Kanellakis. </author> <title> Constraint Programming and Database Languages: A Tutorial. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <year> 1995. </year>
Reference-contexts: The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. In Chapter 4, we present the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraint tuples, and define the algebraic operations over this representation. We apply the principle of semantic closure to establish the correctness of the dense-order algebra. We note that this algebra satisfies all the criteria for a good algebra presented in in Chapter 2. <p> In order to match the expressiveness of the relational queries, it is necessary to allow aggregation operations in CQA expressions. In fact, generalizing aggregation 10 operators to constraint databases has been identified as one of the most important open research issues in the constraint database area <ref> [KG94, Kan95] </ref>. In Chapter 6, we provide a natural restriction on Constraint Database schema which guarantees the safety of algebraic expressions involving aggregation. This restriction, called variable independence, is a generalization of the assumptions underlying the classical relational model of data. <p> We then highlighted the importance of the syntax-semantics duality on two im-plementational issues in particular: representation of data and implementation of the 95 algebraic operations. In Chapter 4, we presented the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraints, and define the algebraic operations over the data. We established the correctness of the dense-order algebra, by proving the commutativity of its syntactic and semantic interpretations.
Reference: [KG94] <author> P.C. Kanellakis and D.Q. Goldin. </author> <title> Constraint Programming and Database Query Languages. </title> <booktitle> Symposium on Theoretical Aspects of Computer Software, </booktitle> <volume> LNCS 789, </volume> <pages> pp. 96-120, </pages> <address> Sendai Japan, </address> <month> April </month> <year> 1994. </year> <month> 105 </month>
Reference-contexts: In order to match the expressiveness of the relational queries, it is necessary to allow aggregation operations in CQA expressions. In fact, generalizing aggregation 10 operators to constraint databases has been identified as one of the most important open research issues in the constraint database area <ref> [KG94, Kan95] </ref>. In Chapter 6, we provide a natural restriction on Constraint Database schema which guarantees the safety of algebraic expressions involving aggregation. This restriction, called variable independence, is a generalization of the assumptions underlying the classical relational model of data. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Chapters 2-7 represent original work, most of which has already been published <ref> [KG94, CGK96, GK96, GK95] </ref>. Further work is in progress in the areas of aggregation, monotone constraints, and similarity querying. 11 Chapter 2 Constraint Query Algebras Constraint Query Algebras are a generalization of Relational Algebra for different classes of constraints. <p> The example is an algebra for dense order constraints, which was published in <ref> [KG94] </ref> in preliminary form. As explained in Section 2.1, the representation of tuples used in this algebra also has other good properties; this representation also easily extends to temporal constraints. <p> We provide both the description and the implementational details for a framework for similarity querying of time-series data. Similarity queries are strictly more expressive than approximate match queries, for which a framework had been supplied in [FRM94]. Sections 5-7 represent original work, most of which has already been published <ref> [KG94, GK95, CGK96, GK96, BG96] </ref>.
Reference: [KKR95] <author> P. C. Kanellakis, G. M. Kuper, P. Z. Revesz. </author> <title> Constraint Query Languages. </title> <journal> JCSS, </journal> <volume> 51:1:26-52, </volume> <month> August </month> <year> 1995. </year>
Reference-contexts: It was not immediately clear how to apply the same type of insight to Relational Databases, since the bottom-up and set-at-a-time style of database query evaluation emphasized in databases seems to contradict the top-down, depth-first intuition behind Constraint Logic Programming. In <ref> [KKR95] </ref>, the gap between database programming and constraint solving was bridged, with the following key intuition: * a tuple (or a record or ground fact) in standard relational databases can be regarded as a conjunction of equality constraints on the attributes of the tuple. <p> Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success. There has been work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>), concentrating on data complexity (i.e. treating the number of variables k in a query as a constant, see [CH82, Var82]). This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving. <p> Expressiveness is therefore gained by replacing unification with constraint solving, and allowing constraints in logic programs. This advance in CLP is very relevant for database applications. CLP was adapted to database querying by <ref> [KKR95] </ref>, who proposed a framework for Constraint Database (CDB) queries by combining bottom-up, efficient, declarative database programming with efficient constraint solving. <p> Integrating constraints with databases leads to the following: a conjunction of quantifier-free constraints over k variables, where k depends on the database schema and not the instance, 4 is an appropriate generalization of the k-tuple. In <ref> [KKR95] </ref>, finite relations are generalized to finitely representable relations and appropriate calculi for their data manipulation can be developed in this framework. Thus, constraint databases are a natural generalization of the relational model of data by allowing infinite relations that are finitely representable using constraints. <p> Thus, constraint databases are a natural generalization of the relational model of data by allowing infinite relations that are finitely representable using constraints. The work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>) has concentrated on data complexity, treating the number of variables k in a query as a constant. In many cases, the calculi have polynomial time data complexity. <p> Constraint Query Algebras (for various constraint classes) are the equivalent of Relational Algebras for the Constraint Database model. No syntax or semantics for CQAs was provided in <ref> [KKR95] </ref>; however, the existence of an efficient bottom-up evaluation strategy for dense order constraints was proven by grouping the tuples in a generalized relation into r-configurations. <p> Note that Relational Algebra is equivalent to the domain-independent Relational Calculus for both finite and infinite (i.e., unrestricted) relations [Kan90]. 1.6 Principles of Constraint Query Languages As a background for the rest of the work, we present a summary of the framework from <ref> [KKR95] </ref>. Definition 1 A generalized k-tuple, or a constraint tuple, is a quantifier-free conjunction of constraints on k variables, where these variables range over a set D. There are many kinds of generalized tuples depending on the kind of constraints used. <p> We assume a standard binary encoding of generalized relations. The notion of data complexity arose from a study of the expressibility of computations over finite structures. It corresponds nicely to the intuition that the database size is much larger than the query program size. The framework of <ref> [KKR95] </ref> imposes two critical requirements on queries: * For each input, the queries must be evaluable in closed form and bottom-up. The analogue for the relational model is that relations are finite structures, and queries are supposed to preserve this finiteness. <p> The computation and the updating of these intervals should therefore be very efficient. We shall use the term canonical form for a specific data representation format. For example, we may view the r-configurations of <ref> [KKR95] </ref> as a particular canonical form. There is an explicit algorithm provided there to convert a generalized relation into this format. <p> Constants, =, , and &lt; are interpreted respectively as elements, equality, the dense order, and the irreflexive dense order of D. For the first-order theory of dense order see [FG77] and for its data complexity (with and without recursion) see <ref> [KKR95] </ref>. Most commonly, dense order constraints are used to represent (multi-dimensional) rectangles, or intersections of rectangles and diagonal hyperplanes. Their expressibility suffices in many spatial database applications. <p> F (t 0 ) ^ ( 1 ^ ) (F (t 0 ) ^ 1 ) ^ F (t 0 ) ^ . Therefore, (F (t 0 ) ^ ) is unsatisfiable. 6. Let = (u &lt; x i &lt; b). By the work of <ref> [KKR95] </ref> on r-configurations, F (t 0 ) ^ F (t 0 ) ^ . Therefore, (F (t 0 ) ^ ) is unsatisfiable. 7. Let 0 = (u &lt; x i b); note that 1 ( 3 _ 0 ). <p> With appropriate adjustments to the definitions, all algorithms and results hold for these new tabular formats. 41 To conclude this section, we would like to compare canonical tuples with a different tuple construct used in <ref> [KKR95] </ref>, called r-configurations. <p> By Lemma 8, the union of the resulting sets is globally consistent. 2 55 Chapter 6 Variable Independence and Aggregation Closure A query language resulting from adding aggregation to a constraint query calculus <ref> [KKR95] </ref> or algebra [GK96] should be well behaved. However, unrestricted use of aggregation may fail to produce a closed language, as shown in [Kup94]. In this section, we approach this problem by restricting the way aggregate operators are used in constraint queries. <p> This translation also clarifies the relationship of the problem as defined in Section 1.2 with the semantics of Section 2.2. The syntax could be embedded in most constraint logic programming languages [Col80, DVSAGB88, JL87] or constraint query languages <ref> [BNW91, BJM93, KSW90, KKR95] </ref>. This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see [BJM93, KRVV93]) and the main motivation for this work. <p> CQAs constitute the algebraic component of the Constraint Query Language (CQL) paradigm. CQLs were first introduced in <ref> [KKR95] </ref>, with a focus on the semantics of the declarative constraint query paradigm (Constraint Query Calculi). This paper complements the original work by focusing specifically on Constraint Query Algebras, which we believe is the proper query language for considering the implementational issues of constraint query languages.
Reference: [KRVV93] <author> P. C. Kanellakis, S. Ramaswamy, D. E. Vengroff, J. S. Vitter. </author> <title> Indexing for Data Models with Constraints and Classes. </title> <booktitle> Proc. 12th ACM PODS, </booktitle> <pages> 233-243, </pages> <year> 1993. </year> <note> To appear in JCSS. </note>
Reference-contexts: This is a well-known problem with many elegant solutions from computational geometry [PS]. Optimal in-core dynamic interval management is one of the basic tools of computational geometry. However, I/O optimal solutions are non-trivial, even for the static case. For the first optimal static solution see <ref> [KRVV93] </ref> and for an optimal dynamic one see [AV95]. 3.2 Algebraic Expressions and Query Optimization The use of algebraic expressions in query optimization is based on two key notions: * Specifying a search space of semantically equivalent relational algebraic expres sions that could have different evaluation costs. 23 * Estimating the <p> This completes the connection between high level specification and implementation. The importance of combining high-level specification with efficient implementation is the common theme of constraint databases (e.g., see <ref> [BJM93, KRVV93] </ref>) and the main motivation for this work.
Reference: [KR87] <author> R. M. Karp and M. O. Rabin. </author> <title> Efficient Randomized Pattern-Matching Algorithms. </title> <journal> IBM J. Res. Develop., </journal> <volume> 31(2), </volume> <year> 1987 </year>
Reference-contexts: The part of this technology that is most related to our paper is the Rabin-Karp randomized linear-time algorithm <ref> [KR87] </ref>, which provides an efficient in-core solution based on fingerprint functions. Fingerprints are a form of sequence hashing that allow constant-time comparisons between hash values and are incrementally computable. A variant of the above problem involves finding all occurrences; this is called the All-Occurrences Subsequence Matching problem. <p> We first adapt the criteria put forth in [AFS93, FRM94] (Section 3.1) and satisfy them using fingerprints of the normal form (Section 3.2). We then argue that fingerprints are incrementally computable (Section 3.3), can be used ala Rabin-Karp <ref> [KR87] </ref> for internal searching (Section 3.4), and most importantly external indexing (Section 3.5). This is the implementation technology that is needed to support the internal representation of Section 2. Our new indexing technique combines the MBR structure of [FRM94] with our internal representation. <p> An efficient implementation of similarity querying cannot afford to compute D S every time for each sequence in the data set (for the Whole-Sequence case), or for each contiguous subsequence in the time-series (for the Subsequence case). Following the approach of <ref> [KR87] </ref>, which has gained wide acceptance, we introduce a fingerprint function F , together with a fingerprint distance metric D F . This fingerprint mechanism provides fast rejection, filtering out most of the non-similar sequences. <p> As a result, 2-3 coefficients are sufficient to provide good accuracy for most applications, including the one used for our experiments. Therefore, we may assume that the length l of the fingerprint is 3. (Note that the randomization ala Rabin-Karp <ref> [KR87] </ref> makes no assumptions about the spectrum.) 7.3.3 Updateability of Fingerprinting When computing fingerprints of all subsequences of length n for a much longer sequence of length N , the efficiency of the algorithm hinges on a property of the fingerprint that we call updateability: Given the fingerprint of a subsequence <p> The updateability results established in Section 7.3.3 allow us to answer the internal query for the in-core case with an algorithm much like that of <ref> [KR87] </ref>. We proceed through the sequences and the subsequences, comparing D F (F (Q); F (S)) to * i and checking ff (S) and (S) against the bounds. If we want to avoid run-time linear scanning of the data, we need to create an index structure for it. <p> The existence of a fingerprint function for internal similarity querying that is a real hash function but is also distance-preserving and updateable is an interesting open question. Is there a fingerprint method that gives a provably linear performance for the Rabin-Karp algorithm <ref> [KR87] </ref>, either for approximate matching or for similarity querying? Can it be truly randomized for any adversary? 94 Chapter 8 Summary In this work, we have studied Constraint Query Algebras (CQAs). CQAs constitute the algebraic component of the Constraint Query Language (CQL) paradigm.
Reference: [Klu82] <author> A. Klug. </author> <title> Equivalence of Relational Algebra and Relational Calculus Query Languages Having Aggregate Functions. </title> <journal> Journal of the ACM, </journal> <volume> 29:3:699-717, </volume> <year> 1982. </year>
Reference-contexts: To be able to express practical queries, some additional operators are needed, just as for relational query languages. One of them is the generalized version of the "aggregation" operator, a general operator that expresses statistical operations such as avg, min, count <ref> [Klu82] </ref>. Some aggregation operators like count are not applicable to infinite relations. On the other hand, new operators like area [AS91] (or its generalization, n-dimensional volume [GK94]) occur there quite naturally. [Kup94] describes a general framework, modeled after [Klu82], for adding aggregate operators to relational algebra and calculus. <p> a general operator that expresses statistical operations such as avg, min, count <ref> [Klu82] </ref>. Some aggregation operators like count are not applicable to infinite relations. On the other hand, new operators like area [AS91] (or its generalization, n-dimensional volume [GK94]) occur there quite naturally. [Kup94] describes a general framework, modeled after [Klu82], for adding aggregate operators to relational algebra and calculus. However, in general, adding aggregates to constraint databases results in langua ges that are not closed, i.e., the result of a query that uses aggregation is not finitely representable using the constraint language of the database.
Reference: [Klu88] <author> A. Klug. </author> <title> On Conjunctive Queries Containing Inequalities. </title> <journal> JACM, </journal> <volume> 35:1:146-160, </volume> <year> 1988. </year>
Reference-contexts: The CDB framework has provided a unified view of some previous database research: for example, on the power of constraints for the implicit specification of temporal data [CI89], on relational query safety [AGSS86], on conjunctive queries with inequalities <ref> [Klu88] </ref> and on extending magic sets [Ram88]. <p> The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. In Chapter 4, we present the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraint tuples, and define the algebraic operations over this representation. We apply the principle of semantic closure to establish the correctness of the dense-order algebra. We note that this algebra satisfies all the criteria for a good algebra presented in in Chapter 2. <p> We then highlighted the importance of the syntax-semantics duality on two im-plementational issues in particular: representation of data and implementation of the 95 algebraic operations. In Chapter 4, we presented the syntax for data representation for dense-order <ref> [FG77, Kan95, Klu88] </ref> and temporal constraints, and define the algebraic operations over the data. We established the correctness of the dense-order algebra, by proving the commutativity of its syntactic and semantic interpretations.
Reference: [Kou93] <author> M. Koubarakis. </author> <title> Foundations of Temporal Constraint Databases. </title> <type> PhD Thesis. </type> <institution> Nat. Tech. Univ. of Athens and Imperial College. </institution> <year> 1993. </year>
Reference-contexts: has been studied for temporal constraints [DMP91] where it was shown that: "a decomposable constraint set equivalent to a given one can be found in time polynomial in the size of the constraints for any number of variables k". (For an extensive treatment of temporal databases using constraint programming see <ref> [Kou93] </ref>). Clearly, if a class of constraints allows us to compute efficiently, for any constraint set, an equivalent globally consistent representation, then it is possible to implement fast projections for this class. A lot depends on what one considers efficient.
Reference: [Kup94] <author> G. Kuper. </author> <title> Aggregation in Constraint Databases. </title> <booktitle> PPCP'93, First International Workshop on Principles and Practice of Constraint Programming, </booktitle> <pages> 161-172. </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Some aggregation operators like count are not applicable to infinite relations. On the other hand, new operators like area [AS91] (or its generalization, n-dimensional volume [GK94]) occur there quite naturally. <ref> [Kup94] </ref> describes a general framework, modeled after [Klu82], for adding aggregate operators to relational algebra and calculus. <p> However, unrestricted use of aggregation may fail to produce a closed language, as shown in <ref> [Kup94] </ref>. In this section, we approach this problem by restricting the way aggregate operators are used in constraint queries. <p> It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog [MPR90, RS92, SSRB93, VG92, Rev95]). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in <ref> [Kup94] </ref>, relational algebra over linear constraint databases is not closed under aggregation using area. The typical example where a query is not closed is where we have a region whose boundaries vary with time, and we want to know how the area of the region varies with time. <p> As shown in <ref> [Kup94] </ref>, Klug's relational calculus and algebra with aggregation can be extended to constraint databases with minor modifications, at least as far as the underlying semantics on unrestricted relations is concerned. The definitions in [Kup94] are as follows: Definition 21 An aggregate function f maps (possibly infinite) relations with an appropriate schema <p> As shown in <ref> [Kup94] </ref>, Klug's relational calculus and algebra with aggregation can be extended to constraint databases with minor modifications, at least as far as the underlying semantics on unrestricted relations is concerned. The definitions in [Kup94] are as follows: Definition 21 An aggregate function f maps (possibly infinite) relations with an appropriate schema to the domain D of the constraints. <p> For more details, including the construction of an equivalent calculus, see <ref> [Kup94] </ref>. Generalized relational algebra with aggregation may fail to be closed even for dense order constraints [Kup94]. <p> For more details, including the construction of an equivalent calculus, see <ref> [Kup94] </ref>. Generalized relational algebra with aggregation may fail to be closed even for dense order constraints [Kup94].
Reference: [Las90] <author> J.L. Lassez. </author> <title> Querying Constraints. </title> <booktitle> 9th ACM Symposium on Principles of Database Systems, </booktitle> <pages> 288-298, </pages> <year> 1990. </year>
Reference-contexts: In this section, we provide such an independence test for linear constraint databases. As a side effect, this test generates on success a relation R equivalent to R where X and Y are independent for all tuples. Unlike the work of <ref> [Las90] </ref>, where all tests are performed on constraint sets (i.e., tuples), this test is for disjunctions of constraint sets (i.e., relations). The independence test consists of three steps: 65 Step 1.
Reference: [Mac77] <author> A.K. Mackworth. </author> <title> Consistency in Networks of Relations. </title> <booktitle> AI, </booktitle> <address> 8:1, </address> <year> 1977. </year>
Reference-contexts: Programming with constraints as primitives (or constraint programming) is appealing because constraints are the normal language of discourse for many high-level applications. Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence <ref> [Mon74, Mac77, Fre78, Ste80] </ref>, in graphical-interfaces [Bor81], and in logic programming languages [JL87, DVSAGB88, Col80].
Reference: [MM] <author> M. Mantyla. </author> <title> Solid Modeling. </title> <publisher> Computer Science Press, </publisher> <year> 1988. </year>
Reference-contexts: B (R) is obtained by unioning together the boundary representations of the individual tuples, a standard Solid Modeling operation. See [FvDFH] for an introduction to polyhedral Solid Modeling, or <ref> [MM] </ref> for details of the algorithms. These algorithms extend to an arbitrary number of dimensions, as in [PS86]. Step 2. The next step of the algorithm is to create a vertex grid partitioning R of R.
Reference: [MP] <author> Modenov and Pakhomenko. </author> <title> Geometric Transformations, </title> <publisher> Academic Press, </publisher> <year> 1965 </year>
Reference-contexts: A good time-series data-mining mechanism should be able to find similar sequences, as illustrated by these examples, up to scaling and shifting. Combinations of scaling and shifting are shape-preserving transformations, known as similarity transformations in the mathematical field of Transformational Geometry <ref> [MP] </ref>. We will approach the definition of similarity from this well established geometrical perspective: Let G be a set of transformations then two sets of points are similar if there exists a transformation, in G, which maps one to the other. In geometry, a transformation typically belongs to a group.
Reference: [Mon74] <author> U. Montanari. </author> <title> Networks of Constraints: Fundamental Properties and Application to Picture Processing. </title> <journal> Information Science, </journal> <volume> 7, </volume> <year> 1974. </year>
Reference-contexts: Programming with constraints as primitives (or constraint programming) is appealing because constraints are the normal language of discourse for many high-level applications. Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence <ref> [Mon74, Mac77, Fre78, Ste80] </ref>, in graphical-interfaces [Bor81], and in logic programming languages [JL87, DVSAGB88, Col80]. <p> Unfortunately, this makes the optimizations of Hochbaum and Naor inapplicable to the projection problem. Thus, the problem of a strongly polynomial algebra for two-variable linear constraints is open. 18 In Artificial Intelligence, the property of global consistency, also known as decom-posability <ref> [Mon74] </ref>, is desirable because it allows a backtrack-free search [Fre82].
Reference: [MFPR90] <author> I. S. Mumick, S. J. Finkelstein, H. Pirahesh, R. Ramakrishnan. </author> <title> Magic Conditions. </title> <booktitle> Proc. 9th ACM PODS, </booktitle> <pages> 314-330, </pages> <year> 1990. </year>
Reference-contexts: An important question is: how do various optimization methods (such as selection propagation, join ordering and other algebraic transformations, magic sets, etc.) combine with constraint query algebras? For some recent research in this direction we refer to <ref> [Ram88, MFPR90, SR92, SS94] </ref>. In contrast to algebraic transformations, estimation-based optimization heuristics are implementation-dependent.
Reference: [MPR90] <author> I.S. Mumick, H. Pirahesh, and R. Ramakrishnan. </author> <title> Duplicates and Aggregates in Deductive Databases. </title> <booktitle> International Conference on Very Large Data Bases, </booktitle> <month> August </month> <year> 1990. </year> <month> 106 </month>
Reference-contexts: It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog <ref> [MPR90, RS92, SSRB93, VG92, Rev95] </ref>). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in [Kup94], relational algebra over linear constraint databases is not closed under aggregation using area.
Reference: [MSW] <author> W. Mendenhall, R. Scheaffer, D. Wackerly. </author> <title> Mathematical Statistics with Ap--plications. </title> <publisher> Duxbury Press, </publisher> <address> Boston MA, </address> <year> 1986. </year>
Reference-contexts: The normalized product has some other useful properties, well-known in the field of mathematical statistics (see <ref> [MSW] </ref> for details): 89 Property 1: XY = ff X ff Y + X Y XY Property 2: 1 XY 1 Property 3: X 2 = ff 2 X + 2 Property 4: X 2 X + 2 Here, X 2 is the normalized product of X with itself. <p> Note that in our calculations, we will assume that 0 XY 1, because if XY is negative, this means that as the values in X increase, the values in Y tend to decrease, and vice versa <ref> [MSW] </ref>. This, then, corresponds to the need for a negative scale factor in our similarity relation, which we disallow. Therefore, we do not wish these two sequences to be considered similar under our definition of similarity.
Reference: [Nel78] <author> G. Nelson. </author> <title> An n log n algorithm for the two-variable-per-constraint linear programming satisfiability problem. </title> <type> Technical Report AIM-319, </type> <institution> Stanford University, </institution> <year> 1978. </year>
Reference-contexts: Also, there might be other cases between exponential and strongly polynomial. When the linear inequalities contain at most two variables each, <ref> [Nel78] </ref>, has shown that the Fourier-Motzkin algorithm can be optimized to reduce the exponent to log k. Variable elimination, which is exponential in (m; k), should be contrasted with the linear programming problem, where all variables are eliminated.
Reference: [DS] <author> A.V. Oppenheim and R.W. Schafer. </author> <title> Digital Signal Processing, </title> <publisher> Prentice Hall, </publisher> <year> 1975 </year>
Reference-contexts: A further characteristic of time-series data that is used to advantage, is that they have a skewed energy spectrum, to use the terminology borrowed from Discrete Signal Processing <ref> [DS] </ref>. As a result, most of the technology of information retrieval in this area is influenced by signal processing methods. 7.1.2 Approximately Similar Time-Series Data The database applications of interest involve queries expressing notions of "user-perceived similarity".
Reference: [PBCF93] <author> A.Paoluzzi, F.Bernandini, C.Cattani, V.Ferrucci. </author> <title> Dimension-Independent Modeling with Simplicial Complexes. </title> <journal> ACM Trans. Graphics, </journal> <volume> 12:1:56-102, </volume> <year> 1993. </year>
Reference: [Pa94] <author> C. Papadimitriou. </author> <title> Computational Complexity. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Recursive queries are common for databases that hold travel information (such as train or airplane connections), manufacturing information (such as bill-of-materials), or genealogical information (such as family relations). It is well-known that first-order calculi, or equivalently algebras, are not expressive enough to answer such queries <ref> [Pa94] </ref>. A well-behaved recursion operation for Constraint Databases should have the standard fixpoint semantics of recursion and the queries should be guaranteed to terminate and to have a finitely representable answer. Some progress in this area has been made very recently [GK97].
Reference: [PVV94] <author> J. Paredaens, J. Van den Bussche, and D. Van Gucht. </author> <title> Towards a Theory of Spatial Database Queries. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <pages> 279-288, </pages> <address> Minneapolis, Minnesota, </address> <year> 1994. </year>
Reference-contexts: A perfect example is spatiotemporal data, which consists of points in time and/or in space <ref> [BJM93, BLLM95, PVV94, VGVG95, Cho94] </ref>, typical of the applications mentioned at the beginning of the chapter. Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success.
Reference: [PVV95] <author> J. Paredaens, J. Van den Bussche, D. Van Gucht. </author> <title> First-order Queries on Finite Structures over the Reals. </title> <booktitle> Proc. IEEE LICS, </booktitle> <year> 1995. </year>
Reference-contexts: Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success. There has been work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>), concentrating on data complexity (i.e. treating the number of variables k in a query as a constant, see [CH82, Var82]). This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving. <p> Thus, constraint databases are a natural generalization of the relational model of data by allowing infinite relations that are finitely representable using constraints. The work on the complexity of constraint database queries ( <ref> [KKR95, GST94, PVV95] </ref>) has concentrated on data complexity, treating the number of variables k in a query as a constant. In many cases, the calculi have polynomial time data complexity.
Reference: [PS] <author> F.P. Preparata, M.I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: The use of generalized one-dimensional indexes reduces redundancy of representation and transforms one-dimensional searching on generalized relational attribute x into the problem of dynamic interval management. This is a well-known problem with many elegant solutions from computational geometry <ref> [PS] </ref>. Optimal in-core dynamic interval management is one of the basic tools of computational geometry. However, I/O optimal solutions are non-trivial, even for the static case. <p> We assume that we are eliminating x i , and that there are n edges incident on x i . By making use of convex hulls algorithms from computational geometry <ref> [PS] </ref>, we show that the time complexity is O (n log n). We start by observing the following: the non-trivial edges of a given cluster correspond to the facets of the convex hull for the set of all points defined by the cluster. <p> Therefore, the problem of determining which of the edges in a cluster are non-trivial is equivalent to the problem of computing the convex hull for a set of points defined as an intersection of half-planes. For k half-planes, such algorithms have time complexity O (k log k) <ref> [PS] </ref>. Each facet of the convex hull is defined by the pair of vertices that bound it. Given a non-trivial edge, its domain and range can be obtained directly from the coordinates of the vertices bounding the corresponding facet (see Figure 5.1).
Reference: [PS86] <author> L.K. Putnam, </author> <title> P.A. Subrahmanyam. Boolean Operations on n-dimensional objects. </title> <journal> IEEE Comput. Graph. Appl., </journal> <volume> 6:6:43-51, </volume> <year> 1986. </year>
Reference-contexts: B (R) is obtained by unioning together the boundary representations of the individual tuples, a standard Solid Modeling operation. See [FvDFH] for an introduction to polyhedral Solid Modeling, or [MM] for details of the algorithms. These algorithms extend to an arbitrary number of dimensions, as in <ref> [PS86] </ref>. Step 2. The next step of the algorithm is to create a vertex grid partitioning R of R. We project the set of B (R)'s vertices onto each of the n axes, and sort the k j obtained values, for 1 j n.
Reference: [Ram88] <author> R. Ramakrishnan. </author> <title> Magic Templates: A Spellbinding Approach to Logic Programs. </title> <booktitle> Proc. 5th International Conference on Logic Programming, </booktitle> <pages> 141-159, </pages> <year> 1988. </year>
Reference-contexts: The CDB framework has provided a unified view of some previous database research: for example, on the power of constraints for the implicit specification of temporal data [CI89], on relational query safety [AGSS86], on conjunctive queries with inequalities [Klu88] and on extending magic sets <ref> [Ram88] </ref>. <p> An important question is: how do various optimization methods (such as selection propagation, join ordering and other algebraic transformations, magic sets, etc.) combine with constraint query algebras? For some recent research in this direction we refer to <ref> [Ram88, MFPR90, SR92, SS94] </ref>. In contrast to algebraic transformations, estimation-based optimization heuristics are implementation-dependent.
Reference: [RS94] <author> S. Ramaswamy, S. Subramanian. </author> <title> Path Caching: A Technique for Optimal External Searching. </title> <booktitle> Proc. 13th ACM PODS, </booktitle> <pages> 14-25, </pages> <year> 1994. </year>
Reference: [Ren92] <author> J. Renegar. </author> <title> On the Computational Complexity and Geometry of the First-order Theory of the Reals: Parts I-III. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 13 </volume> <pages> 255-352, </pages> <year> 1992. </year>
Reference-contexts: Definition 3 The syntax of a Constraint Query Calculus is the union of a relational database query language and formulas in a decidable logical theory. For example: Relational calculus [Cod70] + the theory of real closed fields <ref> [Tar, Ren92] </ref>; Relational calculus (or even Inflationary Datalog : , [AHV] + the theory of dense order with constants [FG77]. Definition 4 The semantics of CQC is based on that of the decidable logical theory, by interpreting database atoms as shorthands for formulas of the theory. <p> For optimization, we suggest two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see [Tar]; for recent developments and a symbolic computation survey see <ref> [Ren92] </ref>, and for numerical computation see [VMK95]. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. <p> For optimization, we suggested two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see [Tar]; for recent developments and a symbolic computation survey see <ref> [Ren92] </ref>, and for numerical computation see [VMK95]. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. We then highlighted the importance of the syntax-semantics duality on two im-plementational issues in particular: representation of data and implementation of the 95 algebraic operations.
Reference: [Rev95] <author> P. Z. Revesz. </author> <title> Safe Stratified Datalog with Integer Order Programs. </title> <booktitle> International Conference on Constraint Programming, </booktitle> <address> Marseilles, France, </address> <month> September </month> <year> 1995. </year> <note> Springer-Verlag, LNCS 1000. 107 </note>
Reference-contexts: It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog <ref> [MPR90, RS92, SSRB93, VG92, Rev95] </ref>). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in [Kup94], relational algebra over linear constraint databases is not closed under aggregation using area.
Reference: [RS92] <author> K. A. Ross and Y. Sagiv. </author> <title> Monotonic Aggregation in Deductive Databases. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <pages> 114-126, </pages> <year> 1992. </year>
Reference-contexts: It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog <ref> [MPR90, RS92, SSRB93, VG92, Rev95] </ref>). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in [Kup94], relational algebra over linear constraint databases is not closed under aggregation using area.
Reference: [Sam] <author> H. Samet. </author> <title> The Design and Analysis of Spatial Data Structures. </title> <publisher> Addison-Wesley, </publisher> <address> Reading MA, </address> <year> 1990. </year>
Reference-contexts: It is a central problem in spatial databases for which there are many solutions with good secondary memory access performance, e.g., grid-files, quad-trees, R-trees, hB-trees, k-d-B-trees etc for a survey, see <ref> [Sam] </ref>. <p> This is the implementation technology that is needed to support the internal representation of Section 2. Our new indexing technique combines the MBR structure of [FRM94] with our internal representation. Many spatial data-structures can be used, for examples varieties of R-trees (see <ref> [Sam] </ref> for a comprehensive survey of the available external data-structures). In Section 4, we provide a constraint syntax for similarity querying. We show how various query variations can be expressed and translated into the internal representation of Section 2.
Reference: [Sch] <author> A. Schrijver. </author> <title> Theory of Linear and Integer Programming. </title> <publisher> John Wiley and Sons, </publisher> <year> 1986. </year>
Reference-contexts: Note that it is relatively easy to transform a Constraint Query Calculus into an "naive" algebra, one where operations are defined as syntactic manipulations of constraint formulas. For example, with Fourier-Motzkin elimination <ref> [Sch] </ref> one easily derives a "naive" algebra for linear constraint databases [GST94]. In this work, we will show that the "naive" approach is not sufficient to define a 5 good Constraint Query Algebra, i.e. one that preserves the practical advantages of the algebraic querying paradigm. <p> In Chapter 5, we consider how the implementation of the project operation can be made more efficient for a subclass of linear inequality constraints (see the comprehensive survey in <ref> [Sch] </ref>). We prove a new result, which is that the projective closure of a given monotone constraint tuple is often strongly polynomial in the size of the tuple (i.e., whenever the path expression for the corresponding monotone network is of polynomial size). <p> The argument in favor of lazy evaluation hinges on the complexity gap between performing satisfiability checks and performing variable elimination for sets of linear constraints. The former is polynomial whereas the latter is exponential <ref> [Sch] </ref>. What's worse, variable elimination can be exponential just because of the size of the result [Yan88] if a non-lazy, or "eager", representation is used. <p> This is due to the fact that computing projections onto a single variable x is equivalent to the optimization problem for a set of linear constraints, where we seek the minimum and the maximum allowable values for x. These optimization problems have polynomial complexity <ref> [Sch] </ref>, giving us an interval over x 26 that is used in the indexing scheme. The extraneous variables in the lazy representation will of course need to be removed (via projection) when the relation is output to the user. <p> We next show how to use the network representation of M to implement the Fourier-Motzkin Elimination algorithm for computing the projection of M onto some subset S of X, where jSj 1. We begin with an informal description of the original algorithm; see <ref> [Sch] </ref> for a formal discussion. Fourier-Motzkin Elimination The variables in X S are eliminated one by one. The set of constraints at the beginning of stage i is denoted by M i , where M 1 = M . <p> We established the correctness of the dense-order algebra, by proving the commutativity of its syntactic and semantic interpretations. In Section 5, we considered how the implementation of the project operation can be made more efficient for a subclass of linear inequality constraints (see the comprehensive survey in <ref> [Sch] </ref>. We presented an algorithm for projection over monotone constraints which is strongly polynomial in the size of the constraint set whenever the path expression for the corresponding monotone network is of polynomial size.
Reference: [SLR94] <author> P. Sheshadri, M. Livny, R. Ramakrishnan. </author> <title> Sequence Query Processing Proc. </title> <booktitle> ACM SIGMOD Conf., </booktitle> <pages> 430-441, </pages> <month> May </month> <year> 1994 </year>
Reference-contexts: However, any proposal 71 of such linguistic facilities must be supported by indexing (i.e., be implementable with reasonable I/O efficiency) for very large data sets. Examples of recent database research towards this goal include <ref> [AFS93, FRM94, SLR94] </ref>. A most basic problem in this area is First-Occurrence Subsequence Matching, defined as follows: given a query sequence Q of length n and a much longer data sequence S of length N , find the first occurrence of a contiguous subsequence within S that matches Q exactly. <p> The All-Occurrences Approximate Matching problems (either Subsequence or Whole-Sequence) are defined as before, but with "match approximately within tolerance *" instead of "match exactly". For external solutions to the All-Occurrence Whole-Sequence Approximate version, we refer to <ref> [AFS93, SLR94] </ref>; for the All-Occurrence Subsequence Approximate version, we refer to [FRM94]. A further characteristic of time-series data that is used to advantage, is that they have a skewed energy spectrum, to use the terminology borrowed from Discrete Signal Processing [DS].
Reference: [SR92] <author> D. Srivastava, R. Ramakrishnan. </author> <title> Pushing Constraint Selections. </title> <booktitle> Proc. 11th ACM PODS, </booktitle> <pages> 301-316, </pages> <year> 1992. </year>
Reference-contexts: An important question is: how do various optimization methods (such as selection propagation, join ordering and other algebraic transformations, magic sets, etc.) combine with constraint query algebras? For some recent research in this direction we refer to <ref> [Ram88, MFPR90, SR92, SS94] </ref>. In contrast to algebraic transformations, estimation-based optimization heuristics are implementation-dependent.
Reference: [Ste80] <author> G.L. Steele. </author> <title> The Definition and Implementation of a Computer Programming Language Based on Constraints. </title> <type> Ph.D. thesis, </type> <institution> MIT, </institution> <address> AI-TR 595, </address> <year> 1980. </year>
Reference-contexts: Programming with constraints as primitives (or constraint programming) is appealing because constraints are the normal language of discourse for many high-level applications. Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD [Sut]. The theme has been investigated since the 1970's, e.g., in artificial intelligence <ref> [Mon74, Mac77, Fre78, Ste80] </ref>, in graphical-interfaces [Bor81], and in logic programming languages [JL87, DVSAGB88, Col80].
Reference: [SS94] <author> P.J. Stuckey, S. Sudarshan. </author> <title> Compiling Query Constraints. </title> <booktitle> Proc. 13th ACM PODS, </booktitle> <pages> 56-68, </pages> <year> 1994. </year>
Reference-contexts: An important question is: how do various optimization methods (such as selection propagation, join ordering and other algebraic transformations, magic sets, etc.) combine with constraint query algebras? For some recent research in this direction we refer to <ref> [Ram88, MFPR90, SR92, SS94] </ref>. In contrast to algebraic transformations, estimation-based optimization heuristics are implementation-dependent.
Reference: [SSRB93] <author> S. Sudarshan, D. Srivastava, R. Ramakrishnan, and C. Beeri. </author> <title> Extending the Well-Founded and Valid Model Semantics for Aggregation. </title> <booktitle> International Logic Programming Symposium, </booktitle> <year> 1993. </year>
Reference-contexts: It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog <ref> [MPR90, RS92, SSRB93, VG92, Rev95] </ref>). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in [Kup94], relational algebra over linear constraint databases is not closed under aggregation using area.
Reference: [Sut] <author> I.E. Sutherland. </author> <title> SKETCHPAD: A Man-Machine Graphical Communication System. </title> <publisher> Spartan Books, </publisher> <year> 1963. </year>
Reference-contexts: Programming with constraints as primitives (or constraint programming) is appealing because constraints are the normal language of discourse for many high-level applications. Pioneering work in constraint programming goes back to the early 1960's, e.g., Sutherland's SKETCHPAD <ref> [Sut] </ref>. The theme has been investigated since the 1970's, e.g., in artificial intelligence [Mon74, Mac77, Fre78, Ste80], in graphical-interfaces [Bor81], and in logic programming languages [JL87, DVSAGB88, Col80].
Reference: [Tar81a] <author> R.E. Tarjan. </author> <title> A Unified Approach to Path Problems. </title> <journal> JACM, </journal> <volume> 28:3:577-593, </volume> <year> 1981. </year>
Reference-contexts: We say that p is non-trivial iff its domain is non-trivial. In this subsection, we show the connection between simple non-trivial paths and path expressions <ref> [Tar81a, Tar81b] </ref>. We start with an introduction to path expressions [Tar81a, Tar81b]. Let M = (X; E) be a directed multigraph; any path in M can be regarded as a string over the alphabet E. <p> We say that p is non-trivial iff its domain is non-trivial. In this subsection, we show the connection between simple non-trivial paths and path expressions <ref> [Tar81a, Tar81b] </ref>. We start with an introduction to path expressions [Tar81a, Tar81b]. Let M = (X; E) be a directed multigraph; any path in M can be regarded as a string over the alphabet E. The set of all paths in M forms a regular language (see [AHU] for an introduction to regular languages).
Reference: [Tar81b] <author> R.E. Tarjan. </author> <title> Fast Algorithms for Solving Path Problems. </title> <journal> JACM, 28:3:594-614 </journal>
Reference-contexts: We say that p is non-trivial iff its domain is non-trivial. In this subsection, we show the connection between simple non-trivial paths and path expressions <ref> [Tar81a, Tar81b] </ref>. We start with an introduction to path expressions [Tar81a, Tar81b]. Let M = (X; E) be a directed multigraph; any path in M can be regarded as a string over the alphabet E. <p> We say that p is non-trivial iff its domain is non-trivial. In this subsection, we show the connection between simple non-trivial paths and path expressions <ref> [Tar81a, Tar81b] </ref>. We start with an introduction to path expressions [Tar81a, Tar81b]. Let M = (X; E) be a directed multigraph; any path in M can be regarded as a string over the alphabet E. The set of all paths in M forms a regular language (see [AHU] for an introduction to regular languages).
Reference: [Tar] <author> A. Tarski. </author> <title> A Decision Method for Elementary Algebra and Geometry. </title> <institution> University of California Press, Berkeley, California, </institution> <year> 1951. </year>
Reference-contexts: Definition 3 The syntax of a Constraint Query Calculus is the union of a relational database query language and formulas in a decidable logical theory. For example: Relational calculus [Cod70] + the theory of real closed fields <ref> [Tar, Ren92] </ref>; Relational calculus (or even Inflationary Datalog : , [AHV] + the theory of dense order with constants [FG77]. Definition 4 The semantics of CQC is based on that of the decidable logical theory, by interpreting database atoms as shorthands for formulas of the theory. <p> In Chapter 3, we briefly consider two issues that are crucial to practical implementations of constraint database querying: indexing and optimization. For optimization, we suggest two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see <ref> [Tar] </ref>; for recent developments and a symbolic computation survey see [Ren92], and for numerical computation see [VMK95]. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. <p> In Chapter 3, we briefly considered two issues that are crucial to practical implementations of constraint database querying: indexing and optimization. For optimization, we suggested two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see <ref> [Tar] </ref>; for recent developments and a symbolic computation survey see [Ren92], and for numerical computation see [VMK95]. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras.
Reference: [Ull] <author> J. D. Ullman. </author> <booktitle> Principles of Database Systems, </booktitle> <address> 2 nd edition. </address> <publisher> Computer Science Press, </publisher> <year> 1982. </year> <month> 108 </month>
Reference-contexts: Indeed, having such languages for ad-hoc database querying is a requirement in today's relational technology (see <ref> [AHV, Kan90, Ull] </ref>. CQLs can be viewed as a specialized form of constraint programming, similar to the way that relational query languages can be viewed as a form of first-order theorem proving. Constraint programming paradigms are inherently declarative, since they implicitly describe computations by specifying how these computations are constrained. <p> The analogue for the relational model is that relations are finite structures, and queries are supposed to preserve this finiteness. This is a requirement that creates various "safety" problems in relational databases <ref> [Cod70, Ull] </ref>. The precise analogue in relational databases is the notion of weak safety of [AGSS86]. Evaluation of a query corresponds to an instance of a decision problem. Quantifier elimination procedures realize the goal of closed form and use induction on the structure of formulas, which leads to bottom-up evaluation. <p> An example of a relational database system which uses the implementation-related information for query optimization is System R <ref> [Ull] </ref>. <p> A sampling technique must be used to estimate the latter. * Which, if any are clustering indices? For relational databases, a clustering index is an index on an attribute (or attributes) such that tuples with the same value for those attributes will tend to be adjacent in memory. <ref> [Ull] </ref> has an example of how the above parameters affect the choice of an evaluation strategy for a given query. Some of the issues involved in generalizing the approach of System R to constraint databases are highlighted below.
Reference: [VanH] <author> P. Van Hentenryck. </author> <title> Constraint Satisfaction in Logic Programming. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Now, CLP is finding applications in Operations Research (which has been revolutionized by this technology), in Scientific Problem Solving, and in other areas where problems can be stated declaratively and 2 can be solved by a combinatorial search approach <ref> [VanH] </ref>. It was not immediately clear how to apply the same type of insight to Relational Databases, since the bottom-up and set-at-a-time style of database query evaluation emphasized in databases seems to contradict the top-down, depth-first intuition behind Constraint Logic Programming. <p> One of the most important advances in constraint programming in the 1980's has been the development of Constraint Logic Programming (CLP) as a general-purpose framework for computations, e.g., in CLP (&lt;) [JL87], in Prolog III [Col80], and in CHIP <ref> [DVSAGB88, VanH] </ref>. The insight that led to CLP is: the unification mechanism of standard Logic Programming can be regarded as a trivial constraint solver (for equality constraints only). Expressiveness is therefore gained by replacing unification with constraint solving, and allowing constraints in logic programs. <p> We call such variables extraneous, as opposed to essential. The approach is akin to the CLP approach to constraint stores, where many variables will not participate in the output <ref> [VanH] </ref>. What matters is that the constraints are satisfiable, i.e., for some assignment of values to the extraneous variables, the essential variables will form a tuple in the constraint store. For linear constraints, satisfiability can be guaranteed using linear programming.
Reference: [VMK95] <author> P. Van Hentenryck, D. McAllester, D. Kapur. </author> <title> Solving Polynomial Systems using a Branch and Prune Approach. </title> <type> Brown CS Tech. Rep. </type> <institution> CS-95-01, </institution> <year> 1995. </year> <note> To appear in the SIAM J. of Numerical Analysis. </note>
Reference-contexts: For optimization, we suggest two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see [Tar]; for recent developments and a symbolic computation survey see [Ren92], and for numerical computation see <ref> [VMK95] </ref>. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. In Chapter 4, we present the syntax for data representation for dense-order [FG77, Kan95, Klu88] and temporal constraint tuples, and define the algebraic operations over this representation. <p> For linear constraints, satisfiability can be guaranteed using linear programming. For nonlinear constraints, satisfiability can be implemented efficiently using numerical methods <ref> [VMK95] </ref>. Lazy Evaluation: The approach involves delaying the symbolic processing (quantifier/variable elimination) whenever possible. We call this approach lazy evaluation, borrowing this terminology from functional programming. The resulting representation of generalized relations, containing unevaluated existentially quantified variables, is called the lazy representation. <p> For optimization, we suggested two promising approaches to optimizing CQA queries. The first is lazy evaluation of linear and nonlinear constraints (for real polynomial constraints see [Tar]; for recent developments and a symbolic computation survey see [Ren92], and for numerical computation see <ref> [VMK95] </ref>. The second approach is to optimize CQAs by using the indexing information, just as for relational algebras. We then highlighted the importance of the syntax-semantics duality on two im-plementational issues in particular: representation of data and implementation of the 95 algebraic operations.
Reference: [VG92] <author> A. Van Gelder. </author> <title> The Well-Founded Semantics of Aggregation. </title> <booktitle> ACM Symposium on Principles of Database Systems, </booktitle> <pages> 127-138, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: It is an open issue whether variable independence can also be defined for other query languages with aggregation (e.g., relational calculus, Datalog <ref> [MPR90, RS92, SSRB93, VG92, Rev95] </ref>). Also, it is possible that the variable independence can be applied to provide closure conditions for CQA+fixpoint. 6.1 Background As shown in [Kup94], relational algebra over linear constraint databases is not closed under aggregation using area.
Reference: [VGVG95] <author> L. Vandeurzen, M. Gyssens, and D. Van Gucht. </author> <title> On the Desirability and Limitations of Linear Spatial Database Models. </title> <booktitle> International Symposium on Large Spatial Databases, </booktitle> <pages> 14-28, </pages> <year> 1995. </year>
Reference-contexts: A perfect example is spatiotemporal data, which consists of points in time and/or in space <ref> [BJM93, BLLM95, PVV94, VGVG95, Cho94] </ref>, typical of the applications mentioned at the beginning of the chapter. Besides extending the data model with constraints, Constraint Databases also integrate constraints into the queries, while preserving the efficient bottom-up declarative semantics that enabled relational databases to become such a success.
Reference: [Var82] <author> M. Y. Vardi. </author> <title> The Complexity of Relational Query Languages. </title> <booktitle> Proc. 14th ACM STOC, </booktitle> <pages> 137-146, </pages> <year> 1982. </year>
Reference-contexts: There has been work on the complexity of constraint database queries ( [KKR95, GST94, PVV95]), concentrating on data complexity (i.e. treating the number of variables k in a query as a constant, see <ref> [CH82, Var82] </ref>). This use of data complexity, a common tool for studying expressibility in finite model theory, distinguishes the CDB framework from arbitrary, and inherently exponential, theorem proving. <p> From that work, follows that Relational Calculus on finite sets can be evaluated bottom-up in closed form. Relational Algebra, an operator-based query paradigm with bottom-up expression evaluation semantics and LOGSPACE data complexity <ref> [Var82] </ref>, is just the query language whose existence was foreseen in [Cod70]. The algebraic querying paradigm is not declarative, since the algebraic expressions represent a `plan' or a `recipe' for evaluating a query.
Reference: [Yan88] <author> M. Yannakakis. </author> <title> Expressing Combinatorial Optimization Problems by Linear Programs. </title> <booktitle> Proc. 20th ACM STOC, </booktitle> <pages> 223-228, </pages> <year> 1988. </year> <month> 109 </month>
Reference-contexts: Strongly Polynomial Projections: For a set of m linear constraints with k variables, elimination of some variables, i.e., existential quantifier elimination for any i k variables, has a worst-case bound exponential in k. Elimination could be exponential because of the output size. A good example is given in <ref> [Yan88] </ref>, consisting of a linear constraint set for representing a parity polytope in k dimensions. This polytope, though simple to describe, has O (2 k ) facets, requiring an exponential number of constraints (if no existentially quantified variables are involved in the representation). <p> The former is polynomial whereas the latter is exponential [Sch]. What's worse, variable elimination can be exponential just because of the size of the result <ref> [Yan88] </ref> if a non-lazy, or "eager", representation is used. Therefore, we propose to use the lazy methodology not just for the intermediate representation during the evaluation of positive algebraic operations, but also for the internal representation when storing generalized relations.
References-found: 92

