URL: http://www.cs.umd.edu/~rich/courses/cmsc818G-s98/papers/corba_ieee.ps
Refering-URL: http://www.cs.umd.edu/~rich/courses/cmsc818G-s98/schedule.html
Root-URL: 
Email: fgokhale,schmidtg@cs.wustl.edu  
Title: Measuring and Optimizing CORBA Latency and Scalability Over High-speed Networks  
Author: Aniruddha S. Gokhale and Douglas C. Schmidt 
Address: St. Louis, MO 63130  
Affiliation: Department of Computer Science Washington University  
Abstract: This paper has been submitted to IEEE Transaction on Computers. An earlier version of this paper appeared in the Proceedings of the International Conference on Distributed Computing Systems (ICDCS '97), Baltimore, MD, May 27-30, 1997. Abstract There is increasing demand to extend object-oriented middle-ware, such as OMG CORBA, to support applications with stringent quality of service (QoS) requirements. However, conventional CORBA Object Request Broker (ORB) implementations incur high latency and low scalability when used for performance-sensitive applications. These inefficiencies discourage developers from using CORBA for mission/life-critical applications such as real-time avionics, telecom call processing, and medical imaging. This paper provides two contributions to the research on CORBA performance. First, we systematically analyze the latency and scalability of two widely used CORBA ORBs, VisiBroker and Orbix. These results reveal key sources of overhead in conventional ORBs. Second, we describe techniques used to improve latency and scalability in TAO, which is a high-performance, real-time implementation of CORBA. Although conventional ORBs do not yet provide adequate QoS guarantees to applications, our research results indicate it is possible to implement ORBs that can support high-performance, real-time applications. Keywords: Distributed object computing, CORBA communication middleware performance, Real-time CORBA. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David D. Clark and David L. Tennenhouse, </author> <title> Architectural Considerations for a New Generation of Protocols, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIGCOMM), </booktitle> <address> Philadelphia, PA, </address> <month> Sept. </month> <year> 1990, </year> <booktitle> ACM, </booktitle> <pages> pp. </pages> <year> 200208. </year>
Reference-contexts: In addition, communication software must allow bandwidth-sensitive applications to transfer data efficiently over high-speed networks. Robustness, flexibility, and reusability are essential to respond rapidly to changing application requirements that span an increasingly wide range of media types and access patterns <ref> [1] </ref>. Requirements for flexibility and reusability motivate the use of the Common Object Request Broker Architecture (CORBA) [2]. CORBA automates common network programming tasks such as object location, object activation, parameter marshaling/demarshaling, framing, and error handling. <p> Minimizing intra-ORB function calls requires sophisticated compiler optimizations such as integrated layer processing <ref> [1] </ref>. <p> In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques, such as program flow analysis [25, 26] and integrated layer processing (ILP) <ref> [1] </ref> to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls. Most importantly, this streamlining can be performed without requiring modifications to the standard CORBA specification. <p> This overhead arises from the amount of time the CORBA ORBs spend performing presentation layer conversions and data copying. 6.2 Presentation Layer and Data Copying The presentation layer is a major bottleneck in high-performance communication subsystems <ref> [1] </ref>. This layer transforms typed data from higher-level representations to lower-level representations (marshaling) and vice versa (demarshal-ing). In both RPC toolkits and CORBA, this transformation process is performed by client-side stubs and server-side skeletons that are generated by interface definition language (IDL) compilers. <p> One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 33, 34] </ref> and Integrated Layer Processing (ILP) [1, 35, 36]. ILP ensures that lower layer protocols deal with data in units specified by the application. <p> One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 33, 34] and Integrated Layer Processing (ILP) <ref> [1, 35, 36] </ref>. ILP ensures that lower layer protocols deal with data in units specified by the application.
Reference: [2] <author> Object Management Group, </author> <title> The Common Object Request Broker: Architecture and Specification, </title> <address> 2.0 edition, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Robustness, flexibility, and reusability are essential to respond rapidly to changing application requirements that span an increasingly wide range of media types and access patterns [1]. Requirements for flexibility and reusability motivate the use of the Common Object Request Broker Architecture (CORBA) <ref> [2] </ref>. CORBA automates common network programming tasks such as object location, object activation, parameter marshaling/demarshaling, framing, and error handling. CORBA also provides the basis for defining higher layer distributed services such as naming, events, replication, transactions, and security [3]. <p> For servers executing remotely, CORBA 2.0-compliant <ref> [2] </ref> ORB Cores communicate via the General Inter-ORB Protocol (GIOP) and the Internet Inter-ORB Protocol (IIOP), which runs atop the TCP transport protocol.
Reference: [3] <institution> Object Management Group, </institution> <month> CORBAServices: </month> <title> Common Object Services Specification, </title> <note> Revised Edition, 95-3-31 edition, </note> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: CORBA automates common network programming tasks such as object location, object activation, parameter marshaling/demarshaling, framing, and error handling. CORBA also provides the basis for defining higher layer distributed services such as naming, events, replication, transactions, and security <ref> [3] </ref>. The success of CORBA in mission-critical distributed computing environments depends heavily on the ability of Object Request Brokers (ORBs) to provide the necessary quality of service (QoS) to applications.
Reference: [4] <author> Timothy H. Harrison, David L. Levine, and Douglas C. Schmidt, </author> <title> The Design and Performance of a Real-time CORBA Event Service, </title> <booktitle> in Proceedings of OOPSLA '97, </booktitle> <address> Atlanta, GA, </address> <month> October </month> <year> 1997, </year> <note> ACM. </note>
Reference-contexts: If not corrected, this overhead will force developers to avoid CORBA middleware and continue to use lower-level tools like sockets. Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as robustness, flexibility, and reusability, which are crucial to the success of complex latency-sensitive distributed applications <ref> [4] </ref>. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Our earlier work [5, 6, 7, 8] has focused on measuring and optimizing the throughput of CORBA ORBs. <p> This jitter is 10 DII Oneway DII SII Oneway SII DII Oneway DII 11 SII Twoway SII DII Twoway DII SII Twoway SII 12 DII Twoway DII generally unacceptable for real-time systems that require predictable behavior <ref> [4] </ref>. The Orbix DII latency for octets is nearly double the latency for VisiBroker. The oneway SII latencies for Orbix and VisiBroker for BinStructs are comparable. However, the oneway DII latency for Orbix increases rapidly compared to that of VisiBroker. <p> We have used these results to guide the development of a high-performance, real-time ORB, called TAO <ref> [19, 4] </ref>. <p> A second perfect hashing function is then used to locate the operation. Both servant and operation lookup take constant time. Perfect hashing is applicable when the keys to be hashed are known a priori. In many hard real-time systems (such as avionic control systems <ref> [4] </ref>), the servants and operations can be configured statically. In this scenario, it is possible to use perfect hashing to hash the servant and operations. For our experiment, we used the GNU gperf [28] tool to generate perfect hash functions for object keys and operation names. <p> For this reason, we are using the perfect hashing demultiplexing strategy in the TAO ORB we are building for real-time avionics applications <ref> [19, 4] </ref>. 5.4 Reducing ORB Latency with IIOP Opti mizations To expedite the research goals of the TAO project, and to avoid re-inventing existing components, we based TAO on SunSoft IIOP, which is a freely available reference implementation of the Internet Inter-ORB Protocol (IIOP).
Reference: [5] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Measuring the Performance of Communication Middleware on High-Speed Networks, </title> <booktitle> in Proceedings of SIGCOMM '96, </booktitle> <address> Stanford, CA, </address> <month> August </month> <year> 1996, </year> <booktitle> ACM, </booktitle> <pages> pp. 306317. </pages>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as robustness, flexibility, and reusability, which are crucial to the success of complex latency-sensitive distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Our earlier work <ref> [5, 6, 7, 8] </ref> has focused on measuring and optimizing the throughput of CORBA ORBs. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several CORBA ORBs and lower-level mechanisms like sockets. <p> In addition, layered demultiplexing can cause priority inversions because servant-level QoS information is inaccessible to the lowest-level device drivers and protocol stacks in the I/O subsystem of an ORB endsystem. Conventional implementations of CORBA incur significant demultiplexing overhead. In particular, <ref> [5, 17] </ref> show that ~17% of the total server processing time is spent demultiplex-ing requests. Unless this overhead is reduced and demultiplex-ing is performed predictably, ORBs cannot provide consistent quality of service guarantees to applications. <p> Similarly, Figures 19 through 26 depict the average latency for sending richly-typed struct data and untyped octet for twoway operations. These figures reveal that as the sender buffer size increases, the marshaling and data copying overhead also grows <ref> [5, 6] </ref>, thereby increasing latency. These results demonstrate the benefit of using more efficient buffer management techniques and highly optimized stubs [9] to reduce the presentation conversion and data copying overhead. Oneway latency The oneway SII latencies for Orbix and VisiBroker for octets are comparable. <p> These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. <ref> [5, 6] </ref> precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using SII and DII. The latency for sending octets is much less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. [32] discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency. Earlier work <ref> [5, 6] </ref> using untyped data and typed data in a similar CORBA/ATM testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of TTCP are nearly equivalent for a given socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [24]. Our earlier results <ref> [5, 6] </ref> have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [6] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> The Performance of the CORBA Dynamic Invocation Interface and Dynamic Skeleton Interface over High-Speed ATM Networks, </title> <booktitle> in Proceedings of GLOBECOM '96, </booktitle> <address> London, England, </address> <month> November </month> <year> 1996, </year> <journal> IEEE, </journal> <pages> pp. 5056. </pages>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as robustness, flexibility, and reusability, which are crucial to the success of complex latency-sensitive distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Our earlier work <ref> [5, 6, 7, 8] </ref> has focused on measuring and optimizing the throughput of CORBA ORBs. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several CORBA ORBs and lower-level mechanisms like sockets. <p> Similarly, Figures 19 through 26 depict the average latency for sending richly-typed struct data and untyped octet for twoway operations. These figures reveal that as the sender buffer size increases, the marshaling and data copying overhead also grows <ref> [5, 6] </ref>, thereby increasing latency. These results demonstrate the benefit of using more efficient buffer management techniques and highly optimized stubs [9] to reduce the presentation conversion and data copying overhead. Oneway latency The oneway SII latencies for Orbix and VisiBroker for octets are comparable. <p> These sources of overhead reduce the receiver's performance, thereby triggering the flow control mechanisms of the transport protocol, which impede the sender's progress. <ref> [5, 6] </ref> precisely pinpoint the marshaling and data copying overheads when transferring richly-typed data using SII and DII. The latency for sending octets is much less than that for BinStructs due to significantly lower overhead of presentation layer conversions. <p> They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. [32] discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency. Earlier work <ref> [5, 6] </ref> using untyped data and typed data in a similar CORBA/ATM testbed as the one in this paper reveal that the low-level C socket version and the C++ socket wrapper versions of TTCP are nearly equivalent for a given socket queue size. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [24]. Our earlier results <ref> [5, 6] </ref> have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [7] <author> Irfan Pyarali, Timothy H. Harrison, and Douglas C. Schmidt, </author> <title> Design and Performance of an Object-Oriented Framework for High-Performance Electronic Medical Imaging, </title> <booktitle> USENIX Computing Systems, </booktitle> <volume> vol. 9, no. 4, </volume> <month> November/December </month> <year> 1996. </year>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as robustness, flexibility, and reusability, which are crucial to the success of complex latency-sensitive distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Our earlier work <ref> [5, 6, 7, 8] </ref> has focused on measuring and optimizing the throughput of CORBA ORBs. <p> Each ENI card has 512 Kbytes of on-board memory. A maximum of 32 Kbytes is allotted per ATM virtual circuit connection for receiving and transmitting frames (for a total of 64 K). This allows up to eight switched virtual connections per card. 3.2 Traffic Generators Our earlier studies <ref> [5, 6, 7] </ref> tested bulk data performance using flooding models that transferred untyped bytestream data, as well as richly typed data between hosts using several CORBA ORBs and lower-level mechanisms like sockets. <p> char c; long l; octet o; double d; -; interface ttcp_sequence - typedef sequence&lt;BinStruct&gt; StructSeq; typedef sequence&lt;octet&gt; OctetSeq; // Routines to send sequences of various data types void sendStructSeq_2way (in StructSeq ttcp_seq); void sendOctetSeq_2way (in OctetSeq ttcp_seq); void sendNoParams_2way (); void sendNoParams_1way (); -; 3.3 TTCP Parameter Settings Related work <ref> [12, 13, 14, 7] </ref> on transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffers, and number of servants in a server.
Reference: [8] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Principles for Optimizing CORBA Internet Inter-ORB Protocol Performance, </title> <booktitle> in Hawaiian International Conference on System Sciences, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: Unfortunately, lower-level tools fail to provide other key benefits of CORBA such as robustness, flexibility, and reusability, which are crucial to the success of complex latency-sensitive distributed applications [4]. Therefore, it is imperative to eliminate the sources of CORBA overhead shown in this paper. Our earlier work <ref> [5, 6, 7, 8] </ref> has focused on measuring and optimizing the throughput of CORBA ORBs. <p> SunSoft IIOP is written in C++ and provides many features of a CORBA 2.0 ORB. However, it performs poorly over high-speed networks <ref> [8] </ref>. The source of overhead in SunSoft IIOP include many factors reported in Section 4. <p> Our goal in precisely pinpointing the sources of overhead for CORBA is to optimize the performance of TAO [19] TAO is a high-performance, real-time ORB endsystem designed to meet the QoS requirements of bandwidth- and delay-sensitive applications. Our development strategy for TAO is guided by applying principle-driven performance optimizations <ref> [8] </ref>, such as optimizing for the common case; eliminating gratuitous waste; replacing general purpose methods with specialized, efficient ones; precomputing values, if possible; storing redundant state to speed up expensive operations; passing information between layers; optimizing for the processor cache; and optimizing demultiplexing strategies.
Reference: [9] <author> Eric Eide, Kevin Frei, Bryan Ford, Jay Lepreau, and Gary Lind-strom, Flick: </author> <title> A Flexible, Optimizing IDL Compiler, </title> <booktitle> in Proceedings of ACM SIGPLAN '97 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <address> Las Vegas, NV, </address> <month> June </month> <year> 1997, </year> <note> ACM. </note>
Reference-contexts: The transformation between OMG IDL definitions and an application programming language like C++ or Java is automated by an IDL compiler. IDL compilers eliminate common sources of network programming errors and increase opportunities for automated compiler optimizations <ref> [9] </ref>. * Dynamic Invocation Interface (DII) The DII allows a client to directly access the underlying request transport mechanisms provided by the ORB Core. The DII is useful when an application has no compile-time knowledge of the interface it is accessing. <p> These figures reveal that as the sender buffer size increases, the marshaling and data copying overhead also grows [5, 6], thereby increasing latency. These results demonstrate the benefit of using more efficient buffer management techniques and highly optimized stubs <ref> [9] </ref> to reduce the presentation conversion and data copying overhead. Oneway latency The oneway SII latencies for Orbix and VisiBroker for octets are comparable. However, as depicted in Figures 12 through 16, due to inefficient internal buffering strategies, there is substantial variance in latency. <p> Eliminating the demultiplexing overhead requires de-layered strategies and fast, flexible message demul-tiplexing [20, 18]. Eliminating the presentation layer overhead requires optimized stub generators <ref> [21, 9] </ref> for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> IDL compilers translate interfaces written in an description language to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler [21]) and the Flick IDL compiler <ref> [9] </ref>. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [24]. Our earlier results [5, 6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data.
Reference: [10] <institution> Object Management Group, </institution> <note> Specification of the Portable Object Adapter (POA), OMG Document orbos/97-05-15 edition, </note> <month> June </month> <year> 1997. </year>
Reference-contexts: While current CORBA implementations are typically limited to a single Object Adapter per ORB, recent CORBA portability enhancements <ref> [10] </ref> define the Portable Object Adapter (POA), with support for multiple POAs per ORB. The use of CORBA as communication middleware enhances application flexibility and portability by automating common network programming tasks such as object location, object activation, and parameter marshaling.
Reference: [11] <author> USNA, TTCP: </author> <title> a test of TCP and UDP Performance, </title> <month> Dec </month> <year> 1984. </year>
Reference-contexts: In addition, we measure CORBA scalability by determining the demultiplexing overhead incurred when increasing the number of servants in an endsystem server process. Traffic for the latency experiment was generated and consumed by an enhanced version of TTCP <ref> [11] </ref>. TTCP is a widely used benchmarking tool to evaluate the performance of TCP/IP and UDP/IP networks. We extended TTCP to handle oneway and twoway operations for Orbix 2.1 and VisiBroker 2.0.
Reference: [12] <author> Sudheer Dharnikota, Kurt Maly, and C. M. Overstreet, </author> <title> Performance Evaluation of TCP(UDP)/IP over ATM networks, </title> <institution> Department of Computer Science, </institution> <type> Technical Report CSTR 94 23, </type> <institution> Old Dominion University, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: char c; long l; octet o; double d; -; interface ttcp_sequence - typedef sequence&lt;BinStruct&gt; StructSeq; typedef sequence&lt;octet&gt; OctetSeq; // Routines to send sequences of various data types void sendStructSeq_2way (in StructSeq ttcp_seq); void sendOctetSeq_2way (in OctetSeq ttcp_seq); void sendNoParams_2way (); void sendNoParams_1way (); -; 3.3 TTCP Parameter Settings Related work <ref> [12, 13, 14, 7] </ref> on transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffers, and number of servants in a server. <p> In general, less attention has been paid to integrating the following topics related to communication middleware: 6.1 Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [12, 13, 14] </ref> present results on performance of TCP/IP (and UDP/IP [12]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> the following topics related to communication middleware: 6.1 Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. [12, 13, 14] present results on performance of TCP/IP (and UDP/IP <ref> [12] </ref>) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size). <p> This work indicates that in addition to the host architecture and host network interface, parameters configurable in software (like TCP window size, socket queue size, and user data size) significantly affect throughput. <ref> [12] </ref> shows that UDP performs better than TCP over ATM networks, which is attributed to redundant TCP processing overhead on highly-reliable ATM links. [12] also describes techniques to tune TCP to be a less bulky protocol so that its performance can be comparable to UDP. <p> addition to the host architecture and host network interface, parameters configurable in software (like TCP window size, socket queue size, and user data size) significantly affect throughput. <ref> [12] </ref> shows that UDP performs better than TCP over ATM networks, which is attributed to redundant TCP processing overhead on highly-reliable ATM links. [12] also describes techniques to tune TCP to be a less bulky protocol so that its performance can be comparable to UDP.
Reference: [13] <author> Minh DoVan, Louis Humphrey, Geri Cox, and Carl Ravin, </author> <title> Initial Experience with Asynchronous Transfer Mode for Use in a Medical Imaging Network, </title> <journal> Journal of Digital Imaging, </journal> <volume> vol. 8, no. 1, </volume> <pages> pp. 4348, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: char c; long l; octet o; double d; -; interface ttcp_sequence - typedef sequence&lt;BinStruct&gt; StructSeq; typedef sequence&lt;octet&gt; OctetSeq; // Routines to send sequences of various data types void sendStructSeq_2way (in StructSeq ttcp_seq); void sendOctetSeq_2way (in OctetSeq ttcp_seq); void sendNoParams_2way (); void sendNoParams_1way (); -; 3.3 TTCP Parameter Settings Related work <ref> [12, 13, 14, 7] </ref> on transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffers, and number of servants in a server. <p> In general, less attention has been paid to integrating the following topics related to communication middleware: 6.1 Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [12, 13, 14] </ref> present results on performance of TCP/IP (and UDP/IP [12]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [14] <author> K. Modeklev, E. Klovning, and O. Kure, </author> <title> TCP/IP Behavior in a High-Speed Local ATM Network Environment, </title> <booktitle> in Proceedings of the 19 th Conference on Local Computer Networks, </booktitle> <address> Minneapolis, MN, Oct. 1994, </address> <publisher> IEEE, </publisher> <pages> pp. 176185. </pages>
Reference-contexts: char c; long l; octet o; double d; -; interface ttcp_sequence - typedef sequence&lt;BinStruct&gt; StructSeq; typedef sequence&lt;octet&gt; OctetSeq; // Routines to send sequences of various data types void sendStructSeq_2way (in StructSeq ttcp_seq); void sendOctetSeq_2way (in OctetSeq ttcp_seq); void sendNoParams_2way (); void sendNoParams_1way (); -; 3.3 TTCP Parameter Settings Related work <ref> [12, 13, 14, 7] </ref> on transport protocol performance over ATM demonstrate the performance impact of parameters such as the size of socket queues, data buffers, and number of servants in a server. <p> These parameters influence the size of the TCP segment window, which has been shown <ref> [14] </ref> to significantly affect CORBA-level and TCP-level performance on high-speed net works. 4 * TCP No Delay option Since the request sizes for our tests are relatively small, the TCP NODELAY option is set on the client side. <p> In general, less attention has been paid to integrating the following topics related to communication middleware: 6.1 Transport Protocol Performance over ATM Networks The underlying transport protocols used by the ORB must be flexible and possess the necessary hooks to tune different parameters of the underlying transport protocol. <ref> [12, 13, 14] </ref> present results on performance of TCP/IP (and UDP/IP [12]) on ATM networks by varying a number of parameters (such as TCP window size, socket queue size, and user data size).
Reference: [15] <author> PureAtria Software Inc., </author> <title> Quantify User's Guide, </title> <institution> PureAtria Software Inc., </institution> <year> 1996. </year>
Reference-contexts: This system call uses the SunOS 5.5 high-resolution timer, which expresses time in nanoseconds from an arbitrary time in the past. The time returned by gethrtime is very accurate since it does not drift. The profile information for the empirical analysis was obtained using the Quantify <ref> [15] </ref> performance measurement tool. Quantify analyzes performance bottlenecks and identifies sections of code that dominate execution time. Unlike traditional sampling-based profilers (such as the UNIX gprof tool), Quantify reports results without including its own overhead.
Reference: [16] <author> David L. Tennenhouse, </author> <title> Layered Multiplexing Considered Harmful, </title> <booktitle> in Proceedings of the 1 st International Workshop on High-Speed Networks, </booktitle> <month> May </month> <year> 1989. </year>
Reference-contexts: Demultiplexing client requests through all these layers is expensive, particularly when a large number of operations appear in an IDL interface and/or a large number of servants are managed by an ORB. Layered demultiplexing is particularly inappropriate for real-time applications <ref> [16] </ref>. Layering increases latency by multiplying the number internal tables that must be searched as incoming client requests traverse a protocol stack. <p> In addition, conventional CORBA ORBs utilize several extra levels of demultiplexing at the application layer to associate incoming client requests with the appropriate servant and operation (as shown in Figure 4). Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems <ref> [16] </ref> due to the additional overhead incurred at each layer.[20] describes a fast and flexible message demultiplexing strategy based on dynamic code generation. [18] evaluates the performance of alternative demultiplexing strategies for real-time CORBA. Our results for latency measurements have shown that with increasing number of servants, the latency increases.
Reference: [17] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Evaluating Latency and Scalability of CORBA Over High-Speed ATM Networks, </title> <booktitle> in Proceedings of the International Conference on Distributed Computing Systems, </booktitle> <address> Baltimore, Maryland, </address> <month> May </month> <year> 1997, </year> <note> IEEE. </note>
Reference-contexts: In addition, layered demultiplexing can cause priority inversions because servant-level QoS information is inaccessible to the lowest-level device drivers and protocol stacks in the I/O subsystem of an ORB endsystem. Conventional implementations of CORBA incur significant demultiplexing overhead. In particular, <ref> [5, 17] </ref> show that ~17% of the total server processing time is spent demultiplex-ing requests. Unless this overhead is reduced and demultiplex-ing is performed predictably, ORBs cannot provide consistent quality of service guarantees to applications.
Reference: [18] <author> Aniruddha Gokhale and Douglas C. Schmidt, </author> <title> Evaluating the Performance of Demultiplexing Strategies for Real-time CORBA, </title> <booktitle> in Proceedings of GLOBECOM '97, </booktitle> <address> Phoenix, AZ, </address> <month> November </month> <year> 1997, </year> <note> IEEE. </note>
Reference-contexts: Conventional implementations of CORBA incur significant demultiplexing overhead. In particular, [5, 17] show that ~17% of the total server processing time is spent demultiplex-ing requests. Unless this overhead is reduced and demultiplex-ing is performed predictably, ORBs cannot provide consistent quality of service guarantees to applications. Prior work <ref> [18] </ref> analyzed the impact of various IDL skeleton demultiplexing techniques such as linear search and direct demultiplexing. However, in many applications the number of operations defined per-IDL interface is relatively small and static, compared to the number of potential servants, which can be quite large and dynamic. <p> The experiments conducted for the Orbix 2.1 and Vis-iBroker 2.0 ORBs use the request train and round robin invocation algorithms described below. Section 5 describes how we applied active demultiplexing and perfect hashing to optimized demultiplexing in our high-performance, real-time ORB called TAO <ref> [19, 18] </ref>. To test these 6 strategies, we developed two additional request invocation al-gorithms called random invocation and worst-case invocation, respectively. These algorithms are used to evaluate the predictability, consistency, and scalability properties of TAO's demultiplexing strategies, and to compare their performance with the worst-case performance of linear-search demultiplex-ing. <p> Eliminating the demultiplexing overhead requires de-layered strategies and fast, flexible message demul-tiplexing <ref> [20, 18] </ref>. Eliminating the presentation layer overhead requires optimized stub generators [21, 9] for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> In contrast, TAO utilizes perfect hashing and active demultiplexing in conjunction with explicit dynamic linking <ref> [18] </ref> shown in and configure optimal demultiplexing of client requests within ORB endsystems. Inefficient presentation layer conversions Conventional ORBs are not optimized to generate efficient stubs and skeletons. <p> Layered multiplexing and demultiplexing is generally disparaged for high-performance communication systems [16] due to the additional overhead incurred at each layer.[20] describes a fast and flexible message demultiplexing strategy based on dynamic code generation. <ref> [18] </ref> evaluates the performance of alternative demultiplexing strategies for real-time CORBA. Our results for latency measurements have shown that with increasing number of servants, the latency increases. This is partly due to the additional overhead of demultiplexing the request to the appropriate operation of the appropriate servant. <p> Our results for latency measurements have shown that with increasing number of servants, the latency increases. This is partly due to the additional overhead of demultiplexing the request to the appropriate operation of the appropriate servant. TAO uses a de-layered demultiplexing architecture <ref> [18] </ref> that can select optimal demultiplexing strategies based on compile-time and run-time analysis of CORBA IDL interfaces. 7 Concluding Remarks An important class of applications (such as avionics, distributed interactive simulation, and telecommunication systems) require scalable, low-latency communication.
Reference: [19] <author> Douglas C. Schmidt, David L. Levine, and Sumedh Mungee, </author> <title> The Design and Performance of Real-Time Object Request Brokers, </title> <journal> Computer Communications, </journal> <note> 1997, to appear. </note>
Reference-contexts: The experiments conducted for the Orbix 2.1 and Vis-iBroker 2.0 ORBs use the request train and round robin invocation algorithms described below. Section 5 describes how we applied active demultiplexing and perfect hashing to optimized demultiplexing in our high-performance, real-time ORB called TAO <ref> [19, 18] </ref>. To test these 6 strategies, we developed two additional request invocation al-gorithms called random invocation and worst-case invocation, respectively. These algorithms are used to evaluate the predictability, consistency, and scalability properties of TAO's demultiplexing strategies, and to compare their performance with the worst-case performance of linear-search demultiplex-ing. <p> We have used these results to guide the development of a high-performance, real-time ORB, called TAO <ref> [19, 4] </ref>. <p> application timing requirements are specified and enforced end-to-end [22]. * Real-time Object Adapter In addition to associating servants with the ORB and demultiplexing incoming requests to servants, TAO's Real-time Object Adapter (ROA) dispatches servant operations in accordance with various real-time scheduling strategies such as Rate Monotonic and Earliest Deadline First <ref> [19] </ref>. * ORB QoS Interface Applications use the ORB QoS interface to map real-time processing requirements to ORB endsys-tem/network resources. Common real-time processing requirements include end-to-end latency bounds and periodic scheduling deadlines. <p> In contrast, TAO integrates a high-performance I/O subsystem and the APIC network adapter with its ORB Core and optimized Object Adapter to produce a real-time ORB endsystem <ref> [19] </ref> that interoperates seamlessly with IIOP-compliant ORBs. Non-optimized buffering algorithms used for network reads and writes Conventional ORBS utilize non-optimized internal buffers for writing to and reading from the network, as shown in Section 4.3. <p> For this reason, we are using the perfect hashing demultiplexing strategy in the TAO ORB we are building for real-time avionics applications <ref> [19, 4] </ref>. 5.4 Reducing ORB Latency with IIOP Opti mizations To expedite the research goals of the TAO project, and to avoid re-inventing existing components, we based TAO on SunSoft IIOP, which is a freely available reference implementation of the Internet Inter-ORB Protocol (IIOP). <p> Our goal in precisely pinpointing the sources of overhead for CORBA is to optimize the performance of TAO <ref> [19] </ref> TAO is a high-performance, real-time ORB endsystem designed to meet the QoS requirements of bandwidth- and delay-sensitive applications.
Reference: [20] <author> Dawson R. Engler and M. Frans Kaashoek, DPF: </author> <title> Fast, Flexible Message Demultiplexing using Dynamic Code Generation, </title> <booktitle> in Proceedings of ACM SIGCOMM '96 Conference in Computer Communication Review, </booktitle> <institution> Stanford University, California, USA, </institution> <month> August </month> <year> 1996, </year> <pages> pp. 5359, </pages> <publisher> ACM Press. </publisher>
Reference-contexts: Eliminating the demultiplexing overhead requires de-layered strategies and fast, flexible message demul-tiplexing <ref> [20, 18] </ref>. Eliminating the presentation layer overhead requires optimized stub generators [21, 9] for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing.
Reference: [21] <author> Sean W. O'Malley, Todd A. Proebsting, and Allen B. Montz, </author> <title> USC: A Universal Stub Compiler, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIG-COMM), </booktitle> <address> London, UK, </address> <month> Aug. </month> <year> 1994. </year>
Reference-contexts: Eliminating the demultiplexing overhead requires de-layered strategies and fast, flexible message demul-tiplexing [20, 18]. Eliminating the presentation layer overhead requires optimized stub generators <ref> [21, 9] </ref> for richly-typed data. * Demultiplexing overhead The Orbix demultiplexing performs worse than VisiBroker demultiplexing since Orbix uses a linear search strategy based on string comparisons for operation demultiplexing. <p> IDL compilers translate interfaces written in an description language to other forms such as a network wire format. Eliminating the overhead of presentation layer conversions requires highly optimized stub compilers (e.g., Universal Stub Compiler <ref> [21] </ref>) and the Flick IDL compiler [9]. The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) [24].
Reference: [22] <author> Victor Fay Wolfe, Lisa Cingiser DiPippo, Roman Ginis, Michael Squadrito, Steven Wohlever, Igor Zykh, and Rus-sel Johnston, </author> <title> Real-Time CORBA, </title> <booktitle> in Proceedings of the Third IEEE Real-Time Technology and Applications Symposium, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: following enhancements designed to overcome the shortcomings of conventional ORBs for high-performance and real-time applications: * Real-time IDL Stubs and Skeletons In addition to marshaling and demarshaling of operation parameters, TAO's Real-time IDL (RIDL) stubs and skeletons are responsible for ensuring that application timing requirements are specified and enforced end-to-end <ref> [22] </ref>. * Real-time Object Adapter In addition to associating servants with the ORB and demultiplexing incoming requests to servants, TAO's Real-time Object Adapter (ROA) dispatches servant operations in accordance with various real-time scheduling strategies such as Rate Monotonic and Earliest Deadline First [19]. * ORB QoS Interface Applications use the ORB
Reference: [23] <author> Zubin D. Dittia, Guru M. Parulkar, and Jr. Jerome R. Cox, </author> <title> The APIC Approach to High Performance Network Interface Design: Protected DMA and Other Techniques, </title> <booktitle> in Proceedings of INFOCOM '97, </booktitle> <address> Kobe, Japan, </address> <month> April </month> <year> 1997, </year> <note> IEEE. </note>
Reference-contexts: subsystem TAO's real-time I/O subsystem performs admission control and assigns priorities to real-time I/O threads so that the schedulability of application components and ORB endsystem resources can be guaranteed. * High-speed network adapters TAO's I/O subsystem contains a daisy-chained interconnect comprising a number of ATM Port Interconnect Controller (APIC) chips <ref> [23] </ref>. APIC is designed to sustain an aggregate bi-directional data rate of 2.4 Gbps.
Reference: [24] <author> Phillip Hoschka and Christian Huitema, </author> <title> Automatic Generation of Optimized Code for Marshalling Routines, </title> <booktitle> in IFIP Conference of Upper Layer Protocols, Architectures and Applications ULPAA'94, </booktitle> <address> Barcelona, Spain, 1994, </address> <publisher> IFIP. </publisher>
Reference-contexts: In contrast, TAO produces and configures multiple encoding/decoding strategies for interface definition language (IDL) descriptions. Each strategy can be configured for different time/space tradeoffs between compiled vs. interpreted OMG IDL stubs and skeletons <ref> [24] </ref>, and the application's use of parameters (e.g., pass-without-touching, read-only, mutable). Excessive data copying and intra-ORB calls Conventional ORBs are not optimized to reduce the overhead of data copies. In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. <p> The generated stub code must make an optimal tradeoff between compiled code (which is efficient, but large in size) and interpreted code (which is slow, but compact) <ref> [24] </ref>. Our earlier results [5, 6] have presented detailed measurements of presentation layer overhead for transmitting richly-typed data. Our results for sending structs reveal that with increasing sender buffer sizes, the marshaling overhead increases, thereby increasing the latency.
Reference: [25] <author> Jong-Deok Choi, Ron Cytron, and Jeanne Ferrante, </author> <title> Automatic Construction of Sparse Data Flow Evaluation Graphs, </title> <booktitle> in Conference Record of the Eighteenth Annual ACE Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Excessive data copying and intra-ORB calls Conventional ORBs are not optimized to reduce the overhead of data copies. In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques, such as program flow analysis <ref> [25, 26] </ref> and integrated layer processing (ILP) [1] to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls. Most importantly, this streamlining can be performed without requiring modifications to the standard CORBA specification.
Reference: [26] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Weg-man, and F. Kenneth Zadeck, </author> <title> Efficiently Computing Static Single Assignment Form and the Control Dependence Graph, </title> <journal> in ACM Transactions on Programming Languages and Systems. ACM, </journal> <month> October </month> <year> 1991. </year>
Reference-contexts: Excessive data copying and intra-ORB calls Conventional ORBs are not optimized to reduce the overhead of data copies. In addition, these ORBs suffer from excessive intra-ORB function call overhead as shown in Section 4.3. In contrast, TAO uses advanced compiler techniques, such as program flow analysis <ref> [25, 26] </ref> and integrated layer processing (ILP) [1] to automatically omit unnecessary data copies between the CORBA infrastructure and applications. In addition, ILP reduces the overhead of excessive intra-ORB function calls. Most importantly, this streamlining can be performed without requiring modifications to the standard CORBA specification.
Reference: [27] <author> Douglas C. Schmidt and Chris Cleeland, </author> <title> Applying Patterns to Develop Extensible and Maintainable ORB Middleware, </title> <journal> Communications of the ACM, </journal> <note> to appear, </note> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: As a result, we designed TAO's Object Adapter to support multiple demultiplex-ing strategies <ref> [27] </ref>.
Reference: [28] <author> Douglas C. Schmidt, GPERF: </author> <title> A Perfect Hash Function Generator, </title> <booktitle> in Proceedings of the 2 nd C++ Conference, </booktitle> <address> San Fran-cisco, California, </address> <month> April </month> <year> 1990, </year> <booktitle> USENIX, </booktitle> <pages> pp. 87102. 24 </pages>
Reference-contexts: In many hard real-time systems (such as avionic control systems [4]), the servants and operations can be configured statically. In this scenario, it is possible to use perfect hashing to hash the servant and operations. For our experiment, we used the GNU gperf <ref> [28] </ref> tool to generate perfect hash functions for object keys and operation names.
Reference: [29] <author> George Varghese, </author> <title> Algorithmic Techniques for Efficient Pro--tocol Implementations , in SIGCOMM '96 Tutorial, </title> <publisher> Stanford, </publisher> <address> CA, </address> <month> August </month> <year> 1996, </year> <note> ACM. </note>
Reference-contexts: In particular, it has (1) high invocation overhead for small, frequently called methods, (2) repeated computation of invariant values, (3) excessive memory management and data copying overhead, and (4) inefficient, large functions that overflow the process cache. To alleviate this overhead, we applied a set of principle-based optimizations <ref> [29] </ref> to SunSoft IIOP in order to improve the performance of TAO. Table 3 summarizes the optimization principles used in TAO.
Reference: [30] <author> Jonathan Kay and Joseph Pasquale, </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP, </title> <booktitle> in Proceedings of SIGCOMM '93, </booktitle> <address> San Francisco, CA, </address> <month> September </month> <year> 1993, </year> <booktitle> ACM, </booktitle> <pages> pp. 259269. </pages>
Reference-contexts: They also show that the TCP delay characteristics are predictable and that it varies with the throughput. <ref> [30] </ref> present detailed measurements of various categories of processing overhead times of TCP/IP and UDP/IP.
Reference: [31] <author> Christos Papadopoulos and Gurudatta Parulkar, </author> <title> Experimental Evaluation of SUNOS IPC and TCP/IP Protocol Implementation, </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. </pages> <address> 199216, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The authors show that most messages sent are short (less than 200 bytes). They claim that these overheads are hard to eliminate and techniques such as integrated layer processing can be used to reduce the overhead. <ref> [31] </ref> presents performance results of the SunOS 4.x IPC and TCP/IP implementations. They show that increasing the socket buffer sizes improves the IPC performance.
Reference: [32] <author> S. J. Leffler, M.K. McKusick, M.J. Karels, and J.S. Quarterman, </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: They show that increasing the socket buffer sizes improves the IPC performance. They also show that the socket layer overhead is more significant on the receiver side. <ref> [32] </ref> discusses the TCP NODELAY option, which allows TCP to send small packets as soon as possible to reduce latency.
Reference: [33] <author> Isabelle Chrisment, </author> <title> Impact of ALF on Communication Subsystems Design and Performance, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 33, 34] </ref> and Integrated Layer Processing (ILP) [1, 35, 36]. ILP ensures that lower layer protocols deal with data in units specified by the application.
Reference: [34] <author> Atanu Ghosh, Jon Crowcroft, Michael Fry, and Mark Hand-ley, </author> <title> Integrated Layer Video Decoding and Application Layer Framed Secure Login: General Lessons from Two or Three Very Different Applications, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) <ref> [1, 33, 34] </ref> and Integrated Layer Processing (ILP) [1, 35, 36]. ILP ensures that lower layer protocols deal with data in units specified by the application.
Reference: [35] <author> M. Abbott and L. Peterson, </author> <title> Increasing Network Throughput by Integrating Protocol Layers, </title> <journal> ACM Transactions on Networking, </journal> <volume> vol. 1, no. 5, </volume> <month> October </month> <year> 1993. </year>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 33, 34] and Integrated Layer Processing (ILP) <ref> [1, 35, 36] </ref>. ILP ensures that lower layer protocols deal with data in units specified by the application.
Reference: [36] <author> Antony Richards, Ranil De Silva, Anne Fladenmuller, Aruna Seneviratne, and Michael Fry, </author> <title> The Application of ILP/ALF to Configurable Protocols, </title> <booktitle> in First International Workshop on High Performance Protocol Architectures, HIPPARCH '94, </booktitle> <institution> Sophia Antipolis, France, </institution> <month> December </month> <year> 1994, </year> <institution> INRIA France. </institution>
Reference-contexts: One proposed remedy for this problem is to use Application Level Framing (ALF) [1, 33, 34] and Integrated Layer Processing (ILP) <ref> [1, 35, 36] </ref>. ILP ensures that lower layer protocols deal with data in units specified by the application.
Reference: [37] <author> Torsten Braun and Christophe Diot, </author> <title> Protocol Implementation Using Integrated Layer Processnig, </title> <booktitle> in Proceedings of the Symposium on Communications Architectures and Protocols (SIG-COMM). ACM, </booktitle> <month> September </month> <year> 1995. </year> <month> 25 </month>
Reference-contexts: ILP ensures that lower layer protocols deal with data in units specified by the application. ILP provides the implementor with the option of performing all data manipulations in one or two integrated processing loops, rather than manipulating the data sequentially. <ref> [37] </ref> have shown that although ILP reduces the number of memory accesses, it does not reduce the number of cache misses compared to a carefully designed non-ILP implementation.
References-found: 37

