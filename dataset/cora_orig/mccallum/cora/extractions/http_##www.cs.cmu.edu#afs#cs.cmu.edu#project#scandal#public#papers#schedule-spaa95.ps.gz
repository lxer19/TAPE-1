URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/schedule-spaa95.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/schedule-spaa95.html
Root-URL: 
Email: blelloch@cs.cmu.edu  gibbons@research.att.com  matias@research.att.com  
Title: Provably Efficient Scheduling for Languages with Fine-Grained Parallelism  
Author: Guy E. Blelloch Phillip B. Gibbons Yossi Matias 
Address: Pittsburgh, PA 15213-3891  600 Mountain Avenue Murray Hill, NJ 07974  600 Mountain Avenue Murray Hill, NJ 07974  
Affiliation: School of Computer Science Carnegie Mellon University  AT&T Bell Laboratories  AT&T Bell Laboratories  
Abstract: Many high-level parallel programming languages allow for fine-grained parallelism. As in the popular work-time framework for parallel algorithm design, programs written in such languages can express the full parallelism in the program without specifying the mapping of program tasks to processors. A common concern in executing such programs is to dynamically schedule tasks to processors so as to not only minimize the execution time, but also to minimize the amount of memory needed. Without careful scheduling, the parallel execution on p processors can use a factor of p or larger more memory than a sequential implementation of the same program. This paper first identifies a class of parallel schedules that are provably efficient in both time and space, even for programs whose task structure is revealed only during execution. For programs with sufficient parallelism, the schedule guarantees that the amount of memory used by the program is within a factor of 1 + o(1) of a sequential implementation. This space bound is obtained by proving a graph-theoretic result relating parallel and sequential traversals of directed acyclic graphs. The paper then describes an efficient dynamic scheduling algorithm that generates schedules in this class, for languages with nested fine-grained parallelism. The algorithm is relatively simple, performing the necessary processor allocation and task synchronization while incurring at most a constant factor overhead in time and space. The correctness and performance guarantees of the algorithm rely on properties of depth-first-like traversals of series-parallel graphs. The algorithm is the first efficient solution to the scheduling problem discussed here, even if space considerations are ignored. 
Abstract-found: 1
Intro-found: 1
Reference: [ANP89] <author> Arvind, R. S. Nikhil, and K. K. Pingali. </author> <title> I structures: Data structures for parallel computing. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 11(4) </volume> <pages> 598-632, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Many high-level parallel programming languages encourage the use of dynamic fine-grained parallelism. Such languages include both data-parallel languages such as HPF [Hig93] and Nesl [BCH + 94], as well as control-parallel languages such as ID <ref> [ANP89] </ref>, Sisal [FCO90] or Proteus [MNP + 90]. The goal of these languages is to have the user expose the full parallelism in an algorithm, which is often much more than the number of processors that will be used, and have the language implementation schedule the fine-grained parallelism onto processors.
Reference: [BCH + 94] <author> Guy E. Blelloch, Siddhartha Chatterjee, Jonathan C. Hardwick, Jay Sipelstein, and Marco Zagha. </author> <title> Implementation of a portable nested data-parallel language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 4-14, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Many high-level parallel programming languages encourage the use of dynamic fine-grained parallelism. Such languages include both data-parallel languages such as HPF [Hig93] and Nesl <ref> [BCH + 94] </ref>, as well as control-parallel languages such as ID [ANP89], Sisal [FCO90] or Proteus [MNP + 90]. <p> S 1 2 O (W )). These results apply to nearly all data-parallel languages, both nested and not, as well as most languages that supply fork-and-join style parallelism, even permitting arbitrary fanout and arbitrary nesting. In the appendix we show how the Nesl <ref> [Ble93, BCH + 94] </ref> language, a nested data-parallel language, maps onto the appropriate type of dags. Together with previous and recent results on automatic memory allocation [GMV91, BGM95], our algorithm can be used for time-, work-, and space-efficient execution of programs written in these languages.
Reference: [BG95] <author> Guy E. Blelloch and John Greiner. </author> <title> Parallelism in se quential functional languages. </title> <booktitle> In Proceedings of the Symposium on Functional Programming and Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: With this implementation, a contained Nesl program with W work and D depth runs in O (W=p+D lg p) time on a p-processor pram. Similar results where shown by Suciu and Tannen for a parallel language based on while loops and map recursion [ST94]. Blelloch and Greiner <ref> [BG95] </ref> proved that the nested-parallelism available in call-by-value functional languages can be mapped onto the pram with the same time bounds as given in this paper. This parallelism is limited to binary fanout.
Reference: [BGM95] <author> G.E. Blelloch, P.B. Gibbons, and Y. Matias, </author> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: In the appendix we show how the Nesl [Ble93, BCH + 94] language, a nested data-parallel language, maps onto the appropriate type of dags. Together with previous and recent results on automatic memory allocation <ref> [GMV91, BGM95] </ref>, our algorithm can be used for time-, work-, and space-efficient execution of programs written in these languages. The algorithm is the first efficient solution to the scheduling problem discussed here, even if space considerations are ignored. Approach and key ideas. <p> In a recent work <ref> [BGM95] </ref>, we have developed data structures that are weaker than the general dictionary data structure, yet sufficient for our purposes, that obtain a simpler execution on the crcw pram; allowing a logarithmic factor overhead in space, our data structure enables linear work, logarithmic time execution on an erew pram.
Reference: [BL93] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Space-efficient scheduling of multithreaded computations. </title> <booktitle> In Proc. 25th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 362-371, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: These are specified by placing upper bounds on the running time and the space of the implementation as a function of the work, depth and sequential space. As with the work of Blumofe and Leiser-son <ref> [BL93, BL94] </ref>, we formalize the notion of work, depth and space, by modeling computations as directed acyclic graphs (dags) that may unfold dynamically as the computation proceeds. <p> Thus for programs with sufficient parallelism (i.e. S 1 =p D, recalling that S 1 is at least the size of the input), this is within a factor of 1 + o (1) of the sequential space. Previously, the best known bound was S 1 p <ref> [Bur88, BL93, BL94, BS94] </ref>, a factor of p from the sequential space. These bounds apply when individual tasks allocate at most a constant amount of memory. <p> The above bounds do not account for overheads to implement the schedule. Approach and key ideas. To obtain a good bound on the number of steps, we use the common approach (e.g. <ref> [BL93] </ref>) of greedily scheduling p independent nodes of the dag each step, if possible, where p is the number of processors. <p> By labeling individual nodes with their memory requirements, we allow for more fine-grained memory allocation than in previous models that associate memory requirements with entire threads in the computation <ref> [BL93, BL94] </ref>. Block memory allocations, e.g. for arrays, are indicated by nodes whose weight is the size of the block to be allocated. The primary question is which (greedy) parallel schedules, if any, have provably good space bounds on all computation dags. <p> There have been several previous works on scheduling static dags (e.g. [PU87, PY88]). Memory usage was not considered in any of this work. An early work that provided provable space bounds for tree-structured programs was due to Burton [Bur88]. Blumofe and Leiserson <ref> [BL93, BL94] </ref> considered space, time and communication bounds for a multithreaded model in which each thread contains any number of tasks. They showed that with their model and scheduling scheme the parallel space is bounded by O (S 1 p) and parallel time is bounded by O (W=p+D). <p> The frontier of G at step i in T is the set of all nodes scheduled prior to i with an unscheduled child node. A greedy p-traversal <ref> [BL93] </ref> is a p-traversal such that at each step i, if at least p nodes are ready, then jV i j = p, and if fewer than p are ready, then V i consists of all the ready nodes. <p> Dynamically unfolding DAGs. We model a computation as a dag that unfolds dynamically as the program executes on a given input. As in previous work (e.g. <ref> [PY88, BL93, BL94] </ref>), we assume the programs are deterministic, in the sense that the dag for a computation does not depend on the order in which nodes are scheduled. There is a node in the dag for each unit-work task in the computation, which we identify with the task.
Reference: [BL94] <author> R. D. Blumofe and C. E. Leiserson. </author> <title> Scheduling mul tithreaded computations by work stealing. </title> <booktitle> In Proc. 35th IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: These are specified by placing upper bounds on the running time and the space of the implementation as a function of the work, depth and sequential space. As with the work of Blumofe and Leiser-son <ref> [BL93, BL94] </ref>, we formalize the notion of work, depth and space, by modeling computations as directed acyclic graphs (dags) that may unfold dynamically as the computation proceeds. <p> Thus for programs with sufficient parallelism (i.e. S 1 =p D, recalling that S 1 is at least the size of the input), this is within a factor of 1 + o (1) of the sequential space. Previously, the best known bound was S 1 p <ref> [Bur88, BL93, BL94, BS94] </ref>, a factor of p from the sequential space. These bounds apply when individual tasks allocate at most a constant amount of memory. <p> By labeling individual nodes with their memory requirements, we allow for more fine-grained memory allocation than in previous models that associate memory requirements with entire threads in the computation <ref> [BL93, BL94] </ref>. Block memory allocations, e.g. for arrays, are indicated by nodes whose weight is the size of the block to be allocated. The primary question is which (greedy) parallel schedules, if any, have provably good space bounds on all computation dags. <p> There have been several previous works on scheduling static dags (e.g. [PU87, PY88]). Memory usage was not considered in any of this work. An early work that provided provable space bounds for tree-structured programs was due to Burton [Bur88]. Blumofe and Leiserson <ref> [BL93, BL94] </ref> considered space, time and communication bounds for a multithreaded model in which each thread contains any number of tasks. They showed that with their model and scheduling scheme the parallel space is bounded by O (S 1 p) and parallel time is bounded by O (W=p+D). <p> Dynamically unfolding DAGs. We model a computation as a dag that unfolds dynamically as the program executes on a given input. As in previous work (e.g. <ref> [PY88, BL93, BL94] </ref>), we assume the programs are deterministic, in the sense that the dag for a computation does not depend on the order in which nodes are scheduled. There is a node in the dag for each unit-work task in the computation, which we identify with the task.
Reference: [Ble90] <author> G. E. Blelloch. </author> <title> Vector Models for Data-Parallel Com puting. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Also, as with the work of Blumofe and Leiserson, their space bounds are O (S 1 p) rather than our S 1 + O (p D). Provable time bounds for mapping nested data-parallel languages onto the pram were considered in <ref> [Ble90] </ref>. These results were used for implementing the Nesl language [Ble93] but the time bounds are restricted to a class of programs that are called contained. With this implementation, a contained Nesl program with W work and D depth runs in O (W=p+D lg p) time on a p-processor pram.
Reference: [Ble93] <author> G. E. Blelloch. NESL: </author> <title> A nested data-parallel lan guage (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year> <note> Updated version of Technical Report CMU-CS-92-103, </note> <institution> Carnegie Mellon U., </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: S 1 2 O (W )). These results apply to nearly all data-parallel languages, both nested and not, as well as most languages that supply fork-and-join style parallelism, even permitting arbitrary fanout and arbitrary nesting. In the appendix we show how the Nesl <ref> [Ble93, BCH + 94] </ref> language, a nested data-parallel language, maps onto the appropriate type of dags. Together with previous and recent results on automatic memory allocation [GMV91, BGM95], our algorithm can be used for time-, work-, and space-efficient execution of programs written in these languages. <p> Provable time bounds for mapping nested data-parallel languages onto the pram were considered in [Ble90]. These results were used for implementing the Nesl language <ref> [Ble93] </ref> but the time bounds are restricted to a class of programs that are called contained. With this implementation, a contained Nesl program with W work and D depth runs in O (W=p+D lg p) time on a p-processor pram.
Reference: [Ble95] <author> G. E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <note> 1995. To appear. </note>
Reference-contexts: Note that the standard depth-first sequential schedule of this graph uses only fi (n 2 ) space, counting the space for the input and output matrices. grams (such as for parallel quicksort <ref> [JaJ92, Ble95] </ref>) whose task structure or data dependencies are revealed only as the execution proceeds. The work of a computation corresponds to the number of nodes in the dag, and the depth corresponds to the longest path in the dag.
Reference: [Bre74] <author> R. P. Brent. </author> <title> The parallel evaluation of general arith metic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-208, </pages> <year> 1974. </year>
Reference-contexts: Time bounds are proven under various scenarios, including a result that a particular greedy schedule is within a factor of two of optimal. Brent <ref> [Bre74] </ref> showed that a level-by-level schedule of a dag obtains a time bound within a factor of two of optimal. There have been several previous works on scheduling static dags (e.g. [PU87, PY88]). Memory usage was not considered in any of this work. <p> Our upper bounds on space hold for any p-traversal based on a 1-traversal, greedy or not. An advantage of focusing on greedy p-traversals is that they are provably efficient in their number of steps. Blumofe and Leiserson, generalizing a result due to Brent <ref> [Bre74] </ref>, showed the following: Theorem 4.1 ([BL93]) For any dag of n nodes and depth d, and for any p 1, the number of parallel steps in any greedy p-traversal T p is at most n=p + d.
Reference: [BS81] <author> F. W. Burton and M. R. Sleep. </author> <title> Executing func tional programs on a virtual tree of processors. </title> <booktitle> In Proc. Conf. on Functional Programming Languages and Computer Architecture, </booktitle> <pages> pages 187-194, </pages> <month> October </month> <year> 1981. </year>
Reference-contexts: pseudo-code for matrix multiplication would require fi (n 3 ) space, whereas a sequential computation requires only fi (n 2 ) space. (See Figure 1.) In order to obtain the same bounds for a parallel implementation, heuristic techniques that limit the amount of parallelism in the implementation have been used <ref> [BS81, Hal85, RS87, CA88] </ref>, but these are not guaranteed to be space efficient in general. In this paper we are interested in specifying universal implementations that guarantee performance bounds, both in terms of time and space.
Reference: [BS94] <author> F. W. Burton and D. J. Simpson. </author> <title> Space effi cient execution of deterministic parallel programs. </title> <type> Manuscript., </type> <month> December </month> <year> 1994. </year>
Reference-contexts: Thus for programs with sufficient parallelism (i.e. S 1 =p D, recalling that S 1 is at least the size of the input), this is within a factor of 1 + o (1) of the sequential space. Previously, the best known bound was S 1 p <ref> [Bur88, BL93, BL94, BS94] </ref>, a factor of p from the sequential space. These bounds apply when individual tasks allocate at most a constant amount of memory. <p> These extensions allow us to cover a significantly wider class of applications and languages, but makes it much harder for us to account for communication costs. Recently, and independently, Burton and Simpson <ref> [BS94] </ref> developed a scheduling algorithm for dynamically unfolding dags. Their model, like ours, allows for dynamic memory allocation and for arbitrary fanin and fanout but differs in that it defines S 1 as the memory used in the worst case of all possible depth-first traversals.
Reference: [Bur88] <author> F. W. Burton. </author> <title> Storage management in virtual tree machines. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 37(3) </volume> <pages> 321-328, </pages> <year> 1988. </year>
Reference-contexts: Thus for programs with sufficient parallelism (i.e. S 1 =p D, recalling that S 1 is at least the size of the input), this is within a factor of 1 + o (1) of the sequential space. Previously, the best known bound was S 1 p <ref> [Bur88, BL93, BL94, BS94] </ref>, a factor of p from the sequential space. These bounds apply when individual tasks allocate at most a constant amount of memory. <p> There have been several previous works on scheduling static dags (e.g. [PU87, PY88]). Memory usage was not considered in any of this work. An early work that provided provable space bounds for tree-structured programs was due to Burton <ref> [Bur88] </ref>. Blumofe and Leiserson [BL93, BL94] considered space, time and communication bounds for a multithreaded model in which each thread contains any number of tasks.
Reference: [BV93] <author> O. Berkman and U. Vishkin. </author> <title> Recursive star-tree par allel data structure. </title> <journal> SIAM Journal on Computing, </journal> <volume> 22(2) </volume> <pages> 221-242, </pages> <year> 1993. </year>
Reference-contexts: Algorithms for approximate prefix-sums and for chaining are known to take O (t aps ), where t aps = lg lg p in the worst case and t aps = lg fl p with high probability <ref> [BV93, GMV94, GZ95, Rag93] </ref>.
Reference: [CA88] <author> D. E. Culler and Arvind. </author> <title> Resource requirements of dataflow programs. </title> <booktitle> In Proc. 15th International Symp. on Computer Architecture, </booktitle> <pages> pages 141-150, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: pseudo-code for matrix multiplication would require fi (n 3 ) space, whereas a sequential computation requires only fi (n 2 ) space. (See Figure 1.) In order to obtain the same bounds for a parallel implementation, heuristic techniques that limit the amount of parallelism in the implementation have been used <ref> [BS81, Hal85, RS87, CA88] </ref>, but these are not guaranteed to be space efficient in general. In this paper we are interested in specifying universal implementations that guarantee performance bounds, both in terms of time and space.
Reference: [CLR90] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1990. </year>
Reference-contexts: Graph terminology. We use standard graph terminology (see, e.g., <ref> [CLR90] </ref>), some of which is reviewed below. Consider a directed acyclic graph G.
Reference: [FCO90] <author> J. T. Feo, D. C. Cann, and R. R. Oldehoeft. </author> <title> A Report on the Sisal Language Project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> Decem-ber </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Many high-level parallel programming languages encourage the use of dynamic fine-grained parallelism. Such languages include both data-parallel languages such as HPF [Hig93] and Nesl [BCH + 94], as well as control-parallel languages such as ID [ANP89], Sisal <ref> [FCO90] </ref> or Proteus [MNP + 90]. The goal of these languages is to have the user expose the full parallelism in an algorithm, which is often much more than the number of processors that will be used, and have the language implementation schedule the fine-grained parallelism onto processors.
Reference: [GMV91] <author> J. Gil, Y. Matias, and U. Vishkin. </author> <title> Towards a theory of nearly constant time parallel algorithms. </title> <booktitle> In Proc. 32nd IEEE Symp. on Foundations of Computer Science, </booktitle> <pages> pages 698-710, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In the appendix we show how the Nesl [Ble93, BCH + 94] language, a nested data-parallel language, maps onto the appropriate type of dags. Together with previous and recent results on automatic memory allocation <ref> [GMV91, BGM95] </ref>, our algorithm can be used for time-, work-, and space-efficient execution of programs written in these languages. The algorithm is the first efficient solution to the scheduling problem discussed here, even if space considerations are ignored. Approach and key ideas. <p> Several recent works have studied automatic processor allocation in certain computation paradigms, such as task decaying algorithms, geometric decaying algorithms, and loosely-specified algorithms (see <ref> [GMV91, ?, ?, Mat92, MV91] </ref> and references therein). The techniques that were used, however, do not consider space bounds and are insufficient to cope with the model considered in this paper, since they assumed that whenever a thread goes to sleep, it is known precisely which step it will awake. <p> space linear in the number of representatives in the S data structure, or in the number of program variables, can be implemented with p processors and logarithmic time on the erew pram [PVW83], and in O (lg fl p) time and linear work with high probability, on a crcw pram <ref> [GMV91] </ref>.
Reference: [GMV94] <author> M.T. Goodrich, Y. Matias, and U. Vishkin. </author> <title> Optimal parallel approximation algorithms for prefix sums and integer sorting. </title> <booktitle> In Proc. 5th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <pages> pages 241-250, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Algorithms for approximate prefix-sums and for chaining are known to take O (t aps ), where t aps = lg lg p in the worst case and t aps = lg fl p with high probability <ref> [BV93, GMV94, GZ95, Rag93] </ref>.
Reference: [Gra66] <author> R. L. Graham. </author> <title> Bounds for certain multiprocess ing anomalies. </title> <journal> The Bell System Technical Journal, </journal> <volume> 45(9) </volume> <pages> 1563-1581, </pages> <year> 1966. </year>
Reference-contexts: Section 4 presents our results for provably efficient schedules, while Section 5 presents our scheduling algorithm. Further discussion is in Section 6, and the appendix describes the mapping of Nesl to our computation model. 2 Related work In early parallel scheduling work, Graham <ref> [Gra66, Gra69] </ref> modeled a parallel computation as a dag of tasks, where each task takes an arbitrary, but known number of time units and must be scheduled on a single processor without preemption.
Reference: [Gra69] <author> R. L. Graham. </author> <title> Bounds on multiprocessing tim ing anomalies. </title> <journal> SIAM J. of Applied Mathematics, </journal> <volume> 17(2) </volume> <pages> 416-429, </pages> <year> 1969. </year>
Reference-contexts: Section 4 presents our results for provably efficient schedules, while Section 5 presents our scheduling algorithm. Further discussion is in Section 6, and the appendix describes the mapping of Nesl to our computation model. 2 Related work In early parallel scheduling work, Graham <ref> [Gra66, Gra69] </ref> modeled a parallel computation as a dag of tasks, where each task takes an arbitrary, but known number of time units and must be scheduled on a single processor without preemption.
Reference: [GZ95] <author> T. Goldberg and U. Zwick. </author> <title> Optimal deterministic ap proximate parallel prefix sums and their applications. </title> <booktitle> In Proc. 3rd Israel Symp. on Theory of Computing and Systems, </booktitle> <pages> pages 220-228, </pages> <month> January </month> <year> 1995. </year>
Reference-contexts: Algorithms for approximate prefix-sums and for chaining are known to take O (t aps ), where t aps = lg lg p in the worst case and t aps = lg fl p with high probability <ref> [BV93, GMV94, GZ95, Rag93] </ref>.
Reference: [Hal85] <author> R. H. Halstead, Jr. </author> <title> Multilisp: A language for concurrent symbolic computation. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 7(4) </volume> <pages> 501-538, </pages> <year> 1985. </year>
Reference-contexts: pseudo-code for matrix multiplication would require fi (n 3 ) space, whereas a sequential computation requires only fi (n 2 ) space. (See Figure 1.) In order to obtain the same bounds for a parallel implementation, heuristic techniques that limit the amount of parallelism in the implementation have been used <ref> [BS81, Hal85, RS87, CA88] </ref>, but these are not guaranteed to be space efficient in general. In this paper we are interested in specifying universal implementations that guarantee performance bounds, both in terms of time and space.
Reference: [Hig93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Many high-level parallel programming languages encourage the use of dynamic fine-grained parallelism. Such languages include both data-parallel languages such as HPF <ref> [Hig93] </ref> and Nesl [BCH + 94], as well as control-parallel languages such as ID [ANP89], Sisal [FCO90] or Proteus [MNP + 90].
Reference: [HWe91] <editor> Paul Hudak, Philip Wadler, and Simon Peyton Jones (eds). </editor> <booktitle> Report on the Functional Programming Language HASKELL, </booktitle> <month> June </month> <year> 1991. </year> <note> Version 1.1. </note>
Reference: [JaJ92] <author> J. JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year>
Reference-contexts: Note that the standard depth-first sequential schedule of this graph uses only fi (n 2 ) space, counting the space for the input and output matrices. grams (such as for parallel quicksort <ref> [JaJ92, Ble95] </ref>) whose task structure or data dependencies are revealed only as the execution proceeds. The work of a computation corresponds to the number of nodes in the dag, and the depth corresponds to the longest path in the dag. <p> Finally, we assume that the number of outgoing edges from a node is revealed when a node is scheduled, even before any space has been allocated for the edges. Our scheduling algorithm performs a constant number of erew pram <ref> [JaJ92] </ref> steps and a constant number of parallel prefix-sums computations [LF80] for each round of scheduling. 5.1 A stack-based scheduling algorithm We will use the following property of 1-dfts on series-parallel dags.
Reference: [LF80] <author> R.E. Ladner and M.J. Fischer. </author> <title> Parallel prefix com putation. </title> <journal> Journal of the ACM, </journal> <volume> 27 </volume> <pages> 831-838, </pages> <year> 1980. </year>
Reference-contexts: Finally, we assume that the number of outgoing edges from a node is revealed when a node is scheduled, even before any space has been allocated for the edges. Our scheduling algorithm performs a constant number of erew pram [JaJ92] steps and a constant number of parallel prefix-sums computations <ref> [LF80] </ref> for each round of scheduling. 5.1 A stack-based scheduling algorithm We will use the following property of 1-dfts on series-parallel dags. <p> Each of these steps can be shown to take O (lg p) time on p-processors, as follows: A prefix-sums computation of size p lg p can be implemented on a p-processor erew pram or hypercube in O (lg p) time <ref> [LF80] </ref>. Using random hashing techniques, the shared memory of a (p lg p)-processor erew pram can be placed on a p-processor hypercube so that each step of a (p lg p)-processor erew pram can be implemented on the hypercube in O (lg p) time with high probability [Val90].
Reference: [Mat92] <author> Y. Matias. </author> <title> Highly Parallel Randomized Algorithmics. </title> <type> PhD thesis, </type> <institution> Tel Aviv University, Tel Aviv 69978, Is-rael, </institution> <year> 1992. </year>
Reference-contexts: Several recent works have studied automatic processor allocation in certain computation paradigms, such as task decaying algorithms, geometric decaying algorithms, and loosely-specified algorithms (see <ref> [GMV91, ?, ?, Mat92, MV91] </ref> and references therein). The techniques that were used, however, do not consider space bounds and are insufficient to cope with the model considered in this paper, since they assumed that whenever a thread goes to sleep, it is known precisely which step it will awake.
Reference: [MNP + 90] <author> P. H. Mills, L. S. Nyland, J. F. Prins, J. H. Reif, and R. A. Wagner. </author> <title> Prototyping parallel and distributed programs in Proteus. </title> <type> Technical Report UNC-CH TR90-041, </type> <institution> Computer Science Dept., University of North Carolina, </institution> <year> 1990. </year>
Reference-contexts: 1 Introduction Many high-level parallel programming languages encourage the use of dynamic fine-grained parallelism. Such languages include both data-parallel languages such as HPF [Hig93] and Nesl [BCH + 94], as well as control-parallel languages such as ID [ANP89], Sisal [FCO90] or Proteus <ref> [MNP + 90] </ref>. The goal of these languages is to have the user expose the full parallelism in an algorithm, which is often much more than the number of processors that will be used, and have the language implementation schedule the fine-grained parallelism onto processors.
Reference: [MV91] <author> Y. Matias and U. Vishkin. </author> <title> Converting high prob ability into nearly-constant time|with applications to parallel hashing. </title> <booktitle> In Proc. 23rd ACM Symp. on Theory of Computing, </booktitle> <pages> pages 307-316, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Several recent works have studied automatic processor allocation in certain computation paradigms, such as task decaying algorithms, geometric decaying algorithms, and loosely-specified algorithms (see <ref> [GMV91, ?, ?, Mat92, MV91] </ref> and references therein). The techniques that were used, however, do not consider space bounds and are insufficient to cope with the model considered in this paper, since they assumed that whenever a thread goes to sleep, it is known precisely which step it will awake.
Reference: [PU87] <author> C. H. Papadimitriou and J. D. Ullman. </author> <title> A communication-time tradeoff. </title> <journal> SIAM J. on Computing, </journal> <volume> 16(4) </volume> <pages> 639-646, </pages> <year> 1987. </year>
Reference-contexts: Brent [Bre74] showed that a level-by-level schedule of a dag obtains a time bound within a factor of two of optimal. There have been several previous works on scheduling static dags (e.g. <ref> [PU87, PY88] </ref>). Memory usage was not considered in any of this work. An early work that provided provable space bounds for tree-structured programs was due to Burton [Bur88].
Reference: [PVW83] <author> W.J. Paul, U. Vishkin, and H. Wagener. </author> <title> Parallel dic tionaries on 2-3 trees. </title> <booktitle> In Proc. 10th Int. Colloquium on Automata Languages and Programming, </booktitle> <publisher> Springer LNCS 154, </publisher> <pages> pages 597-609, </pages> <year> 1983. </year>
Reference-contexts: An adaptive allocation and deallocation of space, so as to maintain at each step space linear in the number of representatives in the S data structure, or in the number of program variables, can be implemented with p processors and logarithmic time on the erew pram <ref> [PVW83] </ref>, and in O (lg fl p) time and linear work with high probability, on a crcw pram [GMV91].
Reference: [PY88] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> Towards an architecture-independent analysis of parallel algorithms. </title> <booktitle> In Proc. 20th ACM Symp. on Theory of Computing, </booktitle> <pages> pages 510-513, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Brent [Bre74] showed that a level-by-level schedule of a dag obtains a time bound within a factor of two of optimal. There have been several previous works on scheduling static dags (e.g. <ref> [PU87, PY88] </ref>). Memory usage was not considered in any of this work. An early work that provided provable space bounds for tree-structured programs was due to Burton [Bur88]. <p> Dynamically unfolding DAGs. We model a computation as a dag that unfolds dynamically as the program executes on a given input. As in previous work (e.g. <ref> [PY88, BL93, BL94] </ref>), we assume the programs are deterministic, in the sense that the dag for a computation does not depend on the order in which nodes are scheduled. There is a node in the dag for each unit-work task in the computation, which we identify with the task.
Reference: [Rag93] <author> P.L. Ragde. </author> <title> The parallel simplicity of compaction and chaining. </title> <journal> Journal of Algorithms, </journal> <volume> 14 </volume> <pages> 371-380, </pages> <year> 1993. </year>
Reference-contexts: Algorithms for approximate prefix-sums and for chaining are known to take O (t aps ), where t aps = lg lg p in the worst case and t aps = lg fl p with high probability <ref> [BV93, GMV94, GZ95, Rag93] </ref>.
Reference: [RS87] <author> C. A. Ruggiero and J. Sargeant. </author> <title> Control of parallelism in the Manchester dataflow machine. </title> <booktitle> In Functional Programming Languages and Computer Architecture, Lecture Notes in Computer Science, </booktitle> <volume> Vol. 174, </volume> <pages> pages 1-15. </pages> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: pseudo-code for matrix multiplication would require fi (n 3 ) space, whereas a sequential computation requires only fi (n 2 ) space. (See Figure 1.) In order to obtain the same bounds for a parallel implementation, heuristic techniques that limit the amount of parallelism in the implementation have been used <ref> [BS81, Hal85, RS87, CA88] </ref>, but these are not guaranteed to be space efficient in general. In this paper we are interested in specifying universal implementations that guarantee performance bounds, both in terms of time and space.
Reference: [SDDS86] <author> J. T. Schwartz, R. B. K. Dewar, E. Dubinsky, and E. Schonberg. </author> <title> Programming with Sets: An Introduction to SETL. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference: [Sip95] <author> J. Sipelstein. </author> <title> Data Representation Optimizations for Collection-Oriented Languages. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA, </address> <year> 1995. </year> <note> To appear. </note>
Reference-contexts: How to keep this ordering and how much the heuristic really helps is an area of current research. Another optimization would be to reschedule computation threads only when there is a significant load imbalance among the processors (see, e.g. <ref> [Sip95] </ref>). An open question is whether our algorithm for time-, work-, and space-efficient execution of nested parallel programs can be extended so as to guarantee provably communication-efficient execution as well.
Reference: [ST94] <author> Dan Suciu and Val Tannen. </author> <title> Efficient compilation of high-level data parallel algorithms. </title> <booktitle> In Proc. 6th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <pages> pages 57-66, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: With this implementation, a contained Nesl program with W work and D depth runs in O (W=p+D lg p) time on a p-processor pram. Similar results where shown by Suciu and Tannen for a parallel language based on while loops and map recursion <ref> [ST94] </ref>. Blelloch and Greiner [BG95] proved that the nested-parallelism available in call-by-value functional languages can be mapped onto the pram with the same time bounds as given in this paper. This parallelism is limited to binary fanout.
Reference: [Val90] <author> L. G. Valiant. </author> <title> General purpose parallel architec tures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 943-972. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference-contexts: Using random hashing techniques, the shared memory of a (p lg p)-processor erew pram can be placed on a p-processor hypercube so that each step of a (p lg p)-processor erew pram can be implemented on the hypercube in O (lg p) time with high probability <ref> [Val90] </ref>. Thus the scheduling can be done in O (lg p) time. Likewise, the p lg p unit-work tasks can be performed in O (lg p) time. The program variable space is bounded as needed using Theorem 4.7.
References-found: 39

