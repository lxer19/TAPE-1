URL: http://www.mcs.anl.gov/~thakur/papers/access_reorg.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Phone: Tel (315) 443-4061  
Title: Access Reorganizations in Compiling Out-of-core Data Parallel Programs on Distributed Memory Machines using the Intel
Author: Rajesh Bordawekar Alok Choudhary Rajeev Thakur 
Address: 111 College Place, Rm. 3-228  Syracuse, NY 13244-4100  
Affiliation: Dept. of Elect. and Comp. Eng. and NPAC Syracuse University Northeast Parallel Architectures Center  Syracuse University  
Note: Data  This work was supported in part by NSF Young Investigator Award CCR-9357840, a grant from Intel SSD, and IBM, and in part by USRA CESDIS Contract 5555-26. This work was performed in part  was provided by CRPC.  
Abstract: NPAC Technical Report SCCS-622, Sept. 1994 Abstract This paper describes techniques for translating out-of-core programs written in a data parallel language like HPF to message passing node programs with explicit parallel I/O. We describe the basic compilation model and various steps involved in the compilation. The compilation process is explained with the help of an out-of-core matrix multiplication program. We first discuss how an out-of-core program can be translated by extending the method used for translating in-core programs. We demonstrate that straightforward extension of in-core compiler does not work for out-of-core programs. We then describe how the compiler can optimize the code by (1) estimating the I/O costs associated with different array access patterns, (2) reorganizing array accesses, (3) selecting the method with the least I/O cost, and (4) allocating memory according to access cost for competing out-of-core arrays. These optimizations can reduce the amount of I/O by as much as an order of magnitude. Performance results on the Intel Touchstone Delta are presented and analyzed. 
Abstract-found: 1
Intro-found: 1
Reference: [ASKL81] <author> W. Abu-Sufah, David Kuck, and D. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and program transformation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(5):341-356, </volume> <month> May </month> <year> 1981. </year> <month> 18 </month>
Reference-contexts: As explained earlier, using the loop bounds and index variables, the compiler can determine which array requires more I/O accesses and accordingly allocate the available memory. 5 Related Work Abu-Sufah first investigated strategies for improving performance of fortran programs in virtual memory environment <ref> [ASKL81] </ref>. Compiler transformations such as tiling, strip-mining, loop interchange, loop skewing are proposed by Wolfe [Wol89b]. Transformations like Unroll-and-jam and Scalar replacement are proposed by Carr [Car93]. Callahan studies the problem of register allocation [CCK90]. The notion reference window is proposed by Gannon et al. [GJG88].
Reference: [BBG + 93] <author> F. Bodin, P. Beckman, D. Gannon, S. Narayana, and S. Yang. </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language. </title> <booktitle> In Proceedings of the First Annual Object-Oriented Numerics Conference, </booktitle> <pages> pages 1-24, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: An overview of the various issues involved in high performance I/O is given in [dRC94]. Data parallel languages like HPF [For93] and pC++ <ref> [BBG + 93] </ref> have recently been developed to provide support for portable high performance programming on parallel machines. In order that these languages can be used for large scale scientific computations, support for performing large scale I/O from programs written in these languages is necessary.
Reference: [BCF + 93] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> Fortran 90D/HPF Compiler for Distributed Memory MIMD Computers: Design, Implementation, and Performance Results. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 351-360, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: This results in each processor having a local array associated with it. In an in-core program, the local array resides in a local memory of the processor. Our group at Syracuse University has developed a compiler for in-core HPF programs <ref> [BCF + 93] </ref>. For large data sets, however, local arrays cannot entirely fit in main memory. In such cases, parts of the local array have to be stored on disk. We refer to such a local array as an Out-of-core Local Array (OCLA). <p> The j th column of the result is computed using the intrinsic function SUM. 3.2 In-core Compilation Our research group at Syracuse University has developed a compiler to translate in-core HPF programs to message passing node programs for distributed memory machines <ref> [BCF + 93] </ref>. We will focus on compilation of FORALL statements in the HPF program. 1 A FORALL statement is essentially a parallel loop with copy-in-copy-out semantics [For93]. <p> Note that synchronization and copying are only part of the specification and can be avoided in most cases with appropriate compiler analysis <ref> [BCF + 93] </ref>. The compiler uses distribution directives (Figure 3, lines 3-7) in the source program to find the distribution pattern of the arrays. Using the data distribution information, the arrays are partitioned into local arrays. After data distribution, the compiler analyzes the array operations (Figure 3, lines 8-13).
Reference: [BDRR93] <author> Pierre Boulet, Alain Darte, Tanguy Risset, and Yves Robert. </author> <type> (Pen)-ultimate tiling? Technical Report 93-36, </type> <institution> Ecole Normale Superieure de Lyon, </institution> <address> 46, Alle'e d'Italie, 69364 Lyon Cedex 07, France, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: Ramanujam and Sa-dayappan use locality transformations to minimize interprocessor communication. They use a linear programming formulation to obtain optimal shape of the tile [RS90]. Further studies in optimal tiling are done by Boulet et al. <ref> [BDRR93] </ref>. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism [WL91]. Blockability in numerical algorithms has also been studied extensively [CK92, GJMS88]. An excellent description of the compiler transformations is given in [BGS93]. Most of this work is targeted for locality optimizations for sequential programs.
Reference: [BGS93] <author> David F. Bacon, Susan Graham, and Oliver J. Sharp. </author> <title> Compiler Transformations for High-Performance Computing. </title> <type> Technical Report UCB/CSD-93-781, </type> <institution> Computer Science Division, University of California, Berkeley, Computer Science Division, University of California, </institution> <address> Berkeleyley, California 94720, </address> <year> 1993. </year>
Reference-contexts: Further studies in optimal tiling are done by Boulet et al. [BDRR93]. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism [WL91]. Blockability in numerical algorithms has also been studied extensively [CK92, GJMS88]. An excellent description of the compiler transformations is given in <ref> [BGS93] </ref>. Most of this work is targeted for locality optimizations for sequential programs. Our work deals with locality optimizations for out-of-core programs running on distributed memory machines.
Reference: [Car93] <author> Steve Carr. </author> <title> Memory-Hierarchy Management. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Compiler transformations such as tiling, strip-mining, loop interchange, loop skewing are proposed by Wolfe [Wol89b]. Transformations like Unroll-and-jam and Scalar replacement are proposed by Carr <ref> [Car93] </ref>. Callahan studies the problem of register allocation [CCK90]. The notion reference window is proposed by Gannon et al. [GJG88]. The reference window is used by the compiler to study reuse in the program and perform corresponding transformations.
Reference: [CCK90] <author> David Callahan, Steve Carr, and Ken Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> Proc. of SIGPLAN'90 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: Compiler transformations such as tiling, strip-mining, loop interchange, loop skewing are proposed by Wolfe [Wol89b]. Transformations like Unroll-and-jam and Scalar replacement are proposed by Carr [Car93]. Callahan studies the problem of register allocation <ref> [CCK90] </ref>. The notion reference window is proposed by Gannon et al. [GJG88]. The reference window is used by the compiler to study reuse in the program and perform corresponding transformations. Irigoin and Triolet also propose transformations to improve locality [IT88]. 17 Schriber and Dongarra describe strategies to perform automatic tiling.
Reference: [CFPB93] <author> P. Corbett, D. Feitelson, J. Prost, and S. Baylor. </author> <title> Parallel Access to Files in the Vesta File System. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Unfortunately, the performance of the I/O subsystems of massively parallel computers has not kept pace with their processing and communications capabilities <ref> [CFPB93] </ref>. Hence, the performance bottleneck is the time taken to perform disk I/O. The need for high performance I/O is so significant that almost all the present generation parallel computers provide some kind of hardware and software support for parallel I/O.
Reference: [CK92] <author> Steve Carr and Ken Kennedy. </author> <title> Compiler Blockabilty of Numerical Algorithms. </title> <booktitle> Proc. of Supercomputing'92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Further studies in optimal tiling are done by Boulet et al. [BDRR93]. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism [WL91]. Blockability in numerical algorithms has also been studied extensively <ref> [CK92, GJMS88] </ref>. An excellent description of the compiler transformations is given in [BGS93]. Most of this work is targeted for locality optimizations for sequential programs. Our work deals with locality optimizations for out-of-core programs running on distributed memory machines.
Reference: [CMZ92] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. Scientific Programming, </title> <type> Vol.1, </type> <institution> No.1, </institution> <year> 1992. </year>
Reference-contexts: In order that these languages can be used for large scale scientific computations, support for performing large scale I/O from programs written in these languages is necessary. Issue of providing language support for high performance I/O has been addressed recently <ref> [CMZ92, Sni92] </ref>. It is, therefore, essential to provide compiler support for these languages so that the programs can be translated automatically and efficiently. 1.1 Contributions of the paper In this paper we describe data access reorganization strategies for efficient compilation of out-of-core data parallel programs on distributed memory machines.
Reference: [CR93] <institution> High Performance Computing and Communications: </institution> <note> Grand Challenges 1993 Report. A Report by the Committee on Physical, </note> <institution> Mathematical and Engineering Sciences, Federal Coordinating Council for Science, Engineering and Technology, </institution> <year> 1993. </year>
Reference-contexts: This is primarily due to the tremendous improvements in the computational speeds of parallel computers in the last few years. Many of these applications, also referred to as Grand Challenge Applications <ref> [CR93] </ref>, have computational requirements which stretch the capabilities of even the fastest supercomputer available today. In addition to requiring a great deal of computational power, these applications usually deal with large quantities of data.
Reference: [dRC94] <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: In addition to requiring a great deal of computational power, these applications usually deal with large quantities of data. At present, a typical Grand Challenge Application could require 1Gbyte to 4Tbytes of data per run <ref> [dRC94] </ref>. Main memories are not large enough to hold this much amount of data; so data needs to be stored on disks and fetched during the execution of the program. <p> The need for high performance I/O is so significant that almost all the present generation parallel computers provide some kind of hardware and software support for parallel I/O. An overview of the various issues involved in high performance I/O is given in <ref> [dRC94] </ref>. Data parallel languages like HPF [For93] and pC++ [BBG + 93] have recently been developed to provide support for portable high performance programming on parallel machines.
Reference: [FHK + 90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, and C. Tseng. </author> <title> Fortran D Language Specifications. </title> <type> Technical Report COMP TR90-141, </type> <institution> Rice University, </institution> <year> 1990. </year>
Reference-contexts: The compiler uses the information provided by these directives to compile global name space programs for distributed memory computers. Examples of parallel languages which support data distribution include Vienna Fortran [ZBC + 92], Fortran D <ref> [FHK + 90] </ref> and High Performance Fortran (HPF) [For93, KLS + 94]. In this paper, we describe the compilation of out-of-core HPF programs, but the discussion is applicable to any other data parallel language in general. <p> This operation is performed using a global sum reduction routine. Using the knowledge that the index j is in global name space and 1 Any array assignment statement can be converted into a corresponding FORALL statement, so we will use them interchangeably <ref> [FHK + 90, For93] </ref>. 5 parameter (n=64, nproc=4, local n=16) C Partition the arrays using the distribution information. real a (n,local n), b (local n,n), c (n,local n) do j=1, n Initialize the temporary array. do i= 1, local n do k=1, n temp (k,i) = a (k,i)fib (i,j) end do
Reference: [For93] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification. </title> <note> Version 1.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: The need for high performance I/O is so significant that almost all the present generation parallel computers provide some kind of hardware and software support for parallel I/O. An overview of the various issues involved in high performance I/O is given in [dRC94]. Data parallel languages like HPF <ref> [For93] </ref> and pC++ [BBG + 93] have recently been developed to provide support for portable high performance programming on parallel machines. In order that these languages can be used for large scale scientific computations, support for performing large scale I/O from programs written in these languages is necessary. <p> The compiler uses the information provided by these directives to compile global name space programs for distributed memory computers. Examples of parallel languages which support data distribution include Vienna Fortran [ZBC + 92], Fortran D [FHK + 90] and High Performance Fortran (HPF) <ref> [For93, KLS + 94] </ref>. In this paper, we describe the compilation of out-of-core HPF programs, but the discussion is applicable to any other data parallel language in general. The DISTRIBUTE directive in HPF specifies which elements of the array are mapped to each processor. <p> We will focus on compilation of FORALL statements in the HPF program. 1 A FORALL statement is essentially a parallel loop with copy-in-copy-out semantics <ref> [For93] </ref>. According to the HPF specifications, the following steps describe a correct sequential implementation of a FORALL statement; 1) copy rhs, 2) synchronize, 3) evaluate expression, 4) synchronize, 5) assign to lhs. <p> This operation is performed using a global sum reduction routine. Using the knowledge that the index j is in global name space and 1 Any array assignment statement can be converted into a corresponding FORALL statement, so we will use them interchangeably <ref> [FHK + 90, For93] </ref>. 5 parameter (n=64, nproc=4, local n=16) C Partition the arrays using the distribution information. real a (n,local n), b (local n,n), c (n,local n) do j=1, n Initialize the temporary array. do i= 1, local n do k=1, n temp (k,i) = a (k,i)fib (i,j) end do
Reference: [Fox91] <author> Geoffrey Fox. </author> <title> The architecture of problems and portable parallel software systems. </title> <type> Technical Report SCCS-78b, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, Syracuse, </institution> <address> NY 13244, </address> <year> 1991. </year>
Reference-contexts: In this model, parallelism is achieved by partitioning data among processors which effectively represents parallelism in a class of applications called loosely synchronous applications <ref> [Fox91] </ref>. To achieve load-balance, express locality of access, reduce communication and other optimizations, several distribution and data alignment strategies are often used (eg., block, cyclic, along rows, columns, etc.). Many parallel programming languages or language extensions have been developed which support such distributions.
Reference: [GJG88] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for the Cache and Local Memory Management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5(5) </volume> <pages> 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Compiler transformations such as tiling, strip-mining, loop interchange, loop skewing are proposed by Wolfe [Wol89b]. Transformations like Unroll-and-jam and Scalar replacement are proposed by Carr [Car93]. Callahan studies the problem of register allocation [CCK90]. The notion reference window is proposed by Gannon et al. <ref> [GJG88] </ref>. The reference window is used by the compiler to study reuse in the program and perform corresponding transformations. Irigoin and Triolet also propose transformations to improve locality [IT88]. 17 Schriber and Dongarra describe strategies to perform automatic tiling.
Reference: [GJMS88] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh. </author> <title> The impact of hierarchical memory systems on linear algebra algorithm design. </title> <journal> Intl. Journal of Supercomputing Applications, </journal> <volume> 2(1) </volume> <pages> 14-48, </pages> <year> 1988. </year>
Reference-contexts: Further studies in optimal tiling are done by Boulet et al. [BDRR93]. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism [WL91]. Blockability in numerical algorithms has also been studied extensively <ref> [CK92, GJMS88] </ref>. An excellent description of the compiler transformations is given in [BGS93]. Most of this work is targeted for locality optimizations for sequential programs. Our work deals with locality optimizations for out-of-core programs running on distributed memory machines.
Reference: [IT88] <author> Francois Irigoin and Remi Triolet. </author> <title> Supernode Partitioning. </title> <booktitle> POPL'88, Fifteenth Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: Callahan studies the problem of register allocation [CCK90]. The notion reference window is proposed by Gannon et al. [GJG88]. The reference window is used by the compiler to study reuse in the program and perform corresponding transformations. Irigoin and Triolet also propose transformations to improve locality <ref> [IT88] </ref>. 17 Schriber and Dongarra describe strategies to perform automatic tiling. They propose a linear algebraic formulation to find optimal size and shape of the data tile [SD90]. Ramanujam and Sa-dayappan use locality transformations to minimize interprocessor communication.
Reference: [KLS + 94] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, and M. Zosel. </author> <title> High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: The compiler uses the information provided by these directives to compile global name space programs for distributed memory computers. Examples of parallel languages which support data distribution include Vienna Fortran [ZBC + 92], Fortran D [FHK + 90] and High Performance Fortran (HPF) <ref> [For93, KLS + 94] </ref>. In this paper, we describe the compilation of out-of-core HPF programs, but the discussion is applicable to any other data parallel language in general. The DISTRIBUTE directive in HPF specifies which elements of the array are mapped to each processor.
Reference: [RS90] <author> J. Ramanujam and P. Sadayappan. </author> <title> Tiling of Iteration Spaces for Multicomputers. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <pages> pages II179-II186, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: They propose a linear algebraic formulation to find optimal size and shape of the data tile [SD90]. Ramanujam and Sa-dayappan use locality transformations to minimize interprocessor communication. They use a linear programming formulation to obtain optimal shape of the tile <ref> [RS90] </ref>. Further studies in optimal tiling are done by Boulet et al. [BDRR93]. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism [WL91]. Blockability in numerical algorithms has also been studied extensively [CK92, GJMS88]. An excellent description of the compiler transformations is given in [BGS93].
Reference: [SD90] <author> R. Schriber and J. Dongarra. </author> <title> Automatic Blocking of Nested Loops. </title> <type> Technical report, </type> <institution> Research Institute for Advanced Computer Science, </institution> <month> May </month> <year> 1990. </year>
Reference-contexts: Irigoin and Triolet also propose transformations to improve locality [IT88]. 17 Schriber and Dongarra describe strategies to perform automatic tiling. They propose a linear algebraic formulation to find optimal size and shape of the data tile <ref> [SD90] </ref>. Ramanujam and Sa-dayappan use locality transformations to minimize interprocessor communication. They use a linear programming formulation to obtain optimal shape of the tile [RS90]. Further studies in optimal tiling are done by Boulet et al. [BDRR93].
Reference: [Sni92] <author> Marc Snir. </author> <title> Proposal for IO. </title> <note> Posted to HPFF I/O Forum by Marc Snir, July 7 1992. 19 </note>
Reference-contexts: In order that these languages can be used for large scale scientific computations, support for performing large scale I/O from programs written in these languages is necessary. Issue of providing language support for high performance I/O has been addressed recently <ref> [CMZ92, Sni92] </ref>. It is, therefore, essential to provide compiler support for these languages so that the programs can be translated automatically and efficiently. 1.1 Contributions of the paper In this paper we describe data access reorganization strategies for efficient compilation of out-of-core data parallel programs on distributed memory machines.
Reference: [TBC94a] <author> R. Thakur, R. Bordawekar, and A. Choudhary. </author> <title> Compiler and Runtime Support for Out-of-Core HPF Programs. </title> <booktitle> In Proceedings of the 8 th ACM International Conference on Supercomputing, </booktitle> <pages> pages 382-391, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: This paper builds on our previous work on basic compilation techniques for out-of-core data parallel programs <ref> [TBC94a, TBC + 94b] </ref>.
Reference: [TBC + 94b] <author> R. Thakur, R. Bordawekar, A. Choudhary, R. Ponnusamy, and T. Singh. </author> <title> PASSION Runtime Library for Parallel I/O. </title> <booktitle> In Proceedings of the Scalable Parallel Libraries Conference, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: This paper builds on our previous work on basic compilation techniques for out-of-core data parallel programs <ref> [TBC94a, TBC + 94b] </ref>.
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Loop Transformation Theory and An Algorithm to Maximize Parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 452-471, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: They use a linear programming formulation to obtain optimal shape of the tile [RS90]. Further studies in optimal tiling are done by Boulet et al. [BDRR93]. Wolf and Lam propose an elegant loop transformation theory to improve locality and parallelism <ref> [WL91] </ref>. Blockability in numerical algorithms has also been studied extensively [CK92, GJMS88]. An excellent description of the compiler transformations is given in [BGS93]. Most of this work is targeted for locality optimizations for sequential programs. Our work deals with locality optimizations for out-of-core programs running on distributed memory machines.
Reference: [Wol89a] <author> M. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Each processor performs computation on the data in its ICLA. Some of the issues in out-of-core compilation are similar to compiler optimizations carried out to gain advantage of processor caches or pipelines. This optimization, commonly known as stripmining <ref> [Wol89a, ZC91] </ref>, sections the loop iterations so that data of a fixed size (equal to cache size or pipeline stages) could be operated on in each iteration.
Reference: [Wol89b] <author> M. J. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> Proceedings of Supercomputing'89, </booktitle> <month> November </month> <year> 1989. </year>
Reference-contexts: Compiler transformations such as tiling, strip-mining, loop interchange, loop skewing are proposed by Wolfe <ref> [Wol89b] </ref>. Transformations like Unroll-and-jam and Scalar replacement are proposed by Carr [Car93]. Callahan studies the problem of register allocation [CCK90]. The notion reference window is proposed by Gannon et al. [GJG88]. The reference window is used by the compiler to study reuse in the program and perform corresponding transformations.
Reference: [ZBC + 92] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a language specification. </title> <type> Technical Report ICASE Interim Report 21, </type> <institution> MS 132c, ICASE, NASA, </institution> <address> Hampton VA 23681, </address> <year> 1992. </year>
Reference-contexts: The compiler uses the information provided by these directives to compile global name space programs for distributed memory computers. Examples of parallel languages which support data distribution include Vienna Fortran <ref> [ZBC + 92] </ref>, Fortran D [FHK + 90] and High Performance Fortran (HPF) [For93, KLS + 94]. In this paper, we describe the compilation of out-of-core HPF programs, but the discussion is applicable to any other data parallel language in general.
Reference: [ZC91] <author> H. Zima and B. Chapman. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1991. </year> <month> 20 </month>
Reference-contexts: Each processor performs computation on the data in its ICLA. Some of the issues in out-of-core compilation are similar to compiler optimizations carried out to gain advantage of processor caches or pipelines. This optimization, commonly known as stripmining <ref> [Wol89a, ZC91] </ref>, sections the loop iterations so that data of a fixed size (equal to cache size or pipeline stages) could be operated on in each iteration.
References-found: 29

