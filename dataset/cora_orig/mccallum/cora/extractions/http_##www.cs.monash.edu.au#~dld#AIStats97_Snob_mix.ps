URL: http://www.cs.monash.edu.au/~dld/AIStats97_Snob_mix.ps
Refering-URL: http://www.cs.monash.edu.au/~dld/mixture.modelling.page.html
Root-URL: 
Email: e-mail: fcsw, dldg@cs.monash.edu.au  
Title: MML mixture modelling of multi-state, Poisson, von Mises circular and Gaussian distributions  
Author: Chris S. Wallace and David L. Dowe 
Address: Victoria 3168, Australia  
Affiliation: Department of Computer Science, Monash University, Clayton,  
Abstract: Minimum Message Length (MML) is an invariant Bayesian point estimation technique which is also consistent and efficient. We provide a brief overview of MML inductive inference (Wallace and Boulton (1968), Wallace and Freeman (1987)), and how it has both an information-theoretic and a Bayesian interpretation. We then outline how MML is used for statistical parameter estimation, and how the MML mixture mod-elling program, Snob (Wallace and Boulton (1968), Wal-lace (1986), Wallace and Dowe(1994)) uses the message lengths from various parameter estimates to enable it to combine parameter estimation with selection of the number of components. The message length is (to within a constant) the logarithm of the posterior probability of the theory. So, the MML theory can also be regarded as the theory with the highest posterior probability. Snob currently assumes that variables are uncorrelated, and permits multi-variate data from Gaussian, discrete multi-state, Poisson and von Mises circular distributions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.R. </author> <title> Barron and T.M. Cover. Minimum complexity density estimation. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37 </volume> <pages> 1034-1054, </pages> <year> 1991. </year>
Reference-contexts: For the von Mises distribution, the estimators take a messier form [30, 18, 39, 14]. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency <ref> [43, 36, 1] </ref> of MML. <p> Indeed, MML can be thought of as extending Chaitin's idea of randomness [9] to always trying to fit given data with the shortest possible computer program (plus noise) for generating it. This general convergence result for MML has been explicitly re-stated elsewhere <ref> [36, 1] </ref>. Similar arguments show that MML estimates are not only consistent, but that they are also efficient, i.e., that they converge to any true underlying parameter value as quickly as possible. <p> If we do not minimise the message length (by taking advantage of the coding trick), as with MAP estimation, inconsistencies will arise. Results of Barron and Cover <ref> [1] </ref> show MML to be consistent for any i.i.d. problem, and other results [36][43, p 241] show MML (and Strict MML [38, 43]) to be consistent and efficient for problems of arbitrary generality. 2 hence, Peter Cheeseman (private communication) refers to MML as "quantised Bayes" 3 and part 1d in particular
Reference: [2] <author> R.A. Baxter. </author> <title> Finding Overlapping Distributions with MML. </title> <type> Technical Report 95/244, </type> <institution> Dept. of Computer Science, Monash University, Clayton 3168, Australia, </institution> <month> November </month> <year> 1995. </year>
Reference-contexts: The correction for this can be computationally very slow and has been inspected in the Gaussian case by Baxter <ref> [2] </ref>. 9 Availability of the Snob pro gram The current version of the Snob program (written in Fortran 77) is freely available for not-for-profit, academic research, and not for re-distribution, from ftp://ftp.cs.monash.edu.au/pub/snob/Snob.README (or from C. S. Wallace). Published or otherwise recorded work using Snob should cite the current paper.
Reference: [3] <author> D.M. Boulton. </author> <title> The Information Criterion for Intrinsic Classification. </title> <type> PhD thesis, </type> <institution> Dept. Computer Science, Monash University, Australia, </institution> <year> 1975. </year>
Reference-contexts: Solomonoff [31, p20] and was re-stated and apparently first applied in a series of papers by Wallace and Boulton [37, p185][4][5, pp63-64][6, 8, 7, 38] dealing with model selection and parameter estimation (for Normal and multi-state variables) for problems of mixture modelling (also known as clustering, numerical taxonomy or, e.g. <ref> [3] </ref>, "intrinsic classification"). An important special case of the Minimum Message Length principle is an observation of Chaitin [9] that data can be regarded as "random" if there is no theory, H, describing the data which results in a shorter total message length than the null theory results in. <p> Fisher's CobWeb [17]). Discussions of early alternative algorithms for Gaussian mixture modelling have been given by Boulton <ref> [3] </ref>. 6.1 Comparison with AutoClass II Like Snob, AutoClass II [10] assumes 6 a prior distribution over the number of classes and independent prior densities over the distribution parameters of the sample class densities.
Reference: [4] <author> D.M. Boulton and C.S. Wallace. </author> <title> The information content of a multistate distribution. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 23 </volume> <pages> 269-278, </pages> <year> 1969. </year>
Reference: [5] <author> D.M. Boulton and C.S. Wallace. </author> <title> A program for numerical classification. </title> <journal> Computer Journal, </journal> <volume> 13 </volume> <pages> 63-69, </pages> <year> 1970. </year>
Reference-contexts: 1 Introduction About Minimum Message Length (MML) The Minimum Message Length (MML)[37, p185][43] (and, e.g., <ref> [5, pp63-64] </ref>[38]) principle of inductive inference is based on information theory, and hence lies on the interface on computer science and statistics.
Reference: [6] <author> D.M. Boulton and C.S. Wallace. </author> <title> An information measure for hierarchic classification. </title> <journal> The Computer Journal, </journal> <volume> 16 </volume> <pages> 254-261, </pages> <year> 1973. </year>
Reference: [7] <author> D.M. Boulton and C.S. Wallace. </author> <title> An information measure for single-link classification. </title> <journal> The Computer Journal, </journal> <volume> 18(3) </volume> <pages> 236-238, </pages> <month> August </month> <year> 1975. </year>
Reference: [8] <author> D.M. Boulton and C.S. Wallace. </author> <title> A comparison between information measure classification. </title> <booktitle> In Proceedings of ANZAAS Congress, </booktitle> <address> Perth, </address> <month> August </month> <year> 1973. </year>
Reference: [9] <author> G.J. Chaitin. </author> <title> On the length of programs for computing finite sequences. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 13 </volume> <pages> 547-549, </pages> <year> 1966. </year>
Reference-contexts: An important special case of the Minimum Message Length principle is an observation of Chaitin <ref> [9] </ref> that data can be regarded as "random" if there is no theory, H, describing the data which results in a shorter total message length than the null theory results in. For a comparison with the related Minimum Description Length (MDL) work of Rissanen [28, 29], see, e.g., [32]. <p> This fact and the fact that general MML codes are (by definition) optimal implicitly suggest that, given sufficient data, MML will converge as closely as possible to any underlying model. Indeed, MML can be thought of as extending Chaitin's idea of randomness <ref> [9] </ref> to always trying to fit given data with the shortest possible computer program (plus noise) for generating it. This general convergence result for MML has been explicitly re-stated elsewhere [36, 1].
Reference: [10] <author> P. Cheeseman, M. Self, J. Kelly, W. Taylor, D. Freeman, and J. Stutz. </author> <title> Bayesian classification. </title> <booktitle> In Seventh National Conference on Artificial Intelligence, </booktitle> <pages> pages 607-611, </pages> <address> Saint Paul, Minnesota, </address> <year> 1988. </year>
Reference-contexts: Fisher's CobWeb [17]). Discussions of early alternative algorithms for Gaussian mixture modelling have been given by Boulton [3]. 6.1 Comparison with AutoClass II Like Snob, AutoClass II <ref> [10] </ref> assumes 6 a prior distribution over the number of classes and independent prior densities over the distribution parameters of the sample class densities. However [34], AutoClass II is not based on a message length criterion, but instead makes a more direct inference of the number of classes, J .
Reference: [11] <author> J.H. Conway and N.J.A Sloane. </author> <title> Sphere Pack-ings, Lattices and Groups. </title> <publisher> Springer-Verlag, </publisher> <address> London, </address> <year> 1988. </year>
Reference-contexts: This is quantified elsewhere [43, 39] in terms of lattice constants <ref> [11] </ref> for optimally 1 tesselating Voronoi regions. 3 Applying MML to Mixture Modelling the Snob Program Snob uses MML for both the model selection (number of components and assignment of data things to components) and parameter estimation (estimating means and standard deviations, etc.).
Reference: [12] <author> D.L. Dowe, L. Allison, T.I. Dix, L. Hunter, C.S. Wallace, and T. Edgoose. </author> <title> Circular clustering of protein dihedral angles by Minimum Message Length. </title> <booktitle> In Proceedings of the 1st Pacific Symposium on Bio-computing (PSB-1), </booktitle> <pages> pages 242-255, </pages> <address> Hawaii, U.S.A., </address> <month> January </month> <year> 1996. </year>
Reference-contexts: For small , it tends to a uniform distribution and for large , it tends to a Normal distribution with variance 1=. Circular data arises commonly in many fields <ref> [18, 12] </ref>. <p> On this dataset, a shorter message length was obtained by using a Normal model than a Poisson model, and hence MML advocated the Normal model. The von Mises module has found clusters in data of several thousand sets of protein dihedral angles <ref> [12] </ref>. The Poisson module is currently being used to model run lengths of helices and other protein conformations as being a mixture of Poisson distributions. This work should indirectly lead to a better way of predicting protein conformations.
Reference: [13] <author> D.L. Dowe and K.B. Korb. </author> <title> Conceptual difficulties with the Efficient Market Hypothesis: Towards a naturalized economics. In D.L. Dowe, K.B. Korb, </title> <editor> and J.J. Oliver, editors, </editor> <booktitle> Proceedings of the Information, Statistics and Induction in Science (ISIS) Conference, </booktitle> <pages> pages 212-223, </pages> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific. </title>
Reference-contexts: Further references are given in <ref> [13] </ref>. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [14] <author> D.L. Dowe, J.J. Oliver, R.A. Baxter, and C.S. Wal-lace. </author> <title> Bayesian estimation of the von Mises concentration parameter. </title> <booktitle> In Proc. 15th Maximum Entropy Conference, </booktitle> <address> Santa Fe, New Mexico, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: For the Gaussian, multi-state and Poisson distributions, the MML estimate can be written in a simple analytic form and closely approximates the Maximum Likelihood (ML) estimate. For the von Mises distribution, the estimators take a messier form <ref> [30, 18, 39, 14] </ref>. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency [43, 36, 1] of MML. <p> Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions <ref> [39, 40, 14] </ref>. <p> "quantised Bayes" 3 and part 1d in particular see Section 3.1 4 see Section 3.2 Furthermore, whereas MML is known to be invariant [38, 43] under 1-to-1 transformations, the MAP (posterior mode) estimate is known generally not to be invariant under 1-to-1 transformations - e.g., von Mises circular parameter estimation <ref> [14] </ref> in polar and Cartesian co-ordinates. While the authors do not advocate MAP, another Bayesian method which the authors do advocate is estimation by minimising the Expected Kullback-Leibler distance (min EKL). Like the MML estimator, min EKL is invariant under re-parameterisation. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16].
Reference: [15] <author> D.L. Dowe, J.J. Oliver, and C.S. Wallace. </author> <title> MML estimation of the parameters of the spherical Fisher distribution. </title> <editor> In A. et al. Sharma, editor, </editor> <booktitle> Proc. 7th Conf. Algorithmic Learning Theory (ALT'96), LNAI 1160, </booktitle> <pages> pages 213-227, </pages> <address> Sydney, Australia, </address> <month> Oc-tober </month> <year> 1996. </year>
Reference-contexts: It would not be too difficult [41] to permit the user to modify the colourless priors (see Section 2) used by Snob to better represent the user's prior beliefs (or knowledge, or bias). MML estimators have been obtained for the spherical Fisher distribution <ref> [15] </ref> and work is currently underway [26] to deal with the mixture modelling of these. When there are two or more overlapping components, a slight inefficiency will arise in the message length calculations since parameters will be stated to a slightly higher than necessary degree of precision.
Reference: [16] <author> D.L. Dowe and C.S. Wallace. </author> <title> Resolving the Neyman-Scott problem by Minimum Message Length. </title> <booktitle> In Proc. Sydney International Statistical Congess (SISC-96), </booktitle> <pages> pages 197-198, </pages> <address> Sydney, Aus-tralia, </address> <year> 1996. </year>
Reference-contexts: It is well known that Maximum Likelihood can become inconsistent (or very inefficient) with such problems, e.g. multiple factor analysis [35] and the Neyman-Scott problem <ref> [24, 16] </ref>. 5 Alternative Bayesian methods In doing inductive inference of mixture models from data, there are several levels of inference that we might conceivably wish to make. We might wish simply to in-fer the most likely number of components. <p> In general, with problems such as mixture modelling or multiple factor analysis where the number of parameters to be estimated increases with (and is potentially proportional to) the amount of data, one must beware Maximum Likelihood and MAP methods, which are both liable <ref> [24, 16] </ref> to give inconsistent results. 7 Snob (and MML) Applications Earlier applications of Snob include several to medical, psychological, biological and exploratory geological data, with a survey in [41]. The Poisson module seems to be accurately able to discriminate between pseudo-randomly generated classes from different Poisson distributions. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models <ref> [43, 36, 44, 35, 16] </ref>. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [17] <author> D.H. Fisher. </author> <title> Conceptual clustering, learning from examples, and inference. </title> <booktitle> In Machine Learning: Proceedings of the Fourth International Workshop, </booktitle> <pages> pages 38-49. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1987. </year>
Reference-contexts: Fisher's CobWeb <ref> [17] </ref>). Discussions of early alternative algorithms for Gaussian mixture modelling have been given by Boulton [3]. 6.1 Comparison with AutoClass II Like Snob, AutoClass II [10] assumes 6 a prior distribution over the number of classes and independent prior densities over the distribution parameters of the sample class densities.
Reference: [18] <author> N.I. Fisher. </author> <title> Statistical Analysis of Circular Data. </title> <publisher> Cambridge University Press, </publisher> <year> 1993. </year>
Reference-contexts: For the Gaussian, multi-state and Poisson distributions, the MML estimate can be written in a simple analytic form and closely approximates the Maximum Likelihood (ML) estimate. For the von Mises distribution, the estimators take a messier form <ref> [30, 18, 39, 14] </ref>. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency [43, 36, 1] of MML. <p> h (r) = (1=ff):e r=ff , we get an MML estimate of ^r MML = (c + 1=2)=(t + 1=ff) (7) 2.4 von Mises Circular Variables The von Mises distribution, M 2 (; ), with mean direction , and concentration parameter, , is a circular analogue of the Normal distribution <ref> [18, 21, 39] </ref>, both being maximum entropy distributions. <p> For small , it tends to a uniform distribution and for large , it tends to a Normal distribution with variance 1=. Circular data arises commonly in many fields <ref> [18, 12] </ref>.
Reference: [19] <author> M.P. Georgeff and C.S. Wallace. </author> <title> A general criterion for inductive inference. </title> <editor> In T. O'Shea, editor, </editor> <booktitle> Advances in Artificial Intelligence : Proc. Sixth Eu-ropean Conference on Artificial Intelligence, </booktitle> <pages> pages 473-482, </pages> <address> Amsterdam, 1984. </address> <publisher> North Holland. </publisher>
Reference-contexts: As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models <ref> [19] </ref> and other multi-variate models [43, 36, 44, 35, 16]. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [20] <author> D. W. Kissane, S. Bloch, D. L. Dowe, R. D. Snyder, P. Onghena, D. P. McKenzie, and C. S. Wallace. </author> <title> The Melbourne family grief study, I: Perceptions of family functioning in bereavement. </title> <journal> American Journal of Psychiatry, </journal> <volume> 153 </volume> <pages> 650-658, </pages> <year> 1996. </year>
Reference: [21] <author> K.V. Mardia. </author> <title> Statistics of Directional Data. </title> <publisher> Aca--demic Press, </publisher> <year> 1972. </year>
Reference-contexts: h (r) = (1=ff):e r=ff , we get an MML estimate of ^r MML = (c + 1=2)=(t + 1=ff) (7) 2.4 von Mises Circular Variables The von Mises distribution, M 2 (; ), with mean direction , and concentration parameter, , is a circular analogue of the Normal distribution <ref> [18, 21, 39] </ref>, both being maximum entropy distributions.
Reference: [22] <author> G.J. McLachlan. </author> <title> Discriminant Analysis and Statistical Pattern Recognition. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1992. </year>
Reference: [23] <author> G.J. McLachlan and K.E. Basford. </author> <title> Mixture Models. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1988. </year>
Reference: [24] <author> J. Neyman and E.L. Scott. </author> <title> Consistent estimates based on partially consistent observations. </title> <journal> Econometrika, </journal> <volume> 16 </volume> <pages> 1-32, </pages> <year> 1948. </year>
Reference-contexts: It is well known that Maximum Likelihood can become inconsistent (or very inefficient) with such problems, e.g. multiple factor analysis [35] and the Neyman-Scott problem <ref> [24, 16] </ref>. 5 Alternative Bayesian methods In doing inductive inference of mixture models from data, there are several levels of inference that we might conceivably wish to make. We might wish simply to in-fer the most likely number of components. <p> In general, with problems such as mixture modelling or multiple factor analysis where the number of parameters to be estimated increases with (and is potentially proportional to) the amount of data, one must beware Maximum Likelihood and MAP methods, which are both liable <ref> [24, 16] </ref> to give inconsistent results. 7 Snob (and MML) Applications Earlier applications of Snob include several to medical, psychological, biological and exploratory geological data, with a survey in [41]. The Poisson module seems to be accurately able to discriminate between pseudo-randomly generated classes from different Poisson distributions.
Reference: [25] <author> J. Oliver, Baxter R., and Wallace C. </author> <title> Unsupervised learning using MML. </title> <booktitle> In Proc. 13th International Conf. Machine Learning (ICML 96), </booktitle> <pages> pages 364-372. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference: [26] <author> J.J. Oliver and D.L. Dowe. </author> <title> Minimum Message Length Mixture Modelling of Spherical von Mises-Fisher distributions. </title> <booktitle> In Proc. Sydney International Statistical Congess (SISC-96), </booktitle> <pages> page 198, </pages> <address> Sydney, Australia, </address> <year> 1996. </year>
Reference-contexts: It would not be too difficult [41] to permit the user to modify the colourless priors (see Section 2) used by Snob to better represent the user's prior beliefs (or knowledge, or bias). MML estimators have been obtained for the spherical Fisher distribution [15] and work is currently underway <ref> [26] </ref> to deal with the mixture modelling of these. When there are two or more overlapping components, a slight inefficiency will arise in the message length calculations since parameters will be stated to a slightly higher than necessary degree of precision.
Reference: [27] <author> J.D. Patrick. </author> <title> Snob: A program for discriminating between classes. </title> <type> Technical Report TR 91/151, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Australia, </address> <year> 1991. </year>
Reference-contexts: The Poisson module is currently being used to model run lengths of helices and other protein conformations as being a mixture of Poisson distributions. This work should indirectly lead to a better way of predicting protein conformations. Extensive surveys of Snob applications are given in Patrick <ref> [27] </ref> and Wallace and Dowe [41], with a recent application of Gaussian mixture modelling to data on members of grieving families is given in Kissane et al.[20].
Reference: [28] <author> J. J. Rissanen. </author> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471, </pages> <year> 1978. </year>
Reference-contexts: For a comparison with the related Minimum Description Length (MDL) work of Rissanen <ref> [28, 29] </ref>, see, e.g., [32]. <p> 6.2 Comparison with other methods Oliver et al.[25] re-wrote the Gaussian mixture modelling part of Snob [41, 42] by modifying the Bayesian priors and introducing lattice constants [43, 39] (see Section 2.5) and then empirically showed a successful performance of (this slightly modified) Snob against AIC (Akaike's Information Criterion), BIC <ref> [28] </ref> and other methods. The literature does not yet seem to contain any alternative algorithms for mixture modelling of von Mises circular and Poisson distributions.
Reference: [29] <author> J. J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1989. </year>
Reference-contexts: For a comparison with the related Minimum Description Length (MDL) work of Rissanen <ref> [28, 29] </ref>, see, e.g., [32].
Reference: [30] <author> G. Schou. </author> <title> Estimation of the concentration parameter in von Mises-Fisher distributions. </title> <journal> Biometrika, </journal> <volume> 65 </volume> <pages> 369-377, </pages> <year> 1978. </year>
Reference-contexts: For the Gaussian, multi-state and Poisson distributions, the MML estimate can be written in a simple analytic form and closely approximates the Maximum Likelihood (ML) estimate. For the von Mises distribution, the estimators take a messier form <ref> [30, 18, 39, 14] </ref>. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency [43, 36, 1] of MML.
Reference: [31] <author> R.J. Solomonoff. </author> <title> A formal theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 7 1-22,224-254, </volume> <year> 1964. </year>
Reference-contexts: Hence the name "minimum message length" (principle) for thus choosing a theory, H, to fit observed data, D. The principle seems to have first been stated by Solomonoff <ref> [31, p20] </ref> and was re-stated and apparently first applied in a series of papers by Wallace and Boulton [37, p185][4][5, pp63-64][6, 8, 7, 38] dealing with model selection and parameter estimation (for Normal and multi-state variables) for problems of mixture modelling (also known as clustering, numerical taxonomy or, e.g. [3], "intrinsic
Reference: [32] <author> R.J. Solomonoff. </author> <title> The discovery of algorithmic probability: A guide for the programming of true creativity. </title> <editor> In P. Vitanyi, editor, </editor> <booktitle> Computational Learning Theory: EuroCOLT'95, </booktitle> <pages> pages 1-22. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: For a comparison with the related Minimum Description Length (MDL) work of Rissanen [28, 29], see, e.g., <ref> [32] </ref>.
Reference: [33] <author> C.S. Wallace. </author> <title> An improved program for classification. </title> <booktitle> In Proceedings of the Nineteenth Australian Computer Science Conference (ACSC-9), </booktitle> <volume> volume 8, </volume> <pages> pages 357-366, </pages> <institution> Monash University, Australia, </institution> <year> 1986. </year>
Reference-contexts: pp191-194][41] by This nominally gives rise to a (minimum) message length [37, p187 (4),p194 (28)] of (M 1) log (N=12+1)=2log (M 1)! m (6) for both stating the parameter estimates and then en coding the things in light of these parameter estimates. 2.3 Poisson Variables Earlier versions of Snob originally <ref> [37, 33, 34] </ref> permitted models of classes whose variables were assumed to come from a combination of either (discrete) multi-state or (continuous) Normal distributions. Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions [39, 40, 14]. <p> That stated and understood, it seems conceptually simpler to continue below in the message length paradigm. 3.1 Stating the message a first draft Following earlier work <ref> [37, 33, 34, 41] </ref>, we suppose the data (for mixture modelling) to be given as a matrix of D attribute values for each of N "things", with some attribute values possibly missing. We assume the variables to be independent of one another. <p> The reason for this is that <ref> [33, x3] </ref>[34, p77] if p (j; x); j = 1; :::; J , is the probability of component j generating datum x, then the total assignment of x to its best component results in a message length of log (max j p (j; x)) to encode x whereas, letting P (x) <p> As shown in <ref> [33, x3] </ref>[34, p77][41], this shorter length is achievable by a message which asserts definite membership of each thing by use of a special coding trick. 4 Consistency, invariance and ef ficiency of MML estimates If the outcomes of any random process are encoded using a code that is optimal for that <p> In applying Snob, a difference of more than 5 to 6 bits [43, p251] or of more than 10 bits <ref> [33] </ref> might be deemed to be statistically significant under certain mod-elling conditions.
Reference: [34] <author> C.S. Wallace. </author> <title> Classification by MinimumMessage-Length inference. </title> <editor> In G. Goos and J. Hartmanis, editors, </editor> <booktitle> Advances in Computing and Information - ICCI '90, </booktitle> <pages> pages 72-81. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: pp191-194][41] by This nominally gives rise to a (minimum) message length [37, p187 (4),p194 (28)] of (M 1) log (N=12+1)=2log (M 1)! m (6) for both stating the parameter estimates and then en coding the things in light of these parameter estimates. 2.3 Poisson Variables Earlier versions of Snob originally <ref> [37, 33, 34] </ref> permitted models of classes whose variables were assumed to come from a combination of either (discrete) multi-state or (continuous) Normal distributions. Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions [39, 40, 14]. <p> That stated and understood, it seems conceptually simpler to continue below in the message length paradigm. 3.1 Stating the message a first draft Following earlier work <ref> [37, 33, 34, 41] </ref>, we suppose the data (for mixture modelling) to be given as a matrix of D attribute values for each of N "things", with some attribute values possibly missing. We assume the variables to be independent of one another. <p> Discussions of early alternative algorithms for Gaussian mixture modelling have been given by Boulton [3]. 6.1 Comparison with AutoClass II Like Snob, AutoClass II [10] assumes 6 a prior distribution over the number of classes and independent prior densities over the distribution parameters of the sample class densities. However <ref> [34] </ref>, AutoClass II is not based on a message length criterion, but instead makes a more direct inference of the number of classes, J . Let V be the vector of abundance and distribution parameters needed to specify a model with J components. <p> Baxter and J. Oliver 6 This sub-section is very much a re-writing of <ref> [34, pp78-80] </ref>. The calculation of the posterior, P (J jX), requires the calculation of an integral for each possible number of classes, J , in order to obtain the joint probability, P (J; X).
Reference: [35] <author> C.S. Wallace. </author> <title> Multiple Factor Analysis by MML Estimation. </title> <type> Technical Report 95/218, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victo-ria 3168, Australia, </address> <year> 1995. </year> <note> submitted to J. Multiv. Analysis. </note>
Reference-contexts: It is well known that Maximum Likelihood can become inconsistent (or very inefficient) with such problems, e.g. multiple factor analysis <ref> [35] </ref> and the Neyman-Scott problem [24, 16]. 5 Alternative Bayesian methods In doing inductive inference of mixture models from data, there are several levels of inference that we might conceivably wish to make. We might wish simply to in-fer the most likely number of components. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models <ref> [43, 36, 44, 35, 16] </ref>. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated. <p> Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated. This could be modified to permit single linear (Gaussian) factor analysis [44] or multiple linear (Gaussian) factor analysis <ref> [35] </ref>, or to model correlations via an inverse Wishart or some other such prior. It would not be too difficult [41] to permit the user to modify the colourless priors (see Section 2) used by Snob to better represent the user's prior beliefs (or knowledge, or bias).
Reference: [36] <author> C.S. Wallace. </author> <title> False Oracles and SMML Estimators. In D.L. Dowe, K.B. Korb, </title> <editor> and J.J. Oliver, editors, </editor> <booktitle> Proceedings of the Information, Statistics and Induction in Science (ISIS) Conference, </booktitle> <pages> pages 304-316, </pages> <address> Melbourne, Australia, </address> <month> August </month> <year> 1996. </year> <title> World Scientific. </title> <type> Was Tech Rept 89/128, </type> <institution> Dept. Comp. Sci., Monash Univ., Australia, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: recorded. (In practice [41], as below equation (3), we assume that, for a given continuous or circular attribute, all measurements are made to some accuracy, *.) Just as all recorded data is finitely recorded and can be finitely represented, by acknowledging an uncertainty region in the MML estimate of approximately <ref> [43, 36, 39] </ref> p 12=F (), the MML estimate is stated to a (non-zero) finite precision. <p> For the von Mises distribution, the estimators take a messier form [30, 18, 39, 14]. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency <ref> [43, 36, 1] </ref> of MML. <p> Indeed, MML can be thought of as extending Chaitin's idea of randomness [9] to always trying to fit given data with the shortest possible computer program (plus noise) for generating it. This general convergence result for MML has been explicitly re-stated elsewhere <ref> [36, 1] </ref>. Similar arguments show that MML estimates are not only consistent, but that they are also efficient, i.e., that they converge to any true underlying parameter value as quickly as possible. <p> While the authors do not advocate MAP, another Bayesian method which the authors do advocate is estimation by minimising the Expected Kullback-Leibler distance (min EKL). Like the MML estimator, min EKL is invariant under re-parameterisation. Work to appear 5 follows Wallace <ref> [36] </ref> and shows strong similarities between Strict MML [38, 43] and min EKL (as is easily seen in the case of M -state Bernoulli sampling). 6 Alternative mixture modelling programs The first Snob program (since out-dated)[37] was possibly the first program for Gaussian mixture modelling, although many statistical and machine learning <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models <ref> [43, 36, 44, 35, 16] </ref>. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [37] <author> C.S. Wallace and D.M. Boulton. </author> <title> An information measure for classification. </title> <journal> Computer Journal, </journal> <volume> 11 </volume> <pages> 185-194, </pages> <year> 1968. </year>
Reference-contexts: Hence the name "minimum message length" (principle) for thus choosing a theory, H, to fit observed data, D. The principle seems to have first been stated by Solomonoff [31, p20] and was re-stated and apparently first applied in a series of papers by Wallace and Boulton <ref> [37, p185] </ref>[4][5, pp63-64][6, 8, 7, 38] dealing with model selection and parameter estimation (for Normal and multi-state variables) for problems of mixture modelling (also known as clustering, numerical taxonomy or, e.g. [3], "intrinsic classification"). <p> Letting s 2 = P that 2 ML = s 2 =N and <ref> [37, p190] </ref> that 2 corrects this minor but well-known bias in the Maximum Likelihood estimate. 2.2 Discrete, Multi-State Variables Since multi-state attributes are discrete, the above issues of measurement accuracy do not arise. <p> Letting n m be the number of things in state m and N = n 1 +:::+n M , minimising the message length formula gives that the MML estimate of p m is given <ref> [37, p187 (4), pp191-194] </ref>[41] by This nominally gives rise to a (minimum) message length [37, p187 (4),p194 (28)] of (M 1) log (N=12+1)=2log (M 1)! m (6) for both stating the parameter estimates and then en coding the things in light of these parameter estimates. 2.3 Poisson Variables Earlier versions of <p> Letting n m be the number of things in state m and N = n 1 +:::+n M , minimising the message length formula gives that the MML estimate of p m is given [37, p187 (4), pp191-194][41] by This nominally gives rise to a (minimum) message length <ref> [37, p187 (4),p194 (28)] </ref> of (M 1) log (N=12+1)=2log (M 1)! m (6) for both stating the parameter estimates and then en coding the things in light of these parameter estimates. 2.3 Poisson Variables Earlier versions of Snob originally [37, 33, 34] permitted models of classes whose variables were assumed to <p> pp191-194][41] by This nominally gives rise to a (minimum) message length [37, p187 (4),p194 (28)] of (M 1) log (N=12+1)=2log (M 1)! m (6) for both stating the parameter estimates and then en coding the things in light of these parameter estimates. 2.3 Poisson Variables Earlier versions of Snob originally <ref> [37, 33, 34] </ref> permitted models of classes whose variables were assumed to come from a combination of either (discrete) multi-state or (continuous) Normal distributions. Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions [39, 40, 14]. <p> That stated and understood, it seems conceptually simpler to continue below in the message length paradigm. 3.1 Stating the message a first draft Following earlier work <ref> [37, 33, 34, 41] </ref>, we suppose the data (for mixture modelling) to be given as a matrix of D attribute values for each of N "things", with some attribute values possibly missing. We assume the variables to be independent of one another. <p> The details of the encoding and of the calculation of the length of part 1 of the message may be found in Section 2 and elsewhere <ref> [37, 39] </ref>. It is perhaps worth noting here that since our objective is to minimise the message length (and maximise the posterior probability), we never need construct a message we only need be able to calculate its length. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16].
Reference: [38] <author> C.S. Wallace and D.M. Boulton. </author> <title> An invariant Bayes method for point estimation. </title> <journal> Classification Society Bulletin, </journal> <volume> 3(3) </volume> <pages> 11-34, </pages> <year> 1975. </year>
Reference-contexts: For the von Mises distribution, the estimators take a messier form [30, 18, 39, 14]. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance <ref> [43, 38] </ref> of MML and the consistency and efficiency [43, 36, 1] of MML. <p> Recall throughout the equivalence <ref> [38] </ref> between the probability paradigm and the message length paradigm, with an event of probability p corresponding to a message of length l = log 2 p bits, and a message of length l bits corresponding to a probability of p = 2 l . <p> Although we do not wish to advocate the use of a Jeffrey's prior, we do note that h= p F is invariant under parameter transformation. Since the likelihood function is also invariant under parameter transformation, we see from equation (3) that MML is also invariant under parameter transformation <ref> [43, 38] </ref>. The problem of model selection and parameter estimation in mixture modelling can, at its worst, be thought of as a problem for which the number of parameters to be estimated grows with the data. <p> If we do not minimise the message length (by taking advantage of the coding trick), as with MAP estimation, inconsistencies will arise. Results of Barron and Cover [1] show MML to be consistent for any i.i.d. problem, and other results [36][43, p 241] show MML (and Strict MML <ref> [38, 43] </ref>) to be consistent and efficient for problems of arbitrary generality. 2 hence, Peter Cheeseman (private communication) refers to MML as "quantised Bayes" 3 and part 1d in particular see Section 3.1 4 see Section 3.2 Furthermore, whereas MML is known to be invariant [38, 43] under 1-to-1 transformations, the <p> show MML (and Strict MML <ref> [38, 43] </ref>) to be consistent and efficient for problems of arbitrary generality. 2 hence, Peter Cheeseman (private communication) refers to MML as "quantised Bayes" 3 and part 1d in particular see Section 3.1 4 see Section 3.2 Furthermore, whereas MML is known to be invariant [38, 43] under 1-to-1 transformations, the MAP (posterior mode) estimate is known generally not to be invariant under 1-to-1 transformations - e.g., von Mises circular parameter estimation [14] in polar and Cartesian co-ordinates. <p> While the authors do not advocate MAP, another Bayesian method which the authors do advocate is estimation by minimising the Expected Kullback-Leibler distance (min EKL). Like the MML estimator, min EKL is invariant under re-parameterisation. Work to appear 5 follows Wallace [36] and shows strong similarities between Strict MML <ref> [38, 43] </ref> and min EKL (as is easily seen in the case of M -state Bernoulli sampling). 6 Alternative mixture modelling programs The first Snob program (since out-dated)[37] was possibly the first program for Gaussian mixture modelling, although many statistical and machine learning approaches to this problem have been developed since <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16].
Reference: [39] <author> C.S. Wallace and D.L. Dowe. </author> <title> MML estimation of the von Mises concentration parameter. </title> <type> Technical report TR 93/193, </type> <institution> Dept. of Computer Science, Monash University, Clayton, </institution> <address> Victoria 3168, Aus-tralia, </address> <year> 1993. </year> <note> prov. accepted, Aust. J. Stat. </note>
Reference-contexts: recorded. (In practice [41], as below equation (3), we assume that, for a given continuous or circular attribute, all measurements are made to some accuracy, *.) Just as all recorded data is finitely recorded and can be finitely represented, by acknowledging an uncertainty region in the MML estimate of approximately <ref> [43, 36, 39] </ref> p 12=F (), the MML estimate is stated to a (non-zero) finite precision. <p> For the Gaussian, multi-state and Poisson distributions, the MML estimate can be written in a simple analytic form and closely approximates the Maximum Likelihood (ML) estimate. For the von Mises distribution, the estimators take a messier form <ref> [30, 18, 39, 14] </ref>. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency [43, 36, 1] of MML. <p> Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions <ref> [39, 40, 14] </ref>. <p> h (r) = (1=ff):e r=ff , we get an MML estimate of ^r MML = (c + 1=2)=(t + 1=ff) (7) 2.4 von Mises Circular Variables The von Mises distribution, M 2 (; ), with mean direction , and concentration parameter, , is a circular analogue of the Normal distribution <ref> [18, 21, 39] </ref>, both being maximum entropy distributions. <p> Circular data arises commonly in many fields [18, 12]. MML estimation of the von Mises concentration parameter, , is obtained by minimising the earlier formula for the message length, using <ref> [39] </ref> a uniform prior on in [0, 2) and the prior h 3 () = =(1 + 2 ) 3=2 contrast between MML and ML estimation is sharper for the von Mises distribution than it is for the Normal, multi-state and Poisson distributions, with Monte Carlo simulations [39, pp12-18] showing a <p> message length, using [39] a uniform prior on in [0, 2) and the prior h 3 () = =(1 + 2 ) 3=2 contrast between MML and ML estimation is sharper for the von Mises distribution than it is for the Normal, multi-state and Poisson distributions, with Monte Carlo simulations <ref> [39, pp12-18] </ref> showing a very impressive performance by the MML estimator against ML and other classical rivals (e.g. marginalised Maximum Likelihood)[30, 18]. <p> This is quantified elsewhere <ref> [43, 39] </ref> in terms of lattice constants [11] for optimally 1 tesselating Voronoi regions. 3 Applying MML to Mixture Modelling the Snob Program Snob uses MML for both the model selection (number of components and assignment of data things to components) and parameter estimation (estimating means and standard deviations, etc.). <p> For each component, the distribution parameters of the component (as discussed in Section 2). Each parameter is considered to be specified to a precision of the order of its expected estimation error or uncertainty (see Section 2 or, e.g., <ref> [39, pp3-4] </ref>). For a larger component, the parameters will be encoded to greater precision and hence by longer fragments than for a less abundant component. 1d. <p> The details of the encoding and of the calculation of the length of part 1 of the message may be found in Section 2 and elsewhere <ref> [37, 39] </ref>. It is perhaps worth noting here that since our objective is to minimise the message length (and maximise the posterior probability), we never need construct a message we only need be able to calculate its length. <p> Thus, although AutoClass II is differently motivated from Snob, in practice it gives almost identical results. 6.2 Comparison with other methods Oliver et al.[25] re-wrote the Gaussian mixture modelling part of Snob [41, 42] by modifying the Bayesian priors and introducing lattice constants <ref> [43, 39] </ref> (see Section 2.5) and then empirically showed a successful performance of (this slightly modified) Snob against AIC (Akaike's Information Criterion), BIC [28] and other methods. The literature does not yet seem to contain any alternative algorithms for mixture modelling of von Mises circular and Poisson distributions. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing <ref> [43, 39] </ref>, Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [40] <author> C.S. Wallace and D.L. Dowe. </author> <title> Estimation of the von Mises concentration parameter using Minimum Message Length. </title> <booktitle> In Proc. 12th Australian Statistical Soc. Conf., </booktitle> <institution> Monash University, Australia, </institution> <year> 1994. </year>
Reference-contexts: Snob has since been augmented by permitting Poisson distributions and von Mises circular distributions <ref> [39, 40, 14] </ref>. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16].
Reference: [41] <author> C.S. Wallace and D.L. Dowe. </author> <title> Intrinsic classification by MML the Snob program. </title> <editor> In C. Zhang, J. Debenham, and D Lukose, editors, </editor> <booktitle> Proc. 7th Australian Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 37-44. </pages> <publisher> World Scientific, </publisher> <address> Singapore, </address> <year> 1994. </year> <note> See ftp://ftp.cs.monash.edu.au/pub/snob/Snob.README. </note>
Reference-contexts: For continuous data, we also acknowledge that it must only have been stated to finite precision by virtue of the fact that it was able to be (finitely) recorded. (In practice <ref> [41] </ref>, as below equation (3), we assume that, for a given continuous or circular attribute, all measurements are made to some accuracy, *.) Just as all recorded data is finitely recorded and can be finitely represented, by acknowledging an uncertainty region in the MML estimate of approximately [43, 36, 39] p <p> That stated and understood, it seems conceptually simpler to continue below in the message length paradigm. 3.1 Stating the message a first draft Following earlier work <ref> [37, 33, 34, 41] </ref>, we suppose the data (for mixture modelling) to be given as a matrix of D attribute values for each of N "things", with some attribute values possibly missing. We assume the variables to be independent of one another. <p> Thus, although AutoClass II is differently motivated from Snob, in practice it gives almost identical results. 6.2 Comparison with other methods Oliver et al.[25] re-wrote the Gaussian mixture modelling part of Snob <ref> [41, 42] </ref> by modifying the Bayesian priors and introducing lattice constants [43, 39] (see Section 2.5) and then empirically showed a successful performance of (this slightly modified) Snob against AIC (Akaike's Information Criterion), BIC [28] and other methods. <p> (and is potentially proportional to) the amount of data, one must beware Maximum Likelihood and MAP methods, which are both liable [24, 16] to give inconsistent results. 7 Snob (and MML) Applications Earlier applications of Snob include several to medical, psychological, biological and exploratory geological data, with a survey in <ref> [41] </ref>. The Poisson module seems to be accurately able to discriminate between pseudo-randomly generated classes from different Poisson distributions. It has also been used to analyse word-counts from a data-set of 17th Century texts. <p> This work should indirectly lead to a better way of predicting protein conformations. Extensive surveys of Snob applications are given in Patrick [27] and Wallace and Dowe <ref> [41] </ref>, with a recent application of Gaussian mixture modelling to data on members of grieving families is given in Kissane et al.[20]. <p> This could be modified to permit single linear (Gaussian) factor analysis [44] or multiple linear (Gaussian) factor analysis [35], or to model correlations via an inverse Wishart or some other such prior. It would not be too difficult <ref> [41] </ref> to permit the user to modify the colourless priors (see Section 2) used by Snob to better represent the user's prior beliefs (or knowledge, or bias). <p> S. Wallace). Published or otherwise recorded work using Snob should cite the current paper. User guidelines are given in <ref> [41] </ref> and in the documentation file, snob.doc . 10 Acknowledgments Section This work was supported by Australian Research Council (ARC) Grant A49330656 and ARC 1992 small grant 9103169 . The authors wish to thank Graham Farr for valuable feedback.
Reference: [42] <author> C.S. Wallace and D.L. Dowe. </author> <title> MML mixture mod-elling of Multi-state, Poisson, von Mises circular and Gaussian distributions. </title> <booktitle> In Proc. Sydney International Statistical Congess (SISC-96), </booktitle> <pages> page 197, </pages> <address> Sydney, Australia, </address> <year> 1996. </year>
Reference-contexts: Thus, although AutoClass II is differently motivated from Snob, in practice it gives almost identical results. 6.2 Comparison with other methods Oliver et al.[25] re-wrote the Gaussian mixture modelling part of Snob <ref> [41, 42] </ref> by modifying the Bayesian priors and introducing lattice constants [43, 39] (see Section 2.5) and then empirically showed a successful performance of (this slightly modified) Snob against AIC (Akaike's Information Criterion), BIC [28] and other methods.
Reference: [43] <author> C.S. Wallace and P.R. Freeman. </author> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 49 </volume> <pages> 240-252, </pages> <year> 1987. </year>
Reference-contexts: Then the MML estimate of ~ <ref> [43, p245] </ref> is that value of ~ minimising the message length, log (h ( ~ )p (xj ~ )= F ( ~ )) + 1=2 log (e=12) (3) (If * is the measurement accuracy of the data and N is the number of data things, then we add the constant term <p> This is elaborated upon elsewhere <ref> [43, p245] </ref>[39, pp1-3].) The two-part message describing the data thus comprises first, a theory, which is the MML parameter estimate (s), and, second, the data given this theory. It is reasonably clear to see that a finite coding can be given when the data is discrete or multi-state. <p> recorded. (In practice [41], as below equation (3), we assume that, for a given continuous or circular attribute, all measurements are made to some accuracy, *.) Just as all recorded data is finitely recorded and can be finitely represented, by acknowledging an uncertainty region in the MML estimate of approximately <ref> [43, 36, 39] </ref> p 12=F (), the MML estimate is stated to a (non-zero) finite precision. <p> For the von Mises distribution, the estimators take a messier form [30, 18, 39, 14]. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance <ref> [43, 38] </ref> of MML and the consistency and efficiency [43, 36, 1] of MML. <p> For the von Mises distribution, the estimators take a messier form [30, 18, 39, 14]. The following two sections are on extending MML parameter estimation to MML mixture modelling, and on the invariance [43, 38] of MML and the consistency and efficiency <ref> [43, 36, 1] </ref> of MML. <p> This is quantified elsewhere <ref> [43, 39] </ref> in terms of lattice constants [11] for optimally 1 tesselating Voronoi regions. 3 Applying MML to Mixture Modelling the Snob Program Snob uses MML for both the model selection (number of components and assignment of data things to components) and parameter estimation (estimating means and standard deviations, etc.). <p> which asserts definite membership of each thing by use of a special coding trick. 4 Consistency, invariance and ef ficiency of MML estimates If the outcomes of any random process are encoded using a code that is optimal for that process, the resulting binary string forms a completely random process <ref> [43, p241] </ref>. This fact and the fact that general MML codes are (by definition) optimal implicitly suggest that, given sufficient data, MML will converge as closely as possible to any underlying model. <p> Although we do not wish to advocate the use of a Jeffrey's prior, we do note that h= p F is invariant under parameter transformation. Since the likelihood function is also invariant under parameter transformation, we see from equation (3) that MML is also invariant under parameter transformation <ref> [43, 38] </ref>. The problem of model selection and parameter estimation in mixture modelling can, at its worst, be thought of as a problem for which the number of parameters to be estimated grows with the data. <p> By shortening the length of the message to a minimum, MML arrives at the (quantised) theory of the highest probability (see Section 1) whose resulting binary string forms <ref> [43, p 241] </ref>[41, p 41] a completely random process. <p> If we do not minimise the message length (by taking advantage of the coding trick), as with MAP estimation, inconsistencies will arise. Results of Barron and Cover [1] show MML to be consistent for any i.i.d. problem, and other results [36][43, p 241] show MML (and Strict MML <ref> [38, 43] </ref>) to be consistent and efficient for problems of arbitrary generality. 2 hence, Peter Cheeseman (private communication) refers to MML as "quantised Bayes" 3 and part 1d in particular see Section 3.1 4 see Section 3.2 Furthermore, whereas MML is known to be invariant [38, 43] under 1-to-1 transformations, the <p> show MML (and Strict MML <ref> [38, 43] </ref>) to be consistent and efficient for problems of arbitrary generality. 2 hence, Peter Cheeseman (private communication) refers to MML as "quantised Bayes" 3 and part 1d in particular see Section 3.1 4 see Section 3.2 Furthermore, whereas MML is known to be invariant [38, 43] under 1-to-1 transformations, the MAP (posterior mode) estimate is known generally not to be invariant under 1-to-1 transformations - e.g., von Mises circular parameter estimation [14] in polar and Cartesian co-ordinates. <p> While the authors do not advocate MAP, another Bayesian method which the authors do advocate is estimation by minimising the Expected Kullback-Leibler distance (min EKL). Like the MML estimator, min EKL is invariant under re-parameterisation. Work to appear 5 follows Wallace [36] and shows strong similarities between Strict MML <ref> [38, 43] </ref> and min EKL (as is easily seen in the case of M -state Bernoulli sampling). 6 Alternative mixture modelling programs The first Snob program (since out-dated)[37] was possibly the first program for Gaussian mixture modelling, although many statistical and machine learning approaches to this problem have been developed since <p> Thus, although AutoClass II is differently motivated from Snob, in practice it gives almost identical results. 6.2 Comparison with other methods Oliver et al.[25] re-wrote the Gaussian mixture modelling part of Snob [41, 42] by modifying the Bayesian priors and introducing lattice constants <ref> [43, 39] </ref> (see Section 2.5) and then empirically showed a successful performance of (this slightly modified) Snob against AIC (Akaike's Information Criterion), BIC [28] and other methods. The literature does not yet seem to contain any alternative algorithms for mixture modelling of von Mises circular and Poisson distributions. <p> Extensive surveys of Snob applications are given in Patrick [27] and Wallace and Dowe [41], with a recent application of Gaussian mixture modelling to data on members of grieving families is given in Kissane et al.[20]. In applying Snob, a difference of more than 5 to 6 bits <ref> [43, p251] </ref> or of more than 10 bits [33] might be deemed to be statistically significant under certain mod-elling conditions. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation <ref> [37, 38, 43, 36, 39, 40, 14, 16] </ref>, hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing <ref> [43, 39] </ref>, Hidden Markov Models [19] and other multi-variate models [43, 36, 44, 35, 16]. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated. <p> As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models <ref> [43, 36, 44, 35, 16] </ref>. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated.
Reference: [44] <author> C.S. Wallace and P.R. Freeman. </author> <title> Single factor analysis by MML estimation. </title> <journal> Journal of the Royal Statistical Society (Series B), </journal> <volume> 54 </volume> <pages> 195-209, </pages> <year> 1992. </year>
Reference-contexts: As well as having been applied to mixture models (discussed here), MML has also been successfully applied to a variety of problems of parameter estimation [37, 38, 43, 36, 39, 40, 14, 16], hypothesis testing [43, 39], Hidden Markov Models [19] and other multi-variate models <ref> [43, 36, 44, 35, 16] </ref>. Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated. <p> Further references are given in [13]. 8 Notes on further work and Snob program extensions The Snob program currently implicitly assumes that variables are independent and uncorrelated. This could be modified to permit single linear (Gaussian) factor analysis <ref> [44] </ref> or multiple linear (Gaussian) factor analysis [35], or to model correlations via an inverse Wishart or some other such prior.
References-found: 44

