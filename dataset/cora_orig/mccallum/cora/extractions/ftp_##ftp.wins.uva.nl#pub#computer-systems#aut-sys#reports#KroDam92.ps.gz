URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/KroDam92.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: krose@fwi.uva.nl  
Title: Adaptive state space quantisation for reinforcement learning of collision-free navigation  
Author: Ben J.A. Krose and Joris W.M. van Dam 
Keyword: reinforcement learning, neural networks, state-space quantisation, mobile robot navigation.  
Address: Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands  
Affiliation: Faculty of Mathematics and Computer Science, University of Amsterdam  
Abstract: The paper describes a self-learning control system for a mobile robot. Based on sensor information the control system has to provide a steering signal in such a way that collisions are avoided. Since in our case no `examples' are available, the system learns on the basis of an external reinforcement signal which is negative in case of a collision and zero otherwise. Rules from Temporal Difference learning are used to find the correct mapping between the (discrete) sensor input space and the steering signal. We describe the algorithm for learning the correct mapping from the input (state) vector to the output (steering) signal, and the algorithm which is used for a discrete coding of the input state space. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Brooks, R.A. </author> <title> "A robust layered control system for a mobile robot", </title> <journal> IEEE Journal on Robotics and Automation, </journal> <volume> 2 (1), </volume> <year> (1986), </year> <pages> 4-10. </pages>
Reference: [2] <author> Arkin R.C. </author> <title> "Motor Schema Based Mobile Robot Navigation", </title> <journal> Int. Journal of Robotics Research, </journal> <year> (1989), </year> <pages> 92-112. </pages>
Reference: [3] <author> Koren, Y and J. Borenstein, </author> <title> "Analysis of contol methods for mobile robot obstacle avoidance", </title> <booktitle> IEEE Int. Workshop on Intelligent Motion Control, Isanbul (1990), </booktitle> <pages> pp 457-463 </pages>
Reference: [4] <author> Touretzky, D.S. and D.A. Pomerleau, </author> <title> "What's hidden in the hidden layers?",Byte, </title> <month> August </month> <year> 1989, </year> <pages> pp 227-233. </pages>
Reference-contexts: Given a desired curvature and velocity, CAR-SIM calculates the position of the vehicle as a function of time. This position serves again as input for ASSIM. The control frequency is 2 Hz. The vehicle was trained in an environment as shown in Figure <ref> [4] </ref>. A trial consists of a series of steps until a collision occurs. When a collision occurs, the system is backed-up 30 (M = 30) steps and a new trial is started. At the start of the first trial, the vehicle is positioned at location 1 (see Figure [4]). <p> in Figure <ref> [4] </ref>. A trial consists of a series of steps until a collision occurs. When a collision occurs, the system is backed-up 30 (M = 30) steps and a new trial is started. At the start of the first trial, the vehicle is positioned at location 1 (see Figure [4]). When the vehicle has learned to move more than 2000 steps without collision it is put in starting position 2, and training is continued. In some cases the system may become trapped: the number of steps until collision does not improve any more.
Reference: [5] <author> Smagt, P.P van der, and B.J.A. Krose. </author> <title> "A real-time learning neural robot controller", </title> <booktitle> Proceedings of the 1991 Int. Conf. on Artificial Neural Networks, Fin-land (1991), </booktitle> <pages> 351-356. </pages>
Reference-contexts: In practice the safe state is characterised by the neuron x 0 at the origin of the state space. During the collision mode, the learning parameter ff is high. When, after a series of unsuccesfull trials a transition to a safe state occurs, ^r is high (see <ref> [5] </ref> and [6]), resulting in high w ij values for actions which succesfully led to the collision avoidance. This causes these actions to generated with a high probability. <p> The performance as a function of the number of trials. In this figure also the best performance of an a-priori state space quantisation with 2 8 = 256 states has been plot ted. is shown in Figure <ref> [5] </ref>. The results are averaged over 21 experimental epochs. The results show that the performance of the system improves with learning. Because of the fact that in some epochs the system became trapped, the average performance is not always 2000. VI.
Reference: [6] <author> Rumelhart, D.E. and J.L.McClelland. </author> <title> Parallel Distributed Processing. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: In practice the safe state is characterised by the neuron x 0 at the origin of the state space. During the collision mode, the learning parameter ff is high. When, after a series of unsuccesfull trials a transition to a safe state occurs, ^r is high (see [5] and <ref> [6] </ref>), resulting in high w ij values for actions which succesfully led to the collision avoidance. This causes these actions to generated with a high probability.
Reference: [7] <author> Moody, J and C. Darken. </author> <title> "Fast learning in networks of locally-tuned processing units", </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <year> 1989, </year> <pages> 281-294. </pages>
Reference-contexts: All new ASE and ACE weights are updated using Eqs. [9] resp. <ref> [7] </ref> and a steering signal is generated. This step 4 is repeated. Upon collision only the latest series of copies is saved, a back-up is performed, a steering signal is generated and the procedure restarts at step 4. 5.
Reference: [8] <author> Michie, D and R.A. Chambers. </author> <title> "Boxes: An experiment in adaptive control", </title> <editor> In: E. Dale and D. Michie (ed.), </editor> <booktitle> Machine Intelligence 2, </booktitle> <publisher> Oliver and Boyd, </publisher> <year> 1968, </year> <pages> 137-152, </pages>
Reference: [9] <author> Barto, A.G., R.S. Sutton en C.W. Anderson, </author> <title> "Neu-ronlike adaptive elements that can solve difficult learning control problems", </title> <journal> IEEE Trans. on Systems, Man and Cybernetics, </journal> <volume> 13 (1983), </volume> <pages> 834-846 </pages>
Reference-contexts: A new neuron x N+2 is positioned at current state vector ( ~ t N+2 = ~s), and its ASE weights ~w N+2 and ACE weights v N+2 are copied from the weights ~w k and v k . All new ASE and ACE weights are updated using Eqs. <ref> [9] </ref> resp. [7] and a steering signal is generated. This step 4 is repeated. Upon collision only the latest series of copies is saved, a back-up is performed, a steering signal is generated and the procedure restarts at step 4. 5.
Reference: [10] <author> Rosen, B.E., J.M.Goodwin and J.J.Vidal, </author> <title> "Adaptive range coding", </title> <editor> In: D. Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kauffman (1990). </publisher>
Reference: [11] <author> Sutton, </author> <title> R.S. Temporal credit assignment in reinforcement learning Ph.D. </title> <institution> dissertation at University of Massachusets (1984). </institution>
Reference: [12] <author> Sutton, </author> <title> R.S. "Integrated architectures for learning, planning and reacting based on approximating dynamic programming", </title> <booktitle> Proc. of the Seventh Int. Conf. on Machine Learning, </booktitle> <year> 1990 </year>
Reference: [13] <author> Kohonen, T. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference: [14] <author> Whitehead, S and D Ballard. </author> <title> "Learning to Perceive and Act" Techn Rep. </title> <type> 331, </type> <institution> Dept of Computer Science, University of Rochester, </institution> <year> 1990. </year>
Reference: [15] <author> Krose, B.J.A. and J.W.M. van Dam, </author> <title> "Learning to avoid collision: a reinforcement learning paradigm for mobile robot navigation" IFAC/IFIP/IMACS International Symposium on Artificial Intelligence in Real-Time Control, </title> <address> Delft, </address> <year> 1992. </year>
Reference-contexts: A quantisa-tion into 256 states was used in the fixed range coding experiments, whereas the results obtained with the self-organising quantisation are based on approximately 80 discrete states. Results are comparable with earlier results obtained with a Kohonen network as decoder <ref> [15] </ref>. However, in the Kohonen network we used 128 neurons, significantly more than the 80 neurons which were created by our algorithm. Apparently the self-organising input state quantisation is able to place neurons where they are needed, i.e. near states where collisions are likely.
Reference: [16] <author> Krose, B.J.A. and E. Dondorp, </author> <title> "A Sensor Simulation System for Mobile Robots", </title> <editor> in: T. Kanade, F.C.A. Groen and L.O. Hertzberger (ed.), </editor> <booktitle> Intelligent Autonomous Systems 2, </booktitle> <month> December </month> <year> 1989. </year> <title> [17] van Albada, </title> <editor> G.D., J.M. Lagerberg, B.J.A. Krose. </editor> <booktitle> "Software architecture and simulation tools for autonomous mobile robots", Proc. of Euriscon '91 Corfu, </booktitle> <address> Greece, </address> <publisher> Kluwer Press (in press). </publisher>
Reference-contexts: V. Experiments and results Experiments are carried out in our simulation environment, consisting of a sensor simulation program (ASSIM <ref> [16] </ref>) and the simulator of the dynamic and kinematic behaviour of our mobile robot (CARSIM [17]). A 2 1 2 - dimensional map of the environment is specified, in which the robot can be positioned at any location.
Reference: [18] <author> Verschure, P.F.M.J., B.J.A. Krose and R. </author> <title> Pfeifer "Distributed Adaptive Control: the self organization of structured behavior", </title> <booktitle> Robotics and Autonomous Systems 9 (2) (1992). </booktitle>
Reference-contexts: Instead of having a single neuron active for each state, the sensor readings can be mapped directly into motor commands, which also has shown to result in a collision 6 of 6 free navigation <ref> [18] </ref>. However, this can only be done for linear control rules. The controller described in this paper results in a reactive behaviour of the robot, aimed at avoiding collisions with obstacles. If also a goal directed behaviour has to be learned, basically the same approach can be used.
References-found: 17

