URL: http://www.cs.cornell.edu/home/yanhong/Psr.ps
Refering-URL: 
Root-URL: 
Title: Principled Strength Reduction  
Author: Yanhong A. Liu 
Date: August 1996  
Abstract: This paper presents a principled approach for optimizing iterative (or recursive) programs. The approach formulates a loop body as a function f and a change operation , incrementalizes f with respect to , and adopts an incrementalized loop body to form a new loop that is more efficient. Three general optimizations are performed as part of the adoption; they systematically handle initializations, termination conditions, and final return values on exits of loops. These optimizations are either omitted, or done in implicit, limited, or ad hoc ways in previous methods. The new approach generalizes classical loop optimization techniques, notably strength reduction, in optimizing compilers, and it unifies and systematizes various optimization strategies in transformational programming. Such principled strength reduction performs drastic program efficiency improvement via incrementalization and appreciably reduces code size via associated optimizations. We give examples where this approach can systematically produce strength-reduced programs while no previous method can.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley Series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year> <month> 14 </month>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: <ref> [1] </ref> n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output <p> This requires adding the computation and maintenance of such information to those other branches. Since the original termination condition often uses values of induction variables, this optimization gen eralizes the elimination of induction variables in classical strength reduction <ref> [1, 3, 10] </ref> to the elimination of all replaceable variables. Example. Consider the running example. It is obvious that the variable i is maintained only to test the termination condition, since i is used in the new loop body ~ b 0 in (13) only for updating itself. <p> From the O (n 3 ) time program on the left, we obtain the program on the right that takes O (n 2 ) time: min := a <ref> [1] </ref>; for j := i to n do for k := i to j do min := min (min; sum) for i := 1 to n do for j := i to n do min := min (min; sum 1 ) 12 The transformation proceeds as follow. <p> the O (n 3 ) time program on the left, which is the same as the one on the left of (25) except that j goes from i down to 1 instead of to n, we obtain the program on the right that takes O (n) time: min := a <ref> [1] </ref>; for j := i downto 1 do sum := 0; sum := sum + a [k] min := a [1]; sum 1 := 0; for j := i downto 1 do sum 1 := sum 1 + a [j]; min := a [1]; for i := 1 to n do <p> of (25) except that j goes from i down to 1 instead of to n, we obtain the program on the right that takes O (n) time: min := a <ref> [1] </ref>; for j := i downto 1 do sum := 0; sum := sum + a [k] min := a [1]; sum 1 := 0; for j := i downto 1 do sum 1 := sum 1 + a [j]; min := a [1]; for i := 1 to n do min := min (min; min 1 ) The transformations on the innermost and the middle loops are similar to those <p> takes O (n) time: min := a <ref> [1] </ref>; for j := i downto 1 do sum := 0; sum := sum + a [k] min := a [1]; sum 1 := 0; for j := i downto 1 do sum 1 := sum 1 + a [j]; min := a [1]; for i := 1 to n do min := min (min; min 1 ) The transformations on the innermost and the middle loops are similar to those for (25), and we first obtain the program in the middle of (26), which is the same as the one on the right
Reference: [2] <author> F. E. Allen. </author> <title> Program optimization. </title> <booktitle> In Annual Review of Automatic Programming, </booktitle> <volume> volume 5, </volume> <pages> pages 239-307. </pages> <publisher> Pergamon: </publisher> <address> Elmsford, New York, </address> <year> 1969. </year>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; <ref> [2] </ref> m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part
Reference: [3] <author> F. E. Allen, J. Cocke, and K. Kennedy. </author> <title> Reduction of operator strength. </title> <editor> In S. S. Muchnick and N. D. Jones, editors, </editor> <title> Program Flow Analysis, </title> <booktitle> chapter 3, </booktitle> <pages> pages 79-101. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> This requires adding the computation and maintenance of such information to those other branches. Since the original termination condition often uses values of induction variables, this optimization gen eralizes the elimination of induction variables in classical strength reduction <ref> [1, 3, 10] </ref> to the elimination of all replaceable variables. Example. Consider the running example. It is obvious that the variable i is maintained only to test the termination condition, since i is used in the new loop body ~ b 0 in (13) only for updating itself. <p> The latter is a problem that needs further study. 13 7 Related work and conclusion Strength reduction <ref> [3, 10] </ref> is a classical compiler optimization technique that can be traced back to recursive address calculation for early ALGOL 60 compilers [20, 22]. <p> Composite hoisting-strength reduction [25, 26] is also syntactic and locally updating and it requires "admissible" term structure. Our method exploits program semantics to achieve greater strength-reduction, utilizes program analyses to guarantee correctness and efficiency, and is generally applicable. In particular, eliminating induction variables <ref> [3] </ref> is a special case of our optimizations. Optimal code motion [41] is a principled method for optimal placement of computations within a program with respect to the Herbrand interpretation.
Reference: [4] <author> U. Banerjee. </author> <title> Unimodular transformations of double loops. </title> <booktitle> In Proceedings of the Workshop on Advances in Languages and Compilers for Parallel Processing, </booktitle> <pages> pages 192-219, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; <ref> [4] </ref> while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 <p> Of course, given programs could be arbitrary. We may need to develop special program analysis to recognize such continuity and perform loop interchanging transformations, similar to those used for enhancing parallelism and data locality <ref> [4, 43, 44] </ref>. Section 6 gives examples with nested loops. 5.2 Recursive programs For a recursive program, determining an input change operation that corresponds to how the recursion proceeds is not always simple.
Reference: [5] <author> F. L. Bauer, B. Moller, H. Partsch, and P. Pepper. </author> <title> Formal program construction by transformations|Computer-aided, </title> <journal> intuition-guided programming. IEEE Transactions on Software Engineering, </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do <ref> [5] </ref> p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 and 10 is then transformed <p> As discussed in [30], its underlying principle is essentially incrementalization. But their work stresses mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed for automatic or semi-automatic uses. Transforming recursive functions in CIP <ref> [5, 8, 37] </ref> uses a collection of optimization strategies and techniques, including memoization, tabulation, relocation, precomputation, differencing, specialization, and simplifying recursion. They are essentially all subsumed by principled strength reduction, which is also more systematic.
Reference: [6] <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> October </month> <year> 1984. </year>
Reference-contexts: Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; <ref> [6] </ref> if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 and 10 is then transformed into a while loop of form (1): <p> They are essentially all subsumed by principled strength reduction, which is also more systematic. Other work on transformational programming for improving program efficiency, including the extension technique in [13], the promotion and accumulation strategies in <ref> [6, 7] </ref>, and finite differencing of functional programs in KIDS [40], can also be further automated with principled strength reduction. Principled strength reduction improves over previous approaches for program efficiency improvement. It systematically handles program constructs and operations that were not handled systematically before.
Reference: [7] <author> R. S. Bird. </author> <title> Addendum: The promotion and accumulation strategies in transformational programming. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 490-492, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: Transforming the for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then <ref> [7] </ref> m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 and 10 is then transformed into a while loop of form (1): &lt; n; m; i &gt; := <p> They are essentially all subsumed by principled strength reduction, which is also more systematic. Other work on transformational programming for improving program efficiency, including the extension technique in [13], the promotion and accumulation strategies in <ref> [6, 7] </ref>, and finite differencing of functional programs in KIDS [40], can also be further automated with principled strength reduction. Principled strength reduction improves over previous approaches for program efficiency improvement. It systematically handles program constructs and operations that were not handled systematically before.
Reference: [8] <author> M. Broy. </author> <title> Algebraic methods for program construction: The project CIP. </title> <editor> In P. Pepper, editor, </editor> <booktitle> Program Transformation and Programming Environments, volume 8 of NATO Advanced Science Institutes Series F: Computer and System Sciences, </booktitle> <pages> pages 199-222. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1984. </year> <booktitle> Proceedings of the NATO Advanced Research Workshop on Program Transformation and Programming Environments, </booktitle> <editor> directed by F. L. Bauer and H. Remus, </editor> <address> Munich, Germany, </address> <month> September </month> <year> 1983. </year>
Reference-contexts: for loop into a while loop, the program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i <ref> [8] </ref> else if p &lt; 0 then [9] m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 and 10 is then transformed into a while loop of form (1): &lt; n; m; i &gt; := &lt; input; 2 l1 ; l 2 <p> As discussed in [30], its underlying principle is essentially incrementalization. But their work stresses mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed for automatic or semi-automatic uses. Transforming recursive functions in CIP <ref> [5, 8, 37] </ref> uses a collection of optimization strategies and techniques, including memoization, tabulation, relocation, precomputation, differencing, specialization, and simplifying recursion. They are essentially all subsumed by principled strength reduction, which is also more systematic.
Reference: [9] <author> J. Cai and R. Paige. </author> <title> Program derivation by fixed point computation. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 11 </volume> <pages> 197-261, </pages> <month> September </month> <year> 1988/89. </year>
Reference-contexts: program in Figure 3 (a) is transformed into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then <ref> [9] </ref> m := m 2 i ; [11] output (m) (3) The part of the program between lines 1 and 10 is then transformed into a while loop of form (1): &lt; n; m; i &gt; := &lt; input; 2 l1 ; l 2 &gt;; while i 0 do &lt; n; <p> Similar optimizations for the preprocessing code are discussed by Cai and Paige <ref> [9] </ref>, whose programs are written in a very-high-level language based on sets. While they can obtain simplified programs by exploiting properties of sets, our optimizations apply also to programs written in lower level languages. <p> We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages [16, 17, 18] generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing [34, 35, 36] and fixed point recomputation <ref> [9] </ref> systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification. These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do.
Reference: [10] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. 
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> This requires adding the computation and maintenance of such information to those other branches. Since the original termination condition often uses values of induction variables, this optimization gen eralizes the elimination of induction variables in classical strength reduction <ref> [1, 3, 10] </ref> to the elimination of all replaceable variables. Example. Consider the running example. It is obvious that the variable i is maintained only to test the termination condition, since i is used in the new loop body ~ b 0 in (13) only for updating itself. <p> The latter is a problem that needs further study. 13 7 Related work and conclusion Strength reduction <ref> [3, 10] </ref> is a classical compiler optimization technique that can be traced back to recursive address calculation for early ALGOL 60 compilers [20, 22].
Reference: [11] <author> J. Cocke and J. T. Schwartz. </author> <title> Programming Languages and Their Compilers; Preliminary Notes. </title> <type> Technical report, </type> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, </address> <year> 1970. </year>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> into: [1] n := input; [2] m := 2 l1 ; [4] while i 0 do [5] p := n m 2 ; [6] if p &gt; 0 then [7] m := m + 2 i [8] else if p &lt; 0 then [9] m := m 2 i ; <ref> [11] </ref> output (m) (3) The part of the program between lines 1 and 10 is then transformed into a while loop of form (1): &lt; n; m; i &gt; := &lt; input; 2 l1 ; l 2 &gt;; while i 0 do &lt; n; m; i &gt; := &lt; n; update
Reference: [12] <author> R. L. Constable et al. </author> <title> Implementing Mathematics with the Nuprl Proof Development System. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: The strength-reduced program was manually discovered and proved correct using Nuprl <ref> [12] </ref>. As discussed above, the optimizations used in [33] either incurred extra levels of proofs or were not handled formally. Another drawback is that there was no formal treatment of cost. As mentioned in [30], the final program in [33] contains an unnecessary shift.
Reference: [13] <author> N. Dershowitz. </author> <title> The Evolution of Programs, </title> <booktitle> volume 5 of Progress in Computer Science. </booktitle> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1983. </year>
Reference-contexts: We have showed through the presentation how our method is used to systematically derive a strength-reduced program, which automates and simplifies the VLSI circuit design process. Many similar programs, such as the various versions of real/integer division/square-root algorithms discussed in <ref> [13] </ref>, can also be derived using our method. 6.2 Minimum-sum section problem This example is taken from [24]. Given an array a [1::n] of numbers, where n 1. A minimum-sum section of a is a non-empty sequence of adjacent elements whose sum is a minimum. <p> They are essentially all subsumed by principled strength reduction, which is also more systematic. Other work on transformational programming for improving program efficiency, including the extension technique in <ref> [13] </ref>, the promotion and accumulation strategies in [6, 7], and finite differencing of functional programs in KIDS [40], can also be further automated with principled strength reduction. Principled strength reduction improves over previous approaches for program efficiency improvement.
Reference: [14] <author> E. W. Dijkstra. </author> <title> A Discipline of Programming. </title> <booktitle> Prentice-Hall Series in Automatic Computation. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1976. </year>
Reference-contexts: These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do. In general, they apply only to programs written in very-high-level languages like SETL; our method applies also to lower-level languages. Loop development by maintaining and strengthening loop invariants <ref> [14, 23, 24, 38] </ref> has been advocated by Dijkstra, Gries, and others for almost two decades as a standard strategy. As discussed in [30], its underlying principle is essentially incrementalization.
Reference: [15] <author> I. Flores. </author> <booktitle> The Logic of Computer Arithmetic. Prentice-Hall International Series in Electrical Engineering. </booktitle> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1963. </year>
Reference-contexts: The initial specification of the algorithm is given in Figure 3 (a). Given a binary integer n of l bits, where n &gt; 0 (and l is usually 8, 16, ...), it computes the binary integer square root m of n using the non-restoring method <ref> [15, 33] </ref>, which is exact for perfect squares and off by at most 1 for other integers. In hardware, multiplications and exponentials are much more expensive than additions and shifts (doublings or halvings), so the goal is to replace the former by the latter.
Reference: [16] <author> A. C. Fong. </author> <title> Generalized common subexpressions in very high level languages. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 48-57, </pages> <address> Los Angeles, California, </address> <month> January </month> <year> 1977. </year>
Reference-contexts: In fact, their method would not perform any strength reduction on the square root example [27]. Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages <ref> [16, 17, 18] </ref> generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing [34, 35, 36] and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification.
Reference: [17] <author> A. C. Fong. </author> <title> Inductively computable constructs in very high level languages. </title> <booktitle> In Conference Record of the 6th Annual ACM Symposium on POPL, </booktitle> <pages> pages 21-28, </pages> <address> San Antonio, Texas, </address> <month> January </month> <year> 1979. </year>
Reference-contexts: In fact, their method would not perform any strength reduction on the square root example [27]. Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages <ref> [16, 17, 18] </ref> generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing [34, 35, 36] and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification.
Reference: [18] <author> A. C. Fong and J. D. Ullman. </author> <title> Inductive variables in very high level languages. </title> <booktitle> In Conference Record of the 3rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 104-112, </pages> <address> Atlanta, Georgia, </address> <month> January </month> <year> 1976. </year>
Reference-contexts: In fact, their method would not perform any strength reduction on the square root example [27]. Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages <ref> [16, 17, 18] </ref> generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing [34, 35, 36] and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification.
Reference: [19] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guid to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Yet, the number of additional copy statements and the number of additional temporary variables can sometimes be reduced substantially by carefully ordering the assignment statements. The problem of minimizing the number S of additional copy statements is NP-complete <ref> [19, 39] </ref>, but good heuristics exist [39]. We can use an algorithm that easily decides an appropriate order of assignments when no additional temporary variables or copy statements are needed and reduces the problem to the problem of minimizing S otherwise.
Reference: [20] <author> A. A. Grau, U. Hill, and H. Langmaac. </author> <title> Translation of ALGOL 60, volume 1 of Handbook for automatic computation. </title> <publisher> Springer, </publisher> <address> Berlin, </address> <year> 1967. </year>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> The latter is a problem that needs further study. 13 7 Related work and conclusion Strength reduction [3, 10] is a classical compiler optimization technique that can be traced back to recursive address calculation for early ALGOL 60 compilers <ref> [20, 22] </ref>. As discussed in [42], it is syntactic (ignoring semantic equivalences between syntactically different terms), locally updating (thus not guaranteeing safety or speedup), and structurally restricted (only working on induction variables and region constants).
Reference: [21] <author> S. A. Greibach. </author> <title> Theory of Program Structures: Schemes, Semantics, Verification, </title> <booktitle> volume 36 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1975. </year>
Reference-contexts: If the recursion is not linear, then there is no direct correspondence of it to an iteration. In fact, it is known that the while scheme is equivalent to flow chart, which is strictly less expressive than the recursive scheme <ref> [21] </ref>. Simple heuristics exist for recognizing how recursions proceed on the argument, e.g., for an integer argument, a change operation may be x 0 = x + 1; for a list argument, a change operation may be x 0 = cons (y; x).
Reference: [22] <author> D. Gries. </author> <title> Compiler Construction for Digital Computers. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> The latter is a problem that needs further study. 13 7 Related work and conclusion Strength reduction [3, 10] is a classical compiler optimization technique that can be traced back to recursive address calculation for early ALGOL 60 compilers <ref> [20, 22] </ref>. As discussed in [42], it is syntactic (ignoring semantic equivalences between syntactically different terms), locally updating (thus not guaranteeing safety or speedup), and structurally restricted (only working on induction variables and region constants).
Reference: [23] <editor> D. Gries. </editor> <booktitle> The Science of Programming. Texts and Monographs in Computer Science. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1981. </year> <month> 15 </month>
Reference-contexts: These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do. In general, they apply only to programs written in very-high-level languages like SETL; our method applies also to lower-level languages. Loop development by maintaining and strengthening loop invariants <ref> [14, 23, 24, 38] </ref> has been advocated by Dijkstra, Gries, and others for almost two decades as a standard strategy. As discussed in [30], its underlying principle is essentially incrementalization.
Reference: [24] <author> D. Gries. </author> <title> A note on a standard strategy for developing loop invariants and loops. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 2 </volume> <pages> 207-214, </pages> <year> 1984. </year>
Reference-contexts: Many similar programs, such as the various versions of real/integer division/square-root algorithms discussed in [13], can also be derived using our method. 6.2 Minimum-sum section problem This example is taken from <ref> [24] </ref>. Given an array a [1::n] of numbers, where n 1. A minimum-sum section of a is a non-empty sequence of adjacent elements whose sum is a minimum. A naive algorithm takes O (n 3 ) time. <p> These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do. In general, they apply only to programs written in very-high-level languages like SETL; our method applies also to lower-level languages. Loop development by maintaining and strengthening loop invariants <ref> [14, 23, 24, 38] </ref> has been advocated by Dijkstra, Gries, and others for almost two decades as a standard strategy. As discussed in [30], its underlying principle is essentially incrementalization.
Reference: [25] <author> S. M. Joshi and D. M. Dhamdhere. </author> <title> A composite hoisting-strength reuction transformation for global program optimization|part I. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 11 </volume> <pages> 21-41, </pages> <year> 1982. </year>
Reference-contexts: As discussed in [42], it is syntactic (ignoring semantic equivalences between syntactically different terms), locally updating (thus not guaranteeing safety or speedup), and structurally restricted (only working on induction variables and region constants). Composite hoisting-strength reduction <ref> [25, 26] </ref> is also syntactic and locally updating and it requires "admissible" term structure. Our method exploits program semantics to achieve greater strength-reduction, utilizes program analyses to guarantee correctness and efficiency, and is generally applicable. In particular, eliminating induction variables [3] is a special case of our optimizations.
Reference: [26] <author> S. M. Joshi and D. M. Dhamdhere. </author> <title> A composite hoisting-strength reuction transformation for global program optimization|part II. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 11 </volume> <pages> 111-126, </pages> <year> 1982. </year>
Reference-contexts: As discussed in [42], it is syntactic (ignoring semantic equivalences between syntactically different terms), locally updating (thus not guaranteeing safety or speedup), and structurally restricted (only working on induction variables and region constants). Composite hoisting-strength reduction <ref> [25, 26] </ref> is also syntactic and locally updating and it requires "admissible" term structure. Our method exploits program semantics to achieve greater strength-reduction, utilizes program analyses to guarantee correctness and efficiency, and is generally applicable. In particular, eliminating induction variables [3] is a special case of our optimizations.
Reference: [27] <author> J. Knoop. Private comminication, </author> <month> December </month> <year> 1994. </year>
Reference-contexts: Our method is also a principled approach, based on the idea of incrementalization. It exploits properties of more primitive operators, data structures, and conditionals, and thus is a more comprehensive exploration of availability. In fact, their method would not perform any strength reduction on the square root example <ref> [27] </ref>. Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages [16, 17, 18] generalize conventional strength reduction and the elimination of induction variables to set-based languages.
Reference: [28] <author> Y. A. Liu. CACHET: </author> <title> An interactive, incremental-attribution-based program transformation system for deriving incremental programs. </title> <booktitle> In Proceedings of the 10th Knowledge-Based Software Engineering Conference, </booktitle> <pages> pages 19-26, </pages> <address> Boston, Massachusetts, November 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Third, used off-line on paper, it supports a general methodology for systematic program efficiency improvement, which is one of the most important issues in program development and maintenance. A prototype implementation for semi-automatic use is under development <ref> [28] </ref>. To scale up the method for application to larger problems, we can select only expensive subcomputations in an iteration to be strength reduced. Note that, as with usual compiler optimization techniques, the effectiveness of our techniques depends on the original programs. Section 6 illustrates this.
Reference: [29] <author> Y. A. Liu. </author> <title> Incremental Computation: A Semantics-Based Systematic Transformational Approach. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, New York, </address> <month> January </month> <year> 1996. </year> <note> Also appeared as Technical Report TR 95-1551, </note> <month> October </month> <year> 1995. </year>
Reference-contexts: We have previously proposed a general systematic approach to incrementalization <ref> [29, 30] </ref>. Given a function f and an input change operation , the approach aims to obtain an incremental program that fl The author gratefully acknowledges the support of ONR under contract No. N00014-92-J-1973 and NSF under contract No. CCR-9503319. <p> Section 4 describes optimizations in adopting incremental programs to form more efficient iterations. Section 5 discusses extensions and applications of the approach. Section 6 gives examples. Finally, Section 7 discusses related work and concludes. 2 Preliminaries Our incrementalization method <ref> [29, 30] </ref> has been described using a first-order, call-by-value functional programming language. <p> Of course, maintaining additional information takes extra space. Our primary goal is to improve the asymptotic running time of the incremental computation. We attempt to save space by maintaining only information useful for achieving this. Given a program f and an operation , we can use the approach in <ref> [29, 30] </ref> to derive a program ~ f (x) that extends f (x) to return useful additional information about x, a program ~ f 0 (x; y; ~r) that incrementally computes f (x y) and maintains additional information about x y when ~r = ~ f (x), and a function that <p> function f and operation : f (n; m; i) = &lt; n; update (n; m; i); i1 &gt; and hn; m; ii h i = hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in <ref> [29, 30, 31, 32] </ref>, we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results [31], and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function <p> In addition, the parameter x is always included in the return value. Therefore, the corresponding function ~ f 0 actually uses only the parameter ~r. Incrementalization per se is not the subject of this paper. Thus, we treat the methods of <ref> [29, 30, 31, 32] </ref> as a black box and, in the running example, give only the result without further explanation. Example. For the function f in (5), it is obvious that components 1 and 3 of the return value are trivially updated by the operation in (5).
Reference: [30] <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on POPL, </booktitle> <pages> pages 157-170, </pages> <address> St. Petersburg Beach, Florida, </address> <month> January </month> <year> 1996. </year>
Reference-contexts: We have previously proposed a general systematic approach to incrementalization <ref> [29, 30] </ref>. Given a function f and an input change operation , the approach aims to obtain an incremental program that fl The author gratefully acknowledges the support of ONR under contract No. N00014-92-J-1973 and NSF under contract No. CCR-9503319. <p> CCR-9503319. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong@cs.cornell.edu 1 computes f (x y) efficiently by making use of f (x) [32], the intermediate results computed in computing f (x) [31], and auxiliary information about f (x) that can be inexpensively maintained <ref> [30] </ref>. Since every non-trivial computation proceeds by iteration (or recursion), the approach can be used for achieving efficient computation by computing each iteration using an appropriate incremental program. <p> Section 4 describes optimizations in adopting incremental programs to form more efficient iterations. Section 5 discusses extensions and applications of the approach. Section 6 gives examples. Finally, Section 7 discusses related work and concludes. 2 Preliminaries Our incrementalization method <ref> [29, 30] </ref> has been described using a first-order, call-by-value functional programming language. <p> Of course, maintaining additional information takes extra space. Our primary goal is to improve the asymptotic running time of the incremental computation. We attempt to save space by maintaining only information useful for achieving this. Given a program f and an operation , we can use the approach in <ref> [29, 30] </ref> to derive a program ~ f (x) that extends f (x) to return useful additional information about x, a program ~ f 0 (x; y; ~r) that incrementally computes f (x y) and maintains additional information about x y when ~r = ~ f (x), and a function that <p> function f and operation : f (n; m; i) = &lt; n; update (n; m; i); i1 &gt; and hn; m; ii h i = hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in <ref> [29, 30, 31, 32] </ref>, we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results [31], and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function <p> i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in [29, 30, 31, 32], we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results [31], and certain auxiliary information <ref> [30] </ref> of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function ~ f 0 that incrementally computes f (x y) and maintains the additional information, and a constant-time projection function that projects the value of f out of ~ f . <p> In addition, the parameter x is always included in the return value. Therefore, the corresponding function ~ f 0 actually uses only the parameter ~r. Incrementalization per se is not the subject of this paper. Thus, we treat the methods of <ref> [29, 30, 31, 32] </ref> as a black box and, in the running example, give only the result without further explanation. Example. For the function f in (5), it is obvious that components 1 and 3 of the return value are trivially updated by the operation in (5). <p> Example. For the function f in (5), it is obvious that components 1 and 3 of the return value are trivially updated by the operation in (5). Thus, we consider only the function update in component 2. Incremen talizing update under , as done in <ref> [30] </ref>, we obtain the function g update and incremental function g update 0 below and projection ( ~r 1 ) = 1st ( ~r 1 ). g update (n; m; i) = let p = n m 2 in if p &gt; 0 then let u = 2 i in &lt; <p> Basically, it builds a directed graph G representing the dependencies 3 The third branch of each conditional is a 2-component tuple instead of a 5-component tuple because components 3 to 5 are irrelevant and can be safely eliminated <ref> [30, 31] </ref>. 6 in the given assignments, solves the assignment problem for each of the strongly connected components (SCC) of G, and puts the resulting assignments for each of the SCCs together according to a topological order of the SCCs. <p> The strength-reduced program was manually discovered and proved correct using Nuprl [12]. As discussed above, the optimizations used in [33] either incurred extra levels of proofs or were not handled formally. Another drawback is that there was no formal treatment of cost. As mentioned in <ref> [30] </ref>, the final program in [33] contains an unnecessary shift. We have showed through the presentation how our method is used to systematically derive a strength-reduced program, which automates and simplifies the VLSI circuit design process. <p> 2 into the recursive case, we obtain a simpler program: f f ib (v) = if v 1 then &lt; 1; 1 &gt; else let ~r = f fib (v 1) in &lt; 1st (~r)+2nd (~r); 1st (~r) &gt; Similar optimizations can be performed to simplify the resulting program in <ref> [30] </ref>, where an algorithm for Bird's path sequence problem is improved from exponential time to square time. 4 We are mainly showing that different ways of writing the original programs end up in different optimized programs, not that how one way can be transformed systematically into another that has a better <p> Loop development by maintaining and strengthening loop invariants [14, 23, 24, 38] has been advocated by Dijkstra, Gries, and others for almost two decades as a standard strategy. As discussed in <ref> [30] </ref>, its underlying principle is essentially incrementalization. But their work stresses mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed for automatic or semi-automatic uses.
Reference: [31] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Caching intermediate results for program improvement. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on PEPM, </booktitle> <pages> pages 190-201, </pages> <address> La Jolla, California, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: N00014-92-J-1973 and NSF under contract No. CCR-9503319. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong@cs.cornell.edu 1 computes f (x y) efficiently by making use of f (x) [32], the intermediate results computed in computing f (x) <ref> [31] </ref>, and auxiliary information about f (x) that can be inexpensively maintained [30]. Since every non-trivial computation proceeds by iteration (or recursion), the approach can be used for achieving efficient computation by computing each iteration using an appropriate incremental program. <p> function f and operation : f (n; m; i) = &lt; n; update (n; m; i); i1 &gt; and hn; m; ii h i = hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in <ref> [29, 30, 31, 32] </ref>, we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results [31], and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function <p> hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in [29, 30, 31, 32], we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results <ref> [31] </ref>, and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function ~ f 0 that incrementally computes f (x y) and maintains the additional information, and a constant-time projection function that projects the value of f <p> In addition, the parameter x is always included in the return value. Therefore, the corresponding function ~ f 0 actually uses only the parameter ~r. Incrementalization per se is not the subject of this paper. Thus, we treat the methods of <ref> [29, 30, 31, 32] </ref> as a black box and, in the running example, give only the result without further explanation. Example. For the function f in (5), it is obvious that components 1 and 3 of the return value are trivially updated by the operation in (5). <p> Basically, it builds a directed graph G representing the dependencies 3 The third branch of each conditional is a 2-component tuple instead of a 5-component tuple because components 3 to 5 are irrelevant and can be safely eliminated <ref> [30, 31] </ref>. 6 in the given assignments, solves the assignment problem for each of the strongly connected components (SCC) of G, and puts the resulting assignments for each of the SCCs together according to a topological order of the SCCs. <p> Such folding can be done if the final return value can be computed using one more iteration based on the maintained additional information. The backward dependence analysis needed for this optimization can use the one developed in <ref> [31] </ref>. Example. For the running example, the value that is needed on exit of the loop, i.e., the value of m, is the first component computed by g update 1 . Analyze (19). <p> is initialized to 0 before L i starts; and the loop L j is eliminated. 6.3 Fibonacci function and path sequence problem The Fibonacci function f ib is improved from an exponential time program to a linear time program f ib (x) = 1st ( f f ib (x)) in <ref> [31] </ref>, by considering fib as a function f , x 0 = x + 1 as an operation , and using the derived constant time function f 0 to form a new recursion: fib (x) = if x 1 then 1 else f ib (x 1) + fib (x 2) f
Reference: [32] <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: N00014-92-J-1973 and NSF under contract No. CCR-9503319. Author's address: Department of Computer Science, Cornell University, Ithaca, NY 14853. Email: yanhong@cs.cornell.edu 1 computes f (x y) efficiently by making use of f (x) <ref> [32] </ref>, the intermediate results computed in computing f (x) [31], and auxiliary information about f (x) that can be inexpensively maintained [30]. Since every non-trivial computation proceeds by iteration (or recursion), the approach can be used for achieving efficient computation by computing each iteration using an appropriate incremental program. <p> Note that some of the parameters of ~ f 0 may be dead and eliminated <ref> [32] </ref>. 3 n := input; m := 2 l1 ; for i := l 2 downto 0 do p := n m 2 ; if p &gt; 0 then m := m + 2 i else if p &lt; 0 then m := m 2 i ; output (m) (a) fi <p> function f and operation : f (n; m; i) = &lt; n; update (n; m; i); i1 &gt; and hn; m; ii h i = hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in <ref> [29, 30, 31, 32] </ref>, we can derive an incremental program that computes f (x y) incrementally by using the return value [32], the intermediate results [31], and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function <p> ii h i = hn; update (n; m; i); i1i (5) 3.2 Step 2: Incrementalization Given a function f and an input change operation , using the approach in [29, 30, 31, 32], we can derive an incremental program that computes f (x y) incrementally by using the return value <ref> [32] </ref>, the intermediate results [31], and certain auxiliary information [30] of f (x), i.e., we obtain a function ~ f that computes f (x) and necessary additional information, a function ~ f 0 that incrementally computes f (x y) and maintains the additional information, and a constant-time projection function that projects <p> In addition, the parameter x is always included in the return value. Therefore, the corresponding function ~ f 0 actually uses only the parameter ~r. Incrementalization per se is not the subject of this paper. Thus, we treat the methods of <ref> [29, 30, 31, 32] </ref> as a black box and, in the running example, give only the result without further explanation. Example. For the function f in (5), it is obvious that components 1 and 3 of the return value are trivially updated by the operation in (5).
Reference: [33] <author> J. O'Leary, M. Leeser, J. Hickey, and M. Aagaard. </author> <title> Non-restoring integer square root: A case study in design by principled optimization. </title> <editor> In R. Kumar and T. Kropf, editors, </editor> <booktitle> Proceedings of TPCD '94: the 2nd International Conference on Theorem Provers in Circuit Design|Theory, Practice and Experience, volume 901 of Lecture Notes in Computer Science, </booktitle> <pages> pages 52-71, </pages> <address> Bad Herrenalb (Black Forest), Germany, September 1994. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: This paper considers iterative programs written in such an imperative language. For simplicity, only structured programs are considered. Example. We use the derivation of an efficient binary integer square root algorithm for VLSI design <ref> [33] </ref> as a running example. The initial specification of the algorithm is given in Figure 3 (a). <p> The initial specification of the algorithm is given in Figure 3 (a). Given a binary integer n of l bits, where n &gt; 0 (and l is usually 8, 16, ...), it computes the binary integer square root m of n using the non-restoring method <ref> [15, 33] </ref>, which is exact for perfect squares and off by at most 1 for other integers. In hardware, multiplications and exponentials are much more expensive than additions and shifts (doublings or halvings), so the goal is to replace the former by the latter. <p> Thus, in the final program in Figure 3 (b), only the underlined components, which correspond to variables p, a, and d, are initialized. This optimization would have saved a level in the 2-level proofs in <ref> [33] </ref>. <p> Due to the folding done at the end of Step 6, which will be explained in the next subsection, the test is d 1 instead of d 4. In the design work in <ref> [33] </ref>, the termination condition is not handled formally, and elimination of the induction variable i is deferred to transformations in the hardware implementation. <p> We fold the retrieval (22) into the loop body by finally retrieving the value of m from a and changing the termination condition from d 4 to d 1 (1 = d=4). We obtain the final program in Figure 3 (b). Similar optimization is needed in <ref> [33] </ref> but is done implicitly; it is never said explicitly or proved formally how the desired value is finally returned. This is another subtle and error-prone optimization. 5 Extension and discussion So far, we have limited ourselves to considering only iterative programs with only single loops. <p> In particular, we show that different ways of writing the original programs result in different optimized programs. We also show that the general principles underlying our approach apply to programs that use arrays. 6.1 Non-restoring binary integer square root The running example is taken from VLSI circuit design <ref> [33] </ref>, which transforms the original specification into a strength reduced version and further into a hardware implementation. The strength-reduced program was manually discovered and proved correct using Nuprl [12]. As discussed above, the optimizations used in [33] either incurred extra levels of proofs or were not handled formally. <p> Non-restoring binary integer square root The running example is taken from VLSI circuit design <ref> [33] </ref>, which transforms the original specification into a strength reduced version and further into a hardware implementation. The strength-reduced program was manually discovered and proved correct using Nuprl [12]. As discussed above, the optimizations used in [33] either incurred extra levels of proofs or were not handled formally. Another drawback is that there was no formal treatment of cost. As mentioned in [30], the final program in [33] contains an unnecessary shift. <p> As discussed above, the optimizations used in <ref> [33] </ref> either incurred extra levels of proofs or were not handled formally. Another drawback is that there was no formal treatment of cost. As mentioned in [30], the final program in [33] contains an unnecessary shift. We have showed through the presentation how our method is used to systematically derive a strength-reduced program, which automates and simplifies the VLSI circuit design process.
Reference: [34] <author> B. Paige and J. T. Schwartz. </author> <title> Expression continuity and the formal differentiation of algorithms. </title> <booktitle> In Conference Record of the 4th Annual ACM Symposium on POPL, </booktitle> <pages> pages 58-71, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. Finite differencing <ref> [34, 35, 36] </ref> generalizes strength reduction to languages with expressions composed of aggregate operations like set operations. Basically, a set of finite differencing rules are developed to transform aggregate operations in loop bodies into more efficient incremental operations. <p> Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages [16, 17, 18] generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing <ref> [34, 35, 36] </ref> and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification. These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do.
Reference: [35] <author> R. Paige. </author> <title> Transformational programming|Applications to algorithms and systems. </title> <booktitle> In Conference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. Finite differencing <ref> [34, 35, 36] </ref> generalizes strength reduction to languages with expressions composed of aggregate operations like set operations. Basically, a set of finite differencing rules are developed to transform aggregate operations in loop bodies into more efficient incremental operations. <p> Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages [16, 17, 18] generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing <ref> [34, 35, 36] </ref> and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification. These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do.
Reference: [36] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. Finite differencing <ref> [34, 35, 36] </ref> generalizes strength reduction to languages with expressions composed of aggregate operations like set operations. Basically, a set of finite differencing rules are developed to transform aggregate operations in loop bodies into more efficient incremental operations. <p> Of course, the complexity of our algorithm is larger. We plan to analyze complexity on certain classes of problems. Inductively computable constructs in very-high-level languages [16, 17, 18] generalize conventional strength reduction and the elimination of induction variables to set-based languages. Finite differencing <ref> [34, 35, 36] </ref> and fixed point recomputation [9] systematically reduce strength of programs that use fixed point iteration and set-theoretic notations as the initial program specification. These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do.
Reference: [37] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: As discussed in [30], its underlying principle is essentially incrementalization. But their work stresses mental tools for programming, rather than mechanical assistance, so no systematic procedures were proposed for automatic or semi-automatic uses. Transforming recursive functions in CIP <ref> [5, 8, 37] </ref> uses a collection of optimization strategies and techniques, including memoization, tabulation, relocation, precomputation, differencing, specialization, and simplifying recursion. They are essentially all subsumed by principled strength reduction, which is also more systematic.
Reference: [38] <author> J. C. Reynolds. </author> <title> The Craft of Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: These techniques do not handle function abstractions, conditionals, or data types other than sets, as we do. In general, they apply only to programs written in very-high-level languages like SETL; our method applies also to lower-level languages. Loop development by maintaining and strengthening loop invariants <ref> [14, 23, 24, 38] </ref> has been advocated by Dijkstra, Gries, and others for almost two decades as a standard strategy. As discussed in [30], its underlying principle is essentially incrementalization.
Reference: [39] <author> R. Sethi. </author> <title> A note on implementing parallel assignment instructions. </title> <journal> Information Processing Letter, </journal> <volume> 2 </volume> <pages> 91-95, </pages> <year> 1973. </year>
Reference-contexts: Yet, the number of additional copy statements and the number of additional temporary variables can sometimes be reduced substantially by carefully ordering the assignment statements. The problem of minimizing the number S of additional copy statements is NP-complete <ref> [19, 39] </ref>, but good heuristics exist [39]. We can use an algorithm that easily decides an appropriate order of assignments when no additional temporary variables or copy statements are needed and reduces the problem to the problem of minimizing S otherwise. <p> Yet, the number of additional copy statements and the number of additional temporary variables can sometimes be reduced substantially by carefully ordering the assignment statements. The problem of minimizing the number S of additional copy statements is NP-complete [19, 39], but good heuristics exist <ref> [39] </ref>. We can use an algorithm that easily decides an appropriate order of assignments when no additional temporary variables or copy statements are needed and reduces the problem to the problem of minimizing S otherwise.
Reference: [40] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: They are essentially all subsumed by principled strength reduction, which is also more systematic. Other work on transformational programming for improving program efficiency, including the extension technique in [13], the promotion and accumulation strategies in [6, 7], and finite differencing of functional programs in KIDS <ref> [40] </ref>, can also be further automated with principled strength reduction. Principled strength reduction improves over previous approaches for program efficiency improvement. It systematically handles program constructs and operations that were not handled systematically before. Also, it systematically handles initializations and termination conditions, which are often particularly error-prone.
Reference: [41] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> The value flow graph: A program representation for optimal program transformation. </title> <booktitle> In Proceedings of the 3rd ESOP, volume 432 of Lecture Notes in Computer Science, </booktitle> <pages> pages 389-405, </pages> <address> Copenhagen, Denmark, 1990. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: Our method exploits program semantics to achieve greater strength-reduction, utilizes program analyses to guarantee correctness and efficiency, and is generally applicable. In particular, eliminating induction variables [3] is a special case of our optimizations. Optimal code motion <ref> [41] </ref> is a principled method for optimal placement of computations within a program with respect to the Herbrand interpretation. It is adopted for strength reduction by exploring the additional availability obtained from properties, such as distributivity, of numeric operators [42], and it improves over conventional methods.
Reference: [42] <author> B. Steffen, J. Knoop, and O. Ruthing. </author> <title> Efficient code motion and an adaption to strength reduction. </title> <booktitle> In Proceedings of the 4th International Joint Conference on TAPSOFT, volume 494 of Lecture Notes in Computer Science, </booktitle> <pages> pages 394-415, </pages> <address> Brighton, U.K., 1991. </address> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: 1 Introduction Strength reduction <ref> [1, 2, 3, 10, 11, 20, 22, 42] </ref> is a classical loop optimization technique in optimizing compilers. The idea is to replace certain operations in loop bodies by faster operations. For example, a multiplication involving an induction variable can sometimes be transformed into an addition. <p> The latter is a problem that needs further study. 13 7 Related work and conclusion Strength reduction [3, 10] is a classical compiler optimization technique that can be traced back to recursive address calculation for early ALGOL 60 compilers [20, 22]. As discussed in <ref> [42] </ref>, it is syntactic (ignoring semantic equivalences between syntactically different terms), locally updating (thus not guaranteeing safety or speedup), and structurally restricted (only working on induction variables and region constants). Composite hoisting-strength reduction [25, 26] is also syntactic and locally updating and it requires "admissible" term structure. <p> Optimal code motion [41] is a principled method for optimal placement of computations within a program with respect to the Herbrand interpretation. It is adopted for strength reduction by exploring the additional availability obtained from properties, such as distributivity, of numeric operators <ref> [42] </ref>, and it improves over conventional methods. Our method is also a principled approach, based on the idea of incrementalization. It exploits properties of more primitive operators, data structures, and conditionals, and thus is a more comprehensive exploration of availability.
Reference: [43] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on PLDI, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Of course, given programs could be arbitrary. We may need to develop special program analysis to recognize such continuity and perform loop interchanging transformations, similar to those used for enhancing parallelism and data locality <ref> [4, 43, 44] </ref>. Section 6 gives examples with nested loops. 5.2 Recursive programs For a recursive program, determining an input change operation that corresponds to how the recursion proceeds is not always simple.
Reference: [44] <author> M. Wolf and M. Lam. </author> <title> A loop transformation theory and an algorithm to maximize parallelism. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> October </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: Of course, given programs could be arbitrary. We may need to develop special program analysis to recognize such continuity and perform loop interchanging transformations, similar to those used for enhancing parallelism and data locality <ref> [4, 43, 44] </ref>. Section 6 gives examples with nested loops. 5.2 Recursive programs For a recursive program, determining an input change operation that corresponds to how the recursion proceeds is not always simple.
References-found: 44

