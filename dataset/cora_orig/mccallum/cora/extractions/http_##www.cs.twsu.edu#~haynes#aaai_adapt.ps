URL: http://www.cs.twsu.edu/~haynes/aaai_adapt.ps
Refering-URL: http://adept.cs.twsu.edu/~thomas/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: e-mail: [haynes,sandip]@euler.mcs.utulsa.edu  
Title: Adaptation Using Cases in Cooperative Groups  
Author: Thomas Haynes and Sandip Sen 
Address: 600 South College Avenue Tulsa, OK 74104-3189  
Affiliation: Department of Mathematical Computer Sciences The University of Tulsa  
Abstract: In order to effectively exploit opportunities presented in the environment agents in a group must be well-adapted to each other and to the environment. Agents that fail to adapt and modify their behavior to suit environmental demands can hinder, rather than aid, in achieving group goals. Adaptability and flexibility are key components of intelligent behavior which allow agent groups to improve performance in a given domain using prior problem solving experience. This paper focuses on a particular incremental learning mechanism by which agents can better adapt to each other using problem solving experience. In particular, we propose a framework in which individual group members learn cases to improve their model of other group members. Using these models agents can choose less greedy and more appropriate actions in the context of the group. We use a testbed problem from the distributed AI literature to show that simultaneous learning by group members can lead to significant improvement in group performance and efficiency over agent groups following static behavioral rules.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David W. Aha, Dennis Kibler, and Marc K. Albert. </author> <title> Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6(1) </volume> <pages> 37-66, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The set of actions corresponding to the most relevant case is then adapted to fit the current situation. Cardie [2] defined case-based learning (CBL) as a machine learning 3 technique used to extend instance-based learning (IBL) <ref> [1] </ref>. The IBL algorithm retrieves the nearest instance (for our purposes, an instance can be thought of a case) to a state, and performs the suggested actions. There is no case adaptation if the retrieved instance is not a direct match to the current state.
Reference: [2] <author> Claire Cardie. </author> <title> Using decision trees to improve case-based learning. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pages 25-32. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: If there is no such match, then cases that are similar to the current state are retrieved from the case library. The set of actions corresponding to the most relevant case is then adapted to fit the current situation. Cardie <ref> [2] </ref> defined case-based learning (CBL) as a machine learning 3 technique used to extend instance-based learning (IBL) [1]. The IBL algorithm retrieves the nearest instance (for our purposes, an instance can be thought of a case) to a state, and performs the suggested actions.
Reference: [3] <author> Andrew Garland and Richard Alterman. </author> <title> Preparation of multi-agent knowledge for reuse. </title> <editor> In D. W. Aha and A. Ram, editors, </editor> <booktitle> AAAI Symposium on Adaptation of Knowldege for Reuse, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: These exceptions form a case library. The agent does not reason with these cases, as in CBR [9], but rather adapts an inaccurate individual model to improve performance. Though researchers have used CBR in multiagent systems [14], little work has been done in learning cases in multiagent systems <ref> [3, 12] </ref>. 2 Case-Based Learning Case-based reasoning (CBR) [4, 6, 9] is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems.
Reference: [4] <author> Andrew R. Golding and Paul S. Rosenbloom. </author> <title> Improving rule-based systems through case-based reasoning. </title> <booktitle> In AAAI, </booktitle> <pages> pages 22-27, </pages> <year> 1991. </year>
Reference-contexts: Though researchers have used CBR in multiagent systems [14], little work has been done in learning cases in multiagent systems [3, 12]. 2 Case-Based Learning Case-based reasoning (CBR) <ref> [4, 6, 9] </ref> is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems. <p> Cases can be positive or negative <ref> [4, 6] </ref>. A positive case informs the agent what to do, i.e. it reorders the set of actions. A negative case can reorder the actions and/or delete actions from the set. <p> Our cases are negative in the sense they tell the agents what not to do. (A positive case would tell the agent what to do in a certain situation <ref> [4] </ref>.) A crucial piece of information in deciding local action is where does the agent believe the other agents are going to move? This is modeled by storing the orientation of the prey's position with respect to the desired direction of movement of the agent.
Reference: [5] <author> Joseph Halpern and Yoram Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <journal> Journal of the ACM, </journal> <volume> 37(3) </volume> <pages> 549-587, </pages> <year> 1990. </year>
Reference-contexts: Even if agents are allowed to communicate, communication delays, improper use of language, different underlying assumptions, etc. can prevent agents from developing a shared common body of knowledge <ref> [5] </ref>. Given the above assumption about the initial model of other agents, an adaptive agent can possibly use various different learning methods to incrementally improve its model of other group members.
Reference: [6] <author> Kristian Hammond, Timothy Converse, and Mitchell Marks. </author> <title> Towards a theory of agency. </title> <booktitle> In Proceedings of the Workshop on Innovative Approaches to Planning, Scheduling and Control, </booktitle> <pages> pages 354-365, </pages> <address> San Diego, November 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Though researchers have used CBR in multiagent systems [14], little work has been done in learning cases in multiagent systems [3, 12]. 2 Case-Based Learning Case-based reasoning (CBR) <ref> [4, 6, 9] </ref> is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems. <p> Cases can be positive or negative <ref> [4, 6] </ref>. A positive case informs the agent what to do, i.e. it reorders the set of actions. A negative case can reorder the actions and/or delete actions from the set.
Reference: [7] <author> Thomas Haynes, Kit Lau, and Sandip Sen. </author> <title> Learning cases to compliment rules for conflict resolution in multiagent systems. </title> <editor> In Sandip Sen, editor, </editor> <booktitle> AAAI Symposium on Adaptation, Co-evolution and Learning in Multiagent Systems, </booktitle> <month> March </month> <year> 1996. </year>
Reference-contexts: There are two problems with this setup: the number of cases is too large, and the agents do not act independently. This case window and others are analyzed and rejected in <ref> [7] </ref>. Unless the entire world is used as a case, any narrowing of the case window is going to suffer from the above points of the "effective" case window presented above. The same case can represent several actual configurations of the domain being modeled. <p> The case window employed is that depicted in Figure 1. We have also identified two enhancements to break ties caused by the default rules employed in the MD metric: look ahead and least conflict <ref> [7] </ref>. Look ahead breaks ties in which two moves are equidistant via MD, the one which is potentially closer in two moves is selected. <p> Initially we were interested in the ability of predator behavioral rules to effectively capture the Still prey. We tested three behavioral strategies: MD the basic MD algorithm, MD-EDR the MD modified with the enhancements discussed in <ref> [7] </ref>, and MD-CBL which is MD-EDR utilizing a case base learned from training on 100 random simulations. The algorithms produced the following captures: MD - 3, MD-EBL - 46, and MD-CBL - 97. <p> We also conducted a set of experiments in which the prey used the Linear algorithm as its behavioral rule. The MD-CBL algorithm was trained on the Still prey. We trained on a Still prey because the Linear prey typically degrades to a Still prey <ref> [7] </ref>. We also present the results of training the MD-CBL on the Linear prey (MD-CBL ? ). The algorithms produced the following captures: MD - 2, MD-EDR - 20, MD-CBL - 54, and MD-CBL ? - 66.
Reference: [8] <author> Thomas Haynes and Sandip Sen. </author> <title> Evolving behavioral strategies in predators and prey. </title> <editor> In G. Wei and S. Sen, editors, </editor> <title> Adaptation and Learning in Multiagent Systems, </title> <publisher> LNAI. Springer Verlag, </publisher> <month> Spring </month> <year> 1996. </year>
Reference-contexts: If it has not, then A j 's behavioral rules have changed, and A i must update its model of A j . 4 3 Predator-Prey The predator-prey, or pursuit, domain has been widely used in distributed AI research as a testbed for investigating cooperation and conflict resolution <ref> [8, 10, 13] </ref>. Four predator try agents to capture a prey agent. In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective [8]. <p> Four predator try agents to capture a prey agent. In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective <ref> [8] </ref>. Simple greedy strategies for the predators have long been postulated to efficiently capture the prey [10]. The underlying assumption that the prey moves first, then the predators move in order simplifies the domain such that efficient capture is possible. <p> Relaxing the assumption leads to a more natural model in which all agents move at once. This model has been shown to create deadlock situations for simple prey algorithms of moving in a straight line (Linear) or even not moving at all (Still) <ref> [8] </ref>! Two possible solutions have been identified: allowing communication and adding state information. We investigate a learning system that utilizes past expectations to reduce deadlock situations. The predator agents have to capture the prey agent by blocking its orthogonal movement. <p> All ties are randomly broken. The MD strategy is more successful than the MN in capturing a Linear prey (22% vs 0%) <ref> [8] </ref>. Despite the fact that it can often block the forward motion of the prey, its success is still very low. The MD strategy is very susceptible to deadlock situations.
Reference: [9] <author> Janet L. Kolodner, </author> <title> editor. </title> <booktitle> Proceedings of a Workshop on Case-Based Reasoning (DARPA). </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: The multiagent case-based learning (MCBL) algorithm thus utilizes exceptions to a default ruleset, which describes the behavior of an agent. These exceptions form a case library. The agent does not reason with these cases, as in CBR <ref> [9] </ref>, but rather adapts an inaccurate individual model to improve performance. <p> Though researchers have used CBR in multiagent systems [14], little work has been done in learning cases in multiagent systems [3, 12]. 2 Case-Based Learning Case-based reasoning (CBR) <ref> [4, 6, 9] </ref> is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems. <p> The MD strategy is very susceptible to deadlock situations. How then should the agents manage conflict resolution? An answer can be found in the ways we as humans manage conflict resolution, with cases <ref> [9] </ref>. In the simplest sense, if predator 1 senses that if predator 2 is in its Northeast cell, and it has determined to move North, then if the other agent moves West there will be a conflict with predator 2.
Reference: [10] <author> Richard E. Korf. </author> <title> A simple solution to pursuit games. </title> <booktitle> In Working Papers of the 11th International Workshop on Distributed Artificial Intelligence, </booktitle> <pages> pages 183-194, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: If it has not, then A j 's behavioral rules have changed, and A i must update its model of A j . 4 3 Predator-Prey The predator-prey, or pursuit, domain has been widely used in distributed AI research as a testbed for investigating cooperation and conflict resolution <ref> [8, 10, 13] </ref>. Four predator try agents to capture a prey agent. In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective [8]. <p> In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective [8]. Simple greedy strategies for the predators have long been postulated to efficiently capture the prey <ref> [10] </ref>. The underlying assumption that the prey moves first, then the predators move in order simplifies the domain such that efficient capture is possible. Relaxing the assumption leads to a more natural model in which all agents move at once.
Reference: [11] <author> Victor R. Lesser. </author> <title> Multiagent systems: An emerging subdiscipline of AI. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(3) </volume> <pages> 340-342, </pages> <month> September </month> <year> 1995. </year>
Reference: [12] <author> M. V. Nagendra Prasad, Victor R. Lesser, and Susan Lander. </author> <title> Reasoning and retrieval in distributed case bases. Journal of Visual Communication and Image Representation, </title> <journal> Special Issue on Digital Libraries, </journal> <year> 1995. </year>
Reference-contexts: These exceptions form a case library. The agent does not reason with these cases, as in CBR [9], but rather adapts an inaccurate individual model to improve performance. Though researchers have used CBR in multiagent systems [14], little work has been done in learning cases in multiagent systems <ref> [3, 12] </ref>. 2 Case-Based Learning Case-based reasoning (CBR) [4, 6, 9] is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems.
Reference: [13] <author> Larry M. Stephens and Matthias B. Merx. </author> <title> The effect of agent control strategy on the performance of a DAI pursuit problem. </title> <booktitle> In Distributed AI Workshop, </booktitle> <month> October </month> <year> 1990. </year>
Reference-contexts: If it has not, then A j 's behavioral rules have changed, and A i must update its model of A j . 4 3 Predator-Prey The predator-prey, or pursuit, domain has been widely used in distributed AI research as a testbed for investigating cooperation and conflict resolution <ref> [8, 10, 13] </ref>. Four predator try agents to capture a prey agent. In spite of its apparent simplicity, it has been shown that the domain provides for complex interactions between agents and no hand-coded coordination strategy is very effective [8]. <p> We investigate a learning system that utilizes past expectations to reduce deadlock situations. The predator agents have to capture the prey agent by blocking its orthogonal movement. The game is typically played on a 30 by 30 grid world, which is toroidal <ref> [13] </ref>. The behavioral strategies of the predators use one of two distance metrics: Manhattan distance (MD) and max norm (MN). The MD metric is the sum of the differences of the x and y coordinates between two agents.
Reference: [14] <author> Katia Sycara. </author> <title> Planning for negotiation: A case-based approach. </title> <booktitle> In DARPA Knowledge-Based Planning Workshop, </booktitle> <pages> pages 11.1-11.10, </pages> <month> December </month> <year> 1987. </year> <month> 9 </month>
Reference-contexts: These exceptions form a case library. The agent does not reason with these cases, as in CBR [9], but rather adapts an inaccurate individual model to improve performance. Though researchers have used CBR in multiagent systems <ref> [14] </ref>, little work has been done in learning cases in multiagent systems [3, 12]. 2 Case-Based Learning Case-based reasoning (CBR) [4, 6, 9] is a model of this definition of intelligence and is a reasoning process for information retrieval and modification of solutions to problems.
References-found: 14

