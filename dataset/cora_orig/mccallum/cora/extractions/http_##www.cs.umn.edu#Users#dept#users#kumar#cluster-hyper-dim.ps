URL: http://www.cs.umn.edu/Users/dept/users/kumar/cluster-hyper-dim.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: fhan,karypis,kumar,mobasherg@cs.umn.edu  
Phone: (612) 626 7503  
Title: Clustering In A High-Dimensional Space Using Hypergraph Models  
Author: Eui-Hong (Sam) Han George Karypis Vipin Kumar Bamshad Mobasher 
Keyword: Clustering, data mining, association rules, hypergraph partitioning, dimensionality reduction, Principal Component Analysis, Latent Semantic Indexing.  
Address: 4-192 EECS Bldg., 200 Union St. SE Minneapolis, MN 55455, USA  
Affiliation: Department of Computer Science and Engineering/Army HPC Research Center University of Minnesota  
Abstract: Clustering of data in a large dimension space is of a great interest in many data mining applications. Most of the traditional algorithms such as K-means or AutoClass fail to produce meaningful clusters in such data sets even when they are used with well known dimensionality reduction techniques such as Principal Component Analysis and Latent Semantic Indexing. In this paper, we propose a method for clustering of data in a high dimensional space based on a hypergraph model. The hypergraph model maps the relationship present in the original data in high dimensional space into a hypergraph. A hyperedge represents a relationship (affinity) among subsets of data and the weight of the hyperedge reflects the strength of this affinity. A hypergraph partitioning algorithm is used to find a partitioning of the vertices such that the corresponding data items in each partition are highly related and the weight of the hyperedges cut by the partitioning is minimized. We present results of experiments on three different data sets: S&P500 stock data for the period of 1994-1996, protein coding data, and Web document data. Wherever applicable, we compared our results with those of AutoClass and K-means clustering algorithm on original data as well as on the reduced dimensionality data obtained via Principal Component Analysis or Latent Semantic Indexing scheme. These experiments demonstrate that our approach is applicable and effective in a wide range of domains. More specifically, our approach performed much better than traditional schemes for high dimensional data sets in terms of quality of clusters and runtime. Our approach was also able to filter out noise data from the clusters very effectively without compromising the quality of the clusters. fl This work was supported by NSF ASC-9634719, by Army Research Office contract DA/DAAH04-95-1-0538, by Army High Performance Computing Research Center cooperative agreement number DAAH04-95-2-0003/contract number DAAH04-95-C-0008, the content of which does not necessarily reflect the position or the policy of the government, and no official endorsement should be inferred. Additional support was provided by the IBM Partnership Award, and by the IBM SUR equipment grant. Access to computing facilities was provided by AHPCRC, Minnesota Supercomputer Institute. See http://www.cs.umn.edu/han for other related papers. 
Abstract-found: 1
Intro-found: 1
Reference: [AGM C 90] <author> Stephen Altschul, Warren Gish, Webb Miller, Eugene Myers, and David Lipman. </author> <title> Basic local alignment search tool. </title> <journal> Journal of Molecular Biology, </journal> <volume> 215 </volume> <pages> 403-410, </pages> <year> 1990. </year>
Reference-contexts: To rapidly determine the function of many previously unknown genes, biologists generate short segments of protein-coding sequences (called expressed sequence tags, or ESTs) and match each EST against the sequences of known proteins, using similarity matching algorithms <ref> [AGM C 90, PL88] </ref>. The result of this matching is a table showing similarities between ESTs and known proteins.
Reference: [AIS93] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of 1993 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Washington, D.C., </address> <year> 1993. </year>
Reference-contexts: The first step is to discover all the frequent item-sets (candidate sets that have support greater than the minimum support threshold specified). The second step is to generate association rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available.
Reference: [AMS C 96] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A.I. Verkamo. </author> <title> Fast discovery of association rules. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: We first present a brief overview of association rules that are used to model the information in a transaction database as a hypergraph, and then describe the hypergraph modeling and the clustering algorithm. 3.1 Association Rules Association rules capture the relationship of items that are present in a transaction <ref> [AMS C 96] </ref>. Let T be the set of transactions where each transaction is a subset of the item-set I . <p> In our experiments, we set this connectivity threshold to 0.1. 3.4 Computational Complexity The problem of finding association rules that meet a minimum support criterion has been shown to be linearly scalable with respect to the number of transactions <ref> [AMS C 96] </ref>. Highly efficient algorithms such as Apriori are able to quickly find association rules in very large databases provided the support is high enough.
Reference: [AS94] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 487-499, </pages> <address> Santiago, Chile, </address> <year> 1994. </year>
Reference-contexts: The relationship among the data points can be based on association rules <ref> [AS94] </ref> or correlations among a set of items [BMS97], or a distance metric defined for pairs of items. In the current version of our clustering algorithm, we use frequent item sets found using Apriori algorithm [AS94] to capture the relationship, and hypergraph partitioning algorithm HMETIS [KAKS97] to find partitions of highly <p> The relationship among the data points can be based on association rules <ref> [AS94] </ref> or correlations among a set of items [BMS97], or a distance metric defined for pairs of items. In the current version of our clustering algorithm, we use frequent item sets found using Apriori algorithm [AS94] to capture the relationship, and hypergraph partitioning algorithm HMETIS [KAKS97] to find partitions of highly related items in a hypergraph. Frequent item sets are derived by Apriori, as part of the association rules discovery, that meet a specified minimum support criteria. <p> In our current implementation, we use frequent item sets found by the association rule algorithm <ref> [AS94] </ref> as hyperedges. <p> The first step is to discover all the frequent item-sets (candidate sets that have support greater than the minimum support threshold specified). The second step is to generate association rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available. <p> The second step is to generate association rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets [AIS93, AS94, HS95]. Apriori algorithm presented in <ref> [AS94] </ref> is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97b] to use their large memory and processing power effectively. 3.2 Hypergraph Modeling A hypergraph <p> A number of algorithms have been developed for discovering frequent item-sets [AIS93, AS94, HS95]. Apriori algorithm presented in <ref> [AS94] </ref> is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97b] to use their large memory and processing power effectively. 3.2 Hypergraph Modeling A hypergraph [Ber76] H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ). <p> PCA, LSI, and K-means were performed for many values of the parameters used by these algorithms (such as number of dimensions and the number of clusters). We report only the best results found. In all of our experiments, we used a locally implemented version of Apriori algorithm <ref> [AS94] </ref> to find the association rules and construct the association-rule hypergraph. 4.1 S&P 500 Stock Data Our first data-set consists of the daily price movement of the stocks that belong to the S&P500 index.
Reference: [BDO95] <author> M.W. Berry, S.T. Dumais, and G.W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) <ref> [BDO95] </ref> is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. An inherent problem with dimensionality reduction is that in the presence of noise in the data, it may result in the degradation of the clustering results. <p> Another disadvantage of PCA is that its memory requirement is O.m 2 / and computational requirement is higher than O.m 2 / depending on the number of eigenvalues. These requirements can be unacceptably high for large m. Latent Semantic Indexing (LSI) <ref> [BDO95] </ref> is a dimensionality reduction technique extensively used in information retrieval domain and is similar in nature to PCA. Instead of finding the singular value decomposition of the covariance matrix, it finds the singular value decomposition of the original n fi m data.
Reference: [Ber76] <author> C. Berge. </author> <title> Graphs and Hypergraphs. </title> <publisher> American Elsevier, </publisher> <year> 1976. </year>
Reference-contexts: This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers [HKK97b] to use their large memory and processing power effectively. 3.2 Hypergraph Modeling A hypergraph <ref> [Ber76] </ref> H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ). A hypergraph is an extension of a graph in the sense that each hyperedge can connect more than two vertices.
Reference: [BMS97] <author> S. Brin, R. Motwani, and C. Silversteim. </author> <title> Beyond market baskets: Generalizing association rules to correlations. </title> <booktitle> In Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: The relationship among the data points can be based on association rules [AS94] or correlations among a set of items <ref> [BMS97] </ref>, or a distance metric defined for pairs of items. In the current version of our clustering algorithm, we use frequent item sets found using Apriori algorithm [AS94] to capture the relationship, and hypergraph partitioning algorithm HMETIS [KAKS97] to find partitions of highly related items in a hypergraph.
Reference: [CHY96] <author> M.S. Chen, J. Han, and P.S. Yu. </author> <title> Data mining: An overview from database perspective. </title> <journal> IEEE Transactions on Knowledge and Data Eng., </journal> <volume> 8(6) </volume> <pages> 866-883, </pages> <month> December </month> <year> 1996. </year> <month> 23 </month>
Reference-contexts: 1 Introduction Clustering in data mining <ref> [SAD C 93, CHY96] </ref> is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized [CHY96]. These discovered clusters are used to explain the characteristics of the data distribution. <p> 1 Introduction Clustering in data mining [SAD C 93, CHY96] is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized <ref> [CHY96] </ref>. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [CS96] <author> P. Cheeseman and J. Stutz. </author> <title> Baysian classification (autoclass): Theory and results. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smith, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> We present results on three different data sets: S&P500 stock data for the period of 1994-1996, protein coding data, and Web document data. Wherever applicable, we compared our results with those of AutoClass <ref> [CS96] </ref> and K-Means clustering algorithm on original data as well as on the reduced dimensionality data obtained via PCA or LSI. These experiments demonstrate that our approach is applicable and effective in a wide range of domains. <p> Section 2 contains review of related work. Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small. <p> Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning [SD90, Fis95], and data mining <ref> [NH94, CS96] </ref>. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small. These scheme tend to break down when the dimensionality of the data is very large for several reasons. <p> If the dimensionality is high, then the calculated mean values do not differ significantly from one cluster to the next. Hence the clustering based on these mean values does not always produce good clusters. Similarly, probabilistic methods such as Bayesian approach used in AutoClass <ref> [CS96] </ref>, do not perform well when the size of the feature space is much larger than the size of the sample set, which is common in many large dimensional data sets.
Reference: [DJ80] <author> R. Dubes and A.K. Jain. </author> <title> Clustering methodologies in exploratory data analysis. </title> <editor> In M.C. Yovits, editor, </editor> <booktitle> Advances in Computers. </booktitle> <publisher> Academic Press Inc., </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Section 2 contains review of related work. Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small.
Reference: [EG95] <author> T. Eiter and G. Gottlob. </author> <title> Identifying the minimal transversals of a hypergraph and related problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 24(6) </volume> <pages> 1278-1304, </pages> <year> 1995. </year>
Reference-contexts: The use of hypergraphs in data mining has been studied recently. For example in [GKMT97] it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem <ref> [EG95] </ref>. The clustering or grouping of association rules have been proposed in [TKR C 95], [LSW97] and [KA96]. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters.
Reference: [Fis95] <author> D. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <booktitle> In Proc. of the First Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 118-123, </pages> <address> Montreal, Quebec, </address> <year> 1995. </year>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning <ref> [SD90, Fis95] </ref>, and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small.
Reference: [Fra92] <author> W.B. Frakes. </author> <title> Stemming algorithms. </title> <editor> In W. B. Frakes and R. Baeza-Yate, editors, </editor> <booktitle> Information Retrieval Data Structures and Algorithms. </booktitle> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: The ability to find clusters of documents can be useful in filtering and categorizing a large collection of documents. 4.3.1 Clustering of Related Words We collected 87 documents from the Network for Excellence in Manufacturing (NEM Online) 2 site. From these documents we used a stemming algorithm <ref> [Fra92] </ref> to find the distinct stems that appear in them. There were a total of 5772 distinct word stems. This data set was represented by a table of size 5772 fi 87, such that each row corresponds to one of the words and each column corresponds to a document.
Reference: [GKMT97] <author> D. Gunopulos, R. Khardon, H. Mannila, and H. Toivonen. </author> <title> Data mining, hypergraph transversals, </title> <booktitle> and machine learning. In Proc. of Symposium on Principles of Database Systems, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: Furthermore many of these techniques have high computational complexity of O.n 2 / where n is the number of items being clustered. The use of hypergraphs in data mining has been studied recently. For example in <ref> [GKMT97] </ref> it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95]. The clustering or grouping of association rules have been proposed in [TKR C 95], [LSW97] and [KA96].
Reference: [HBG C 97] <author> E.H. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, B. Mobasher, and J. Moore. Webace: </author> <title> A web agent for document categorization and exploartion. </title> <type> Technical Report TR-97-049, </type> <institution> Department of Computer Science, University of Minnesota, M inneapolis, </institution> <year> 1997. </year>
Reference-contexts: a surprise, as the original dimension of the data is already small and the result was not very good with the original data. 4.3.2 Clustering of Related Documents For the problem of document clustering, an extensive comparison of our hypergraph-based method, AutoClass and distance-based hierarchical clustering algorithm was reported in <ref> [HBG C 97, MHB C 97] </ref>. These results show that hyper-graph based method consistently gave much better clusters than AutoClass and hierarchical clustering on many different data sets prepared from web documents. Here we discuss results of an experiment with 185 documents spread over 10 different document categories. <p> Here we discuss results of an experiment with 185 documents spread over 10 different document categories. Results of clustering these documents using our hypergraph-based method and Auto-Class are shown in Figure 2 and Figure 3 (taken from <ref> [HBG C 97] </ref>), respectively. As the reader can see, the hypergraph-based scheme provides much better clusters than AutoClass. We have also performed dimensionality reduction on the data using PCA and LSI, and then found clusters on the resulting data using the K-means algorithm. <p> For this type of domains, 22 in which continuous variables with higher values imply greater importance, we have developed a new algorithm called Min-Apriori [HKK97a] that operates directly on these continuous variables without discretizing them. In fact, for the clustering of documents in Section 4.3.2 (also results in <ref> [HBG C 97] </ref>), the hypergraph was constructed using Min-Apriori. Our current clustering algorithm relies on the hypergraph partitioning algorithm HMETIS to find a good k-way partitioning. As discussed in Section 3, even though HMETIS produces high quality partitions, it has some limitations.
Reference: [HHS92] <author> N. Harris, L. Hunter, and D. States. Mega-classification: </author> <title> Discovering motifs in massive datastreams. </title> <booktitle> In Proceedings of the Tenth International Conference on Artificial Intelligence (AAAI), </booktitle> <year> 1992. </year>
Reference-contexts: At this point, the biologists can focus on experimentally verifying the functions of the new EST represented by the matching EST clusters. Hence finding clusters of related ESTs is an important problem. Related work in this area can be found in <ref> [HHS92] </ref>, which reports clustering of the sequence-level building blocks of proteins by finding transitive closure of the pairwise probabilistic similarity judgments. To assess the utility of hypergraph-based clustering in this domain, we performed experiments with data provided by the authors of [NRS C 95, SCC C 95].
Reference: [HKK97a] <author> E.H. Han, G. Karypis, and V. Kumar. Min-apriori: </author> <title> An algorithm for finding association rules in data with continuous attributes. </title> <type> Technical Report TR-97-068, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: For this type of domains, 22 in which continuous variables with higher values imply greater importance, we have developed a new algorithm called Min-Apriori <ref> [HKK97a] </ref> that operates directly on these continuous variables without discretizing them. In fact, for the clustering of documents in Section 4.3.2 (also results in [HBG C 97]), the hypergraph was constructed using Min-Apriori. Our current clustering algorithm relies on the hypergraph partitioning algorithm HMETIS to find a good k-way partitioning.
Reference: [HKK97b] <author> E.H. Han, G. Karypis, and V. Kumar. </author> <title> Scalable parallel data mining for association rules. </title> <booktitle> In Proc. of 1997 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: Apriori algorithm presented in [AS94] is one of the most efficient algorithms available. This algorithm has been experimentally shown to be linearly scalable with respect to the size of the database [AS94], and can also be implemented on parallel computers <ref> [HKK97b] </ref> to use their large memory and processing power effectively. 3.2 Hypergraph Modeling A hypergraph [Ber76] H D .V ; E / consists of a set of vertices (V ) and a set of hyperedges (E ).
Reference: [HKKM97a] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs (position paper). </title> <booktitle> In Proc. of the Workshop on Research Issues on Data Mining and Knowledge Discovery, </booktitle> <pages> pages 9-13, </pages> <address> Tucson, Arizona, </address> <year> 1997. </year>
Reference-contexts: This is true for all the data sets used in our experiments. To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, HKKM97b] </ref>. We present results on three different data sets: S&P500 stock data for the period of 1994-1996, protein coding data, and Web document data.
Reference: [HKKM97b] <author> E.H. Han, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Clustering based on association rule hypergraphs. </title> <type> Technical Report TR-97-019, </type> <institution> Department of Computer Science, University of Minnesota, Minneapolis, </institution> <year> 1997. </year>
Reference-contexts: This is true for all the data sets used in our experiments. To test the applicability and robustness of our scheme, we evaluated it on a wide variety of data sets <ref> [HKKM97a, HKKM97b] </ref>. We present results on three different data sets: S&P500 stock data for the period of 1994-1996, protein coding data, and Web document data.
Reference: [HS95] <author> M. A. W. Houtsma and A. N. Swami. </author> <title> Set-oriented mining for association rules in relational databases. </title> <booktitle> In Proc. of the 11th Int'l Conf. on Data Eng., </booktitle> <pages> pages 25-33, </pages> <address> Taipei, Taiwan, </address> <year> 1995. </year>
Reference-contexts: The first step is to discover all the frequent item-sets (candidate sets that have support greater than the minimum support threshold specified). The second step is to generate association rules from these frequent item-sets. A number of algorithms have been developed for discovering frequent item-sets <ref> [AIS93, AS94, HS95] </ref>. Apriori algorithm presented in [AS94] is one of the most efficient algorithms available.
Reference: [Jac91] <author> J. E. Jackson. </author> <title> A User's Guide To Principal Components. </title> <publisher> John Wiley & Sons, </publisher> <year> 1991. </year>
Reference-contexts: One way of handling this problem is to reduce the dimensionality of the data while preserving the relationships among the data. Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) <ref> [Jac91] </ref>, Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) [BDO95] is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. <p> Hence, for large dimensional data sets, AutoClass' run time can be unacceptably high. A well-known and widely used technique for dimensionality reduction is Principal Component Analysis (PCA) <ref> [Jac91] </ref>. Consider a data set with n data items and m variables. PCA computes a covariance matrix of size m fi m, and then calculate the k leading eigenvectors of this covariance matrix. These k leading eigenvectors of this matrix are principal features of the data. <p> PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix. However, as noted in <ref> [Jac91] </ref>, different methods provide widely different guideline for k on the same data, and thus it can be dif 3 ficult to find the right number of dimension. The choice of small k can lose important features of the data. <p> Instead of finding the singular value decomposition of the covariance matrix, it finds the singular value decomposition of the original n fi m data. Since LSI does not require calculation of the covariance matrix, it has smaller memory and CPU requirements when n is less than m <ref> [Jac91] </ref>. The Kohonen Self-Organizing Feature Map (SOFM) [Koh88] is a scheme based on neural networks that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [JD88] <author> A.K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: One way of handling this problem is to reduce the dimensionality of the data while preserving the relationships among the data. Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) <ref> [JD88] </ref> and Kohonen Self-Organizing Feature Maps (SOFM) [Koh88] are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) [BDO95] is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. <p> These k leading eigenvectors of this matrix are principal features of the data. The original data is mapped along these new principal directions. This projected data has lower dimensions and can now be clustered using traditional clustering algorithms such as K-means <ref> [JD88] </ref>, Hierarchical clustering [JD88], or AutoClass. PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix. <p> These k leading eigenvectors of this matrix are principal features of the data. The original data is mapped along these new principal directions. This projected data has lower dimensions and can now be clustered using traditional clustering algorithms such as K-means <ref> [JD88] </ref>, Hierarchical clustering [JD88], or AutoClass. PCA provides several guidelines on how to determine the right number of dimension k for given data based on the proportion of variance explained or the characteristic roots of the covariance matrix. <p> SOFM does not provide a measure of how good the transformation is. Furthermore, on data sets with very large dimensions, convergence in the network training could be very slow. Multidimensional Scaling (MDS) <ref> [JD88] </ref> transforms original data into a smaller dimensional space while trying to preserve the rank ordering of the distances among data points. MDS does not provide any good guidelines on how to find the right number of dimension to capture all the variations in the original data. <p> The minimum spanning tree (MST) is constructed and the edges in the minimum spanning tree are proposed as the most interesting associations. Successive removal of edges from the minimum spanning tree is used to produce clusters of attributes <ref> [KA96, JD88] </ref>. Since the MST algorithm only focuses on finding a set of edges to connect all the vertices while minimizing the sum of these weights, the resulting minimum spanning tree does not have information on the density of edges connecting related vertices.
Reference: [KA96] <author> A.J. Knobbe and P.W. Adriaans. </author> <title> Analysing binary associations. </title> <booktitle> In Proc. of the Second Int'l Conference on Knowledge Discovery and Data Mining, </booktitle> <pages> pages 311-314, </pages> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: For example in [GKMT97] it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95]. The clustering or grouping of association rules have been proposed in [TKR C 95], [LSW97] and <ref> [KA96] </ref>. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations. <p> The clustering or grouping of association rules have been proposed in [TKR C 95], [LSW97] and <ref> [KA96] </ref>. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations. In this approach, the association graph is constructed by taking attributes in a database as vertex set and con 4 sidering binary associations among the items as edges in the graph. <p> The minimum spanning tree (MST) is constructed and the edges in the minimum spanning tree are proposed as the most interesting associations. Successive removal of edges from the minimum spanning tree is used to produce clusters of attributes <ref> [KA96, JD88] </ref>. Since the MST algorithm only focuses on finding a set of edges to connect all the vertices while minimizing the sum of these weights, the resulting minimum spanning tree does not have information on the density of edges connecting related vertices.
Reference: [KAKS97] <author> G. Karypis, R. Aggarwal, V. Kumar, and S. Shekhar. </author> <title> Multilevel hypergraph partitioning: Application in VLSI domain. </title> <booktitle> In Proceedings ACM/IEEE Design Automation Conference, </booktitle> <year> 1997. </year>
Reference-contexts: In the current version of our clustering algorithm, we use frequent item sets found using Apriori algorithm [AS94] to capture the relationship, and hypergraph partitioning algorithm HMETIS <ref> [KAKS97] </ref> to find partitions of highly related items in a hypergraph. Frequent item sets are derived by Apriori, as part of the association rules discovery, that meet a specified minimum support criteria. <p> We only need to find right amount of information from the association rules so that we can cluster the items. If the minimum support is too low, then we will capture too many minor relationships among data points, most of which could be noise. Furthermore, our hypergraph partitioning algorithm <ref> [KAKS97] </ref> can find good partitions only if the hypergraph is reasonably sparse. Thus by keeping the support threshold high enough, we can limit the computational complexity of the Apriori algorithm. <p> Thus by keeping the support threshold high enough, we can limit the computational complexity of the Apriori algorithm. Hypergraph partitioning is a well studied problem in the context of VLSI circuit partitioning and highly efficient algorithms such as HMETIS have been recently developed <ref> [KAKS97] </ref>. This algorithm can find very good bisection of circuits containing over 100,000 nodes in a few minutes on a workstation.
Reference: [Koh88] <author> T. Kohonen. </author> <title> Self-Organization and Associated Memory. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: One way of handling this problem is to reduce the dimensionality of the data while preserving the relationships among the data. Then the traditional clustering algorithms can be applied to this transformed data space. Principal Component Analysis (PCA) [Jac91], Multidimensional Scaling (MDS) [JD88] and Kohonen Self-Organizing Feature Maps (SOFM) <ref> [Koh88] </ref> are some of the commonly used techniques for dimensionality reduction. In addition, Latent Semantic Indexing (LSI) [BDO95] is a method frequently used in the information retrieval domain that employs a dimensionality reduction technique similar to PCA. <p> Since LSI does not require calculation of the covariance matrix, it has smaller memory and CPU requirements when n is less than m [Jac91]. The Kohonen Self-Organizing Feature Map (SOFM) <ref> [Koh88] </ref> is a scheme based on neural networks that projects high dimensional input data into a feature map of a smaller dimension such that the proximity relationships among input data are preserved.
Reference: [Lee81] <author> R.C.T. Lee. </author> <title> Clustering analysis and its applications. </title> <editor> In J.T. Toum, editor, </editor> <booktitle> Advances in Information Systems Science. </booktitle> <publisher> Plenum Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Section 2 contains review of related work. Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics <ref> [DJ80, Lee81, CS96] </ref>, machine learning [SD90, Fis95], and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small.
Reference: [LSW97] <author> B. Lent, A. Swami, and J. Widom. </author> <title> Clustering association rules. </title> <booktitle> In Proc. of the 13th Int'l Conf. on Data Eng., </booktitle> <address> Birmingham, U.K., </address> <year> 1997. </year>
Reference-contexts: For example in [GKMT97] it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95]. The clustering or grouping of association rules have been proposed in [TKR C 95], <ref> [LSW97] </ref> and [KA96]. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations. <p> The clustering or grouping of association rules have been proposed in [TKR C 95], <ref> [LSW97] </ref> and [KA96]. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations.
Reference: [MHB C 97] <author> J. Moore, E. Han, D. Boley, M. Gini, R. Gross, K. Hastings, G. Karypis, V. Kumar, and B. Mobasher. </author> <title> Web page categorization and feature selection using association rule and principal component clustering. </title> <booktitle> In 7th Workshop on Information Technologies and Systems, </booktitle> <month> Dec. </month> <year> 1997. </year>
Reference-contexts: a surprise, as the original dimension of the data is already small and the result was not very good with the original data. 4.3.2 Clustering of Related Documents For the problem of document clustering, an extensive comparison of our hypergraph-based method, AutoClass and distance-based hierarchical clustering algorithm was reported in <ref> [HBG C 97, MHB C 97] </ref>. These results show that hyper-graph based method consistently gave much better clusters than AutoClass and hierarchical clustering on many different data sets prepared from web documents. Here we discuss results of an experiment with 185 documents spread over 10 different document categories.
Reference: [NH94] <author> R. Ng and J. Han. </author> <title> Efficient and effective clustering method for spatial data mining. </title> <booktitle> In Proc. of the 20th VLDB Conference, </booktitle> <pages> pages 144-155, </pages> <address> Santiago, Chile, </address> <year> 1994. </year> <month> 24 </month>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning [SD90, Fis95], and data mining <ref> [NH94, CS96] </ref>. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small. These scheme tend to break down when the dimensionality of the data is very large for several reasons.
Reference: [NRS C 95] <author> T. Newman, E.F. Retzel, E. Shoop, E. Chi, and C. Somerville. </author> <title> Arabidopsis thaliana expressed sequence tags: Generation, analysis and dissemination. In Plant Genome III: </title> <booktitle> International Conference on the Status of Plant Genome Research, </booktitle> <address> San Diego, CA, </address> <year> 1995. </year>
Reference-contexts: To assess the utility of hypergraph-based clustering in this domain, we performed experiments with data provided by the authors of <ref> [NRS C 95, SCC C 95] </ref>. Our data set consists of a binary table of size 662 fi 11986. Each row of this table corresponds to an EST, and each column corresponds to a protein.
Reference: [PL88] <author> William R. Pearson and David J. Lipman. </author> <title> Improved tools for biological sequence comparison. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 85 </volume> <pages> 2444-2448, </pages> <year> 1988. </year>
Reference-contexts: To rapidly determine the function of many previously unknown genes, biologists generate short segments of protein-coding sequences (called expressed sequence tags, or ESTs) and match each EST against the sequences of known proteins, using similarity matching algorithms <ref> [AGM C 90, PL88] </ref>. The result of this matching is a table showing similarities between ESTs and known proteins.
Reference: [SA96] <author> R. Srikant and R. Agrawal. </author> <title> Mining quantitative association rules in large relational tables. </title> <booktitle> In Proc. of 1996 ACM-SIGMOD Int. Conf. on Management of Data, </booktitle> <address> Montreal, Quebec, </address> <year> 1996. </year>
Reference-contexts: Non-binary discrete variables can be handled by creating a binary variable for each possible value of the variable. Continuous variables can be handled once they have been discretized using the techniques similar to those presented in <ref> [SA96] </ref>. Association rules are particularly good in representing affinity among a set of items where each item is present only a small fraction of the transactions and presence of an item in a transaction is more significant than the absence of the item.
Reference: [SAD C 93] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> DBMS research at a crossroads: The vienna update. </title> <booktitle> In Proc. of the 19th VLDB Conference, </booktitle> <pages> pages 688-692, </pages> <address> Dublin, Ireland, </address> <year> 1993. </year>
Reference-contexts: 1 Introduction Clustering in data mining <ref> [SAD C 93, CHY96] </ref> is a discovery process that groups a set of data such that the intracluster similarity is maximized and the intercluster similarity is minimized [CHY96]. These discovered clusters are used to explain the characteristics of the data distribution.
Reference: [SCC C 95] <author> E. Shoop, E. Chi, J. Carlis, P. Bieganski, J. Riedl, N. Dalton, T. Newman, and E. Retzel. </author> <title> Implementation and testing of an automated EST processing and analysis system. </title> <editor> In Lawrence Hunter and Bruce Shriver, editors, </editor> <booktitle> Proceedings of the 28th Annual Hawaii International Conference on System Sciences, </booktitle> <volume> volume 5, </volume> <pages> pages 52-61. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1995. </year>
Reference-contexts: To assess the utility of hypergraph-based clustering in this domain, we performed experiments with data provided by the authors of <ref> [NRS C 95, SCC C 95] </ref>. Our data set consists of a binary table of size 662 fi 11986. Each row of this table corresponds to an EST, and each column corresponds to a protein.
Reference: [SD90] <author> J.W. Shavlik and T.G. Dietterich. </author> <booktitle> Readings in Machine Learning. </booktitle> <address> Morgan-Kaufman, </address> <year> 1990. </year>
Reference-contexts: Given a set of n data items with m variables, traditional clustering techniques <ref> [NH94, CS96, SD90, Fis95, DJ80, Lee81] </ref> group the data based on some measure of similarity or distance between data points. Most of these clustering algorithms are able to effectively cluster data when the dimensionality of the space (i.e., the number of variables) is relatively small. <p> Section 3 presents our clustering method based on hypergraph models. Section 4 presents the experimental results. Section 5 contains conclusion and directions for future work. 2 Related Work Clustering methods have been studied in several areas including statistics [DJ80, Lee81, CS96], machine learning <ref> [SD90, Fis95] </ref>, and data mining [NH94, CS96]. Most of the these approaches are based on either probability, distance or similarity measure. These schemes are effective when the dimensionality of the data is relatively small.
Reference: [TKR C 95] <author> H. Toivonen, M. Klemettinen, P. Ronkainen, K. Hatonen, and H. Mannila. </author> <title> Pruning and grouping discovered association rules. </title> <booktitle> In ECML-95 Workshop on Statistics, Machine Learning, and Knowledge Discovery in Databases,, </booktitle> <address> Heraklion, Greece, </address> <year> 1995. </year>
Reference-contexts: For example in [GKMT97] it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95]. The clustering or grouping of association rules have been proposed in <ref> [TKR C 95] </ref>, [LSW97] and [KA96]. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations. <p> For example in [GKMT97] it is shown that the problem of finding maximal elements in a lattice of patterns is closely related to the hypergraph transversal problem [EG95]. The clustering or grouping of association rules have been proposed in <ref> [TKR C 95] </ref>, [LSW97] and [KA96]. In [TKR C 95] and [LSW97], the focus is on finding clusters of association rules that have the same right hand side rather than on finding item clusters. In [KA96], a scheme is proposed to cluster database attributes based on binary associations.
Reference: [TSM85] <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <year> 1985. </year>
Reference-contexts: Similarly, probabilistic methods such as Bayesian approach used in AutoClass [CS96], do not perform well when the size of the feature space is much larger than the size of the sample set, which is common in many large dimensional data sets. Furthermore, the underlying expectation-maximization (EM) algorithm <ref> [TSM85] </ref> in AutoClass has the computational complexity of O.kd 2 n I / where k is the number of clusters, d is the number of attributes, n is the number of items to be clustered, and I is the average number of iterations of the EM algorithm.
Reference: [WP97] <author> Marilyn Wulfekuhler and Bill Punch. </author> <title> Finding salient features for personal web page categories. </title> <booktitle> In 6th WWW Conference, </booktitle> <address> Santa Clara, CA, </address> <year> 1997. </year> <month> 25 </month>
Reference-contexts: Word clusters can be used to find similar documents from the Web, or potentially serve as a description or label for classifying documents <ref> [WP97] </ref>. The ability to find clusters of documents can be useful in filtering and categorizing a large collection of documents. 4.3.1 Clustering of Related Words We collected 87 documents from the Network for Excellence in Manufacturing (NEM Online) 2 site.
References-found: 39

