URL: ftp://ftp.cag.lcs.mit.edu/pub/papers/piano-thesis.ps.Z
Refering-URL: http://www.cag.lcs.mit.edu/alewife/papers/
Root-URL: 
Title: Multigrain Shared Memory  
Author: by Donald Yeung Anant Agarwal Arthur C. Smith 
Degree: 1993 Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY at the  All rights reserved. Signature of Author:  Certified by:  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by:  Chairman, Departmental Graduate Committee  
Note: c 1998  
Date: February 1998  December 17, 1997  
Address: 1990  
Affiliation: B.S., Computer Systems Engineering Stanford University,  S.M., Electrical Engineering and Computer Science Massachusetts Institute of Technology,  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Massachusetts Institute of Technology.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: BACKGROUND consistency models <ref> [1, 25, 19] </ref>. We discuss relaxed memory consistency models below; a discussion and evaluation of all three techniques appears in [27].
Reference: [2] <author> Anant Agarwal, Johnathan Babb, David Chaiken, Godfrey D'Souza, Kirk Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Gino Maa, Ken Mackenzie, Dan Nussbaum, Mike Parkin, and Donald Yeung. Sparcle: </author> <title> Today's Micro for Tomorrow's Multiprocessor. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: At one corner of the mesh, an interface to the VME standard I/O bus allows the Alewife core to communicate with a host workstation. Each node in the Alewife compute partition consists of a SPARC integer core, called Sparcle <ref> [2] </ref>, an off-the-shelf SPARC family floating point unit, 64K-bytes of static RAM that forms an off-chip first-level processor cache, 8M-bytes of dynamic RAM, the Elko series 2-D mesh router chip from Caltech (EMRC) [24], and the CMMU, Communications and Memory Management Unit, which synthesizes a shared memory address space across all <p> In fast context switching, the processor switches between cached threads each time a thread suffers a cache miss to a remote memory module to hide the long latency of remote cache misses (the Sparcle processor can perform such a context switch in 14 cycles <ref> [2] </ref>). Another use of multiple hardware contexts is fast message handling. When a message arrives at a processor, it can process the handler associated with the message in a free hardware context, as long as one exists.
Reference: [3] <author> Anant Agarwal, Richard Simoni, John Hennessy, and Mark Horowitz. </author> <title> An Evaluation of Directory Schemes for Cache Coherence. </title> <booktitle> In Proceedings of the 15th International Symposium on Computer Architecture, </booktitle> <pages> pages 280-289, </pages> <address> Honolulu, HI, </address> <month> June </month> <year> 1988. </year> <note> IEEE. </note>
Reference-contexts: The cache coherence problem in shared memory multiprocessors is addressed by maintaining coherence on cached data using a cache-coherence protocol. DSMs typically employ cache-coherence protocols that are directory based <ref> [15, 32, 3, 14, 63] </ref>. Directory-based cache-coherence protocols maintain a directory entry for each cache block of data 26 CHAPTER 2. BACKGROUND in shared memory. <p> Special-purpose shared memory hardware on each node services shared memory requests made by the local processor, either by satisfying requests locally, or through communication with a memory module on a remote node via messages. Cache coherence is maintained using directory-based cache-coherence protocols, such as the one in <ref> [3] </ref>. Because communication and memory resources are distributed, the available communications bandwidth provided between processors and physical memory scales with the number of nodes in the system since each node has its own dedicated communications and memory interfaces. <p> THE ALEWIFE MULTIPROCESSOR 81 5.2.1 Hardware Cache-Coherent Shared Memory The cache-coherence protocol in Alewife is a single-writer write-invalidate protocol that supports a sequentially consistent memory model [45]. The protocol uses a directory scheme <ref> [3] </ref> to track outstanding cache block copies in the system. This directory scheme, called LimitLESS [15], is based on a fixed hardware-managed directory structure that supports 5 pointers, but extends the hardware-managed directory to accommodate more pointers by trapping the home node processor to handle directory overflow in software.
Reference: [4] <author> Jennifer M. Anderson and Monica S. Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: In the Water workload, we implemented a loop tiling and tile scheduling transformation. Similar transformations have been proposed in the compiler literature. In particular, loop tiling has been studied in [69] as a technique for improving data locality on parallel codes. Other work <ref> [4, 70] </ref> has looked at loop tiling as well, which is sometimes referred to as "strip-mine and interchange" and "unroll and jam" transformations. The technique is mature enough that many parallelizing compilers already perform loop tiling automatically.
Reference: [5] <author> Rajeev Barua, David Kranz, and Anant Agarwal. </author> <title> Addressing Partitioned Arrays in Distributed Memory Multiprocessors the Software Virtual Memory Approach. </title> <booktitle> In Proceedings of the Fifth MIT Student Workshop on Scalable Computing, </booktitle> <address> Wellesley, MA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Since the software shared memory layer in MGS relies heavily on virtual memory support, MGS must build virtual memory on top of existing Alewife shared memory mechanisms in software. There have been several schemes proposed in the literature for supporting virtual memory in software <ref> [57, 31, 5] </ref>. The general idea in software virtual memory (SVM) is that the compiler or the software system assumes responsibility for address translation and protection against unprivileged accesses in the absence of hardware TLBs.
Reference: [6] <author> Brian N. Bershad and Matthew J. Zekauskas. Midway: </author> <title> Shared Memory Parallel Programming with Entry Consistency for Distributed Memory Multiprocessors. </title> <type> CMU-CS 91-170, </type> <institution> Carnegie Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include [46, 44, 23, 38, 47]), and the software page-based DSM (examples include <ref> [48, 6, 13, 36, 37] </ref>). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. The primary difference between hardware and software DSMs is the level in the memory hierarchy on each DSM node where caching is performed.
Reference: [7] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway Distributed Shared Memory System. </title> <booktitle> In Proceedings of the 38th IEEE Computer Society International Conference, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: a decoupled approach 2 In addition to CRL, another system that supports the regions abstraction is described in [56]. 3 The use of application-specific data layout information by the shared memory layer is similar to what is supported under Entry Consistency as implemented by the Midway distributed shared memory system <ref> [7] </ref>. 4 The Alewife implementation of CRL only uses the efficient communication interfaces provided by Alewife. Alewife's hardware support for shared memory was purposefully bypassed to measure the discrepancy in performance when shared memory mechanisms are provided in software only. 8.2.
Reference: [8] <author> Angelos Bilas, Liviu Iftode, David Martin, and Jaswinder Pal Singh. </author> <title> Shared Virtual Memory Across SMP Nodes Using Automatic Update: Protocols and Performance. </title> <type> Technical Report TR-517-96, </type> <institution> Princeton University, </institution> <year> 1996. </year> <note> 197 198 BIBLIOGRAPHY </note>
Reference-contexts: Therefore, a coherence protocol, in software, is still necessary to enforce coherence, but the protocol can leverage the remote write mechanism as an efficient fine-grain messaging facility. Examples of SMP clusters that use fine-grain NI-based communication are [40] and <ref> [8] </ref>. [40] describes a 32-node cluster of 4-way DEC AlphaServer SMPs connected by the Memory Channel network. They implement two software coherence protocols, the Cashmere [62] protocol and the Treadmarks [36] protocol, and compare their performance. [8] describes a 16-node cluster of 4-way PC-based SMPs connected by the SHRIMP network interface. <p> Examples of SMP clusters that use fine-grain NI-based communication are [40] and <ref> [8] </ref>. [40] describes a 32-node cluster of 4-way DEC AlphaServer SMPs connected by the Memory Channel network. They implement two software coherence protocols, the Cashmere [62] protocol and the Treadmarks [36] protocol, and compare their performance. [8] describes a 16-node cluster of 4-way PC-based SMPs connected by the SHRIMP network interface. Coherence is provided by the AURC [30] protocol. A fine-grained SMP cluster with even less hardware than the intelligent NI approach described above is the Wisconsin COW [58].
Reference: [9] <author> David Black, Richard F. Rashid, David B. Golub, Charles R. Hill, and Robert V. Baron. </author> <title> Translation Lookaside Buffer Consistency: A Software Approach. </title> <booktitle> In Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 113-122, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1989. </year> <note> ACM. </note>
Reference-contexts: The system must ensure that any changes made to the mapping state in the page tables do not leave a stale copy of a mapping entry in any processor's TLB. Many approaches for providing mapping consistency (also known as TLB consistency) have been proposed <ref> [17, 64, 54, 9] </ref>. The solution used in MGS is closest to the one proposed in the PLATINUM system [17]. Each SSMP has a TLB directory that tracks the cached page table entries for all the pages resident in the SSMP's physical memory.
Reference: [10] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Fel-ten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <type> TR 437-93, </type> <institution> Princeton University, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: One approach to building fine-grained SMP clusters is to build hardware support for fine-grain transfers in the network interface (NI) to the SMP. The DEC Memory Channel [26] and the SHRIMP network interface <ref> [10] </ref> are examples of such "intelligent" NIs. These network interfaces support fine-grain communication between workstations by providing a remote write mechanism between host memories. Special transmit and receive regions can be defined to the network interface in each host's address space.
Reference: [11] <author> Nanette J. Boden, danny Cohen, Robert E. Felderman, Alan E. Kulawik, Charles L. Seitz, Jakov N. Seizovic, and Wen-King Su. Myrinet: </author> <title> A Gigabit-per-Second Local Area Network. </title> <journal> IEEE Micro, </journal> <volume> 15(1) </volume> <pages> 29-36, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Coherence is provided by the AURC [30] protocol. A fine-grained SMP cluster with even less hardware than the intelligent NI approach described above is the Wisconsin COW [58]. This system is comprised of 40 dual-processor Sun SPARCStation 20s connected across a Myrinet network <ref> [11] </ref>. The COW uses less hardware than the NI-based approaches because transfers between SMPs (in addition to coherence) are off-loaded onto one of the processors on the host SMP. Fine-grain transfers are enabled by a small piece of checking hardware, called the Typhoon-0 board.
Reference: [12] <author> Ralph Butler and Ewing Lusk. </author> <title> User's Guide to the p4 Programming System. </title> <type> Technical Report ANL-92/17, </type> <institution> Argonne National Laboratory, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Furthermore, instead of using the multigrain synchronization primitives described in Section 4.3 of Chapter 4, we use standard shared memory synchronization primitives, as provided by the P4 macro library 3 <ref> [12] </ref>. Therefore, the performance reported in the "SVM-Par" column is the performance on a hardware cache-coherent DSM (modulo software address translation), and is what we compare DSSMP performance against later in Section 6.3.2.
Reference: [13] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the 13th Annual Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include [46, 44, 23, 38, 47]), and the software page-based DSM (examples include <ref> [48, 6, 13, 36, 37] </ref>). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. The primary difference between hardware and software DSMs is the level in the memory hierarchy on each DSM node where caching is performed. <p> In such delayed update systems, no coherence operations are invoked on normal shared memory accesses. All updates occur locally and are buffered in the page cache in each node's physical memory. A data structure, known as the Delayed Update Queue <ref> [13] </ref>, records all the pages in the page cache that are dirty. At a release operation, the processor performing the release is stalled and coherence is initiated on all the pages listed in the Delayed Update Queue. The stalled processor resumes computation only after its updates have been consolidated. <p> Instead, it is necessary to determine what locations in each cache block have changed and to only update those locations at the home node. Protocols that support multiple writers perform twinning and diffing to enable the merge of multiple dirty cache blocks <ref> [13] </ref>. Any processor that wishes to write a page in its page cache must first make a copy of the page, known as a twin. All writes are performed on the original, leaving the twin unmodified. <p> The software layer in MGS that supports page-based shared memory borrows many mechanisms from conventional software DSM systems that run on clusters of uniprocessor workstations. Our design of software page-based shared memory is closest to the Munin system <ref> [13] </ref>. In particular, we support a release consistent (RC) memory consistency model using the Delayed Update Queue structure proposed by Munin (see Section 4.2.3). In addition, we support multiple writers via twinning and diffing, another technique first proposed by Munin. <p> The DUQ is the same structure that appears in the Munin system <ref> [13] </ref>. The Local Client adds an entry to its DUQ each time a page fault for a write access, or a TLB fault that results in an upgrade occurs. There are two instances in which entries are removed from the DUQ. <p> MGS heavily leverages the body of work on software DSMs since MGS uses the same mechanisms proposed for traditional software DSM systems to provide shared memory across SSMPs. The system that has had the most impact on the design of MGS by far is Munin <ref> [13] </ref>. MGS borrows directly from Munin the use of delayed coherence to minimize communication using the Delayed Update Queue structure, and the implementation of multiple writers to reduce false sharing via twinning and diffing (see Section 2.2.2 of Chapter 2 for details on these mechanisms).
Reference: [14] <author> Lucien M. Censier and Paul Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-27(12):1112-1118, </volume> <month> December </month> <year> 1978. </year>
Reference-contexts: The cache coherence problem in shared memory multiprocessors is addressed by maintaining coherence on cached data using a cache-coherence protocol. DSMs typically employ cache-coherence protocols that are directory based <ref> [15, 32, 3, 14, 63] </ref>. Directory-based cache-coherence protocols maintain a directory entry for each cache block of data 26 CHAPTER 2. BACKGROUND in shared memory.
Reference: [15] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: The cache coherence problem in shared memory multiprocessors is addressed by maintaining coherence on cached data using a cache-coherence protocol. DSMs typically employ cache-coherence protocols that are directory based <ref> [15, 32, 3, 14, 63] </ref>. Directory-based cache-coherence protocols maintain a directory entry for each cache block of data 26 CHAPTER 2. BACKGROUND in shared memory. <p> THE ALEWIFE MULTIPROCESSOR 81 5.2.1 Hardware Cache-Coherent Shared Memory The cache-coherence protocol in Alewife is a single-writer write-invalidate protocol that supports a sequentially consistent memory model [45]. The protocol uses a directory scheme [3] to track outstanding cache block copies in the system. This directory scheme, called LimitLESS <ref> [15] </ref>, is based on a fixed hardware-managed directory structure that supports 5 pointers, but extends the hardware-managed directory to accommodate more pointers by trapping the home node processor to handle directory overflow in software.
Reference: [16] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Raja-mony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Of all the related work covered in this chapter, these systems have the most in common with MGS. Coupling hardware cache-coherent shared memory with software page-based shared memory was first suggested in <ref> [16] </ref>. Their work investigates the performance of a system with up to 64 processors built using 8-way SMPs connected across an ATM network. Software shared memory between SMPs is provided using the LRC protocol. The evaluation is simulation based in which a very simple machine model was employed.
Reference: [17] <author> Alan L. Cox and Robert J. Fowler. </author> <title> The Implementation of a Coherent Memory Abstraction on a NUMA Multiprocessor: Experiences with PLATINUM. </title> <type> Technical Report 263, </type> <institution> University of Rochester Computer Science Department, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: The system must ensure that any changes made to the mapping state in the page tables do not leave a stale copy of a mapping entry in any processor's TLB. Many approaches for providing mapping consistency (also known as TLB consistency) have been proposed <ref> [17, 64, 54, 9] </ref>. The solution used in MGS is closest to the one proposed in the PLATINUM system [17]. Each SSMP has a TLB directory that tracks the cached page table entries for all the pages resident in the SSMP's physical memory. <p> Many approaches for providing mapping consistency (also known as TLB consistency) have been proposed [17, 64, 54, 9]. The solution used in MGS is closest to the one proposed in the PLATINUM system <ref> [17] </ref>. Each SSMP has a TLB directory that tracks the cached page table entries for all the pages resident in the SSMP's physical memory. The TLB directory is updated whenever a TLB fill is performed by marking the ID of the processor caching the mapping entry.
Reference: [18] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(6) </volume> <pages> 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: A shared memory abstraction is synthesized across the physically distributed memories via shared memory modules, one per physical memory and controller, that communicate using point-to-point messages across a switched interconnection network, such as those discussed in <ref> [18] </ref>. Figure 2.2 illustrates these components that make up the DSM architecture. In the figure, each processor, its local memory, and its local shared memory module together form a DSM node. Synthesis of a shared memory abstraction in a DSM occurs in the following manner.
Reference: [19] <author> M. Dubois, C. Scheurich, and F. Briggs. </author> <title> Memory Access Buffering in Multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year> <note> BIBLIOGRAPHY 199 </note>
Reference-contexts: BACKGROUND consistency models <ref> [1, 25, 19] </ref>. We discuss relaxed memory consistency models below; a discussion and evaluation of all three techniques appears in [27].
Reference: [20] <author> S. J. Eggers and R. H. Katz. </author> <title> Evaluating the Performance of Four Snooping Cache Coherency Protocols. </title> <booktitle> In Proceedings of the 16th International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1989. </year> <note> IEEE. </note>
Reference-contexts: The SMP is a popular cache-coherent architecture in which physical memory is centralized. Tight coupling is achieved by connecting processors via a bus. Each processor is responsible for maintaining coherence on data in its own hardware cache by snooping shared memory transactions that are broadcasted across the bus interconnect <ref> [20] </ref>. As a fine-grain architecture, the SMP offers the advantage of simplicity, due in large part to the existence of a broadcast primitive made possible by the bus interconnect. Its simplicity has contributed to its success as a cost-effective architecture.
Reference: [21] <author> Susan J. Eggers and Tor E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <pages> pages I-377-I-381, </pages> <year> 1991. </year>
Reference-contexts: Otherwise, delayed updates can only be applied to pages with a single writer and zero or more readers. The multiple-writer case is important because of false sharing. False sharing occurs when two shared memory accesses with distinct addresses fall on the same cache block <ref> [21] </ref>. The coherence protocol is fooled into believing that the accesses conflict because it treats each cache block as an indivisible unit of data. Therefore, communication results to maintain coherence even though the coherence is unnecessary. False sharing becomes more severe as the size of the cache block increases.
Reference: [22] <author> Andrew Erlichson, Neal Nuckolls, Greg Chesson, and John Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Proceedings of the Seventh ACM Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210-221, </pages> <address> Cambridge, Massachusetts, </address> <month> October </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: We call this approach physical clustering because the cluster boundaries that partition the DSSMP are fixed in hardware. All the systems currently proposed in the literature that resemble MGS <ref> [22, 50] </ref> (and that we are aware of) use physical clustering. 1 More justification will be given for this claim when we discuss our performance framework in Section 6.2 of Chapter 6. 77 78 CHAPTER 5. <p> Contention through the hardware network interface between processors on an SSMP and the inter-SSMP network can be a problem especially on SSMPs where the amount of bandwidth available through the network interface is fixed, regardless of the SSMP node size <ref> [22] </ref> (see Section 8.2 of Chapter 8). In such SSMP architectures, the fixed network interface bandwidth resource becomes a bottleneck as the SSMP node size is scaled since larger SSMP nodes generally place a greater demand on messaging into and out of the node. <p> The simulation treats all the processors in the same SMP as a single DSM node. Therefore, none of the design nor performance issues associated with integrating hardware and software shared memory were explored. The system with the greatest similarity to MGS is SoftFLASH from Stanford <ref> [22] </ref>. SoftFLASH is a multigrain shared memory system implemented on a cluster of SGI Challenge SMPs connected across a switched HIPPI high-speed LAN. SoftFLASH implements a page-based version of the FLASH multiprocessor [44] coherence protocol; the page-based software DSM layer is integrated into the Irix 6.2 Unix kernel.
Reference: [23] <author> Anant Agarwal et. al. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include <ref> [46, 44, 23, 38, 47] </ref>), and the software page-based DSM (examples include [48, 6, 13, 36, 37]). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. <p> IMPLEMENTATION 5.2 The Alewife Multiprocessor In this section, we discuss the hardware platform for our prototype of the MGS system, the Alewife multiprocessor <ref> [23] </ref>. The focus will be on aspects of the Alewife architecture, particularly those that impact the implementation of MGS. Details of how the implementation of MGS is carried out are deferred to Section 5.3. memory multiprocessor that supports the shared memory abstraction and cache coherence in hardware. <p> All measurements are in cycles. overheads for software address translation, and overheads in page-based software shared memory. Table 6.1 reports overheads associated with cache-coherent shared memory by enumerating several types of cache-miss penalties. These numbers reflect shared memory performance provided on a single Alewife machine. These data appear in <ref> [23] </ref>, and have been reprinted here. Cache-miss penalties are reported for the two types of shared memory accesses, loads and stores, as indicated by the column labeled "Type" in Table 6.1.
Reference: [24] <author> Charles M. Flaig. </author> <title> Vlsi mesh routing systems. </title> <type> Master's thesis, </type> <institution> California Institute of Technology, Department of Computer Science, California Institute of Technology, </institution> <address> 256-80, Pasadena, CA 91125, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Each node in the Alewife compute partition consists of a SPARC integer core, called Sparcle [2], an off-the-shelf SPARC family floating point unit, 64K-bytes of static RAM that forms an off-chip first-level processor cache, 8M-bytes of dynamic RAM, the Elko series 2-D mesh router chip from Caltech (EMRC) <ref> [24] </ref>, and the CMMU, Communications and Memory Management Unit, which synthesizes a shared memory address space across all the distributed memories, and implements a cache-coherence protocol. All chips on the Alewife nodes are clocked at 20 MHz 3 .
Reference: [25] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hen-nessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings 17th Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> June </month> <year> 1990. </year> <note> IEEE. </note>
Reference-contexts: BACKGROUND consistency models <ref> [1, 25, 19] </ref>. We discuss relaxed memory consistency models below; a discussion and evaluation of all three techniques appears in [27]. <p> An example of such a relaxed memory model is release consistency (RC) <ref> [25] </ref>. Programs written assuming an RC memory consistency model must include special annotations, known as releases and acquires, that specify to the memory system when coherence is necessary. <p> Fortunately, this does not cause any data consistency problems due to the relaxed access ordering guarantees provided in the Release Consistency memory model. In RC, shared memory access ordering needs to be guaranteed only for special accesses <ref> [25] </ref> (acquires and releases). The ordering on acquires and releases are ensured explicitly by proper synchronization at the application level. The ordering of all other shared memory accesses can be arbitrary without violating the semantics of Release Consistency. <p> This does not, however, complicate the analysis and will be discussed later in the section. 150 CHAPTER 7. ANALYSIS viewed as a summary of the data dependence information. The dotted lines indicate data dependences, and the solid line indicates a synchronization dependence. identify release points <ref> [25] </ref>. Consequently, there is no mystery behind when coherence happens. Analysis can identify all coherence points by simply looking at the application's source code.
Reference: [26] <author> R. Gillett. </author> <title> Memory Channel: An Optimized Cluster Interconnect. </title> <journal> IEEE Micro, </journal> <volume> 16(2), </volume> <month> February </month> <year> 1996. </year>
Reference-contexts: One approach to building fine-grained SMP clusters is to build hardware support for fine-grain transfers in the network interface (NI) to the SMP. The DEC Memory Channel <ref> [26] </ref> and the SHRIMP network interface [10] are examples of such "intelligent" NIs. These network interfaces support fine-grain communication between workstations by providing a remote write mechanism between host memories. Special transmit and receive regions can be defined to the network interface in each host's address space.
Reference: [27] <author> Anoop Gupta, John Hennessy, Kourosh Gharachorloo, Todd Mowry, and Wolf-Dietrich Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: BACKGROUND consistency models [1, 25, 19]. We discuss relaxed memory consistency models below; a discussion and evaluation of all three techniques appears in <ref> [27] </ref>. Relaxed Memory Consistency The memory consistency model supported by a shared memory multiprocessor determines the ordering in which individual processors view the memory operations performed by other processors in the system. The strongest form of memory consistency is sequential consistency [45]. <p> In addition to fast interrupts, Sparcle provides another mechanism that helps reduce the latency of message handling, multiple hardware contexts. Multiple hardware contexts allow Sparcle to cache up to 4 threads of execution inside the processor. One use of multiple hardware contexts is fast context switching for latency tolerance <ref> [27] </ref>. In fast context switching, the processor switches between cached threads each time a thread suffers a cache miss to a remote memory module to hide the long latency of remote cache misses (the Sparcle processor can perform such a context switch in 14 cycles [2]).
Reference: [28] <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1989. </year>
Reference-contexts: And coherence misses, the most complex miss type to analyze, arise due to the interleaving of references in the local processor's reference 3 We assume the reader is familiar with these terms. We refer the interested reader to <ref> [28] </ref> for a detailed explanation. 148 CHAPTER 7. ANALYSIS stream with conflicting references performed by other processors in the system. Cache miss behavior, and thus communication behavior, in hardware DSMs is dictated by a complex interaction between different processor reference streams, and the architecture of the local memory hierarchy.
Reference: [29] <author> Jerry Huck and Jim Hays. </author> <title> Architectural Support for Translation Table Management in Large Address Space Machines. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 39-50, </pages> <year> 1993. </year>
Reference-contexts: Both the Local-Client and Remote-Client machines access the page table. The Local Client reads the table during TLB faults, and modifies the table during page faults. The Remote Client modifies the table during invalidation requests. Many different page table organizations are possible <ref> [29] </ref>. The organization we choose for MGS is a bit unconventional because of particular constraints imposed by our hardware platform (see Section 5.2 for more details). However, in general, the page table used here is no different from any page table one would find in a normal operating system.
Reference: [30] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In Proceedings of the 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: They implement two software coherence protocols, the Cashmere [62] protocol and the Treadmarks [36] protocol, and compare their performance. [8] describes a 16-node cluster of 4-way PC-based SMPs connected by the SHRIMP network interface. Coherence is provided by the AURC <ref> [30] </ref> protocol. A fine-grained SMP cluster with even less hardware than the intelligent NI approach described above is the Wisconsin COW [58]. This system is comprised of 40 dual-processor Sun SPARCStation 20s connected across a Myrinet network [11].
Reference: [31] <author> B. L. Jacob and T. N. Mudge. </author> <title> Software-Manged Address Translation. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Computer Architecture, </booktitle> <address> San Antonio, TX, </address> <month> February </month> <year> 1997. </year> <note> IEEE. 200 BIBLIOGRAPHY </note>
Reference-contexts: Since the software shared memory layer in MGS relies heavily on virtual memory support, MGS must build virtual memory on top of existing Alewife shared memory mechanisms in software. There have been several schemes proposed in the literature for supporting virtual memory in software <ref> [57, 31, 5] </ref>. The general idea in software virtual memory (SVM) is that the compiler or the software system assumes responsibility for address translation and protection against unprivileged accesses in the absence of hardware TLBs.
Reference: [32] <author> David V. James, Anthony T. Laundrie, Stein Gjessing, and Gurindar S. Sohi. </author> <title> Distributed-Directory Scheme: Scalable Coherent Interface. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 74-77, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The cache coherence problem in shared memory multiprocessors is addressed by maintaining coherence on cached data using a cache-coherence protocol. DSMs typically employ cache-coherence protocols that are directory based <ref> [15, 32, 3, 14, 63] </ref>. Directory-based cache-coherence protocols maintain a directory entry for each cache block of data 26 CHAPTER 2. BACKGROUND in shared memory.
Reference: [33] <author> Tor E. Jeremiassen and Susan J. Eggers. </author> <title> Reducing False Sharing on Shared Memory Multiprocessors through Compile Time Data Transformations. </title> <booktitle> In Proceedings of the Sixth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming. ACM, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: The second transformation for Barnes-Hut addresses the false sharing problems on the octree nodes. False sharing on shared memory systems has been studied quite extensively. Most notably, the work in <ref> [33] </ref> proposes analysis techniques for a compiler that automatically detects and eliminates several types of false sharing access patterns.
Reference: [34] <author> Xiaohu Daniel Jiang. </author> <title> A scalable parallel inter-cluster communication system for clustered multiprocessors. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Department of Computer Science and Electrical Engineering, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: This perimeter increases as the square root of the number of processors in the virtual SSMP node because of the two-dimensional nature of Alewife's mesh network. Being able to scale inter-SSMP bandwidth with SSMP node size requires scalable communications interfaces for SSMPs. In <ref> [34] </ref>, the design of such a scalable communications interface, based on standard Internet protocols, is proposed and implemented with the MGS system. <p> MGS also places a lower demand on inter-SSMP communication bandwidth since it uses a smaller page size, 1 K-bytes compared to the 16 K-byte pages used in SoftFLASH. The observations on inter-node bandwidth made in SoftFLASH point to the importance of providing scalable communications interfaces for DSSMPs. <ref> [34] </ref> proposes a scalable inter-SSMP communication interface for MGS that uses standard Internet protocols, and studies the effects of contention in the communication processors that run the communication protocol stacks. Finally, the effects of false sharing are greater in SoftFLASH than in MGS for two reasons.
Reference: [35] <author> Kirk Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles, </booktitle> <address> Copper Mountain, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: In addition to the mixed hardware and software shared memory systems described thus far, there have been all-software systems that support multiple coherence granular-ities as well. Two examples of such software-only multigrain systems are CRL <ref> [35] </ref> and Shasta [57]. We first describe CRL, then Shasta.
Reference: [36] <author> Pete Keleher, Alan Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> Proceedings of the 1994 Usenix Conference, </booktitle> <pages> pages 115-131, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include [46, 44, 23, 38, 47]), and the software page-based DSM (examples include <ref> [48, 6, 13, 36, 37] </ref>). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. The primary difference between hardware and software DSMs is the level in the memory hierarchy on each DSM node where caching is performed. <p> Better software DSM performance is possible using a slightly more complex implementation of release consistency known as Lazy Release Consistency (LRC) [37] which has been implemented in the Treadmarks system <ref> [36] </ref>. LRC reduces the number of inter-node messages by further delaying when coherence happens. In Munin (and thus MGS), coherence is enforced eagerly at every release point where data is produced. Enforcing coherence at the producer is pessimistic and may result in unnecessary communication. <p> Examples of SMP clusters that use fine-grain NI-based communication are [40] and [8]. [40] describes a 32-node cluster of 4-way DEC AlphaServer SMPs connected by the Memory Channel network. They implement two software coherence protocols, the Cashmere [62] protocol and the Treadmarks <ref> [36] </ref> protocol, and compare their performance. [8] describes a 16-node cluster of 4-way PC-based SMPs connected by the SHRIMP network interface. Coherence is provided by the AURC [30] protocol. A fine-grained SMP cluster with even less hardware than the intelligent NI approach described above is the Wisconsin COW [58].
Reference: [37] <author> Pete Keleher, Alan L. Cox, and Willy Zwaenepoel. </author> <title> Lazy Release Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include [46, 44, 23, 38, 47]), and the software page-based DSM (examples include <ref> [48, 6, 13, 36, 37] </ref>). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. The primary difference between hardware and software DSMs is the level in the memory hierarchy on each DSM node where caching is performed. <p> MGS borrows the delayed coherence and multiple-writer mechanisms from Munin because they provide good performance and because their implementation is relatively straight forward. Better software DSM performance is possible using a slightly more complex implementation of release consistency known as Lazy Release Consistency (LRC) <ref> [37] </ref> which has been implemented in the Treadmarks system [36]. LRC reduces the number of inter-node messages by further delaying when coherence happens. In Munin (and thus MGS), coherence is enforced eagerly at every release point where data is produced.
Reference: [38] <institution> Kendall Square Research, Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <institution> Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include <ref> [46, 44, 23, 38, 47] </ref>), and the software page-based DSM (examples include [48, 6, 13, 36, 37]). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures.
Reference: [39] <author> Leonard Kleinrock. </author> <title> Queueing Systems, volume I. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The queuing network used to model lock contention is shown in Figure 7.5. It consists of two queues, an M=M=1 queue (Queue 0) and an M=M=s queue (Queue 1) where s = P , the total number of processors in the system 8 (see <ref> [39] </ref> for more details on these queues and their analysis). In this queuing network, customers represent processors, and the queues represent two different processor activities. A customer entering Queue 0 signifies a processor trying to acquire the lock.
Reference: [40] <author> Leonidas Kontothanassis, Galen Hunt, Robert Stets, Nikolaos Hardavellas, Micha lCierniak, Srinivasan Parthasarathy, Wagner Meira, Jr., Sandhya Dwarkadas, and Michael Scott. </author> <title> VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 157-169, </pages> <address> Denver, CO, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Therefore, a coherence protocol, in software, is still necessary to enforce coherence, but the protocol can leverage the remote write mechanism as an efficient fine-grain messaging facility. Examples of SMP clusters that use fine-grain NI-based communication are <ref> [40] </ref> and [8]. [40] describes a 32-node cluster of 4-way DEC AlphaServer SMPs connected by the Memory Channel network. <p> Therefore, a coherence protocol, in software, is still necessary to enforce coherence, but the protocol can leverage the remote write mechanism as an efficient fine-grain messaging facility. Examples of SMP clusters that use fine-grain NI-based communication are <ref> [40] </ref> and [8]. [40] describes a 32-node cluster of 4-way DEC AlphaServer SMPs connected by the Memory Channel network. They implement two software coherence protocols, the Cashmere [62] protocol and the Treadmarks [36] protocol, and compare their performance. [8] describes a 16-node cluster of 4-way PC-based SMPs connected by the SHRIMP network interface.
Reference: [41] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory: Early Experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> New York, </address> <month> May </month> <year> 1993. </year> <note> ACM. </note>
Reference-contexts: It has been argued that shared memory is a desirable programming model since communication happens implicitly each time two threads access the same memory location. This is in contrast to a message passing programming model where the responsibility of managing communication is placed explicitly on the programmer <ref> [41] </ref>. Shared memory multiprocessors implement the shared memory programming model by supporting the shared memory abstraction directly in the system architecture. An example shared memory architecture for which the design faithfully resembles the programming model is the Symmetric Multiprocessor (SMP).
Reference: [42] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Supercomputing Conference, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> BIBLIOGRAPHY 201 </note>
Reference-contexts: Because the message provides the handler address, the processor receiving the message can dispatch the message handler immediately without expending effort to figure out which message handler to execute. Finally, the messaging interface on Alewife supports DMA transfers in hardware <ref> [42] </ref>. DMA allows the movement of bulk data through the messaging interface without burdening the processor with data movement overhead. This allows Alewife to support large messages very efficiently. DMA data in messages are locally coherent.
Reference: [43] <author> Kiyoshi Kurihara, David Chaiken, and Anant Agarwal. </author> <title> Latency Tolerance through Multithreading in Large-Scale Multiprocessors. </title> <booktitle> In Proceedings International Symposium on Shared Memory Multiprocessing, </booktitle> <address> Japan, April 1991. </address> <publisher> IPS Press. </publisher>
Reference-contexts: If the processor performing such a write is stalled for the duration of the invalidation (s), significant cache miss stall can be introduced, thus degrading performance. Many techniques have been proposed to address such cache miss stall overhead, such as software-controlled prefetching [51], block multithreading <ref> [43, 68] </ref>, and relaxed memory 1 In the worst case, there could be P 1 outstanding copies, where P is the number of DSM nodes. 28 CHAPTER 2. BACKGROUND consistency models [1, 25, 19].
Reference: [44] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year> <note> IEEE. </note>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include <ref> [46, 44, 23, 38, 47] </ref>), and the software page-based DSM (examples include [48, 6, 13, 36, 37]). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. <p> The system with the greatest similarity to MGS is SoftFLASH from Stanford [22]. SoftFLASH is a multigrain shared memory system implemented on a cluster of SGI Challenge SMPs connected across a switched HIPPI high-speed LAN. SoftFLASH implements a page-based version of the FLASH multiprocessor <ref> [44] </ref> coherence protocol; the page-based software DSM layer is integrated into the Irix 6.2 Unix kernel. Several differences distinguish the SoftFLASH work from our work.
Reference: [45] <author> Leslie Lamport. </author> <title> How to Make a Multiprocessor Computer That Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9), </volume> <month> Septem-ber </month> <year> 1979. </year>
Reference-contexts: Relaxed Memory Consistency The memory consistency model supported by a shared memory multiprocessor determines the ordering in which individual processors view the memory operations performed by other processors in the system. The strongest form of memory consistency is sequential consistency <ref> [45] </ref>. Sequential consistency states that a total order can be defined across all shared memory operations, and furthermore, all processors see the same total order. Multiprocessors that stall a processor on each shared memory write until all outstanding cache copies are invalidated support sequential consistency. <p> THE ALEWIFE MULTIPROCESSOR 81 5.2.1 Hardware Cache-Coherent Shared Memory The cache-coherence protocol in Alewife is a single-writer write-invalidate protocol that supports a sequentially consistent memory model <ref> [45] </ref>. The protocol uses a directory scheme [3] to track outstanding cache block copies in the system.
Reference: [46] <author> James Laudon and Daniel Lenoski. </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 241-251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include <ref> [46, 44, 23, 38, 47] </ref>), and the software page-based DSM (examples include [48, 6, 13, 36, 37]). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. <p> An example of such a system might be the SGI Origin <ref> [46] </ref> in a small-scale configuration. 42 CHAPTER 3. THE MULTIGRAIN APPROACH As Figure 3.3 shows, DSSMPs have two types of networks that form the communication substrate: an internal network and an external network. The internal network provides interconnection between processors within each SSMP.
Reference: [47] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include <ref> [46, 44, 23, 38, 47] </ref>), and the software page-based DSM (examples include [48, 6, 13, 36, 37]). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures.
Reference: [48] <author> Kai Li and Paul Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: In this section, we take a closer look at two specific DSM implementations, the hardware cache-coherent DSM (examples include [46, 44, 23, 38, 47]), and the software page-based DSM (examples include <ref> [48, 6, 13, 36, 37] </ref>). We will focus on how the implementation of distributed shared memory and cache coherence differ on these two architectures. The primary difference between hardware and software DSMs is the level in the memory hierarchy on each DSM node where caching is performed. <p> Finally, we discuss other distributed shared memory systems that leverage SMPs as DSM nodes. 8.1 Page-Based Shared Memory The initial idea to implement shared memory using a shared virtual address space thus enabling the construction of DSMs using commodity nodes originated from Kai Li's Ph.D. work <ref> [48] </ref>. Since then, several page-based software DSM systems have been proposed, many of which have been discussed and cited in Chapter 2. MGS heavily leverages the body of work on software DSMs since MGS uses the same mechanisms proposed for traditional software DSM systems to provide shared memory across SSMPs.
Reference: [49] <author> Beng-Hong Lim. </author> <title> Parallel C Functions for the Alewife System. </title> <type> Alewife Memo 37, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: First, we reduce the frequency of inlining by using SVM only on memory references that have the potential for being shared, and therefore must be kept coherent through multigrain shared memory. As was shown in Table 5.1, many 7 The Alewife Parallel C compiler <ref> [49] </ref> reserves two registers at every memory reference site for computation related to software virtual memory. 88 CHAPTER 5. IMPLEMENTATION memory. of the memory objects in MGS are placed in physical memory.
Reference: [50] <author> Magnus Karlsson and Per Stenstrom. </author> <title> Performance Evaluation of a Cluster-Based Multiprocessor Built from ATM Switches and Bus-Based Multiprocessor Servers. </title> <booktitle> In Proceedings of the Second IEEE Symposium on High-Performance Computer Architecture. IEEE, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: We call this approach physical clustering because the cluster boundaries that partition the DSSMP are fixed in hardware. All the systems currently proposed in the literature that resemble MGS <ref> [22, 50] </ref> (and that we are aware of) use physical clustering. 1 More justification will be given for this claim when we discuss our performance framework in Section 6.2 of Chapter 6. 77 78 CHAPTER 5. <p> Second, as was stated above, SoftFLASH uses larger pages than MGS. As Section 6.5.2 of Chapter 6 showed, large page sizes can have a negative impact on performance for those applications that are vulnerable to false sharing. MGS is also very similar in architecture to the system studied in <ref> [50] </ref>. In this work, a 16-processor system configured using 4-way SMPs over an ATM network is simulated. The authors study the effectiveness of prefetching techniques to hide the large latencies associated with paging between SMP nodes.
Reference: [51] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating Latency Through Software-Controlled Prefetching in Shared-Memory Multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: If the processor performing such a write is stalled for the duration of the invalidation (s), significant cache miss stall can be introduced, thus degrading performance. Many techniques have been proposed to address such cache miss stall overhead, such as software-controlled prefetching <ref> [51] </ref>, block multithreading [43, 68], and relaxed memory 1 In the worst case, there could be P 1 outstanding copies, where P is the number of DSM nodes. 28 CHAPTER 2. BACKGROUND consistency models [1, 25, 19].
Reference: [52] <author> Shubu Mukherjee, Shamik Sharma, Mark Hill, Jim Larus, Anne Rogers, and Joel Saltz. </author> <title> Efficient Support for Irregular Applications on Distributed-Memory Machines. </title> <booktitle> In Proceedings of the 5th Annual Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 68-79. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Because of the pruning inherent to branch and bound algorithms, TSP has dynamic (data-dependent) control flow. Finally, Unstructured is a computation over an unstructured mesh from the University of Wisconsin, Madison, and the University of Maryland, College Park <ref> [52] </ref>. The computation resembles solving Eular equations on unstructured meshes, but does not actually produce meaningful numeric results. The code exhibits highly irregular memory access patterns and dynamic control flow. Unstructured-Kernel is a variant of Unstructured, and is explained in Section 6.4.
Reference: [53] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <address> New York, </address> <month> April </month> <year> 1994. </year> <note> IEEE. 202 BIBLIOGRAPHY </note>
Reference-contexts: The coherence protocol handlers are implemented at user level; therefore, applications can link against a library of common protocols, or provide a protocol that is tailored to the application for higher performance <ref> [53] </ref>. Fine-grained SMP clusters offer an interesting alternative to multigrain systems. Like multigrain systems, they represent an intermediate architecture between traditional all 8.3. FINE-GRAINED SMP CLUSTERS 187 software and all-hardware DSMs.
Reference: [54] <author> Bryan S. Rosenburg. </author> <title> Low-Synchronization Translation Lookaside Buffer Consistency in Large-Scale Shared-Memory Multiprocessors. </title> <journal> ACM Operating Systems Review, </journal> <volume> 23(5) </volume> <pages> 137-146, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The system must ensure that any changes made to the mapping state in the page tables do not leave a stale copy of a mapping entry in any processor's TLB. Many approaches for providing mapping consistency (also known as TLB consistency) have been proposed <ref> [17, 64, 54, 9] </ref>. The solution used in MGS is closest to the one proposed in the PLATINUM system [17]. Each SSMP has a TLB directory that tracks the cached page table entries for all the pages resident in the SSMP's physical memory.
Reference: [55] <author> Joel H. Saltz, Ravi Mirchandaney, and Kay Crowley. </author> <title> Run-Time Parallelization and Scheduling of Loops. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 40(5) </volume> <pages> 603-612, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Unstructured is by far the most difficult application to achieve high performance on DSSMPs because of its highly irregular data access patterns. The application performs a computation on a static undirected graph which is read from an input file. Because of the graph's unstructured nature, runtime preprocessing techniques <ref> [55] </ref> are used to schedule computation associated with the graph onto processors. After preprocessing, much of the execution time is spent in edge loops, or loops that perform computations associated with the edges in the unstructured graph.
Reference: [56] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The Shared Regions Approach to Software Cache Coherence on Multiprocessors. </title> <booktitle> In Principles and Practices of Parallel Programming, </booktitle> <year> 1993, </year> <pages> pages 229-238, </pages> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: However, to achieve CRL-like performance on MGS, the programmer must observe the locality issues that were addressed in Chapter 6. One benefit of a decoupled approach 2 In addition to CRL, another system that supports the regions abstraction is described in <ref> [56] </ref>. 3 The use of application-specific data layout information by the shared memory layer is similar to what is supported under Entry Consistency as implemented by the Midway distributed shared memory system [7]. 4 The Alewife implementation of CRL only uses the efficient communication interfaces provided by Alewife.
Reference: [57] <author> Daniel J. Scales, Kourosh Gharachorloo, and Chandramohan A. Thekkath. </author> <note> Shasta: </note>
Reference-contexts: Since the software shared memory layer in MGS relies heavily on virtual memory support, MGS must build virtual memory on top of existing Alewife shared memory mechanisms in software. There have been several schemes proposed in the literature for supporting virtual memory in software <ref> [57, 31, 5] </ref>. The general idea in software virtual memory (SVM) is that the compiler or the software system assumes responsibility for address translation and protection against unprivileged accesses in the absence of hardware TLBs. <p> In addition to the mixed hardware and software shared memory systems described thus far, there have been all-software systems that support multiple coherence granular-ities as well. Two examples of such software-only multigrain systems are CRL [35] and Shasta <ref> [57] </ref>. We first describe CRL, then Shasta. <p> Shasta proposes several compiler optimizations that reduce the cost of software translation and checking code so that fine-grain access control can be supported efficiently. In <ref> [57] </ref>, the authors report translation and checking overheads in the range of 5% - 35% (compare to 50% - 100% for MGS, with one application exceeding 100%).
References-found: 57

