URL: http://www.cs.brown.edu/people/jmk/work/iros96.ps
Refering-URL: http://www.cs.brown.edu/people/jmk/
Root-URL: http://www.cs.brown.edu
Email: jmkg@cs.brown.edu  
Title: Discrete Bayesian Uncertainty Models for Mobile-Robot Navigation  
Author: Anthony R. Cassandra and Leslie Pack Kaelbling and James A. Kurien farc, lpk, 
Affiliation: Department of Computer Science Brown University  
Abstract: Discrete Bayesian models have been used to model uncertainty for mobile-robot navigation, but the question of how actions should be chosen remains largely unexplored. This paper presents the optimal solution to the problem, formulated as a partially observable Markov decision process. Since solving for the optimal control policy is intractable, in general, it goes on to explore a variety of heuristic control strategies. The control strategies are compared experimentally, both in simulation and in runs on a real robot. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Buhmann, W. Burgard, Cremers A., D. Fox, T. Hofmann, F. Scheider, J. Strikos, and S. Thrun. </author> <title> The mobile robot RHINO. </title> <journal> AI Magazine, </journal> <volume> 16(2) </volume> <pages> 31-37, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: Restricting the grid to a local region allows numerous simplifications over global occupancy-grid methods such as those used in RHINO <ref> [1] </ref>. First, even when the robot is operating in very narrow corridors there is no preprocessing of ultrasonic data to eliminate higher-order specular reflection. Any ultrasonic reading greater than 1.5 meters is not relevant to the grid and is discarded, eliminating the majority of troublesome readings.
Reference: [2] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA, </address> <year> 1994. </year>
Reference-contexts: This model was developed in the operations research community [10, 20] and has been recently introduced to artificial intelligence <ref> [2] </ref>.
Reference: [3] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: The remaining difficulty is that the belief process is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces and the existing exact pomdp solution procedures are computationally intractable [14]. A number of algorithms exist for solving the belief mdp <ref> [19, 3, 11, 9] </ref>, but even the most efficient of these can only solve small problems with on the order of 10 states and 10 observations. 5 Heuristic Control Strategies Since it is computationally intractable to compute the optimal pomdp control strategy for all but the simplest environments, we consider a
Reference: [4] <author> Richard Duda and Peter Hart. </author> <title> Use of the Hough transform to detect lines and curves in pictures. </title> <journal> Graphics and Image Processing, </journal> <volume> 15(1) </volume> <pages> 11-15, </pages> <year> 1972. </year>
Reference-contexts: Once a world axis is estimated, a simple search for doors, openings and walls in the robot's vicinity is made along the axes to the left, front and right of the robot. If there is a rough line segment (found using a Hough transform <ref> [4] </ref>) in the occupancy grid that is parallel or perpendicular to the estimated axis, then a wall is observed. Similarly, if the occupancy grid is largely clear along an axis an open observation results.
Reference: [5] <author> Sven Koenig and Reid Simmons. </author> <title> Unsupervised learning of probabilistic models for robot navigation. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: Another line of future work is to learn the world model from experience using techniques adapted from hidden Markov models; this is currently being pursued (for learning the probabilities, but not the topology) by Koenig and Simmons <ref> [5] </ref>. Acknowledgments Hagit Shatkay implemented an algorithm to find homing sequences. This work was supported in part by NSF grants IRI-9453383 and IRI-9312395.
Reference: [6] <author> Zvi Kohavi. </author> <title> Switching and finite automata theory. </title> <publisher> McGraw-Hill, </publisher> <address> New York, N.Y., </address> <year> 1978. </year>
Reference-contexts: While these values may still be too optimistic, they provide a truer picture of the value of being confused. We used a sequence of length 20, which consisted of a repetition of 5 move-forward actions followed by a single turn-left action. This sequence was based upon the homing sequence <ref> [6] </ref> for a determinized version of our domain.
Reference: [7] <author> J.J. Leonard and Hugh Durrant-Whyte. </author> <title> Localization by tracking geometric beacons. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 7(6), </volume> <year> 1991. </year>
Reference-contexts: It supplies a well-founded method for taking the robot's current beliefs and combining them with uncertain information gained from sensing and acting. The Bayesian framework has been used for a long time in robotics with good results <ref> [7] </ref>. In this paper, we develop a two-level architecture, with Bayesian modeling done at the top level only. In addition, we explore sub-optimal control strategies given the Bayesian belief state.
Reference: [8] <author> Michael Littman, Anthony Cassandra, and Leslie Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is similar to the action-choice method used in Xavier [17]. The only difference is that they used a deterministic planning algorithm to choose the best action for each world state, rather than finding the best action in the underlying mdp. The Q MDP method <ref> [8] </ref> is a more refined version of the voting method, in which the votes of each state are apportioned among the actions according to their Q value: QMDP (b) = argmax a X b (s) Q (s; a)) : This is in contrast to the "winner take all" behavior of the <p> All of the control strategies we have explored here are essentially myopic with respect to uncertainty. None of them is able to take a long string of actions in order to disambiguate its belief state. We are beginning to apply some more sophisticated methods from the machine learning literature <ref> [8, 15] </ref> that find approximations to the true value function of the pomdp. We expect that they will improve performance when the starting location is extremely uncertain. The simple domains explored in this paper do not exercise the abilities of the pomdp models to control active perception.
Reference: [9] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kaelbling. </author> <title> An efficient algorithm for dynamic programming in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1995. </year>
Reference-contexts: The remaining difficulty is that the belief process is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces and the existing exact pomdp solution procedures are computationally intractable [14]. A number of algorithms exist for solving the belief mdp <ref> [19, 3, 11, 9] </ref>, but even the most efficient of these can only solve small problems with on the order of 10 states and 10 observations. 5 Heuristic Control Strategies Since it is computationally intractable to compute the optimal pomdp control strategy for all but the simplest environments, we consider a
Reference: [10] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: This model was developed in the operations research community <ref> [10, 20] </ref> and has been recently introduced to artificial intelligence [2].
Reference: [11] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: The remaining difficulty is that the belief process is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces and the existing exact pomdp solution procedures are computationally intractable [14]. A number of algorithms exist for solving the belief mdp <ref> [19, 3, 11, 9] </ref>, but even the most efficient of these can only solve small problems with on the order of 10 states and 10 observations. 5 Heuristic Control Strategies Since it is computationally intractable to compute the optimal pomdp control strategy for all but the simplest environments, we consider a
Reference: [12] <author> H. Moravec and A. Elfes. </author> <title> High resolution maps from wide angle sonar. </title> <booktitle> In Proceedings of the IEEE Conference on Robotics and Automation, </booktitle> <pages> pages 19-24, </pages> <year> 1985. </year>
Reference-contexts: They are combined in a local occupancy grid, which is the basis of high-level feature detection. Occupancy Grid The pilot fuses ultrasonic measurements in an occupancy grid using a simplified variant of the algorithm of Moravec and Elfes <ref> [12] </ref>. Since the pilot is responsible solely for local navigation, the occupancy grid only maps features whose distance from the robot is within a small range.
Reference: [13] <author> Illah Nourbakhsh, Rob Powers, and Stan Birch-field. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <pages> pages 53-60, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: It may be the case, for example, that the robot will know that it is in a corner of the building, but not which one. This is by no means the first project to use discrete belief models. The Dervish project at Stan-ford University <ref> [13] </ref> used a topological map combined with robust low-level behaviors. Their belief-state update was heuristic, based only on a model of observational error (but not error due to actions). The Xavier project at Carnegie-Mellon University [17] used a full belief-state update. <p> pomdp control strategy for all but the simplest environments, we consider a number of simple heuristic control strategies, some of which are fairly ad hoc, and others with more principled motivations. 5.1 Belief Replanning We have implemented a strategy that is a slight variation on the strategy used in Dervish <ref> [13] </ref>. The algorithm starts by finding the most likely world state and planning a path to the goal in a deterministic idealization of the domain. In addition, it generates a sequence of predicted world states that will be traversed if the nominal trajectory is followed.
Reference: [14] <author> Christos H. Papadimitriou and John N. Tsitsik-lis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: The remaining difficulty is that the belief process is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces and the existing exact pomdp solution procedures are computationally intractable <ref> [14] </ref>.
Reference: [15] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1088-1094. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: All of the control strategies we have explored here are essentially myopic with respect to uncertainty. None of them is able to take a long string of actions in order to disambiguate its belief state. We are beginning to apply some more sophisticated methods from the machine learning literature <ref> [8, 15] </ref> that find approximations to the true value function of the pomdp. We expect that they will improve performance when the starting location is extremely uncertain. The simple domains explored in this paper do not exercise the abilities of the pomdp models to control active perception.
Reference: [16] <author> Martin L. Puterman. </author> <title> Markov Decision Processes. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: A policy is optimal if it is not dominated by any other policy. Given a Markov decision process and a value for fl, it is possible to compute the optimal policy fairly efficiently <ref> [16] </ref>. We shall use fl (s) to refer to the optimal policy for an mdp, but drop asterisks from the optimal value and Q functions, V (s) and Q (s; a). 3.2 Adding Partial Observability When the state is not completely observable, we must add a model of observations.
Reference: [17] <author> Reid Simmons and Sven Koenig. </author> <title> Probabilistic navigation in partially observable environments. </title> <booktitle> In Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1080-1087, </pages> <address> Montreal, Canada, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The Dervish project at Stan-ford University [13] used a topological map combined with robust low-level behaviors. Their belief-state update was heuristic, based only on a model of observational error (but not error due to actions). The Xavier project at Carnegie-Mellon University <ref> [17] </ref> used a full belief-state update. Both of these projects used fairly ad hoc action strategies, based on planning paths as if the domain were deterministic. <p> Then, we choose the action that is most likely to be optimal: vote (b) = argmax a w a (b). This is similar to the action-choice method used in Xavier <ref> [17] </ref>. The only difference is that they used a deterministic planning algorithm to choose the best action for each world state, rather than finding the best action in the underlying mdp.
Reference: [18] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year>
Reference-contexts: The reward function, , is constructed from R by taking expectations according to the belief state; that is, (b; a) = s2S The belief mdp is Markov <ref> [18] </ref>, that is, having information about previous belief states cannot improve the choice of action. Most importantly, if an agent adopts the optimal policy for the belief mdp, the resulting behavior will be optimal for the partially observable process.
Reference: [19] <author> Edward J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference-contexts: The remaining difficulty is that the belief process is continuous; the established algorithms for finding optimal policies in mdps work only in finite state spaces and the existing exact pomdp solution procedures are computationally intractable [14]. A number of algorithms exist for solving the belief mdp <ref> [19, 3, 11, 9] </ref>, but even the most efficient of these can only solve small problems with on the order of 10 states and 10 observations. 5 Heuristic Control Strategies Since it is computationally intractable to compute the optimal pomdp control strategy for all but the simplest environments, we consider a
Reference: [20] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: This model was developed in the operations research community <ref> [10, 20] </ref> and has been recently introduced to artificial intelligence [2].
References-found: 20

