URL: ftp://dis.cs.umass.edu/pub/whitehair-thesis2.ps.Z
Refering-URL: http://dis.cs.umass.edu/research/idp.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: EMAIL: whitehair@cs.umass.edu  
Title: A FRAMEWORK FOR THE ANALYSIS OF SOPHISTICATED CONTROL  
Author: Robert C. Whitehair 
Note: This work was supported by DARPA contract N00014-92-J-1698, Office of Naval Research contract N00014-92-J-1450, and NSF contract CDA 8922572. The content of the information does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred.  
Address: Amherst MA 01003-4610  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: UMass CMPSCI Technical Report 95-****** February 1996 
Abstract-found: 1
Intro-found: 1
Reference: [ Berliner, 1979 ] <author> Hans Berliner. </author> <title> The B* tree search algorithm: A best-first proof procedure. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 23-40, </pages> <year> 1979. </year>
Reference-contexts: Figure 2.2 summarizes these topics and illustrates their relationships to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following sections. 2.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing <ref> [ Berliner, 1979, Samuel, 1963 ] </ref> , and theorem proving [ Newell et al., 1963 ] . The heuristic knowledge used in the search process was relatively limited. <p> The total number of solutions generated for an arbitrary set I can be enormous. In general, the role of sophisticated control is to 52 limit the size of I by implicitly enumerating as much of the search space as possible. This objective is illustrated in Fig. 3.7, from Berliner <ref> [ Berliner, 1979 ] </ref> . Figure 3.7.a represents the set I generated by grammar G without the use of sophisticated control techniques. In this example, problem solving is essentially exhaustive every possible derivation tree is generated and compared. Figure 3.7.b shows the effects of sophisticated control. <p> This is a significant observation because it can be used in the design of meta-operators and problem solving strategies. For example, problem solving can be viewed as the production and application of constraint. As discussed by a number of researchers, including Berliner <ref> [ Berliner, 1979 ] </ref> and Carver [ III, 1990 ] , search can be thought of in terms of either a find best or a disprove rest strategy.
Reference: [ Bonissone and Decker, 1986 ] <author> Piero P. Bonissone and Keith S. Decker. </author> <title> Selecting uncertainty calculi and granularity: An experiment in trading-off precision and complexity. </title> <editor> In L. N. Kanal and J. F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence. </booktitle> <publisher> North Holland, </publisher> <year> 1986. </year>
Reference-contexts: C.2 Belief Representation and Uncertainty Uncertainty arises in any system from the reliability of the initial data, the imprecision of that data or the language used to represent it, the incompleteness of the data, and the aggregation of the data from data approximations or multiple sources <ref> [ Bonissone and Decker, 1986 ] </ref> . Additional uncertainty is introduced with the use of approximate search, data and knowledge. For example, the credibility of a partial result constructed by ignoring potentially corroborating data must be distinguishable from the credibility of a similar partial result constructed using all available data.
Reference: [ Bonissone et al., 1987 ] <author> Piero P. Bonissone, Steven S. Gans, and Keith S. Decker. RUM: </author> <title> A layered architecture for reasoning with uncertainty. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <month> August </month> <year> 1987. </year>
Reference-contexts: To represent uncertainty caused by the use of approximate search, data, and knowledge, the representation of credibility must be expanded to a four-valued system. The new belief system was derived from evidential reasoning [ Lowrance and Garvey, 1982 ] and is similar to that in RUM <ref> [ Bonissone et al., 1987 ] </ref> . The belief in a hypothesis is now represented by a measure of positive belief (certainty) and of negative belief (refutation). To represent the completeness of the solution, the positive and negative beliefs are further divided into upper and lower bounds.
Reference: [ Campbell et al., 1991 ] <author> I. C. Campbell, K. J. Luczynski, and I. Hood. </author> <title> Putting knowledge-based concepts to work for mechanical design. </title> <editor> In Reid Smith and Carlisle Scott, editors, </editor> <booktitle> Innovative Applications of Artificial Intelligence 3: Proceedings of the IAAI-91 Conference, </booktitle> <pages> pages 157-176. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <note> ISBN 0-262-68068-8. </note>
Reference-contexts: Other work indicates that context-free grammars can be exploited in a top-down manner as well. For example, impressive results have been achieved in automated design tasks by researchers such as Campbell <ref> [ Campbell et al., 1991 ] </ref> and Mullins and Rinderle [ Mullins and Rinderle, 1991a, Mullins and Rinderle, 1991b ] who use grammatical approaches to engineering design.
Reference: [ Carver and Lesser, 1991 ] <author> Norman Carver and Victor Lesser. </author> <title> The Evolution of Blackboard Control. </title> <journal> Expert Systems with Applications, </journal> <volume> 7(1), </volume> <year> 1991. </year> <note> Special issue on The Blackboard Paradigm and Its Applications. </note>
Reference-contexts: Consequently, at this point in time, the design of sophisticated blackboard-based problem solvers is largely a trial and error process <ref> [ Carver and Lesser, 1991 ] </ref> . <p> As discussed by Carver <ref> [ Carver and Lesser, 1991 ] </ref> , the blackboard architecture was originally developed to deal with the difficult characteristics of a typical interpretation task: a very large search space; errorful or incomplete input data; and imprecise and/or incomplete problem-solving knowledge. <p> Control is one of the important issues that must be addressed in the successful formulation of Hearsay II type architectures and control continues to be an active area of research in the field of blackboard systems <ref> [ Carver and Lesser, 1991 ] </ref> . The issues related to the design of a blackboard system's control component are often associated with the tradeoff between domain processing and meta-level processing. <p> For example, MARKER (V1,S8) = 0.07. Consequently, ignoring S8 data might be a reasonable strategy in this situation. In general, the marker and differentiator relationships can be used to explain the success of techniques such as approximate processing and incremental planning <ref> [ Carver and Lesser, 1991 ] </ref> . From a bottom-up perspective, the differentiator relationship can identify the intermediate results that are most appropriate for abstracting and clustering.
Reference: [ Carver and Lesser, 1993 ] <author> Norman Carver and Victor R. Lesser. </author> <title> Planning for the control of an interpretation system. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(6), </volume> <year> 1993. </year> <note> Special Issue on Scheduling, Planning, and Control. </note>
Reference-contexts: Non-local control will also be referred to as sophisticated control. As a consequence of sophisticated control, the problem solving process takes on a recursive quality where the control issue becomes a search problem in itself <ref> [ Carver and Lesser, 1993 ] </ref> . The control component becomes a knowledge based mechanism that reasons and searches for the best operator. The control mechanism must reason about which states to evaluate, when to evaluate them, and which evaluation architectures to use. <p> Also, this information could be used in differential diagnosis processing to disambiguate competing hypotheses <ref> [ Carver and Lesser, 1993 ] </ref> . For example, in certain situations it is possible to use marker information to determine if a hypothesis was erroneously derived from noise. <p> Knowledge about differentiators can be used both in the design of problem solving architectures and dynamic control algorithms. Architecturally, differentiators can be used to construct special operators for differential diagnosis <ref> [ Carver and Lesser, 1993 ] </ref> . In control algorithms, differentiators can be used to focus problem solving activity [ Erman et al., 1980 ] . In the vehicle tracking grammar, S1, S2, and S3 are strong differentiators for the event V1 (a vehicle location of type 1).
Reference: [ Corkill and Lesser, 1981 ] <author> Daniel D. Corkill and Victor R. Lesser. </author> <title> A goal-directed Hearsay-II architecture: Unifying data-directed and goal-directed control. </title> <type> Technical Report 81-15, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> June </month> <year> 1981. </year>
Reference-contexts: domain, the signal interpretation problem domain. (Intuitively, interpretation problems are tasks where a stream of input data is analyzed and an explanation is postulated as to what domain events occurred to generate the signal data the problem solver is attempting to interpret the signal data and determine what caused it <ref> [ Corkill and Lesser, 1981, Erman et al., 1980 ] </ref> .) The specific contributions made by this work include the following. 1.
Reference: [ Corkill et al., 1982 ] <author> Daniel D. Corkill, Victor R. Lesser, and Eva Hudlick a. </author> <title> Unifying data-directed and goal-directed control: An example and experiments. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 143-147, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> August </month> <year> 1982. </year>
Reference-contexts: The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II [ Erman et al., 1980 ] blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces <ref> [ Corkill et al., 1982, Corkill, 1983 ] </ref> . To our knowledge, no other analysis framework provides a perspective where different approaches to control can be analyzed as part of a unified domain theory.
Reference: [ Corkill, 1983 ] <author> Daniel David Corkill. </author> <title> A Framework for Organizational Self-Design in Distributed Problem Solving Networks. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> February </month> <year> 1983. </year> <note> (Also published as Technical Report 82-33, </note> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> December </month> <year> 1982.). </year>
Reference-contexts: The control mechanisms modeled are based on 12 techniques developed for the Hearsay II [ Erman et al., 1980 ] , the DVMT <ref> [ Corkill, 1983 ] </ref> , and a real-time version of the DVMT [ Decker et al., 1990 ] . <p> The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II [ Erman et al., 1980 ] blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces <ref> [ Corkill et al., 1982, Corkill, 1983 ] </ref> . To our knowledge, no other analysis framework provides a perspective where different approaches to control can be analyzed as part of a unified domain theory. <p> Given these rules, there is a great deal of ambiguity in this grammar. This grammar is based loosely on the vehicle tracking domain of the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [ Corkill, 1983 ] </ref> . Figure 1.12 shows the same grammar modified to include a class of meta-level operators referred to as goal operators presented in [ Lesser et al., 1989b ] . <p> Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in projects including <ref> [ Erman et al., 1980, Corkill, 1983, Durfee, 1987 ] </ref> . The interpretation search spaces presented in this thesis are all convergent search spaces. The general structures that define these spaces are defined in Chapter 3.2. <p> This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound [ Papadimitriou and Steiglitz, 1982, Pearl, 1984, Kumar and Kanal, 1988 ] . Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems <ref> [ Erman et al., 1980, Corkill, 1983 ] </ref> . In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states. <p> The grammar used in the example of redundancy will not be used in any other examples.) Redundancy is also shown in Fig. 4.14. This example depicts a stylized version of redundant processing that can occur in certain interpretation tasks such as the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [ Corkill, 1983 ] </ref> . The problem solving strategy embodied in this example offers alternative paths for interpreting tracks of data, in this case the input uvwxyz. In Fig. 4.14.b, a track interpretation is formed incrementally by extending a partial interpretation one time unit. <p> p (y 1 : : : y k ))), where x i and y i represent the i th element of the rule p, and function p is the semantic credibility function associated with p, then the interpretation problem is monotone. 73 For many interpretation domains, such as the DVMT <ref> [ Corkill, 1983 ] </ref> and Hearsay-II [ Er-man et al., 1980 ] , monotonicity does not hold. A simple example illustrates how this might occur. Figure 4.20 shows two interpretations of signal data in a speech understanding domain. <p> The interpretation domain that will be studied in detail in Chapter 11 is a vehicle tracking problem as defined in previous work on distributed vehicle monitoring in the RESUN research project [ III, 1990 ] and in the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [ Corkill, 1983 ] </ref> . The problem solver's input consists of preprocessed sensor data gathered from a single contiguous region. The problem solver then processes the data in an attempt to identify the type of vehicle that generated the signals and the path it traversed through the region. <p> Each of these tracks is composed of vehicle locations. Vehicle locations are composed of group classes and group classes consist of signal classes. The signal classes can be thought of as preprocessed signal data <ref> [ Corkill, 1983 ] </ref> . A grammar that will generate problem instances for this domain is shown in Figures 4.25, 4.27, 4.29, and 4.30. These figures show the production rules of the grammar and the probability distribution values for associated with each of the rules. <p> The analysis contained in this section will focus on abstractions used in sophisticated control mechanisms that have been implemented in the Distributed Vehicle Monitoring Testbed (DVMT) <ref> [ Corkill, 1983 ] </ref> and that have been implemented to exploit problem structures of non-monotonic domains. <p> 1 6 ) + cost (op 2 6 ) + cost (op 3 6 ) + cost (op 4 6 ). (For the sake of simplicity, the cost of merging identical states will be ignored.) Analysis of these costs will use models similar to the cost models from the DVMT <ref> [ Corkill, 1983 ] </ref> . In the DVMT, the cost of an operator application can be approximated by a constant factor plus a function of the number of inputs and the number of outputs. <p> Though these figures are approximations, they are representative of the costs of DVMT search operators. The input component, n, of Equation 4.8.2 reflects the cost of retrieving data from the blackboard and the combinatorial nature of the reasoning processes used by DVMT operators <ref> [ Decker et al., 1990, Corkill, 1983 ] </ref> . The output component, m, reflects the cost of writing data to the blackboard. In this simple example, goal processing has clear performance advantages. <p> This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful. More specifically, this analysis did not consider the potential benefits of the subgoaling mechanisms described in <ref> [ Corkill, 1983 ] </ref> . In the example presented in this section, the principle difference in cost can be attributed to the fact that without goal processing, op 6 had to be applied four times to connect the low-level states. <p> In the DVMT, operators that extend goals are pruned under certain conditions. Furthermore, such pruning is done often enough that it is advantageous to use goal processing as described in <ref> [ Corkill, 1983 ] </ref> . 4.9 Chapter Summary This chapter demonstrates how the IDP formalism modifies a domain grammar to represent phenomena such as noise and missing data are discussed. <p> random problem instances each Sig: Whether or not the difference between expected cost and the actual average cost was statistically significant Y: yes, there is a statistically significant difference N: no, there is not a statistically significant difference % Correct: percentage of correct answers found 171 associated with the DVMT <ref> [ Corkill, 1983 ] </ref> For each of the SNTs of G 0 , M, O, A, B, N, we added nonterminals of the form A1, A2, A3, A4, A5 where the symbol could be any one of the SNTs and the number indicates the number of time-locations in a vehicle track. <p> The effectiveness of preconditions has been demonstrated in many interpretation systems including Hearsay II [ Erman et al., 1980 ] and the DVMT <ref> [ Corkill, 1983 ] </ref> . The original control architecture is shown in Fig. 10.3 for comparison. During each control cycle, new states are created and the problem solver identifies which operators can be applied to the new states. Preconditions are applied to determine which operators are eligible for execution. <p> G-T1 ! G-T1 GT 1;T 1 op 53 M.P.54. T1 ! T1 V 1;N op 55 to the Base Space 179 Corkill and Lesser, 1981 ] . In Chapter 4, we showed how a specific form of goal processing used in the DVMT to eliminate redundant local activity <ref> [ Corkill, 1983 ] </ref> could be represented in IDP I grammars. Another form of goal processing was developed and applied in the DVMT to focus problem solving activity more effectively. <p> The goal processing mechanism is patterned after goal processing mechanisms introduced in Hearsay II [ Erman et al., 1980 ] and the DVMT <ref> [ Corkill, 1983 ] </ref> . Figure 4.28 on page 85 shows some of the specific problem instances that are generated with this grammar. The use of the feature list convention is shown in Chapter 4.6.
Reference: [ Davis, 1980 ] <author> Randall Davis. </author> <title> Meta-rules: Reasoning about control. </title> <journal> Artificial Intelligence, </journal> <volume> 15 </volume> <pages> 179-222, </pages> <year> 1980. </year>
Reference: [ Decker et al., 1989 ] <author> Keith Decker, Marty Humphrey, and Victor Lesser. </author> <title> Experimenting with control in the DVMT. </title> <type> Technical Report 89-00, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> March </month> <year> 1989. </year> <month> 310 </month>
Reference-contexts: For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool [ Lesser et al., 1989a, Lesser et al., 1989b ] . Subsequent work exploited this observation and resulted in significant performance improvements <ref> [ Decker et al., 1989 ] </ref> . This work was specific to one aspect of goal processing in the DVMT domain, and left open questions regarding the general properties of domains where goal processing is useful.
Reference: [ Decker et al., 1990 ] <author> Keith S. Decker, Victor R. Lesser, and Robert C. Whitehair. </author> <title> Extending a blackboard architecture for approximate processing. </title> <journal> The Journal of Real-Time Systems, </journal> 2(1/2):47-79, 1990. Also COINS TR-89-115. 
Reference-contexts: of the blackboard architecture, many research projects have focused on the development of control mechanisms that focus or limit search in such a way that the overall cost of problem solving is reduced and a consistent level of quality or correctness is either maintained or altered in a well-defined manner <ref> [ Decker et al., 1990, Garvey and Lesser, 1993 ] </ref> . 6 Given the success of some sophisticated blackboard-based interpretation control mechanisms, there is an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains. <p> Vehicle Monitoring Testbed (DVMT) [ Carver and Lesser, 1991, Corkill, 1983, Decker et al., 1989 ] , sophisticated control techniques such as goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser et al., 1989b ] , and abstracting and approximating computational domain theories <ref> [ Decker et al., 1990, Lesser and Pavlin, 1988 ] </ref> . the sophisticated control mechanisms studied in this thesis. <p> The control mechanisms modeled are based on 12 techniques developed for the Hearsay II [ Erman et al., 1980 ] , the DVMT [ Corkill, 1983 ] , and a real-time version of the DVMT <ref> [ Decker et al., 1990 ] </ref> . <p> Though these figures are approximations, they are representative of the costs of DVMT search operators. The input component, n, of Equation 4.8.2 reflects the cost of retrieving data from the blackboard and the combinatorial nature of the reasoning processes used by DVMT operators <ref> [ Decker et al., 1990, Corkill, 1983 ] </ref> . The output component, m, reflects the cost of writing data to the blackboard. In this simple example, goal processing has clear performance advantages. <p> For example, if the initial input data is very noisy, and if the noise and the correct data have very similar characteristics, they can be clustered into a single datum with characteristics encompassing both <ref> [ Decker et al., 1990 ] </ref> . has an attribute for time and location. Shown are the clusters formed by ignoring one of the attributes vehicle location. This form of approximation can also be thought of as replacing a single value with a range of values. <p> These strategies are defined and discussed in <ref> [ Decker et al., 1990, Lesser et al., 1988a ] </ref> . The specific techniques described here are eliminating corroborating support and level hopping. These techniques are also defined and discussed in [ Decker et al., 1990, Lesser et al., 1988a ] . <p> These strategies are defined and discussed in <ref> [ Decker et al., 1990, Lesser et al., 1988a ] </ref> . The specific techniques described here are eliminating corroborating support and level hopping. These techniques are also defined and discussed in [ Decker et al., 1990, Lesser et al., 1988a ] . Both general strategies are based on reducing the amount of the search space that is explored in generating an interpretation. As a consequence, the resulting interpretation is considered an abstract state in a projection space. <p> As mentioned previously, it has been demonstrated that using approximations to developing a general understanding of a particular problem instance and then using that understanding to more effectively control problem solving can be an effective problem solving technique. Approximations can also be used for real-time processing <ref> [ Decker et al., 1990 ] </ref> . Such use would complicate IDP representations by setting a fixed time constraint on an interpretation problem which would affect the precision and credibility of possible results. This would change the optimality criterion and the associated decision strategy. <p> If the approximate 1 In general, the use of meta-operators will reduce the cost of problem solving in two ways, as described in <ref> [ Decker et al., 1990 ] </ref> . One is by eliminating certain search paths from consideration. This reduces the cost of problem solving because the costs of executing the operators required to explore that area are not incurred. <p> In particular, through experimentation and analysis, many commonalities between disparate domains are being discovered that seem to indicate that certain control architectures can be used to form a basis for a very powerful and general form of problem solving. Specifically, it is becoming apparent that approximate processing <ref> [ Lesser and Pavlin, 1988, Decker et al., 1990 ] </ref> and goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser et al., 1989b ] are forms of processing that can be generalized to many real-world domains. <p> The use of approximate processing affects a system's representation of partial results and solutions. In a state space search, a partial result or a solution is represented as a sequence of operators that were applied to the start state. As discussed in <ref> [ Decker et al., 1990 ] </ref> , the use 284 of approximate search and approximate knowledge result in partial results and solutions that are offset along the axes defining solution quality. In addition, the use of approximate data confounds the solution path by extending it over multiple levels of abstraction.
Reference: [ Durfee and Lesser, 1986 ] <author> Edmund H. Durfee and Victor R. Lesser. </author> <title> Incremental planning to control a blackboard-based problem solver. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 58-64, </pages> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Examples of these in interpretation domains include the clustering mechanisms and associated abstractions used by Durfee and Lesser in <ref> [ Durfee and Lesser, 1986 ] </ref> and similar mechanisms developed in Hearsay II [ Erman et al., 1980 ] . In addition, these strategies are commonly applied to other domains with similar results, such as those demonstrated by Knoblock in planning tasks [ Knoblock, 1991b ] .
Reference: [ Durfee, 1987 ] <author> Edmund H. Durfee. </author> <title> A Unified Approach to Dynamic Coordination: Planning Actions and Interactions in a Distributed Problem Solving Network. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> September </month> <year> 1987. </year> <note> (Also published as Technical Report 87-84, </note> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, MA, September,1987.). </address>
Reference-contexts: Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in projects including <ref> [ Erman et al., 1980, Corkill, 1983, Durfee, 1987 ] </ref> . The interpretation search spaces presented in this thesis are all convergent search spaces. The general structures that define these spaces are defined in Chapter 3.2. <p> The specific objective was to determine if the analysis tools are capable of representing and analyzing the sophisticated control mechanisms used in problem solvers such as that described by Durfee in <ref> [ Durfee, 1987 ] </ref> . The experiments that were conducted are described in this section. The problem solver used for these experiments is based on the grammar introduced in Chapter 12 and shown again in Fig. 13.1.
Reference: [ Erman et al., 1980 ] <author> Lee D. Erman, Frederick Hayes-Roth, Victor R. Lesser, and D. Raj Reddy. </author> <title> The Hearsay-II speech-understanding system: Integrating knowledge to resolve uncertainty. </title> <journal> Computing Surveys, </journal> <volume> 12(2) </volume> <pages> 213-253, </pages> <month> June </month> <year> 1980. </year>
Reference-contexts: domain, the signal interpretation problem domain. (Intuitively, interpretation problems are tasks where a stream of input data is analyzed and an explanation is postulated as to what domain events occurred to generate the signal data the problem solver is attempting to interpret the signal data and determine what caused it <ref> [ Corkill and Lesser, 1981, Erman et al., 1980 ] </ref> .) The specific contributions made by this work include the following. 1. <p> Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system <ref> [ Erman et al., 1980, Hayes-Roth and Lesser, 1977 ] </ref> and the Distributed Vehicle Monitoring Testbed (DVMT) [ Carver and Lesser, 1991, Corkill, 1983, Decker et al., 1989 ] , sophisticated control techniques such as goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser <p> Chapters 4 and 10 present representations of problem solvers that use sophisticated mechanisms such as preconditions, pruning functions, a form of goal processing to focus problem solving activity more efficiently, and approximate processing. The control mechanisms modeled are based on 12 techniques developed for the Hearsay II <ref> [ Erman et al., 1980 ] </ref> , the DVMT [ Corkill, 1983 ] , and a real-time version of the DVMT [ Decker et al., 1990 ] . <p> The IDP/UPC framework is particularly effective for analyzing problem solving systems, such as the extended Hearsay-II <ref> [ Erman et al., 1980 ] </ref> blackboard model introduced by Lesser and Corkill, that integrate both top-down and bottom-up processing in a hierarchy of abstraction spaces [ Corkill et al., 1982, Corkill, 1983 ] . <p> Convergent search spaces are associated with a set of properties referred to as opportunistic. Opportunistic processing was introduced in work on the Hearsay-II project <ref> [ Erman et al., 1980 ] </ref> . Opportunistic processing enables a system to be guided by the most recently discovered results and not by a requirement to satisfy specific subgoals. <p> Opportunistic search enables problem solving to proceed bottom-up from low-level data, top-down from general expectations and abstractions, or either top-down or bottom-up from intermediate results. The advantages of opportunistic processing have been demonstrated in projects including <ref> [ Erman et al., 1980, Corkill, 1983, Durfee, 1987 ] </ref> . The interpretation search spaces presented in this thesis are all convergent search spaces. The general structures that define these spaces are defined in Chapter 3.2. <p> This technique has been used extensively in Operations Research, where it is referred to as Branch-and-Bound [ Papadimitriou and Steiglitz, 1982, Pearl, 1984, Kumar and Kanal, 1988 ] . Top-down and bottom-up architectures have been effectively combined in a number of systems such as blackboard systems <ref> [ Erman et al., 1980, Corkill, 1983 ] </ref> . In these systems, the top-down strategies, such as goal processing techniques, can be considered meta-control architectures because they use abstract or approximate states. <p> In general, control architectures such as the hierarchical problem solving strategies used in <ref> [ Erman et al., 1980 ] </ref> and [ Durfee and Lesser, 1986, Durfee, 1987, Lesser et al., 1989b ] can be modeled by transforming an IDP problem representation to include abstractions and approximations used in control actions in the form of meta-operators. <p> In many interpretation domains, such as speech recognition, components with high individual credibilities are often inconsistent with some or all of the other components of a partial interpretation <ref> [ Erman et al., 1980 ] </ref> . For example, a particular word may be the best interpretation for a particular time-slice of data, but that word may not be consistent with the meaning (i.e., the semantics) of a partial interpretation the problem solver is trying to extend. <p> However, in a situation where the system parameters included a loose grammar and a 1,000 word vocabulary, the results generated by this termination criterion were unacceptable. This set of experiments demonstrated that tight grammars/small vocabularies lead to less ambiguity than loose grammars/large vocabularies <ref> [ Erman et al., 1980 ] </ref> . Like interacting subproblems, bounding functions and monotone properties are structures that can be exploited by a control architecture to reduce the overall cost of problem solving. <p> Thus, if the rating of a potential search operation is heavily dependent on non-local information, the problem solver will not be able to accurately evaluate that action. The effectiveness of preconditions has been demonstrated in many interpretation systems including Hearsay II <ref> [ Erman et al., 1980 ] </ref> and the DVMT [ Corkill, 1983 ] . The original control architecture is shown in Fig. 10.3 for comparison. During each control cycle, new states are created and the problem solver identifies which operators can be applied to the new states. <p> Examples of these in interpretation domains include the clustering mechanisms and associated abstractions used by Durfee and Lesser in [ Durfee and Lesser, 1986 ] and similar mechanisms developed in Hearsay II <ref> [ Erman et al., 1980 ] </ref> . In addition, these strategies are commonly applied to other domains with similar results, such as those demonstrated by Knoblock in planning tasks [ Knoblock, 1991b ] . <p> Knowledge about differentiators can be used both in the design of problem solving architectures and dynamic control algorithms. Architecturally, differentiators can be used to construct special operators for differential diagnosis [ Carver and Lesser, 1993 ] . In control algorithms, differentiators can be used to focus problem solving activity <ref> [ Erman et al., 1980 ] </ref> . In the vehicle tracking grammar, S1, S2, and S3 are strong differentiators for the event V1 (a vehicle location of type 1). <p> Constructive search spaces are often associated with opportunistic problem solving strategies that can use any of the low-level data as a starting point for the derivation of an interpretation. This is also referred to as island driving <ref> [ Erman et al., 1980 ] </ref> . As will be discussed in Chapter 12.7, this technique can result in significant gains in efficiency. However, opportunistic search can also result in large amounts of redundancy [ Lesser et al., 1989b ] . <p> The example shows how different types of correlated and uncorrelated noise and missing data are modeled in real-world domains and how a sophisticated control mechanism, goal processing, is represented in an IDP i grammar. The goal processing mechanism is patterned after goal processing mechanisms introduced in Hearsay II <ref> [ Erman et al., 1980 ] </ref> and the DVMT [ Corkill, 1983 ] . Figure 4.28 on page 85 shows some of the specific problem instances that are generated with this grammar. The use of the feature list convention is shown in Chapter 4.6.
Reference: [ Fox, 1983 ] <author> Mark S. Fox. </author> <title> Constraint-directed Search: A Case Study of Job-Shop Scheduling. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, </institution> <year> 1983. </year> <note> (Also published as Technical Report CMU-CS-83-161, </note> <institution> Computer Science Department, Carnegie-Mellon University, Pitts-burgh, Pennsylvania 15213.). </institution>
Reference-contexts: These include projects such as ABSTRIPS, MOLGEN, and Hearsay-II. Although some of the work was not explicitly presented as being related to the structure of a problem domain, these previous efforts are relevant to the current project. Work done by Fox, et. al <ref> [ Fox, 1983 ] </ref> , explicitly examines the structure of the problem domain. For problems defined as constrained heuristic search (CHS), i.e., problems that combine the process of constraint satisfaction with heuristic search, Fox identifies eight texture measures that define a problem's topology. <p> One such feature is what we will call arity. Issues related to arity are associated with the frequency of certain domain events and the related implications for the branching factor of the resulting search space. As many researchers including Fox and Kanal have noted <ref> [ Fox, 1983, Kumar and Kanal, 1988 ] </ref> , the order in which a problem solver attempts to solve subproblems can have a significant impact on the overall cost of problem solving. <p> Such a plan would have well defined tasks that correspond to base space operators as well as mechanisms for verifying that the plan was working. Constraint Directed Search A hybrid approach involves treating the projection space solution as a constraint network, such as the constraint networks defined by Fox <ref> [ Fox, 1983 ] </ref> . Selective refinement is used to map portions of the projection space solution back to the base search space, or sophisticated control based search can be used to achieve a similar result.
Reference: [ Fu, 1982 ] <author> King Sun Fu. </author> <title> Syntactic Pattern Recognition and Applications. </title> <publisher> PH, </publisher> <year> 1982. </year>
Reference-contexts: In addition, the use of formal grammars and the associated graph structures as a basis for analyzing interpretation problems and for constructing problem solving systems is a common approach. One of the more significant uses of formal grammars is presented by Fu in <ref> [ Fu, 1982 ] </ref> . Fu uses a representation he formalizes as a stochastic grammar that is superficially similar to the component structure of the IDP/UPC framework. However, Fu's work differs widely from this work in both representation and analysis.
Reference: [ Garvey and Lesser, 1993 ] <author> Alan Garvey and Victor Lesser. </author> <title> Design-to-time real-time scheduling. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 23(6) </volume> <pages> 1491-1502, </pages> <year> 1993. </year>
Reference-contexts: of the blackboard architecture, many research projects have focused on the development of control mechanisms that focus or limit search in such a way that the overall cost of problem solving is reduced and a consistent level of quality or correctness is either maintained or altered in a well-defined manner <ref> [ Decker et al., 1990, Garvey and Lesser, 1993 ] </ref> . 6 Given the success of some sophisticated blackboard-based interpretation control mechanisms, there is an obvious desire to understand the underlying principles and domain properties in order to generalize the techniques to other domains.
Reference: [ Gazdar et al., 1982 ] <author> Gerald Gazdar, Geoffrey K. Pullum, and Ivan A. Sag. </author> <title> Auxiliaries and related phenomena in a restrictive theory of grammar. </title> <booktitle> Language, </booktitle> <volume> 58(3) </volume> <pages> 591-638, </pages> <year> 1982. </year>
Reference-contexts: In Chapter 4.7, we describe such a domain more fully and we discuss the techniques that can be used to construct IDP grammars capable of modeling context-sensitive elements of a problem domain. These techniques are primarily based on exploiting the feature list convention <ref> [ Gazdar et al., 1982, Knuth, 1968 ] </ref> presented in [ Whitehair and Lesser, 1993 ] . Similarly, the analysis tools are also dependent on an accurate representation of the problem solving architecture and the relationships among potential problem solving operators. <p> Semantics associated with each production rule determine the actual domain interpretation. The semantic functions will typically make use of grammar element attributes that are not used 42 by syntactic functions. These attributes will be represented using the feature list convention described by Gazdar, et al. <ref> [ Gazdar et al., 1982 ] </ref> and similar to the attribute grammars of Knuth [ Knuth, 1968 ] . This is an important point. <p> to which an operator appears to be applied. i.e., an implicit state is one that contains exactly 1 In applying the IDP/UPC framework to more complex, real-world domains, we will use an augmented version of a grammar that makes use of the feature list convention discussed by Gazdar, et al., <ref> [ Gazdar et al., 1982 ] </ref> and Knuth [ Knuth, 1968 ] . This will increase the expressiveness of the grammar and enable it to model real world events more accurately, but it will not invalidate the analysis techniques that will be discussed later in this thesis. <p> The conditional probabilities and expected cost components of the UPC vectors are computed a priori. The domain characteristics that change from run to run are represented with the feature list convention <ref> [ Gazdar et al., 1982, Knuth, 1968 ] </ref> . Using this testbed, we have conducted two sets of verification/validation experiments using the grammars shown in Figures 7.1 and 7.2.
Reference: [ Genesereth and Smith, 1982 ] <author> Michael R. Genesereth and David E. Smith. </author> <title> Meta-level architecture. </title> <type> Technical report, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> California 94305, </address> <month> December </month> <year> 1982. </year>
Reference: [ Genesereth, 1983 ] <author> Michael R. Genesereth. </author> <title> An overview of meta-level architecture. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 119-124, </pages> <address> Washington, D.C., </address> <month> August </month> <year> 1983. </year>
Reference: [ Hayes-Roth and Hayes-Roth, 1979 ] <author> Barbara Hayes-Roth and Frederick Hayes-Roth. </author> <title> A cognitive model of planning. </title> <journal> Cognitive Science, </journal> <volume> 3(4) </volume> <pages> 275-310, </pages> <month> October-December </month> <year> 1979. </year>
Reference: [ Hayes-Roth and Lesser, 1977 ] <author> Frederick Hayes-Roth and Victor R. Lesser. </author> <title> Focus of attention in the Hearsay-II system. </title> <booktitle> In Proceedings of the Fifth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 27-35, </pages> <address> Tiblisi, Georgia, USSR, </address> <month> August </month> <year> 1977. </year>
Reference-contexts: Such techniques include the focus of control mechanisms introduced in the Hearsay-II speech understanding system <ref> [ Erman et al., 1980, Hayes-Roth and Lesser, 1977 ] </ref> and the Distributed Vehicle Monitoring Testbed (DVMT) [ Carver and Lesser, 1991, Corkill, 1983, Decker et al., 1989 ] , sophisticated control techniques such as goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser
Reference: [ Hayes-Roth, 1985 ] <author> Barbara Hayes-Roth. </author> <title> A blackboard architecture for control. </title> <journal> Artificial Intelligence, </journal> <volume> 26(3) </volume> <pages> 251-321, </pages> <month> July </month> <year> 1985. </year> <month> 311 </month>
Reference: [ Hudlick a and Lesser, 1984 ] <author> Eva Hudlick a and Victor R. Lesser. </author> <title> Meta-level control through fault detection and diagnosis. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 153-161, </pages> <address> Austin, Texas, </address> <month> August </month> <year> 1984. </year>
Reference: [ III, 1990 ] <author> Norman F. Carver III. </author> <title> Sophisticated Control for Interpretation: Planning to Resolve Sources of Uncertainty. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Interpretation is a form of constructive problem solving based on abductive inferencing <ref> [ III, 1990 ] </ref> . Interpretation is similar to a closely related form of problem solving, classification, and to a more distant form of problem solving, parsing. <p> It will be used in this and the following sections to illustrate important aspects of the IDP/UPC framework. The interpretation domain that will be studied in detail in Chapter 11 is a vehicle tracking problem as defined in previous work on distributed vehicle monitoring in the RESUN research project <ref> [ III, 1990 ] </ref> and in the Distributed Vehicle Monitoring Testbed (DVMT) [ Corkill, 1983 ] . The problem solver's input consists of preprocessed sensor data gathered from a single contiguous region. <p> This is a significant observation because it can be used in the design of meta-operators and problem solving strategies. For example, problem solving can be viewed as the production and application of constraint. As discussed by a number of researchers, including Berliner [ Berliner, 1979 ] and Carver <ref> [ III, 1990 ] </ref> , search can be thought of in terms of either a find best or a disprove rest strategy.
Reference: [ Johnson and Hayes-Roth, 1987 ] <author> M. Vaughn Johnson and Barbara Hayes-Roth. </author> <title> Integrating diverse reasoning methods in the BB1 blackboard control architecture. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 30-35, </pages> <address> Seattle, Washington, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: Previous research indicates that this is a reasonable perspective. For example, the BB1 system was implemented using a similar approach in which sophisticated control mechanisms were represented as problem solving operators that were explicitly considered by the scheduler <ref> [ Johnson and Hayes-Roth, 1987 ] </ref> . 35 2.3 Representing Complex Domains The analysis framework depends on an explicit representation of the abstract and approximate search spaces that are used by a problem solvers meta-operators. Other researchers have reported analyses of the structures of abstract and approximate search spaces.
Reference: [ Knoblock, 1991a ] <author> Craig A. Knoblock. </author> <title> Automatically Generating Abstractions for Problem Solving. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <year> 1991. </year> <note> (Also published as Technical Report CMU-CS-91-120, </note> <institution> School of Computer Science, Carnegie Mellon University.). </institution>
Reference-contexts: Such domains are characteristic of real-world domains such as signal interpretation, robotic audition, image processing, and natural language processing. 4.8.1 Goal Processing Goal processing is a form of hierarchical problem solving <ref> [ Knoblock, 1991a ] </ref> that has been incorporated in the DVMT to enable a problem solver to reason about courses of action in ways that are independent of the means for instantiating the actions.
Reference: [ Knoblock, 1991b ] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 686-691, </pages> <address> San Diego, California, </address> <month> July </month> <year> 1991. </year>
Reference-contexts: Other researchers have reported analyses of the structures of abstract and approximate search spaces. For example, Knoblock discusses the maximum potential reduction in problem solving costs that can be achieved with the use of abstract and approximate search spaces <ref> [ Knoblock, 1991b ] </ref> . However, his analysis does not explain how these reductions might be achieved and his techniques do not address the issues presented in Chapter 1. <p> This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell [ Newell et al., 1962 ] , Minsky [ Minsky, 1963 ] , and Knoblock <ref> [ Knoblock, 1991b ] </ref> . 8.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures. <p> In addition, these strategies are commonly applied to other domains with similar results, such as those demonstrated by Knoblock in planning tasks <ref> [ Knoblock, 1991b ] </ref> .
Reference: [ Knoblock, 1994 ] <author> Craig A. Knoblock. </author> <title> Automatically generating abstractions for planning. </title> <journal> Artificial Intelligence, </journal> <volume> 68 </volume> <pages> 243-302, </pages> <year> 1994. </year>
Reference-contexts: + DISTANCE ( j ; k) / k COMBINE CS j and CS k such that tolerance i;j is minimized ENDREPEAT 190 formal representation of approximation techniques that is described in the following sections is therefore similar to Knoblock's approach in that it is based on dropping or simplifying constraints <ref> [ Knoblock, 1994 ] </ref> . More specifically, eliminating corroborating support is an approximation strategy we have defined for interpretation domains that avoids some of the processing that would otherwise be used to generate a precise interpretation.
Reference: [ Knuth, 1968 ] <author> D. Knuth. </author> <title> Semantics of context-free grammars. </title> <journal> Mathematical Systems Theory, </journal> <volume> 2(2) </volume> <pages> 127-145, </pages> <year> 1968. </year>
Reference-contexts: In Chapter 4.7, we describe such a domain more fully and we discuss the techniques that can be used to construct IDP grammars capable of modeling context-sensitive elements of a problem domain. These techniques are primarily based on exploiting the feature list convention <ref> [ Gazdar et al., 1982, Knuth, 1968 ] </ref> presented in [ Whitehair and Lesser, 1993 ] . Similarly, the analysis tools are also dependent on an accurate representation of the problem solving architecture and the relationships among potential problem solving operators. <p> In both cases, mechanical design tasks are solved by embedding knowledge about how to design certain artifacts in a grammar and then treating the design task as a form of natural language generation. The foundation for this approach was established by Knuth and the concept of an attribute grammar <ref> [ Knuth, 1968 ] </ref> . <p> The semantic functions will typically make use of grammar element attributes that are not used 42 by syntactic functions. These attributes will be represented using the feature list convention described by Gazdar, et al. [ Gazdar et al., 1982 ] and similar to the attribute grammars of Knuth <ref> [ Knuth, 1968 ] </ref> . This is an important point. <p> i.e., an implicit state is one that contains exactly 1 In applying the IDP/UPC framework to more complex, real-world domains, we will use an augmented version of a grammar that makes use of the feature list convention discussed by Gazdar, et al., [ Gazdar et al., 1982 ] and Knuth <ref> [ Knuth, 1968 ] </ref> . This will increase the expressiveness of the grammar and enable it to model real world events more accurately, but it will not invalidate the analysis techniques that will be discussed later in this thesis. <p> The conditional probabilities and expected cost components of the UPC vectors are computed a priori. The domain characteristics that change from run to run are represented with the feature list convention <ref> [ Gazdar et al., 1982, Knuth, 1968 ] </ref> . Using this testbed, we have conducted two sets of verification/validation experiments using the grammars shown in Figures 7.1 and 7.2.
Reference: [ Kumar and Kanal, 1988 ] <editor> Vipin Kumar and Laveen N. Kanal. </editor> <title> The CDP: A unifying formulation for heuristic search, dynamic programming, and branch-and-bound. </title> <editor> In L. Kanal and V. Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, Symbolic computation, chapter 1, </booktitle> <pages> pages 1-27. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Extended studies and analyses of the work on search techniques for restricted domains have resulted in the identification of a taxonomy of search procedures by Kanal and Kumar 31 <ref> [ Kumar and Kanal, 1988 ] </ref> . Each of the procedural categories specified by Kanal and Kumar is defined in terms of the fundamental structure of the search spaces to which the procedures included in the category can be successfully applied. <p> Kanal and Kumar's taxonomy links heuristic search techniques with problem solving techniques that are more closely associated with operations research (OR). This work has shown that basic AI search techniques can be classified as belonging to a subcategory of either branch-and-bound procedures or dynamic programming procedures <ref> [ Kumar and Kanal, 1988 ] </ref> . More recently, AI problem solving has ventured into complex domains such as speech recognition, natural language processing, vision processing, pattern recognition, etc., that differ from the earlier, more restricted domains in two important ways. <p> These structures do not exist in the more complex domains and most of the search and pruning techniques that were developed in earlier work, such as AO fl , B fl , SSS fl , Alpha-Beta, and their generalizations, are not applicable <ref> [ Kumar and Kanal, 1988 ] </ref> . There are a variety of reasons for this. The most significant is that complex problem formulations do not exhibit monotone properties and structures. <p> Kanal and Kumar's taxonomy only classifies problems that are, minimally, monotone. As discussed by Kanal and Kumar <ref> [ Kumar and Kanal, 1988 ] </ref> , in order for AO fl , alpha-beta, B fl , SSS fl and their generalizations to be applicable to a given domain, the problem formulation for that domain must exhibit monotonic properties. <p> This suggests that the applicability of this work is limited by the extent to which a problem's solution space can be represented as the language generated by a grammar. Also, the IDP/UPC framework extends the formalization work of others, most notably Kanal and Kumar <ref> [ Kumar and Kanal, 1988 ] </ref> and, in so doing, helps integrate problems from artificial intelligence with operations research problems in a unified framework. <p> This approach is similar to that used in the Composite Decision Process (CDP) model of Kanal and Kumar <ref> [ Kumar and Kanal, 1988 ] </ref> . In these situations, interpretations take the form of derivation trees of X and the constructive search operators used in interpretation problems are viewed as production rules of G. G, therefore, defines how interpretations are decomposed. <p> However, we will 51 often use approximations of these techniques for computability reasons. Chapter 6.7 presents formal definitions and examples of structural interactions. 3.4 Interpretation Problem Solving and Formal Problem Solving Paradigms As discussed by Kanal and Kumar in <ref> [ Kumar and Kanal, 1988 ] </ref> , formal problem solving paradigms that are based on search techniques can be divided into two general categories, either top-down or bottom-up. The IDP/UPC framework is consistent with this and supports the analysis of control architectures from either a top-down or a bottom-up perspective. <p> Though this example was kept intentionally simple to illustrate the representation used in the IDP formalism, subsequent examples in will be based on very similar analysis. 4.5 Non-Monotonicity and Bounding Functions Monotonicity is a property of problem structures that has been used to construct many important search control architectures <ref> [ Kumar and Kanal, 1988 ] </ref> . Though there is no guarantee that a monotone problem can be solved efficiently, most existing efficient control architectures such as AO fl , alpha-beta, B fl , SSS fl and their generalizations are only applicable in monotone domains. <p> As a consequence, the resulting 74 extension may have an arbitrarily low-rated credibility depending on how ridiculous it is. 4.5.2 Bounding Functions In certain monotone domains, problem solving is simplified by the fact that optimal subproblem solutions are guaranteed to be components of the optimal solution <ref> [ Kumar and Kanal, 1988 ] </ref> . In non-monotone interpretation problems, this guarantee obviously does not hold. However, based on the problem structure defined by a domain's characteristic grammar, G, and evaluation function f , certain bounding functions can be defined. <p> One such feature is what we will call arity. Issues related to arity are associated with the frequency of certain domain events and the related implications for the branching factor of the resulting search space. As many researchers including Fox and Kanal have noted <ref> [ Fox, 1983, Kumar and Kanal, 1988 ] </ref> , the order in which a problem solver attempts to solve subproblems can have a significant impact on the overall cost of problem solving. <p> Furthermore, in developing the IDP/UPC framework, this thesis develops a better understanding of sophisticated problem solving and its relationship to other forms of problem solving. The IDP/UPC framework was designed intentionally as an extension to the framework introduced by Kanal and Kumar <ref> [ Kumar and Kanal, 1988 ] </ref> and used to develop a taxonomy of search-based problem solving techniques. <p> An extended definition of monotonicity is presented in Chapter 4.5. This definition is an extension of a similar definition from Kanal and Kumar <ref> [ Kumar and Kanal, 1988 ] </ref> and it clearly distinguishes AI problem domains from other domains in which search-based problem solvers are used. Specifically, Kanal and Kumar use their definition of monotonicity to differentiate between a variety of search techniques.
Reference: [ Lesser and Corkill, 1983 ] <author> Victor Lesser and Daniel Corkill. </author> <title> The Distributed Vehicle Monitoring Testbed: A tool for investigating distributed problem solving networks. </title> <journal> AI Magazine, </journal> <volume> 4(3) </volume> <pages> 15-33, </pages> <month> Fall </month> <year> 1983. </year> <note> (Also to appear in Blackboard Systems, </note> <editor> Robert S. Engelmore and Anthony Morgan, editors, </editor> <title> Addison-Wesley, </title> <note> in press, 1988 and in Readings from AI Magazine 1980-1985, in press, </note> <year> 1988). </year>
Reference-contexts: The system shown in Fig. 1.3 is a high-level schematic for the integrated data-directed and goal-directed control architecture as implemented in the DVMT <ref> [ Lesser and Corkill, 1983, Lesser et al., 1987 ] </ref> . The basic blackboard architecture is modified to include a goal blackboard and a goal processor. The goal blackboard, which mirrors the data blackboard in dimensionality, contains goals representing intentions to create particular results on the data blackboard.
Reference: [ Lesser and Pavlin, 1988 ] <author> Victor R. Lesser and Jasmina Pavlin. </author> <title> Performing approximate processing to address real-time constraints. </title> <type> COINS Technical Report 87-126, </type> <institution> University of Massachusetts, </institution> <year> 1988. </year>
Reference-contexts: Vehicle Monitoring Testbed (DVMT) [ Carver and Lesser, 1991, Corkill, 1983, Decker et al., 1989 ] , sophisticated control techniques such as goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser et al., 1989b ] , and abstracting and approximating computational domain theories <ref> [ Decker et al., 1990, Lesser and Pavlin, 1988 ] </ref> . the sophisticated control mechanisms studied in this thesis. <p> In particular, through experimentation and analysis, many commonalities between disparate domains are being discovered that seem to indicate that certain control architectures can be used to form a basis for a very powerful and general form of problem solving. Specifically, it is becoming apparent that approximate processing <ref> [ Lesser and Pavlin, 1988, Decker et al., 1990 ] </ref> and goal processing [ Corkill and Lesser, 1981, Corkill et al., 1982, Lesser et al., 1989a, Lesser et al., 1989b ] are forms of processing that can be generalized to many real-world domains.
Reference: [ Lesser et al., 1987 ] <author> Victor R. Lesser, Daniel D. Corkill, and Edmund H. Durfee. </author> <title> An update on the Distributed Vehicle Monitoring Testbed. </title> <type> Technical Report 87-111, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: The system shown in Fig. 1.3 is a high-level schematic for the integrated data-directed and goal-directed control architecture as implemented in the DVMT <ref> [ Lesser and Corkill, 1983, Lesser et al., 1987 ] </ref> . The basic blackboard architecture is modified to include a goal blackboard and a goal processor. The goal blackboard, which mirrors the data blackboard in dimensionality, contains goals representing intentions to create particular results on the data blackboard.
Reference: [ Lesser et al., 1988a ] <author> Victor Lesser, Jasmina Pavlin, and Edmund Durfee. </author> <title> Approximate processing in real-time problem solving. </title> <journal> AI Magazine, </journal> <volume> 9(1) </volume> <pages> 49-61, </pages> <month> Spring </month> <year> 1988. </year>
Reference-contexts: These strategies are defined and discussed in <ref> [ Decker et al., 1990, Lesser et al., 1988a ] </ref> . The specific techniques described here are eliminating corroborating support and level hopping. These techniques are also defined and discussed in [ Decker et al., 1990, Lesser et al., 1988a ] . <p> These strategies are defined and discussed in <ref> [ Decker et al., 1990, Lesser et al., 1988a ] </ref> . The specific techniques described here are eliminating corroborating support and level hopping. These techniques are also defined and discussed in [ Decker et al., 1990, Lesser et al., 1988a ] . Both general strategies are based on reducing the amount of the search space that is explored in generating an interpretation. As a consequence, the resulting interpretation is considered an abstract state in a projection space.
Reference: [ Lesser et al., 1988b ] <author> Victor R. Lesser, Jasmina Pavlin, and Edmund Durfee. </author> <title> Approximate processing in real-time problem solving. </title> <journal> AI Magazine, </journal> <volume> 9(1) </volume> <pages> 49-61, </pages> <month> Spring </month> <year> 1988. </year> <month> 312 </month>
Reference-contexts: The results of searching the abstract space can be mapped back to the original space and used to enhance problem solving in that space. Experiments with approximate processing <ref> [ Lesser et al., 1988b, 149 Decker et al., 1990 ] </ref> showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms.
Reference: [ Lesser et al., 1989a ] <author> V. R. Lesser, D. D. Corkill, R. C. Whitehair, and J. A. Hernandez. </author> <title> Focus of control through goal relationships. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: Redundancy provides a problem solver with flexibility in choosing problem-solving activities but also allows results to be rederived using alternative paths, possibly without recognizing the redundancy until the last step. Various studies have shown that the cost associated with redundancy can be substantial <ref> [ Lesser et al., 1989a ] </ref> . Figure 4.13 shows an example of redundancy. (Note that this grammar is not the same as the grammar that is being used to illustrate other phenomena. <p> Definition 4.4.4 Subproblem Relationships and Interactions If a function, r, is defined in terms of the Component Sets and/or Result Sets of two or more subproblems, we say that r defines a relationship (or interaction) between the subproblems. 68 69 In <ref> [ Lesser et al., 1989a ] </ref> a number of relationships were defined and their use in controlling problem solving was discussed. <p> In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool <ref> [ Lesser et al., 1989a, Lesser et al., 1989b ] </ref> . Subsequent work exploited this observation and resulted in significant performance improvements [ Decker et al., 1989 ] .
Reference: [ Lesser et al., 1989b ] <author> V. R. Lesser, R. C. Whitehair, D. D. Corkill, and J. A. Hernandez. </author> <title> Goal relationships and their use in a blackboard architecture. </title> <editor> In V. Jagannathan, Rajendra Dodhiawala, and Lawrence Baum, editors, </editor> <booktitle> Blackboard Architectures and Applications, </booktitle> <pages> pages 9-26. </pages> <publisher> Academic Press, Inc., </publisher> <year> 1989. </year>
Reference-contexts: This unified representation views meta-operators 4 used in sophisticated control architectures as mechanisms that evaluate problem solving actions by selectively applying a process that examines a search path's relationship with other, possibly interacting, search paths <ref> [ Lesser et al., 1989b ] </ref> . (Relationships are based on the distribution of domain events and are statistical in nature. <p> In contrast, sophisticated control architectures evaluate problem solving actions by selectively applying a process that examines a search path's relationship with other, possibly interacting, search paths <ref> [ Lesser et al., 1989b ] </ref> . This process must be applied selectively in order to prevent a combinatorial increase in cost that would result from examining the relationships between every possible set of interacting search paths. <p> This grammar is based loosely on the vehicle tracking domain of the Distributed Vehicle Monitoring Testbed (DVMT) [ Corkill, 1983 ] . Figure 1.12 shows the same grammar modified to include a class of meta-level operators referred to as goal operators presented in <ref> [ Lesser et al., 1989b ] </ref> . This grammar is analyzed at length in Chapter 4.8. 23 24 1.9 Demonstrating the Analysis of Heuristic Control Chapter 7 showed how the IDP and UPC formalisms could be used to correctly predict problem solver performance. <p> In the case of cooperating paths, potential solutions to a subproblem impose constraints on sibling subproblems. These ideas are very important in the IDP/UPC framework and a more extensive and formal description is discussed in Chapter 4 and in previous work of mine in <ref> [ Lesser et al., 1989b ] </ref> . <p> Though the majority of discussion related to interacting subproblems is contained in later chapters and other publications <ref> [ Lesser et al., 1989b ] </ref> , their definitions and representations are included here for consistency. 4.4.1 Defining Interacting Subproblems Formal definitions of interacting subproblems will be based on the following definitions of component set and result set. <p> In this simple example, goal processing has clear performance advantages. However, it is still unclear as to when, in general, goal processing is effective and when it is detrimental to performance. For example, in the DVMT, an analysis indicated that goal processing is not always an effective tool <ref> [ Lesser et al., 1989a, Lesser et al., 1989b ] </ref> . Subsequent work exploited this observation and resulted in significant performance improvements [ Decker et al., 1989 ] . <p> Research The effort to formally incorporate notions of projected or abstracted search spaces into the traditional model is based on Approximate Processing concepts described in [ Erman et al., 1980, Lesser and Pavlin, 1988, Lesser et al., 1988b, Decker et al., 1990 ] and on goal processing concepts described in <ref> [ Lesser et al., 1989b ] </ref> . Approximate processing is based on exploiting the structure of a search space to form abstractions of the space with well understood effects. <p> This is also referred to as island driving [ Erman et al., 1980 ] . As will be discussed in Chapter 12.7, this technique can result in significant gains in efficiency. However, opportunistic search can also result in large amounts of redundancy <ref> [ Lesser et al., 1989b ] </ref> . Markers can be used to reduce this uncertainty by identifying paths that can be pruned a priori without the risk of eliminating the ability to connect the search path.
Reference: [ Lesser et al., 1993 ] <author> Victor Lesser, Hamid Nawab, Izaskun Gallastegi, and Frank Klassner. IPUS: </author> <title> An architecture for integrated signal processing and signal interpretation in complex environments. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: The characteristic signals are present, but are below some threshold and are not `heard' by the sensors. Missing data also results from inappropriately processed low-level data <ref> [ Lesser et al., 1993 ] </ref> . For example, a filtering algorithm may be used with a poor selection of tuning parameters resulting in the loss of significant data.
Reference: [ Lowrance and Garvey, 1982 ] <author> John D. Lowrance and Thomas D. Garvey. </author> <title> Evidential reasoning: A developing concept. </title> <booktitle> IEE 1982 Proceedings of the International Conference on Cybernetics and Society, </booktitle> <pages> pages 6-9, </pages> <year> 1982. </year>
Reference-contexts: To represent uncertainty caused by the use of approximate search, data, and knowledge, the representation of credibility must be expanded to a four-valued system. The new belief system was derived from evidential reasoning <ref> [ Lowrance and Garvey, 1982 ] </ref> and is similar to that in RUM [ Bonissone et al., 1987 ] . The belief in a hypothesis is now represented by a measure of positive belief (certainty) and of negative belief (refutation). <p> Figure C.1 introduces a graphic representation of the four-valued belief system. Furthermore, second-order relationships between the elements of the belief system can also be used to control problem solving. Specifically, various measures of ignorance <ref> [ Lowrance and Garvey, 1982 ] </ref> and conflict can be computed. Ignorance measures, such as (plausibility certainty), (doubt ref utation), (1 plausibility), (1 doubt) and (1 (certainty + ref utation)), indicate the amount of useful work that can be done to refine the belief in a hypothesis.
Reference: [ Minsky, 1963 ] <author> Marvin Minsky. </author> <title> Steps towards artificial intelligence. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 406-450. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell [ Newell et al., 1962 ] , Minsky <ref> [ Minsky, 1963 ] </ref> , and Knoblock [ Knoblock, 1991b ] . 8.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures.
Reference: [ Mullins and Rinderle, 1991a ] <author> Scott Mullins and James Rinderle. </author> <title> Grammatical approaches to engineering design, part I: An introduction and commentary. </title> <booktitle> Research in Engineering Design, </booktitle> <volume> 2 </volume> <pages> 121-135, </pages> <year> 1991. </year>
Reference-contexts: Other work indicates that context-free grammars can be exploited in a top-down manner as well. For example, impressive results have been achieved in automated design tasks by researchers such as Campbell [ Campbell et al., 1991 ] and Mullins and Rinderle <ref> [ Mullins and Rinderle, 1991a, Mullins and Rinderle, 1991b ] </ref> who use grammatical approaches to engineering design. In both cases, mechanical design tasks are solved by embedding knowledge about how to design certain artifacts in a grammar and then treating the design task as a form of natural language generation.
Reference: [ Mullins and Rinderle, 1991b ] <author> Scott Mullins and James Rinderle. </author> <title> Grammatical approaches to engineering design, part II: Melding configuration adn parametric design using atrribute grammars. </title> <booktitle> Research in Engineering Design, </booktitle> <volume> 2 </volume> <pages> 137-146, </pages> <year> 1991. </year>
Reference-contexts: Other work indicates that context-free grammars can be exploited in a top-down manner as well. For example, impressive results have been achieved in automated design tasks by researchers such as Campbell [ Campbell et al., 1991 ] and Mullins and Rinderle <ref> [ Mullins and Rinderle, 1991a, Mullins and Rinderle, 1991b ] </ref> who use grammatical approaches to engineering design. In both cases, mechanical design tasks are solved by embedding knowledge about how to design certain artifacts in a grammar and then treating the design task as a form of natural language generation.
Reference: [ Newell et al., 1962 ] <author> A. Newell, J. C. Shaw, and H. a. Simon. </author> <title> The process of creative thinking. </title> <booktitle> In Contemporary Approaches to Creative Thinking, </booktitle> <pages> pages 63-119. </pages> <publisher> Atherton Press, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: Experiments with approximate processing [ Lesser et al., 1988b, 149 Decker et al., 1990 ] showed that significant efficiencies can be gained with careful exploitation of approximate processing mechanisms. This paradigm can be viewed as a form of hierarchical problem solving, such as that discussed by Newell <ref> [ Newell et al., 1962 ] </ref> , Minsky [ Minsky, 1963 ] , and Knoblock [ Knoblock, 1991b ] . 8.2 Formalizing Projection Spaces The extended UPC formalism is intended to explicitly represent characteristics of search spaces that are used implicitly in control architectures.
Reference: [ Newell et al., 1963 ] <author> A. Newell, J. C. Shaw, and H. a. Simon. </author> <title> Empirical explorations with the logic theory machine: A case history of heuristics. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 109-133. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following sections. 2.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing [ Berliner, 1979, Samuel, 1963 ] , and theorem proving <ref> [ Newell et al., 1963 ] </ref> . The heuristic knowledge used in the search process was relatively limited.
Reference: [ Papadimitriou and Steiglitz, 1982 ] <author> Christos H. Papadimitriou and Kenneth Steiglitz. </author> <title> Combinatorial Optimization: Algorithms and Complexity. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: The objective strategy of a problem solver can be thought of as being analogous to the objective function of problem solving strategies such as the simplex algorithm <ref> [ Papadimitriou and Steiglitz, 1982 ] </ref> . In a typical analysis situation, the object of consideration will be the control architecture - the algorithm (s) used by a problem solver to choose its next problem solving action. Control architectures will be expressed in terms of the problem structure.
Reference: [ Pearl, 1984 ] <author> Judea Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> first edition, </address> <year> 1984. </year>
Reference-contexts: Viewing interpretation problems as discrete optimization problems sets the analysis framework apart from previous analysis techniques that are used to analyze problem solving in domains where the objective is to find the shortest, or lowest-cost search path, the highest-rated solution, or a solution path to a winning position <ref> [ Pearl, 1984 ] </ref> . 1.2 Formally Specifying Domain Structures and Problem Solving Architectures Chapter 4 demonstrates how the IDP/UPC framework can formally represent problem domains and problem solving architectures. <p> In research projects involving restricted domains, the emphasis was on developing algorithms that enabled the problem solver to prune paths based on intermediate problem solving results [ Berliner, 1979, Kumar and Kanal, 1988, Pearl, 1984, Stockman, 1979 ] . Previously, Pearl <ref> [ Pearl, 1984 ] </ref> has shown that statistical properties of a problem solving technique, such as expected cost, can be determined from an analysis of the structure of a graphical representation of the search paths explored by the problem solver. <p> In particular, Pearl examines the effects of pruning operators and heuristics for ordering the application of operators in various game playing domains and in other restricted domains. In contrast to the analysis techniques, the analysis techniques described by Pearl in <ref> [ Pearl, 1984 ] </ref> do not take into consideration dynamic subproblem interactions or the long-term effects of an action.
Reference: [ Samuel, 1963 ] <author> A. L. Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <editor> In E. A. Fiegenbaum and J. Feldman, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 71-105. </pages> <publisher> McGraw-Hill, </publisher> <year> 1963. </year>
Reference-contexts: Figure 2.2 summarizes these topics and illustrates their relationships to each other relative to monotone and non-monotone domains, which are also defined and discussed in the following sections. 2.1 Complex and Restricted Problem Domains Early work on the search paradigm was restricted to constrained domains such as game playing <ref> [ Berliner, 1979, Samuel, 1963 ] </ref> , and theorem proving [ Newell et al., 1963 ] . The heuristic knowledge used in the search process was relatively limited.
Reference: [ Simon, 1969 ] <author> Herbert A. Simon. </author> <booktitle> The Sciences of the Artificial. </booktitle> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: For example, for a speech understanding system, there are implied requirements that the system respond to spoken input in a timely fashion. 280 Situations where this strategy is appropriate are sometimes described as satisficing problems <ref> [ Simon, 1969 ] </ref> . For interpretation tasks, satisficing strategies are based on the assumption that a good interpretation, i.e., one that is within some * of the optimal (or correct) interpretation, has a semantic interpretation that is very similar to that of the correct interpretation. <p> This process can continue until a solution is determined. 282 Approximate Processing (Satisficing) Another direct method for using the results of projection space search is to return them as the actual result of problem solving. This method is applicable for domains that admit satisficing solutions <ref> [ Simon, 1969 ] </ref> . In particular, approximate processing can be used when an acceptable solution can sacrifice precision, certainty or completeness [ Lesser and Pavlin, 1988, Lesser et al., 1988b, Decker et al., 1990 ] .
Reference: [ Stefik, 1981 ] <author> Mark Stefik. </author> <title> Planning and meta-planning (MOLGEN: Part 2). </title> <journal> Artificial Intelligence, </journal> <volume> 16 </volume> <pages> 141-170, </pages> <year> 1981. </year> <month> 313 </month>
Reference: [ Stockman, 1979 ] <author> G. C. Stockman. </author> <title> A minimax algorithm better than Alpha-Beta? Artificial Intelligence, </title> <booktitle> 12 </booktitle> <pages> 179-196, </pages> <year> 1979. </year>
Reference: [ V.R.Lesser et al., 1975 ] <author> V.R.Lesser, R.D.Fennell, L.D.Erman, and D.R.Reddy. </author> <title> Organization of the hearsay-ii speech understanding system. </title> <journal> In IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> ASSP-23, </volume> <pages> pages 11-23, </pages> <month> January </month> <year> 1975. </year>
Reference-contexts: Later, they were always executed immediately after being created. As a result, in later presentations, preconditions were not considered to be separate knowledge sources <ref> [ V.R.Lesser et al., 1975 ] </ref> . Thus, the treatment of preconditions here is reasonable and, as stated above, the problem solver forces preconditions to be executed before any domain actions.
Reference: [ Whitehair and Lesser, 1993 ] <author> Robert C. Whitehair and Victor R. Lesser. </author> <title> A Framework for the Analysis of Sophisticated Control in Interpretation Systems. </title> <type> Technical Report 93-53, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <address> Amherst, Massachusetts 01003, </address> <year> 1993. </year>
Reference-contexts: These techniques are primarily based on exploiting the feature list convention [ Gazdar et al., 1982, Knuth, 1968 ] presented in <ref> [ Whitehair and Lesser, 1993 ] </ref> . Similarly, the analysis tools are also dependent on an accurate representation of the problem solving architecture and the relationships among potential problem solving operators. <p> These figures show the production rules of the grammar and the probability distribution values for associated with each of the rules. The feature lists are shown as bracketed subscripts. The key to interpreting the feature lists and functions of feature lists is the following <ref> [ Whitehair and Lesser, 1993 ] </ref> : f: A feature list. <p> The technique employed in this work is based on transforming non-singularities to singularities by mapping the rules of IDP i to a new grammar. (The grammar mapping technique was introduced in <ref> [ Whitehair and Lesser, 1993 ] </ref> as a means for calculating a variety of measures, including potential.) The frequencies of the elements of the new grammar are then used to determine the frequency of elements of IDP i . <p> The power of comparative analysis is based on the context-free nature of the IDP g and IDP i grammars. As discussed in Chapters 4.7 and 10 of this thesis and in <ref> [ Whitehair and Lesser, 1993 ] </ref> , the implication of a context-free grammar is that the subproblems represented by different interpretation/generation subtrees interact in strictly defined ways.
Reference: [ Wilensky, 1981 ] <author> Robert Wilensky. Meta-planning: </author> <title> Representing and using knowledge about planning in problem solving and natural language understanding. </title> <journal> Cognitive Science, </journal> <volume> 5(3) </volume> <pages> 197-233, </pages> <month> July-September </month> <year> 1981. </year>
References-found: 55

