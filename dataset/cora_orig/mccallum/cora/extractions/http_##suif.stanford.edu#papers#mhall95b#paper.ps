URL: http://suif.stanford.edu/papers/mhall95b/paper.ps
Refering-URL: http://suif.stanford.edu/papers/papers.html
Root-URL: 
Title: Interprocedural Analysis for Parallelization  
Author: Mary W. Hally, Brian R. Murphy, Saman P. Amarasinghe, Shih-Wei Liao, Monica S. Lam 
Address: Stanford, CA 94305  Pasadena, CA 91125  
Affiliation: Computer Systems Laboratory Stanford University  yComputer Science Dept. California Institute of Technology  
Abstract: This paper presents an extensive empirical evaluation of an interprocedural parallelizing compiler, developed as part of the Stanford SUIF compiler system. The system incorporates a comprehensive and integrated collection of analyses, including privatization and reduction recognition for both array and scalar variables, and symbolic analysis of array subscripts. The interprocedural analysis framework is designed to provide analysis results nearly as precise as full inlining but without its associated costs. Experimentation with this system on programs from standard benchmark suites demonstrate that an integrated combination of interprocedural analyses can substantially advance the capability of automatic parallelization technology.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. P. Banning. </author> <title> An efficient way to find the side effects of procedure calls and the aliases of variables. </title> <booktitle> In Proceedings of the Sixth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1979. </year>
Reference-contexts: These analyses locate scalar dependences, locate opportunities for scalar reduction transformations and determine privatiz-able scalars. We apply these analyses interprocedurally. A simple flow-insensitive mod-ref analysis <ref> [1] </ref> detects scalar dependences and, with a straightforward extension, provides the necessary information to locate scalar reductions. A flow-sensitive live-variable analysis, discussed below, allows detection of privatizable scalar variables. The flow-sensitive symbolic analysis of Section 4.1 also finds induction and loop-invariant integer variables, which can then be privatized.
Reference: 2. <author> B. Blume, R. Eigenmann, K. Faigin, J. Grout, Jay Hoeflinger, D. Padua, P. Petersen, B. Pottenger, L. Rauchwerger, P. Tu, and S. Weatherford. </author> <title> Polaris: The next generation in parallelizing compilers. </title> <booktitle> In Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1994. </year>
Reference-contexts: The Polaris system at University of Illinois is also pushing the state of the art in parallelization technology <ref> [2] </ref>. The most fundamental difference between our system and Polaris is that Polaris performs no interprocedural analysis, instead relying on full inlining of the programs to obtain interprocedural information. The Polaris group has demonstrated that good coverage (fraction of the program parallelized) can be obtained automatically.
Reference: 3. <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Benchmarks programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: As hardware technology advances make pervasive parallel computing a possibility, compilers which can extract parallelism from sequential codes become important tools to simplify parallel programming. Unfortunately, today's commercially available parallelizing compilers are not effective at getting good performance on multiprocessors <ref> [3, 19] </ref>. These compilers tend to be successful in parallelizing only innermost loops. Parallelizing just inner loops is not adequate for multiprocessors for two reasons. First, inner loops may not make up a significant portion of the sequential computation, thus limiting the parallel speedup by limiting the amount of parallelism.
Reference: 4. <author> K. Cooper, M.W. Hall, and K. Kennedy. </author> <title> A methodology for procedure cloning. </title> <journal> Computer Languages, </journal> <volume> 19(2), </volume> <month> April </month> <year> 1993. </year>
Reference-contexts: To avoid such excessive space usage, we utilize path-specific information only when it may provide opportunities for improved optimization. Our system incorporates selective procedure cloning, a program restructuring in which the compiler replicates the analysis results for a procedure to analyze it in the context of distinct calling environments <ref> [4] </ref>.
Reference: 5. <author> B. Creusillet and F. Irigoin. </author> <title> Interprocedural array region analyses. </title> <booktitle> In Proceedings of the 8th International Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1995. </year> <month> 519 </month>
Reference-contexts: Irigoin et al. developed an interprocedural analysis system, called PIPS, that is part of an environment for parallel programming [12]. More recently, PIPS has been extended to incorporate interprocedural array privatization <ref> [11, 5] </ref>. PIPS is most similar to our work, but lacks three important features: (1) path-specific in-terprocedural information such as obtained through selective procedure cloning, (2) interprocedural reductions, and (3) extensive interprocedural scalar data-flow analysis such as scalar privatization.
Reference: 6. <author> M. W. Hall, J. Mellor-Crummey, A. Carle, and R. Rodriguez. FIAT: </author> <title> A framework for interprocedural analysis and transformation. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year>
Reference-contexts: In an interprocedural setting, a framework is even more important because of the complexity of collecting and managing information about all the procedures in a program. We use Fiat <ref> [6] </ref>, a tool which encapsulates the common features of interpro-cedural analysis, in combination with the Stanford SUIF compiler to constitute our interprocedural parallelization system.
Reference: 7. <author> M.W. Hall, S.P. Amarasinghe, B.R. Murphy, S. Liao, and M.S. Lam. </author> <title> Detecting coarse-grain parallelism using an interprocedural parallelizing compiler. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Static loop count comparison of our system with Fida. 7 Experience with this System This system has been used as an experimental platform in an extensive empirical evaluation of the effectiveness of automatic parallelization technology. The full results are presented elsewhere <ref> [7] </ref>, but we present a few highlights in this section. We have compared the results of our interpocedural analysis with the Fida system (Full Interprocedural Data-Flow Analysis), an interprocedural system that performs precise flow-insensitive array analysis [10] (see Section 2).
Reference: 8. <author> W.L. Harrison. </author> <title> The interprocedural analysis and automatic parallelization of Scheme programs. </title> <journal> Lisp and Symbolic Computation, </journal> 2(3/4):179-396, October 1989. 
Reference-contexts: Path-specific interprocedural information has previously been obtained either by inline substitution or by tagging data-flow sets with a path history through the call graph, incurring a data-flow set expansion problem corresponding to the code explosion problem of inlining <ref> [8, 16, 17, 18] </ref>. To avoid such excessive space usage, we utilize path-specific information only when it may provide opportunities for improved optimization. <p> We have compared the results of our interpocedural analysis with the Fida system (Full Interprocedural Data-Flow Analysis), an interprocedural system that performs precise flow-insensitive array analysis [10] (see Section 2). The Fida system was the first to measure how interprocedural analysis on full applications <ref> (from the Perfect and Spec89 benchmark suites) </ref> affects the number of parallel loops that the system can automatically recognize. We compare how many loops containing procedure calls are parallelized using the two systems in loops than Fida.
Reference: 9. <author> P. Havlak and K. Kennedy. </author> <title> An implementation of interprocedural bounded regular section analysis. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 350-360, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Section 6 describes how the interprocedural array analysis is extended to recognize array reductions. The final two sections discuss experiences with this system and conclude. 52 2 Related Work In the late 1980s, a series of papers presented results on interprocedural par-allelization analysis <ref> [9, 15, 20] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations.
Reference: 10. <author> M. Hind, M. Burke, P. Carini, and S. Midkiff. </author> <title> An empirical study of precise inter-procedural array analysis. </title> <journal> Scientific Programming, </journal> <volume> 3(3) </volume> <pages> 255-271, </pages> <year> 1994. </year>
Reference-contexts: These techniques were shown to be effective in parallelizing linear algebra libraries. More recently, the Fida system was developed at IBM to obtain more precise array sections through partial inlining of array accesses <ref> [10] </ref> (see Section 7). Irigoin et al. developed an interprocedural analysis system, called PIPS, that is part of an environment for parallel programming [12]. More recently, PIPS has been extended to incorporate interprocedural array privatization [11, 5]. <p> The full results are presented elsewhere [7], but we present a few highlights in this section. We have compared the results of our interpocedural analysis with the Fida system (Full Interprocedural Data-Flow Analysis), an interprocedural system that performs precise flow-insensitive array analysis <ref> [10] </ref> (see Section 2). The Fida system was the first to measure how interprocedural analysis on full applications (from the Perfect and Spec89 benchmark suites) affects the number of parallel loops that the system can automatically recognize.
Reference: 11. <author> F. Irigoin. </author> <title> Interprocedural analyses for programming environments. </title> <booktitle> In NSF-CNRS Workshop on Evironments and Tools for Parallel Scientific Programming, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: Irigoin et al. developed an interprocedural analysis system, called PIPS, that is part of an environment for parallel programming [12]. More recently, PIPS has been extended to incorporate interprocedural array privatization <ref> [11, 5] </ref>. PIPS is most similar to our work, but lacks three important features: (1) path-specific in-terprocedural information such as obtained through selective procedure cloning, (2) interprocedural reductions, and (3) extensive interprocedural scalar data-flow analysis such as scalar privatization.
Reference: 12. <author> F. Irigoin, P. Jouvelot, and R. Triolet. </author> <title> Semantical interprocedural parallelization: An overview of the PIPS project. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: More recently, the Fida system was developed at IBM to obtain more precise array sections through partial inlining of array accesses [10] (see Section 7). Irigoin et al. developed an interprocedural analysis system, called PIPS, that is part of an environment for parallel programming <ref> [12] </ref>. More recently, PIPS has been extended to incorporate interprocedural array privatization [11, 5].
Reference: 13. <author> J. Kam and J. Ullman. </author> <title> Global data flow analysis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 159-171, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: Traditional data-flow analysis frameworks help reduce development time and improve correctness by capturing these common features in a single module <ref> [13] </ref>. In an interprocedural setting, a framework is even more important because of the complexity of collecting and managing information about all the procedures in a program.
Reference: 14. <author> W. Landi and B.G. Ryder. </author> <title> A safe approximate algorithm for interprocedural pointer aliasing. </title> <booktitle> In SIGPLAN '92 Conference on Programming Language Design and Implementation, SIGPLAN Notices 27(7), </booktitle> <pages> pages 235-248, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Iterative analysis over this structure is slow because the number of control flow paths through which information flows increases greatly. Such analysis also loses precision by propagating information along unrealizable paths <ref> [14] </ref>; the analysis may propagate calling context information from one caller through a procedure and return the side-effect information to a different caller. In our system, we use a region-based analysis that solves the problems of unrealizable paths and slow convergence.
Reference: 15. <author> Z. Li and P. Yew. </author> <title> Efficient interprocedural analysis for program restructuring for parallel programs. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Section 6 describes how the interprocedural array analysis is extended to recognize array reductions. The final two sections discuss experiences with this system and conclude. 52 2 Related Work In the late 1980s, a series of papers presented results on interprocedural par-allelization analysis <ref> [9, 15, 20] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations.
Reference: 16. <author> E. Myers. </author> <title> A precise inter-procedural data flow algorithm. </title> <booktitle> In Conference Record of the Eighth Annual Symposium on Principles of Programming Languages. ACM, </booktitle> <month> January </month> <year> 1981. </year>
Reference-contexts: For example, in a straightforward interprocedural adaptation of traditional iterative analysis, analysis might be carried out over a program representation called the supergraph <ref> [16] </ref>, where individual control flow graphs for the procedures in the program are linked together at procedure call and return points. Iterative analysis over this structure is slow because the number of control flow paths through which information flows increases greatly. <p> Path-specific interprocedural information has previously been obtained either by inline substitution or by tagging data-flow sets with a path history through the call graph, incurring a data-flow set expansion problem corresponding to the code explosion problem of inlining <ref> [8, 16, 17, 18] </ref>. To avoid such excessive space usage, we utilize path-specific information only when it may provide opportunities for improved optimization.
Reference: 17. <author> M. Sharir and A. Pnueli. </author> <title> Two approaches to interprocedural data flow analysis. </title> <editor> In S. Muchnick and N.D. Jones, editors, </editor> <title> Program Flow Analysis: Theory and Applications. </title> <publisher> Prentice Hall Inc, </publisher> <year> 1981. </year>
Reference-contexts: Path-specific interprocedural information has previously been obtained either by inline substitution or by tagging data-flow sets with a path history through the call graph, incurring a data-flow set expansion problem corresponding to the code explosion problem of inlining <ref> [8, 16, 17, 18] </ref>. To avoid such excessive space usage, we utilize path-specific information only when it may provide opportunities for improved optimization.
Reference: 18. <author> O. Shivers. </author> <title> Control-Flow Analysis of higher-order languages. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Path-specific interprocedural information has previously been obtained either by inline substitution or by tagging data-flow sets with a path history through the call graph, incurring a data-flow set expansion problem corresponding to the code explosion problem of inlining <ref> [8, 16, 17, 18] </ref>. To avoid such excessive space usage, we utilize path-specific information only when it may provide opportunities for improved optimization.
Reference: 19. <author> J. P. Singh and J. L. Hennessy. </author> <title> An empirical investigation of the effectiveness of and limitations of automatic parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessors, </booktitle> <address> Tokyo, Japan, </address> <month> April </month> <year> 1991. </year>
Reference-contexts: As hardware technology advances make pervasive parallel computing a possibility, compilers which can extract parallelism from sequential codes become important tools to simplify parallel programming. Unfortunately, today's commercially available parallelizing compilers are not effective at getting good performance on multiprocessors <ref> [3, 19] </ref>. These compilers tend to be successful in parallelizing only innermost loops. Parallelizing just inner loops is not adequate for multiprocessors for two reasons. First, inner loops may not make up a significant portion of the sequential computation, thus limiting the parallel speedup by limiting the amount of parallelism.
Reference: 20. <author> R. Triolet, F. Irigoin, and P. Feautrier. </author> <title> Direct parallelization of call statements. </title> <booktitle> In Proceedings of the SIGPLAN '86 Symposium on Compiler Construction, SIGPLAN Notices 21(7), </booktitle> <pages> pages 176-185. </pages> <publisher> ACM, </publisher> <month> July </month> <year> 1986. </year>
Reference-contexts: Section 6 describes how the interprocedural array analysis is extended to recognize array reductions. The final two sections discuss experiences with this system and conclude. 52 2 Related Work In the late 1980s, a series of papers presented results on interprocedural par-allelization analysis <ref> [9, 15, 20] </ref>. Their common approach was to determine the sections of arrays that are modified or referenced by each procedure call, enabling parallelization of some loops containing calls whenever each invocation modifies array elements distinct from those that are referenced or modified in other invocations.
Reference: 21. <author> P. Tu and D. Padua. </author> <title> Automatic array privatization. </title> <booktitle> In Proceedings of the Sixth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, OR, </address> <month> August </month> <year> 1993. </year> <title> This article was processed using the L a T E X macro package with LLNCS style 520 </title>
Reference-contexts: The array privatization test is applied only to the variables that are involved in dependences to determine if privatization will eliminate these dependences. Our formulation of array privatization is an extension of Tu and Padua's algorithm <ref> [21] </ref>. Tu and Padua recognize an array as privatizable only if there are no upwards-exposed reads within the loop.
References-found: 21

