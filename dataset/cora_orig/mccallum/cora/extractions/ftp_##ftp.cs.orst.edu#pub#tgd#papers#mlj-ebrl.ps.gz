URL: ftp://ftp.cs.orst.edu/pub/tgd/papers/mlj-ebrl.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Title: Machine Learning,  Explanation-Based Learning and Reinforcement Learning: A Unified View  
Author: THOMAS G. DIETTERICH NICHOLAS S. FLANN Editor: Andrew Barto 
Keyword: Explanation-based learning, reinforcement learning, dynamic programming, goal regression, speedup learning, incomplete theory problem, intractable theory problem  
Address: Corvallis, OR 97331-3202  Logan, UT 84322-4205  
Affiliation: Department of Computer Science, Oregon State University,  Department of Computer Science, Utah State University,  
Note: c 1997 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 28,  
Email: tgd@cs.orst.edu  flann@nick.cs.usu.edu  
Date: 169-214 (1997)  
Abstract: In speedup-learning problems, where full descriptions of operators are known, both explanation-based learning (EBL) and reinforcement learning (RL) methods can be applied. This paper shows that both methods involve fundamentally the same process of propagating information backward from the goal toward the starting state. Most RL methods perform this propagation on a state-by-state basis, while EBL methods compute the weakest preconditions of operators, and hence, perform this propagation on a region-by-region basis. Barto, Bradtke, and Singh (1995) have observed that many algorithms for reinforcement learning can be viewed as asynchronous dynamic programming. Based on this observation, this paper shows how to develop dynamic programming versions of EBL, which we call region-based dynamic programming or Explanation-Based Reinforcement Learning (EBRL). The paper compares batch and online versions of EBRL to batch and online versions of point-based dynamic programming and to standard EBL. The results show that region-based dynamic programming combines the strengths of EBL (fast learning and the ability to scale to large state spaces) with the strengths of reinforcement learning algorithms (learning of optimal policies). Results are shown in chess endgames and in synthetic maze tasks. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G. </author> <year> (1990). </year> <title> Using local models to control movement. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 316-323. </pages> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Perhaps the most popular approach is to represent the value function by some function approximation method, such as local weighted regression <ref> (Atkeson, 1990) </ref> or a feed-forward neural network (Tesauro, 1992; Sutton, 1988; Lin, 1992).
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <pages> 81-138. </pages>
Reference-contexts: If operators are missing effects, then the post-images will be too large, so backups will be performed on inappropriate states. The unified perspective of this paper suggests a solution: model-free RL algorithms such as Q-learning or adaptive real-time dynamic programming <ref> (ARTDP, Barto et al., 1995) </ref> can be applied in cases where nothing is known about the operators.
Reference: <author> Bellman, R. E. </author> <year> (1957). </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: The fundamental step in most dynamic programming algorithms is called the "Bellman backup" <ref> (after Bellman, 1957) </ref>.
Reference: <author> Bern, M. </author> <year> (1990). </year> <title> Hidden surface removal for rectangles. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 40, </volume> <pages> 49-69. </pages> <note> 212 T. </note> <author> G. DIETTERICH AND N. S. FLANN Bertsekas, D. P., & Castanon, D. A. </author> <year> (1989). </year> <title> Adaptive aggregation methods for infinite horizon dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-34, </volume> <pages> 589-598. </pages>
Reference: <author> Bratko, I., & Michie, D. </author> <year> (1980). </year> <title> A representation for pattern-knowledge in chess endgames. </title> <editor> In Clarke, M. R. (Ed.), </editor> <booktitle> Advances in Computer Chess, </booktitle> <volume> Vol. 2. </volume> <publisher> Edinburgh University Press. </publisher>
Reference: <author> Chapman, D., & Kaelbling, L. P. </author> <year> (1991). </year> <title> Input generalization in delayed reinforcement learning: An algorithm and performance comparisons. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 726-731. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Christiansen, A. D. </author> <year> (1992). </year> <title> Learning to predict in uncertain continuous tasks. </title> <editor> In Sleeman, D., & Edwards, P. (Eds.), </editor> <booktitle> Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 72-81 San Francisco. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Cormen, T. H., Leiserson, C. E., & Rivest, R. L. </author> <year> (1990). </year> <title> Introduction to Algorithms. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Dijkstra backups can be performed either in the forward, online algorithm or in the reverse trajectory algorithm. The deterministic version of prioritized sweeping is Dijkstra's shortest path algorithm <ref> (Cormen, Leiserson, & Rivest, 1990) </ref>.
Reference: <author> Dijkstra, E. W. </author> <year> (1959). </year> <title> A note on two problems in connexion with graphs. </title> <journal> Nu-merische Mathematik, </journal> <volume> 1, </volume> <pages> 269-271. </pages>
Reference: <author> Edelsbrunner, H. </author> <year> (1983). </year> <title> A new approach to rectangle intersections. </title> <journal> International Journal of Computer Mathematics, </journal> <volume> 13, </volume> <pages> 209-219. </pages>
Reference-contexts: However, instead of manipulating two-dimensional rectangles, our counter-planning version of Offline-Rect-Dp handles higher-dimensional rectangular regions. Each such region contains a two-dimensional rectangle for each playing piece on the board. To store and retrieve these regions, a multi-dimensional rectangle tree is employed <ref> (Edelsbrunner, 1983) </ref> that provides a retrieval complexity of O (log 2d (2d) + N ), where d is the number of pieces in the region and N is the number of regions retrieved.
Reference: <author> Flann, N. S. </author> <year> (1992). </year> <title> Correct Abstraction in Counter-planning: A Knowledge Compilation Approach. </title> <type> Ph.D. thesis, </type> <institution> Oregon State University. </institution>
Reference-contexts: The shallower regions are generated when the maximizing side is backing up and new loss states are only generated when the set of forward operators is reduced to ;. Similar results have been generated for other, more complicated endings in chess and in checkers <ref> (see Flann, 1992, for more details) </ref>. 4. Discussion Our current stock of algorithms for reinforcement learning and explanation-based learning suffer from many problems. The algorithms and experiments presented above provide partial solutions to some of these problems. 208 T. G. DIETTERICH AND N. S. FLANN 4.1.
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference-contexts: Unfortunately, this property does not carry over to the stochastic case. This review of RL algorithms has focused on algorithms that employ dynamic programming to learn a value function. There are many other approaches to RL <ref> (see Kaelbling, Littman & Moore, 1996, for a review) </ref>. Some methods explicitly learn a policy (either with or without learning a value function), which permits them to avoid the one-step lookahead search needed to choose actions when only a value function is learned.
Reference: <author> Laird, J., Rosenbloom, P., & Newell, A. </author> <year> (1986). </year> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 11-46. </pages>
Reference-contexts: There are many important applications that could benefit from effective speedup learning algorithms. 170 T. G. DIETTERICH AND N. S. FLANN 1.1. Explanation-Based Learning In the field of machine learning, the best-studied speedup learning method is Explanation-Based Learning (EBL), as exemplified by the Prodigy (Minton, 1988) and SOAR <ref> (Laird, Rosenbloom, & Newell, 1986) </ref> systems. EBL systems model problem solving as a process of state-space search. The problem solver begins in a start state, and by applying operators to the start state and succeeding states, the problem solver seeks to reach a goal state, where the problem is solved.
Reference: <author> Lin, L.-J. </author> <year> (1992). </year> <title> Self-improving reactive agents based on reinforcement learning, planning and teaching. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 293-322. </pages>
Reference: <author> Minton, S. </author> <year> (1988). </year> <title> Learning effective search control knowledge: An explanation-based approach. </title> <type> Ph.D. thesis, </type> <institution> Carnegie-Mellon University. </institution> <note> Technical Report CMU-CS-88-133. </note>
Reference-contexts: There are many important applications that could benefit from effective speedup learning algorithms. 170 T. G. DIETTERICH AND N. S. FLANN 1.1. Explanation-Based Learning In the field of machine learning, the best-studied speedup learning method is Explanation-Based Learning (EBL), as exemplified by the Prodigy <ref> (Minton, 1988) </ref> and SOAR (Laird, Rosenbloom, & Newell, 1986) systems. EBL systems model problem solving as a process of state-space search. <p> This ability to reason with regions has permitted EBL to be applied to problems with infinite state spaces, such as traditional AI planning and scheduling domains, where point-based RL would be inapplicable <ref> (Minton, 1988) </ref>. These observations concerning the relationship between EBL and RL suggest that it would be interesting to investigate hybrid algorithms that could perform region-based backups. These backups would combine the region-based reasoning of EBL with the value function approach of RL. <p> These high costs are the primary cause of the so-called "Utility Problem" of explanation-based learning (Minton, 1990; Subramanian & Feldman, 1990). Some researchers have explored algorithms that combine region-based policies with a default policy <ref> (Minton, 1988) </ref>. This has the advantage of reducing the number of rectangles that need to be stored and manipulated (but at the cost of eliminating the ability to learn an optimal policy). EXPLANATION-BASED REINFORCEMENT LEARNING 209 4.2.
Reference: <author> Minton, S. </author> <year> (1990). </year> <title> Quantitative results concerning the utility of explanation-based learning. </title> <journal> Artificial Intelligence, </journal> <volume> 42, </volume> <pages> 363-392. </pages>
Reference: <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1 (1), </volume> <pages> 47-80. </pages> <note> EXPLANATION-BASED REINFORCEMENT LEARNING 213 Moore, </note> <author> A. W. </author> <year> (1993). </year> <title> The Parti-game algorithm for variable resolution reinforcement learning in multidimensional state-spaces. </title> <booktitle> In Advances in Neural Information Processing, </booktitle> <volume> Vol. 6, </volume> <pages> pp. </pages> <address> 711-718 San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This analytical process is sometimes called "goal regression," because the goal is regressed through the sequence of operators to compute P . Consider for example, the LEX2 system <ref> (Mitchell, Keller, & Kedar-Cabelli, 1986) </ref>, which applies EBL to speed up symbolic integration. A state in LEX2 is an expression, such as R 5x 2 dx. The goal is to transform this expression to one that does not contain the integral sign.
Reference: <author> Moore, A. W., & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less time. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 103-130. </pages>
Reference-contexts: Lin (1992) employed a somewhat more complex version of reverse trajectory updates in a simulated robot problem. A third way that Bellman backups can be applied is by an o*ine search technique known as prioritized sweeping <ref> (Moore & Atkeson, 1993) </ref>.
Reference: <author> Puterman, M. L. </author> <year> (1994). </year> <title> Markov Decision Processes. </title> <editor> J. </editor> <publisher> Wiley & Sons, </publisher> <address> New York. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess endgames. </title> <booktitle> In Machine learning: An artificial intelligence approach, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 463-482. </pages> <publisher> Tioga Press, </publisher> <address> Palo Alto, CA. </address>
Reference: <author> Sammut, C., & Cribb, J. </author> <year> (1990). </year> <booktitle> Is learning rate a good performance criterion for learning? In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 170-178 San Francisco, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schaeffer, J. </author> <year> (1991). </year> <title> Checkers program earns the right to play for world title. </title> <journal> Computing Research News, </journal> <volume> 1, 12. </volume> <month> January. </month>
Reference-contexts: In chess, Thompson (1986) has determined many 100-move or greater forced wins for five- and six-piece endings. In checkers, endgame databases up to seven pieces have significantly contributed to a program's expertise and earned it the right to play against the world champion <ref> (Schaeffer, 1991) </ref>. However, since the size of the tables needed to store these policies grows exponentially with the number of pieces, Offline-Point-Dp has effectively reached its limit of usefulness in these applications.
Reference: <author> Subramanian, D., & Feldman, R. </author> <year> (1990). </year> <title> The utility of EBL in recursive domain theories. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence (AAAI-90) Menlo Park, </booktitle> <address> CA. </address> <publisher> AAAI Press. </publisher>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 (1), </volume> <pages> 9-44. </pages>
Reference: <author> Tambe, M., Newell, A., & Rosenbloom, P. </author> <year> (1990). </year> <title> The problem of expensive chunks and its solution by restricting expressiveness. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 299-348. </pages>
Reference-contexts: For higher dimensions, Edelsbrunner's (1983) rectangle-tree data structure requires O (log 2d (2d) + N ) to retrieve N rectangles in d-dimensions. However, in discrete-valued spaces with many dimensions, the costs may be prohibitive <ref> (Tambe, Newell, & Rosenbloom, 1990) </ref>. These high costs are the primary cause of the so-called "Utility Problem" of explanation-based learning (Minton, 1990; Subramanian & Feldman, 1990). Some researchers have explored algorithms that combine region-based policies with a default policy (Minton, 1988).
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 257-278. </pages>
Reference: <author> Thompson, K. </author> <year> (1986). </year> <title> Retrograde analysis of certain endgames. </title> <journal> ICCA Journal, </journal> <volume> 9 (3), </volume> <pages> 131-139. </pages>
Reference: <author> Watkins, C. J. C. </author> <year> (1989). </year> <title> Learning from delayed rewards. </title> <type> Ph.D. thesis, </type> <institution> King's College, </institution> <address> Cambridge. </address>
Reference: <author> Watkins, C. J., & Dayan, P. </author> <year> (1992). </year> <title> Technical note: </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Yee, R. C., Saxena, S., Utgoff, P. E., & Barto, A. G. </author> <year> (1990). </year> <title> Explaining temporal differences to create useful concepts for evaluating states. </title> <booktitle> In Proceedings of the Eight National Conference on Artificial Intelligence (AAAI-90), </booktitle> <pages> pp. </pages> <address> 882-888 Cambridge, MA. </address> <publisher> AAAI Press/MIT Press. </publisher>

References-found: 30

