URL: http://robotics.stanford.edu/~birch/publications/p2p_iccv1998.ps.gz
Refering-URL: http://robotics.stanford.edu/~birch/p2p/
Root-URL: http://www.cs.stanford.edu
Email: birchfield@cs.stanford.edu tomasi@cs.stanford.edu  
Title: Depth Discontinuities by Pixel-to-Pixel Stereo  
Author: Stan Birchfield Carlo Tomasi 
Address: Stanford, California 94305 Stanford, California 94305  
Affiliation: Department of Electrical Engineering Department of Computer Science Stanford University Stanford University  
Abstract: Proceedings of the 1998 IEEE International Conference on Computer Vision, Bombay, India An algorithm to detect depth discontinuities from a stereo pair of images is presented. The algorithm matches individual pixels in corresponding scanline pairs while allowing occluded pixels to remain unmatched, then propagates the information between scanlines by means of a fast postprocessor. The algorithm handles large untextured regions, uses a measure of pixel dissimilarity that is insensitive to image sampling, and prunes bad search nodes to increase the speed of dynamic programming. The computation is relatively fast, taking about 1.5 microseconds per pixel per disparity on a workstation. Approximate disparity maps and precise depth discontinuities (along both horizontal and vertical boundaries) are shown for five stereo images containing textured, untextured, fronto-parallel, and slanted objects. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> H. H. Baker and T. O. Binford. </author> <title> Depth from edge and intensity based stereo. </title> <booktitle> In IJCAI, </booktitle> <pages> pp. 631-636, </pages> <year> 1981. </year>
Reference-contexts: (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> The methods of Baker and Binford <ref> [1] </ref> and Ohta and Kanade [12] would probably match the intensity edges correctly, yielding a sparse disparity map. However, in interpolating the disparity of the untextured regions neither method would preserve the sharp depth discontinuities.
Reference: [2] <author> P. N. Belhumeur and D. Mumford. </author> <title> A Bayesian treatment of the stereo correspondence problem using half-occluded regions. </title> <booktitle> In CVPR, </booktitle> <pages> pages 506-512, </pages> <year> 1992. </year>
Reference-contexts: Our approach inverts the traditional role of a stereo algorithm because, instead of using the knowledge of depth discontinuities to compute disparity more accurately, we compute a rough disparity map in order to get crisp discontinuities. Like several previous algorithms <ref> [2, 5, 7, 8] </ref>, our algorithm uses a form of dynamic programming to match epipolar scanlines independently, detecting occlusions and depth discontinu-ities simultaneously with a disparity map. <p> The depth-discontinuity pixels are labelled as those pixels that border a change of at least 1 Our occlusions correspond roughly to Belhumeur's half-occluded regions <ref> [2] </ref>. q q q q q q q q q q q q q L: 0 5 10 pixel (7,3), (8,4), (9,5), (10,6), (11,9), (12; 10)i. The five middle matches correspond to a near object. two levels of disparity and that lie on the far object. <p> Instead of deriving a maximum a posteriori (MAP) cost function from a Bayesian formulation (as is done in <ref> [2, 7, 11] </ref>), we propose a simple cost function justified solely by empirical evidence. <p> Together, the two terms act like an occlusion penalty that is dependent on the length of the occlusion <ref> [2, 7] </ref>. Nevertheless, we keep the terms separate because a constant occlusion penalty is central to our method of pruning the search space, as described in Section 3.2. <p> Typically, the problem is alleviated either by working at subpixel resolution <ref> [2, 11] </ref> or by adding robustness through window-based matching [4, 6, 7, 9]. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> However, minimizing such a function in a computationally efficient manner is not a straightforward and the standard algorithm (dashed). task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 800% or more <ref> [2, 12] </ref>. As a result, some approaches avoid the extension altogether [7, 8]. We have devised a method for postprocessing the disparity map by propagating reliable disparity values into regions of unreliable disparity values. <p> An Indigo 2 Extreme needed 5:5 and 1:5 sec., respectively. 6 Comparison with Previous Work It is instructive to imagine how other stereo algorithms would handle the image in Figure 8a. Intensity-based algorithms such as those by Belhumeur and Mumford <ref> [2] </ref>, Cox et al. [5], Geiger et al. [7], and Intille and Bobick [8] have no mechanism for preferring to place depth discontinuities near intensity variation and would therefore not place the discontinuities along the contour of the lamp.
Reference: [3] <author> S. Birchfield and C. Tomasi. </author> <title> Depth discontinuities by pixel-to-pixel stereo. </title> <type> Technical Report STAN-CS-TR-96-1573, </type> <institution> Stanford University, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: The only restriction is that the continuous intensity function incident upon the sensor be either concave or convex in the vicinity of x i and y i (Interested readers can find the theorems and proofs in <ref> [3] </ref>). In practice, inflection points cause no problem since the regions surrounding them are approximately linear and linear functions are both concave and convex.
Reference: [4] <author> S. D. Cochran and G. Medioni. </author> <title> 3-D surface description from binocular stereo. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intell., </journal> <volume> 14(10) </volume> <pages> 981-994, </pages> <year> 1992. </year>
Reference-contexts: Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [4, 6, 7, 9] </ref>. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> As we will demonstrate, however, untextured, nearly fronto-parallel surfaces can be handled quite nicely as long as one assumption remains true, namely that intensity variation accompanies depth discontinuities. 2 (Similar assumptions have been used in <ref> [4, 6] </ref>.) Because our threshold of declaring intensity variation is small, we are not trying to place the depth discontinuities along strong intensity edges but are merely preventing the cost function from making a poor decision in a region with no information. <p> For a similar reason, the algorithms of Luo and Burkhardt [11] and Jones and Malik [9] would not be able to find the lamp's boundary. Although the algorithms of Fua [6] and Cochran and Medioni <ref> [4] </ref> try to align the depth discontinuities with the intensity edges, it is not clear how well they would perform on this image because the initial disparity map would be so far from the true solution (due to the untextured regions and the algorithms' dependence upon local information for the initial
Reference: [5] <author> I. J. Cox, S. L. Hingorani, S. B. Rao, and B. Maggs. </author> <title> A maximum likelihood stereo algorithm. </title> <booktitle> Comp. Vision and Image Understanding, </booktitle> <volume> 63(3) </volume> <pages> 542-567, </pages> <year> 1996. </year>
Reference-contexts: Our approach inverts the traditional role of a stereo algorithm because, instead of using the knowledge of depth discontinuities to compute disparity more accurately, we compute a rough disparity map in order to get crisp discontinuities. Like several previous algorithms <ref> [2, 5, 7, 8] </ref>, our algorithm uses a form of dynamic programming to match epipolar scanlines independently, detecting occlusions and depth discontinu-ities simultaneously with a disparity map. <p> (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> An Indigo 2 Extreme needed 5:5 and 1:5 sec., respectively. 6 Comparison with Previous Work It is instructive to imagine how other stereo algorithms would handle the image in Figure 8a. Intensity-based algorithms such as those by Belhumeur and Mumford [2], Cox et al. <ref> [5] </ref>, Geiger et al. [7], and Intille and Bobick [8] have no mechanism for preferring to place depth discontinuities near intensity variation and would therefore not place the discontinuities along the contour of the lamp.
Reference: [6] <author> P. Fua. </author> <title> Combining stereo and monocular information to compute dense depth maps that preserve depth discontinuities. </title> <booktitle> In IJCAI, </booktitle> <pages> pages 1292-1298, </pages> <year> 1991. </year>
Reference-contexts: Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [4, 6, 7, 9] </ref>. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> As we will demonstrate, however, untextured, nearly fronto-parallel surfaces can be handled quite nicely as long as one assumption remains true, namely that intensity variation accompanies depth discontinuities. 2 (Similar assumptions have been used in <ref> [4, 6] </ref>.) Because our threshold of declaring intensity variation is small, we are not trying to place the depth discontinuities along strong intensity edges but are merely preventing the cost function from making a poor decision in a region with no information. <p> For a similar reason, the algorithms of Luo and Burkhardt [11] and Jones and Malik [9] would not be able to find the lamp's boundary. Although the algorithms of Fua <ref> [6] </ref> and Cochran and Medioni [4] try to align the depth discontinuities with the intensity edges, it is not clear how well they would perform on this image because the initial disparity map would be so far from the true solution (due to the untextured regions and the algorithms' dependence upon
Reference: [7] <author> D. Geiger, B. Ladendorf, and A. Yuille. </author> <title> Occlusions and binocular stereo. </title> <journal> International Journal of Computer Vision, </journal> <volume> 14(3) </volume> <pages> 211-226, </pages> <year> 1995. </year>
Reference-contexts: Our approach inverts the traditional role of a stereo algorithm because, instead of using the knowledge of depth discontinuities to compute disparity more accurately, we compute a rough disparity map in order to get crisp discontinuities. Like several previous algorithms <ref> [2, 5, 7, 8] </ref>, our algorithm uses a form of dynamic programming to match epipolar scanlines independently, detecting occlusions and depth discontinu-ities simultaneously with a disparity map. <p> Instead of deriving a maximum a posteriori (MAP) cost function from a Bayesian formulation (as is done in <ref> [2, 7, 11] </ref>), we propose a simple cost function justified solely by empirical evidence. <p> Together, the two terms act like an occlusion penalty that is dependent on the length of the occlusion <ref> [2, 7] </ref>. Nevertheless, we keep the terms separate because a constant occlusion penalty is central to our method of pruning the search space, as described in Section 3.2. <p> Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [4, 6, 7, 9] </ref>. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 800% or more [2, 12]. As a result, some approaches avoid the extension altogether <ref> [7, 8] </ref>. We have devised a method for postprocessing the disparity map by propagating reliable disparity values into regions of unreliable disparity values. This postprocessing is rather global in nature and is quite effective at propagating the background disparities into regions with little intensity variation. <p> An Indigo 2 Extreme needed 5:5 and 1:5 sec., respectively. 6 Comparison with Previous Work It is instructive to imagine how other stereo algorithms would handle the image in Figure 8a. Intensity-based algorithms such as those by Belhumeur and Mumford [2], Cox et al. [5], Geiger et al. <ref> [7] </ref>, and Intille and Bobick [8] have no mechanism for preferring to place depth discontinuities near intensity variation and would therefore not place the discontinuities along the contour of the lamp.
Reference: [8] <author> S. S. Intille and A. F. Bobick. </author> <title> Disparity-space images and large occlusion stereo. </title> <booktitle> In Proc. of the 3rd Euro-pean Conf. on Comp. Vision, </booktitle> <pages> pages 179-186, </pages> <year> 1994. </year>
Reference-contexts: Our approach inverts the traditional role of a stereo algorithm because, instead of using the knowledge of depth discontinuities to compute disparity more accurately, we compute a rough disparity map in order to get crisp discontinuities. Like several previous algorithms <ref> [2, 5, 7, 8] </ref>, our algorithm uses a form of dynamic programming to match epipolar scanlines independently, detecting occlusions and depth discontinu-ities simultaneously with a disparity map. <p> (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 800% or more [2, 12]. As a result, some approaches avoid the extension altogether <ref> [7, 8] </ref>. We have devised a method for postprocessing the disparity map by propagating reliable disparity values into regions of unreliable disparity values. This postprocessing is rather global in nature and is quite effective at propagating the background disparities into regions with little intensity variation. <p> Intensity-based algorithms such as those by Belhumeur and Mumford [2], Cox et al. [5], Geiger et al. [7], and Intille and Bobick <ref> [8] </ref> have no mechanism for preferring to place depth discontinuities near intensity variation and would therefore not place the discontinuities along the contour of the lamp.
Reference: [9] <author> D. G. Jones and J. Malik. </author> <title> Computational framework for determining stereo correspondence from a set of linear spatial filters. </title> <journal> Image and Vision Computing, </journal> <volume> 10(10) </volume> <pages> 699-708, </pages> <year> 1992. </year>
Reference-contexts: Typically, the problem is alleviated either by working at subpixel resolution [2, 11] or by adding robustness through window-based matching <ref> [4, 6, 7, 9] </ref>. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> For a similar reason, the algorithms of Luo and Burkhardt [11] and Jones and Malik <ref> [9] </ref> would not be able to find the lamp's boundary.
Reference: [10] <author> J. J. Little and W. E. Gillett. </author> <title> Direct evidence for occlusion in stereo and motion. </title> <journal> Image and Vision Computing, </journal> <volume> 8(4) </volume> <pages> 328-340, </pages> <year> 1990. </year>
Reference-contexts: Moreover, edge detectors have difficulty in dealing with weak edges, such as that of the recorder in Figure 8d (column 190). Some algorithms directly detect depth discontinuities, without computing dense correspondence. Because the approaches of Little and Gillett <ref> [10] </ref> and Toh and Forrest [13] use only local information, they would find few if any depth discontinuities in Figure 8a, which contains little texture.
Reference: [11] <author> A. Luo and H. Burkhardt. </author> <title> An intensity-based cooperative bidirectional stereo matching with simultaneous detection of discontinuities and occlusions. </title> <journal> Intl. Journal of Computer Vision, </journal> <volume> 15(3) </volume> <pages> 171-188, </pages> <year> 1995. </year>
Reference-contexts: Instead of deriving a maximum a posteriori (MAP) cost function from a Bayesian formulation (as is done in <ref> [2, 7, 11] </ref>), we propose a simple cost function justified solely by empirical evidence. <p> Typically, the problem is alleviated either by working at subpixel resolution <ref> [2, 11] </ref> or by adding robustness through window-based matching [4, 6, 7, 9]. But subpixel resolution is computationally expensive for algorithms that explicitly search over all possible disparities, and windows degrade the precision of the depth discontinuities since depth discontinuities violate the fundamental assumption behind windows. <p> For a similar reason, the algorithms of Luo and Burkhardt <ref> [11] </ref> and Jones and Malik [9] would not be able to find the lamp's boundary.
Reference: [12] <author> Y. Ohta and T. Kanade. </author> <title> Stereo by intra- and inter-scanline search using dynamic programming. </title> <journal> IEEE Trans. on PAMI, </journal> <volume> 7(2) </volume> <pages> 139-154, </pages> <year> 1985. </year>
Reference-contexts: (a) (b) (c) cells). (b) The matches (white cells) that can immediately precede a match (striped cell). (c) The matches that can immediately follow a match. must be matched. 3 Searching Along Epipolar Scanlines Thanks to the structure of the cost function, the technique of dynamic programming (also used in <ref> [1, 2, 5, 7, 8, 12] </ref>), can be used to find the optimal match sequence by conducting an exhaustive search. having 10 pixels each, using a maximum disparity of three pixels (i.e., = 3). <p> However, minimizing such a function in a computationally efficient manner is not a straightforward and the standard algorithm (dashed). task. In the extension from 1D to 2D, it is not uncommon for the computing time to increase by 800% or more <ref> [2, 12] </ref>. As a result, some approaches avoid the extension altogether [7, 8]. We have devised a method for postprocessing the disparity map by propagating reliable disparity values into regions of unreliable disparity values. <p> The methods of Baker and Binford [1] and Ohta and Kanade <ref> [12] </ref> would probably match the intensity edges correctly, yielding a sparse disparity map. However, in interpolating the disparity of the untextured regions neither method would preserve the sharp depth discontinuities.
Reference: [13] <author> P.-S. Toh and A. K. Forrest. </author> <title> Occlusion detection in early vision. </title> <booktitle> In ICCV, </booktitle> <pages> pages 126-132, </pages> <year> 1990. </year>
Reference-contexts: Moreover, edge detectors have difficulty in dealing with weak edges, such as that of the recorder in Figure 8d (column 190). Some algorithms directly detect depth discontinuities, without computing dense correspondence. Because the approaches of Little and Gillett [10] and Toh and Forrest <ref> [13] </ref> use only local information, they would find few if any depth discontinuities in Figure 8a, which contains little texture.
Reference: [14] <author> L. E. Wixson. </author> <title> Detecting occluding edges without computing dense correspondence. </title> <booktitle> In Proceedings of the DARPA Image Understanding Workshop, </booktitle> <year> 1993. </year> <title> (a) (c) (e) Wide Web at http://vision.stanford.edu. </title>
Reference-contexts: Some algorithms directly detect depth discontinuities, without computing dense correspondence. Because the approaches of Little and Gillett [10] and Toh and Forrest [13] use only local information, they would find few if any depth discontinuities in Figure 8a, which contains little texture. Wixson's algorithm <ref> [14] </ref> is similar to that of Toh and Forrest in that it matches nearly vertical edges in both images by correlating the two regions on either side of the edge.
References-found: 14

