URL: http://www.cs.jhu.edu/~sheppard/mlj-fin.ps.gz
Refering-URL: http://www.cs.jhu.edu/~sheppard/pubs.html
Root-URL: 
Email: Email: jsheppar@arinc.com  
Phone: Phone: (410) 266-2099  
Title: Co-Learning in Differential Games  
Author: John W. Sheppard 
Keyword: Markov games, differential games, pursuit games, multi-agent learning, reinforcement learning, Q-learning  
Address: 2551 Riva Road Annapolis, MD 21401  
Affiliation: ARINC  
Abstract: Game playing has been a popular problem area for research in artificial intelligence and machine learning for many years. In almost every study of game playing and machine learning, the focus has been on games with a finite set of states and a finite set of actions. Further, most of this research has focused on a single player or team learning how to play against another player or team that is applying a fixed strategy for playing the game. In this paper, we explore multi-agent learning in the context of game playing and develop algorithms for co-learning in which all players attempt to learn their optimal strategies simultaneously. Specifically, we address two approaches to co-learning, demonstrating strong performance by a memory-based reinforcement learner and comparable but faster performance with a tree based reinforcement learner. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. </author> <year> (1992). </year> <title> Tolerating noise, irrelevant, and novel attributes in instance-based learning algorithms. </title> <journal> International Journal of Man-Machine Studies, 16:267287. </journal>
Reference: <author> Anderson, C. and Crawford-Hines, S. </author> <year> (1994). </year> <title> Multigrid Q-learning. </title> <type> Technical Report CS-94-121, </type> <institution> Department of Computer Science, Colorado State University. </institution>
Reference: <author> Barto, A., Bradtke, S., and Singh, S. </author> <year> (1993). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence. </journal>
Reference-contexts: One of the problems with methods such as asynchronous dynamic programming and RTDP is that these methods require a complete understanding of the transition probabilities, ),|( assp , underlying the MDP <ref> ( Barto, Bradtke, and Singh 1993) </ref>. They also require knowledge of the immediate costs, c (s,a). The requirement to know the cost holds, in particular, in the offline case but can be relaxed when learning on line as in RTDP.
Reference: <author> Barto, A., Sutton, R., and Watkins, C. </author> <year> (1990). </year> <title> Learning and sequential decision making. </title> <editor> In Gabriel and Moore, editors, </editor> <publisher> Learning and Computational Neuroscience , MIT Press, Cambridge, </publisher> <pages> pages 539602. </pages> <note> 31 Basar, </note> <author> T. and Olsder, G. </author> <year> (1982). </year> <title> Dynamic Noncooperative Game Theory , Academic Press, </title> <publisher> London. </publisher>
Reference: <author> Bentley, J. </author> <year> (1980). </year> <title> Multidimensional divide and conquer. </title> <journal> Communications of the ACM , 23(4):214229. </journal>
Reference: <author> Bertsekas, D. </author> <year> (1987). </year> <title> Dynamic Programming: Deterministic and Stochastic Models , Prentice-Hall, </title> <publisher> Inc. </publisher>
Reference-contexts: Barto et al. (Barto, Bradtke, and Singh 1993; Barto, Sutton, and Watkins 1991) describe two alternative approaches to solving an MDP that are both forms of asynchronous dynamic programming. The first approach, which they refer to as 3 asynchronous dynamic programming, is a derivative of Gauss- Seidel dynamic programming <ref> (Bertsekas 1987) </ref>; however, where Gauss- Seidel dynamic programming still performs a systematic sweep of all states, asynchronous dynamic programming allows states to be updated at arbitrary points in time. When a state is updated, it uses the current values of successive states.
Reference: <author> Collins, R. </author> <year> (1992). </year> <booktitle> Studies in Artificial Evolution . PhD thesis, </booktitle> <institution> Department of Computer Science, University of California at Los Angeles, Los Angeles, California. </institution>
Reference-contexts: In addition, problems in multi-agent systems are distinct from problems in artificial life in that multi-agent systems focus on individual behaviors and artificial life focuses on population dynamics <ref> (Collins 1992) </ref>. So far, most work in learning and multi-agent systems has emphasized multiple agents learning complementary behaviors in a coordinated environment to accomplish some task, such as 2 team game playing (Stone and Veloso 1996b; Tambe 1996a), combinatorial optimization (Dorigo, Maniezzo, and Colorni 1996), and obstacle avoidance (Grefenstette 1991).
Reference: <author> Dayan, P. </author> <year> (1992). </year> <title> The convergence of TD( l ) for general l . Machine Learning , 8:341 362. </title>
Reference: <author> Deng, K. and Moore, A. </author> <year> (1995). </year> <title> Multiresolution instance-based learning. </title> <booktitle> In Proceedings of the 1995 International Joint Conference on Artificial Intelligence </booktitle> . 
Reference: <author> Dorigo, M., Maniezzo, V., and Colorni, A. </author> <year> (1996). </year> <title> The ant system: Optimization by a colony of cooperating agents. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics , 26(1)113. </journal>
Reference-contexts: So far, most work in learning and multi-agent systems has emphasized multiple agents learning complementary behaviors in a coordinated environment to accomplish some task, such as 2 team game playing (Stone and Veloso 1996b; Tambe 1996a), combinatorial optimization <ref> (Dorigo, Maniezzo, and Colorni 1996) </ref>, and obstacle avoidance (Grefenstette 1991). The research discussed in this paper focuses on exploring methods of learning in the context of competitive multi-agent systems.
Reference: <author> Erev, I. and Roth, A. </author> <year> (1995). </year> <title> On the need for low rationality, cognitive game theory: Reinforcement learning in experimental games with unique mixed strategy equilibria. </title> <type> Unpublished manuscript, </type> <month> August. </month>
Reference: <author> Grefenstette, J. </author> <year> (1988). </year> <title> Credit assignment in rule discovery systems based on genetic algorithms. Machine Learning, </title> <publisher> 3:225245. </publisher>
Reference: <editor> Grefenstette, J. </editor> <booktitle> (1991). Lamarkian learning in multi-agent environments. In Proceedings of the Fourth International Conference on Genetic Algorithms , Morgan Kaufman, </booktitle> <pages> pages 303310. </pages>
Reference-contexts: So far, most work in learning and multi-agent systems has emphasized multiple agents learning complementary behaviors in a coordinated environment to accomplish some task, such as 2 team game playing (Stone and Veloso 1996b; Tambe 1996a), combinatorial optimization (Dorigo, Maniezzo, and Colorni 1996), and obstacle avoidance <ref> (Grefenstette 1991) </ref>. The research discussed in this paper focuses on exploring methods of learning in the context of competitive multi-agent systems.
Reference: <author> Grefenstette, J. and Daley, R. </author> <year> (1995). </year> <title> Methods for competitive and cooperative coevolution. In Adaptation, Coevolution, and Learning in Multiagent Systems (ICMAS 95), </title> <publisher> AAAI Press, </publisher> <pages> pages 276282. </pages>
Reference-contexts: Recently, work in coevolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results. Potter, DeJong, and Grefenstette developed an approach to coevolution in which an agent is decomposed into subagents each responsible for learning an activity to be combined to solve a complex task <ref> (Potter, De Jong, and Grefenstette 1995) </ref>. In this approach, multiple agents operate on a single task in parallel. The agents are initialized with rules to bias their activity toward some subset of the total problem. <p> The agents are initialized with rules to bias their activity toward some subset of the total problem. The multi-agent (or composite) plan then consists of the concatenation of the best plans learned by the subagents. Grefenstette and Daley consider another coevolutionary strategy for cooperative and competitive multi-agent plans <ref> ( Grefenstette and Daley 1995) </ref>. These are the first experiments by Grefenstette et al. in which multiple competitive agents learn simultaneously. In their approach, rather than coordinating subplans, each agent is responsible for its complete plan.
Reference: <author> Grefenstette, J., Ramsey, C., and Schultz, A. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. Machine Learning, </title> <publisher> 5:355381. </publisher>
Reference: <author> Harmon, M. and Baird, L. </author> <year> (1995). </year> <title> Residual advantage learning applied to a differential game. </title> <booktitle> In Neural Information Processing Systems 7. </booktitle>
Reference-contexts: In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (specifically, artificial neural networks) to learning solutions to differential games <ref> (Harmon, Baird, and Klopf 1995) </ref>.
Reference: <author> Harmon, M., Baird, L., and Klopf, A. </author> <year> (1995). </year> <title> Reinforcement learning applied to a differential game. Adaptive Behavior. </title> <type> 32 Heath, </type> <address> D. </address> <year> (1992). </year> <title> A Geometric Framework for Machine Learning . PhD thesis, </title> <institution> Department of Computer Science, The Johns Hopkins University, Baltimore, Maryland. </institution>
Reference-contexts: In independent research, Harmon, Baird, and Klopf investigated applying reinforcement learning in function approximators (specifically, artificial neural networks) to learning solutions to differential games <ref> (Harmon, Baird, and Klopf 1995) </ref>.
Reference: <author> Huberman, B. and Glance, N. </author> <year> (1995). </year> <title> Evolutionary games and computer simulations. </title> <booktitle> Proceedings of the National Academy of Sciences. </booktitle>
Reference: <author> Isaacs, R. </author> <year> (1975). </year> <title> Differential Games. </title> <editor> Robert E. </editor> <publisher> Krieger, </publisher> <address> New York. </address>
Reference-contexts: This latter game is an extension of the traditional Homicidal Chauffeur game <ref> ( Isaacs 1975) </ref> which limits the mobility of P but not E. To reduce the size of the state space, all games are played with state variables relative to P. Further, all game matrices are constructed such that each player has n = 10 strategy ranges.
Reference: <author> Lewin, J. </author> <year> (1994). </year> <title> Differential Games. </title> <publisher> Springer-Verlag, </publisher> <address> New York. </address>
Reference: <author> Littman, M. </author> <year> (1994). </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Proceedings of the Eleventh International Machine Learning Conference , Morgan Kaufmann, </booktitle> <address> New Brunswick, New Jersey, </address> <pages> pages 157163. </pages>
Reference: <author> Littman, M. </author> <year> (1996). </year> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University. </institution>
Reference-contexts: This policy is given by returning an action according to mixed strategies derived for one player which is then fixed to permit selecting the other players action through simple maximization <ref> ( Littman 1996) </ref>. The mixed strategy for the first player is determined by solving the linear program derived from a game matrix of expected payoffs taken from the Q table. <p> These results (and the results of TBCL) can be extended to alternating Markov games (in which players take turns) <ref> (Littman 1996) </ref>, team games (in which teams of players cooperate to devise mutual strategies) ( Tambe 1996a; Tambe 1996b), and community games (in which players choose opponents to maximize their personal payoff) (Stanley, Ashlock, and Tesfatsion 1993).
Reference: <author> Moore, A. </author> <year> (1990). </year> <title> Efficient Memory-Based Learning for Robot Control . PhD thesis, </title> <institution> Computer Laboratory, Cambridge University. </institution>
Reference-contexts: Specifically, a kd-tree is a binary tree where each interior node of the tree tests the values of one of the attributes in the k-dimensional attribute space. In addition, each node corresponds to a single instance in the memory base <ref> (Moore 1990) </ref>. Nodes are selected for splitting until no further splits are required (i.e., until all points are represented in the tree). In memory-based learning, the kd-tree can provide significant speedup in searching for nearest neighbors; however, the size of the memory base does not change.
Reference: <author> Moore, A. and Atkeson, C. </author> <year> (1995). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional statespaces. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Murthy, S. </author> <year> (1995). </year> <title> On Growing Better Decision Trees from Data . PhD thesis, </title> <institution> Department of Computer Science, The Johns Hopkins University, Baltimore, Maryland. </institution>
Reference: <author> Potter M., De Jong, K., and Grefenstette, J. </author> <year> (1995). </year> <title> A coevolutionary approach to learning sequential decision rules. </title> <booktitle> In Proceedings of the International Conference on Genetic Algorithms, </booktitle> <pages> pages 366372. </pages>
Reference-contexts: Recently, work in coevolutionary algorithms has begun to suggest approaches to multi-agent co-learning with some encouraging initial results. Potter, DeJong, and Grefenstette developed an approach to coevolution in which an agent is decomposed into subagents each responsible for learning an activity to be combined to solve a complex task <ref> (Potter, De Jong, and Grefenstette 1995) </ref>. In this approach, multiple agents operate on a single task in parallel. The agents are initialized with rules to bias their activity toward some subset of the total problem.
Reference: <author> Rajan, N., Prasad, U., and Rao, N. </author> <year> (1980). </year> <title> Pursuit-evasion of two aircraft in a horizontal plane. </title> <journal> Journal of Guidance and Control, 3(3):261267. </journal>
Reference: <author> Rosenschein, J. and Genesereth, M. </author> <year> (1985). </year> <title> Deals among rational agents. </title> <booktitle> In Proceedings of the 1985 International Joint Conference on Artificial Intelligence , pages 9199. </booktitle>
Reference: <author> Roth, A. and Erev, I. </author> <year> (1995). </year> <title> Learning in extensive-form games: Experimental data and simple dynamic models in the intermediate term. Games and Economic Behavior, </title> <address> 8:164 212. </address>
Reference: <author> Salzberg, S. </author> <year> (1991). </year> <title> Distance metrics for instance-based learning. </title> <booktitle> In Methodologies for Intelligence Systems: 6 th International Symposium, </booktitle> <pages> pages 399408. </pages>
Reference: <author> Samuel, A. </author> <year> (1959). </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development, </journal> <note> 3(3):211229. 33 Sandholm, </note> <author> T. and Crites, R. </author> <year> (1995). </year> <title> Multiagent reinforcement learning in the iterated prisoners dilemma. </title> <journal> Biosystems, 37:147166. </journal>
Reference-contexts: The worth of the strategy was determined based on how well the computer fared against the human. How many times did the computer win? How long did the game last? Was the computer, at least, an interesting opponent to play? Until Arthur Samuel developed his checkers player <ref> (Samuel 1959) </ref>, the thought of constructing a machine that could learn to play a game capable of competing with a human was just a dream. <p> It is interesting that one of the earliest success stories in machine learning was an approach similar to temporal difference learning applied to game playing. In 1959, Arthur Samuel reported on experiments he performed with a computer learning an evaluation function for board positions in the game of checkers <ref> (Samuel 1959) </ref>. Samuels idea was to use experience from actual play to learn the evaluation function. Then the computer could adapt its play to improve its performance by gradually improving the ability of the evaluation function to predict performance.
Reference: <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A general method for incremental self-improvement and multi-agent learning in unrestricted environments. Evolutionary Computation: </title> <journal> Theory and Applications. </journal>
Reference: <author> Sheppard, J. </author> <year> (1996). </year> <title> Multi-Agent Reinforcement Learning in Markov Games. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, The Johns Hopkins University, Baltimore, Maryland. </institution>
Reference-contexts: For these experiments, behavior strategies (i.e., mixed strategies at each stage in the game) are determined for both players based on learned 6 expected payoffs <ref> (Sheppard 1996) </ref>. Two novel algorithms are presented and compareda memory-based algorithm and a decision tree-based algorithm. In static games, the strategies of all players are generally known. Further, in solving static games, payoffs assigned to selected strategy combinations are also known. <p> We note that the players were still able to learn appropriate optimal strategies for this revised game. Experiments investigating the effects of quantizing the game are described in <ref> (Sheppard 1996) </ref>. 4.2.3 Pursuit with Simple Motion in a Half Plane For the third experiment, we consider a variation of the pursuit game with simple motion in which a boundary exists at x = 0 thus forcing the game to take place in the half plane.
Reference: <author> Sheppard, J. and Salzberg, S. </author> <year> (1997). </year> <title> A teaching method for memory-based control. </title> <journal> Artificial Intelligence Review, </journal> <volume> 11 </volume> <pages> 343-370. </pages>
Reference-contexts: However, the results of our experiments seem to indicate that small trees are sufficient to learn fairly complex tasks. This implies small memory-bases may also be effective, given the right set of examples. Note this is consistent with experiments reported in <ref> (Sheppard and Salzberg 1997) </ref>. One possible limitation in both MBCL and TBCL is the reliance on a common set of Q-values.
Reference: <author> Simons, J., van Brussel, H., DeSchutter, J., and Verhaert, J. </author> <year> (1982). </year> <title> A self-learning automaton with variable resolution for high precision assembly by industrial robots. </title> <journal> IEEE Transactions on Automatic Control, 27(5):11091113. </journal>
Reference: <author> Smith, R. and Gray, B. </author> <year> (1993). </year> <title> Co-adaptive genetic algorithms: An example in Othello strategy. </title> <type> Technical Report TCGA Report No. 94002, </type> <institution> University of Alabama, Tuscaloosa, Alabama. </institution>
Reference-contexts: They apply their approach to a food-gathering task in which two agents compete against each other to obtain the most food. Smith and Gray describe an alternative approach to coevolution applied to the game of Othello, which they call a co-adaptive genetic algorithm <ref> (Smith and Gray 1993) </ref>. Their approach focuses on developing a fitness function that is derived from the ability of a member of the population to compete against other members of the population.
Reference: <author> Stanley, E., Ashlock, D., and Tesfatsion, L. </author> <year> (1993). </year> <title> Iterated prisoners dilemma with choice and refusal of partners. </title> <booktitle> In Proceedings of Alife III. </booktitle> <institution> Sante Fe Institute. </institution>
Reference-contexts: results (and the results of TBCL) can be extended to alternating Markov games (in which players take turns) (Littman 1996), team games (in which teams of players cooperate to devise mutual strategies) ( Tambe 1996a; Tambe 1996b), and community games (in which players choose opponents to maximize their personal payoff) <ref> (Stanley, Ashlock, and Tesfatsion 1993) </ref>. They can also be applied to games that are more traditional with homogeneous agents such as backgammon, checkers, and othello. The strengths of the approach include 30 the relative simplicity in storing examples and updating value estimates for game play.
Reference: <author> Suguwara, T. and Lesser, V. </author> <year> (1993). </year> <title> Online learning of coordination plans. </title> <type> Technical Report COINS TR 93-27, </type> <institution> University of Massachusetts, Amherts, Massachusetts. </institution>
Reference: <author> Sutton, R. </author> <year> (1988). </year> <title> Learning to predict by methods of temporal differences. Machine Learning, </title> <publisher> 3:944. </publisher>
Reference: <author> Stone, P. and Veloso, M. </author> <year> (1995). </year> <title> Beating a defender in robotic soccer: Memory-based learning of a continuous function. </title> <booktitle> In Proceedings of Neural Information Processing Systems. </booktitle>
Reference-contexts: Problems in multi-agent systems are distinct from problems in DAI and distributed computing, from which the field was derived, in that DAI and distributed computing focus on information processing and multi-agent systems focus on behavior development and behavior management <ref> (Stone and Veloso 1995) </ref>. In addition, problems in multi-agent systems are distinct from problems in artificial life in that multi-agent systems focus on individual behaviors and artificial life focuses on population dynamics (Collins 1992).
Reference: <author> Stone, P. and Veloso, M. </author> <year> (1996a). </year> <title> Multiagent systems: A survey from a machine learning perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <note> submitted. </note>
Reference-contexts: Each player is constrained differently, thus requiring different strategies. This means that a single strategy cannot be learned through self-play and given to both players. Thus they are heterogeneous agents <ref> (Stone and Veloso 1996a) </ref>. For P, the magnitude of the force is fixed and P must determine the appropriate angle with which to apply the force. For E, the angle is fixed and E must determine the appropriate magnitude of force to apply.
Reference: <author> Stone, P. and Veloso, M. </author> <year> (1996b). </year> <title> Towards collaborative and adversarial learning: A case study in robotic soccer. </title> <booktitle> In 1996 AAAI Spring Symposium on Adaptation, CoEvolution, and Learning in Multiagent Systems. </booktitle>
Reference: <author> Tambe, M. </author> <year> (1996a). </year> <title> Teamwork in real-world, dynamic environments. </title> <booktitle> In 1996 International Conference on Multiagent Systems, </booktitle> <publisher> AAAI Press. </publisher>
Reference: <author> Tambe, M. </author> <year> (1996b). </year> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the 13 th National Conference on Artificial Intelligence. </booktitle> <publisher> AAAI Press. 34 Tan, </publisher> <editor> M. </editor> <year> (1993). </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Machine Learning: Proceedings of the Tenth International Conference , Morgan Kaufmann, </booktitle> <address> San Mateo, California. </address>
Reference: <author> Tesauro, G. </author> <year> (1992). </year> <title> Practical issues in temporal difference learning. </title> <booktitle> Machine Learning , 8:257277. </booktitle>
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 5867. </pages>
Reference-contexts: The evaluation function was updated in the course of playing several games in which one player, using the current evaluation function, challenged a second player, using the best evaluation function found so far. More recently, Gerald Tesauro applied temporal difference learning in self-play to the game of backgammon <ref> ( Tesauro 1995) </ref>. As with Samuels checkers player, Tesauros TD-Gammon has the two players playing each other using an evaluation function (implemented as a feed-forward neural network). The approaches differ in that, for TD-Gammon, both players use the current evaluation function that has been learned.
Reference: <author> Watkins, C. </author> <year> (1989). </year> <title> Learning with Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cambridge University, </institution> <address> Cambridge, England. </address>
Reference: <author> Watkins, C. and Dayan, P. </author> <year> (1992). </year> <note> Q-learning. Machine Learning, 8:279292. </note>
References-found: 48

