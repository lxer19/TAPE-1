URL: http://ptolemy.eecs.berkeley.edu/papers/97/order/order.ps
Refering-URL: http://ptolemy.eecs.berkeley.edu/papers/97/order/
Root-URL: 
Email: Email: -sriram,eal-@eecs.Berkeley.EDU  
Phone: Phone: (510)642-0395, (510)642-0455  
Title: Determining the Order of Processor Transactions in Statically Scheduled Multiprocessors  
Author: S. Sriram, Edward A. Lee 
Address: Cory Hall, University of California, Berkeley CA94720  
Affiliation: Department of Electrical Engineering and Computer Sciences  
Abstract: This paper addresses embedded multiprocessor implementation of iterative, real-time applications, such as digital signal and image processing, that are specified as dataow graphs. Scheduling dataow graphs on multiple processors involves assigning tasks to processors (processor assignment), ordering the execution of tasks within each processor (task ordering), and determining when each task must commence execution. We consider three scheduling strategies: fully-static, self-timed and ordered transactions, all of which perform the assignment and ordering steps at compile time. Run time costs are small for the fully-static strategy; however it is not robust with respect to changes or uncertainty in task execution times. The self-timed approach is tolerant of variations in task execution times, but pays the penalty of high run time costs, because processors need to explicitly synchronize whenever they communicate. The ordered transactions approach lies between the fully-static and self-timed strategies; in this approach the order in which processors communicate is determined at compile time and enforced at run time. The ordered transactions strategy retains some of the exibility of self-timed schedules and at the same time has lower run time costs than the self-timed approach. In this paper we determine an order of processor transactions that is nearly optimal given information about task execution times at compile time, and for a given processor assignment and task ordering. The criterion for optimality is the average throughput achieved by the schedule. Our main result is that it is possible to choose a transaction order such that the resulting ordered transactions schedule incurs no performance penalty compared to the more exible self-timed strategy, even when the higher run time costs implied by the self-timed strategy are ignored. Journal of VLSI Signal Processing, Vol. 15, No. 3, pp. 207-220, March1997 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. A. Lee and D. G. Messerschmitt, </author> <title> Static Scheduling of Synchronous Dataow Programs for Digital Signal Processing, </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. C-36, no. 2, </volume> <month> February </month> <year> 1982. </year>
Reference-contexts: 1 Introduction In this paper we address embedded multiprocessor implementation of iterative, real-time applications, such as digital signal and image processing (DSP and IP), that are specified as coarse grained Synchronous Data Flow (SDF) graphs <ref> [1] </ref>. Such applications have been found to be particularly amenable to compile-time (static) scheduling techniques mainly because their control ow is predictable at compile time.
Reference: [2] <author> G. C. Sih, </author> <title> Multiprocessor Scheduling to account for Interprocessor Communication, </title> <publisher> Ph. </publisher> <address> D. </address> <note> Thesis, Memorandum No. </note> <institution> UCB/ERL M91/29, Electronics Research Laboratory, University of California at Berkeley, </institution> <month> April </month> <year> 1991. </year>
Reference-contexts: Dataow programming apart from being an intuitive specification mechanism for signal processing algorithms has the obvious advantage that it exposes parallelism in the program. There is a significant body of work on techniques for compile-time scheduling of dataow graphs on multiple processors (a few examples are <ref> [2, 3, 5, 6, 7] </ref>). In this paper we assume that we are already given a multiprocessor schedule (which could be determined using the aforementioned techniques, or even manually) along with the SDF description of an application. <p> G start v k,( ) p v 1 v n 1+ V E,( ) v k p v k p D p ( ) d v i v j ,( ) = t v ( ) v 9 technique [21] and Sihs scheduling heuristics that take IPC costs into account <ref> [2] </ref>, generate a schedule assuming the program terminates after a finite number of iterations. Others explicitly take overlapping of successive iterations of the DFG into account, for example the techniques in [3, 4, 5] adapt list scheduling to generate overlapped schedules. <p> why not obtain a fully-static schedule that has a period to begin with, thus eliminating the post-processing step suggested in Claim 1? Recall from Section 3.0 that an FS schedule is usually obtained using heuristic techniques that are either based on blocked non-overlapped scheduling (which use critical path based heuristics) <ref> [2] </ref> or are based on overlapped scheduling techniques that employ list scheduling heuristics [3, 5]. None of these techniques guarantee that the generated FS schedule will have an iteration period within one unit of the period achieved if the same schedule were run in a self-timed manner.
Reference: [3] <author> M. Lam, </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines, </title> <booktitle> Proceedings of the SIGPLAN 1988 Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 318-328. </pages>
Reference-contexts: Dataow programming apart from being an intuitive specification mechanism for signal processing algorithms has the obvious advantage that it exposes parallelism in the program. There is a significant body of work on techniques for compile-time scheduling of dataow graphs on multiple processors (a few examples are <ref> [2, 3, 5, 6, 7] </ref>). In this paper we assume that we are already given a multiprocessor schedule (which could be determined using the aforementioned techniques, or even manually) along with the SDF description of an application. <p> Such a strategy is employed in VLIW arrays <ref> [3] </ref>, and systolic arrays [19, 20]. The classic controller/datapath architecture is also in a sense an example of fully-static scheduling: the controller determines what each functional unit in the datapath does during each clock cycle. <p> Others explicitly take overlapping of successive iterations of the DFG into account, for example the techniques in <ref> [3, 4, 5] </ref> adapt list scheduling to generate overlapped schedules. Thus, if processors are available, these heuristics determine the processor assignment for each actor , and specify when the th invocation of each actor starts ( ). <p> thus eliminating the post-processing step suggested in Claim 1? Recall from Section 3.0 that an FS schedule is usually obtained using heuristic techniques that are either based on blocked non-overlapped scheduling (which use critical path based heuristics) [2] or are based on overlapped scheduling techniques that employ list scheduling heuristics <ref> [3, 5] </ref>. None of these techniques guarantee that the generated FS schedule will have an iteration period within one unit of the period achieved if the same schedule were run in a self-timed manner.
Reference: [4] <author> V. H. Allan, R. B. Jones, R. M. Lee, S. J. Allan, </author> <title> Software pipelining, </title> <journal> ACM Computing Surveys, </journal> <month> Sept. </month> <year> 1995, </year> <note> vol.27, (no.3):367-432. </note>
Reference-contexts: Others explicitly take overlapping of successive iterations of the DFG into account, for example the techniques in <ref> [3, 4, 5] </ref> adapt list scheduling to generate overlapped schedules. Thus, if processors are available, these heuristics determine the processor assignment for each actor , and specify when the th invocation of each actor starts ( ). <p> If the execution times do in fact vary significantly, then even an ST strategy is not practical: it then becomes necessary to use a more dynamic strategy such as static assignment or fully dynamic scheduling <ref> [4] </ref> to make the best use of computing resources. It is possible to quantify the effects of variations in actor execution times on the average throughput achieved by an ST or an OT schedule; such an analysis is however beyond the scope of this paper.
Reference: [5] <author> S. M. H. de Groot, S. Gerez, and O. Herrmann, </author> <title> Range-Chart-Guided Iterative Data-Flow Graph Scheduling, </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <month> May </month> <year> 1992, </year> <pages> pp. 351-364. </pages>
Reference-contexts: Dataow programming apart from being an intuitive specification mechanism for signal processing algorithms has the obvious advantage that it exposes parallelism in the program. There is a significant body of work on techniques for compile-time scheduling of dataow graphs on multiple processors (a few examples are <ref> [2, 3, 5, 6, 7] </ref>). In this paper we assume that we are already given a multiprocessor schedule (which could be determined using the aforementioned techniques, or even manually) along with the SDF description of an application. <p> Others explicitly take overlapping of successive iterations of the DFG into account, for example the techniques in <ref> [3, 4, 5] </ref> adapt list scheduling to generate overlapped schedules. Thus, if processors are available, these heuristics determine the processor assignment for each actor , and specify when the th invocation of each actor starts ( ). <p> thus eliminating the post-processing step suggested in Claim 1? Recall from Section 3.0 that an FS schedule is usually obtained using heuristic techniques that are either based on blocked non-overlapped scheduling (which use critical path based heuristics) [2] or are based on overlapped scheduling techniques that employ list scheduling heuristics <ref> [3, 5] </ref>. None of these techniques guarantee that the generated FS schedule will have an iteration period within one unit of the period achieved if the same schedule were run in a self-timed manner.
Reference: [6] <author> K. Parhi, and D. G. Messerschmitt, </author> <title> Static Rate-optimal Scheduling of Iterative Data-ow Programs via Optimum Unfolding, </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. 40, no. 2, </volume> <month> Febru-ary </month> <year> 1991, </year> <pages> pp. 178-194. </pages>
Reference-contexts: Dataow programming apart from being an intuitive specification mechanism for signal processing algorithms has the obvious advantage that it exposes parallelism in the program. There is a significant body of work on techniques for compile-time scheduling of dataow graphs on multiple processors (a few examples are <ref> [2, 3, 5, 6, 7] </ref>). In this paper we assume that we are already given a multiprocessor schedule (which could be determined using the aforementioned techniques, or even manually) along with the SDF description of an application. <p> The and values are as follows: , , , , and , , , , and In some cases it is advantageous to unfold a graph by a certain unfolding factor, say , and schedule iterations of the graph together in order to exploit inter-iteration parallelism more effectively <ref> [16, 6] </ref>. The unfolded graph contains copies of each actor of the original graph.
Reference: [7] <author> D. A. Schwartz, and T. P. Barnwell III, </author> <title> Cyclo-Static Solutions: Optimal Multiprocessor Realizations of Recursive Algorithms, VLSI Signal Processing II, </title> <journal> IEEE Special Publications, </journal> <month> June </month> <year> 1985, </year> <pages> pp. 117-128. </pages>
Reference-contexts: Dataow programming apart from being an intuitive specification mechanism for signal processing algorithms has the obvious advantage that it exposes parallelism in the program. There is a significant body of work on techniques for compile-time scheduling of dataow graphs on multiple processors (a few examples are <ref> [2, 3, 5, 6, 7] </ref>). In this paper we assume that we are already given a multiprocessor schedule (which could be determined using the aforementioned techniques, or even manually) along with the SDF description of an application. <p> What we propose can therefore be added as an efficient post-processing step in existing schedulers. Of course, an exhaustive search procedure like the one proposed in <ref> [7] </ref> will certainly find the schedule directly. We set the transaction order to be the transaction order suggested by the modified schedule (as opposed to the transaction order from used in Fig. 5). Thus .
Reference: [8] <author> S. K. Rao, and T. Kailath, </author> <title> Regular Iterative Algorithms and their Implementation on Processor Arrays, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 76, No. 3, </volume> <year> 1988. </year>
Reference-contexts: The optimality is in the sense that the transaction order we determine statically is the best possible one, given the timing information available at compile time. The periodic schedule in (6) is similar to affine schedules in systolic array literature <ref> [9, 8] </ref>. Affine schedules are usually defined over a multi-dimensional index space. In our formulation, however, the index space has only one dimension, representing time, and the scaling of the index is done by the iteration period .
Reference: [9] <author> A. Darte, Y. Robert, </author> <title> Constructive methods for scheduling uniform loop nests, </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> Aug. </month> <year> 1994, </year> <note> vol.5, (no.8):814-22. </note>
Reference-contexts: The optimality is in the sense that the transaction order we determine statically is the best possible one, given the timing information available at compile time. The periodic schedule in (6) is similar to affine schedules in systolic array literature <ref> [9, 8] </ref>. Affine schedules are usually defined over a multi-dimensional index space. In our formulation, however, the index space has only one dimension, representing time, and the scaling of the index is done by the iteration period .
Reference: [10] <author> E. A. Lee, J. C. Bier, </author> <title> Architectures for Statically Scheduled Dataow, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 10, </volume> <month> December </month> <year> 1990, </year> <pages> pp. 333-348. </pages>
Reference-contexts: Determining such an order during compile time and enforcing it at run time eliminates run time arbitration and synchronization costs. This concept has been demonstrated in <ref> [10, 11] </ref>, where the authors discuss the design and implementation of a shared memory multiprocessor system optimized for parallel 3 implementation of embedded DSP applications specified as dataow graphs. We discuss this multiprocessor further in Section 6. <p> Each of these three tasks may be performed either at run time (a dynamic strategy) or at compile time (static strategy). In [18] and <ref> [10] </ref> the authors propose a scheduling taxonomy based on which of these tasks are performed at compile time and which at run time; in this paper we use the same terminology that was introduced there. <p> the absence of precise timing information, we set equal to the estimated execution time of actor . 4 Fully-static schedule In the fully-static scheduling strategy all the three scheduling operations assigning actors to processors, ordering of actors on processors, and determining when each actor fires are performed at compile time <ref> [10] </ref>. Such a strategy is employed in VLIW arrays [3], and systolic arrays [19, 20]. The classic controller/datapath architecture is also in a sense an example of fully-static scheduling: the controller determines what each functional unit in the datapath does during each clock cycle. <p> discard the precise timing information specified in the fully-static schedule; we retain the processor assignment ( ) and actor ordering on each processor as specified by ; and, in addition, we also retain the order in which processors communicate with one another and we enforce this order at run time <ref> [10] </ref>. We formalize the concept of transaction order below. Suppose there are IPC edges where each is a send-receive pair in the FS schedule we obtain. Let be the set of receive actors, and be the set of send actors (i.e. and ).
Reference: [11] <author> S. Sriram, and E. A. Lee, </author> <title> Design and Implementation of an Ordered Memory Access Architecture, </title> <booktitle> Proceedings of the International Conference on Acoustics Speech and Signal Processing, </booktitle> <volume> vol. 1, </volume> <month> April </month> <year> 1993, </year> <pages> pp. 345-348. </pages>
Reference-contexts: Determining such an order during compile time and enforcing it at run time eliminates run time arbitration and synchronization costs. This concept has been demonstrated in <ref> [10, 11] </ref>, where the authors discuss the design and implementation of a shared memory multiprocessor system optimized for parallel 3 implementation of embedded DSP applications specified as dataow graphs. We discuss this multiprocessor further in Section 6. <p> whereas IPC in the FS schedule simply involves reading and writing from shared memory (no synchronization or arbitration needed), implying a cost of a few processor cycles for IPC, the ST strategy requires of the order of tens of processor cycles, unless special hardware is employed for synchronization and arbitration <ref> [11] </ref>. Another feature of the ST strategy is that, since processors do not re-synchronize at the end of each iteration of the DFG (there is no need for such synchronization), successive iterations of the DFG overlap in a natural manner. <p> This results in efficient IPC (comparable to the FS strategy) at relatively low hardware cost. The OMA multiprocessor is described in detail in <ref> [11] </ref>. s t s t v 1 ( ) s t v 2 ( ) s t v 2k 1 ( ) s t v 2k ( ) Transaction order: E C G A Proc 1 Proc 2 Proc 3 Proc 4 Proc 5 s 1 s 2 s 3 s
Reference: [12] <author> J. T. Buck, S. Ha, E. A. Lee, and D. G. Messerschmitt, Ptolemy: </author> <title> A Framework for Simulating and Prototyping Heterogeneous Systems, </title> <journal> International Journal of Computer Simulation, </journal> <volume> vol. 4, </volume> <month> January </month> <year> 1994, </year> <pages> pp. 155-182. </pages>
Reference-contexts: SDF and closely related models have been widely used for representing a significant class of digital signal processing (DSP) algorithms in CAD tools for system-level simulation, design, and prototyping <ref> [12, 13, 14, 15] </ref>. In this paper we assume that the application is a homogeneous SDF graph, i.e. a graph in which actors always produce and consume exactly one token; henceforth whenever we refer to a dataow graph (DFG) we imply a homogeneous SDF graph. <p> There is no loss of generality in the homogeneity assumption, because an arbitrary SDF graph can be efficiently transformed into a homogeneous graph [16]. The DFG corresponding to an application may be extracted directly from a block diagram specification (e.g. in Ptolemy <ref> [12] </ref>) or from an applicative language like Silage (as done for example in Hyper [17]). The dataow graphs of interest for the purpose of representing DSP algorithms are run repetitively in a non-terminating fashion (i.e. are iterative in nature); tokens ow from source actors to sink actors continually. <p> Empirical evidence indicates that such a model for actor execution times applies to most DSP applications that are represented as SDF graphs and are implemented on programmable processors <ref> [12, 13, 15] </ref>. For such applications the performance penalty due to lack of dynamic load balancing is overcome by the much smaller run time scheduling overhead involved when static assignment and ordering is employed.
Reference: [13] <author> R. Lauwereins, M. Engels, J.A. Peperstraete, E. Steegmans, and J. Van Ginderdeuren, </author> <title> GRAPE: A CASE Tool for Digital Signal Parallel Processing, </title> <journal> IEEE ASSP Magazine, </journal> <volume> Vol. 7, No. 2, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: SDF and closely related models have been widely used for representing a significant class of digital signal processing (DSP) algorithms in CAD tools for system-level simulation, design, and prototyping <ref> [12, 13, 14, 15] </ref>. In this paper we assume that the application is a homogeneous SDF graph, i.e. a graph in which actors always produce and consume exactly one token; henceforth whenever we refer to a dataow graph (DFG) we imply a homogeneous SDF graph. <p> Empirical evidence indicates that such a model for actor execution times applies to most DSP applications that are represented as SDF graphs and are implemented on programmable processors <ref> [12, 13, 15] </ref>. For such applications the performance penalty due to lack of dynamic load balancing is overcome by the much smaller run time scheduling overhead involved when static assignment and ordering is employed.
Reference: [14] <author> D. B. Powell, E. A. Lee, and W. C. Newman, </author> <title> Direct Synthesis of Optimized DSP Assembly Code from Signal Flow Block Diagrams, </title> <booktitle> Proceedings of the International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <address> San Francisco, </address> <month> March </month> <year> 1992. </year> <month> 26 </month>
Reference-contexts: SDF and closely related models have been widely used for representing a significant class of digital signal processing (DSP) algorithms in CAD tools for system-level simulation, design, and prototyping <ref> [12, 13, 14, 15] </ref>. In this paper we assume that the application is a homogeneous SDF graph, i.e. a graph in which actors always produce and consume exactly one token; henceforth whenever we refer to a dataow graph (DFG) we imply a homogeneous SDF graph.
Reference: [15] <author> S. Ritz, M. Pankert, and H. Meyr, </author> <title> High Level Software Synthesis for Signal Processing Systems, </title> <booktitle> Proceedings of the International Conference on Application Specific Array Processors, </booktitle> <address> Berkeley, </address> <month> August </month> <year> 1992, </year> <month> pp.679-693. </month>
Reference-contexts: SDF and closely related models have been widely used for representing a significant class of digital signal processing (DSP) algorithms in CAD tools for system-level simulation, design, and prototyping <ref> [12, 13, 14, 15] </ref>. In this paper we assume that the application is a homogeneous SDF graph, i.e. a graph in which actors always produce and consume exactly one token; henceforth whenever we refer to a dataow graph (DFG) we imply a homogeneous SDF graph. <p> Empirical evidence indicates that such a model for actor execution times applies to most DSP applications that are represented as SDF graphs and are implemented on programmable processors <ref> [12, 13, 15] </ref>. For such applications the performance penalty due to lack of dynamic load balancing is overcome by the much smaller run time scheduling overhead involved when static assignment and ordering is employed.
Reference: [16] <author> E. A. Lee, </author> <title> A Coupled Hardware and Software Architecture for Programmable DSPs, </title> <type> Ph. D. Thesis, </type> <institution> Department of Electrical Engineering and Computer Sciences, University of Califor-nia Berkeley, </institution> <month> May </month> <year> 1986. </year>
Reference-contexts: There is no loss of generality in the homogeneity assumption, because an arbitrary SDF graph can be efficiently transformed into a homogeneous graph <ref> [16] </ref>. The DFG corresponding to an application may be extracted directly from a block diagram specification (e.g. in Ptolemy [12]) or from an applicative language like Silage (as done for example in Hyper [17]). <p> The and values are as follows: , , , , and , , , , and In some cases it is advantageous to unfold a graph by a certain unfolding factor, say , and schedule iterations of the graph together in order to exploit inter-iteration parallelism more effectively <ref> [16, 6] </ref>. The unfolded graph contains copies of each actor of the original graph.
Reference: [17] <author> J. M. Rabaey, C. Chu, P. Hoang, and M. Potkonjak, </author> <title> Fast Prototyping of Datapath Intensive Architectures, </title> <journal> IEEE Design and Test of Computers, </journal> <volume> vol. 8, no. 2, </volume> <month> June </month> <year> 1991, </year> <pages> pp. 40-51. </pages>
Reference-contexts: The DFG corresponding to an application may be extracted directly from a block diagram specification (e.g. in Ptolemy [12]) or from an applicative language like Silage (as done for example in Hyper <ref> [17] </ref>). The dataow graphs of interest for the purpose of representing DSP algorithms are run repetitively in a non-terminating fashion (i.e. are iterative in nature); tokens ow from source actors to sink actors continually. An iteration refers to one complete execution of all the actors in the DFG.
Reference: [18] <author> E. A. Lee, and S. Ha, </author> <title> Scheduling Strategies for Multiprocessor Real-Time DSP, </title> <booktitle> Proceedings of the Globecom Conference, </booktitle> <address> Dallas Texas, </address> <month> November </month> <year> 1989, </year> <pages> pp. 1279-1283. </pages>
Reference-contexts: Each of these three tasks may be performed either at run time (a dynamic strategy) or at compile time (static strategy). In <ref> [18] </ref> and [10] the authors propose a scheduling taxonomy based on which of these tasks are performed at compile time and which at run time; in this paper we use the same terminology that was introduced there. <p> An obvious strategy for solving this problem is to introduce explicit synchronization whenever processors communicate. This leads to the self-timed scheduling strategy of <ref> [18] </ref>. In this strategy we first obtain an FS schedule using the execution time estimates, but we only retain the processor assignment and the ordering of actors on each processor as specified by , and discard the precise timing information specified in the fully-static schedule.
Reference: [19] <author> S. Borkar et. al., </author> <title> iWarp: An Integrated Solution to High-Speed Parallel Computing, </title> <booktitle> Proceedings of Supercomputing 1988 Conference, </booktitle> <address> Orlando, Florida, </address> <year> 1988. </year>
Reference-contexts: Such a strategy is employed in VLIW arrays [3], and systolic arrays <ref> [19, 20] </ref>. The classic controller/datapath architecture is also in a sense an example of fully-static scheduling: the controller determines what each functional unit in the datapath does during each clock cycle.
Reference: [20] <author> L. Thiele, </author> <title> Resource constrained scheduling of uniform algorithms, </title> <journal> Journal of VLSI Signal Processing, </journal> <month> Aug. </month> <year> 1995, </year> <note> vol.10, (no.3):295-310 </note>
Reference-contexts: Such a strategy is employed in VLIW arrays [3], and systolic arrays <ref> [19, 20] </ref>. The classic controller/datapath architecture is also in a sense an example of fully-static scheduling: the controller determines what each functional unit in the datapath does during each clock cycle.
Reference: [21] <author> T. C. Hu, </author> <title> Parallel Sequencing and Assembly Line Problems, </title> <journal> Operations Research, </journal> <volume> vol. 9(6), </volume> <month> November </month> <year> 1961, </year> <pages> pp. 841-848. </pages>
Reference-contexts: v k,( ) 0= end v k,( ) 0= k 0&lt; G start v k,( ) p v 1 v n 1+ V E,( ) v k p v k p D p ( ) d v i v j ,( ) = t v ( ) v 9 technique <ref> [21] </ref> and Sihs scheduling heuristics that take IPC costs into account [2], generate a schedule assuming the program terminates after a finite number of iterations.
Reference: [22] <author> G. De Micheli, </author> <title> Synthesis and Optimization of Digital Circuits, </title> <publisher> McGraw Hill Inc., </publisher> <address> New Jersey, </address> <year> 1994. </year>
Reference-contexts: One way to get around this problem is to use guaranteed worst-case execution time estimates when computing the FS schedule. Such worst-case estimates are often used when scheduling hardware in high-level synthesis <ref> [22] </ref>.
Reference: [23] <author> S. Y. Kung, P. S. Lewis, and S. C. Lo, </author> <title> Performance Analysis and Optimization of VLSI Dataow Arrays Journal of Parallel and Distributed Computing, </title> <journal> vol. </journal> <volume> 4, </volume> <year> 1987, </year> <pages> pp. 592-618. </pages>
Reference-contexts: Such buffers may be implemented using shared memory, or by using hardware FIFOs between processors. It is possible to optimize (minimize) buffer sizes such that the throughput is not constrained by the fact that buffer sizes are bounded <ref> [23] </ref>; however, since we are mainly interested in determining the best performance achievable by a ST strategy, we do not consider buffer optimization in this paper. Instead, we assume that the buffers are large enough so that their finite sizes do not affect the throughput of the system of processors. <p> Instead, we assume that the buffers are large enough so that their finite sizes do not affect the throughput of the system of processors. Such an ST strategy is used in wavefront arrays <ref> [23] </ref>, and in situations where a fully-static approach is impractical due variability in execution times of operations. An ST strategy is robust with respect to changes in execution times of actors, because sender-receiver synchronization is performed at run time.
Reference: [24] <author> F. Baccelli, G. Cohen, G. J. Olsder, and J.-P. Quadrat, </author> <title> Synchronization and Linearity, </title> <publisher> John Wiley & Sons Inc., </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: It can be shown that a ST schedule always settles down into a periodic execution pattern, but the number of iterations spanned by the periodic repeating pattern can be exponential in the size of the DFG <ref> [24] </ref>. From Fig. 2, therefore, the average iteration period under the self-timed schedule is 9 units. The average iteration period ( ) for such an idealized self-timed schedule represents a lower bound on the iteration period achievable by any schedule that maintains the same processor assignment and actor ordering. <p> v j v i ,( )d,( ) k v j v i ,( )d" G ipc v t v ( ) 15 in (2) to obtain (3) ASAP execution implies: (4) is really an instance of Reiters computation graph [26], also known as a Timed Marked graph in Petrinet theory <ref> [24, 25] </ref> (the vertices correspond to the transitions in the marked graph, edges correspond to places, and the initial tokens correspond to the initial marking). <p> The quantity on the right hand side of (5) is commonly referred to as the maximum cycle mean of . A cycle in that maximizes the quotient in the RHS of (5) is called a critical cycle. We refer to <ref> [24, 26, 25] </ref> for the proof of (5). If we only have execution time estimates available instead of exact values, and we set above to be these estimated values, then we obtain the estimated iteration period by calculating . <p> Unfortunately, the number of iterations that the repeating pattern spans depends on the structure of , and it can be exponential in the size of the DFG <ref> [24] </ref>. Consequently the memory requirements on the controller that enforces the transaction order can be prohibitively large in certain cases.
Reference: [25] <author> J. L. Peterson, </author> <title> Petri Net Theory and the Modelling of Systems, </title> <publisher> Prentice-Hall Inc., </publisher> <address> Engle-wood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: v j v i ,( )d,( ) k v j v i ,( )d" G ipc v t v ( ) 15 in (2) to obtain (3) ASAP execution implies: (4) is really an instance of Reiters computation graph [26], also known as a Timed Marked graph in Petrinet theory <ref> [24, 25] </ref> (the vertices correspond to the transitions in the marked graph, edges correspond to places, and the initial tokens correspond to the initial marking). <p> The quantity on the right hand side of (5) is commonly referred to as the maximum cycle mean of . A cycle in that maximizes the quotient in the RHS of (5) is called a critical cycle. We refer to <ref> [24, 26, 25] </ref> for the proof of (5). If we only have execution time estimates available instead of exact values, and we set above to be these estimated values, then we obtain the estimated iteration period by calculating .
Reference: [26] <author> R. Reiter, </author> <title> Scheduling Parallel Computations, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> vol. 15. No. 4, </volume> <month> October </month> <year> 1968, </year> <pages> pp. 590-599. </pages>
Reference-contexts: 5 k start v i k,( ) end v j k v j v i ,( )d,( ) k v j v i ,( )d" G ipc v t v ( ) 15 in (2) to obtain (3) ASAP execution implies: (4) is really an instance of Reiters computation graph <ref> [26] </ref>, also known as a Timed Marked graph in Petrinet theory [24, 25] (the vertices correspond to the transitions in the marked graph, edges correspond to places, and the initial tokens correspond to the initial marking). <p> The quantity on the right hand side of (5) is commonly referred to as the maximum cycle mean of . A cycle in that maximizes the quotient in the RHS of (5) is called a critical cycle. We refer to <ref> [24, 26, 25] </ref> for the proof of (5). If we only have execution time estimates available instead of exact values, and we set above to be these estimated values, then we obtain the estimated iteration period by calculating .
Reference: [27] <author> E. L. Lawler, </author> <title> Combinatorial Optimization: Networks and Matroids, </title> <publisher> Holt, Rinehart and Winston, </publisher> <address> New York, </address> <pages> pp. 65-80, </pages> <year> 1976. </year>
Reference-contexts: ( ) v j v i ,( )" E ipc + + G ipc v j v i ,( ) G ipc E ipc V 22 The system of inequalities (8) is a difference constraint problem that can be solved in polynomial time ( ) using the Bellman-Ford shortest-path algorithm <ref> [27, 28] </ref>. The details of this approach are well described in [28]; the essence of it is to construct a constraint graph that has one vertex for each unknown .

References-found: 27

