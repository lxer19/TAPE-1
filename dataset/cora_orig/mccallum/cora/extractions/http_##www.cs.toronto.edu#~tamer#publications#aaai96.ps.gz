URL: http://www.cs.toronto.edu/~tamer/publications/aaai96.ps.gz
Refering-URL: http://www.cs.toronto.edu/~tamer/publications/publications.html
Root-URL: 
Email: e-mail: ftamer|dtg@cs.toronto.edu  
Title: Motion and Color Analysis for Animat Perception  
Author: Tamer F. Rabie and Demetri Terzopoulos 
Address: 10 King's College Road, Toronto, Ontario, M5S 3G4, Canada  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: We propose novel gaze control algorithms for active perception in mobile autonomous agents with directable, foveated vision sensors. Our agents are realistic artificial animals, or animats, situated in physics-based virtual worlds. Their active perception systems continuously analyze photoreal-istic retinal image streams to glean information useful for controlling the animat's eyes and body. The vision system computes optical flow and segments moving targets in the low-resolution visual periphery. It then matches segmented targets against mental models of colored objects of interest. The eyes saccade to increase acuity by foveating objects. The resulting sensorimotor control loop supports complex behaviors, such as predation. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Adler, H. E. </author> <year> 1975. </year> <title> Fish Behavior: Why Fishes do What They Do. </title> <address> Neptune City, NJ: </address> <publisher> T.F.H Publications. </publisher>
Reference-contexts: Motion and color play an important role in animal perception. Birds and insects exploit optical flow for obstacle avoidance and to control their ego-motion (Gibson 1979). Some species of fish are able to recognize the color signatures of other fish and use this information in certain piscene behaviors <ref> (Adler 1975) </ref>. The human visual system is highly sensitive to motion and color. We tend to focus our attention on moving colorful objects.
Reference: <editor> Bajcsy, R. </editor> <year> 1988. </year> <title> Active perception. </title> <booktitle> Proceedings of the IEEE 76(8) </booktitle> <pages> 996-1005. </pages>
Reference: <author> Ballard, D. </author> <year> 1991. </year> <title> Animate vision. </title> <booktitle> Artificial Intelligence 48 </booktitle> <pages> 57-86. </pages>
Reference-contexts: To detect and localize any target that may be imaged in the low resolution periphery of its retinas, the animat vision system of the fish employs an improved version of a color indexing algorithm proposed by Swain <ref> (Swain & Ballard 1991) </ref>. 1 Since each model object has a unique color histogram signature, it can be detected in the retinal image by histogram intersection and localized by histogram backpro-jection.
Reference: <author> Black, M., and Anandan, P. </author> <year> 1990. </year> <title> A model for the detection of motion over time. </title> <booktitle> In Proc. Inter. Conf. Computer Vision (ICCV'90), </booktitle> <pages> 33-37. </pages>
Reference: <author> Black, M., and Anandan, P. </author> <year> 1993. </year> <title> A framework for the robust estimation of optical flow. </title> <booktitle> In Proc. Inter. Conf. Computer Vision (ICCV'93), </booktitle> <pages> 231-236. </pages>
Reference: <author> Black, M. </author> <year> 1992. </year> <title> Robust incremental optical flow. </title> <type> Technical Report YALEU/DCS/RR-923, </type> <institution> Yale University, Dept. of Computer Science. </institution>
Reference-contexts: As is noted by Black, the goal is incrementally to integrate motion information from new images with previous optical flow estimates to obtain more accurate information about the motion in the scene over time. A detailed description of this method can be found in <ref> (Black 1992) </ref>. Here we describe our adaptation of the algorithm to the animat vision system.
Reference: <author> Blake, A., and Zisserman, A., eds. </author> <year> 1987. </year> <title> Visual Reconstruction. </title> <address> Cambridge, Massachusetts: </address> <publisher> The MIT Press. </publisher>
Reference-contexts: The minimum is then tracked using the graduated non-convexity (GNC) continuation method <ref> (Blake & Zisserman 1987) </ref> by decreasing the values of the parameters from one iteration to the next, which serves to gradually return the cost function to its non-convex shape, thereby introducing discontinuities in the data, spatial, and temporal terms.
Reference: <author> Burt, P., and Adelson, E. </author> <year> 1983. </year> <title> The laplacian pyramid as a compact image code. </title> <journal> IEEE Trans. on Communications 31(4) </journal> <pages> 532-540. </pages> <note> 283 284 285 286 287 300 pursues a red reference fish (frames 283-285). A blue reference fish appears in the predator's right periphery and is recognized, </note> <author> fixated, </author> <title> and tracked (frames 286-300). The white lines indicate the gaze direction. points at frame 286 from left (negative angles) to right (pos itive angles). </title>
Reference-contexts: The values of the parameters are determined empirically (typically D = 10; S = T = 1). To deal with large motions in the image sequence, we perform the minimization using a coarse-to-fine flow-through strategy. A Gaussian pyramid <ref> (Burt & Adelson 1983) </ref> is constructed for each image in the sequence, and minimization starts at the coarsest level and flows through to the finest resolution level.
Reference: <author> Burt, P.; Bergen, J.; Hingorani, R.; Kolczynski, R.; Lee, W.; Le-ung, A.; Lubin, J.; and Shvaytser, H. </author> <year> 1989. </year> <title> Object tracking with a moving camera: An application of dynamic motion analysis. </title> <booktitle> Proc. IEEE Workshop Visual Motion 2 - 12. </booktitle>
Reference-contexts: Where to Look Next Redirecting gaze when a target of interest appears in the periphery can be a complex problem. One solution would be to section the peripheral image into smaller patches or focal probes <ref> (Burt et al. 1989) </ref> and search of all the probes. The strategy will work well for sufficiently small images, but for dynamic vision systems that must process natural or photorealistic images the approach is not effective.
Reference: <author> Campani, M.; Giachetti, A.; and Torre, V. </author> <year> 1995. </year> <title> Optic flow and autonomous navigation. </title> <booktitle> Perception 24 </booktitle> <pages> 253-267. </pages>
Reference: <author> Cedras, C., and Shah, M. </author> <year> 1995. </year> <title> Motion-based recognition: A survey. </title> <booktitle> Image and Vision Computing 13(2) </booktitle> <pages> 129-155. </pages>
Reference-contexts: The human visual system is highly sensitive to motion and color. We tend to focus our attention on moving colorful objects. Motionless objects whose colors blend in to the background are not as easily detectable, and several camouflage strategies in the animal kingdom rely on this fact <ref> (Cedras & Shah 1995) </ref>. Following the animat vision paradigm, the motion and color based gaze control algorithms that we propose in this paper are implemented and evaluated within artificial fishes in a virtual marine world (Fig. 1).
Reference: <author> Dubuisson, M., and Jain, A. </author> <year> 1993. </year> <title> Object contour extraction using color and motion. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf. (CVPR'93), </booktitle> <pages> 471-476. </pages>
Reference-contexts: The conjunction of color and motion cues has recently been exploited to produce more exact segmentations and for the extraction of object contours from natural scenes <ref> (Dubuisson & Jain 1993) </ref>. Color and motion features of video images have been used for color video image classification and understanding (Gong & Sakauchi 1992). Integrating motion and color for object recognition can improve the robustness of moving colored object recognition.
Reference: <author> Gibson, J. J. </author> <year> 1979. </year> <title> The Ecological Approach to Visual Perception. </title> <address> Boston, MA: </address> <publisher> Houghton Mifflin. </publisher>
Reference-contexts: Introduction Animals are active observers of their environment <ref> (Gibson 1979) </ref>. This fact has inspired a trend in the computer vision field popularly known as active vision (Bajcsy 1988; Ballard 1991; Swain & Stricker 1993). Unfortunately, efforts to create active vision systems for physical robots have been hampered by hardware and processor limitations. <p> In this paper we present a solution to this problem through the exploitation of motion and color information. Motion and color play an important role in animal perception. Birds and insects exploit optical flow for obstacle avoidance and to control their ego-motion <ref> (Gibson 1979) </ref>. Some species of fish are able to recognize the color signatures of other fish and use this information in certain piscene behaviors (Adler 1975). The human visual system is highly sensitive to motion and color. We tend to focus our attention on moving colorful objects. <p> Robust Optical Flow A key component of the selective attention algorithm is the use of optical flow. Given a sequence of time-varying images, points on the retina appear to move because of the relative motion between the eye and objects in the scene <ref> (Gibson 1979) </ref>. The vector field of this apparent motion is usually called optical flow (Horn 1986). Optical flow can give important information about the spatial arrangement of objects viewed and the rate of change of this arrangement.
Reference: <author> Gong, Y., and Sakauchi, M. </author> <year> 1992. </year> <title> An object-oriented method for color video image classification using the color and motion features of video images. </title> <booktitle> In 2nd. Inter. Conf. on Automation, Robotics and Computer Vision. </booktitle>
Reference-contexts: The conjunction of color and motion cues has recently been exploited to produce more exact segmentations and for the extraction of object contours from natural scenes (Dubuisson & Jain 1993). Color and motion features of video images have been used for color video image classification and understanding <ref> (Gong & Sakauchi 1992) </ref>. Integrating motion and color for object recognition can improve the robustness of moving colored object recognition. Motion may be considered a bottom-up alerting cue, while color can be used as a top-down cue for model-based recognition (Swain, Kahn, & Ballard 1992).
Reference: <author> Hampel, F. </author> <year> 1974. </year> <title> The influence curve and its role in robust estimation. </title> <journal> J. Amer. Statistical Association 69(346) </journal> <pages> 383-393. </pages>
Reference: <author> Horn, B. K. P. </author> <year> 1986. </year> <title> Robot Vision. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The displacement is computed as a translational offset in the retinotopic coordinate system by a least squares minimization of the optical flow between image frames at times t and t 1 <ref> (Horn 1986) </ref>. The optical flow stabilization method is robust only for small displacements between frames. Consequently, when the displacement of the target between frames is large enough that the method is likely to produce bad estimates, the foveation module is invoked to re-detect and re-foveate the target as described earlier. <p> Given a sequence of time-varying images, points on the retina appear to move because of the relative motion between the eye and objects in the scene (Gibson 1979). The vector field of this apparent motion is usually called optical flow <ref> (Horn 1986) </ref>. Optical flow can give important information about the spatial arrangement of objects viewed and the rate of change of this arrangement. For our specific application, however, we require efficiency, robustness to outliers, and an optical flow estimate at all times.
Reference: <author> Jagersand, M. </author> <year> 1995. </year> <title> Saliency maps and attention selection in scale and spatial coordinates: An information theoretic approach. </title> <booktitle> In Proc. Inter. Conf. Computer Vision (ICCV'95), </booktitle> <pages> 195-202. </pages>
Reference-contexts: Fig. 6 shows this more 2 Reasonably small areas suffice, since objects in the 64 fi 64 peripheral image are typically small at peripheral resolution. Methods for estimating appropriate areas for the object, such as Jagersand's information theoretic approach <ref> (Jagersand 1995) </ref>, may be applicable. 3 By continuously, we mean that there is an estimate of the optical flow at every time instant. 283 284 285 286 Saliency Image bright areas indicating large motions. time. clearly.
Reference: <editor> Maver, J., and Bajcsy, R. </editor> <year> 1990. </year> <title> How to decide from from the first view where to look next. </title> <booktitle> In Proc. DARPA Image Understanding Workshop. </booktitle>
Reference: <author> Rimey, R., and Brown, C. </author> <year> 1992. </year> <title> Where to look next using a bayes net: Incorporating geometric relations. </title> <booktitle> In Proc. Euro. Conf. Computer Vision (ECCV'92), </booktitle> <pages> 542-550. </pages>
Reference: <author> Swain, M., and Ballard, D. </author> <year> 1991. </year> <title> Color indexing. </title> <journal> Inter. J. Computer Vision 7:11 - 32. </journal>
Reference-contexts: To detect and localize any target that may be imaged in the low resolution periphery of its retinas, the animat vision system of the fish employs an improved version of a color indexing algorithm proposed by Swain <ref> (Swain & Ballard 1991) </ref>. 1 Since each model object has a unique color histogram signature, it can be detected in the retinal image by histogram intersection and localized by histogram backpro-jection.
Reference: <author> Swain, M., and Stricker, M. </author> <year> 1993. </year> <title> Promising directions in active vision. </title> <journal> Inter. J. Computer Vision 11(2):109 - 126. </journal>
Reference: <author> Swain, M.; Kahn, R.; and Ballard, D. </author> <year> 1992. </year> <title> Low resolution cues for guiding saccadic eye movements. </title> <booktitle> In Proc. Inter. Conf. Computer Vision (ICCV'92), </booktitle> <pages> 737-740. </pages>
Reference-contexts: Integrating motion and color for object recognition can improve the robustness of moving colored object recognition. Motion may be considered a bottom-up alerting cue, while color can be used as a top-down cue for model-based recognition <ref> (Swain, Kahn, & Ballard 1992) </ref>. Therefore, integrating motion and color can increase the robustness of the recognition problem by bridging the gap between bottom-up and top-down processes, thus, improving the selective attention of dynamic perceptual systems such as the animat vision system that we are developing.
Reference: <author> Terzopoulos, D., and Rabie, T. </author> <year> 1995. </year> <title> Animat vision: </title> <booktitle> Active vision in artificial animals. In Proc. Fifth Inter. Conf. Computer Vision (ICCV'95), </booktitle> <volume> 801 - 808. </volume>
Reference-contexts: In the present work, the fish animat serves as an autonomous mobile robot situated in a photorealistic, dynamic environment. Our new gaze control algorithms significantly enhance the prototype animat vision system that we implemented in prior work <ref> (Terzopoulos & Rabie 1995) </ref> and they support more robust vision-guided navigation abilities in the artificial fish. We review the animat vision system in the next section before presenting our new work on integrating motion and color analysis for animat perception in subsequent sections. <p> We review the animat vision system in the next section before presenting our new work on integrating motion and color analysis for animat perception in subsequent sections. A Prototype Animat Vision System The basic functionality of the animat vision system, which is described in detail in <ref> (Terzopoulos & Rabie 1995) </ref>, starts with binocular perspective projection of the color 3D world onto the animat's 2D retinas. Retinal imaging is accomplished by photorealistic graphics rendering of the world from the animat's point of view. This projection respects occlusion relationships among objects. <p> Module A in Fig. 2 performs the saccades by incrementing 1 Our improvements, which include iterative model histogram scaling and weighted histograms, make the technique much more robust against the large variations in scale that occur in our application. The details of the improved algorithm are presented in <ref> (Terzopoulos & Rabie 1995) </ref>. l = 0 l = 1 l = 2 l = 3 l = 0 l = 1 l = 2 l = 3 Left eye Right eye (b) are peripheral images; l = 3 is foveal image. (b) Composited retinal images (borders of composited component images
Reference: <author> Terzopoulos, D.; Tu, X.; and Grzeszczuk, R. </author> <year> 1994. </year> <title> Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world. </title> <booktitle> Artificial Life 1(4) </booktitle> <pages> 327-351. </pages>
Reference-contexts: Up/down turn motor commands are issued to the fish's pectoral fins, with an above-threshold positive P interpreted as up and negative as down. The motor controllers are explained in <ref> (Terzopoulos, Tu, & Grzeszczuk 1994) </ref>. The remainder of the paper presents our new work on integrating color and motion analysis in active vision. Integrating Motion and Color for Attention Selective attention is an important mechanism for dealing with the combinatorial aspects of search in vision (Tsotsos et al. 1995).
Reference: <author> Terzopoulos, D. </author> <year> 1995. </year> <title> Modeling living systems for computer vision. </title> <booktitle> In Proc. Int. Joint Conf. Artificial Intelligence (IJCAI'95), </booktitle> <pages> 1003-1013. </pages>
Reference-contexts: In the present work, the fish animat serves as an autonomous mobile robot situated in a photorealistic, dynamic environment. Our new gaze control algorithms significantly enhance the prototype animat vision system that we implemented in prior work <ref> (Terzopoulos & Rabie 1995) </ref> and they support more robust vision-guided navigation abilities in the artificial fish. We review the animat vision system in the next section before presenting our new work on integrating motion and color analysis for animat perception in subsequent sections. <p> We review the animat vision system in the next section before presenting our new work on integrating motion and color analysis for animat perception in subsequent sections. A Prototype Animat Vision System The basic functionality of the animat vision system, which is described in detail in <ref> (Terzopoulos & Rabie 1995) </ref>, starts with binocular perspective projection of the color 3D world onto the animat's 2D retinas. Retinal imaging is accomplished by photorealistic graphics rendering of the world from the animat's point of view. This projection respects occlusion relationships among objects. <p> Module A in Fig. 2 performs the saccades by incrementing 1 Our improvements, which include iterative model histogram scaling and weighted histograms, make the technique much more robust against the large variations in scale that occur in our application. The details of the improved algorithm are presented in <ref> (Terzopoulos & Rabie 1995) </ref>. l = 0 l = 1 l = 2 l = 3 l = 0 l = 1 l = 2 l = 3 Left eye Right eye (b) are peripheral images; l = 3 is foveal image. (b) Composited retinal images (borders of composited component images
Reference: <author> Tsotsos, J.; Culhane, S.; Wai, W.; Lai, Y.; Davis, N.; and Nuflo, F. </author> <year> 1995. </year> <title> Modeling visual attention via selective tuning. </title> <booktitle> Artificial Intelligence 78 </booktitle> <pages> 507-545. </pages>
Reference-contexts: The remainder of the paper presents our new work on integrating color and motion analysis in active vision. Integrating Motion and Color for Attention Selective attention is an important mechanism for dealing with the combinatorial aspects of search in vision <ref> (Tsotsos et al. 1995) </ref>. Deciding where to redirect the fovea can involve a complex search process (Tsotsos et al. 1995; Rimey & Brown 1992; Maver & Bajcsy 1990). In this section we offer an efficient solution which integrates motion and color to increase the robustness of our animat's perceptual functions.
References-found: 26

