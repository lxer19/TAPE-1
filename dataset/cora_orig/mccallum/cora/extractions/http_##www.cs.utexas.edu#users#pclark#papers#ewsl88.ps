URL: http://www.cs.utexas.edu/users/pclark/papers/ewsl88.ps
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/ewsl88.abs.html
Root-URL: 
Email: (pete@turing.ac.uk)  
Title: Representing Arguments as Background Knowledge for Constraining Generalisation  
Author: Peter Clark 
Address: 36 N.Hanover St Glasgow, UK  
Affiliation: The Turing Institute  
Web: http://www.cs.utexas.edu/users/pclark/papers/ewsl88.ps  
Note: In: Proc. Third European Working Session on Learning (EWSL-88), pp37-44, Ed: Derek Sleeman, UK: Pitman (Oct 1988)  
Abstract: The use of statistical measures to constrain generalisa-tion in learning systems has proved successful in many domains, but can only be applied where large numbers of examples exist. In domains where few training examples are available, other mechanisms for constraining gener-alisation are required. In this paper, we propose a representation of background knowledge based on arguments for and against a hypothesis rather than as statements in logic or probabilistic relations, and show how it can be used to constrain generalisation from single examples (sometimes referred to as `case-based reasoning'). Examples are characterised by the set of arguments for and against a hypothesis of interest, and the resolution of conflicting arguments in a current problem is obtained by firstly locating an old example where the same or a similar conflict occurred, then secondly generalising the solution in the old example to also apply to the new problem. This allows learning to occur in domains where few training examples exist and background knowledge is available. We provide a description of this method in logical form, and analyse the assumptions under which it is valid, its limitations and possible future extensions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bundy. </author> <title> Probability, truth and logic: reply to cheeseman. </title> <journal> Computational Intelligence, </journal> <volume> 4(1) </volume> <pages> 69-70, </pages> <month> Feb </month> <year> 1988. </year>
Reference-contexts: A second problem is that many probabilistic representations are only `proof functional' (ie. able to calculate a value for A ^ B given A and B <ref> [1] </ref>) by making assumptions of independence generally not justified in many domains. Cheeseman [2] and the subsequent debate provides an excellent discussion on the issue of Bayesian and other probabilistic representations for learning systems.
Reference: [2] <author> P. Cheeseman. </author> <title> An inquiry into computer under standing. </title> <journal> Computational Intelligence, </journal> <volume> 4(1), </volume> <month> Feb </month> <year> 1988. </year> <note> (followed by 23 commentaries). </note>
Reference-contexts: A second problem is that many probabilistic representations are only `proof functional' (ie. able to calculate a value for A ^ B given A and B [1]) by making assumptions of independence generally not justified in many domains. Cheeseman <ref> [2] </ref> and the subsequent debate provides an excellent discussion on the issue of Bayesian and other probabilistic representations for learning systems.
Reference: [3] <author> P. Clark. </author> <title> A comparison of exemplar-based and rule based learning systems. </title> <booktitle> In International workshop on Machine Learning, Meta-reasoning and Logics, </booktitle> <pages> pages 69-81, </pages> <month> February </month> <year> 1988. </year> <note> (Also available as TIMLG-15, </note> <institution> Turing Inst., Glasgow, UK). </institution>
Reference-contexts: There are corresponding trade-offs between CBR and rule induction concerning memory usage and efficiency. For a more detailed discussion of these relationships, see <ref> [3] </ref>. It should be noted that, instead of performing `case-based reasoning' to use arguments, we could alternatively form an explicit representation of a concept boundary given a previous case and a set of arguments and use this for classifying new cases rather than use a matching algorithm.
Reference: [4] <author> T. R. Davies and S. J. Russell. </author> <title> A logical approach to reasoning by analogy. </title> <booktitle> In IJCAI-87, </booktitle> <pages> pages 264-270, </pages> <year> 1987. </year>
Reference-contexts: G a G 6= fg ) Stronger (a G ; fg) (4) and that the relation is antisymmetric: 8a G ; a :G Stronger (a G ; a :G ) , :Stronger (a :G ; a G ) 2 This can be expressed using Davies and Russell's notation for determinations <ref> [4] </ref> as: Args (a G ; "G (&lt;x &gt;)") ^ Args (a :G ; ":G (&lt;x i being a polar variable representing the truth value of G (x 3 ie.
Reference: [5] <author> J. DeKleer. </author> <title> Choices without backtracking. </title> <booktitle> In AAAI-84, </booktitle> <year> 1984. </year>
Reference-contexts: This allows a limited form of uncertainty to be represented, namely that the truth of the logical relations in the domain theory is subject to question (similar to the approach taken in truth maintenance systems <ref> [6, 5] </ref>). This allows more scope for learning (eg. [9, 19]), and is sometimes referred to as `reasoning with an imperfect domain theory' [13].
Reference: [6] <author> J. Doyle. </author> <title> A truth maintenance system. </title> <journal> Artificial Intelligence, </journal> <volume> 12 </volume> <pages> 231-272, </pages> <year> 1979. </year>
Reference-contexts: This allows a limited form of uncertainty to be represented, namely that the truth of the logical relations in the domain theory is subject to question (similar to the approach taken in truth maintenance systems <ref> [6, 5] </ref>). This allows more scope for learning (eg. [9, 19]), and is sometimes referred to as `reasoning with an imperfect domain theory' [13].
Reference: [7] <author> R. Fikes, P. Hart, and N. Nilsson. </author> <title> Learning and executing generalized robot plans. </title> <editor> In B. Webber and N. Nilsson, editors, </editor> <booktitle> Readings in AI, </booktitle> <pages> pages 231-249, </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1981. </year>
Reference-contexts: The representation and use of such domain knowledge in the generalisation process is the subject of this paper. In contrast to work on the inductive rule learning paradigm, research in the paradigm of explanation-based learning has concerned itself with the use of knowledge beyond a set of examples (eg. <ref> [7, 14, 10] </ref>). Explanation-based learning (EBL) makes use of logical implication as the basic relation for expressing domain knowledge. For example, a typical statement used by an EBL system might be Partof (x; y) ^ Isa (y; Bottom) ^ Is (y; Flat) ) Stable (x) (taken from [13]).
Reference: [8] <author> M. R. Genesereth and N. J. Nilsson. </author> <booktitle> Logical Foun dations for Artificial Intelligence. </booktitle> <publisher> Kaufmann, </publisher> <address> Ca, </address> <year> 1987. </year>
Reference-contexts: When referring to a fact itself (rather than its truth value), we follow the convention of Gene-sereth and Nilsson of writing the fact in quotes <ref> [8] </ref>.
Reference: [9] <author> R. Hall. </author> <title> Learning by Failing to Explain. </title> <type> Technical Report 906 (AI-TR-906), </type> <institution> MIT AI Laboratory, </institution> <year> 1986. </year>
Reference-contexts: This allows a limited form of uncertainty to be represented, namely that the truth of the logical relations in the domain theory is subject to question (similar to the approach taken in truth maintenance systems [6, 5]). This allows more scope for learning (eg. <ref> [9, 19] </ref>), and is sometimes referred to as `reasoning with an imperfect domain theory' [13].
Reference: [10] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Chunking in soar: the anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: The representation and use of such domain knowledge in the generalisation process is the subject of this paper. In contrast to work on the inductive rule learning paradigm, research in the paradigm of explanation-based learning has concerned itself with the use of knowledge beyond a set of examples (eg. <ref> [7, 14, 10] </ref>). Explanation-based learning (EBL) makes use of logical implication as the basic relation for expressing domain knowledge. For example, a typical statement used by an EBL system might be Partof (x; y) ^ Isa (y; Bottom) ^ Is (y; Flat) ) Stable (x) (taken from [13]).
Reference: [11] <author> R. S. Michalski and R. Chilausky. </author> <title> Learning by being told and learning from examples: an experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean diagnosis. </title> <journal> Policy Analysis and Information Systems, </journal> <volume> 4(2) </volume> <pages> 125-160, </pages> <year> 1980. </year>
Reference: [12] <author> R. S. Michalski and J. Larson. </author> <title> Incremental genera tion of VL 1 hypotheses: the underlying methodology and the description of program AQ11. </title> <type> Technical Report ISG 83-5, </type> <institution> The University of of Illinois at Urbana-Champaign, Department of Computer Science, Urbana, </institution> <year> 1983. </year>
Reference-contexts: which generalisations justified by arguments are valid, the limitations of this formalism and its possible future extensions. 2 The Problem Much work in machine learning makes use of statistical measures to select between alternative inductive hypotheses drawn on the basis of examples (for example the systems ID3 [15] and AQ11 <ref> [12] </ref>). In order that the application of statistical measures is reliable, it is necessary to have a large number of training examples available. In many domains where the constraint for a large number of examples is met, rule induction methodology has proved successful ([18, 17, 11, 16]).
Reference: [13] <author> T. M. Mitchell, R. M. Keller, and S. T. Kedar Cabelli. </author> <title> Explanation-based generalization: a unifying view. </title> <journal> Machine Learning Journal, </journal> <volume> 1(1) </volume> <pages> 47-80, </pages> <year> 1986. </year>
Reference-contexts: Explanation-based learning (EBL) makes use of logical implication as the basic relation for expressing domain knowledge. For example, a typical statement used by an EBL system might be Partof (x; y) ^ Isa (y; Bottom) ^ Is (y; Flat) ) Stable (x) (taken from <ref> [13] </ref>). Using such knowledge, EBL techniques have been demonstrated as a useful means for improving the efficiency of problem-solving systems. <p> This allows more scope for learning (eg. [9, 19]), and is sometimes referred to as `reasoning with an imperfect domain theory' <ref> [13] </ref>. However, this still does not allow representation of degrees of belief in the truth of a logical implication or of probabilistic implication; statements of the form `x is usually true', although a common part of much expert knowledge, are still unrepresentable within this framework.
Reference: [14] <author> J. Mostow and N. Bhatnagar. </author> <title> Failsafe a floor plan ner that uses ebg to learn from its failures. </title> <editor> In J. Mc-Dermott, editor, </editor> <booktitle> IJCAI-87, </booktitle> <pages> pages 249-255, </pages> <address> Kauf-mann, Ca, </address> <year> 1987. </year>
Reference-contexts: The representation and use of such domain knowledge in the generalisation process is the subject of this paper. In contrast to work on the inductive rule learning paradigm, research in the paradigm of explanation-based learning has concerned itself with the use of knowledge beyond a set of examples (eg. <ref> [7, 14, 10] </ref>). Explanation-based learning (EBL) makes use of logical implication as the basic relation for expressing domain knowledge. For example, a typical statement used by an EBL system might be Partof (x; y) ^ Isa (y; Bottom) ^ Is (y; Flat) ) Stable (x) (taken from [13]).
Reference: [15] <author> J. R. Quinlan. </author> <title> Learning efficient classification pro cedures and their application to chess endgames. </title> <editor> In J. G. Carbonell, R. S. Michalski, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> vol. 1, </volume> <publisher> Tioga, </publisher> <address> Palo Alto, Ca, </address> <year> 1983. </year>
Reference-contexts: the assumptions under which generalisations justified by arguments are valid, the limitations of this formalism and its possible future extensions. 2 The Problem Much work in machine learning makes use of statistical measures to select between alternative inductive hypotheses drawn on the basis of examples (for example the systems ID3 <ref> [15] </ref> and AQ11 [12]). In order that the application of statistical measures is reliable, it is necessary to have a large number of training examples available. In many domains where the constraint for a large number of examples is met, rule induction methodology has proved successful ([18, 17, 11, 16]).
Reference: [16] <author> J. R. Quinlan, P. J. Compton, K. A. Horn, and L. Lazarus. </author> <title> Inductive knowledge acquisition: a case study. </title> <booktitle> In Proceedings of the second Australian Conference on the Applications of Expert Systems, </booktitle> <pages> pages 183-204, </pages> <institution> New South Wales Institute of Technology, </institution> <address> Sydney, </address> <year> 1986. </year>
Reference: [17] <author> A. Shapiro and T. Niblett. </author> <title> Automatic induction of classification rules for a chess endgame, </title> <journal> pages 73-91. </journal> <volume> Volume 3, </volume> <publisher> Pergamon, Oxford, </publisher> <year> 1982. </year>
Reference: [18] <author> B. Shepherd. </author> <title> An appraisal of a decsion tree ap proach to image classification. </title> <booktitle> In IJCAI-83, </booktitle> <address> Kauf-mann, Ca, </address> <year> 1983. </year>
Reference: [19] <author> K. VanLehn. </author> <title> Learning a domain theory by complet ing explanations. </title> <editor> In T. M. Mitchell, J. G. Carbonell, and R. S. Michalski, editors, </editor> <title> Machine Learning: A Guide to Current Research, </title> <publisher> Kluwer, </publisher> <address> Lancaster, UK, </address> <year> 1986. </year>
Reference-contexts: This allows a limited form of uncertainty to be represented, namely that the truth of the logical relations in the domain theory is subject to question (similar to the approach taken in truth maintenance systems [6, 5]). This allows more scope for learning (eg. <ref> [9, 19] </ref>), and is sometimes referred to as `reasoning with an imperfect domain theory' [13].
References-found: 19

