URL: ftp://ftp.icsi.berkeley.edu/pub/techreports/1997/tr-97-021.ps.gz
Refering-URL: http://www.icsi.berkeley.edu/techreports/1997.html
Root-URL: http://www.icsi.berkeley.edu
Email: (bilmes@cs.berkeley.edu)  
Title: A Gentle Tutorial of the EM Algorithm and its Application to Parameter Estimation for Gaussian
Phone: (510) 643-9153 FAX (510) 643-7684  
Author: Jeff A. Bilmes and 
Date: April 1998  
Address: I 1947 Center St. Suite 600 Berkeley, California 94704-1198  Berkeley CA, 94704  U.C. Berkeley  
Affiliation: INTERNATIONAL COMPUTER SCIENCE INSTITUTE  International Computer Science Institute  Computer Science Division Department of Electrical Engineering and Computer Science  
Pubnum: TR-97-021  
Abstract: We describe the maximum-likelihood parameter estimation problem and how the Expectation-Maximization (EM) algorithm can be used for its solution. We first describe the abstract form of the EM algorithm as it is often given in the literature. We then develop the EM parameter estimation procedure for two applications: 1) finding the parameters of a mixture of Gaussian densities, and 2) finding the parameters of a hidden Markov model (HMM) (i.e., the Baum-Welch algorithm) for both discrete and Gaussian mixture observation models. We derive the update equations in fairly explicit detail but we do not prove any convergence properties. We try to emphasize intuition rather than mathematical rigor. 
Abstract-found: 1
Intro-found: 1
Reference: [ALR77] <author> A.P.Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum-likelihood from incomplete data via the em algorithm. </title> <journal> J. Royal Statist. Soc. Ser. B., </journal> <volume> 39, </volume> <year> 1977. </year>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm. <p> That is, we find: fi (i) = argmax fi These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood and the algorithm is guaranteed to converge to a local maximum of the likelihood function. There are many rate-of-convergence papers (e.g., <ref> [ALR77, RW84, Wu83, JX96, XJ96] </ref>) but we will not discuss them here. A modified form of the M-step is to, instead of maximizing Q (fi; fi (i1) ), we find some fi (i) such that Q (fi (i) ; fi (i1) ) &gt; Q (fi; fi (i1) ).
Reference: [Bis95] <author> C. Bishop. </author> <title> Neural Networks for Pattern Recognition. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1995. </year>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm.
Reference: [GJ95] <author> Z. Ghahramami and M. Jordan. </author> <title> Learning from incomplete data. </title> <note> Technical Report AI Lab Memo No. 1509, CBCL Paper No. 108, </note> <institution> MIT AI Lab, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm.
Reference: [JJ94] <author> M. Jordan and R. Jacobs. </author> <title> Hierarchical mixtures of experts and the em algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214, </pages> <year> 1994. </year>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm.
Reference: [JX96] <author> M. Jordon and L. Xu. </author> <title> Convergence results for the em approach to mixtures of experts architectures. </title> <booktitle> Neural Networks, </booktitle> <volume> 8 </volume> <pages> 1409-1431, </pages> <year> 1996. </year>
Reference-contexts: That is, we find: fi (i) = argmax fi These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood and the algorithm is guaranteed to converge to a local maximum of the likelihood function. There are many rate-of-convergence papers (e.g., <ref> [ALR77, RW84, Wu83, JX96, XJ96] </ref>) but we will not discuss them here. A modified form of the M-step is to, instead of maximizing Q (fi; fi (i1) ), we find some fi (i) such that Q (fi (i) ; fi (i1) ) &gt; Q (fi; fi (i1) ).
Reference: [RJ93] <author> L. Rabiner and B.-H. Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall Signal Processing Series, </publisher> <year> 1993. </year>
Reference-contexts: Find fl = argmax p (Oj). The Baum-Welch (also called forward-backward or EM for HMMs) algorithm solves this problem, and we will develop it presently. In subsequent sections, we will consider only the first and third problems. The second is addressed in <ref> [RJ93] </ref>. 4.1 Efficient Calculation of Desired Quantities One of the advantages of HMMs is that relatively efficient algorithms can be derived for the three problems mentioned above. Before we derive the EM algorithm directly using the Q function, we review these efficient procedures. Recall the forward procedure. <p> The update equations for HMMs with multiple observation sequences can similarly be derived and are addressed in <ref> [RJ93] </ref>.
Reference: [RW84] <author> R. Redner and H. Walker. </author> <title> Mixture densities, maximum likelihood and the em algorithm. </title> <journal> SIAM Review, </journal> <volume> 26(2), </volume> <year> 1984. </year> <month> 13 </month>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm. <p> That is, we find: fi (i) = argmax fi These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood and the algorithm is guaranteed to converge to a local maximum of the likelihood function. There are many rate-of-convergence papers (e.g., <ref> [ALR77, RW84, Wu83, JX96, XJ96] </ref>) but we will not discuss them here. A modified form of the M-step is to, instead of maximizing Q (fi; fi (i1) ), we find some fi (i) such that Q (fi (i) ; fi (i1) ) &gt; Q (fi; fi (i1) ).
Reference: [Wu83] <author> C.F.J. Wu. </author> <title> On the convergence properties of the em algorithm. </title> <journal> The Annals of Statistics, </journal> <volume> 11(1) </volume> <pages> 95-103, </pages> <year> 1983. </year>
Reference-contexts: For many problems, however, it is not possible to find such analytical expressions, and we must resort to more elaborate techniques. 2 Basic EM The EM algorithm is one such elaborate technique. The EM algorithm <ref> [ALR77, RW84, GJ95, JJ94, Bis95, Wu83] </ref> is a general method of finding the maximum-likelihood estimate of the parameters of an underlying distribution from a given data set when the data is incomplete or has missing values. There are two main applications of the EM algorithm. <p> That is, we find: fi (i) = argmax fi These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood and the algorithm is guaranteed to converge to a local maximum of the likelihood function. There are many rate-of-convergence papers (e.g., <ref> [ALR77, RW84, Wu83, JX96, XJ96] </ref>) but we will not discuss them here. A modified form of the M-step is to, instead of maximizing Q (fi; fi (i1) ), we find some fi (i) such that Q (fi (i) ; fi (i1) ) &gt; Q (fi; fi (i1) ).
Reference: [XJ96] <author> L. Xu and M.I. Jordan. </author> <title> On convergence properties of the em algorithm for gaussian mixtures. </title> <journal> Neural Computation, </journal> <volume> 8 </volume> <pages> 129-151, </pages> <year> 1996. </year> <month> 14 </month>
Reference-contexts: That is, we find: fi (i) = argmax fi These two steps are repeated as necessary. Each iteration is guaranteed to increase the log-likelihood and the algorithm is guaranteed to converge to a local maximum of the likelihood function. There are many rate-of-convergence papers (e.g., <ref> [ALR77, RW84, Wu83, JX96, XJ96] </ref>) but we will not discuss them here. A modified form of the M-step is to, instead of maximizing Q (fi; fi (i1) ), we find some fi (i) such that Q (fi (i) ; fi (i1) ) &gt; Q (fi; fi (i1) ).
References-found: 9

