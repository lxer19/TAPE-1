URL: http://http.cs.berkeley.edu/~soumen/pldi.ps
Refering-URL: http://http.cs.berkeley.edu/~soumen/restmt.html
Root-URL: http://www.cs.berkeley.edu
Title: Global Communication Analysis and Optimization  
Author: Soumen Chakrabarti Manish Gupta Jong-Deok Choi 
Abstract: Reducing communication cost is crucial to achieving good performance on scalable parallel machines. This paper presents a new compiler algorithm for global analysis and optimization of communication in data-parallel programs. Our algorithm is distinct from existing approaches in that rather than handling loop-nests and array references one by one, it considers all communication in a procedure and their interactions under different placements before making a final decision on the placement of any communication. It exploits the flexibility resulting from this advanced analysis to eliminate redundancy, reduce the number of messages, and reduce contention for cache and communication buffers, all in a unified framework. In contrast, single loop-nest analysis often retains redundant communication, and more aggressive dataflow analysis on array sections can generate too many messages or cache and buffer contention. The algorithm has been implemented in the IBM pHPF compiler for High Performance Fortran. During compilation, the number of messages per processor goes down by as much as a factor of nine for some HPF programs. We present performance results for the IBM SP2 and a network of Sparc workstations (NOW) connected by a Myrinet switch. In many cases, the communication cost is reduced by a factor of two. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agarwal, J. Saltz, and R. Das. </author> <title> Interprocedural partial redundancy elimination and its application to distributed memory compilation. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These include dataflow analysis over array sections for regular computations [10, 14, 17, 18] and over entire arrays for irregular computations <ref> [27, 1] </ref>. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity.
Reference: [2] <author> F. Allen, M. Burke, P. Charles, R. Cytron, and J. Ferrante. </author> <title> An overview of the ptran analysis system for multiprocessing. </title> <booktitle> Proc. ACM 1987 International Conference on Supercomputing, </booktitle> <year> 1987. </year> <note> Also published in Journal of Parallel and Distributed Computing, </note> <month> Oct., </month> <year> 1988, </year> <pages> 5(5) pages 617-640. </pages>
Reference-contexts: code in Figure 4 as a running example to illustrate the operation of the steps of the algorithm. 4.1 Representation and notation We represent the program using the augmented control flow graph (CFG), which makes loop (interval) structure more explicit than the standard CFG by placing preheader and postexit nodes <ref> [2, 23] </ref>. These extra nodes also provide convenient locations for summarizing dataflow information for the loop. The CFG is a directed graph where each node is a basic block, a sequence of statements without jumps.
Reference: [3] <author> S. P. Amarasinghe and M. S. Lam. </author> <title> Communication optimization and code generation for distributed memory machines. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156). <p> Local analysis of array accesses based on dependence testing alone often retains redundant communication. Naturally, the next step was the use of dataflow analysis, e.g.. using precise array dataflow analysis to detect redundant communication within a loop nest <ref> [3] </ref>, and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations [10, 14, 17, 18] and over entire arrays for irregular computations [27, 1].
Reference: [4] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, and S. Ranka. </author> <title> A compilation approach for Fortran 90D/HPF compilers on distributed memory MIMD computers. </title> <booktitle> In Proc. Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Port-land, Oregon, </address> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156).
Reference: [5] <author> T. Brandes. </author> <title> ADAPTOR: A compilation system for data-parallel Fortran programs. </title> <editor> In C. W. Kessler, editor, </editor> <title> Automatic parallelization new approaches to code generation, data distribution, and performance prediction. </title> <booktitle> Vieweg Advanced Studies in Computer Science, </booktitle> <publisher> Vieweg, Wiesbaden, </publisher> <month> Jan. </month> <year> 1994. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156).
Reference: [6] <author> J.-D. Choi, R. Cytron, and J. Ferrante. </author> <title> On the efficient engineering of ambitious program analysis. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 20(2) </volume> <pages> 105-114, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: This is typically done using a backward and forward dataflow approach with array section descriptors or bitvectors. We find it more efficient to exploit the SSA defuse information already computed in an earlier phase <ref> [8, 6] </ref>, refined by array dependence-testing [29]. 2. For each non-local reference, identify a set of candidate positions, any one of which can be potentially chosen as the final point to emit a call to a message-passing runtime routine (x4.4). 3.
Reference: [7] <author> C. Click. </author> <title> Global code motion global value numbering. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 246-257. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1995. </year>
Reference-contexts: In the common message cost model using fixed overhead per message and bandwidth, minimizing the cost is N P-hard (also see x6). In practice, simple greedy heuristics work quite well; see Figure 9 (g). It is similar to Click's global code motion heuristic <ref> [7] </ref>: consider the most constrained communication entry next, and put it where it is compatible in communication pattern (as shown by the test below) with the largest number of other candidate communication.
Reference: [8] <author> R. Cytron, J. Ferrante, B. Rosen, M. Wegman, and F. Zadeck. </author> <title> Efficiently computing static single assignment form and the control dependence graph. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: This is typically done using a backward and forward dataflow approach with array section descriptors or bitvectors. We find it more efficient to exploit the SSA defuse information already computed in an earlier phase <ref> [8, 6] </ref>, refined by array dependence-testing [29]. 2. For each non-local reference, identify a set of candidate positions, any one of which can be potentially chosen as the final point to emit a call to a message-passing runtime routine (x4.4). 3.
Reference: [9] <author> H. P. F. Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Rice University, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Distributed memory architectures provide a cost-effective method of building scalable parallel computers. However, the absence of global address space, and the resulting need for explicit message passing makes these machines difficult to program. This has motivated the design of languages like High Performance Fortran (HPF) <ref> [9] </ref>, which allow the programmer to write sequential or shared-memory parallel programs that are annotated with directives specifying data decomposition.
Reference: [10] <author> E. Granston and A. Veidenbaum. </author> <title> Detecting redundant accesses to array data. </title> <booktitle> In Proc. Supercomputing '91, </booktitle> <pages> pages 854-965, </pages> <year> 1991. </year>
Reference-contexts: Naturally, the next step was the use of dataflow analysis, e.g.. using precise array dataflow analysis to detect redundant communication within a loop nest [3], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [10, 14, 17, 18] </ref> and over entire arrays for irregular computations [27, 1]. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity.
Reference: [11] <author> M. Gupta and P. Banerjee. </author> <title> A methodology for high-level synthesis of communication on multicomputers. </title> <booktitle> In Proc. 6th ACM International Conference on Supercomputing, </booktitle> <address> Washington D.C., </address> <month> July </month> <year> 1992. </year>
Reference-contexts: Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization [15, 30], using collective communication <ref> [11, 20] </ref>, message coalescing [15], and exploiting pipelined communication [15, 12], all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication.
Reference: [12] <author> M. Gupta, S. Midkiff, E. Schonberg, V. Seshadri, K. Wang, D. Shields, W.-M. Ching, and T. Ngo. </author> <title> An HPF compiler for the IBM SP2. </title> <booktitle> In Proc. Supercomputing '95, </booktitle> <address> San Diego, CA, </address> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156). <p> Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization [15, 30], using collective communication [11, 20], message coalescing [15], and exploiting pipelined communication <ref> [15, 12] </ref>, all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication. <p> The algorithm achieves both redundancy elimination and message combining globally, and is able to reduce the number of messages to an extent that is not achievable with any previous approach. Our algorithm has been implemented in the IBM pHPF prototype compiler <ref> [12] </ref>. We report results from a preliminary study of some well-known HPF programs. The performance gains are impressive. Reduction in static message count can be up to a factor of almost nine. Time spent in communication is reduced in many cases by a factor of two or more. <p> In particular, redundancy elimination via earliest placement can prevent the combining possibilities from being exploited. To demonstrate this, we study the NCAR shallow water code, which has NN message 1 URL http://www.npac.syr.edu/hpfa. pattern. As discussed in <ref> [12] </ref>, the message coalescing optimization implemented in the pHPF compiler allows the diagonal communication to be subsumed by an augmented form of the NNC along the two axes. A simplified form of the original code is shown in Figure 2. <p> Even if the programmer were careful enough to write the code in the first column, intermediate passes of compilation may destroy the interval containment. In fact, the current IBM HPF scalarizer <ref> [12] </ref> will translate the F90-style source to the scalarized form in the second column. If loop fusion can be performed before this analysis, as in this case, the problem can be avoided. But this is not always possible [28, x 9.2]. <p> This analysis is done after the compiler has performed transformations like loop distribution and loop interchange to increase opportunities for moving communication outside loops <ref> [12] </ref>. 1. For each RHS expression that may need communication, identify the earliest (x4.3) and latest (x4.2) safe position to place that communication. This is typically done using a backward and forward dataflow approach with array section descriptors or bitvectors. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [30, 15, 12] </ref>. Given a use u, let d range over the reaching regular defs of u. Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u).
Reference: [13] <author> M. Gupta and E. Schonberg. </author> <title> Static analysis to reduce synchronization costs in data-parallel programs. </title> <booktitle> In Principles of Programming Languages (POPL), </booktitle> <address> St. Petersburg Beach, FL, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [26, 22, 13] </ref>. Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations.
Reference: [14] <author> M. Gupta, E. Schonberg, and H. Srinivasan. </author> <title> A unified framework for optimizing communication in data-parallel programs. </title> <type> Technical Report RC 19872(87937) 12/14/94, </type> <institution> IBM Research, </institution> <year> 1994. </year> <note> To appear in IEEE Transactions on Parallel and Distributed Systems. </note>
Reference-contexts: Naturally, the next step was the use of dataflow analysis, e.g.. using precise array dataflow analysis to detect redundant communication within a loop nest [3], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [10, 14, 17, 18] </ref> and over entire arrays for irregular computations [27, 1]. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity. <p> There is thus a clear need for global scheduling of communication. In this paper we present a novel compiler algorithm that includes and extends all the optimizations mentioned above. Our algorithm derives from static single assignment analysis, array dependence analysis, and the recently introduced data availability analysis <ref> [14] </ref>, which is extended to detect compatibility of communication patterns in addition to redundancy. We differ significantly from existing research in that the position of communication code for each remote access is not decided independent of other remote accesses; instead, the positions are decided in an interdependent and global manner. <p> 6= S Place each group of combined entries at the latest position common to the candidate placements of the entries it contains, including entries disabled during redundancy elimination. placement. (f) Pseudocode for global redundancy elimination. (g) Simple greedy heuristic to choose a final position from the set of candidates. defs <ref> [14] </ref>, would lead to Earliest 0 (a 1 ) = Earliest 0 (a 2 ) = f4; 6g. In both cases, a 2 subsumes a 1 . We prove Claim 4.1 using the following three lemmas. We defer their proofs to the appendix. Lemma 4.2 d 1 dominates u. <p> This test is based on the Available Section Descriptor (ASD) representation of communication <ref> [14] </ref>. Briefly, an ASD consists of a pair hD; M i, where D represents the data (scalar variable or an array section) being communicated, and M is a mapping function that maps data to the processors which receive that data. <p> Thus, by choosing a later (than the earliest possible) placement for b 1 , we are able to eliminate that communication completely. In contrast, the solution proposed in <ref> [14] </ref> would move each communication to the earliest point, and reduce the communication for b 2 to ASD (b 2 )ASD (b 1 ), while the communication for b 1 would remain unchanged. <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [14] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [14]. 4.8 Code generation As shown in Figure 6, the step after communication analysis and optimization is to insert communication code in the form <p> The check for M 1 M 2 is done in the virtual processor space of template positions, as described in <ref> [14] </ref>. However, we have incorporated extensions to check for equality of mappings in physical processor space for nearest-neighbor communication and for mappings to a constant processor position [14]. 4.8 Code generation As shown in Figure 6, the step after communication analysis and optimization is to insert communication code in the form of subroutine calls to the pHPF runtime library routines, which in turn invoke MPL/MPI.
Reference: [15] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156). <p> This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [26, 22, 13]. Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization <ref> [15, 30] </ref>, using collective communication [11, 20], message coalescing [15], and exploiting pipelined communication [15, 12], all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication. <p> Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization [15, 30], using collective communication [11, 20], message coalescing <ref> [15] </ref>, and exploiting pipelined communication [15, 12], all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication. <p> Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization [15, 30], using collective communication [11, 20], message coalescing [15], and exploiting pipelined communication <ref> [15, 12] </ref>, all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [30, 15, 12] </ref>. Given a use u, let d range over the reaching regular defs of u. Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u).
Reference: [16] <author> K. Keeton, T. Anderson, and D. Patterson. </author> <title> LogP quantified: The case for low-overhead local area networks. </title> <booktitle> In Proc. Hot Interconnects III: A Symposium on High Performance Interconnects, </booktitle> <address> Stanford, CA, </address> <month> Aug. </month> <year> 1995. </year>
Reference-contexts: We pick two platforms: the IBM SP2 with a custom network, and a network of Sparc workstations (NOW) connected by a commodity network (Myrinet). IBM's message passing library MPL and MPICH 2 are used for communication. Details of the networks can be found in <ref> [25, 24, 16] </ref>. We want to measure how large messages are rewarded by the network, while estimating the local buffer-copy cost to collect small messages. Figure 5 shows the profiling code and results. The top curve shows the bandwidth of local bcopy as a function of buffer size.
Reference: [17] <author> K. Kennedy and N. Nedeljkovic. </author> <title> Combining dependence and data-flow analyses to optimize communication. </title> <booktitle> In International Parallel Processing Symposium. IEEE, </booktitle> <year> 1995. </year>
Reference-contexts: Naturally, the next step was the use of dataflow analysis, e.g.. using precise array dataflow analysis to detect redundant communication within a loop nest [3], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [10, 14, 17, 18] </ref> and over entire arrays for irregular computations [27, 1]. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity.
Reference: [18] <author> K. Kennedy and A. Sethi. </author> <title> A constraint-based communication placement framework. </title> <type> Technical Report CRPC-TR95515-S, CRPC, </type> <institution> Rice University, </institution> <year> 1995. </year>
Reference-contexts: Naturally, the next step was the use of dataflow analysis, e.g.. using precise array dataflow analysis to detect redundant communication within a loop nest [3], and those using global dataflow analysis for redundancy elimination across loop nests as well. These include dataflow analysis over array sections for regular computations <ref> [10, 14, 17, 18] </ref> and over entire arrays for irregular computations [27, 1]. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity. <p> Recently, it has been pointed out that communication that is scheduled too eagerly can lead to problems like contention (which reduce effective network bandwidth) and excessive buffer requirement (which upsets the computation's cache and thereby degrades performance) <ref> [18, 21] </ref>. This is similar to the issue of register pressure [19].
Reference: [19] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Recently, it has been pointed out that communication that is scheduled too eagerly can lead to problems like contention (which reduce effective network bandwidth) and excessive buffer requirement (which upsets the computation's cache and thereby degrades performance) [18, 21]. This is similar to the issue of register pressure <ref> [19] </ref>. However, a more striking fact we point out is that earliest placement can also lead to valuable opportunities being missed for reducing the number of messages or eliminating partial redundancy, making it a sub-optimal strategy even in the absence of resource constraints.
Reference: [20] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization [15, 30], using collective communication <ref> [11, 20] </ref>, message coalescing [15], and exploiting pipelined communication [15, 12], all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication.
Reference: [21] <author> T. Mowry, M. Lam, and A. Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 62-73. </pages> <booktitle> ACM SIGPLAN, </booktitle> <year> 1992. </year>
Reference-contexts: Recently, it has been pointed out that communication that is scheduled too eagerly can lead to problems like contention (which reduce effective network bandwidth) and excessive buffer requirement (which upsets the computation's cache and thereby degrades performance) <ref> [18, 21] </ref>. This is similar to the issue of register pressure [19].
Reference: [22] <author> M. O'Boyle and F. Bodin. </author> <title> Compiler reduction of synchronization in shared virtual memory systems. </title> <booktitle> In Proc. 9th ACM International Conference on Supercomputing, </booktitle> <address> Barcelona, Spain, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [26, 22, 13] </ref>. Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations.
Reference: [23] <author> V. Sarkar. </author> <title> The PTRAN parallel programming system. </title> <booktitle> Parallel Functional Programming Languages and Compilers, </booktitle> <pages> pages 309-391, </pages> <year> 1991. </year>
Reference-contexts: code in Figure 4 as a running example to illustrate the operation of the steps of the algorithm. 4.1 Representation and notation We represent the program using the augmented control flow graph (CFG), which makes loop (interval) structure more explicit than the standard CFG by placing preheader and postexit nodes <ref> [2, 23] </ref>. These extra nodes also provide convenient locations for summarizing dataflow information for the loop. The CFG is a directed graph where each node is a basic block, a sequence of statements without jumps.
Reference: [24] <author> M. Snir et al. </author> <title> The communication software and parallel environment of the IBM SP2. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 205-221, </pages> <year> 1995. </year>
Reference-contexts: As a result, communication startup overheads tend to be astronomical on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [25, 24] </ref>. Thus compilers must reduce the number as well as the volume of messages. This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [26, 22, 13]. <p> We pick two platforms: the IBM SP2 with a custom network, and a network of Sparc workstations (NOW) connected by a commodity network (Myrinet). IBM's message passing library MPL and MPICH 2 are used for communication. Details of the networks can be found in <ref> [25, 24, 16] </ref>. We want to measure how large messages are rewarded by the network, while estimating the local buffer-copy cost to collect small messages. Figure 5 shows the profiling code and results. The top curve shows the bandwidth of local bcopy as a function of buffer size. <p> It also depends on the coprocessor and network software. E.g., the implementors of MPL minimize coprocessor assistance because it is much slower than the CPU, and the channel between the CPU and the coprocessor is slow <ref> [24] </ref>. 4 Compiler algorithms In this section we describe the algorithm for placing communication code. This analysis is done after the compiler has performed transformations like loop distribution and loop interchange to increase opportunities for moving communication outside loops [12]. 1.
Reference: [25] <author> C. Stunkel et al. </author> <title> The SP2 high performance switch. </title> <journal> IBM Systems Journal, </journal> <volume> 34(2) </volume> <pages> 185-204, </pages> <year> 1995. </year>
Reference-contexts: As a result, communication startup overheads tend to be astronomical on most distributed memory machines, although reasonable bandwidth can be supported for sufficiently large messages <ref> [25, 24] </ref>. Thus compilers must reduce the number as well as the volume of messages. This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [26, 22, 13]. <p> We pick two platforms: the IBM SP2 with a custom network, and a network of Sparc workstations (NOW) connected by a commodity network (Myrinet). IBM's message passing library MPL and MPICH 2 are used for communication. Details of the networks can be found in <ref> [25, 24, 16] </ref>. We want to measure how large messages are rewarded by the network, while estimating the local buffer-copy cost to collect small messages. Figure 5 shows the profiling code and results. The top curve shows the bandwidth of local bcopy as a function of buffer size.
Reference: [26] <author> C.-W. Tseng. </author> <title> Compiler optimizations for eliminating barrier synchronization. </title> <booktitle> In Principles and Practice of Parallel Programming (PPoPP), </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Thus compilers must reduce the number as well as the volume of messages. This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events <ref> [26, 22, 13] </ref>. Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations.
Reference: [27] <author> R. v Hanxleden and K. Kennedy. </author> <title> Given-Take|a balanced code placement framework. </title> <booktitle> In Programming Language Design and Implementation (PLDI), </booktitle> <address> Orlando, FL, </address> <month> June </month> <year> 1994. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: These include dataflow analysis over array sections for regular computations [10, 14, 17, 18] and over entire arrays for irregular computations <ref> [27, 1] </ref>. Typically, the technique is to move communication to the earliest possible point dictated by data dependency and control flow. Superficially, this appears to give the additional benefit of maximum overlap between CPU and network activity. <p> While the injection bandwidth is much lower than bcopy, it is larger than receive bandwidth for certain message sizes. Our algorithm permits additional techniques like Give-n-Take to be used to overlap this latency with code at the sender <ref> [27] </ref>. We did not implement this because the potential gain was not clear in our architectures and bulksynchronous SPMD model. It also depends on the coprocessor and network software.
Reference: [28] <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: In fact, the current IBM HPF scalarizer [12] will translate the F90-style source to the scalarized form in the second column. If loop fusion can be performed before this analysis, as in this case, the problem can be avoided. But this is not always possible <ref> [28, x 9.2] </ref>. Thus, limited communication analysis at a single loop-nest level or a rigid placement policy may not work well. <p> Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). Given d and u, we can compute all possible direction vectors (each is a CNL (d; u)-dimensional vector) <ref> [28] </ref>. These vectors are used in IsArrayDep in Figure 8 (d). Let DepLevel (d; u) = max ` fIsArrayDep (d; u; `)g. Because of the dependency at level DepLevel (d; u), communication for u cannot be moved outside loop level DepLevel (d; u).
Reference: [29] <author> M. Wolfe and U. Banerjee. </author> <title> Data dependence and its application to parallel processing. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(2) </volume> <pages> 137-178, </pages> <month> Apr. </month> <year> 1987. </year>
Reference-contexts: This is typically done using a backward and forward dataflow approach with array section descriptors or bitvectors. We find it more efficient to exploit the SSA defuse information already computed in an earlier phase [8, 6], refined by array dependence-testing <ref> [29] </ref>. 2. For each non-local reference, identify a set of candidate positions, any one of which can be potentially chosen as the final point to emit a call to a message-passing runtime routine (x4.4). 3. Perform the "array-section" analog of common subex-pression elimination: detect and eliminate subsumed communication (x4.6). 4.
Reference: [30] <author> H. Zima, H. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semiautomatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year>
Reference-contexts: The compilers for these languages are responsible for partitioning the computation, and generating the communication necessary to fetch values of nonlocal data referenced by a processor <ref> [15, 30, 4, 3, 5, 12] </ref>. Accessing remote data is usually orders of magnitude slower than accessing local data. This gap is growing fl Computer Science Division, U. C. Berkeley, CA 94720. Partly supported by ARPA/DOD (DABT63-92-C-0026), DOE (DE-FG03-94ER25206), and NSF (CCR-9210260, CDA-8722788 and CDA-9401156). <p> This can improve performance on shared memory machines as well, because fewer messages translate into fewer synchronization events [26, 22, 13]. Consequently, communication optimization has been extensively researched, from local single loop-nest to global and even interprocedural optimizations. The earliest and most commonly used optimizations include message vec-torization <ref> [15, 30] </ref>, using collective communication [11, 20], message coalescing [15], and exploiting pipelined communication [15, 12], all within the scope of a single loop nest. Local analysis of array accesses based on dependence testing alone often retains redundant communication. <p> This follows from standard communication analysis in which communication is placed just before the outermost loop in which there is no true dependence on u, and is placed just before the statement containing u if no such loop exists <ref> [30, 15, 12] </ref>. Given a use u, let d range over the reaching regular defs of u. Consider some d. Observe that it is never necessary to place communication for u deeper than at CNL (d; u). <p> The runtime library provides a high-level interface through which the compiler specifies the data being communicated in the form of array sections, and the runtime system takes care of packing and unpacking of data. For NNC, data is received in overlap regions <ref> [30] </ref> surrounding the local portion of the arrays. For other kinds of communication involving arrays, data is received into a buffer that is allocated dynamically, and the array reference that led to this communication is replaced by a reference to the buffer.
References-found: 30

