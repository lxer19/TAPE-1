URL: http://www.cs.indiana.edu/pub/dswise/GraphDoc3.ps.Z
Refering-URL: http://www.cs.indiana.edu/pub/dswise/
Root-URL: http://www.cs.indiana.edu
Email: kasiwagi%crl.hitachi.junet@uunet.uu.net  dswise@iuvax.cs.indiana.edu  
Title: Graph Algorithms in a Lazy Functional Programming Language  
Author: Yugo Kashiwagi* David S. Wisey 
Keyword: CR categories and Subject Descriptors: D.1.1 [Applicative (Functional) Programming Techniques]; G.2.2 [Graph Theory]: Graph Algorithms; E.1 [Data Structures]: Lists; C.1.2 [Multiple Data Stream Architectures (Multiprocessors)]: Parallel processors. General Term: Algorithms. Additional Key Words and Phrases: Haskell, lazy evaluation, fixed point.  
Address: 5-20-1 Josuihoncho Kodaira, Tokyo 187, Japan  101 Lindley Hall, Bloomington, IN 47405-4101  
Affiliation: Semiconductor Design and Development Center, Hitachi, Ltd.  Computer Science Department, Indiana University  
Abstract: Solutions to graph problems can be formulated as the fixed point of a set of recursive equations. Traditional algorithms solve these problems by using pointers to build a graph and by iterating side effects to arrive at the fixed point, but this strategy causes serious problems of synchronization under a parallel implementation. In denying side effects, functional programming avoids them, but it also precludes known algorithms that are uniprocessor-optimal. Functional programming provides another, translation scheme that computes the fixed point without relying on the operational concept of a "store". In this approach, laziness plays an essential role to build a cyclic data structure, a graph, and to implement iteration as streams. The resulting algorithm is not optimal on uniprocessors but, avoiding side effects, the algorithm suggests a promising, more general approach to multiprocessor solutions. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. V. Aho, J. E. Hopcroft, and J. D. </author> <title> Ullman [1983]. Data Structures and Algorithms, </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: Of course the graphs are not, in general, thus restricted, but the terminology is apt because the algorithms locally treat nodes as if they were in a tree|up to the penultimate step where the fixed point is discovered. Graph algorithms <ref> [1] </ref> are often defined by a set of recursive equations. Dataflow equations in conventional compiler construction [2] are typical examples.
Reference: 2. <author> A. V. Aho, R. Sethi and J. D. </author> <title> Ullman [1986], </title> <booktitle> Compilers Principles, Techniques, and Tools, </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, Massachusetts. </address>
Reference-contexts: Graph algorithms [1] are often defined by a set of recursive equations. Dataflow equations in conventional compiler construction <ref> [2] </ref> are typical examples.
Reference: 3. <author> D. P. Friedman and D. S. </author> <title> Wise [1976]. Cons should not evaluate its arguments, </title> <editor> in S. Michaelson and R. Milner (Eds), </editor> <booktitle> Automata, Languages and Programming, </booktitle> <publisher> Edinburgh University Press, Edinburgh, </publisher> <pages> pp. 257-284. </pages>
Reference-contexts: A procedural language would implement the graph structure with pointers, and chains of approximations with side effects, which are incompatible with lazy evaluation <ref> [3] </ref>, [4]. However, parallel implementation [5] forces an absence of side effects in order to avoid conflicts in data access. It is possible, of course, to simulate side effects within lazy functional languages.
Reference: 4. <author> P. Henderson and J. Morris, Jr. </author> <year> [1976]. </year> <title> A lazy evaluator, </title> <booktitle> Proc. 3rd ACM Symp. on Principles of programming Languages, </booktitle> <pages> pp. 95-103. </pages>
Reference-contexts: A procedural language would implement the graph structure with pointers, and chains of approximations with side effects, which are incompatible with lazy evaluation [3], <ref> [4] </ref>. However, parallel implementation [5] forces an absence of side effects in order to avoid conflicts in data access. It is possible, of course, to simulate side effects within lazy functional languages.
Reference: 5. <author> D. B. </author> <month> Skillcorn </month> <year> [1990]. </year> <note> Architecture-Independent Parallel Computation, Computer 23, 12, IEEE. </note>
Reference-contexts: A procedural language would implement the graph structure with pointers, and chains of approximations with side effects, which are incompatible with lazy evaluation [3], [4]. However, parallel implementation <ref> [5] </ref> forces an absence of side effects in order to avoid conflicts in data access. It is possible, of course, to simulate side effects within lazy functional languages. A graph, viewed as the state of "store", can be passed as an extra argument to and returned from each updating function.
Reference: 6. <author> P. J. </author> <title> Landin [1965]. A correspondence between ALGOL60 and Church's lambda notation, </title> <journal> Comm. ACM. </journal> <volume> 8,2. </volume> <month> Aug. </month> <pages> 89-101. </pages>
Reference-contexts: Instead of simulating pointers and side effects, the desired solution should use only mechanisms natural to lazy functional languages. For instance, lazy binding can represent the cyclic data structure of a graph. Chains of approximation can be conveniently represented as streams <ref> [6] </ref>, the direct analog of Lucid's histories [9]. The resulting program turned out to be a direct translation of the defining equations, not easily available from non-lazy languages.
Reference: 7. <editor> P. Hudak and P. Wadler (Eds.) </editor> <year> [1990]. </year> <title> Report on the Programming Language Haskell, </title> <note> Version 1.0. </note>
Reference-contexts: The remainder of this paper is organized as follows. Section 1 presents a generic graph package for a modest selection of three typical graph algorithms and its applications. The programs are presented in the lazy functional language Haskell <ref> [7] </ref>. Section 2 discusses storage management problems, where the granularity of data structures plays an essential role. Section 3 presents a method to share graphs among different algorithms. Section 4 discusses efficiency and opportunities for further optimization. 1.
Reference: 8. <author> P. </author> <title> Hudak Para-Functional Programming [1991]. </title> <editor> in B. Szymanski (Eds.) </editor> <booktitle> Parallel Functional Programming Languages and Environments, </booktitle> <publisher> Addison Wesley. </publisher>
Reference-contexts: Section 3 presents a method to share graphs among different algorithms. Section 4 discusses efficiency and opportunities for further optimization. 1. Implementation of a graph package and its applications It is not the purpose of this paper to introduce Haskell since a decent introduction is available elsewhere <ref> [8] </ref>. However, an insightful reader might barge ahead without it because much of the surface syntax is readable. Haskell is a strongly typed, modular, lazy, functional programming language. All code is declarative. The module Graph is the implementation of a graph package as a Haskell module.
Reference: 9. <author> W. W. Wadge, E. A. </author> <title> Ashcroft [1985]. Lucid, the Dataflow Programming Language, </title> <publisher> Academic Press. </publisher>
Reference-contexts: Instead of simulating pointers and side effects, the desired solution should use only mechanisms natural to lazy functional languages. For instance, lazy binding can represent the cyclic data structure of a graph. Chains of approximation can be conveniently represented as streams [6], the direct analog of Lucid's histories <ref> [9] </ref>. The resulting program turned out to be a direct translation of the defining equations, not easily available from non-lazy languages. Although the resulting algorithm is uniprocessor sub-optimal, it successfully eliminates expensive global update of the graph, giving a possibility of further optimization, especially on parallel processors. <p> It is often a subrange of integers elsewhere, but it need not be here. The Chain type (c) is the type of elements of the chain attached to each node. Its counterpart in a procedural implementation is the history of updated data attached to each node <ref> [9] </ref>. The Result type (r) is the type of the final result of the algorithm for each node. 1.1.2 Data structures The input is a list of edges.
Reference: 10. <author> R. E. </author> <title> Tarjan [1983]. Data Structures and Network Algorithms, </title> <publisher> SIAM, </publisher> <address> Philadelphia. </address>
Reference-contexts: In the algorithm, the size of the entire graph is used as a bound on the fixed point. This bounded search is not an efficient algorithm unless there is an "efficient" estimate on the span of the graph. Tarjan <ref> [10] </ref> gives an almost linear uniprocessor algorithm for weak component using side effects. However good it is on a uniprocessor, Tarjan's algorithm is ill-suited to asynchronous multiprocessing just as ours is suboptimal on a uniprocessor.
Reference: 11. <author> S. D. Johnson. </author> <year> [1984]. </year> <title> Synthesis of Digital Designs from Recursion Equations, </title> <publisher> The MIT Press. </publisher>
Reference-contexts: To achieve such an optimal algorithm within the framework of parallelism or lazy functional programming is still an open problem. 2. Data structure vs binding The code presented in this paper uses data recursion <ref> [11] </ref>. Data recursion can be implemented by bindings, as in the program presented here, or by data structures. A data-structure solution would build the links of the graph explicitly as links within the data structure. A binding solution builds the links of the graph implicitly as local bindings.
Reference: 12. <author> T. </author> <title> Johnsson [1985]. Lambda lifting: transforming programs to recursive equations, </title> <editor> in Jouannaud (Eds), </editor> <booktitle> Conference on Functional Programming Languages and Computer Architecture, Nancy, </booktitle> <publisher> LNCS 201. Springer Verlag. </publisher>
Reference-contexts: It is true that a large data structure is still bound in an outer environment to be passed to local functions. However, such a binding can be released at earlier stage by a program transformation. This technique applied to the environment structure has been called lambda lifting <ref> [12] </ref>; we generalize it to apply to recursively specified data types, like streams.
Reference: 13. <author> J. K. </author> <title> Stoy [1979]. Denotational Semantics. The Scott-Stracey Approach to Programming Language Theory, </title> <publisher> The MIT Press. </publisher>
Reference-contexts: Sharing graphs In traditional graph algorithms, the data structure of a graph is shared by several algorithms. This section shows how to implement this sharing in functional languages. The technique can be considered as a kind of "distributive law." The function graphAlgorithm, curried <ref> [13] </ref> to its first argument, can be applied to several graph algorithms. In Haskell code, the expression (graphAlgorithm edges) serves this purpose. This value can be shared by several graph algorithms, each of which supplies its own initialization function, step function and result function.
Reference: 14. <author> P. R. </author> <title> Halmos [1960]. Naive Set Theory, </title> <publisher> Van Nostrand, Princeton. </publisher>
Reference-contexts: Product still necessary to share the links of the graph as well as skeleton of the graph among algorithms applied to the same graph. This leads us to a notion of "Cartesian" product of algorithms. We can apply a set-theoretic "product of functions" <ref> [14] </ref> to algorithms, viewed as functions from inputs to outputs. It is illustrated in the module Product.
References-found: 14

