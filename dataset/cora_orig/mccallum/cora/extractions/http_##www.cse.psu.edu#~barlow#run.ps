URL: http://www.cse.psu.edu/~barlow/run.ps
Refering-URL: http://www.cse.psu.edu/~barlow/papers.html
Root-URL: http://www.cse.psu.edu
Title: Chapter 1 Preliminaries 1.1. Introduction  
Keyword: 1.2. Notation for Vectors, Matrices, and Algorithms  
Note: B B 1 C A fi B B fi 1 C A  
Abstract: We give some necessary background for the study of least squares. The first section gives the notational conventions to be used in the book. In the second section, we give some elementary results and definitions from linear algebra and probability. The final section is on matrix and vector norms, and stresses the importance of the Euclidean norm. No proofs are supplied for any of the results given in this chapter. For such proofs, see some of the references given in x1.5. Vectors and scalars are denoted by lower case Roman or Greek letters. Scalars are written in ordinary type, whereas vectors are written in boldface type. For instance, we write a real n-vector y componentwise in a column as where y 1 ; y 2 ; : : : ; y n are real scalars. We can also have a p-vector fi given by Usually instead of saying "y is an n-vector", we will say "y 2 &lt; n " where &lt; n is the vector space of n-tuples of real numbers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Albert. </author> <title> Regression and the Moore-Penrose pseudoinverse. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: These definitions correspond to those from Rao [44, 1973]. Let PrfAg denote the probability that the event A occurs. A real valued random variable x is a variable associated with a distribution function F () : &lt; ! <ref> [0; 1] </ref> such that F (a) = Prfx &lt; ag for all a 2 &lt;. <p> The distribution is referred to as N (; oe 2 ). 1.3. ELEMENTARY DEFINITIONS AND RESULTS 9 Likewise, we can define a random vector x 2 &lt; n as a vector associated with a distribution function F : &lt; n ! <ref> [0; 1] </ref> such that F (a) = Prfx &lt; ag: Again, we are concerned only with distributions that are continuous and dif ferentiable. <p> A great deal has been written about pseudoinverses. An annotated bibliography is given by 74 CHAPTER 3. BASICS OF LINEAR LEAST SQUARES Nashed and Rall [42, 1976]. Some texts on the role of pseudoinverses in linear models include Rao and Mitra [45, 1971] and Albert <ref> [1, 1972] </ref>. There are also a number of books from the linear algebra point of view. A good one is by Campbell and Meyer [7, 1979].
Reference: [2] <author> E. Anderson, Z. Bai, C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, A. Green-baum, S. Hammarling, A. McKenney, S. Ostrouchov, and D. Sorensen. </author> <title> LA-PACK User's Guide: Second Edition. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1995. </year>
Reference-contexts: Software is available that automatically computes characteristics of a floating point system. MACHAR by Cody [11, 1988] returns the parameters fi, t, e min , e max , " m , !, parameters indicating the type of rounding, and a number of other useful values. The LAPACK routine xLAMCH <ref> [2, pp:66-67, 1995] </ref>, returns just " m , ! and . Kahan's PARANOIA not only returns machine parameters, but tries to determine how well the arithmetic has been implemented. PARANOIA is available from netlib and is self-documenting. <p> These parameters can computed for any floating point system using Cody's [11, 1988] routine MACHAR or using the xLAMCH routine from LAPACK <ref> [2, pp:66-67, 1995] </ref> to compute , !, and " m , taking logarithms, and carefully using integer truncation. If we are working on a computer that implements IEEE binary floating point standard, the paramters e min , e max , and t will be known to us. <p> The paper [6, 1978] did not entirely resolve the issue for two subtle reasons. The first reason is that kxk 1 might be in range when kxk 2 is not. That becomes more likely when n is large as can be seen by examining the inequality (??). The LAPACK <ref> [2, p:137] </ref> routine SLASSQ actually returns two numbers, kxk 1 and kxk 2 =kxk 1 .
Reference: [3] <author> J.L. Barlow. </author> <title> Probabilistic Error Analysis of Floating Point and CRD Arith-metics. </title> <type> PhD thesis, </type> <institution> Northwestern University, </institution> <address> Evanston, IL, </address> <year> 1981. </year>
Reference-contexts: The biggest problems have been subtle differences in rounding from the idealized round in the last section and the handling of numbers at the extreme of the number range. In the early 1970's, a hodge-podge of rounding rules were implemented (see Sterbenz [46, 1974], Cody [13, 1973],and Barlow <ref> [3, 1981] </ref>). A user often had to know a great deal about the arithmetic in a computer to produce reliable numerical software. Moreover, drastic changes might be necessary for a robust program on one computer to become a robust program on another.
Reference: [4] <author> J.L. Barlow. </author> <title> Error analysis of a pairwise summation algorithm to compute the sample variance. </title> <journal> Numerische Mathematik, </journal> <volume> 58 </volume> <pages> 583-590, </pages> <year> 1991. </year>
Reference-contexts: Chan and Lewis [9, 1979] derived a condition number with respect to a data set fx i g n i=1 for the computation of s 2 . We use a slightly less restrictive version of condition number given by Barlow <ref> [4, 1991] </ref>. Suppose that we introduce a relative perturbation of size . <p> This procedure is also appropriate for computation on distributed computers <ref> [4, 1991] </ref> ,[8, 1983]. Barlow [4, 1991] proved that, in floating arithmetic, such a procedure would compute a sample variance ~s such that j~s sj " m g (n) s (x) + O (" 2 where g (n) = O ([log n] 2 ). <p> This procedure is also appropriate for computation on distributed computers <ref> [4, 1991] </ref> ,[8, 1983]. Barlow [4, 1991] proved that, in floating arithmetic, such a procedure would compute a sample variance ~s such that j~s sj " m g (n) s (x) + O (" 2 where g (n) = O ([log n] 2 ).
Reference: [5] <author> E. </author> <month> Beltrami. </month> <institution> Sulle Funzioni Bilineari. Goirnale di Matematiche ud uso Degli Studenti Delle Universita, </institution> <note> 11 98-106, 1873. (in Italian). </note>
Reference-contexts: These complete orthogonal decompositions are used to state the solution of the linear least squares problem and to prove the Gauss-Markov theorem. 3.2. Complete Orthogonal Decompositions 3.2.1. The Singular Value Decomposition The singular value decomposition (SVD) was developed independently by Bel-trami <ref> [5, 1873] </ref> and Jordan [34, 1874], and then rediscovered by Sylvester [51, 1889]. Its importance in the modern methods for solving least squares problems was established Golub and Kahan [24, 1965]. A survey paper by Stewart [48, 1993] gives a nice discussion of the history of the SVD.
Reference: [6] <author> J.L. </author> <title> Blue. A portable FORTRAN program to find the Euclidean norm of a vector. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 4 </volume> <pages> 15-23, </pages> <year> 1978. </year>
Reference-contexts: If n is very large, the algorithm (2.3.81)-(2.3.82) could very slow. It requires two memory accesses for every component of x, one the for the computation (2.3.81) and one for the computation (2.3.82). As was pointed in x2.2, memory accesses take significantly more time than floating point operations. Blue <ref> [6, 1978] </ref> proposed an algorithm for computing kxk 2 that is as resistant to overflow as (2.3.81)-(2.3.82) but requires only one pass through the data. It separates the computation of kxk 2 into three sums over the large, medium, and small portions of the vector. <p> An algorithm based upon this function was used for the BLAS 1 routine xNRM2 to compute the two-norm [39, 1979]. The paper <ref> [6, 1978] </ref> did not entirely resolve the issue for two subtle reasons. The first reason is that kxk 1 might be in range when kxk 2 is not. That becomes more likely when n is large as can be seen by examining the inequality (??).
Reference: [7] <author> S.L. Campbell and Jr. </author> <title> C.D. Meyer. Generalized Inverses of Linear Transformations. </title> <publisher> Pitman, </publisher> <address> London, 1979. </address> <publisher> Reprint by Dover, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Some texts on the role of pseudoinverses in linear models include Rao and Mitra [45, 1971] and Albert [1, 1972]. There are also a number of books from the linear algebra point of view. A good one is by Campbell and Meyer <ref> [7, 1979] </ref>.
Reference: [8] <author> T.F. Chan, G.H. Golub, and R.J. LeVeque. </author> <title> Algorithms for computing the sample variance: analysis and recommendations. </title> <journal> Am. Stat., </journal> <volume> 7 </volume> <pages> 242-247, </pages> <year> 1983. </year>
Reference-contexts: One computes the mean by standard or pairwise summation and then computes the sum t 1 = (x 1 x) 2 (2.3.44) s 2 = n 1 Chan, Golub, and LeVeque <ref> [8, 1983] </ref> call this the standard two-pass algorithm. That is because it requires passing through the data twice: once to compute x and again to compute s 2 . Clearly, pairwise summation could also be used to compute t n . <p> Thus, for this problem, a bound of the form (2.3.54) is sufficient to show that an algorithm is backward stable. Chan, Golub, and LeVeque <ref> [8, 1983] </ref> showed that the two-pass algorithm does better than (2.3.54). <p> It is given by m 1 = x 1 ; t 1 = m 2 m k+1 = m k + x k+1 ; t k+1 = t k + x 2 x = n s 2 = n 1 n x 2 : (2.3.66) Again following <ref> [8, 1983] </ref>, we call this the standard one-pass algorithm. It can be computed using just one pass through the data. Thus it would be faster 2.3. SIMPLE COMPUTATIONS ARE NOT ALWAYS SO SIMPLE 43 on large samples that cannot be placed in fast memory. <p> To avoid this problem, several authors, Youngs and Cramer [54, 1971], West [53, 1979],and E.H. Hansen [26, 1975] derived stable algorithms for computing the sample variance using only one pass through the data. In <ref> [8, 1983] </ref>, it was pointed out that all of these algorithms were essentially equivalent. <p> In many applications it is necessary to combine samples. The following algorithm was proposed by Chan, Golub, and LeVeque <ref> [8, 1983] </ref> to resolve these issues. For 1 i j n, let m ij = k=i and let j X (x k j i Clearly, s 2 = n 1 44 CHAPTER 2.
Reference: [9] <author> T.F. Chan and J.G. Lewis. </author> <title> Computing standard deviations: accuracy. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 22 </volume> <pages> 526-531, </pages> <year> 1979. </year>
Reference-contexts: That is because it requires passing through the data twice: once to compute x and again to compute s 2 . Clearly, pairwise summation could also be used to compute t n . Chan and Lewis <ref> [9, 1979] </ref> derived a condition number with respect to a data set fx i g n i=1 for the computation of s 2 . We use a slightly less restrictive version of condition number given by Barlow [4, 1991]. Suppose that we introduce a relative perturbation of size . <p> We now give the version due to West [53, 1979]. m 1 = x 1 ; t 0 = 0; (2.3.68) t k+1 = t k + k x = n 1 t n : (2.3.71) Chan and Lewis <ref> [9, 1979] </ref> showed that West's algorithm satisfied a bound of the form (2.3.54) where g (n) = O (n): (2.3.72) This algorithm satisfies all of our needs except two: (1) it is not amenable to pairwise summation; (2) it allows us to add only one new sample at a time.
Reference: [10] <author> K.L. Chung. </author> <title> A course in probability theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1974. </year>
Reference-contexts: The first chapter of Stewart's book [47, 1973] gives a summary of the important theorems for applications to least squares. The section on probability, x1.3.2, was deliberately nonrigorous. For a thorough introduction read Chung <ref> [10, 1974] </ref>. Much of the material in x1.3.2 was adapted from Rao [44, 1973]. A thorough treatment of norms is available from a number of books on numerical linear algebra. Householder [30, 1964, Chapter 2] is still an excellent source.
Reference: [11] <author> W.J. Cody. </author> <title> ALGORITHM 665 MACHAR: a subroutine to dynamically determine machine parameters. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 14, </volume> <year> 1988. </year>
Reference-contexts: That is, the machine units " s 1 fi 1t s ; " d 1 fi 1t d should satisfy " d m ) 2 (2.1.9) for some reasonably sized constant c. Software is available that automatically computes characteristics of a floating point system. MACHAR by Cody <ref> [11, 1988] </ref> returns the parameters fi, t, e min , e max , " m , !, parameters indicating the type of rounding, and a number of other useful values. The LAPACK routine xLAMCH [2, pp:66-67, 1995], returns just " m , ! and . <p> MATRIX COMPUTATIONAL CONCERNS Suppose that we are given the floating point system defined by (2.1.1)- (2.1.2), with the parameters e min and e max as the lower and upper limits of the exponent range, and t the precision. These parameters can computed for any floating point system using Cody's <ref> [11, 1988] </ref> routine MACHAR or using the xLAMCH routine from LAPACK [2, pp:66-67, 1995] to compute , !, and " m , taking logarithms, and carefully using integer truncation.
Reference: [12] <author> W.J. Cody. </author> <title> Floating point standards theory and practice. In R.E. </title> <editor> Moore, editor, </editor> <booktitle> Reliability in Computing: The Role of Interval Methods in Scientific Computing, </booktitle> <pages> pages 99-107, </pages> <address> Boston, 1988. </address> <publisher> Academic Press. </publisher>
Reference-contexts: The parameters are subject to the following constraints: * The base fi must be either 2 or 10. As Cody <ref> [12, 1988] </ref> states, the committee "could find no reason for allowing other radices and found several reasons for not allowing them." In particular, hexadecimal (base 16) arithmetic is notorious in the scientific community for its "wobbling precision" (see Cody [13, 1973]). * The value (e max e min )=(t 1) must
Reference: [13] <author> W.J. Cody, Jr. </author> <title> Static and dynamic numerical characteristics of floating point arithmetic. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-22:598-601, </volume> <year> 1973. </year>
Reference-contexts: The biggest problems have been subtle differences in rounding from the idealized round in the last section and the handling of numbers at the extreme of the number range. In the early 1970's, a hodge-podge of rounding rules were implemented (see Sterbenz [46, 1974], Cody <ref> [13, 1973] </ref>,and Barlow [3, 1981]). A user often had to know a great deal about the arithmetic in a computer to produce reliable numerical software. Moreover, drastic changes might be necessary for a robust program on one computer to become a robust program on another. <p> As Cody [12, 1988] states, the committee "could find no reason for allowing other radices and found several reasons for not allowing them." In particular, hexadecimal (base 16) arithmetic is notorious in the scientific community for its "wobbling precision" (see Cody <ref> [13, 1973] </ref>). * The value (e max e min )=(t 1) must exceed 5 and it is recommended to exceed n 10.
Reference: [14] <author> T.J. Dekker. </author> <title> A floating-point technique for extending the available precision. </title> <journal> Numerische Mathematik, </journal> <volume> 18 </volume> <pages> 224-242, </pages> <year> 1971. </year>
Reference-contexts: the computer arithmetic uses guard digits, which are extra digits used to determine a correct round. (Again, CRAY machines do not.) A set of routines for similating extended precision arithmetic operations in double precision (or double precision operations in single precision) for base fi = 2 is given by Dekker <ref> [14, 1971] </ref>. They are short and easy to program, but require the value of t which can be recovered from MACHAR, xLAMCH, or PARANOIA. 2.1.2. <p> Note that (2.3.89) is satisfied, so, by our definition, we have stable algorithm, but n is so large that the statement (2.3.89) is meaningless. Several possible fixes have been proposed. The simplest is to use Dekker's <ref> [14, 1971] </ref> procedures for simulating a doubled precision adds and multiplies. Unfortunately, these procedures require a subroutine call for each add and multiply and that could be unacceptably slow. 2.3. SIMPLE COMPUTATIONS ARE NOT ALWAYS SO SIMPLE 49 Both reasons become important when n is very large.
Reference: [15] <author> J.W. Demmel. </author> <title> Underflow and the reliability of numerical software. </title> <journal> SIAM J. Sci. Stat. Computing, </journal> <volume> 5 </volume> <pages> 887-919, </pages> <year> 1984. </year>
Reference-contexts: On all computer, an overflow will generate an exception or error flag. An underflow is a result whose magnitude is smaller than the smallest nonzero computer number, denoted !. We now give two common philosophies for setting ! as classified by Demmel <ref> [15, 1984] </ref>. One of them is called "store zero" (SZ).
Reference: [16] <author> J.J. Dongarra, J.J. DuCroz, I.S. Duff, and S.J. Hammarling. </author> <title> A set of level 3 basic linear algebra subprograms. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 16 </volume> <pages> 1-17, </pages> <year> 1990. </year> <note> 75 76 BIBLIOGRAPHY </note>
Reference-contexts: The level 1 BLAS (vector operations only) were introduced by Lawson, Hanson, Kincaid, and Krogh [39, 1979]. Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson [17, 1988], and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff <ref> [16, 1990] </ref>. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips [20, 1992]. Freeman and Phillips also explain the role of the BLAS in parallel and distributed computing. The BLAS undergo updates from time to time. <p> Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson [17, 1988], and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff <ref> [16, 1990] </ref>. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips [20, 1992]. Freeman and Phillips also explain the role of the BLAS in parallel and distributed computing. The BLAS undergo updates from time to time. The BLAS web page http://www.netlib.org/utk/papers/blast-forum.html gives the current state of these updates.
Reference: [17] <author> J.J. Dongarra, J.J. DuCroz, S.J. Hammarling, and R.J. Hansen. </author> <title> An extended set of Fortran basic linear algebra subroutines. </title> <journal> ACM Trans. Math. Software, </journal> <volume> 14 </volume> <pages> 1-17, </pages> <year> 1988. </year>
Reference-contexts: The level terminology came about in the design of the BLAS. The level 1 BLAS (vector operations only) were introduced by Lawson, Hanson, Kincaid, and Krogh [39, 1979]. Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson <ref> [17, 1988] </ref>, and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff [16, 1990]. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips [20, 1992]. <p> Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson <ref> [17, 1988] </ref>, and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff [16, 1990]. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips [20, 1992]. Freeman and Phillips also explain the role of the BLAS in parallel and distributed computing. The BLAS undergo updates from time to time. The BLAS web page http://www.netlib.org/utk/papers/blast-forum.html gives the current state of these updates.
Reference: [18] <editor> D.K. Faddeev, V.N. Kublanovskaya, and V.N. Faddeeva. Sur les Systemes Lineires Algebriques de Matrices Rectangularies et Mal-Conditionnees, </editor> <volume> volume VII, </volume> <pages> pages 161-170. </pages> <institution> Editions Centre Nat. Recherche Sci., Paris, </institution> <year> 1968. </year>
Reference-contexts: Farebrother [19, 1996] gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book [38, 1974]. Faddeev, Kublanovskaya, and Faddeeva <ref> [18, 1968] </ref> and Hanson and Lawson [27, 1969] were the first to introduce non-SVD complete orthogonal decompositions to the solution of least squares problems. The four conditions (3.3.68) were set down by Penrose [43, 1955], but an equivalent version was set down earlier by Moore [41, 1935].
Reference: [19] <author> R.E. Farebrother. </author> <title> Some early statistical contributions to the theory and practice of linear algebra. Linear Algebra and Its Applications, </title> ??:??, <year> 1996. </year>
Reference-contexts: Markov proved a weaker version of the theorem in the early 1900s so his name has also become attached to the Theorem. See the original papers [21, 1809] , [40, 1912] and a recent translation of Gauss's least squares papers by Stewart [22, 1995]. Farebrother <ref> [19, 1996] </ref> gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book [38, 1974].
Reference: [20] <author> T.L. Freeman and C. Phillips. </author> <title> Parallel Numerical Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1992. </year>
Reference-contexts: For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips <ref> [20, 1992] </ref>. Freeman and Phillips also explain the role of the BLAS in parallel and distributed computing. The BLAS undergo updates from time to time. The BLAS web page http://www.netlib.org/utk/papers/blast-forum.html gives the current state of these updates.
Reference: [21] <editor> C.F. Gauss. Theoria Motus Corporum Coelestium in Sectionibus Conicus Solem Ambientium. F. Perthes and I.H. Besser, </editor> <booktitle> Hamburg, 1809. (in Latin). </booktitle>
Reference-contexts: The model (3.3.38)-(3.3.39) is called the Gauss-Markov Linear model (see Gauss <ref> [21, 1809] </ref> (in Latin), [22, 1995] (in English translation), Markov [40, 1912] (in German translation from Russian)) . Solving the problem of estimating the parameters y and oe 2 based upon the responses b is called fitting the model. <p> An unbiased estimate of oe 2 is given by ^oe 2 = m k where k = rank (X). The second important property arises out of the Gauss-Markov theorem <ref> [21, 1809] </ref>,[40, 1912] given next. We prove it in x3.3.4. Theorem 3.11 (Gauss-Markov theorem) A solution ~y LS of the least squares problem (3.3.42) is the best linear unbiased estimate (BLUE) of y. <p> Notes Theorem 3.11, the Gauss-Markov theorem, is really due to Gauss. Markov proved a weaker version of the theorem in the early 1900s so his name has also become attached to the Theorem. See the original papers <ref> [21, 1809] </ref> , [40, 1912] and a recent translation of Gauss's least squares papers by Stewart [22, 1995]. Farebrother [19, 1996] gives a nice discussion of the history of the theorem.
Reference: [22] <author> C.F. </author> <title> Gauss. Theory of the Combination of Observations Least Subject to Errors. Part One, Part Two, Supplement. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1995. </year> <title> Translated from the Latin and German by G.W. </title> <type> Stewart. </type>
Reference-contexts: The model (3.3.38)-(3.3.39) is called the Gauss-Markov Linear model (see Gauss [21, 1809] (in Latin), <ref> [22, 1995] </ref> (in English translation), Markov [40, 1912] (in German translation from Russian)) . Solving the problem of estimating the parameters y and oe 2 based upon the responses b is called fitting the model. <p> Markov proved a weaker version of the theorem in the early 1900s so his name has also become attached to the Theorem. See the original papers [21, 1809] , [40, 1912] and a recent translation of Gauss's least squares papers by Stewart <ref> [22, 1995] </ref>. Farebrother [19, 1996] gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book [38, 1974].
Reference: [23] <author> G.H. Golub. </author> <title> Numerical methods for solving linear least squares problems. </title> <journal> Numerische Mathematik, </journal> <volume> 7 </volume> <pages> 206-216, </pages> <year> 1965. </year>
Reference-contexts: That leads to the statement X T ^r LS = 0: (3.3.81) If we combine (3.3.53) and (3.3.81), we obtain X T 0 ^r LS 0 : (3.3.82) This formulation was first used computationally by Golub <ref> [23, 1965] </ref> and is called the augmented system. It is a common way to to state the the Gauss-Markov linear model [44, p. 298]. Equation (3.3.82) illustrates that both ~y LS 3.3. THE LINEAR LEAST SQUARES PROBLEM 73 and ^r LS are part of the solution to this problem.
Reference: [24] <author> G.H. Golub and W.M. Kahan. </author> <title> Calculating the singular values and pseudoin-verse of a matrix. </title> <journal> SIAM J. Num. Anal. Ser. B, </journal> <volume> 2 </volume> <pages> 205-224, </pages> <year> 1965. </year>
Reference-contexts: Complete Orthogonal Decompositions 3.2.1. The Singular Value Decomposition The singular value decomposition (SVD) was developed independently by Bel-trami [5, 1873] and Jordan [34, 1874], and then rediscovered by Sylvester [51, 1889]. Its importance in the modern methods for solving least squares problems was established Golub and Kahan <ref> [24, 1965] </ref>. A survey paper by Stewart [48, 1993] gives a nice discussion of the history of the SVD. Our development is similar to that of Jordan [34, 1874].
Reference: [25] <author> G.H. Golub and C.F. Van Loan. </author> <title> Matrix Computations, Second Edition. </title> <publisher> The Johns Hopkins Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: This is called a gaxpy operation <ref> [25, Chapter 1] </ref> for "generalized Ax plus y". If y = 0, it is simply a matrix-vector multiplication. <p> We refer to g (x) as the value produced by a computer algorithm to compute g (x). If component of the vector x are out of the computer range, they can be rescaled. Input or representation errors can be resolved by the theory of conditioning <ref> [25, 1989] </ref>, [47, 1973], [49, 1990], the same theory we use for rounding errors. Approximation error is an issue in Chapters ??, ?? , ??, and ??. We discuss two methods for analyzing rounding errors, forward error analysis and backward error analysis. Other formalisms have been proposed, see x??. <p> Connections to the Symmetric Eigenvalue Decomposition One of the most commonly used complete orthogonal decompositions is the eigenvalue decomposition of a symmetric matrix. We also use this decomposition for understanding the SVD. The symmetric eigenvalue decomposition theorem is given next, for a proof, see <ref> [25, Chapter 8] </ref>. Theorem 3.8 (Symmetric Eigenvalue Decomposition) Let X 2 &lt; nfin be symmetric.
Reference: [26] <author> R.J. Hanson. </author> <title> Stably updating mean and standard deviation of data. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 18 </volume> <pages> 57-58, </pages> <year> 1975. </year>
Reference-contexts: To avoid this problem, several authors, Youngs and Cramer [54, 1971], West [53, 1979],and E.H. Hansen <ref> [26, 1975] </ref> derived stable algorithms for computing the sample variance using only one pass through the data. In [8, 1983], it was pointed out that all of these algorithms were essentially equivalent.
Reference: [27] <editor> R.J. Hanson and C.L. Lawson. </editor> <title> Extensions and applications of the Householder algorithm for solving linear least squares problems. </title> <journal> Math. Comp., </journal> <volume> 23 </volume> <pages> 787-812, </pages> <year> 1969. </year>
Reference-contexts: Farebrother [19, 1996] gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book [38, 1974]. Faddeev, Kublanovskaya, and Faddeeva [18, 1968] and Hanson and Lawson <ref> [27, 1969] </ref> were the first to introduce non-SVD complete orthogonal decompositions to the solution of least squares problems. The four conditions (3.3.68) were set down by Penrose [43, 1955], but an equivalent version was set down earlier by Moore [41, 1935]. A great deal has been written about pseudoinverses.
Reference: [28] <author> N.J. Higham. </author> <title> Accuracy and Stability of Numerical Algorithms. </title> <publisher> SIAM Publications, </publisher> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: Other subtle differences in summing strategies are discussed in <ref> [28, 1996,Chapter 4] </ref>.
Reference: [29] <author> J.-W. Hong and H.T. Kung. </author> <title> I/O complexity: The Red-Blue pebble game. In R.L. </title> <editor> Rivest, editor, </editor> <booktitle> Thirteenth Annual ACM Symposium on the Theory of Computing, </booktitle> <address> New York, NY, 1981. </address> <publisher> ACM. </publisher>
Reference-contexts: MATRIX COMPUTATIONAL CONCERNS Since fi is constant this is an order of magnitude savings over the matmul0, matmul1, and matmul2 codes. That is because the inner loop of matmul-blocks does q 2 storage access, but O (q 3 ) operations. Hong and Kung <ref> [29, 1981] </ref> show that O (n 3 =p) memory accesses is a lower bound on the implementation of this matrix multiplication routine. The cache size cannot be expected to be O ( p n).
Reference: [30] <author> A.S. </author> <title> Householder. The Theory of Matrices in Numerical Analysis. </title> <publisher> Blaisdell, </publisher> <address> New York, 1964. </address> <publisher> Reprinted by Dover, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: The section on probability, x1.3.2, was deliberately nonrigorous. For a thorough introduction read Chung [10, 1974]. Much of the material in x1.3.2 was adapted from Rao [44, 1973]. A thorough treatment of norms is available from a number of books on numerical linear algebra. Householder <ref> [30, 1964, Chapter 2] </ref> is still an excellent source. A recent treatment of norms with special attention to orthogonally invariant norms is in Stewart and Sun [49, 1990, part II].
Reference: [31] <author> IEEE. </author> <title> IEEE Standard for Binary Floating Point Arithmetic, ANSI/IEEE Standard 754-1985. </title> <booktitle> Institute of Electrical and Electronic Engineers, </booktitle> <address> New York, </address> <year> 1985. </year> <note> Reprinted in SIGPLAN Notices, 22(2) 9-25,1987. </note>
Reference: [32] <author> IEEE. </author> <title> A Radix-Independent Standard for Floating Point Arithmetic, IEEE Standard 854-1987. </title> <publisher> IEEE Computer Society, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Round toward 1. All positve overflows are carried to 1 and negative overflows are carried to . There are other specifications to IEEE arithmetic that include input and output and number conversions. For complete details, refer to the original papers [33, 1981],[31, 1985], and <ref> [32, 1987] </ref>. 2.1.3. Error Analysis of Simple Computations Consider the floating point product p n = x 1 x 2 : : : x n ; (2.1.13) where x i ; i = 1; 2; : : : ; n are floating point numbers.
Reference: [33] <author> Floating Point Working Group IEEE Computer Society Microprocessor Standards Committee. </author> <title> A proposed standard for binary floating-point arithmetic, draft 8.0 of IEEE Task P754. </title> <journal> Computer, </journal> <volume> 14 </volume> <pages> 51-62, </pages> <year> 1981. </year> <note> BIBLIOGRAPHY 77 </note>
Reference-contexts: Round toward 1. All positve overflows are carried to 1 and negative overflows are carried to . There are other specifications to IEEE arithmetic that include input and output and number conversions. For complete details, refer to the original papers <ref> [33, 1981] </ref>,[31, 1985], and [32, 1987]. 2.1.3. Error Analysis of Simple Computations Consider the floating point product p n = x 1 x 2 : : : x n ; (2.1.13) where x i ; i = 1; 2; : : : ; n are floating point numbers.
Reference: [34] <author> C. Jordan. </author> <title> Memoire sur les formes bilineaires. </title> <journal> Journal de Mathematiques Pures et Appliquees, Deuxieme Serie, </journal> <volume> 19 </volume> <pages> 53-54, </pages> <note> 1874. (in French). </note>
Reference-contexts: These complete orthogonal decompositions are used to state the solution of the linear least squares problem and to prove the Gauss-Markov theorem. 3.2. Complete Orthogonal Decompositions 3.2.1. The Singular Value Decomposition The singular value decomposition (SVD) was developed independently by Bel-trami [5, 1873] and Jordan <ref> [34, 1874] </ref>, and then rediscovered by Sylvester [51, 1889]. Its importance in the modern methods for solving least squares problems was established Golub and Kahan [24, 1965]. A survey paper by Stewart [48, 1993] gives a nice discussion of the history of the SVD. <p> Its importance in the modern methods for solving least squares problems was established Golub and Kahan [24, 1965]. A survey paper by Stewart [48, 1993] gives a nice discussion of the history of the SVD. Our development is similar to that of Jordan <ref> [34, 1874] </ref>.
Reference: [35] <author> W. Kahan. </author> <title> Branch cuts for complex elementary functions or much ado about nothing's sign bit. </title> <editor> In A. Iserles and M.J.D. Powell, editors, </editor> <booktitle> The State of the Art in Numerical Analysis, </booktitle> <address> Oxford, UK, 1987. </address> <publisher> Oxford University Press. </publisher>
Reference-contexts: Every operation with standard floating point numbers, infinities, or NaNs results in standard floating point numbers, infinities, or NaNs. The need for +0 and 0 is for the proper implementation of branch cuts as discussed by Kahan <ref> [35, 1996] </ref> . Arithmetic on 1 is always exact and, in itself, signals no exceptions, unless 1 is created by overflow or division by zero or used in the following invalid operations 1. (+1) + (1). 3. 1=1. 2.1. FLOATING POINT COMPUTATION 27 4.
Reference: [36] <author> W. Kahan. </author> <title> Lecture notes on the status of IEEE standard p754 for binary floating-point arithmetic. </title> <note> Available through http://http.cs.berkeley.edu/ ~wkahan, frequently updated, </note> <year> 1996. </year>
Reference-contexts: The precision, exponent range, and rounding rules will already be known to you. Most North American computers now manufactured including mainframe computers, workstations, and personal computers implement ANSI/IEEE 754-1985. The principal exceptions are the CRAY X-MP,Y-MP,C90, and J90, the IBM 370 and 3090, and the DEC VAX (see Kahan <ref> [36, 1995] </ref>) which are fading from the scene. As is also pointed out in [36, 1995], a program will not necessarily obtain exactly the same answers on two different computers that implement the binary standard. <p> The principal exceptions are the CRAY X-MP,Y-MP,C90, and J90, the IBM 370 and 3090, and the DEC VAX (see Kahan <ref> [36, 1995] </ref>) which are fading from the scene. As is also pointed out in [36, 1995], a program will not necessarily obtain exactly the same answers on two different computers that implement the binary standard. There are subtle variations among implementations, but, minimally, all implementations should fit the specifications given below.
Reference: [37] <author> W.M. Kahan. </author> <title> Why do we need a floating-point standard? Technical, </title> <institution> Department of Electrical Engineering and Computer Science, University of California, Berkeley, </institution> <address> CA, </address> <year> 1981. </year>
Reference-contexts: Moreover, drastic changes might be necessary for a robust program on one computer to become a robust program on another. A more recent view of the situation is given by Kahan <ref> [37, 1981] </ref>. That paper states a strong case for developing a floating point standard. In 1977, IEEE task P754 was established to produce a standard for floating point arithmetic.
Reference: [38] <editor> C.L. Lawson and R.J. Hanson. </editor> <title> Solving Least Squares Problems. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliff, NJ, </address> <year> 1974. </year>
Reference-contexts: Farebrother [19, 1996] gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book <ref> [38, 1974] </ref>. Faddeev, Kublanovskaya, and Faddeeva [18, 1968] and Hanson and Lawson [27, 1969] were the first to introduce non-SVD complete orthogonal decompositions to the solution of least squares problems.
Reference: [39] <author> C.L. Lawson, R.J. Hanson, D.R. Kincaid, and F.T. Krogh. </author> <title> Basic linear algebra subprogams for FORTRAN usage. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 5 </volume> <pages> 308-25, </pages> <year> 1979. </year>
Reference-contexts: Many such operations were encapsulated in the Basic Linear Algebra Subroutines (BLAS), a set of FORTRAN callable routines. The level terminology came about in the design of the BLAS. The level 1 BLAS (vector operations only) were introduced by Lawson, Hanson, Kincaid, and Krogh <ref> [39, 1979] </ref>. Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson [17, 1988], and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff [16, 1990]. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary <p> only) were introduced by Lawson, Hanson, Kincaid, and Krogh <ref> [39, 1979] </ref>. Subsequently, level 2 BLAS (matrix-vector operations) were introduced by Dongarra, DuCroz, Hammarling, and Hanson [17, 1988], and level 3 BLAS were introduced by Dongarra, DuCroz, Hammarling, and Duff [16, 1990]. For details on the BLAS, read the original papers [39, 1979], [17, 1988], [16, 1990], or the excellent summary in Freeman and Phillips [20, 1992]. Freeman and Phillips also explain the role of the BLAS in parallel and distributed computing. The BLAS undergo updates from time to time. <p> SIMPLE COMPUTATIONS ARE NOT ALWAYS SO SIMPLE 47 using the algorithm (2.3.81)-(2.3.82). The computation of the BLAS function xNRM2 was based upon this algorithm <ref> [39, 1979] </ref>. We now give a summary of the algorithm below. Also included is a formal and full statement of the algorithm (2.3.81)-(2.3.82) as the function twonorm1. <p> An algorithm based upon this function was used for the BLAS 1 routine xNRM2 to compute the two-norm <ref> [39, 1979] </ref>. The paper [6, 1978] did not entirely resolve the issue for two subtle reasons. The first reason is that kxk 1 might be in range when kxk 2 is not. That becomes more likely when n is large as can be seen by examining the inequality (??).
Reference: [40] <author> A.A. </author> <title> Markov. </title> <publisher> Wahrscheinlinkeitsrechnung. B.G. Teubner, </publisher> <address> Leipzig, </address> <year> 1912. </year> <note> (German translation of second Russian edition). </note>
Reference-contexts: The model (3.3.38)-(3.3.39) is called the Gauss-Markov Linear model (see Gauss [21, 1809] (in Latin), [22, 1995] (in English translation), Markov <ref> [40, 1912] </ref> (in German translation from Russian)) . Solving the problem of estimating the parameters y and oe 2 based upon the responses b is called fitting the model. <p> Notes Theorem 3.11, the Gauss-Markov theorem, is really due to Gauss. Markov proved a weaker version of the theorem in the early 1900s so his name has also become attached to the Theorem. See the original papers [21, 1809] , <ref> [40, 1912] </ref> and a recent translation of Gauss's least squares papers by Stewart [22, 1995]. Farebrother [19, 1996] gives a nice discussion of the history of the theorem. The development of least squares problems along with complete orthogonal decomposition is given in Lawson and Hanson's book [38, 1974].
Reference: [41] <author> E.H. Moore. </author> <title> General Analysis. </title> <publisher> American Phil. Soc., </publisher> <address> Philadelphia, </address> <year> 1935. </year>
Reference-contexts: The Pseudoinverse From Theorem 3.12, the minimum length solution can be written ^y LS = X y b (3.3.66) where X y 2 &lt; pfin is given by X y = V k n k 11 0 11 U T The matrix X y is called the Moore-Penrose pseudoinverse <ref> [41, 1935] </ref>, [43, 1955]. It is the unique matrix satisfying the four Penrose conditions 1: XX y X = X; 3: (XX y ) T = XX y ; (3.3.68) For simplicity, we refer to X y as the pseudoinverse. <p> Faddeev, Kublanovskaya, and Faddeeva [18, 1968] and Hanson and Lawson [27, 1969] were the first to introduce non-SVD complete orthogonal decompositions to the solution of least squares problems. The four conditions (3.3.68) were set down by Penrose [43, 1955], but an equivalent version was set down earlier by Moore <ref> [41, 1935] </ref>. A great deal has been written about pseudoinverses. An annotated bibliography is given by 74 CHAPTER 3. BASICS OF LINEAR LEAST SQUARES Nashed and Rall [42, 1976]. Some texts on the role of pseudoinverses in linear models include Rao and Mitra [45, 1971] and Albert [1, 1972].
Reference: [42] <author> M.Z. Nashed and L.B. Rall. </author> <title> Annotated bibliography on generalized inverses and applications. </title> <editor> In M.Z. Nashed, editor, </editor> <booktitle> Generalized Inverses and Applications, </booktitle> <pages> pages 771-1041, </pages> <address> New York, 1976. </address> <publisher> Academic Press. </publisher>
Reference-contexts: The four conditions (3.3.68) were set down by Penrose [43, 1955], but an equivalent version was set down earlier by Moore [41, 1935]. A great deal has been written about pseudoinverses. An annotated bibliography is given by 74 CHAPTER 3. BASICS OF LINEAR LEAST SQUARES Nashed and Rall <ref> [42, 1976] </ref>. Some texts on the role of pseudoinverses in linear models include Rao and Mitra [45, 1971] and Albert [1, 1972]. There are also a number of books from the linear algebra point of view. A good one is by Campbell and Meyer [7, 1979].
Reference: [43] <author> R. Penrose. </author> <title> A generalized inverse for matrices. </title> <journal> Proc. Camb. Phil. Soc., </journal> <volume> 51 </volume> <pages> 406-413, </pages> <year> 1955. </year>
Reference-contexts: Pseudoinverse From Theorem 3.12, the minimum length solution can be written ^y LS = X y b (3.3.66) where X y 2 &lt; pfin is given by X y = V k n k 11 0 11 U T The matrix X y is called the Moore-Penrose pseudoinverse [41, 1935], <ref> [43, 1955] </ref>. It is the unique matrix satisfying the four Penrose conditions 1: XX y X = X; 3: (XX y ) T = XX y ; (3.3.68) For simplicity, we refer to X y as the pseudoinverse. <p> Faddeev, Kublanovskaya, and Faddeeva [18, 1968] and Hanson and Lawson [27, 1969] were the first to introduce non-SVD complete orthogonal decompositions to the solution of least squares problems. The four conditions (3.3.68) were set down by Penrose <ref> [43, 1955] </ref>, but an equivalent version was set down earlier by Moore [41, 1935]. A great deal has been written about pseudoinverses. An annotated bibliography is given by 74 CHAPTER 3. BASICS OF LINEAR LEAST SQUARES Nashed and Rall [42, 1976].
Reference: [44] <author> C.R. Rao. </author> <title> Linear Statistical Inference and Its Applications. </title> <publisher> Wiley, </publisher> <address> New York, </address> <note> second edition, </note> <year> 1973. </year>
Reference-contexts: PRELIMINARIES 1.3.2. Probability Basics Some definitions and elementary theorems from probability are necessary in explaining the context of linear least squares problems. These definitions correspond to those from Rao <ref> [44, 1973] </ref>. Let PrfAg denote the probability that the event A occurs. A real valued random variable x is a variable associated with a distribution function F () : &lt; ! [0; 1] such that F (a) = Prfx &lt; ag for all a 2 &lt;. <p> The normal distribution can be defined for a singular , but no density function exists for it on all of &lt; n . However, the density function does exist on a subspace, see Rao <ref> [44, 1973,pp.527-528] </ref> or Chapter ??. A vector x of random variables is uncorrelated if the matrix Cov (x) is diagonal, otherwise x is said to be correlated. <p> The first chapter of Stewart's book [47, 1973] gives a summary of the important theorems for applications to least squares. The section on probability, x1.3.2, was deliberately nonrigorous. For a thorough introduction read Chung [10, 1974]. Much of the material in x1.3.2 was adapted from Rao <ref> [44, 1973] </ref>. A thorough treatment of norms is available from a number of books on numerical linear algebra. Householder [30, 1964, Chapter 2] is still an excellent source. A recent treatment of norms with special attention to orthogonally invariant norms is in Stewart and Sun [49, 1990, part II]. <p> It is a common way to to state the the Gauss-Markov linear model <ref> [44, p. 298] </ref>. Equation (3.3.82) illustrates that both ~y LS 3.3. THE LINEAR LEAST SQUARES PROBLEM 73 and ^r LS are part of the solution to this problem. In x??, we discuss a method based upon the formulation (3.3.82) for the case when X has full column rank. <p> From the assumptions of Theorem 3.11, ^y LS has the variance-covariance matrix Cov (^y LS ) = oe 2 A y = oe 2 (X T X) y : (3.3.85) The above expression for Cov (^y LS ) can be derived from equation (3.3.80) (see Exercise ???). Rao <ref> [44, 1973,p.226] </ref> gives an expression for this matrix for all solutions ~y LS (see Exercise ???). If b has a N (Xy; oe 2 I) distribution, then ^y LS has an N (y; oe 2 A y ) distribution.
Reference: [45] <author> C.R. Rao and S.K. Mitra. </author> <title> Generalized Inverse of Matrices and Its Applications. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: A great deal has been written about pseudoinverses. An annotated bibliography is given by 74 CHAPTER 3. BASICS OF LINEAR LEAST SQUARES Nashed and Rall [42, 1976]. Some texts on the role of pseudoinverses in linear models include Rao and Mitra <ref> [45, 1971] </ref> and Albert [1, 1972]. There are also a number of books from the linear algebra point of view. A good one is by Campbell and Meyer [7, 1979].
Reference: [46] <author> P.H. Sterbenz. </author> <title> Floating-Point Computation. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1974. </year>
Reference-contexts: The biggest problems have been subtle differences in rounding from the idealized round in the last section and the handling of numbers at the extreme of the number range. In the early 1970's, a hodge-podge of rounding rules were implemented (see Sterbenz <ref> [46, 1974] </ref>, Cody [13, 1973],and Barlow [3, 1981]). A user often had to know a great deal about the arithmetic in a computer to produce reliable numerical software. Moreover, drastic changes might be necessary for a robust program on one computer to become a robust program on another.
Reference: [47] <author> G.W. Stewart. </author> <title> Introduction to Matrix Computations. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Notes For a more thorough discussion of the results from linear algebra see, for instance, the book by Strang [50, 1993]. The first chapter of Stewart's book <ref> [47, 1973] </ref> gives a summary of the important theorems for applications to least squares. The section on probability, x1.3.2, was deliberately nonrigorous. For a thorough introduction read Chung [10, 1974]. Much of the material in x1.3.2 was adapted from Rao [44, 1973]. <p> We refer to g (x) as the value produced by a computer algorithm to compute g (x). If component of the vector x are out of the computer range, they can be rescaled. Input or representation errors can be resolved by the theory of conditioning [25, 1989], <ref> [47, 1973] </ref>, [49, 1990], the same theory we use for rounding errors. Approximation error is an issue in Chapters ??, ?? , ??, and ??. We discuss two methods for analyzing rounding errors, forward error analysis and backward error analysis. Other formalisms have been proposed, see x??.
Reference: [48] <author> G.W. Stewart. </author> <title> On the early history of the singular value decomposition. </title> <journal> SIAM Review, </journal> <volume> 35 </volume> <pages> 551-566, </pages> <year> 1993. </year>
Reference-contexts: Its importance in the modern methods for solving least squares problems was established Golub and Kahan [24, 1965]. A survey paper by Stewart <ref> [48, 1993] </ref> gives a nice discussion of the history of the SVD. Our development is similar to that of Jordan [34, 1874].
Reference: [49] <author> G.W. Stewart and J.-G. Sun. </author> <title> Matrix Perturbation Theory. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: A thorough treatment of norms is available from a number of books on numerical linear algebra. Householder [30, 1964, Chapter 2] is still an excellent source. A recent treatment of norms with special attention to orthogonally invariant norms is in Stewart and Sun <ref> [49, 1990, part II] </ref>. Chapter 2 Matrix Computational Concerns We give some basics of the analysis and design of numerical algorithms for linear algebra computations in floating point arithmetic. There are two important issues, accuracy, discussed in x2.1, and efficiency, discussed in x2.2. <p> We refer to g (x) as the value produced by a computer algorithm to compute g (x). If component of the vector x are out of the computer range, they can be rescaled. Input or representation errors can be resolved by the theory of conditioning [25, 1989], [47, 1973], <ref> [49, 1990] </ref>, the same theory we use for rounding errors. Approximation error is an issue in Chapters ??, ?? , ??, and ??. We discuss two methods for analyzing rounding errors, forward error analysis and backward error analysis. Other formalisms have been proposed, see x??.
Reference: [50] <author> G. Strang. </author> <title> Introduction to Linear Algebra. </title> <publisher> Wellesley-Cambridge Press, </publisher> <address> Welles-ley, MA, </address> <year> 1993. </year>
Reference-contexts: Notes For a more thorough discussion of the results from linear algebra see, for instance, the book by Strang <ref> [50, 1993] </ref>. The first chapter of Stewart's book [47, 1973] gives a summary of the important theorems for applications to least squares. The section on probability, x1.3.2, was deliberately nonrigorous. For a thorough introduction read Chung [10, 1974]. Much of the material in x1.3.2 was adapted from Rao [44, 1973].
Reference: [51] <author> J.J. </author> <title> Sylvester. Sur la reduction biorthogonale d'une forme lineo-lineaire a sa forme connonique. </title> <institution> Comptes Rendus de l'Aademie des Sciences, </institution> <note> Paris 108 651-653, 1889. (in French). 78 BIBLIOGRAPHY </note>
Reference-contexts: Complete Orthogonal Decompositions 3.2.1. The Singular Value Decomposition The singular value decomposition (SVD) was developed independently by Bel-trami [5, 1873] and Jordan [34, 1874], and then rediscovered by Sylvester <ref> [51, 1889] </ref>. Its importance in the modern methods for solving least squares problems was established Golub and Kahan [24, 1965]. A survey paper by Stewart [48, 1993] gives a nice discussion of the history of the SVD. Our development is similar to that of Jordan [34, 1874].
Reference: [52] <author> E. Van De Velde. </author> <title> Concurrent Scientific Computation. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: Another algorithm to compute s n could obtain a slightly different error bound. The following technique is common for computation on distributed computers <ref> [52, 1995,pp.26-33] </ref>. Let s ij = k=i We can then compute these sum according to s 1;m+` = s 1;m + s m+1;m+` : Since s n = s 1;n we can compute s n from the following recursive procedure. 30 CHAPTER 2.
Reference: [53] <author> D.H.D. West. </author> <title> Updating mean and variance estimates: an improved method. </title> <journal> Comm. Assoc. Comput. Mach., </journal> <volume> 22 </volume> <pages> 532-535, </pages> <year> 1979. </year>
Reference-contexts: The best bound that can be obtained for the relative error in the computed value ~s 2 is j~s sj " m g (n) s (x) 2 + O (" 2 where g (n) satisfies (2.3.56). To avoid this problem, several authors, Youngs and Cramer [54, 1971], West <ref> [53, 1979] </ref>,and E.H. Hansen [26, 1975] derived stable algorithms for computing the sample variance using only one pass through the data. In [8, 1983], it was pointed out that all of these algorithms were essentially equivalent. We now give the version due to West [53, 1979]. m 1 = x 1 <p> Youngs and Cramer [54, 1971], West <ref> [53, 1979] </ref>,and E.H. Hansen [26, 1975] derived stable algorithms for computing the sample variance using only one pass through the data. In [8, 1983], it was pointed out that all of these algorithms were essentially equivalent. We now give the version due to West [53, 1979]. m 1 = x 1 ; t 0 = 0; (2.3.68) t k+1 = t k + k x = n 1 t n : (2.3.71) Chan and Lewis [9, 1979] showed that West's algorithm satisfied a bound of the form (2.3.54) where g (n) = O (n): (2.3.72)
Reference: [54] <author> E.A. Youngs and E.M. Cramer. </author> <title> Some results relevant to choice of sum and sum-of-product algorithms. </title> <journal> Technometrics, </journal> <volume> 13 </volume> <pages> 657-665, </pages> <year> 1971. </year>
Reference-contexts: The best bound that can be obtained for the relative error in the computed value ~s 2 is j~s sj " m g (n) s (x) 2 + O (" 2 where g (n) satisfies (2.3.56). To avoid this problem, several authors, Youngs and Cramer <ref> [54, 1971] </ref>, West [53, 1979],and E.H. Hansen [26, 1975] derived stable algorithms for computing the sample variance using only one pass through the data. In [8, 1983], it was pointed out that all of these algorithms were essentially equivalent.
References-found: 54

