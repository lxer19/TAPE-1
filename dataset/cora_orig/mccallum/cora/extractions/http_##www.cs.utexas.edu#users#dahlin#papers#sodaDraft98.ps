URL: http://www.cs.utexas.edu/users/dahlin/papers/sodaDraft98.ps
Refering-URL: http://www.cs.utexas.edu/users/dahlin/papers.html
Root-URL: http://www.cs.utexas.edu
Email: vlr@cs.utexas.edu  bgrayson@ece.utexas.edu  dahlin@cs.utexas.edu  
Title: Efficient Algorithms on General-Purpose Parallel Models  
Author: Vijaya Ramachandran Brian Grayson Michael D. Dahlin 
Date: July 7, 1998  
Address: Austin, TX 78712  Austin, TX 78712  Austin, TX 78712  
Affiliation: Dept. of Computer Sciences Univ. of Texas  Dept. of ECE Univ. of Texas  Dept. of Computer Sciences Univ. of Texas  
Abstract: We present simple and efficient parallel algorithms for several fundamental problems on the s-QSM, a general-purpose shared-memory model for parallel computation. We justify the general applicability of the s-QSM model by presenting work-preserving emulations with small slowdown between several general-purpose parallel models: BSP, LogP, QSM and s-QSM models. In particular, the emulations of the s-QSM on the other models are quite simple. Using appropriate cost measures, we analyze the performance of four s-QSM algorithms prefix sums, list ranking, sorting, and connected components. Finally, we present simulation results for our prefix sums, list ranking, and sorting algorithms that suggest that s-QSM analysis will accurately predict algorithm performance in practice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Adler, J. Byer, R. M. Karp, </author> <title> Scheduling Parallel Communication: The h-relation Problem. </title> <booktitle> In Proc. MFCS, </booktitle> <year> 1995. </year>
Reference-contexts: II) We route the messages to destination LogP processors for each phase or superstep while satisfying the capacity constraint as follows: 1. Determine a good upper bound on the value of h. 2. Route the h relation while satisfying the capacity constraint using the algorithm in <ref> [1] </ref> which routes an h-relation on the LogP in O (g (h + log 3 P log log P )) time w.h.p. in P . 3. Execute a barrier synchronization on the LogP processors. This can be performed in O (ld log P time using the LogP algorithm in [17]. <p> number of messages to be sent by any processor 3 and m = total number of messages to be sent by all processors. 2. q := (log P )=m repeat pfor each processor do Select each message with probability q and send selected messages to destination using the protocol in <ref> [1] </ref> with h = m q; := max. number of messages received by any processor; q := q log P rofp until q (log P )=s or c log P 3. r := =q; h := max (r; s) Claim 3.1 The algorithm for Step 1 determines the correct value of
Reference: [2] <author> M. Adler, W. Dittrich, B. Juurlink, M. Kutylowski, I. Rieping. </author> <title> Communication-optimal parallel minimum spanning tree algorithms. </title> <booktitle> In Proc. ACM SPAA, </booktitle> <pages> pp. 2736, </pages> <year> 1998. </year>
Reference-contexts: The best previous linear-work connected components algorithm is the QSM algorithm adapted from the EREW algorithm in [13], which is extremely involved and runs in O (log n) phases. Connected components algorithms that run in a number of phases independent of input size are presented in <ref> [7, 2] </ref> for the BSP, but neither achieve linear work for an arbitrary input graph. In contrast our connected components runs in linear work and in o (log P log log P ) phases and is fairly simple.
Reference: [3] <author> M. Ajtai, J. Komlos, E. Szemeredi, </author> <title> An O(n log n) sorting network. </title> <booktitle> In Proc. ACM STOC, </booktitle> <pages> pp. 19, </pages> <year> 1983. </year>
Reference: [4] <author> G. Bilardi, K. T. Herley, A. Pietracaprina, G. Pucci, P. Spirakis. </author> <title> BSP vs LogP. </title> <booktitle> In Proc. ACm SPAA, </booktitle> <pages> pp. 2532, </pages> <year> 1996. </year>
Reference-contexts: The only mis-match we have is between the `stalling' and `nonstalling' LogP models. Here we show that an earlier result claimed in <ref> [4] </ref> is erroneous by giving a counterexample to their claim. Work-preserving emulations between BSP, QSM and s-QSM were presented earlier in [10]. <p> In this section we focus on three aspects of these emulations. First, we develop new, work-preserving emulations of QSM or BSP on LogP; previously known emulations <ref> [4] </ref> required sorting and increased both time and work by a logarithmic factor. Second, we provide new analysis of the known emulation of LogP on BSP [4]; we provide a counter-example to the claim that this emulation holds for the stalling LogP model, and we observe that the original non-work-preserving emulation <p> First, we develop new, work-preserving emulations of QSM or BSP on LogP; previously known emulations <ref> [4] </ref> required sorting and increased both time and work by a logarithmic factor. Second, we provide new analysis of the known emulation of LogP on BSP [4]; we provide a counter-example to the claim that this emulation holds for the stalling LogP model, and we observe that the original non-work-preserving emulation may be trivially extended to be work-preserving. <p> Results in which the LogP model is either the emulated or the emulating machine are new results that appear in bold font in the table and are described in this paper. 1 This result is presented in <ref> [4] </ref> but it is stated there erroneously that it holds for stalling LogP programs. In this paper we give a counter-example to that claim. 3.1 Work-Preserving Emulations of QSM and BSP on LogP We now sketch our results for emulating BSP, QSM and s-QSM on LogP. <p> This emulation is presented in <ref> [4] </ref> as an emulation where both the time and work increases by a factor L=l. We observe that this emulation can be made work-preserving by using a BSP with a smaller number of processors and mapping L=l LogP processors onto each BSP processor. The analysis in [4] erroneously states that the <p> emulation is presented in <ref> [4] </ref> as an emulation where both the time and work increases by a factor L=l. We observe that this emulation can be made work-preserving by using a BSP with a smaller number of processors and mapping L=l LogP processors onto each BSP processor. The analysis in [4] erroneously states that the L=l performance bound holds for stalling LogP computations. We now show a simple example of a stalling LogP computation whose execution time squares when emulated in the above manner on the BSP. <p> The total running time of this algorithm on the LogP is O (r L + g q). However, an emulation of this algorithm on the BSP using the algorithm in <ref> [4] </ref> will result in a BSP running time of O (r (L + gq)), which could be quadratically larger than the running time on LogP if r = gq and r &gt;> L. <p> The one mis-match we have is between stalling and non-stalling LogP, and here we do not know how to provide a work-preserving emulation with small slow-down of a stalling LogP on any of the other models. This is in contrast to the inference made in <ref> [4] </ref> that LogP is essentially equivalent to BSP. The algorithms for emulating LogP (or BSP) on QSM or s-QSM are rather involved due to the use of sorting and multiple compaction. On the other hand QSM and s-QSM have simple emulations on BSP and LogP.
Reference: [5] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K.E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symp. on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 112, </pages> <month> May </month> <year> 1993. </year> <month> 11 </month>
Reference-contexts: 1 Introduction There is a vast amount of literature on parallel algorithms for various problems. However algorithms developed using traditional approaches on PRAM and fixed-interconnect networks do not map well to real machines. In recent years several general-purpose parallel models have been proposed BSP [23], LogP <ref> [5] </ref>, QSM and s-QSM [10]. These models attempt to capture the key features of real machines while retaining a reasonably high-level programming abstraction. <p> The cost, T , of the superstep is defined to be T = max (w; g h; L). The time taken by a BSP algorithm is the sum of the costs of the individual supersteps in the algorithm. LogP Model. The LogP model <ref> [5] </ref> consists of p processor/memory components communicating through point-to-point messages, and has the following parameters: the latency l, which is the time taken by the 1 network to transmit a message from one to processor to another; an overhead o, which is the time spend by a processor to transfer a
Reference: [6] <author> M. Dahlin, B. Grayson, V. Ramachandran, </author> <title> Experimental Evaluation of QSM: A Simple Shared--Memory Model. </title> <note> July 1998 (under preparation). </note>
Reference-contexts: The simulator was set to 7 parameters for a state-of-the-art machine. A detailed description of this experimental work can be found in <ref> [6] </ref>. The results are summarized in the graphs attached to the end of the paper. The plots show the communication and synchronization costs for the algorithms as predicted by s-QSM and BSP as well as the actual values obtained through simulation for synchronization, communication and computation.
Reference: [7] <author> E. Caceres, F. Dehne, A. Ferreira, P. Flocchini, I. Rieping, A. Roncato, N. Santoro, and S. W. Song. </author> <title> Efficient parallel graph algorithms for coarse grained multicomputers and BSP. </title> <booktitle> In Proc. ICALP, </booktitle> <volume> LNCS 1256, </volume> <pages> pp. 390-400, </pages> <year> 1997. </year>
Reference-contexts: All of the algorithms we present have this feature. Related work on minimizing the number of phases (or supersteps) using the notion of rounds is reported in [11] for sorting, and in <ref> [7] </ref> for graph problems. Several lower bounds for the number of rounds needed for basic problems on the QSM and BSP are presented in [20]. A `round' is a phase or superstep that performs linear work (O (gn=p) time on s-QSM, and O (gn=P + L) time on BSP). <p> On the other hand, a computation that proceeds in rounds need not lead to a linear work algorithm if the number of rounds in the algorithm is non-constant. In fact, all of the algorithms presented in <ref> [7] </ref> perform superlinear work. The algorithm in [11] performs superlinear communication when the number of processors is large. <p> We start with a simple, efficient s-QSM algorithm that runs in O (log P ) phases, and performs work that is superlinear by a factor of log P . While other algorithms that achieve this performance are known for the BSP <ref> [7] </ref> our efficient algorithm has some special features: It avoids the use of pointers by restricting tree-loops to be of height at most 2 at all stages. <p> The best previous linear-work connected components algorithm is the QSM algorithm adapted from the EREW algorithm in [13], which is extremely involved and runs in O (log n) phases. Connected components algorithms that run in a number of phases independent of input size are presented in <ref> [7, 2] </ref> for the BSP, but neither achieve linear work for an arbitrary input graph. In contrast our connected components runs in linear work and in o (log P log log P ) phases and is fairly simple. <p> &gt; 0 then procedure Contract runs in O (k) phases, and takes total time O ((n + km)P ) time, where n is the number of vertices and m is the number of edges in H. 5.3 The Wrap-Up Procedure for Small Dense Graphs This is a procedure used in <ref> [7] </ref> for a BSP connected components algorithm.
Reference: [8] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> The Queue-Read Queue-Write PRAM model: Accounting for contention in parallel algorithms. </title> <journal> SIAM Journal on Computing, </journal> <note> 1997. To appear. Preliminary version appears in Proc. </note> <editor> 5th ACM-SIAM Symp. </editor> <booktitle> on Discrete Algorithms, </booktitle> <pages> pages 638-648, </pages> <month> January </month> <year> 1994. </year>
Reference: [9] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Efficient low-contention parallel algorithms. </title> <journal> Journal of Computer and System Sciences, </journal> <note> 53(3):417442, 1996. Special issue devoted to selected papers from the 1994 ACM Symp. on Parallel Algorithms and Architectures. </note>
Reference-contexts: detail in the Appendix.) * Use the shared memory to estimate the number of messages being sent to the each destination proces sor (use sampling, sorting, and queue reads). 4 * Write the messages meant for each processor in a block of shared memory assigned to it (use multiple compaction <ref> [9] </ref>). * Have each processor read the messages sent to it from its subarray in shared memory and write them into the corresponding locations in its local memory. Theorem 3.2 A non-stalling LogP computation can be emulated on the BSP in a work-preserving manner with slowdown O (L=l).
Reference: [10] <author> P. B. Gibbons, Y. Matias, and V. Ramachandran. </author> <title> Can a shared-memory model serve as a bridging model for parallel computation? In Proc. </title> <booktitle> 9th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1997, </year> <pages> pp. 72-83. </pages> <note> To appear. </note>
Reference-contexts: 1 Introduction There is a vast amount of literature on parallel algorithms for various problems. However algorithms developed using traditional approaches on PRAM and fixed-interconnect networks do not map well to real machines. In recent years several general-purpose parallel models have been proposed BSP [23], LogP [5], QSM and s-QSM <ref> [10] </ref>. These models attempt to capture the key features of real machines while retaining a reasonably high-level programming abstraction. <p> The only mis-match we have is between the `stalling' and `nonstalling' LogP models. Here we show that an earlier result claimed in [4] is erroneous by giving a counterexample to their claim. Work-preserving emulations between BSP, QSM and s-QSM were presented earlier in <ref> [10] </ref>. Since both message-passing and shared-memory are widely-used in practice, we suggest that a high-level general-purpose model should be one that maps on to both in a simple and efficient way. <p> If the number of messages in transit to a destination processor is dl=ge then a processor that needs to send a message to stalls, and does not perform any operation until the message can be sent. QSM and s-QSM models. The Queuing Shared Memory (QSM) model <ref> [10] </ref> consists of a number of identical processors, each with its own private memory, that communicate by reading and writing shared memory. Processors execute a sequence of synchronized phases, each consisting of an arbitrary interleaving of shared memory reads, shared memory writes, and local computation. <p> the correct value of h to within a constant factor whp in time O ((g log 3 P log log P + ld log P The proof of Theorem 3.1 now follows from the Claim, the stated time bounds for steps 2 and 3, and from the probabilistic analysis in <ref> [10, 22] </ref> that shows that the hashing of the QSM shared-memory on the LogP distributes the memory accesses uniformly, so that h = O (t S), where t is the time cost of the phase or superstep being emulated by the LogP, and S is the slowdown stated in the theorem. <p> Each QSM (or s-QSM) processor copies into its private memory the messages that were sent in the current superstep to the local memory of the LogP processors mapped to it as follows (this is the method of <ref> [10] </ref> to emulate BSP on QSM, and is described in greater detail in the Appendix.) * Use the shared memory to estimate the number of messages being sent to the each destination proces sor (use sampling, sorting, and queue reads). 4 * Write the messages meant for each processor in a <p> Theorem 4.1 The Prefix Sums algorithm runs in O (gn=P ) time (and hence O (gn) work) and O ( log n log (n=P ) ) phases when P n= log n. This result is optimal since there is a corresponding lower bound for the work <ref> [10] </ref>, time [20] and the number of phases [20]. Note that this algorithm runs in a constant number of rounds if P = n c , for some constant c &gt; 0. In particular, if P = O ( p n) then the algorithm requires just three phases. Broadcasting.
Reference: [11] <author> M. Goodrich. </author> <title> Communication-Efficient Parallel Sorting. </title> <booktitle> In Proc. STOC, </booktitle> <pages> pp. 247256, </pages> <year> 1996. </year>
Reference-contexts: All of the algorithms we present have this feature. Related work on minimizing the number of phases (or supersteps) using the notion of rounds is reported in <ref> [11] </ref> for sorting, and in [7] for graph problems. Several lower bounds for the number of rounds needed for basic problems on the QSM and BSP are presented in [20]. <p> On the other hand, a computation that proceeds in rounds need not lead to a linear work algorithm if the number of rounds in the algorithm is non-constant. In fact, all of the algorithms presented in [7] perform superlinear work. The algorithm in <ref> [11] </ref> performs superlinear communication when the number of processors is large. In contrast to the cost metric that uses the notion of rounds, in this paper we ask for algorithms that perform optimal work and communication, and additionally compute in a small number of phases.
Reference: [12] <author> B. Grayson. Armadillo: </author> <title> A High-Performance Processor Simulator. </title> <type> Masters thesis, ECE, </type> <institution> UT-Austin, </institution> <year> 1996. </year>
Reference-contexts: Theorem 4.3 The List Ranking algorithm runs with optimal work and communication, in O (log P ) phases w.h.p. when the number of processors P = O ( q log n ). 4.5 Simulation Results We investigated the performance of these algorithms on Armadillo <ref> [12] </ref>, which is a simulated architecture with parameterizable configurations and cycle-by-cycle profiling and accuracy. The simulator was set to 7 parameters for a state-of-the-art machine. A detailed description of this experimental work can be found in [6].
Reference: [13] <author> S. Halperin and U. Zwick. </author> <title> Optimal randomized EREW PRAM algorithms for finding spanning forests and other basic graph connectivity problems. </title> <booktitle> In Porc. ACM-SIAM SODA, </booktitle> <pages> pp. 438447, </pages> <year> 1996. </year>
Reference-contexts: Our algorithm also avoids sorting. We then use this algorithm as the `base case' in our linear-work algorithm. The best previous linear-work connected components algorithm is the QSM algorithm adapted from the EREW algorithm in <ref> [13] </ref>, which is extremely involved and runs in O (log n) phases. Connected components algorithms that run in a number of phases independent of input size are presented in [7, 2] for the BSP, but neither achieve linear work for an arbitrary input graph.
Reference: [14] <author> D. R. Karger, P. N. Klein, and R. E. Tarjan. </author> <title> A randomized linear-time algorithm to find minimum spanning trees. </title> <journal> Journal of the ACM, </journal> <volume> 42:321328, </volume> <year> 1995. </year>
Reference: [15] <author> D. Karger, N. Nisan, Parnas. </author> <title> Fast Connected Components Algorithms for the EREW PRAM. </title> <booktitle> in Proc. ACM SPAA, </booktitle> <pages> pp. 373380, </pages> <year> 1992. </year>
Reference-contexts: The correctness of the method follows from a sampling theorem in <ref> [15] </ref>. 2 Finally we present a linear-work algorithm for connected components that runs in linear-work and with O (log P 2 log fl P ) phases w.h.p on any input graph. The algorithm is an adaptation of the parallel EREW MSF algorithm in [21].
Reference: [16] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 869941. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year>
Reference: [17] <author> R. Karp, A. Sahay, E. Santos, and K.E. Schauser, </author> <title> Optimal broadcast and summation in the LogP model, </title> <booktitle> In Proc. 5th ACM Symp. on Parallel Algorithms and Architectures, </booktitle> <volume> 142153, </volume> <month> June-July </month> <year> 1993. </year>
Reference-contexts: Execute a barrier synchronization on the LogP processors. This can be performed in O (ld log P time using the LogP algorithm in <ref> [17] </ref>. To complete the description of the algorithm we describe our method for performing step 1 in the above algorithm. Algorithm for Step 1 1.
Reference: [18] <author> K. Kennedy. </author> <title> A research agenda for high performance computing software. </title> <booktitle> In Developing a Computer Science Agenda for High-Performance Computing, </booktitle> <pages> pages 106109. </pages> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference: [19] <author> V. King, C. K. Poon, V. Ramachandran, S. Sinha. </author> <title> A randomized linear work EREW PRAM algorithm to find a minimum spanning forest. </title> <booktitle> In Proc. 8th Annual International Symposium on Algorithms and Computation (ISAAC '97), </booktitle> <publisher> Springer-Verlag LNCS vol. </publisher> <address> 1530, </address> <year> 1997, </year> <pages> pp. 212-222. </pages>
Reference-contexts: The extensions are that we use a slightly different hooking scheme, we keep track of the edges in the MSF, and we use an MSF verification algorithm to execute Clean-up. For this last algorithm, we adapt the linear-work parallel MSF verification algorithm in <ref> [19] </ref> (which runs in O (log n) phases) to run in O (log P ) phases and linear work. Space limitations prevent us from providing the details.
Reference: [20] <author> P. D. MacKenzie and V. Ramachandran. </author> <title> Computational bounds for fundamental problems on general-purpose parallel models. </title> <booktitle> In Proc. 10th Annual ACM Symp. on Parallel Algs. and Arch. </booktitle> <address> (SPAA'98), </address> <month> June-July </month> <year> 1998, </year> <note> to appear. 12 </note>
Reference-contexts: Related work on minimizing the number of phases (or supersteps) using the notion of rounds is reported in [11] for sorting, and in [7] for graph problems. Several lower bounds for the number of rounds needed for basic problems on the QSM and BSP are presented in <ref> [20] </ref>. A `round' is a phase or superstep that performs linear work (O (gn=p) time on s-QSM, and O (gn=P + L) time on BSP). <p> Theorem 4.1 The Prefix Sums algorithm runs in O (gn=P ) time (and hence O (gn) work) and O ( log n log (n=P ) ) phases when P n= log n. This result is optimal since there is a corresponding lower bound for the work [10], time <ref> [20] </ref> and the number of phases [20]. Note that this algorithm runs in a constant number of rounds if P = n c , for some constant c &gt; 0. In particular, if P = O ( p n) then the algorithm requires just three phases. Broadcasting. <p> This result is optimal since there is a corresponding lower bound for the work [10], time <ref> [20] </ref> and the number of phases [20]. Note that this algorithm runs in a constant number of rounds if P = n c , for some constant c &gt; 0. In particular, if P = O ( p n) then the algorithm requires just three phases. Broadcasting.
Reference: [21] <author> C. K. Poon, V. Ramachandran, </author> <title> A randomized linear work EREW PRAM algorithm to find a mini-mum spanning forest. </title> <booktitle> In Proc. 8th Annual International Symposium on Algorithms and Computation (ISAAC '97), </booktitle> <publisher> Springer-Verlag LNCS vol. </publisher> <address> 1530, </address> <month> December </month> <year> 1997, </year> <pages> pp. 212-222. </pages>
Reference-contexts: In contrast our connected components runs in linear work and in o (log P log log P ) phases and is fairly simple. Our algorithm is an adaptation of the parallel MSF algorithm in <ref> [21] </ref> which runs in linear work with O (log n log log n2 log fl n ) phases. <p> The algorithm is an adaptation of the parallel EREW MSF algorithm in <ref> [21] </ref>. Here, k 0 = P , k i = blog k i1 c for 1 i l and l as the smallest integer such that k l 2. <p> Proof: The analysis for correctness and linear work is similar to <ref> [21] </ref>, and we omit the details. For the number of phases we note that the initial call to LinearCC (H; l; A) will generate 2 recursive calls to LinearCC (H; l 1; A), and in general 2 i calls to LinearCC (H; l i; A), 1 i l 1.
Reference: [22] <author> V. Ramachandran. </author> <title> A general purpose shared-memory model for parallel computation. In Algorithms for Parallel Processing, </title> <journal> Volume 105, </journal> <note> IMA Volumes in Mathematics and its Applications, Springer-Verlag, to appear. </note>
Reference-contexts: the correct value of h to within a constant factor whp in time O ((g log 3 P log log P + ld log P The proof of Theorem 3.1 now follows from the Claim, the stated time bounds for steps 2 and 3, and from the probabilistic analysis in <ref> [10, 22] </ref> that shows that the hashing of the QSM shared-memory on the LogP distributes the memory accesses uniformly, so that h = O (t S), where t is the time cost of the phase or superstep being emulated by the LogP, and S is the slowdown stated in the theorem.
Reference: [23] <author> L. G. Valiant. </author> <title> A bridging model for parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8):103 111, </volume> <year> 1990. </year>
Reference-contexts: 1 Introduction There is a vast amount of literature on parallel algorithms for various problems. However algorithms developed using traditional approaches on PRAM and fixed-interconnect networks do not map well to real machines. In recent years several general-purpose parallel models have been proposed BSP <ref> [23] </ref>, LogP [5], QSM and s-QSM [10]. These models attempt to capture the key features of real machines while retaining a reasonably high-level programming abstraction. <p> More detailed descriptions can be found in the appendix. BSP Model. The Bulk-Synchronous Parallel (BSP) model <ref> [23, 24] </ref> consists of p processor/memory components that communicate by sending point-to-point messages. The interconnection network supporting this communication is characterized by a bandwidth parameter g and a latency parameter L. A BSP computation consists of a sequence of supersteps separated by bulk synchronizations.
Reference: [24] <author> L. G. Valiant. </author> <title> General purpose parallel architectures. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, Volume A, </booktitle> <pages> pages 943972. </pages> <publisher> Elsevier Science Publishers B.V., </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1990. </year> <title> 13 (a) (c) sixteen simulated processors. The computation was started with the input distributed evenly across the processors. Synchronization and communication costs are negligible for this algorithm. 14 (a) (c) sixteen simulated processors. 15 (a) processors. Communication and synchronization costs are minimal for this algorithm, and are not visible on this plot. </title> <type> 16 APPENDIX </type>
Reference-contexts: More detailed descriptions can be found in the appendix. BSP Model. The Bulk-Synchronous Parallel (BSP) model <ref> [23, 24] </ref> consists of p processor/memory components that communicate by sending point-to-point messages. The interconnection network supporting this communication is characterized by a bandwidth parameter g and a latency parameter L. A BSP computation consists of a sequence of supersteps separated by bulk synchronizations.
References-found: 24

