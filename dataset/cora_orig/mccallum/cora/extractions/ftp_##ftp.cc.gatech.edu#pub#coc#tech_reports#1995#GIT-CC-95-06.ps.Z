URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1995/GIT-CC-95-06.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.95.html
Root-URL: 
Email: e-mail: gautam,aman,rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136 Fax: (404) 894-9442  
Title: The Quest for a Zero Overhead Shared Memory Parallel Machine  
Keyword: Key Words: Shared memory multiprocessors, Execution-driven simulation, Communication overheads, latency tol erating techniques, synchronization, Cache coherence protocols,  
Note: This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Gautam Shah Aman Singla Umakishore Ramachandran Technical Report GIT-CC-95-06 January 1995 Abstract In this paper we present a new approach to benchmark the performance of shared memory systems. This approach focuses on recognizing how far off is the performance of a given memory system from a realistic ideal parallel machine. We define such a realistic machine model called the z-machine, which accounts for the inherent communication costs in an application by tracking the data flow in the application. The z-machine is incorporated into an execution-driven simulation framework to be used as a reference for benchmarking for different memory systems. The components of the overheads in these memory systems are identified and quantified for four applications. Using the z-machine performance as the standard to strive for we discuss the implications of the performance results and suggest architectural trends to pursue for realizing a zero overhead shared memory machine. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. J. Anderson and J. C. Setubal. </author> <title> On the parallel implementation of goldberg's maximum flow algorithm. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 168-77, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The applications we studied include Cholesky and Nbody from SPLASH 5 suite [9], Integer Sort from the NAS parallel benchmark suite [2], and Maxflow <ref> [1] </ref>. As we mentioned earlier (see Section 2), by definition the z-machine will not have any write stall or buffer flush times.
Reference: [2] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: The applications we studied include Cholesky and Nbody from SPLASH 5 suite [9], Integer Sort from the NAS parallel benchmark suite <ref> [2] </ref>, and Maxflow [1]. As we mentioned earlier (see Section 2), by definition the z-machine will not have any write stall or buffer flush times.
Reference: [3] <author> M. E. Crovella and T. J. LeBlanc. </author> <title> Parallel Performance Prediction Using Lost Cycles Analysis. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: It is generally recognized that no one technique is universally applicable for reducing or tolerating the communication overheads in all situations [6]. There have been several recent studies in separating the overheads seen in the execution of an application on a parallel architecture <ref> [3, 10] </ref>. These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category.
Reference: [4] <author> F. Dahlgren and P. Stenstrom. </author> <title> Using write caches to improve performance of cache coherence protocols in shared memory multiprocessors. </title> <type> Technical report, </type> <institution> Dept. of Comp. Eng., Lund Univ., </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: To reduce the number of messages on the network we assume an additional merge buffer at each node that combines writes to the same cache line. While it has been shown <ref> [4] </ref> that the merge buffer is effective in reducing the number of messages, it does introduce additional stall time for flushing the merge buffer at synchronization points for guaranteeing the correctness of the protocol.
Reference: [5] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. L. Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: In our work we consider the following four (RCinv, RCupd, RCadapt, RCcomp) memory systems that are built on top of the base hardware by specifying a particular coherence protocol along with a memory model. RCinv: The memory system uses the release consistent (RC) memory model <ref> [5] </ref> and a Berkeley-style write-invalidate protocol. In this system, a processor write that misses in the cache is simply recorded in the write-buffer and the processor continues execution without stalling.
Reference: [6] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W-D. Weber. </author> <title> Comparative evaluation of latency reducing and tolerating techniques. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 254-263, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: In general the goal of all such techniques is to make the parallel machine appear as a zero overhead machine from the point of view of an application. It is generally recognized that no one technique is universally applicable for reducing or tolerating the communication overheads in all situations <ref> [6] </ref>. There have been several recent studies in separating the overheads seen in the execution of an application on a parallel architecture [3, 10]. These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category.
Reference: [7] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W-D Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam. </author> <title> The Stanford DASH multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Each node has a piece of the shared memory with its associated full-mapped directory information, a private cache, and a write buffer (not unlike the Dash multiprocessor <ref> [7] </ref>). In our work we consider the following four (RCinv, RCupd, RCadapt, RCcomp) memory systems that are built on top of the base hardware by specifying a particular coherence protocol along with a memory model.
Reference: [8] <author> U. Ramachandran, G. Shah, A. Sivasubramaniam, A. Singla, and I. Yanasak. </author> <title> Architectural mechanisms for explicit communication in shared memory multiprocessors. </title> <type> Technical Report GIT-CC-94/59, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The protocol used in this memory system was developed for software management of coherent caches through the use of explicit communication primitives <ref> [8] </ref>. The directory controller keeps state information for sending updates to the active set of sharers through the selective-write primitive.
Reference: [9] <author> J. P. Singh, W-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year> <month> 9 </month>
Reference-contexts: The applications we studied include Cholesky and Nbody from SPLASH 5 suite <ref> [9] </ref>, Integer Sort from the NAS parallel benchmark suite [2], and Maxflow [1]. As we mentioned earlier (see Section 2), by definition the z-machine will not have any write stall or buffer flush times.
Reference: [10] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1994 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: It is generally recognized that no one technique is universally applicable for reducing or tolerating the communication overheads in all situations [6]. There have been several recent studies in separating the overheads seen in the execution of an application on a parallel architecture <ref> [3, 10] </ref>. These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category. <p> These studies shed important light on categorizing the sources of overhead, and the relative advantage of a particular technique in reducing a particular overhead category. For example, Sivasubramaniam et al. <ref> [10] </ref> break-down the overheads into algorithmic (i.e. inherent in the application such as serial component), and interaction (i.e. due to the communication and system overheads seen by the application when mapped onto a given architecture). All the techniques for latency reduction and tolerance attempt to shave off this interaction overhead. <p> The quest for a machine model which has zero communication overhead from the point of view of an application is the goal of this work. PRAM [13] has been used quite successfully as a vehicle for parallel algorithm design. In <ref> [10] </ref>, it is shown how PRAM could be used a vehicle for determining the algorithmic overhead in an application as well by using the PRAM in an execution-driven framework. Unfortunately, the PRAM model is too unrealistic for performance estimation since the model assigns unit cost for all memory accesses. <p> However, we want to incorporate it into an execution-driven simulator so that we can benchmark application performance on different memory systems with reference to the z-machine. For this purpose, we have simulated the z-machine within the SPASM framework <ref> [10, 11] </ref> an execution-driven parallel architecture simulator. The simulated shared memory architecture is CC-NUMA using a directory-based cache coherence strategy. The z-machine assumes that each producer knows the set of consumers for a data item. This is simulated by sending updates to all the processors on writes.
Reference: [11] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 411-426, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: However, we want to incorporate it into an execution-driven simulator so that we can benchmark application performance on different memory systems with reference to the z-machine. For this purpose, we have simulated the z-machine within the SPASM framework <ref> [10, 11] </ref> an execution-driven parallel architecture simulator. The simulated shared memory architecture is CC-NUMA using a directory-based cache coherence strategy. The z-machine assumes that each producer knows the set of consumers for a data item. This is simulated by sending updates to all the processors on writes.
Reference: [12] <author> C. P. Thacker and L. C. Stewart. Firefly: </author> <title> A Multiprocessor Workstation. </title> <booktitle> In Proceedings of the First International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 164-172, </pages> <month> October </month> <year> 1987. </year>
Reference-contexts: RCupd: This memory system uses RC memory model, a simple write-update protocol similar to the one used in the Firefly multiprocessor <ref> [12] </ref>. From the point of view of the processor, writes are handled exactly as in RCinv. However, we expect a higher write stall time for this memory system compared to RCinv due to the larger number of messages warranted by update schemes.
Reference: [13] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <address> Ithaca, NY, </address> <year> 1979. </year> <month> 10 </month>
Reference-contexts: The quest for a machine model which has zero communication overhead from the point of view of an application is the goal of this work. PRAM <ref> [13] </ref> has been used quite successfully as a vehicle for parallel algorithm design. In [10], it is shown how PRAM could be used a vehicle for determining the algorithmic overhead in an application as well by using the PRAM in an execution-driven framework.
References-found: 13

