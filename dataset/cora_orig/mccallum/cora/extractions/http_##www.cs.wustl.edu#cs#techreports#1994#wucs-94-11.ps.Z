URL: http://www.cs.wustl.edu/cs/techreports/1994/wucs-94-11.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: Catching Up With the Networks: Host I/O at Gigabit Rates  
Note: This work was supported in part by the Advanced Research Projects Agency (ARPA), the National Science Foundation (NSF), and an industrial consortium of Ascom Timeplex, Bell Communications Research, Bell Northern Research, Goldstar Information and Communication, NTT, Italtel SIT, NEC America, Southwest ern Bell, SynOptics Communications, and Tektronix.  
Abstract: Zubin D. Dittia Jerome R. Cox, Jr. Guru M. Parulkar zubin@dworkin.wustl.edu jrc@hobbs.wustl.edu guru@flora.wustl.edu Technical Report WUCS-94-11 Department of Computer Science Washington University in St. Louis St. Louis, MO 63130. Abstract The last few years have seen network data rates skyrocket from a few Mbps to a Gbps or more. However, a lack of integration of the host-network interface, the operating system, and network protocols has resulted in end-applications seeing only a small fraction of this total bandwidth being available for data transfer. The emergence of demanding applications in the realms of multimedia and virtual reality provides further impetus in the drive to overcome this problem. In this paper, we present the design of a high performance ATM host-network interface for workstations and servers that can support a bidirectional sustained data rate in excess of a gigabit per second. A prototype of the interface is being built at Washington University as part of an ARPA-sponsored gigabit local ATM testbed. Our interface design, which emphasizes seamless integration with the OS and network protocols, features: support for streaming data from I/O devices (e.g., cameras, disk arrays, etc.) to the network or vice-versa, as well as from device-to-device, while bypassing the main system bus; an ATM interconnect that extends to the desk-area; a zero-copy interface to system memory that is achieved through the use of page remapping techniques; full AAL-5 segmentation and reassembly; pacing control that provides for single-parameter bandwidth reservation; a high degree of scalability in terms of the number of I/O devices that can be simultaneously supported; low-cost (one ASIC); and multiprocessor support. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> An ATM PHY Data Path Interface: </institution> <note> Draft Version 0.5, an ATM Forum Document Draft, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: Further, by choosing a standard physical interconnection between APIC chips, we can make the interconnect long. For example, our prototype design uses the UTOPIA 4 standard <ref> [1] </ref> for connecting two APIC chips within a single box. In going from one box to another within the desk-area, we would switch to an optical medium (622 Mbps SONET). An off-the-shelf chipset can be used to translate from electronic (UTOPIA) to optical (SONET) medium, and vice-versa.
Reference: [2] <author> Arnould, </author> <title> Emmanuel et al; The Design of Nectar: A Network Backplane for Heterogeneous Multi-computers, </title> <journal> ASPLOS-III (ACM SIGOPS Operating Systems Review), </journal> <volume> Vol. </volume> <pages> 23, </pages> <address> New York, </address> <month> April </month> <year> 1989, </year> <pages> pp. 205-216. </pages>
Reference-contexts: The Nectar communication accelerator board (CAB) <ref> [2] </ref> is a host-network interface that connects through a 10 Mbyte/sec VME interface to the host system. The CABs on-board processor runs a small kernel and is responsible for transport protocol processing.
Reference: [3] <author> Banks, David; and Prudence, </author> <title> Michael; A High-Performance Network Architecture for a PA-RISC Workstation, </title> <journal> IEEE JSAC, </journal> <volume> Vol. 11 No. 2, </volume> <month> February </month> <year> 1993, </year> <pages> pp. 191-202. </pages>
Reference-contexts: Once it is determined where the packet is to be copied (into the applications address space), the copying is done by the CPU. Checksums can be computed in this copy loop. This interface design is highly flexible, scales easily, and can deliver high performance. The Medusa interface <ref> [3] </ref> is nothing but an Afterburner designed for a 100 Mbps FDDI network. Washington Universitys Axon project [14, 15] also represents an attempt at designing a high performance host communication architecture for high bandwidth distributed applications. <p> The VRAM serves only as a reassembly buffer; once a complete frame has been reassem-bled, the CPU can copy it over from the VRAM into the main memory. A similar approach has been used in the Afterburner [7] and Medusa <ref> [3] </ref> interface designs. If this approach is used, and given that DRAMs usually have cycle times in excess of a 100ns, the extra copy step involving two DRAMs (one is the main memory, the other is within the VRAM) can adversely affect performance.
Reference: [4] <author> Broadband Switching: </author> <title> architectures, protocols, design, and analysis, edited by Chris Dhas, </title> <editor> Vijaya K. Konangi, and M. Sreetharan, </editor> <publisher> IEEE Computer Society Press, </publisher> <year> 1991. </year>
Reference-contexts: Finally, Section 7 provides some concluding remarks and outlines our future plans. 2 Motivation The research community has expended considerable effort toward the design of high speed networks and switching systems <ref> [4, 12] </ref>. Experiences from the five gigabit testbeds have shown that gigabit networking is not only possible, but can be very useful in a wide variety of application areas.
Reference: [5] <author> Cheriton, David R.; VMTP: </author> <title> A Transport Protocol for the Next Generation of Computer Systems, </title> <booktitle> Proc. ACM SIGCOMM 86, </booktitle> <volume> Vol. 16 No. 3, </volume> <year> 1986, </year> <pages> pp. 406-415. </pages>
Reference-contexts: One of the earliest efforts include the network adaptor board (NAB) [11], which was especially designed to support the VMTP transport protocol <ref> [5] </ref>.
Reference: [6] <author> Cox Jr., Jerome R.; Gaddis, M.; and Turner, Jonathan S.; </author> <title> Project Zeus: Design of a Broadband Network and its Application on a University Campus, </title> <journal> IEEE Network, </journal> <volume> Vol. 7 No. 2, </volume> <month> March </month> <year> 1993, </year> <pages> pp. 20-30. </pages>
Reference-contexts: Experiences from the five gigabit testbeds have shown that gigabit networking is not only possible, but can be very useful in a wide variety of application areas. At Washington University, we have an installed and operational 155 Mbps packet switched network <ref> [6] </ref>, and we are currently in the process of building an ATM switching system [18] that will support data rates of 622 Mbps (SONET) and 2.4 Gbps (G-link) per port.
Reference: [7] <author> Dalton, C.; Watson, G.; Banks, D.; Calamvokis, C.; Edwards, A.; and Lumley, J.; </author> <title> Afterburner, </title> <journal> IEEE Network, </journal> <volume> Vol. 7 No. 4, </volume> <month> July </month> <year> 1993, </year> <pages> pp. 36-43. </pages>
Reference-contexts: We believe this approach affords maximum flexibility without compromising efficiency. The Afterburner <ref> [7] </ref> interface design uses a VRAM to hold packets that have been received over the network. The VRAM is mapped into the host kernels address space, so protocols can examine headers while the packets are still in the VRAM. <p> The VRAM serves only as a reassembly buffer; once a complete frame has been reassem-bled, the CPU can copy it over from the VRAM into the main memory. A similar approach has been used in the Afterburner <ref> [7] </ref> and Medusa [3] interface designs. If this approach is used, and given that DRAMs usually have cycle times in excess of a 100ns, the extra copy step involving two DRAMs (one is the main memory, the other is within the VRAM) can adversely affect performance.
Reference: [8] <author> Davie, Bruce S.; </author> <title> The Architecture and Implementation of a High-Speed Host Interface, </title> <journal> IEEE JSAC, </journal> <volume> Vol. 11 No. 2, </volume> <month> February </month> <year> 1993, </year> <pages> pp. 228-239. </pages>
Reference-contexts: The unique feature of this interface is that an extra store-and-forward hop can be avoided by mapping the CABs on-board data memory into the address space of the node process. Some of the more recent efforts have been concerned with the design of host-network interfaces for ATM networks <ref> [8, 17] </ref>. The Bellcore Interface [8] was designed for the DecStation 5000, and it connects to the hosts TURBOchannel I/O bus on one side and to a 622 Mbps SONET STS-12 link on the other. This interface places all data movement and per-cell operations in custom hardware. <p> Some of the more recent efforts have been concerned with the design of host-network interfaces for ATM networks [8, 17]. The Bellcore Interface <ref> [8] </ref> was designed for the DecStation 5000, and it connects to the hosts TURBOchannel I/O bus on one side and to a 622 Mbps SONET STS-12 link on the other. This interface places all data movement and per-cell operations in custom hardware.
Reference: [9] <author> Druschel, Peter; and Peterson, Larry L.; Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility, </title> <booktitle> Proc. 14th Symposium on Operating System Principles, </booktitle> <address> Asheville, NC, </address> <month> Dec. </month> <year> 1993, </year> <pages> pp. 189-202. </pages>
Reference-contexts: There have been some recent research efforts that propose the use of an abstract data type layered above one or more noncontiguous buffers which would enable applications to view these buffers as a single aggregate contiguous buffer (see Figure 2 and Section 5.2 in <ref> [9] </ref>). While such an approach can be used to build up a very flexible (and high performance) I/O buffer management facility, we believe that it may not be suitable to many applications. <p> In particular, an application would find it very difficult to map multidimensional arrays or a complex data structure onto the aggregate abstraction introduced in <ref> [9] </ref>, and would usually be forced to copy the data into contiguous space thus compromising performance. In our approach, an ADU does appear contiguous in the applications address space. However, we believe that a scheme similar to the fbuf scheme (introduced in the same paper cited above, [9]) is crucial in <p> abstraction introduced in <ref> [9] </ref>, and would usually be forced to copy the data into contiguous space thus compromising performance. In our approach, an ADU does appear contiguous in the applications address space. However, we believe that a scheme similar to the fbuf scheme (introduced in the same paper cited above, [9]) is crucial in order to achieve high performance for subsequent cross-domain data transfers. 3. The interface should support zero-copy semantics. A zero-copy interface is defined as one where data moves directly from an applications address space to the network, or vice-versa.
Reference: [10] <editor> IEEE Journal on Selected Areas in Communications, </editor> <volume> Vol. 11 No. 2, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: is a need to revise the host communication architecture and I/O subsystem, if we are to be able to meet the demands imposed by our choice of target applications. 3 Related Work Several research groups have attempted to design and implement high speed host-network interfaces over the past few years <ref> [10] </ref>. One of the earliest efforts include the network adaptor board (NAB) [11], which was especially designed to support the VMTP transport protocol [5].
Reference: [11] <author> Kanakia, Hemant; and Cheriton, David R.; </author> <title> The VMP Network Adapter Board (NAB): High Performance Network Communication for Multiprocessors, </title> <booktitle> Proc. ACM SIGCOMM 88, </booktitle> <volume> Vol. 18 No. 4, </volume> <year> 1988, </year> <pages> pp. 175-187. </pages>
Reference-contexts: One of the earliest efforts include the network adaptor board (NAB) <ref> [11] </ref>, which was especially designed to support the VMTP transport protocol [5].
Reference: [12] <author> Partridge, </author> <title> Craig; Gigabit Networking, </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Finally, Section 7 provides some concluding remarks and outlines our future plans. 2 Motivation The research community has expended considerable effort toward the design of high speed networks and switching systems <ref> [4, 12] </ref>. Experiences from the five gigabit testbeds have shown that gigabit networking is not only possible, but can be very useful in a wide variety of application areas. <p> The above goals helped isolate the following properties that the interface needs to have: 1. All protocol processing above the ATM adaptation layer that did not involve touching data should be done in software. Recent results for TCP, based on work done by Van Jacobson (see page 240 in <ref> [12] </ref>), have shown that the fixed cost (not including costs for handling data) of transport level protocol processing can be as low as 150 RISC instructions per packet, for both sending and receiving.
Reference: [13] <author> SPARC MBUS Interface Specification: </author> <title> Revision 1.2, </title> <booktitle> SPARC International, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: Our choice did result in imposing a few restrictions on the design environment, as pointed out earlier. Based on some of these constraints, our final choice of target workstation was a SPARC 8 based architecture that uses a high performance circuit-switched system bus called the MBUS <ref> [13] </ref>. The MBUS interface is well specified and simple enough that our chosen approach can be successfully implemented.
Reference: [14] <author> Sterbenz, James; and Parulkar, Guru; Axon: </author> <title> Network Virtual Storage Design for High Performance Distributed Applications, </title> <booktitle> Proc. 10th IEEE Conf. on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1990. </year> <note> Host I/O at Gigabit Rates Page 26 </note>
Reference-contexts: Checksums can be computed in this copy loop. This interface design is highly flexible, scales easily, and can deliver high performance. The Medusa interface [3] is nothing but an Afterburner designed for a 100 Mbps FDDI network. Washington Universitys Axon project <ref> [14, 15] </ref> also represents an attempt at designing a high performance host communication architecture for high bandwidth distributed applications.
Reference: [15] <author> Sterbenz, James; and Parulkar, Guru; Axon: </author> <title> Host-Network Interface Architecture for Gigabit Communication, Protocols for High Speed Networking, </title> <publisher> Elsevier (North Holland), </publisher> <year> 1991. </year>
Reference-contexts: Checksums can be computed in this copy loop. This interface design is highly flexible, scales easily, and can deliver high performance. The Medusa interface [3] is nothing but an Afterburner designed for a 100 Mbps FDDI network. Washington Universitys Axon project <ref> [14, 15] </ref> also represents an attempt at designing a high performance host communication architecture for high bandwidth distributed applications.
Reference: [16] <author> Tennenhouse, D.; Telemedia, </author> <title> Networks, </title> <journal> and Systems: </journal> <note> Research Report, </note> <year> 1993. </year>
Reference-contexts: An architecture in which a generalized packet switched interconnect is used to connect processors, memories, and devices has widely come to be known as a desk area network (DAN). The concept of a DAN was first introduced by David Tennenhouse <ref> [16] </ref>. We believe that desk area networks hold promise for the future of computer architecture. Clearly, the APIC interconnect architecture itself falls into the class of DAN-based architectures.
Reference: [17] <author> Traw, C. Brendan S.; and Smith, Jonathan S.; </author> <title> Hardware/Software Organization of a High-Performance ATM Host Interface, </title> <journal> IEEE JSAC, </journal> <volume> Vol. 11 No. 2, </volume> <month> February </month> <year> 1993, </year> <pages> pp. 240-253. </pages>
Reference-contexts: The unique feature of this interface is that an extra store-and-forward hop can be avoided by mapping the CABs on-board data memory into the address space of the node process. Some of the more recent efforts have been concerned with the design of host-network interfaces for ATM networks <ref> [8, 17] </ref>. The Bellcore Interface [8] was designed for the DecStation 5000, and it connects to the hosts TURBOchannel I/O bus on one side and to a 622 Mbps SONET STS-12 link on the other. This interface places all data movement and per-cell operations in custom hardware. <p> This interface places all data movement and per-cell operations in custom hardware. Control path functions including segmentation and reassembly are implemented using two on-board processors, thus making the interface very flexible. The University of Pennsylvania interface <ref> [17] </ref> was designed for the IBM RS/6000, and it interfaces to a SONET OC-3c link (155 Mbps).
Reference: [18] <author> Turner, Jonathan S.; </author> <title> An Optimal Non-Blocking Multicast Virtual Circuit Switch, </title> <type> Technical Report WUCS-93-30, </type> <institution> Department of Computer Science, Washington University in St. Louis, MO, </institution> <year> 1993. </year> <note> Host I/O at Gigabit Rates Page 27 </note>
Reference-contexts: At Washington University, we have an installed and operational 155 Mbps packet switched network [6], and we are currently in the process of building an ATM switching system <ref> [18] </ref> that will support data rates of 622 Mbps (SONET) and 2.4 Gbps (G-link) per port.
References-found: 18

