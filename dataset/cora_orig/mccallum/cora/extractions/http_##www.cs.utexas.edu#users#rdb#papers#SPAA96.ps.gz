URL: http://www.cs.utexas.edu/users/rdb/papers/SPAA96.ps.gz
Refering-URL: http://www.cs.utexas.edu/users/cilk/papers.html
Root-URL: 
Email: rdb@cs.utexas.edu  fathena,cfj,cel,randallg@lcs.mit.edu  
Title: An Analysis of Dag-Consistent Distributed Shared-Memory Algorithms  
Author: Robert D. Blumofe Matteo Frigo Christopher F. Joerg Charles E. Leiserson Keith H. Randall 
Address: Austin, Texas 78712  545 Technology Square Cambridge, Massachusetts 02139  
Affiliation: Department of Computer Sciences The University of Texas at Austin  MIT Laboratory for Computer Science  
Abstract: In this paper, we analyze the performance of parallel multi-threaded algorithms that use dag-consistent distributed shared memory. Specifically, we analyze execution time, page faults, and space requirements for multithreaded algorithms executed by a work-stealing thread scheduler and the BACKER coherence algorithm for maintaining dag consistency. We prove that if the accesses to the backing store are random and independent (the BACKER algorithm actually uses hashing), then the expected execution time of a fully strict multithreaded computation on P processors, each with an LRU cache of C pages, is O(T 1 (C)=P + mCT ), where T 1 (C) is the total work of the computation including page faults, T is its critical-path length excluding page faults, and m is the minimum page transfer time. As a corollary to this theorem, we show that the expected number of page faults incurred by a computation executed on P processors, each with an LRU cache of C pages, is F 1 (C) + O(CPT ), where F 1 (C) is the number of serial page faults. Finally, we give simple bounds on the number of page faults and the space requirements for regular divide-and-conquer algorithms. We use these bounds to analyze parallel multithreaded algorithms for matrix multiplication and LU-decomposition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve and Mark D. Hill. </author> <title> Weak ordering anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <address> Seattle, Washington, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [2] <author> Mustaque Ahamad, Phillip W. Hutto, and Ranjit John. </author> <title> Implementing and programming causal distributed shared memory. </title> <booktitle> In Proceedings of the 11th International Conference on Distributed Computing systems, </booktitle> <pages> pages 274-281, </pages> <address> Arlington, Texas, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: Causal memory <ref> [2] </ref> ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B.
Reference: [3] <author> Monica Beltrametti, Kenneth Bobey, and John R. Zorbas. </author> <title> The control mechanism for the Myrias parallel computer system. </title> <journal> Computer Architecture News, </journal> <volume> 16(4) </volume> <pages> 21-30, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: By leveraging this high-level knowledge, the BACKER algorithm in conjunction with the work-stealing scheduler is able to execute multi-threaded algorithms with the performance bounds shown here. The BLAZE parallel language [25] and the Myrias parallel computer <ref> [3] </ref> define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems. After an extensive literature search, we are aware of no other distributed shared memory with analytical performance bounds for any nontrivial algorithms.
Reference: [4] <author> Brian N. Bershad, Matthew J. Zekauskas, and Wayne A. Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Digest of Papers from the Thirty-Eighth IEEE Computer Society International Conference (Spring COMPCON), </booktitle> <pages> pages 528-537, </pages> <address> San Francisco, California, </address> <month> February </month> <year> 1993. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models <ref> [4] </ref>, while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26]. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [5] <author> Guy E. Blelloch. </author> <title> Programming parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 39(3), </volume> <month> March </month> <year> 1996. </year>
Reference-contexts: Any multithreaded computation can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multi-threaded algorithm is used to solve a given input problem.
Reference: [6] <author> Guy E. Blelloch, Phillip B. Gibbons, and Yossi Matias. </author> <title> Provably efficient scheduling for languages with fine-grained parallelism. </title> <booktitle> In Proceedings of the Seventh Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 1-12, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: The computational work T 1 (n) to multiply n fi n matrices satisfies T 1 (n) = 8T 1 (n=2) + Q (n 2 ), since 4 In recent work, Blelloch, Gibbons, and Matias <ref> [6] </ref> have shown that series-parallel dag computations can be scheduled to achieve substantially better space bounds than we report here. For example, they give a bound of S P (n) = O (n 2 + Plg 2 n) for matrix multiplication.
Reference: [7] <author> Robert D. Blumofe. </author> <title> Executing Multithreaded Programs Efficiently. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized workstealing scheduler from <ref> [7, 10] </ref> and dag consistency is maintained by the BACKER coherence algorithm from [8]. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> Notice that a multithreaded algorithm does not specify at what time or on what processor any given instruction is executed. The resource requirements that a multithreaded algorithm employs to solve a given input problem are modeled, in graph-theoretic terms, by a multithreaded computation <ref> [7] </ref>. A multithreaded computation is composed of two structures: a spawn tree of threads and a dag of instructions. The spawn tree of threads is the parallel analogue of a call tree of procedures. <p> Specifically, for any such algorithm and any number P of processors, the randomized work-stealing scheduler executes the algorithm in expected time O (T 1 =P + T ) <ref> [7, 10] </ref>. The randomized workstealing scheduler operates as follows. Each processor maintains a ready deque (doubly-ended queue) of threads from which work is obtained. When a thread is spawned, the parent thread is suspended and put on the bottom of the deque and execution commences on the spawned child thread. <p> Finally, in this paper, we analyze the space requirements of simple multithreaded algorithms that use dag-consistent shared memory. We assume that the computation is scheduled by a sched-uler, such as the work-stealing algorithm, that maintains the busy-leaves property <ref> [7, 10] </ref>. For a given simple multithreaded algorithm, let S 1 denote the space required by the standard, depth-first serial execution of the algorithm to solve a given problem. <p> In previous work, we have shown that the space used by a P-processor execution is at most S 1 P in the worst case <ref> [7, 10] </ref>. We improve this characterization of the space requirements, and we provide a much stronger upper bound on the space requirements of regular divide-and-conquer multithreaded algorithms. <p> In addition, we bound the number of page faults. The exposition of the proofs in this section makes heavy use of results and techniques from <ref> [7, 10] </ref>. In the following analysis, we consider the fully strict multi-threaded computation that results when a given fully strict multi-threaded algorithm is executed to solve a given input problem. <p> When pages are transferred between processors, congestion may occur at a destination processor, in which case we assume that the transfers are serviced at the destination in FIFO (first-in, first-out) order. The workstealing scheduler assumed in our analysis is the work-stealing scheduler from <ref> [7, 10] </ref>, but with a small technical modification. Between successful steals, we wish to guarantee that a processor performs at least C page transfers (fetches or reconciles) so that it does not steal too often. <p> A busy-leaves scheduler is a scheduler with the property that at all times during the execution, if a thread has no living children, then that thread has a processor working on it. The workstealing scheduler is a busy-leaves scheduler <ref> [7, 10] </ref>. We shall proceed through a series of lemmas that provide an exact characterization of the space used by simple mul-tithreaded algorithms when executed by a busy-leaves scheduler. <p> We shall then specialize this characterization to provide space bounds for regular divide-and-conquer algorithms. Previous work <ref> [7, 10] </ref> has shown that a busy-leaves scheduler can efficiently execute a fully strict multithreaded algorithm on P processors using no more space than P times the space required to execute the algorithm on a single processor. <p> We anticipate that it should be possible to memory-map files and use our existing dag-consistency mechanisms to provide a parallel, asynchronous I/O capability for Cilk. We are also currently working on supporting dag-consistent shared memory in our Cilk-NOW runtime system <ref> [7] </ref> which executes Cilk programs in an adaptively parallel and fault-tolerant manner on networks of workstations. We expect that the well-structured nature of Cilk computations will allow the runtime system to maintain dag consistency efficiently, even in the presence of processor faults.
Reference: [8] <author> Robert D. Blumofe, Matteo Frigo, Christopher F. Joerg, Charles E. Leiserson, and Keith H. Randall. </author> <title> Dag-consistent distributed shared memory. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <address> Honolulu, Hawaii, </address> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In recent work <ref> [8, 17] </ref>, we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system [7, 9, 17]. <p> In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized workstealing scheduler from [7, 10] and dag consistency is maintained by the BACKER coherence algorithm from <ref> [8] </ref>. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> The BACKER coherence algorithm and the work-stealing scheduler have been implemented in the Cilk runtime system with encouraging empirical results <ref> [8] </ref>. In order to model performance for multithreaded algorithms that use dag-consistent shared memory, we observe that running times will vary as a function of the cache size C, so we must introduce measures that account for this dependence. <p> Finally, Section 7 offers some comparisons with other consistency models and some ideas for the future. 2 Dag consistency and the Backer coherence algorithm In this section we give a precise definition of dag consistency, and we describe the BACKER <ref> [8] </ref> coherence algorithm for maintaining dag consistency. Dag consistency is a relaxed consistency model for distributed shared memory, and the BACKER algorithm can maintain dag consistency for multithreaded computations that execute on a parallel computer with physically distributed memory. <p> Instruction w can still have a different viewpoint on x than v. For instance, instruction w may see a write on x performed by some other instruction (such as s and t in the figure) that is incomparable with v. In previous work <ref> [8, 17] </ref>, we presented a weaker definition of dag consistency from Definition 1. <p> A more detailed justification of Definition 1 and an explanation of its properties are beyond the scope of this paper, but we are currently exploring the semantics of dag consistency more fully. We now describe the BACKER coherence algorithm from <ref> [8] </ref>, in which versions of shared-memory objects can reside simultaneously in any of the processor caches and the backing store. Each processor's cache contains objects recently used by the threads that have executed on that processor, and the backing store provides default global storage for each object. <p> Similarly, each secondary transfer during an execution can be associated with a currently running secondary subcomputa-tion such that each secondary subcomputation has at most 3C associated secondary transfers. Proof: For this proof, we use a fact shown in <ref> [8] </ref> that executing a subcomputation starting with an arbitrary cache can only incur C more page faults than the same block of code incurred in the serial execution. <p> Consequently, the number of serial page faults is F 1 (C; n) = (n= p m) 3 = n 3 =m 3=2 , even if we assume that A and R never fault. The divide-and-conquer matrixmul algorithm from <ref> [8] </ref> uses the processor cache much more effectively.
Reference: [9] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An efficient multithreaded runtime system. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 207-216, </pages> <address> Santa Barbara, California, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> Any multithreaded computation can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multi-threaded algorithm is used to solve a given input problem.
Reference: [10] <author> Robert D. Blumofe and Charles E. Leiserson. </author> <title> Scheduling multi-threaded computations by work stealing. </title> <booktitle> In Proceedings of the 35th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 356-368, </pages> <address> Santa Fe, New Mexico, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In this paper, we analyze the execution time, page faults, and space requirements of multithreaded algorithms written with this consistency model when the execution is scheduled by the randomized workstealing scheduler from <ref> [7, 10] </ref> and dag consistency is maintained by the BACKER coherence algorithm from [8]. A multithreaded algorithm is a collection of thread definitions. Analogous to a procedure definition, a thread definition is a block of serial code, possibly with conditional and looping constructions. <p> Any multithreaded computation can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multi-threaded algorithm is used to solve a given input problem. <p> Specifically, for any such algorithm and any number P of processors, the randomized work-stealing scheduler executes the algorithm in expected time O (T 1 =P + T ) <ref> [7, 10] </ref>. The randomized workstealing scheduler operates as follows. Each processor maintains a ready deque (doubly-ended queue) of threads from which work is obtained. When a thread is spawned, the parent thread is suspended and put on the bottom of the deque and execution commences on the spawned child thread. <p> Finally, in this paper, we analyze the space requirements of simple multithreaded algorithms that use dag-consistent shared memory. We assume that the computation is scheduled by a sched-uler, such as the work-stealing algorithm, that maintains the busy-leaves property <ref> [7, 10] </ref>. For a given simple multithreaded algorithm, let S 1 denote the space required by the standard, depth-first serial execution of the algorithm to solve a given problem. <p> In previous work, we have shown that the space used by a P-processor execution is at most S 1 P in the worst case <ref> [7, 10] </ref>. We improve this characterization of the space requirements, and we provide a much stronger upper bound on the space requirements of regular divide-and-conquer multithreaded algorithms. <p> In addition, we bound the number of page faults. The exposition of the proofs in this section makes heavy use of results and techniques from <ref> [7, 10] </ref>. In the following analysis, we consider the fully strict multi-threaded computation that results when a given fully strict multi-threaded algorithm is executed to solve a given input problem. <p> When pages are transferred between processors, congestion may occur at a destination processor, in which case we assume that the transfers are serviced at the destination in FIFO (first-in, first-out) order. The workstealing scheduler assumed in our analysis is the work-stealing scheduler from <ref> [7, 10] </ref>, but with a small technical modification. Between successful steals, we wish to guarantee that a processor performs at least C page transfers (fetches or reconciles) so that it does not steal too often. <p> We shall show that with probability at least 1 e, an execution contains only O (T + lg (1=e)) rounds. To bound the number of rounds, we shall use a delay-sequence argument. We define a modified dag D 0 exactly as in <ref> [10] </ref>. (The dag D 0 is for the purposes of analysis only and has no effect on the computation.) The critical-path length of D 0 is at most 2T . <p> Since there are at least 4P steal requests during the first 22CP events, the probability is at least 1=2 that any task that is critical at the beginning of a round is the target of a steal request <ref> [10, Lemma 10] </ref>, if it is not executed locally by the processor on which it resides. Any task takes at most 3mC + 1 4mC time to execute, since we are ignoring the effects of congestion for the moment. <p> We want to show that with probability at least 1 e, the total number of rounds is O (T + lg (1=e)). Consider a possible delay sequence. Recall from <ref> [10] </ref> that a delay sequence of size R is a maximal path U in the augmented dag D 0 of length at most 2T , along with a partition P of R which represents the number of rounds during which each task of the path in D 0 is critical. <p> Then for any e &gt; 0, the execution time is O (T 1 (C)=P + mCT + m lg P + mC lg (1=e)) with probability at least 1 e. Moreover, the expected execution time is O (T 1 (C)=P + mCT ). Proof: As in <ref> [10] </ref>, we shall use an accounting argument to bound the running time. During the execution, at each time step, each pro cessor puts a dollar into one of 5 buckets according to its activity at that time step. <p> A busy-leaves scheduler is a scheduler with the property that at all times during the execution, if a thread has no living children, then that thread has a processor working on it. The workstealing scheduler is a busy-leaves scheduler <ref> [7, 10] </ref>. We shall proceed through a series of lemmas that provide an exact characterization of the space used by simple mul-tithreaded algorithms when executed by a busy-leaves scheduler. <p> We shall then specialize this characterization to provide space bounds for regular divide-and-conquer algorithms. Previous work <ref> [7, 10] </ref> has shown that a busy-leaves scheduler can efficiently execute a fully strict multithreaded algorithm on P processors using no more space than P times the space required to execute the algorithm on a single processor.
Reference: [11] <author> John B. Carter, John K. Bennett, and Willy Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <address> Pacific Grove, California, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [12] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Consequently, we obtain the recurrence F 1 (C; n) aF 1 (C; n=b) + p (n) if n &gt; n C ; (1) We can solve this recurrence using standard techniques <ref> [12, Section 4.4] </ref>. We iterate the recurrence, stopping as soon as we reach the first value of the iteration count k such that n=b k n C holds, or equivalently when k = dlog b (n=n C )e holds. <p> The solution to this recurrence <ref> [12, Section 4.4] </ref> is * S 1 (n) = Q (lg k+1 n), if s (n) = Q (lg k n) for some constant k 0, and * S 1 (n) = Q (s (n)), if s (n) = W (n e ) for some constant e &gt; 0 and in
Reference: [13] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [14] <author> Guang R. Gao and Vivek Sarkar. </author> <title> Location consistency: Stepping beyond the barriers of memory coherence and serializability. </title> <type> Technical Report 78, </type> <institution> McGill University, School of Computer Science, Advanced Compilers, Architectures, and Parallel Systems (ACAPS) Laboratory, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Shared memory consists of a set of objects that instructions can read and write. When an instruction performs a read of an object, it receives some value, but the particular value it receives depends upon the consistency model. Like location consistency <ref> [14] </ref>, dag consistency is defined separately for each object in shared memory. In order to define dag consistency precisely, we need some terminology. Let G = (V; E) be the dag of a multithreaded computation. <p> Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency <ref> [14] </ref> and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [15] <author> Kourosh Gharachorloo, Daniel Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <address> Seattle, Washington, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency <ref> [1, 13, 15] </ref>, ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition or release of a lock.
Reference: [16] <author> James R. Goodman. </author> <title> Cache consistency and sequential consistency. </title> <type> Technical Report 61, </type> <institution> IEEE Scalable Coherent Interface (SCI) Working Group, </institution> <month> March </month> <year> 1989. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency [22] and various forms of processor consistency <ref> [16] </ref> are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit synchronization operations occur, such as the acquisition
Reference: [17] <author> Christopher F. Joerg. </author> <title> The Cilk System for Parallel Multithreaded Computing. </title> <type> PhD thesis, </type> <institution> Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In recent work <ref> [8, 17] </ref>, we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system [7, 9, 17]. <p> 1 Introduction In recent work [8, 17], we have proposed dag-consistent distributed shared memory as a virtual-memory model for multi-threaded parallel-programming systems such as Cilk, a C-based multithreaded language and runtime system <ref> [7, 9, 17] </ref>. A multi-threaded program defines a partial execution order on its instructions, and we view this partial order as a directed acyclic graph or dag. <p> Instruction w can still have a different viewpoint on x than v. For instance, instruction w may see a write on x performed by some other instruction (such as s and t in the figure) that is incomparable with v. In previous work <ref> [8, 17] </ref>, we presented a weaker definition of dag consistency from Definition 1.
Reference: [18] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-performance all-software distributed shared memory. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 213-228, </pages> <address> Copper Mountain Resort, Colorado, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [19] <author> Edward G. Coffman Jr. and Peter J. Denning. </author> <title> Operating Systems Theory. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1973. </year>
Reference-contexts: We shall further assume that pages in the cache are maintained using the popular LRU (least-recently-used) <ref> [19] </ref> heuristic. In addition to servicing page faults, BACKER must reconcile pages between the processor page caches and the backing store so that the semantics of the execution obey the assumptions of dag consistency.
Reference: [20] <author> Richard M. Karp and Vijaya Ramachandran. </author> <title> Parallel algorithms for shared-memory machines. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer ScienceVolume A: Algorithms and Complexity, chapter 17, </booktitle> <pages> pages 869-941. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Any multithreaded computation can be measured in terms of its work and critical-path length <ref> [5, 9, 10, 20] </ref>. Consider the multithreaded computation that results when a given multi-threaded algorithm is used to solve a given input problem.
Reference: [21] <author> Pete Keleher, Alan L. Cox, Sandhya Dwarkadas, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed shared memory on standard workstations and operating systems. </title> <booktitle> In USENIX Winter 1994 Conference Proceedings, </booktitle> <pages> pages 115-132, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [22] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: Relaxed shared-memory consistency models are motivated by the fact that sequential consistency <ref> [22] </ref> and various forms of processor consistency [16] are too expensive to implement in a distributed setting. (Even modern symmetric multiprocessors do not typically implement sequential consistency.) Relaxed models, such as location consistency [14] and various forms of release consistency [1, 13, 15], ensure consistency (to varying degrees) only when explicit
Reference: [23] <author> James R. Larus, Brad Richards, and Guhan Viswanathan. </author> <title> LCM: Memory system support for parallel language implementation. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 208-218, </pages> <address> San Jose, California, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [23, 26] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [24] <author> F. Thomson Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays Trees Hypercubes. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>
Reference-contexts: Multiplying two n fi n matrices (using the ordinary algorithm, and not a variant of Strassen's algorithm [28]) can be performed using Q (n 3 ) work and can be done in Q (lgn) time <ref> [24] </ref>. Thus, for a problem of size n, we have computational work T 1 (n) = Q (n 3 ) and critical-path length T (n) = Q (lg n).
Reference: [25] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> The BLAZE language: A parallel language for scientific programming. </title> <journal> Parallel Computing, </journal> <volume> 5 </volume> <pages> 339-361, </pages> <year> 1987. </year>
Reference-contexts: By leveraging this high-level knowledge, the BACKER algorithm in conjunction with the work-stealing scheduler is able to execute multi-threaded algorithms with the performance bounds shown here. The BLAZE parallel language <ref> [25] </ref> and the Myrias parallel computer [3] define a high-level relaxed consistency model much like dag consistency, but we do not know of any efficient implementation of either of these systems.
Reference: [26] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336, </pages> <address> Chicago, Illinois, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: Most DSM's implement one of these relaxed consistency models [11, 18, 21, 27], though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies <ref> [23, 26] </ref>. All of these consistency models and the DSM's that implement these models take a low-level view of a parallel program as a collection of cooperating processes.
Reference: [27] <author> Daniel J. Scales and Monica S. Lam. </author> <title> The design and evaluation of a shared object system for distributed memory machines. </title> <booktitle> In Proceedings of the First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 101-114, </pages> <address> Monterey, California, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: Causal memory [2] ensures consistency only to the extent that if a process A reads a value written by another process B, then all subsequent operations by A must appear to occur after the write by B. Most DSM's implement one of these relaxed consistency models <ref> [11, 18, 21, 27] </ref>, though some implement a fixed collection of consistency models [4], while others merely implement a collection of mechanisms on top of which users write their own DSM consistency policies [23, 26].
Reference: [28] <author> Volker Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numerische Mathematik, </journal> <volume> 14(3) </volume> <pages> 354-356, </pages> <year> 1969. </year> <month> 12 </month>
Reference-contexts: Multiplying two n fi n matrices (using the ordinary algorithm, and not a variant of Strassen's algorithm <ref> [28] </ref>) can be performed using Q (n 3 ) work and can be done in Q (lgn) time [24]. Thus, for a problem of size n, we have computational work T 1 (n) = Q (n 3 ) and critical-path length T (n) = Q (lg n).
References-found: 28

