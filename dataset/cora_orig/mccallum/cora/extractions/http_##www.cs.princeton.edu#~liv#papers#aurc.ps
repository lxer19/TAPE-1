URL: http://www.cs.princeton.edu/~liv/papers/aurc.ps
Refering-URL: http://www.cs.princeton.edu/~liv/aurc.html
Root-URL: http://www.cs.princeton.edu
Email: iftode@cs.rutgers.edu  
Title: Shared Virtual Memory with Automatic Update Support  validating the implementation as achieving its goals.  
Author: Liviu Iftode Matthias Blumrich, Cezary Dubnicki, David L. Oppenheimer, Jaswinder Pal Singh and Kai Li 
Note: tation,  
Address: Piscataway, NJ 08855  Princeton, NJ 08544  
Affiliation: Department of Computer Science Rutgers University  Department of Computer Science Princeton University  
Abstract: Shared virtual memory systems provide the abstraction of a shared address space on top of a message-passing communication architecture. The overall performance of an SVM system therefore depends on both the raw performance of the underlying communication mechanism and the efficiency with which the SVM protocol uses that mechanism. The Automatic Update Release Consistency (AURC) protocol was proposed to take advantage of simple memory-mapped communication and automatic update support to accelerate a shared virtual memory protocol. However, there has not yet been a real system on which an implementation of this protocol could be evaluated. This paper reports our evaluation of AURC on the SHRIMP multicomputer, the only hardware platform that supports an automatic update mechanism. Automatic update propagates local memory writes to remote memory locations automatically. We compare the AURC protocol with its all-software counterpart protocol, Home-based Lazy Release Consistency (HLRC). We first show that virtual memory-mapped communication itself helps all home-based LRC protocols because these protocols can easily take advantage of the zero-copy communication allowed by the memory mapping mechanism. By integrating AU support into the protocol as well, the AURC protocol can further improve performance. For applications with write-write false sharing, an AU-based multiple-writer protocol can significantly outperform an all-software home-based multiple-writer LRC protocol that uses diffs. For applications without much write-write false sharing, the two protocols perform similarly. Our results also show that write-through caching and automatic update traffic does not perturb the compu 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. V. Adve, A. L. Cox, S. Dwarkadas, R. Rajamony, and W. Zwaenepoel. </author> <title> A Comparison of Entry Consistency and Lazy Release Consistency Implementation. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [11]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7]. Lazy Release Consistency (LRC) <ref> [21, 8, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [20] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [2] <author> C. Amza, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. </author> <title> Software DSM Protocols that Adapt between Single Writer and Multiple Writer. </title> <booktitle> In The 3rd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <year> 1997. </year>
Reference-contexts: The best case is when the sharing pattern is migratory; then diffs are totally ordered and can be fetched in one message from the last writer. Without using an adaptive scheme of the type recently proposed <ref> [2, 19] </ref>, the sizes of the messages increases during the migration due to diff accumulation (since a processor is fetching diffs, it fetches not only the diffs created by the last writer but also those created by previous writers in the intervals that it has not already seen).
Reference: [3] <institution> BCPR Services Inc. </institution> <note> EISA Specification, Version 3.12, </note> <year> 1992. </year>
Reference-contexts: Each PC uses an Intel Pentium Xpress motherboard [17] that holds a 66 Mhz Pentium CPU, 256 Kbytes of L2 cache, and 40 Mbytes of DRAM memory. Peripherals are connected to the system through the EISA expansion bus <ref> [3] </ref>. Main memory data can be cached by the CPU as write-through or write-back on a per-virtual-page basis, as specified in the process page tables.
Reference: [4] <author> R. Bianchini, L.I Kontothanassis, R. Pinto, M. De Maria, M. Abud, and C.L. Amorim. </author> <title> Hiding Communication Latency and Coherence Overhead in Software DSMs. </title> <booktitle> In The 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: For these reasons the comparison between Cashmere and TreadMarks cannot conclusively assess the performance benefit of zero-software-overhead, non-selective AU support as provided in the SHRIMP network interface. Bianchini et al. <ref> [4] </ref> proposed a dedicated protocol controller to o*oad some of the communication and coherence overheads from the computation processor. Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial.
Reference: [5] <author> R. Bisiani and M. Ravishankar. </author> <title> PLUS: A Distributed Shared-Memory System. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 115-124, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: this the first opportunity to evaluate the actual performance benefit of AURC for SVM on a real SHRIMP system. 3.2.3 Automatic-Update RC with Copyset-2 Previous research in hardware support for update-based shared memory protocols has suggested multicast support for a full-fledged update-based coherence scheme, with updates propagated in circular rings <ref> [18, 5] </ref>. As the SHRIMP hardware design philosophy emphasizes simplicity, we ruled out complicated features such as rings and multicast. <p> Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial. The PLUS <ref> [5] </ref>, Galactica Net [18], Merlin [24] and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [6] <author> M. Blumrich, K. Li, R. Alpert, C. Dubnicki, E. Felten, and J. Sandberg. </author> <title> A Virtual Memory Mapped Network Interface for the SHRIMP Multicomputer. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 142-153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In recent years, industrial and academic researchers have focused a great deal of work on the interconnect and messaging layer so that the performance of these components might improve at a rate comparable to that of processor speed. Memory-mapped network interfaces such as the SHRIMP <ref> [6] </ref> network interface and Digital's Memory Channel [12] transfer individual memory updates from one virtual memory to another across the network in less than 4 microseconds in a non-blocking fashion. An interesting question is how well shared virtual memory protocols can take advantage of such network interface support. <p> The custom network interface is the key system component of the SHRIMP system: it connects each PC node to the routing backplane and implements hardware support for virtual memory-mapped communication (VMMC) <ref> [6] </ref>. The single-word update latency from one node's virtual memory to that of another node is under 4 microseconds. The data transfer bandwidth achieved is about 25 Mbytes/sec, which is close to the PC's I/O bus bandwidth. The network interface supports the VMMC abstraction in three ways. <p> Because it does not require an explicit send instruction, AU allows data to be sent with no software overhead on the sending side. Second, the network in-terface supports deliberate update (DU) operations. An explicit send instruction is required for DU, but a user-level DMA mechanism <ref> [6] </ref> reduces the send overhead to a few instructions. Third, the network interface supports a notification mechanism that uses fast interrupts to notify a receiving process that data has arrived (if the receiving process has requested notifications for a particular mapping). <p> This method can be used to implement a home-based protocol entirely in software. Another way to propagate these writes is to use the automatic update hardware mechanism provided by a network interface like SHRIMP <ref> [6] </ref> or Memory Channel [12]. A previous study showed that home-based protocols could achieve better performance and scalability than the traditional LRC protocol based on distributed diffs [28].
Reference: [7] <author> J.B. Carter, J.K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and Performance of Munin. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: A number of shared virtual memory protocols have been proposed that use relaxed consistency models to tolerate communication latency and end-point overhead but most of these assume a standard network interface. Munin <ref> [7] </ref>, TreadMarks [20] and Home-based Lazy Release Consistency (HLRC) [28], for example, all implement multiple writer protocols without special network interface support. These protocols use the CPU to compute local updates by determining the difference (diff) between a clean and dirty copy of the page that has been written. <p> The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [11]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers <ref> [7] </ref>. Lazy Release Consistency (LRC) [21, 8, 1] further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [20] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [8] <author> A.L. Cox, S. Dwarkadas, P. Keleher, H. Lu, R. Rajamony, and W. Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [11]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7]. Lazy Release Consistency (LRC) <ref> [21, 8, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [20] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [9] <author> S. Dwarkadas, A.L. Cox, and W. Zwaenepoel. </author> <title> An Integrated Compile-Time/Run-Time Software Distributed Shared Memory Systems. </title> <booktitle> In ASPLOS-VII, </booktitle> <year> 1996. </year>
Reference-contexts: In the context of compiler support for SVM systems research has been done in having the compiler detect write-only pages and prevent the SVM protocol from invalidating them at synchronization points <ref> [9] </ref>. 8 Conclusions We have investigated the performance benefit of automatic update in improving shared virtual memory protocols, on a real hardware implementation. Using SHRIMP, the only available platform which supports automatic update exclusively in hardware, we implemented and evaluated several home-based protocols on a subset of SPLASH-2 applications.
Reference: [10] <author> E.W. Felten, R.D. Alpert, A. Bilas, M.A. Blumrich, D.W. Clark, S. Damianakis, C. Dubnicki, L. Iftode, and K. Li. </author> <title> Early Experience with Message-Passing on the SHRIMP Mul-ticomputer. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Breakdowns of execution time show that compared to HLRC, HLRC-copy spends twice as much time in communication, indicating that the extra copy operation halves the effective page transfer bandwidth. This result is consistent with the results Number on top of each bar is speedup. reported in <ref> [10] </ref>. Overall, HLRC performs significantly better than HLRC-copy for the applications with high communication-to-computation ratios (FFT, Radix and Ocean-square), a little better for those with medium communication-to-computation ratios (Barnes and Ocean-row), and similarly for those with very low communication-to-computation ratios (Water and LU).
Reference: [11] <author> K. Gharachorloo, D. Lenoski, J. Laudon, P. Gibbons, A. Gupta, and J. Hennessy. </author> <title> Memory Consistency and Event Ordering in Scalable Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: on the SHRIMP multicomputer system: Home-base Lazy Release Consistency (HLRC) [28], Automatic Update Release Consistency (AURC) [13], and AURC with Copyset-2 optimization (AURC-2) [13] We will first briefly review the lazy release consistency model. 3.1 Lazy Release Consistency Lazy Release Consistency (LRC) is a consistency model similar to release consistency <ref> [11] </ref> that is intended for software implementation. It delays the propagation of (page) invalidations until the latest possible acquire time. To achieve memory coherence with such a degree of laziness, the LRC protocol uses vector timestamps to maintain the "happens-before" partial causal ordering between synchronization events. <p> The Release Consistency (RC) model was proposed in order to improve hardware cache coherence <ref> [11] </ref>. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7]. Lazy Release Consistency (LRC) [21, 8, 1] further relaxed the RC protocol to reduce protocol overhead.
Reference: [12] <author> Richard Gillett. </author> <title> Memory Channel Network for PCI. </title> <booktitle> In Proceedings of Hot Interconnects '95 Symposium, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: Memory-mapped network interfaces such as the SHRIMP [6] network interface and Digital's Memory Channel <ref> [12] </ref> transfer individual memory updates from one virtual memory to another across the network in less than 4 microseconds in a non-blocking fashion. An interesting question is how well shared virtual memory protocols can take advantage of such network interface support. <p> This method can be used to implement a home-based protocol entirely in software. Another way to propagate these writes is to use the automatic update hardware mechanism provided by a network interface like SHRIMP [6] or Memory Channel <ref> [12] </ref>. A previous study showed that home-based protocols could achieve better performance and scalability than the traditional LRC protocol based on distributed diffs [28]. <p> AURC. SVM protocol that implements a home-based multiple-writer scheme using the I/O remote write operations supported by the DEC Memory Channel network interface <ref> [12] </ref> rather than AU. Cashmere has been evaluated in comparison with TreadMarks [20], the best-known all-software distributed diff-based LRC protocol. However, the write propagation mechanism in Memory Channel requires explicit remote write instructions in software and therefore is not a transparent automatic update mechanism. <p> Memory Channel <ref> [12] </ref> network allows remote memory to be mapped into the local virtual address space but without a corresponding local memory mapping. This is why writes to remote memory are not automatically performed locally at the same virtual address, making a software shared-memory scheme more difficult to implement.
Reference: [13] <author> L. Iftode, C. Dubnicki, E. W. Felten, and Kai Li. </author> <title> Improving Release-Consistent Shared Virtual Memory using Automatic Update. </title> <booktitle> In The 2nd IEEE Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: The protocols differ in how and when they propagate and merge diffs. Recently, several protocols have been proposed to take advantage of memory-mapped communication that supports fine-grained remote writes <ref> [13, 15, 22] </ref>. These protocols are all home-based Lazy Release Consistency (HLRC) protocols in which each page is assigned a fixed node called home to collect updates from multiple writers. <p> These protocols are all home-based Lazy Release Consistency (HLRC) protocols in which each page is assigned a fixed node called home to collect updates from multiple writers. The Automatic Update Release Consistency (AURC) protocol was the first proposal to take advantage of memory-mapped communication to implement an LRC protocol <ref> [13] </ref>. It uses memory-mapped communication in two ways. First, it maps non-home copies of a shared page to the home page. <p> This library implements calls for exporting and importing memory buffers, sending data with and without notifications, and managing receive buffer memory. 3 Protocols This section describes three coherence protocols we have compared on the SHRIMP multicomputer system: Home-base Lazy Release Consistency (HLRC) [28], Automatic Update Release Consistency (AURC) <ref> [13] </ref>, and AURC with Copyset-2 optimization (AURC-2) [13] We will first briefly review the lazy release consistency model. 3.1 Lazy Release Consistency Lazy Release Consistency (LRC) is a consistency model similar to release consistency [11] that is intended for software implementation. <p> exporting and importing memory buffers, sending data with and without notifications, and managing receive buffer memory. 3 Protocols This section describes three coherence protocols we have compared on the SHRIMP multicomputer system: Home-base Lazy Release Consistency (HLRC) [28], Automatic Update Release Consistency (AURC) <ref> [13] </ref>, and AURC with Copyset-2 optimization (AURC-2) [13] We will first briefly review the lazy release consistency model. 3.1 Lazy Release Consistency Lazy Release Consistency (LRC) is a consistency model similar to release consistency [11] that is intended for software implementation. It delays the propagation of (page) invalidations until the latest possible acquire time. <p> Previous comparisons between AURC and software-based protocols were performed using simulations <ref> [13, 14] </ref>, making this the first opportunity to evaluate the actual performance benefit of AURC for SVM on a real SHRIMP system. 3.2.3 Automatic-Update RC with Copyset-2 Previous research in hardware support for update-based shared memory protocols has suggested multicast support for a full-fledged update-based coherence scheme, with updates propagated in <p> The AURC-2 protocol is particularly useful for applications that exhibit nearest-neighbor communication <ref> [13] </ref>, but it can reduce the total number of page faults for other applications as well since there are often some pages that are shared by only two nodes in programs that do not exhibit a nearest-neighbor communication pattern. <p> The result implies that a good selection of homes (whose writes do not generate AU) is helpful. This agrees with the previous simulation results <ref> [13] </ref>.
Reference: [14] <author> L. Iftode, J. P. Singh, and Kai Li. </author> <title> Understanding Application Performance on Shared Virtual Memory. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Such a data transfer requires very little software overhead, no memory copy, and no additional overhead at the receiving node. A previous paper compares the AURC protocol with a distributed diff-based LRC protocol <ref> [14] </ref> using a simulator, but its evaluation has two limitations from the viewpoint of evaluating memory mapped communication and AU support. <p> Previous comparisons between AURC and software-based protocols were performed using simulations <ref> [13, 14] </ref>, making this the first opportunity to evaluate the actual performance benefit of AURC for SVM on a real SHRIMP system. 3.2.3 Automatic-Update RC with Copyset-2 Previous research in hardware support for update-based shared memory protocols has suggested multicast support for a full-fledged update-based coherence scheme, with updates propagated in <p> The communication in Water-Nsquared and Water-Spatial also benefits from AURC, but overall performance does not benefit significantly due to low communication-to-computation ratio in these applications. Radix, an application known for its poor performance on SVM systems <ref> [14] </ref> obtains a substantial performance improvement. This is because Radix performs irregularly scattered writes to remotely allocated data in its communication phase, which leads to a lot of diffing in HLRC but is quite well suited to the automatic fine-grained propagation of writes in AURC.
Reference: [15] <author> L. Iftode, J.P. Singh, and K. Li. </author> <title> Scope Consistency: a Bridge Between Release Consistency and Entry Consistency. </title> <booktitle> In Proceedings of the 8th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: The protocols differ in how and when they propagate and merge diffs. Recently, several protocols have been proposed to take advantage of memory-mapped communication that supports fine-grained remote writes <ref> [13, 15, 22] </ref>. These protocols are all home-based Lazy Release Consistency (HLRC) protocols in which each page is assigned a fixed node called home to collect updates from multiple writers.
Reference: [16] <author> Intel Corporation. </author> <title> Paragon XP/S Product Overview, </title> <year> 1991. </year>
Reference-contexts: Notes to the reviewers: we will provide data on 16-node system and AURC with Copyset-2 extension in the final paper. 2 SHRIMP System The SHRIMP multicomputer system consists of 16 Pentium PC nodes connected by an Intel Paragon routing network <ref> [26, 16] </ref>. Each PC uses an Intel Pentium Xpress motherboard [17] that holds a 66 Mhz Pentium CPU, 256 Kbytes of L2 cache, and 40 Mbytes of DRAM memory. Peripherals are connected to the system through the EISA expansion bus [3].
Reference: [17] <author> Intel Corporation. </author> <title> Express Platforms Technical Product Summary: System Overview, </title> <month> April </month> <year> 1993. </year>
Reference-contexts: Each PC uses an Intel Pentium Xpress motherboard <ref> [17] </ref> that holds a 66 Mhz Pentium CPU, 256 Kbytes of L2 cache, and 40 Mbytes of DRAM memory. Peripherals are connected to the system through the EISA expansion bus [3].
Reference: [18] <author> Andrew W. Wilson Jr. Richard P. LaRowe Jr. and Marc J. Teller. </author> <title> Hardware Assist for Distributed Shared Memory. </title> <booktitle> In Proceedings of 13th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 246-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: this the first opportunity to evaluate the actual performance benefit of AURC for SVM on a real SHRIMP system. 3.2.3 Automatic-Update RC with Copyset-2 Previous research in hardware support for update-based shared memory protocols has suggested multicast support for a full-fledged update-based coherence scheme, with updates propagated in circular rings <ref> [18, 5] </ref>. As the SHRIMP hardware design philosophy emphasizes simplicity, we ruled out complicated features such as rings and multicast. <p> Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial. The PLUS [5], Galactica Net <ref> [18] </ref>, Merlin [24] and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [19] <author> P. Keleher. </author> <title> Is There No Place Like Home. </title> <note> In Submitted for publication, </note> <year> 1997. </year>
Reference-contexts: The best case is when the sharing pattern is migratory; then diffs are totally ordered and can be fetched in one message from the last writer. Without using an adaptive scheme of the type recently proposed <ref> [2, 19] </ref>, the sizes of the messages increases during the migration due to diff accumulation (since a processor is fetching diffs, it fetches not only the diffs created by the last writer but also those created by previous writers in the intervals that it has not already seen).
Reference: [20] <author> P. Keleher, A.L. Cox, S. Dwarkadas, and W. Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the Winter USENIX Conference, </booktitle> <pages> pages 115-132, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: A number of shared virtual memory protocols have been proposed that use relaxed consistency models to tolerate communication latency and end-point overhead but most of these assume a standard network interface. Munin [7], TreadMarks <ref> [20] </ref> and Home-based Lazy Release Consistency (HLRC) [28], for example, all implement multiple writer protocols without special network interface support. These protocols use the CPU to compute local updates by determining the difference (diff) between a clean and dirty copy of the page that has been written. <p> In this way an LRC protocol implements causal ordering between the events performed before the release and the events that occur after the acquire, thus satisfying the RC memory model. LRC was first proposed, implemented, and evaluated in the TreadMarks SVM system <ref> [20] </ref>. TreadMarks supports concurrent writers to the same page using a software multiple-writer scheme based on distributed diffs. Every writer records locally the changes it makes to every shared page during each interval. <p> The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7]. Lazy Release Consistency (LRC) [21, 8, 1] further relaxed the RC protocol to reduce protocol overhead. Tread-Marks <ref> [20] </ref> was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines. <p> AURC. SVM protocol that implements a home-based multiple-writer scheme using the I/O remote write operations supported by the DEC Memory Channel network interface [12] rather than AU. Cashmere has been evaluated in comparison with TreadMarks <ref> [20] </ref>, the best-known all-software distributed diff-based LRC protocol. However, the write propagation mechanism in Memory Channel requires explicit remote write instructions in software and therefore is not a transparent automatic update mechanism.
Reference: [21] <author> P. Keleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Lazy Consistency for Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the 19th Annual Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [11]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7]. Lazy Release Consistency (LRC) <ref> [21, 8, 1] </ref> further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [20] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines.
Reference: [22] <author> L. Kontothanassis, G. Hunt, R. Stets, N. Hardavellas, M. Ciernak, S. Parthasarathy, W. Meira Jr., S. Dwarkadas, and M. Scott. </author> <title> VM-based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In ISCA24, </booktitle> <year> 1997. </year>
Reference-contexts: The protocols differ in how and when they propagate and merge diffs. Recently, several protocols have been proposed to take advantage of memory-mapped communication that supports fine-grained remote writes <ref> [13, 15, 22] </ref>. These protocols are all home-based Lazy Release Consistency (HLRC) protocols in which each page is assigned a fixed node called home to collect updates from multiple writers. <p> Lazy Release Consistency (LRC) [21, 8, 1] further relaxed the RC protocol to reduce protocol overhead. Tread-Marks [20] was the first SVM implementation using the LRC protocol on a network of stock computers. That implementation has achieved respectable performance on small-scale machines. Cashmere <ref> [22] </ref> is an eager Release Consistent (RC) Update traffic Protocol traffic Number of Message traffic Number of Message traffic Application messages (Mbytes) messages (Mbytes) HLRC AURC HLRC AURC HLRC AURC HLRC AURC Barnes 5,915 2,498 10.4 10.1 39,600 32,785 2.2 2.16 FFT 2,240 2,240 9.2 9.2 4,600 4,600 0.4 0.4 Ocean-row
Reference: [23] <author> K. Li and P. Hudak. </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the 5th Annual ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 229-239, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: But due to diff overhead and its impact on synchronization overhead which is larger in HLRC, the impact of write only optimization is larger for AURC than for HLRC but overall it remains small. 7 Related Work Since shared virtual memory was first proposed ten years ago <ref> [23] </ref>, a lot of work has been done on it. The Release Consistency (RC) model was proposed in order to improve hardware cache coherence [11]. The model was used to implement shared virtual memory and reduce false sharing by allowing multiple writers [7].
Reference: [24] <author> Creve Maples. </author> <title> A High-Performance, Memory-Based Interconnection System For Multicomputer Environments. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 295-304, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial. The PLUS [5], Galactica Net [18], Merlin <ref> [24] </ref> and its successor SESAME [27], systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [25] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year> <note> Also Stanford University Technical Report No. CSL-TR-92-526, </note> <month> June </month> <year> 1992. </year>
Reference-contexts: The possible drawback of this method, compared to standard AURC, is that AURC-2 increases the amount of overall network traffic since the home nodes also generate AU write traffic. 4 Applications We evaluated the performance of our SVM protocols using a subset of the SPLASH-2 applications and kernels <ref> [25] </ref> that exhibit both regular and irregular sharing patterns. Barnes uses the Barnes-Hut hierarchical N-body method to simulate the interactions among a system of particles over time. The computational domain is represented as an octree of space cells.
Reference: [26] <author> Roger Traylor and Dave Dunning. </author> <title> Routing Chip Set for Intel Paragon Parallel Supercomputer. </title> <booktitle> In Proceedings of Hot Chips '92 Symposium, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Notes to the reviewers: we will provide data on 16-node system and AURC with Copyset-2 extension in the final paper. 2 SHRIMP System The SHRIMP multicomputer system consists of 16 Pentium PC nodes connected by an Intel Paragon routing network <ref> [26, 16] </ref>. Each PC uses an Intel Pentium Xpress motherboard [17] that holds a 66 Mhz Pentium CPU, 256 Kbytes of L2 cache, and 40 Mbytes of DRAM memory. Peripherals are connected to the system through the EISA expansion bus [3].
Reference: [27] <author> Larry D. Wittie, Gudjon Hermannsson, and Ai Li. </author> <title> Eager Sharing for Efficient Massive Parallelism. </title> <booktitle> In Proceedings of the 1992 International Conference on Parall el Processing, </booktitle> <pages> pages 251-255, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Using simulations they show that such a protocol processor can double the performance of TreadMarks on a 16-node configuration and that diff prefetching is not always beneficial. The PLUS [5], Galactica Net [18], Merlin [24] and its successor SESAME <ref> [27] </ref>, systems implement hardware-based shared memory using a sort of write-through mechanism which is similar in some ways to automatic update. These systems do more in hardware, and thus are more expensive and complicated to build.
Reference: [28] <author> Y. Zhou, L. Iftode, and K. Li. </author> <title> Performance Evaluation of Two Home-Based Lazy Release Consistency Protocols for Shared Virtual Memory Systems. </title> <booktitle> In Proceedings of the Operating Systems Design and Implementation Symposium, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: A number of shared virtual memory protocols have been proposed that use relaxed consistency models to tolerate communication latency and end-point overhead but most of these assume a standard network interface. Munin [7], TreadMarks [20] and Home-based Lazy Release Consistency (HLRC) <ref> [28] </ref>, for example, all implement multiple writer protocols without special network interface support. These protocols use the CPU to compute local updates by determining the difference (diff) between a clean and dirty copy of the page that has been written. <p> This library implements calls for exporting and importing memory buffers, sending data with and without notifications, and managing receive buffer memory. 3 Protocols This section describes three coherence protocols we have compared on the SHRIMP multicomputer system: Home-base Lazy Release Consistency (HLRC) <ref> [28] </ref>, Automatic Update Release Consistency (AURC) [13], and AURC with Copyset-2 optimization (AURC-2) [13] We will first briefly review the lazy release consistency model. 3.1 Lazy Release Consistency Lazy Release Consistency (LRC) is a consistency model similar to release consistency [11] that is intended for software implementation. <p> This can result in significant memory consumption by the protocol during program execution; indeed, the memory required to store diffs can eventually exceed the memory used by the application <ref> [28] </ref>. Garbage collection is usually triggered at global barriers to free up the diff storage, but this garbage collection is itself an expensive global operation that increases protocol overhead. A home-based multiple-writer scheme is an alternative approach to supporting multiple concurrent writers. <p> Another way to propagate these writes is to use the automatic update hardware mechanism provided by a network interface like SHRIMP [6] or Memory Channel [12]. A previous study showed that home-based protocols could achieve better performance and scalability than the traditional LRC protocol based on distributed diffs <ref> [28] </ref>.
References-found: 28

