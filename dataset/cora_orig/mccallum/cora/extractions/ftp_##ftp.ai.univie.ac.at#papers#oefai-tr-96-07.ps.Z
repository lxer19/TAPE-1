URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-96-07.ps.Z
Refering-URL: http://www.ai.univie.ac.at/~juffi/Ilp-Vu/ilp-vu-program.html
Root-URL: 
Email: E-mail: juffi@ai.univie.ac.at  
Title: Pruning Algorithms for Rule Learning  
Author: Johannes Furnkranz 
Keyword: Pruning, Noise Handling, Inductive Rule Learning, Inductive Logic Programming  
Web: OEFAI-TR-96-07  
Address: Schottengasse 3, A-1010 Vienna, Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: Pre-Pruning and Post-Pruning are two standard methods of dealing with noise in decision tree learning. Pre-Pruning methods deal with noise during learning, while post-pruning methods try to address this problem after an overfitting theory has been learned. This paper shows how pre- and post-pruning algorithms can be used for separate-and-conquer rule learning algorithms. We discuss some fundamental problems and show how to solve them with two new algorithms that combine and integrate pre- and post-pruning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., J. Friedman, R. Olshen, & C. </author> <title> Stone (1984). Classification and Regression Trees. </title> <address> Pacific Grove, CA: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference-contexts: The resulting theory is subsequently analyzed and (if necessary) simplified and generalized in order to increase its predictive accuracy on unseen data. Post-pruning approaches have been commonly used in the decision tree learning algorithms CART <ref> (Breiman, Friedman, Olshen, and Stone 1984) </ref>, ID3 (Quinlan 1987) and ASSISTANT (Niblett and Bratko 1986). An overview and comparison of various approaches can be found in (Mingers 1989) and (Esposito, Malerba, and Semeraro 1993). 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP). <p> The last theory within the 1-SE margin, which hopefully is a little too specific, 4 This is based on an idea in CART <ref> (Breiman, Friedman, Olshen, and Stone 1984) </ref>, where the most general 10 11 procedure TDP (Examples, SplitRatio) Cutoff = 1:0 BestTheory = ; BestAccuracy = 0:0 SplitExamples (SplitRatio, Examples, GrowingSet, PruningSet) repeat NewTheory = Fossil (GrowingSet,Cutoff) NewAccuracy = Accuracy (NewTheory,PruningSet) if NewAccuracy &gt; BestAccuracy BestTheory = NewTheory BestAccuracy = NewAccuracy LowerBound =
Reference: <author> Brunk, C. A. & M. J. </author> <title> Pazzani (1991). An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <address> Evanston, </address> <publisher> Illinois, </publisher> <pages> pp. 389-393. </pages>
Reference-contexts: Figure 1 shows a schematic depiction of this process. The quality of the found rules and conditions is commonly evaluated on a separate set of training examples that have not been seen during learning. Post-pruning algorithms include Reduced Error Pruning (REP) <ref> (Brunk and Pazzani 1991) </ref> and Grow (Cohen 1993). Both have been shown to be very effective in noise-handling. However, they are also inefficient, because they waste time by learning an overfitting concept description and subsequently pruning a significant portion of its rules and conditions. <p> This simple algorithm has been adapted from decision tree learning (Quinlan 1987) to the separate-and-conquer rule learning framework by (Pagallo and Haussler 1990) and <ref> (Brunk and Pazzani 1991) </ref>. At the beginning the training data are split into two subsets: a growing set (usually 2/3) and a pruning set (1/3). <p> This is repeated until the accuracy of the best pruned theory is below that of its predecessor. REP has been shown to learn more accurate theories than the pre-pruning algorithm Foil in the KRK domain at several levels of noise <ref> (Brunk and Pazzani 1991) </ref>. 6 4.2 Problems with Reduced Error Pruning Although REP is quite effective in raising predictive accuracy in noisy domains (Brunk and Pazzani 1991), it has several shortcomings, which we will discuss in this section. <p> REP has been shown to learn more accurate theories than the pre-pruning algorithm Foil in the KRK domain at several levels of noise <ref> (Brunk and Pazzani 1991) </ref>. 6 4.2 Problems with Reduced Error Pruning Although REP is quite effective in raising predictive accuracy in noisy domains (Brunk and Pazzani 1991), it has several shortcomings, which we will discuss in this section. In particular we will suggest that post-pruning is incompatible with the separate-and-conquer learning strategy. <p> The setup of the experiments will be described in more detail at the beginning of section 7.1. Both algorithms split the supplied data sets into the same growing (ca. 2=3) and pruning sets (ca. 1=3). Both algorithms used Reduced Error Pruning as described in <ref> (Brunk and Pazzani 1991) </ref> for their post-pruning phase.
Reference: <author> Cameron-Jones, R. </author> <year> (1994, </year> <month> May). </month> <title> The complexity of Cohen's grow method. Unpublished draft for comments. </title>
Reference: <author> Clark, P. & R. </author> <title> Boswell (1991). Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session of Learning, Porto, Portugal, </booktitle> <pages> pp. 151-163. </pages>
Reference: <author> Clark, P. & T. </author> <title> Niblett (1989). The CN2 induction algorithm. </title> <booktitle> Machine Learning 3 (4), </booktitle> <pages> 261-283. </pages>
Reference-contexts: The final theory is learned in one pass (see figure 1). Most separate-and-conquer rule learners, like CN2 <ref> (Clark and Niblett 1989) </ref>, Foil (Quinlan 1990), and Fossil (Furnkranz 1994), use this form of noise handling. Another family of algorithms deals with noise after learning. These post-pruning algorithms typically first induce a theory that is complete and consistent with the training data. <p> that cover only a few examples by making sure that the number of bits that are needed to encode the current clause is less than the number of bits needed to encode the instances covered by it. 1 * Significance Testing was first used in the propositional CN2 induction algorithm <ref> (Clark and Niblett 1989) </ref> and later on in the relational learner mFoil (Dzeroski and Bratko 1992).
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Efficient pruning methods for separate-and-conquer rule learning systems. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <address> Chambery, France, </address> <pages> pp. 988-994. </pages>
Reference-contexts: Figure 1 shows a schematic depiction of this process. The quality of the found rules and conditions is commonly evaluated on a separate set of training examples that have not been seen during learning. Post-pruning algorithms include Reduced Error Pruning (REP) (Brunk and Pazzani 1991) and Grow <ref> (Cohen 1993) </ref>. Both have been shown to be very effective in noise-handling. However, they are also inefficient, because they waste time by learning an overfitting concept description and subsequently pruning a significant portion of its rules and conditions. One remedy for this problem is to combine pre- and post-pruning. <p> In particular we will suggest that post-pruning is incompatible with the separate-and-conquer learning strategy. Efficiency In <ref> (Cohen 1993) </ref> it was shown that the worst-case time complexity of REP is as bad as (n 4 ) on random data (n is the number of examples). The growing of the initial concept, on the other hand, is only (n 2 log n). <p> A wrong choice of a literal cannot be undone by pruning. 4.3 The Grow Algorithm To solve some of the problems of section 4.2, in particular efficiency, a top-down post-pruning algorithm based on a technique used in (Pagallo and Haussler 1990) has been proposed in <ref> (Cohen 1993) </ref>. Like REP, the Grow algorithm first finds a theory that overfits the data. <p> Instead of removing the most useless clause or literal from the specific theory it adds the most promising generalization of a rule to an initially empty theory. This results in a significant gain in efficiency, along with a slight gain in accuracy as the experiments in <ref> (Cohen 1993) </ref> show. However, the asymptotic time complexity of the Grow post-pruning method is still above the complexity of the initial rule growing phase as has recently been shown in (Cameron-Jones 1994). <p> Thus it is not surprising that Grow has been shown to outperform REP on a variety of datasets <ref> (Cohen 1993) </ref>. <p> Post-pruning is very inefficient in this case, because most of the work performed in the learning phase has to be undone in the pruning phase. A natural solution to this problem would be to start the pruning phase with a simpler theory. This idea has first been investigated in <ref> (Cohen 1993) </ref>, where the efficient post-pruning algorithm Grow (see section 4.3) has been combined with some weak pre-pruning heuristics that speed up the learning phase. The goal of pre-pruning in this context is not to entirely prevent overfitting, but to reduce its amount. <p> This is significantly lower than the complexity of growing an overfitting theory which has been shown to be (n 2 log n) under the same assumptions <ref> (Cohen 1993) </ref>. As in REP, growing one clause from purely random data costs n log n (each of the approximately log (n) literals has to be tested against n examples). <p> In fact, it is always faster than REP's and Grow's initial growing phase alone, because I-REP avoids to learn an intermediate overfitting theory. It can also be seen that Grow's pruning algorithm is much faster than REP's, which confirms the results of <ref> (Cohen 1993) </ref>. In order to get an idea on the asymptotic complexity of the various algorithms we have performed a log-log analysis as in (Cameron-Jones 1994). <p> In particular the evidence supports that result that REP has a complexity of (n 4 ) and that the initial rule growing phase is O (n 2 log n) as shown in <ref> (Cohen 1993) </ref>. It also 16 confirms the main result of (Cameron-Jones 1994), namely that the asymptotic complexity of Grow is not below the asymptotic complexity of the initial rule growing phase as has been originally suggested in (Cohen 1993). <p> initial rule growing phase is O (n 2 log n) as shown in <ref> (Cohen 1993) </ref>. It also 16 confirms the main result of (Cameron-Jones 1994), namely that the asymptotic complexity of Grow is not below the asymptotic complexity of the initial rule growing phase as has been originally suggested in (Cohen 1993). However, in all our experiments the absolute values for the run-time of Grow's pruning phase were negligible compared to the initial over-fitting phase. REP often gets caught in local maxima and is not able to generalize to the right level.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning. </booktitle>
Reference-contexts: An important advantage of post-pruning methods is that the way of evaluating theories (or rules in I-REP's case) is entirely independent from the basic learning algorithm. Other pruning and stopping criteria can further improve the performance and eliminate weaknesses. For instance, it has been pointed out in <ref> (Cohen 1995) </ref> that accuracy estimates for low-coverage rules will have a high variance and therefore I-REP is likely to stop prematurely and to over-generalize in domains that are susceptible to the Small Disjuncts Problem (Holte, Acker, and Porter 1989). (Cohen 1995) also points out some deficiencies of the accuracy-based pruning criterion <p> For instance, it has been pointed out in <ref> (Cohen 1995) </ref> that accuracy estimates for low-coverage rules will have a high variance and therefore I-REP is likely to stop prematurely and to over-generalize in domains that are susceptible to the Small Disjuncts Problem (Holte, Acker, and Porter 1989). (Cohen 1995) also points out some deficiencies of the accuracy-based pruning criterion and shows how a stopping criterion based on description length and a better pruning criterion can significantly improve I-REP's accuracy without a loss in efficiency. Another way for improving I-REP has been tried in (Furnkranz 1995).
Reference: <author> Dol sak, B., I. Bratko, & A. </author> <month> Jezernik </month> <year> (1994). </year> <title> Finite element mesh design: An engineering domain for ILP application. </title> <booktitle> In Proceedings of the 4th International Workshop on Inductive Logic Programming, Number 237 in GMD-Studien, Bad Honnef, Germany, </booktitle> <pages> pp. 305-320. </pages>
Reference-contexts: Nevertheless it is still the best algorithm in terms of accuracy which shows how poorly all algorithms do in this domain. We hope to be able to improve our results in this domain by trying the faster algorithms on the new data set <ref> (Dolsak, Bratko, and Jezernik 1994) </ref> which contains a total of 10 objects (and thus hopefully provides more redundancy). However, for this comparative study the new data set was too big.
Reference: <author> Dol sak, B. & S. </author> <title> Muggleton (1992). The application of Inductive Logic Programming to finite-element mesh design. </title> <editor> In S. Muggleton (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 453-472. </pages> <address> London: </address> <publisher> Academic Press Ltd. 26 D zeroski, </publisher> <editor> S. & I. </editor> <title> Bratko (1992). Handling noise in Inductive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan. </address>
Reference-contexts: The experiments were performed with version 6.1. 18 19 7.2 The Mesh Domain We have also tested our algorithms on the finite element mesh design problem first studied and described in detail in <ref> (Dolsak and Muggleton 1992) </ref>. The problem of mesh design is to break complex objects into a number of finite elements in order to be able to compute pressure and deformations when a force is applied to the object.
Reference: <author> Esposito, F., D. Malerba, & G. </author> <title> Semeraro (1993). Decision tree pruning as a search in the state space. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <address> Vienna, Austria, </address> <pages> pp. 165-184. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Post-pruning approaches have been commonly used in the decision tree learning algorithms CART (Breiman, Friedman, Olshen, and Stone 1984), ID3 (Quinlan 1987) and ASSISTANT (Niblett and Bratko 1986). An overview and comparison of various approaches can be found in (Mingers 1989) and <ref> (Esposito, Malerba, and Semeraro 1993) </ref>. 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP). This simple algorithm has been adapted from decision tree learning (Quinlan 1987) to the separate-and-conquer rule learning framework by (Pagallo and Haussler 1990) and (Brunk and Pazzani 1991).
Reference: <author> F urnkranz, J. </author> <year> (1994). </year> <title> Fossil: A robust relational learner. </title> <booktitle> In Machine Learning: ECML-94, </booktitle> <pages> pp. 122-137. </pages> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The final theory is learned in one pass (see figure 1). Most separate-and-conquer rule learners, like CN2 (Clark and Niblett 1989), Foil (Quinlan 1990), and Fossil <ref> (Furnkranz 1994) </ref>, use this form of noise handling. Another family of algorithms deals with noise after learning. These post-pruning algorithms typically first induce a theory that is complete and consistent with the training data. <p> For this purpose pre-pruning heuristics are used to reduce (not entirely prevent) the amount of overfitting, so that learning and pruning will be more efficient as sketched in the third part of figure 1. Our particular implementation of this approach, Top-Down Pruning (TDP) <ref> (Furnkranz 1994) </ref>, uses a simple algorithm to generate a set of theories pruned to different degrees in a top-down, general-to-specific order. <p> Motivated by the success of this method, we have developed a more rigorous approach that tightly integrates pre- and post-pruning. Instead of learning an entire theory and pruning it thereafter, Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> prunes single clauses right after they have been learned. This new algorithm entirely avoids the learning of an overfitting theory by using post-pruning methods as a pre-pruning stopping criterion as shown in figure 1. With this method a significant speedup can be achieved in noisy domains. <p> Insignificant rules are rejected. * The Cutoff Stopping Criterion has been used in the separate-and-conquer learning system Fossil <ref> (Furnkranz 1994) </ref>. Fossil uses a search heuristic based on statistical correlation, which enables it to judge the relevance of all literals on the same uniform scale from 0 to 1. <p> A setting of Cutoff = 0:3 is a good general heuristic which seems to be independent of the noise level in the data <ref> (Furnkranz 1994) </ref>. 4 Post-Pruning While pre-pruning approaches try to avoid overfitting during rule generation, post-pruning approaches at first ignore the problem of overfitting the noise and learn a complete and consistent concept description. <p> However, there is always the danger that a predefined stopping criterion will over-generalize the theory. In this section we will therefore discuss an alternative approach that searches for an appropriate starting point for the post-pruning phase. 5.1 Top-Down Pruning One advantage of Fossil's simple and efficient cutoff stopping criterion <ref> (Furnkranz 1994) </ref> is its closeness to the search heuristic. Fossil needs to do a mere comparison between the heuristic value of the best candidate literal and the cutoff value in order to decide whether to add the candidate literal to the clause at hand or not. <p> In predictive accuracy, Foil did poorly. Its stopping criterion (encoding length) is dependent on the training set size and thus too weak to effectively prevent overfitting the noise. From 1000 examples Foil learns concepts that have more than 20 rules and are incomprehensible <ref> (Furnkranz 1994) </ref>. I-REP, on the other hand, consistently produces a 99.57% correct, understandable 4-rule approximation of the correct concept description. <p> Fossil, on the other hand, is the fastest algorithm. Foil, although implemented in C, is slower, because with increasing training set sizes it learns more clauses than Fossil (see also <ref> (Furnkranz 1994) </ref>). REP proves that its pruning method is very inefficient. Grow has an efficient pruning algorithm, but still suffers from the expensive overfitting phase. TDP is faster than REP and Grow, because it is able to start post-pruning with a much better theory than REP or Grow.
Reference: <author> F urnkranz, J. </author> <year> (1994). </year> <title> Top-down pruning in relational learning. </title> <booktitle> In Proceedings of the 11th European Conference on Artificial Intelligence, </booktitle> <address> Amsterdam, The Netherlands, </address> <pages> pp. 453-457. </pages>
Reference-contexts: The final theory is learned in one pass (see figure 1). Most separate-and-conquer rule learners, like CN2 (Clark and Niblett 1989), Foil (Quinlan 1990), and Fossil <ref> (Furnkranz 1994) </ref>, use this form of noise handling. Another family of algorithms deals with noise after learning. These post-pruning algorithms typically first induce a theory that is complete and consistent with the training data. <p> For this purpose pre-pruning heuristics are used to reduce (not entirely prevent) the amount of overfitting, so that learning and pruning will be more efficient as sketched in the third part of figure 1. Our particular implementation of this approach, Top-Down Pruning (TDP) <ref> (Furnkranz 1994) </ref>, uses a simple algorithm to generate a set of theories pruned to different degrees in a top-down, general-to-specific order. <p> Motivated by the success of this method, we have developed a more rigorous approach that tightly integrates pre- and post-pruning. Instead of learning an entire theory and pruning it thereafter, Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> prunes single clauses right after they have been learned. This new algorithm entirely avoids the learning of an overfitting theory by using post-pruning methods as a pre-pruning stopping criterion as shown in figure 1. With this method a significant speedup can be achieved in noisy domains. <p> Insignificant rules are rejected. * The Cutoff Stopping Criterion has been used in the separate-and-conquer learning system Fossil <ref> (Furnkranz 1994) </ref>. Fossil uses a search heuristic based on statistical correlation, which enables it to judge the relevance of all literals on the same uniform scale from 0 to 1. <p> A setting of Cutoff = 0:3 is a good general heuristic which seems to be independent of the noise level in the data <ref> (Furnkranz 1994) </ref>. 4 Post-Pruning While pre-pruning approaches try to avoid overfitting during rule generation, post-pruning approaches at first ignore the problem of overfitting the noise and learn a complete and consistent concept description. <p> However, there is always the danger that a predefined stopping criterion will over-generalize the theory. In this section we will therefore discuss an alternative approach that searches for an appropriate starting point for the post-pruning phase. 5.1 Top-Down Pruning One advantage of Fossil's simple and efficient cutoff stopping criterion <ref> (Furnkranz 1994) </ref> is its closeness to the search heuristic. Fossil needs to do a mere comparison between the heuristic value of the best candidate literal and the cutoff value in order to decide whether to add the candidate literal to the clause at hand or not. <p> In predictive accuracy, Foil did poorly. Its stopping criterion (encoding length) is dependent on the training set size and thus too weak to effectively prevent overfitting the noise. From 1000 examples Foil learns concepts that have more than 20 rules and are incomprehensible <ref> (Furnkranz 1994) </ref>. I-REP, on the other hand, consistently produces a 99.57% correct, understandable 4-rule approximation of the correct concept description. <p> Fossil, on the other hand, is the fastest algorithm. Foil, although implemented in C, is slower, because with increasing training set sizes it learns more clauses than Fossil (see also <ref> (Furnkranz 1994) </ref>). REP proves that its pruning method is very inefficient. Grow has an efficient pruning algorithm, but still suffers from the expensive overfitting phase. TDP is faster than REP and Grow, because it is able to start post-pruning with a much better theory than REP or Grow.
Reference: <author> F urnkranz, J. </author> <year> (1995). </year> <title> A tight integration of pruning and learning. </title> <type> Technical Report OEFAI-TR-95-03, </type> <institution> Austrian Research Institute for Artificial Intelligence. </institution>
Reference-contexts: Another way for improving I-REP has been tried in <ref> (Furnkranz 1995) </ref>. Just as I-REP tries to improve upon REP by pruning at the rule level instead of the theory level, we have investigated a way of taking this further and tried to improve upon I-REP with an algorithm 25 that prunes at the literal level. <p> The resulting algorithm | I 2 -REP | seemed to be a little more stable at low training set sizes, but no significant differences in run-time could be observed <ref> (Furnkranz 1995) </ref>. Besides, I 2 -REP appeared to be a little slower than I-REP, although asymptotically both algorithms are clearly subquadratic. Currently we are investigating the merits of avoiding the loss of information that is caused by the need of splitting the training set into separate growing and pruning sets.
Reference: <author> F urnkranz, J. </author> <year> (1995). </year> <title> A tight integration of pruning and learning (extended abstract). </title>
Reference-contexts: Another way for improving I-REP has been tried in <ref> (Furnkranz 1995) </ref>. Just as I-REP tries to improve upon REP by pruning at the rule level instead of the theory level, we have investigated a way of taking this further and tried to improve upon I-REP with an algorithm 25 that prunes at the literal level. <p> The resulting algorithm | I 2 -REP | seemed to be a little more stable at low training set sizes, but no significant differences in run-time could be observed <ref> (Furnkranz 1995) </ref>. Besides, I 2 -REP appeared to be a little slower than I-REP, although asymptotically both algorithms are clearly subquadratic. Currently we are investigating the merits of avoiding the loss of information that is caused by the need of splitting the training set into separate growing and pruning sets.
Reference: <editor> In N. Lavrac and S. Wrobel (Eds.), </editor> <booktitle> Machine Learning: ECML-95, Lecture Notes in Artificial Intelligence 912, </booktitle> <pages> pp. 291-294. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> F urnkranz, J. & G. </author> <title> Widmer (1994). Incremental Reduced Error Pruning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ, </address> <pages> pp. 70-77. </pages>
Reference-contexts: The final theory is learned in one pass (see figure 1). Most separate-and-conquer rule learners, like CN2 (Clark and Niblett 1989), Foil (Quinlan 1990), and Fossil <ref> (Furnkranz 1994) </ref>, use this form of noise handling. Another family of algorithms deals with noise after learning. These post-pruning algorithms typically first induce a theory that is complete and consistent with the training data. <p> For this purpose pre-pruning heuristics are used to reduce (not entirely prevent) the amount of overfitting, so that learning and pruning will be more efficient as sketched in the third part of figure 1. Our particular implementation of this approach, Top-Down Pruning (TDP) <ref> (Furnkranz 1994) </ref>, uses a simple algorithm to generate a set of theories pruned to different degrees in a top-down, general-to-specific order. <p> Motivated by the success of this method, we have developed a more rigorous approach that tightly integrates pre- and post-pruning. Instead of learning an entire theory and pruning it thereafter, Incremental Reduced Error Pruning (I-REP) <ref> (Furnkranz and Widmer 1994) </ref> prunes single clauses right after they have been learned. This new algorithm entirely avoids the learning of an overfitting theory by using post-pruning methods as a pre-pruning stopping criterion as shown in figure 1. With this method a significant speedup can be achieved in noisy domains. <p> Insignificant rules are rejected. * The Cutoff Stopping Criterion has been used in the separate-and-conquer learning system Fossil <ref> (Furnkranz 1994) </ref>. Fossil uses a search heuristic based on statistical correlation, which enables it to judge the relevance of all literals on the same uniform scale from 0 to 1. <p> A setting of Cutoff = 0:3 is a good general heuristic which seems to be independent of the noise level in the data <ref> (Furnkranz 1994) </ref>. 4 Post-Pruning While pre-pruning approaches try to avoid overfitting during rule generation, post-pruning approaches at first ignore the problem of overfitting the noise and learn a complete and consistent concept description. <p> However, there is always the danger that a predefined stopping criterion will over-generalize the theory. In this section we will therefore discuss an alternative approach that searches for an appropriate starting point for the post-pruning phase. 5.1 Top-Down Pruning One advantage of Fossil's simple and efficient cutoff stopping criterion <ref> (Furnkranz 1994) </ref> is its closeness to the search heuristic. Fossil needs to do a mere comparison between the heuristic value of the best candidate literal and the cutoff value in order to decide whether to add the candidate literal to the clause at hand or not. <p> In predictive accuracy, Foil did poorly. Its stopping criterion (encoding length) is dependent on the training set size and thus too weak to effectively prevent overfitting the noise. From 1000 examples Foil learns concepts that have more than 20 rules and are incomprehensible <ref> (Furnkranz 1994) </ref>. I-REP, on the other hand, consistently produces a 99.57% correct, understandable 4-rule approximation of the correct concept description. <p> Fossil, on the other hand, is the fastest algorithm. Foil, although implemented in C, is slower, because with increasing training set sizes it learns more clauses than Fossil (see also <ref> (Furnkranz 1994) </ref>). REP proves that its pruning method is very inefficient. Grow has an efficient pruning algorithm, but still suffers from the expensive overfitting phase. TDP is faster than REP and Grow, because it is able to start post-pruning with a much better theory than REP or Grow.
Reference: <author> Holte, R., L. Acker, & B. </author> <title> Porter (1989). Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI. </address>
Reference-contexts: For instance, it has been pointed out in (Cohen 1995) that accuracy estimates for low-coverage rules will have a high variance and therefore I-REP is likely to stop prematurely and to over-generalize in domains that are susceptible to the Small Disjuncts Problem <ref> (Holte, Acker, and Porter 1989) </ref>. (Cohen 1995) also points out some deficiencies of the accuracy-based pruning criterion and shows how a stopping criterion based on description length and a better pruning criterion can significantly improve I-REP's accuracy without a loss in efficiency.
Reference: <author> Holte, R. C. </author> <year> (1993). </year> <title> Very simple classification rules perform well on most commonly used datasets. </title> <booktitle> Machine Learning 11, </booktitle> <pages> 63-91. </pages>
Reference-contexts: The appendix of <ref> (Holte 1993) </ref> gives a summary of the results achieved by various algorithms on some of the most commonly used data sets of the UCI repository and a short description of these sets. We selected 9 of them for our experiments. <p> In the Lymphography data set we removed the 6 examples for the classes "normal find" and "fibrosis" in order to get a 2-class problem. All other data were used as described in <ref> (Holte 1993) </ref>. For all data sets the task was to learn a definition for the minority class. In all datasets the background knowledge consisted of &lt; and = relations with one variable 21 Breast Cancer Accuracy Stnd. Dev. <p> Run-times for all datasets were measured in CPU seconds for SUN SPARCstations ELC except for the Mushroom and KRKPa7 datasets which are quite big and thus had to be run on a considerably faster SPARCstation S10. All experiments followed the setup used in <ref> (Holte 1993) </ref>, i.e. the algorithms were trained on 2=3 of the data and tested on the remaining 1=3. However, only 10 runs were performed for each algorithm on each data set. The results can be found in tables 7, 8, and 9. <p> The results of C4.5, a decision tree learning system with extensive noise-handling capabilities (Quinlan 1993), are taken from the experiments performed in <ref> (Holte 1993) </ref> and are meant as an indicator of the performance of state-of-the-art decision tree learning algorithms on these data sets. A short look shows that the results vary in terms of accuracy, but are quite consistent in 22 Glass (G2) Accuracy Stnd. Dev.
Reference: <author> Michalski, R. S. </author> <year> (1980). </year> <title> Pattern recognition and rule-guided inference. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence 2, </journal> <pages> 349-361. </pages>
Reference: <author> Michalski, R. S., I. Mozeti c, J. Hong, & N. </author> <title> Lavra c (1986). The multi-purpose incremental learning system AQ15 and its testing application to three medical domains. </title> <booktitle> In Proceedings of the 5th National Conference on Artificial Intelligence, </booktitle> <address> Philadelphia, PA, </address> <pages> pp. 1041-1045. </pages>
Reference: <author> Mingers, J. </author> <year> (1989). </year> <title> An empirical comparison of pruning methods for decision tree induction. </title> <booktitle> Machine Learning 4, </booktitle> <pages> 227-243. </pages>
Reference-contexts: Post-pruning approaches have been commonly used in the decision tree learning algorithms CART (Breiman, Friedman, Olshen, and Stone 1984), ID3 (Quinlan 1987) and ASSISTANT (Niblett and Bratko 1986). An overview and comparison of various approaches can be found in <ref> (Mingers 1989) </ref> and (Esposito, Malerba, and Semeraro 1993). 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP).
Reference: <author> Mittenecker, E. </author> <year> (1977). </year> <institution> Planung und statistische Auswertung von Experimenten (8th ed.). Vienna, Austria: Verlag Franz Deuticke. </institution> <note> In German. </note>
Reference-contexts: Fossil was significantly (5%) better than TDP in the Votes (VI) domain and outperformed (5%, sometimes 1%) all other algorithms 9 We have used a range test which can be used to quickly determine significant differences between medium values for small (N &lt; 20) sample sizes <ref> (Mittenecker 1977) </ref>. For N = 10 the value of L = 1 2 R 1 +R 2 has to be &gt; 0:152 for a significance level of 5% and &gt; 0:210 for a significance level of 1%. ( i are medium values and R i are ranges.
Reference: <author> Muggleton, S., M. Bain, J. Hayes-Michie, & D. </author> <title> Michie (1989). An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pp. 113-118. </pages>
Reference-contexts: Run-times were measured in CPU seconds for SUN SPARCstations ELC. 17 7.1 Summary of the Experiments in the KRK Domain First we will summarize the experiments in the domain of recognizing illegal chess positions in the KRK endgame <ref> (Muggleton, Bain, Hayes-Michie, and Michie 1989) </ref>. This domain has become a standard benchmark problem for relational learning systems, as it cannot be solved in a trivial way by propositional learning algorithms, because the background knowledge has to contain relations like X = Y, X &lt; Y, and adjacent (X,Y).
Reference: <author> Niblett, T. & I. </author> <title> Bratko (1986). Learning decision rules in noisy domains. </title> <booktitle> In Proceedings of Expert Systems 86. </booktitle> <publisher> Cambridge University Press. </publisher>
Reference-contexts: The resulting theory is subsequently analyzed and (if necessary) simplified and generalized in order to increase its predictive accuracy on unseen data. Post-pruning approaches have been commonly used in the decision tree learning algorithms CART (Breiman, Friedman, Olshen, and Stone 1984), ID3 (Quinlan 1987) and ASSISTANT <ref> (Niblett and Bratko 1986) </ref>. An overview and comparison of various approaches can be found in (Mingers 1989) and (Esposito, Malerba, and Semeraro 1993). 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP).
Reference: <author> Pagallo, G. & D. </author> <title> Haussler (1990). Boolean feature discovery in empirical learning. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 71-99. </pages>
Reference-contexts: CN2 (Clark and Niblett 1989; Clark and Boswell 1991) combined AQ's covering strategy with the greedy information-based test selection of ID3 (Quinlan 1983), which yielded a powerful rule learning algorithm. The term separate-and-conquer has been coined in <ref> (Pagallo and Haussler 1990) </ref> in the context of learning decision lists. <p> This simple algorithm has been adapted from decision tree learning (Quinlan 1987) to the separate-and-conquer rule learning framework by <ref> (Pagallo and Haussler 1990) </ref> and (Brunk and Pazzani 1991). At the beginning the training data are split into two subsets: a growing set (usually 2/3) and a pruning set (1/3). <p> A wrong choice of a literal cannot be undone by pruning. 4.3 The Grow Algorithm To solve some of the problems of section 4.2, in particular efficiency, a top-down post-pruning algorithm based on a technique used in <ref> (Pagallo and Haussler 1990) </ref> has been proposed in (Cohen 1993). Like REP, the Grow algorithm first finds a theory that overfits the data.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell (Eds.), </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pp. 463-482. </pages> <publisher> Tioga Publishing Co. </publisher>
Reference-contexts: CN2 (Clark and Niblett 1989; Clark and Boswell 1991) combined AQ's covering strategy with the greedy information-based test selection of ID3 <ref> (Quinlan 1983) </ref>, which yielded a powerful rule learning algorithm. The term separate-and-conquer has been coined in (Pagallo and Haussler 1990) in the context of learning decision lists.
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies 27, </journal> <pages> 221-234. </pages>
Reference-contexts: The resulting theory is subsequently analyzed and (if necessary) simplified and generalized in order to increase its predictive accuracy on unseen data. Post-pruning approaches have been commonly used in the decision tree learning algorithms CART (Breiman, Friedman, Olshen, and Stone 1984), ID3 <ref> (Quinlan 1987) </ref> and ASSISTANT (Niblett and Bratko 1986). An overview and comparison of various approaches can be found in (Mingers 1989) and (Esposito, Malerba, and Semeraro 1993). 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP). <p> An overview and comparison of various approaches can be found in (Mingers 1989) and (Esposito, Malerba, and Semeraro 1993). 4.1 Reduced Error Pruning The most common among these methods is Reduced Error Pruning (REP). This simple algorithm has been adapted from decision tree learning <ref> (Quinlan 1987) </ref> to the separate-and-conquer rule learning framework by (Pagallo and Haussler 1990) and (Brunk and Pazzani 1991). At the beginning the training data are split into two subsets: a growing set (usually 2/3) and a pruning set (1/3).
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <booktitle> Machine Learning 5, </booktitle> <pages> 239-266. </pages>
Reference-contexts: 1 Introduction Separate-and-conquer rule-learning systems have gained in popularity through the recent success of the Inductive Logic Programming algorithm Foil <ref> (Quinlan 1990) </ref>. We will analyze different pruning methods for this type of inductive rule learning algorithm and discuss some of their problems. <p> The final theory is learned in one pass (see figure 1). Most separate-and-conquer rule learners, like CN2 (Clark and Niblett 1989), Foil <ref> (Quinlan 1990) </ref>, and Fossil (Furnkranz 1994), use this form of noise handling. Another family of algorithms deals with noise after learning. These post-pruning algorithms typically first induce a theory that is complete and consistent with the training data. <p> The term separate-and-conquer has been coined in (Pagallo and Haussler 1990) in the context of learning decision lists. Finally, separate-and-conquer learning is the basic control structure in the Foil algorithm for efficiently inducing logic programs <ref> (Quinlan 1990) </ref>, which pioneered significant research in the field of relational learning and Inductive Logic Programming. the algorithm is a set of positive and negative examples of the target concept. <p> Most separate-and-conquer algorithms employ stopping criteria for noise handling. The most commonly used among them are * Encoding Length Restriction: This heuristic used in the Inductive Logic Programming algorithm Foil <ref> (Quinlan 1990) </ref> is based on the Minimum Description Length principle (Rissanen 1978). <p> The signs of 10% of the training instances were deliberately reversed to generate artificial noise in the data. The learned concepts were evaluated on test sets with 5000 noise-free examples. We used the state-of-the-art relational learner Foil <ref> (Quinlan 1990) </ref> as a benchmark. 8 Foil 6.1, which is implemented in C, was used with its default settings except that the -V 0 option was set to avoid the introduction of new variables, which is not necessary for this task.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Each line shows the average accuracy on the 10 sets, its standard deviation and range (difference between the maximum and the minimum accuracy encountered), and the run-time of the algorithm. The results of C4.5, a decision tree learning system with extensive noise-handling capabilities <ref> (Quinlan 1993) </ref>, are taken from the experiments performed in (Holte 1993) and are meant as an indicator of the performance of state-of-the-art decision tree learning algorithms on these data sets.
Reference: <author> Quinlan, J. R. </author> <year> (1994). </year> <title> The minimum description length principle and categorical theories. </title> <booktitle> In Proceeding of the 11th International Conference on Machine Learning, </booktitle> <address> New Brunswick, NJ, </address> <pages> pp. 233-241. </pages>
Reference-contexts: Several authors have tried ILP methods on this problem (Dolsak and Muggleton 1992; Dzeroski and Bratko 1992; Quinlan 1994). The available background knowledge consists of an attribute-based description of the edges and of topological relations between the edges. The setup of our experiments was the same as in <ref> (Quinlan 1994) </ref>, i.e. we learned rules from four of the five objects in the data set and tested the learned concept on the fifth object. The learned theories were tested as in (Quinlan 1994), which is a little different from the setup used in (Dzeroski and Bratko 1992): Instead of actually <p> The setup of our experiments was the same as in <ref> (Quinlan 1994) </ref>, i.e. we learned rules from four of the five objects in the data set and tested the learned concept on the fifth object. The learned theories were tested as in (Quinlan 1994), which is a little different from the setup used in (Dzeroski and Bratko 1992): Instead of actually predicting a value for the number of finite elements on an edge, we merely checked for all possible values whether this value could be derived from the learned rules or not.
Reference: <author> Rissanen, J. </author> <year> (1978). </year> <title> Modeling by shortest data description. </title> <type> Automatica 14, </type> <pages> 465-471. </pages>
Reference-contexts: Most separate-and-conquer algorithms employ stopping criteria for noise handling. The most commonly used among them are * Encoding Length Restriction: This heuristic used in the Inductive Logic Programming algorithm Foil (Quinlan 1990) is based on the Minimum Description Length principle <ref> (Rissanen 1978) </ref>.
Reference: <author> Schaffer, C. </author> <year> (1993). </year> <title> Overfitting avoidance as bias. </title> <booktitle> Machine Learning 10, </booktitle> <pages> 153-178. </pages>
Reference: <author> Wolpert, D. H. </author> <year> (1993). </year> <title> On overfitting avoidance as bias. </title> <type> Technical Report SFI TR 92-03-5001, </type> <institution> The Santa Fe Institute, </institution> <address> Santa Fe, NM. </address> <month> 28 </month>
References-found: 33

