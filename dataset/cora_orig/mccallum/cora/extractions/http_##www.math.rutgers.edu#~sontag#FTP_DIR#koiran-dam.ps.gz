URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/koiran-dam.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: koiran@lip.ens-lyon.fr  sontag@hilbert.rutgers.edu  
Title: Vapnik-Chervonenkis Dimension of Recurrent Neural Networks  
Author: Pascal Koiran Eduardo D. Sontag 
Note: This research was carried out in part while visiting DIMACS and the Rutgers Center for Systems and Control (SYCON) at Rutgers University. This research was supported in part by US Air Force Grant AFOSR-94-0293.  
Address: 46 allee d'Italie, 69364 Lyon Cedex 07 France  New Brunswick, NJ 08903 USA  
Affiliation: Laboratoire de l'Informatique du Parallelisme Ecole Normale Superieure de Lyon CNRS  Department of Mathematics Rutgers University  
Abstract: Most of the work on the Vapnik-Chervonenkis dimension of neural networks has been focused on feedforward networks. However, recurrent networks are also widely used in learning applications, in particular when time is a relevant parameter. This paper provides lower and upper bounds for the VC dimension of such networks. Several types of activation functions are discussed, including threshold, polynomial, piecewise-polynomial and sigmoidal functions. The bounds depend on two independent parameters: the number w of weights in the network, and the length k of the input sequence. In contrast, for feedforward networks, VC dimension bounds can be expressed as a function of w only. An important difference between recurrent and feedforward nets is that a fixed recurrent net can receive inputs of arbitrary length. Therefore we are particularly interested in the case k w. Ignoring multiplicative constants, the main results say roughly the following: * For architectures with activation = any fixed nonlinear polynomial, the VC dimension is wk. * For architectures with activation = any fixed piecewise polynomial, the VC dimension is between wk and w 2 k. * For architectures with activation = H (threshold nets), the VC dimension is between w log(k=w) and minfwk log wk; w 2 + w log wkg. * For the standard sigmoid (x) = 1=(1 + e x ), the VC dimension is between wk and w 4 k 2 . An earlier version of this paper has appeared in Proc. 3rd European Workshop on Computational Learning Theory, LNCS 1208, pages 223-237, Springer, 1997. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E.B. Baum and D. Haussler, </author> <title> "What size net gives valid generalization?", </title> <journal> Neural Computation, </journal> <volume> 1(1989), </volume> <pages> pp. 151-160. </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. <ref> [4, 1] </ref>, [7], [10]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> The result then follows from the Baum-Haussler bound <ref> [1] </ref>. 2 Proof of Theorem 2. Let S = fu 1 ; : : : ; u s g be a set of s inputs. We will bound the number of distinct transition functions of the architecture for inputs in S. <p> Next we define a family F of functions which shatters S. The functions in this family are indexed by - parameters w 1 ; :::; w - 2 <ref> [0; 1] </ref>. Each parameter is assumed to have a finite -bit binary expansion 0:w i1 : : : w i . <p> One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from [11] are needed (the first one is well-known and easy to prove). 12 Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = <p> One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from [11] are needed (the first one is well-known and easy to prove). 12 Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = 1; : : <p> two lemmas from [11] are needed (the first one is well-known and easy to prove). 12 Lemma 2 Let : <ref> [0; 1] </ref> ! [0; 1] be the logistic map (x) = 4x (1 x). For every n 1 and every * 2 f0; 1g n there exists x 1 2 [0; 1] such that the sequence (x k ) 1kn defined by x k+1 = (x k ) for k = 1; : : : ; n 1 satisfies the following property: 0 x k &lt; 1=2 if * k = 0 and 1=2 &lt; x k 1 if * k
Reference: [2] <author> Y. Bengio, </author> <title> Neural Networks for Speech and Sequence Recognition, </title> <publisher> Thompson Computer Press, </publisher> <address> Boston, </address> <year> 1996. </year>
Reference-contexts: Among other applications, they have been employed in the design of control laws, speech recognition and speaker identification, formal language inference, and sequence extrapolation for time series prediction. For references, see for instance the book <ref> [2] </ref>, which emphasizes digital signal processing, [6] for formal language learning, and ([12] for control. In addition, theoretical results about neural networks established their universality as models for systems approximation ([16]) as well as analog computing devices ([13, 14]). As a simple example, consider the network in Figure 1.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth, </author> <title> "Learnability and the Vapnik-Chervonenkis Dimension", </title> <journal> Journal of the ACM, </journal> <volume> 36(1989), </volume> <pages> pp. 929-965. </pages>
Reference-contexts: Let T i be the threshold function computed by gate number i. If this gate has w i incoming weights, then T i can induce 9 at most 2jDj w i distinct functions on D by, e.g., Sauer's lemma (see for instance <ref> [3] </ref>).
Reference: [4] <author> T.M. </author> <title> Cover, "Capacity problems for linear machines", in: Pattern Recognition (L. </title> <editor> Kanal ed.), </editor> <publisher> Thompson Book Co., </publisher> <year> 1968, </year> <pages> pp. 283-289 </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. <ref> [4, 1] </ref>, [7], [10]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2.
Reference: [5] <author> B. Dasgupta and E.D. Sontag, </author> <title> "Sample complexity for learning recurrent perceptron mappings," </title> <journal> IEEE Trans. Inform. </journal> <note> Theory , September 1996, to appear. (Summary in Advances in Neural Information Processing Systems 8 (NIPS95) (D.S. </note> <editor> Touretzky, M.C. Moser, and M.E. Hasselmo, eds.), </editor> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1996, </year> <pages> pp. 204-210.) </pages>
Reference-contexts: In particular, we continue the work described in <ref> [5] </ref> (see also [18] for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H. <p> The bounds would seem to be too conservative, since they completely disregard the fact that the weights in the different layers of the "unfolded" net are actually the same. The surprising aspect of the results to be stated next (and of the results in <ref> [5] </ref>) is that we obtain lower bounds which do not look much different. We first state two more upper bounds. The first one is interesting because for fixed w, it shows a log k dependence, rather than the k log k obtained by unfolding. <p> The VC dimension of recurrent architectures with activation , with w weights and receiving inputs of length k, is O (kw). Moreover, if is linear this bound can be improved to O (w log k). 6 For a corresponding lower bound in the linear case, see <ref> [5] </ref>. We now turn to other lower bounds. Theorem 4 The VC dimension of threshold recurrent architectures, with w weights and receiving inputs of length k = (w), is (w log (k=w)). <p> Hence by [7] (Theorem 2.3) its VC dimension is O (w fi kw). 2 Interestingly, one can give a better upper bound for polynomial activation functions than for piecewise-polynomial activation functions. The linear case is included in <ref> [5] </ref>. Proof of Theorem 3. We denote by W the vector listing all weights in the three systems matrices ff; fi; fl, so that the parameter vector can be partitioned as (W; 0 ), where 0 lists the weights in ~.
Reference: [6] <author> C.L. Giles, G.Z. Sun, H.H. Chen, Y.C. Lee and D. Chen, </author> <title> "Higher order recurrent networks and grammatical inference", </title> <booktitle> in Advances in Neural Information Processing Systems 2, </booktitle> <editor> D.S. Touretzky (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: Among other applications, they have been employed in the design of control laws, speech recognition and speaker identification, formal language inference, and sequence extrapolation for time series prediction. For references, see for instance the book [2], which emphasizes digital signal processing, <ref> [6] </ref> for formal language learning, and ([12] for control. In addition, theoretical results about neural networks established their universality as models for systems approximation ([16]) as well as analog computing devices ([13, 14]). As a simple example, consider the network in Figure 1.
Reference: [7] <author> P. Goldberg and M. Jerrum, </author> <title> "Bounding the Vapnik-Chervonenkis dimension of concept classes parametrized by real numbers," </title> <booktitle> Machine Learning 18(1995), </booktitle> <pages> pp. 131-148. </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. [4, 1], <ref> [7] </ref>, [10]) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> Hence the whole computation requires O (kw) operations for inputs of length k. The architecture has w + n 2w programmable parameters, where n is the number of units in the network. Hence by <ref> [7] </ref> (Theorem 2.3) its VC dimension is O (w fi kw). 2 Interestingly, one can give a better upper bound for polynomial activation functions than for piecewise-polynomial activation functions. The linear case is included in [5]. Proof of Theorem 3. <p> This follows from: D 1 = 2d, and D k+1 = d (D k + 1). Here "D k + 1" accounts for multiplication between weight and state variables, and multiplication by d accounts for the application of ). By <ref> [7] </ref> (Theorem 2.2) the VC dimension is bounded by 2w log (8eD k ). (note that the degree in the input variables does not appear in this bound.) The theorem follows from the obvious observations: D k = k + 1 for d = 1 and D k &lt; 2d k+1
Reference: [8] <author> S. Grossberg, </author> <title> The Adaptive Brain, 2 vols, </title> <publisher> Elsevier, </publisher> <address> Amsterdam, </address> <year> 1987. </year>
Reference-contexts: Similar results can be obtained in the continuous-time framework, however.) Recurrent networks are among the models considered by Grossberg (see e.g. <ref> [8] </ref>) and his school during the last twenty or more years, and include the networks proposed by Hopfield (see e.g. [9]) for associative memory and optimization.
Reference: [9] <author> J.J. </author> <title> Hopfield, "Neural networks and physical systems with emergent computational abilities," </title> <booktitle> Proceedings Natl. </booktitle> <institution> Acad. Sci. </institution> <address> USA 79 (1982), </address> <pages> pp. 2554-2558. </pages>
Reference-contexts: Similar results can be obtained in the continuous-time framework, however.) Recurrent networks are among the models considered by Grossberg (see e.g. [8]) and his school during the last twenty or more years, and include the networks proposed by Hopfield (see e.g. <ref> [9] </ref>) for associative memory and optimization. Among other applications, they have been employed in the design of control laws, speech recognition and speaker identification, formal language inference, and sequence extrapolation for time series prediction.
Reference: [10] <author> M. Karpinski and A. Macintyre, </author> <title> "Polynomial bounds for VC dimension of sigmoidal and general Pfaffian neural networks," </title> <journal> J. Computer Sys. Sci. </journal> <volume> 54(1997), </volume> <pages> 169-176. </pages>
Reference-contexts: This trivial fact allows one to easily obtain estimates, based on those bounds which were developed (cf. [4, 1], [7], <ref> [10] </ref>) for the feedforward case. Theorem 1 For recurrent architectures, with w weights receiving inputs of length k: 1. The VC dimension of threshold recurrent architectures is O (kw log kw). 2. <p> By unfolding, the recurrent architecture can be simulated by a feedforward net with kn nodes, where n be the number of nodes in the original architecture, and the same number w of programmable parameters. By <ref> [10] </ref> there is a O ((kn) 2 w 2 ) upper bound on the VC dimension of that architecture. This is O (k 2 w 4 ) as claimed. <p> This is O (k 2 w 4 ) as claimed. Note: one can argue that the feedforward architecture has kw weights, but many of those weights are "shared" and there are only w + n 2w programmable parameters. The result in <ref> [10] </ref> explicitly allows such weight-sharing arrangements (see condition e in section 4.1 of their paper). 2 4.2 Lower Bounds Theorem 6 shows that the O (kw) upper bound of Theorem 3 is tight (for non-linear polynomials).
Reference: [11] <author> P. Koiran and E.D. Sontag, </author> <title> "Neural networks with quadratic VC dimension," </title> <journal> J. Computer Sys. Sci. </journal> <volume> 54(1997), </volume> <pages> pp. 190-198. </pages>
Reference-contexts: Note also that the outputs at t = 1 and t = 2 are both 0 as needed. 2 Theorem 6 generalizes Theorem 5 to even more arbitrary activations. For this we need some of the machinery of <ref> [11] </ref>. In particular, we need to allow networks with multiplication and division gates. <p> We will use feedforward architectures as building blocks in our recurrent architectures. The necessary background is standard and can be found for instance in <ref> [11] </ref>. To be self-contained, we recall that the units of feedforward architecture are grouped into layers. We use the same type of units as in recurrent architectures (in particular, multiplication and division gates are allowed, as mentioned earlier in this section). <p> For synchronization reasons, such connections are to be avoided in recurrent nets. One can always convert a non-layered feedforward architecture into a layered one by introducing delays (identity gates). The following two lemmas from <ref> [11] </ref> are needed (the first one is well-known and easy to prove). 12 Lemma 2 Let : [0; 1] ! [0; 1] be the logistic map (x) = 4x (1 x). <p> The next result is essentially Lemma 1 from <ref> [11] </ref>. <p> Since the network has to work only on a finite set of inputs, the construction will be correct if * is small enough (this can be justified as in <ref> [11] </ref>). Finally, the output unit accumulates the values of E, starting from the initial state o = 0. Note that these accumulated values are all (approximately) zero, except at most one of them. <p> Note that since the length of the longest path in the network increases by a constant factor, it is necessary to pad the input sequence with O (log w) dummy inputs. This changes only the implied constants in the symbols. 2 As in <ref> [11] </ref>, this result makes it possible to prove good VC dimension lower bounds for a wide class of transfer functions. The most important case is Theorem 6, which we can now prove. Proof of Theorem 6. Linear and multiplication gates can be simulated by -gates as in [11]. <p> 2 As in <ref> [11] </ref>, this result makes it possible to prove good VC dimension lower bounds for a wide class of transfer functions. The most important case is Theorem 6, which we can now prove. Proof of Theorem 6. Linear and multiplication gates can be simulated by -gates as in [11]. The input sequence must be padded by a small number of dummy inputs as in the proof of Theorem 9. 2 5 Final Remarks We have left several questions unanswered: 1.
Reference: [12] <editor> M.M. Polycarpou, and P.A. Ioannou, </editor> <title> "Neural networks and on-line approximators for adaptive control," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 93-798, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [13] <author> H. Siegelmann and E.D. Sontag, </author> <title> "On the computational power of neural nets," </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 50(1995): </volume> <pages> 132-150. 15 </pages>
Reference: [14] <author> H. Siegelmann and E.D. Sontag, </author> <title> "Analog computation, neural networks, and circuits," </title> <journal> Theor. Comp. Sci. </journal> <volume> 131(1994): </volume> <pages> 331-360. </pages>
Reference: [15] <author> E.D. Sontag, </author> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems, </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Often C is just a projection onto some coordinates, that is, the components of y are simply a subset of the components of x. The linear systems customarily studied in control theory (see e.g. the textbook <ref> [15] </ref>) are precisely the homogeneous initialized recurrent nets with identity activation and x 0 = 0. u - B - h + - - - C - y r 6 To each initialized recurrent net (A; B; C; x 0 ; ) we associate a discrete time input/output behavior.
Reference: [16] <author> E.D. Sontag, </author> <title> "Neural nets as systems models and controllers," </title> <booktitle> in Proc. Seventh Yale Workshop on Adaptive and Learning Systems, </booktitle> <pages> pp. 73-79, </pages> <institution> Yale University, </institution> <year> 1992. </year>
Reference: [17] <author> E.D. Sontag, </author> <title> "Feedforward nets for interpolation and classification," </title> <journal> J. Comp. Syst. Sci. </journal> <volume> 45(1992): </volume> <pages> 20-48. </pages>
Reference-contexts: In particular, we continue the work described in [5] (see also [18] for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H. As in <ref> [17] </ref>, we say that : R ! R is sigmoidal , or a sigmoid , if: 1. is differentiable at some point x 0 where 0 (x 0 )6=0. 2. lim x!1 (x) = 0 and lim x!+1 (x) = 1. (the limits 0 and 1 can be replaced by any
Reference: [18] <author> A.M. Zador and B.A. Pearlmutter, </author> <title> "VC dimension of an integrate-and-fire neuron model," </title> <booktitle> Neural Computation 8(1996): </booktitle> <pages> 611-624. 16 </pages>
Reference-contexts: In particular, we continue the work described in [5] (see also <ref> [18] </ref> for related work), which had obtained estimates of these quantities for architectures with identity activations. By a threshold recurrent architecture we mean a homogeneous one with = H.
References-found: 18

