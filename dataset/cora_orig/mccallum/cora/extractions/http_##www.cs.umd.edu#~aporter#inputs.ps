URL: http://www.cs.umd.edu/~aporter/inputs.ps
Refering-URL: http://www.cs.umd.edu/~aporter/html/references.html
Root-URL: 
Title: Understanding the Sources of Variation in Software Inspections combined effects of process inputs and process
Author: Adam Porter Harvey Siy Audris Mockus, Lawrence Votta 
Keyword: Categories and Subject Descriptors: D.2.5 [Software Engineering]: Testing and Debugging-Code inspections and walk-throughs General Terms: Experimentation Additional Key Words and Phrases: Statistical models, empirical studies, software inspection, software process  
Note: The  must be other factors which need to be identified.  
Date: April 30, 1997  
Affiliation: University of Maryland Bell Laboratories  
Abstract: In a previous experiment, we determined how various changes in three structural elements of the software inspection process (team size, and number and sequencing of sessions), altered effectiveness and interval. Our results showed that such changes did not significantly influence the defect detection rate, but that certain combinations of changes dramatically increased the inspection interval. We also observed a large amount of unexplained variance in the data, indicating that other factors must be affecting inspection performance. The nature and extent of these other factors now have to be determined to ensure that they had not biased our earlier results. Also, identifying these other factors might suggest additional ways to improve the efficiency of inspections. Acting on the hypothesis that the "inputs" into the inspection process (reviewers, authors, and code units) were significant sources of variation, we modeled their effects on inspection performance. We found that they were responsible for much more variation in defect detection than was process structure. This leads us to conclude that better defect detection techniques, not better process structures, are the key to improving inspection effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Victor R. Basili and Harlan D. Mills. </author> <title> Understanding and documenting programs. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-8(3):270-283, </volume> <month> May </month> <year> 1982. </year>
Reference-contexts: How do these effects interact with the process techniques being used? Clearly, more experimentation is needed to address these questions. Also, we should not neglect research to find better techniques with which to review (e.g., systematic reading techniques <ref> [1] </ref> for the preparation step, meetingless techniques [10, 20, 13] for the collection step, etc.). 35 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [2] <author> Victor R. Basili and David M Weiss. </author> <title> A methodology for collecting valid software engineering data. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> SE-10(6):728-738, </volume> <month> Nov. </month> <year> 1984. </year>
Reference-contexts: The entire data set may be examined online at http://www.cs.umd.edu/users/harvey/variance.html. 2.3 Self-Reported Data Self-reported data tend to contain systematic errors. Therefore we minimized the amount of self-reported data by employing direct observation [23] and interviews <ref> [2] </ref>. The IQE attended 125 of the 130 collection meetings 6 to make sure the meeting data was reported accurately and that reviewers do not mistakenly add to their preparation forms any issues that were not found until collection.
Reference: [3] <author> Richard A. Becker, John M. Chambers, and Allan R. Wilks. </author> <title> The New S Language. </title> <publisher> Wadsworth and Brooks/Cole, </publisher> <year> 1988. </year>
Reference-contexts: We built the models in 22 plot showing how the combined amount of defects found in the preparation related to the number of defects found in the meeting (cor = 0.4). the S programming language <ref> [3, 7] </ref>.
Reference: [4] <author> David B. Bisant and James R. Lyle. </author> <title> A two-person inspection method to improve programming productivity. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 15(10) </volume> <pages> 1294-1304, </pages> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: Finally, these defects are corrected by the artifact's author (Repair). Several variants of this approach have been proposed in order to improve inspection performance. These include Fagan Inspections [12], Active Design Reviews [21], N-Fold Inspections [26], Phased Inspections [15], and Two-Person Inspections <ref> [4] </ref>. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed [25]. Some of these variants have been evaluated empirically. However, focus has been on their overall performance.
Reference: [5] <author> George E. Box, William G. Hunter, and J. Stuart Hunter. </author> <title> Statistics for Experimenters. </title> <publisher> John Wiley and Sons, Inc., </publisher> <year> 1978. </year>
Reference: [6] <author> John M. Chambers, William S. Cleveland, Beat Kleiner, and Paul A. Tuckey. </author> <title> Graphical Methods For Data Analysis. </title> <publisher> Chapman & Hall, </publisher> <year> 1983. </year>
Reference: [7] <author> John M. Chambers and Trevor J. Hastie, </author> <title> editors. Statistical Models in S. </title> <publisher> Wadsworth & Brooks, </publisher> <year> 1992. </year>
Reference-contexts: We built the models in 22 plot showing how the combined amount of defects found in the preparation related to the number of defects found in the meeting (cor = 0.4). the S programming language <ref> [3, 7] </ref>. <p> R A + R B + R C + R D + R E + R F + R G + R H + R I + R J + R K (1) In this model, Functionality and Author are categorical variables represented in S as sets of dummy variables <ref> [7, pp. 20-22,32-36] </ref>. They have 7 and 5 degrees of freedom, respectively. Stepwise model selection heuristic 13 selected the following model. 11 The generalized linear model and the rationale for using it are explained in Appendix C. 12 We used S language notation to represent our models [7, pp. 24-31]. <p> They have 7 and 5 degrees of freedom, respectively. Stepwise model selection heuristic 13 selected the following model. 11 The generalized linear model and the rationale for using it are explained in Appendix C. 12 We used S language notation to represent our models <ref> [7, pp. 24-31] </ref>. For example, the model formula y ~ a + b is read as, "y is modeled by a, and b." Furthermore, in this section, y is modeled by a and b with Poisson errors and logarithmic link function. <p> To avoid overfitting the data, the number of parameters must always be kept small or the residual degrees of freedom high. To perform stepwise model selection we used the step () function in S <ref> [7, pp. 233-238] </ref>. 23 Def ects ~ T eamSize + Sessions + Repairs + P hase + Author + F unc + log (Size) + R B + R C + R F + R G + R H + R I This resulting model is not satisfactory because it retained
Reference: [8] <author> Chris Chatfield. </author> <title> Model uncertainty, data mining and statistical inference. </title> <journal> Journal of the Royal Statistical Society, Series A, </journal> <volume> 158(3), </volume> <year> 1995. </year>
Reference: [9] <author> Bill Curtis. </author> <title> Substantiating programmer variability. </title> <booktitle> Proceedings of the IEEE, </booktitle> <address> 69(7):846, </address> <month> July </month> <year> 1981. </year>
Reference-contexts: This has several implications for the design and analysis of industrial experiments. Past studies have cautioned that wide variation in the abilities of individual developers may mask effects due to experimental treatments <ref> [9] </ref>. However, even with our relatively crude models, we managed to devise a suitable means of accounting for individual variation when analyzing the experimental results. But ultimately, we will get better results only if we can identify and control for factors affecting reviewer and author performance.
Reference: [10] <author> Alan R. Dennis and Joseph S. Valacich. </author> <title> Computer brainstorms: More heads are better than one. </title> <journal> Journal of Applied Psychology, </journal> <volume> 78(4) </volume> <pages> 531-537, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: How do these effects interact with the process techniques being used? Clearly, more experimentation is needed to address these questions. Also, we should not neglect research to find better techniques with which to review (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 20, 13] </ref> for the collection step, etc.). 35 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [11] <author> Stephen G. Eick, Clive R. Loader, M. David Long, Lawrence G. Votta, and Scott Vander Wiel. </author> <title> Estimating software fault content before coding. </title> <booktitle> In Proceedings of the 14th International Conference on Software Engineering, </booktitle> <pages> pages 59-65, </pages> <address> Melbourne, Australia, </address> <month> May </month> <year> 1992. </year> <month> 37 </month>
Reference: [12] <author> Michael E. Fagan. </author> <title> Design and code inspections to reduce errors in program development. </title> <journal> IBM Systems Journal, </journal> <volume> 15(3) </volume> <pages> 182-211, </pages> <year> 1976. </year>
Reference-contexts: Next, the review team meets, looks for additional defects, and compiles a list of all discovered defects (Collection). Finally, these defects are corrected by the artifact's author (Repair). Several variants of this approach have been proposed in order to improve inspection performance. These include Fagan Inspections <ref> [12] </ref>, Active Design Reviews [21], N-Fold Inspections [26], Phased Inspections [15], and Two-Person Inspections [4]. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed [25].
Reference: [13] <author> Philip M. Johnson. </author> <title> An instrumented approach to improving software quality through formal technical review. </title> <booktitle> In Proceedings of the 16th International Conference on Software Engineering, </booktitle> <pages> pages 113-122, </pages> <address> Sorrento, Italy, </address> <month> May </month> <year> 1994. </year>
Reference-contexts: How do these effects interact with the process techniques being used? Clearly, more experimentation is needed to address these questions. Also, we should not neglect research to find better techniques with which to review (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 20, 13] </ref> for the collection step, etc.). 35 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [14] <author> David A. Kenny. </author> <title> Correlation and Causality. </title> <publisher> John Wiley & Sons., </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Finally, we rejected models for which, based on our experience, we could not argue that 3 their variables were causal agents of inspection performance. Specifically, there are four conditions that must be satisfied before factor A can be said to cause response B <ref> [14] </ref>: 1. A must occur before B. 2. A and B must be correlated. 3. There is no other factor C that accounts for the correlation between A and B. 4. A mechanism exists that explains how A affects B.
Reference: [15] <author> John C. Knight and E. Ann Myers. </author> <title> An improved inspection technique. </title> <journal> Communications of the ACM, </journal> <volume> 36(11) </volume> <pages> 51-61, </pages> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Finally, these defects are corrected by the artifact's author (Repair). Several variants of this approach have been proposed in order to improve inspection performance. These include Fagan Inspections [12], Active Design Reviews [21], N-Fold Inspections [26], Phased Inspections <ref> [15] </ref>, and Two-Person Inspections [4]. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed [25]. Some of these variants have been evaluated empirically. However, focus has been on their overall performance.
Reference: [16] <author> Brett Kyle. </author> <title> Successful Industrial Experimentation, chapter 5. </title> <publisher> VCH Publishers, Inc., </publisher> <year> 1995. </year>
Reference-contexts: Time flows left to right. example of a cause-and-effect diagram, similar to the ones used in practice <ref> [16] </ref>, but customized here for our use. The number and types of issues raised in the preparation step are influenced by the reviewers selected and by the number of defects originally in the code unit (which in turn may be affected by the author of the code unit).
Reference: [17] <author> David A. Ladd and J. Christopher Ramming. </author> <title> Software research and switch software. </title> <booktitle> In International Conference on Communications Technology, </booktitle> <address> Beijing, China, </address> <year> 1992. </year>
Reference: [18] <author> David A. Ladd and J. Christopher Ramming. </author> <title> Two application languages in software production. </title> <booktitle> In USENIX Symposium on Very-High-Level Languages, </booktitle> <month> Oct. </month> <year> 1994. </year>
Reference: [19] <author> P. McCullagh and J. A. Nelder. </author> <title> Generalized Linear Models. </title> <publisher> Chapman and Hall, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference: [20] <author> J.F. Nunamaker, Alan R. Dennis, Joseph S. Valacich, Douglas R. Vogel, and Joey F. George. </author> <title> Electronic meeting systems to support group work. </title> <journal> Communications of the ACM, </journal> <volume> 34(7) </volume> <pages> 40-61, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: How do these effects interact with the process techniques being used? Clearly, more experimentation is needed to address these questions. Also, we should not neglect research to find better techniques with which to review (e.g., systematic reading techniques [1] for the preparation step, meetingless techniques <ref> [10, 20, 13] </ref> for the collection step, etc.). 35 7 Future Work 7.1 Framework For Further Study Our study revealed a number of influences affecting variation in the data, some internal and some external to the inspection process.
Reference: [21] <author> David L. Parnas and David M. Weiss. </author> <title> Active design reviews: </title> <booktitle> Principles and practices. In Proceedings of the 8th International Conference on Software Engineering, </booktitle> <pages> pages 215-222, </pages> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Finally, these defects are corrected by the artifact's author (Repair). Several variants of this approach have been proposed in order to improve inspection performance. These include Fagan Inspections [12], Active Design Reviews <ref> [21] </ref>, N-Fold Inspections [26], Phased Inspections [15], and Two-Person Inspections [4]. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed [25]. Some of these variants have been evaluated empirically.
Reference: [22] <author> Dewayne E. Perry, Adam A. Porter, and Lawrence G. Votta. </author> <title> Experimental software engineering: A report on the state of the art. </title> <booktitle> In Proceedings of the 17th International Conference on Software Engineering, </booktitle> <pages> pages 277-279, </pages> <address> Seattle, WA, </address> <month> April </month> <year> 1995. </year> <note> Invited talk and short paper appear in the proceedings. 38 </note>
Reference-contexts: As for the element of error, previous observational studies on time usage conducted in this environment have shown that although there are always inaccuracies in self-reported data, the self-reported data is generally within 20% of the observed data <ref> [22] </ref>. 2.4 Results of the Experiment Our experiment produced three general results: 1. Inspection interval and effectiveness of defect detection were not significantly affected by team size (large vs. small). 2. Inspection interval and effectiveness of defect detection were not significantly affected by number of sessions (single vs. multiple). 3.
Reference: [23] <author> Dewayne E. Perry, Nancy A. Staudenmayer, and Lawrence G. Votta. </author> <title> Understanding and improving time usage in software development. </title> <editor> In Alexander Wolf and Alfonso Fuggetta, editors, </editor> <booktitle> Software Process, volume 5 of Trends in Software: Software Process. </booktitle> <publisher> John Wiley & Sons., </publisher> <year> 1995. </year>
Reference-contexts: The entire data set may be examined online at http://www.cs.umd.edu/users/harvey/variance.html. 2.3 Self-Reported Data Self-reported data tend to contain systematic errors. Therefore we minimized the amount of self-reported data by employing direct observation <ref> [23] </ref> and interviews [2]. The IQE attended 125 of the 130 collection meetings 6 to make sure the meeting data was reported accurately and that reviewers do not mistakenly add to their preparation forms any issues that were not found until collection.
Reference: [24] <author> Adam A. Porter, Lawrence G. Votta, and Victor R. Basili. </author> <title> Comparing detection methods for software requirements inspections: A replicated experiment. </title> <journal> IEEE Trans. on Software Engineering, </journal> <volume> 21(6) </volume> <pages> 563-575, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Moreover, our results suggest that better techniques may have a strong effect on inspection performance. Thus, we will continue to investigate and improve them <ref> [24] </ref>. 7.3 Need for Continued Study of Inspection Interval We have not yet adequately studied the factors affecting interval data. Some of the factors are found in process structure (specifically repairing in between sessions) and process inputs, but much of its variance is still unaccounted for.
Reference: [25] <author> Adam A. Porter, Lawrence G. Votta, Harvey P. Siy, and Carol A. Toman. </author> <title> An experiment to assess the cost-benefits of code inspections in large scale software development. </title> <booktitle> In The Third Symposium on the Foundations of Software Engineering, </booktitle> <address> Washington, D.C., </address> <month> Oct. </month> <year> 1995. </year>
Reference-contexts: These include Fagan Inspections [12], Active Design Reviews [21], N-Fold Inspections [26], Phased Inspections [15], and Two-Person Inspections [4]. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed <ref> [25] </ref>. Some of these variants have been evaluated empirically. However, focus has been on their overall performance. Very few investigations attempted to isolate the effects due specifically to structural changes. <p> Therefore, we conducted a controlled experiment in which we manipulated the structure of the inspection process <ref> [25] </ref>. We adjusted the size of the team and the number of sessions. Defects were sometimes repaired in between multiple sessions and sometimes not.
Reference: [26] <author> G. Michael Schneider, Johnny Martin, and Wei-Tek Tsai. </author> <title> An experimental study of fault detection in user requirements documents. </title> <journal> ACM Trans. on Software Engineering and Methodology, </journal> <volume> 1(2) </volume> <pages> 188-204, </pages> <month> Apr. </month> <year> 1992. </year>
Reference-contexts: Finally, these defects are corrected by the artifact's author (Repair). Several variants of this approach have been proposed in order to improve inspection performance. These include Fagan Inspections [12], Active Design Reviews [21], N-Fold Inspections <ref> [26] </ref>, Phased Inspections [15], and Two-Person Inspections [4]. Most involve restructuring the process, e.g., rearranging the steps, changing the number of people working on each step, or the number of times each step is executed [25]. Some of these variants have been evaluated empirically.
Reference: [27] <author> Scott Vander Wiel and Lawrence G. Votta. </author> <title> Assessing software designs using capture-recapture methods. </title> <journal> IEEE Trans. on Software Engineering, </journal> 19(11) 1045-1054, Nov. 1993. <volume> 39 </volume>
References-found: 27

