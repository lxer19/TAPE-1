URL: ftp://ftp.cc.gatech.edu/pub/people/panesar/pads-95.ps.Z
Refering-URL: http://www.cs.gatech.edu/grads/p/Kiran.Panesar/homepage.html
Root-URL: 
Title: Buffer Management in Shared-Memory Time Warp Systems  
Author: Richard M. Fujimoto Kiran S. Panesar 
Address: Atlanta GA 30332 Atlanta GA 30332  
Affiliation: College of Computing College of Computing Georgia Institute of Technology Georgia Institute of Technology  
Abstract: Mechanisms for managing message buffers in Time Warp parallel simulations executing on cache-coherent shared-memory multiprocessors are studied. Two simple buffer management strategies called the sender pool and receiver pool mechanisms are examined with respect to their efficiency, and in particular, their interaction with multiprocessor cache-coherence protocols. Measurements of implementations on a Kendall Square Research KSR-2 machine using both synthetic workloads and benchmark applications demonstrate that sender pools offer significant performance advantages over receiver pools. However, it is also observed that both schemes, especially the sender pool mechanism, are prone to severe performance degradations due to poor locality of reference in large simulations using substantial amounts of message buffer memory. A third strategy called the partitioned buffer pool approach is proposed that exploits the advantages of sender pools, but exhibits much better locality. Measurements of this approach indicate that the partitioned pool mechanism yields substantially better performance than both the sender and receiver pool schemes for large-scale, small-granularity parallel simulation applications. The central conclusions from this study are: (1) buffer management strategies play an important role in determining the overall efficiency of multiprocessor-based parallel simulators, and (2) the partitioned buffer pool organization offers significantly better performance than the sender and receiver pool schemes. These studies demonstrate that poor performance may result if proper attention is not paid to realizing an efficient buffer management mechanism. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Briner, Jr. </author> <title> Fast parallel simulation of digital systems. </title> <booktitle> In Advances in Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 71-77. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1991. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [8]. Time Warp has demonstrated some success in speeding up simulations of combat models [14], communication networks [12], queuing networks [4], and digital logic circuits <ref> [1] </ref>, among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers.
Reference: [2] <author> C. D. Carothers, R. M. Fujimoto, Y-B. Lin, and P. </author> <title> England. Distributed simulation of large-scale pcs networks. </title> <booktitle> In Proceedings of the 1994 MASCOTS Conference, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: In these experiments, 2048 messages are continuously routed through the network in this fashion. different numbers of processors. The partitioned pools strategy again outperforms the other two schemes in all cases. The performance differential increases as the number of processors is increased. 7.2 Personal Communications Services Network PCS <ref> [2] </ref> is a simulation of a wireless communication network with a set of radio ports structured as a square grid (one port per grid sector). Each grid sector, or cell, is assigned a fixed number of channels.
Reference: [3] <author> Thomas H. Dunigan. </author> <title> Multi ring performance of the kendall square multiprocessor. </title> <type> Technical Report ORNL/TM-12331, </type> <institution> Engineering Physics and Mathematics Division, Oak Ridge National Lab, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: Data that is in neither the sub-cache nor the local cache is fetched from another processor's cache, or if it does not reside in another cache, from secondary storage via the virtual memory system. Further details of the machine architecture and its performance are described in [10] and <ref> [3] </ref>. This machine contains 40 MhZ two-way superscalar processors. Accesses to the sub-cache require 2 clock cycles, and accesses to the local cache require 20 cycles. The time to access another processor's cache depends on the ring traffic. <p> Each lock or unlock operation requires 3 sec in the absence of contention, 14 sec for a pair or processors on the same ring, and 32 sec for a pair of processors on different rings <ref> [3, p 10] </ref>. All experiments described here use a single ring, except the 32 processor runs that use processors from two different rings. All experiments were performed on a KSR-2 run ning KSR OS R1.2.2. The manufacturer provided C compiler was used for these experiments. <p> The lock time is measured to be approximately 20 sec, which is consistent with times reported in <ref> [3] </ref> when one considers that contention for the lock increases the access time to some degree.
Reference: [4] <author> R. M. Fujimoto. </author> <title> Time Warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [8]. Time Warp has demonstrated some success in speeding up simulations of combat models [14], communication networks [12], queuing networks <ref> [4] </ref>, and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers. While prior work in this area has focused on data structures [4], synchronization <p> networks <ref> [4] </ref>, and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers. While prior work in this area has focused on data structures [4], synchronization [9], or implementation of shared state [7, 11], we are concerned here with efficient buffer management strategies for message passing in shared-memory machines. We assume that the hardware platform is a cache-coherent, shared-memory multiprocessor. The commercial machines mentioned earlier are all of this type. <p> This implementation uses direct cancellation to minimize the cost of message cancellation <ref> [4] </ref>. Each buffer contains various pointers and flags (as described earlier) that are modified by the receiver of the message. The hardware platform used for these experiments is a Kendall Square Research KSR-2 multiprocessor. Each KSR-2 processor contains 32 MBytes of local cache memory and a faster, 256 KByte sub-cache.
Reference: [5] <author> R. M. Fujimoto. </author> <title> Performance of Time Warp under synthetic workloads. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 23-28. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1990. </year>
Reference-contexts: If the buffer pool contains five or fewer buffers, additional buffers are reclaimed from the global pool (if available) to restore the processor to its initial allocation. Our initial experiments used a synthetic workload model called PHold <ref> [5] </ref>. The model uses a fixed-sized message population. Each event generates one new message with timestamp increment selected from an exponential distribution. The destination logical process (LP) is selected from a uniform distribution. We first measured send times for messages transmitted to a different processor.
Reference: [6] <author> R. M. Fujimoto and M. Hybinette. </author> <title> Computing global virtual time on shared-memory multiprocessors. </title> <type> Technical report, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> August </month> <year> 1994. </year>
Reference-contexts: Specifically, a buffer corresponding to a message sent from processor j must be returned to processor j's buffer pool. Rather than laboriously scan through all of the fossil collected buffers and returning each to its appropriate pool, a mechanism called "on-the-fly fossil collection" is used <ref> [6] </ref>. As soon as a message is processed, it is immediately returned to the appropriate free buffer pool, even though that buffer may still be required to handle future rollbacks. The buffer allocator is only allowed to allocate a buffer if its timestamp is larger than GVT.
Reference: [7] <author> K. Ghosh and R. M. Fujimoto. </author> <title> Parallel discrete event simulation using space-time memory. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <volume> volume 3, </volume> <pages> pages 201-208, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers. While prior work in this area has focused on data structures [4], synchronization [9], or implementation of shared state <ref> [7, 11] </ref>, we are concerned here with efficient buffer management strategies for message passing in shared-memory machines. We assume that the hardware platform is a cache-coherent, shared-memory multiprocessor. The commercial machines mentioned earlier are all of this type.
Reference: [8] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: It is well known that many large-scale discrete event simulation computations are excessively time consuming, and are a natural candidate for parallel computation. Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism <ref> [8] </ref>. Time Warp has demonstrated some success in speeding up simulations of combat models [14], communication networks [12], queuing networks [4], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. <p> they occur, and recovers using a rollback mechanism <ref> [8] </ref>. Time Warp has demonstrated some success in speeding up simulations of combat models [14], communication networks [12], queuing networks [4], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers.
Reference: [9] <author> P. Konas and P.-C. Yew. </author> <title> Synchronous parallel discrete event simulation on shared-memory mul tiprocessors. </title> <booktitle> In 6 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 24, </volume> <pages> pages 12-21. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers. While prior work in this area has focused on data structures [4], synchronization <ref> [9] </ref>, or implementation of shared state [7, 11], we are concerned here with efficient buffer management strategies for message passing in shared-memory machines. We assume that the hardware platform is a cache-coherent, shared-memory multiprocessor. The commercial machines mentioned earlier are all of this type.
Reference: [10] <editor> KSR. </editor> <booktitle> Topics in KSR Principles of Operation. </booktitle>
Reference-contexts: Data that is in neither the sub-cache nor the local cache is fetched from another processor's cache, or if it does not reside in another cache, from secondary storage via the virtual memory system. Further details of the machine architecture and its performance are described in <ref> [10] </ref> and [3]. This machine contains 40 MhZ two-way superscalar processors. Accesses to the sub-cache require 2 clock cycles, and accesses to the local cache require 20 cycles. The time to access another processor's cache depends on the ring traffic. <p> Accesses to the sub-cache require 2 clock cycles, and accesses to the local cache require 20 cycles. The time to access another processor's cache depends on the ring traffic. A cache miss serviced by a processor on the same ring takes approximately 175 cycles <ref> [10] </ref>, and a cache miss serviced by a processor on another ring requires approximately 600 cycles. In the discussion that follows, a cache miss refers to a miss in the 32 Megabyte cache, not the sub-cache.
Reference: [11] <author> H. Mehl and S. Hammes. </author> <title> Shared variables in dis tributed simulation. </title> <booktitle> In 7 th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 68-75. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers. While prior work in this area has focused on data structures [4], synchronization [9], or implementation of shared state <ref> [7, 11] </ref>, we are concerned here with efficient buffer management strategies for message passing in shared-memory machines. We assume that the hardware platform is a cache-coherent, shared-memory multiprocessor. The commercial machines mentioned earlier are all of this type.
Reference: [12] <author> M. Presley, M. Ebling, F. Wieland, and D. R. Jef-ferson. </author> <title> Benchmarking the Time Warp Operating System with a computer network simulation. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 8-13. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [8]. Time Warp has demonstrated some success in speeding up simulations of combat models [14], communication networks <ref> [12] </ref>, queuing networks [4], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers.
Reference: [13] <author> H. S. Stone. </author> <title> High-Performance Computer Archi--tecture. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1990. </year>
Reference-contexts: It is assumed that some mechanism is in place to ensure that duplicate copies of the same memory location in different caches remain consistent. This is typically accomplished by either invalidating copies in other caches when one processor modifies the block, or updating duplicate copies <ref> [13] </ref>. We are particularly concerned with large-scale, small-granularity discrete-event simulation applications. Specifically, we envision applications containing thousands to tensor hundreds- of thousands of simulator objects, but only a modest amount of computation per simulator event.
Reference: [14] <author> F. Wieland, L. Hawley, A. Feinberg, M. DiLorento, L. Blume, P. Reiher, B. Beckman, P. Hontalas, S. Bellenot, and D. R. Jefferson. </author> <title> Distributed combat simulation and Time Warp: The model and its performance. </title> <booktitle> In Proceedings of the SCS Multiconference on Distributed Simulation, </booktitle> <volume> volume 21, </volume> <pages> pages 14-20. </pages> <booktitle> SCS Simulation Series, </booktitle> <month> March </month> <year> 1989. </year>
Reference-contexts: Time Warp is a well known synchronization protocol that detects out-of-order executions of events as they occur, and recovers using a rollback mechanism [8]. Time Warp has demonstrated some success in speeding up simulations of combat models <ref> [14] </ref>, communication networks [12], queuing networks [4], and digital logic circuits [1], among others. We assume that the reader is familiar with the Time Warp mechanism described in [8]. Here, we are concerned with the efficient implementation of Time Warp on shared-memory multiprocessor computers.
Reference: [15] <author> Z. Xiao and F. Gomes. </author> <title> Benchmarking smtw with a ss7 performance model simulation, </title> <note> Fall 1993. unpublished project report for CPSC 601.24. </note>
Reference-contexts: For example, a processor accumulating too many buffers can place extras into the global pool, while those with diminished buffer pools can extract additional buffers, as needed, from the pool <ref> [15] </ref>. Thus, the principal advantages of the sender pool are elimination of the lock on the free pool, and better cache behavior for multiprocessors using cache invalidation protocols. The central disadvantage is the overhead for buffer redistribution. 3.3 Cache Update Protocols We previously described cache behavior in invalidate-based coherence protocols.
References-found: 15

