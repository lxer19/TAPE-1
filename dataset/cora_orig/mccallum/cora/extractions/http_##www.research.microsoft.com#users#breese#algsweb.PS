URL: http://www.research.microsoft.com/users/breese/algsweb.PS
Refering-URL: http://www.research.microsoft.com/users/breese/cfalgs.html
Root-URL: http://www.research.microsoft.com
Email: fbreese,heckerma,carlkg@microsoft.com  
Title: Empirical Analysis of Predictive Algorithms for Collaborative Filtering  
Author: John S. Breese David Heckerman Carl Kadie 
Note: Appears in Proceedings of the Fourteenth Conference on Uncertainty in Artificial Intelligence, Madison, WI, July, 1998. Morgan Kaufmann Publisher.  
Address: Redmond, WA 98052-6399  
Affiliation: Microsoft Research  
Abstract: Collaborative filtering or recommender systems use a database about user preferences to predict additional topics or products a new user might like. In this paper we describe several algorithms designed for this task, including techniques based on correlation coefficients, vector-based similarity calculations, and statistical Bayesian methods. We compare the predictive accuracy of the various methods in a set of representative problem domains. We use two basic classes of evaluation metrics. The first characterizes accuracy over a set of individual predictions in terms of average absolute deviation. The second estimates the utility of a ranked list of suggested items. This metric uses an estimate of the probability that a user will see a recommendation in an ordered list. Experiments were run for datasets associated with 3 application areas, 4 experimental protocols, and the 2 evaluation met-rics for the various algorithms. Results indicate that for a wide range of conditions, Bayesian networks with decision trees at each node and correlation methods outperform Bayesian-clustering and vector-similarity methods. Between correlation and Bayesian networks, the preferred method depends on the nature of the dataset, nature of the application (ranked versus one-by-one presentation), and the availability of votes with which to make predictions. Other considerations include the size of database, speed of predictions, and learning time. 
Abstract-found: 1
Intro-found: 1
Reference: [Breese et al., 1998] <author> Breese, J., Heckerman, D., and Kadie, C. </author> <month> (May, </month> <year> 1998). </year> <title> An experimental comparison of collaborative filtering methods. </title> <type> Technical Report MSR-TR-98-12, </type> <institution> Microsoft Research, Red-mond, WA. </institution>
Reference: [Cheeseman and Stutz, 1995] <author> Cheeseman, P. and Stutz, J. </author> <year> (1995). </year> <title> Bayesian classification (Auto-Class): Theory and results. </title> <editor> In Fayyad, U., Piatesky-Shapiro, G., Smyth, P., and Uthurusamy, R., editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference: [Chickering and Heckerman, 1997] <author> Chickering, D. and Heckerman, D. </author> <year> (1997). </year> <title> Efficient approximations for the marginal likelihood of Bayesian networks with hidden variables. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 181-212. </pages>
Reference: [Chickering et al., 1997] <author> Chickering, D., Heckerman, D., and Meek, C. </author> <year> (1997). </year> <title> A Bayesian approach to learning Bayesian networks with local structure. </title> <booktitle> In Proceedings of Thirteenth Conference on Uncertainty in Artificial Intelligence, </booktitle> <address> Providence, RI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [Dempster et al., 1977] <author> Dempster, A., Laird, N., and Rubin, D. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> B 39 </volume> <pages> 1-38. </pages>
Reference-contexts: Since we never observe the class variables in the database of users, we must employ methods that can learn parameters for models with hidden variables. We use the EM algorithm <ref> [Dempster et al., 1977] </ref> to learn the parameters for a model structure with a fixed number of classes. We choose the number of classes by selecting the model structure that yields the largest (approximate) marginal likelihood of the data. We watched "Melrose Place", with parents "Friend's", and "Beverly Hills, 90201".
Reference: [L.Terveen et al., 1997] <author> L.Terveen, Hill, W., Amento, B., McDconald, D., and Creter, J. </author> <year> (1997). </year> <title> PHOAKS: A system for sharing recommendations. </title> <journal> Communications of the ACM, </journal> <volume> 40(3) </volume> <pages> 59-62. </pages>
Reference-contexts: A second class of collaborative filtering applications present the user with an ordered list of recommended items. Examples of systems that present recommendation lists include PHOAKS <ref> [L.Terveen et al., 1997] </ref> and SiteSeer [Rucker and Polanco, 1997]. In the spirit of the Internet search engines, these systems provide a ranked list of items (Web sites, music recordings) where highest ranked items are predicted to be most preferred.
Reference: [McClave and Dietrich, 1988] <author> McClave, J. T. and Di-etrich, F. H. </author> <year> (1988). </year> <title> Statistics. </title> <publisher> Dellen Publishing Company, </publisher> <address> San Francisco, </address> <note> fourth edition. </note>
Reference-contexts: We use randomized block design where each algorithm is run on the same test cases and observed votes. We will refer to one of these comparisons as an experiment. Our analyses uses ANOVA with the Bon-ferroni procedure for multiple comparisons statistics <ref> [McClave and Dietrich, 1988] </ref>. In the tables that follow, the value in the last row is labeled RD for Required Difference.
Reference: [Miller et al., 1997] <author> Miller, B., Riedl, J., and Konstan, J. </author> <year> (1997). </year> <title> Experiences with GroupLens: Making Usenet useful again. </title> <booktitle> In Proceeding of the USENIX 1997 Annual Technical Conference, </booktitle> <pages> pages 219-231, </pages> <address> Anaheim, CA. </address> <note> [Resnick et al., 1994] Resnick, </note> <author> P., Iacovou, N., Suchak, M., Bergstrom, P., and Riedl, J. </author> <year> (1994). </year> <title> Grouplens: An open architecture for collaborative filtering of netnews. </title> <booktitle> In Proceedings of the ACM 1994 Conference on Computer Supported Cooperative Work, </booktitle> <pages> pages 175-186, </pages> <address> New York. </address> <publisher> ACM. </publisher>
Reference-contexts: This metric was also used in evaluating the GroupLens project <ref> [Miller et al., 1997] </ref>. For ranked scoring, the story is a bit more complex. In information retrieval research, ranked lists of returned items are evaluated in terms of recall and precision.
Reference: [Resnick and Varian, 1997] <author> Resnick, P. and Varian, H. </author> <year> (1997). </year> <journal> Recommender systems. Communications of the ACM, </journal> <volume> 40(3) </volume> <pages> 56-58. </pages>
Reference-contexts: Typically, these systems do not use any information regarding the actual content (e.g. words, author, description) of the items, but are rather based on usage or preference patterns of other users. So called collaborative filtering or recommender systems <ref> [Resnick and Varian, 1997] </ref> are built on the assumption that a good way to find interesting content is to find other people who have similar interests, and then recommend titles that those similar users like.
Reference: [Rucker and Polanco, 1997] <author> Rucker, J. and Polanco, M. J. </author> <year> (1997). </year> <title> Siteseer: Personalized navigation of the web. </title> <journal> Communications of the ACM, </journal> <volume> 40(3) </volume> <pages> 56-58. </pages>
Reference-contexts: A second class of collaborative filtering applications present the user with an ordered list of recommended items. Examples of systems that present recommendation lists include PHOAKS [L.Terveen et al., 1997] and SiteSeer <ref> [Rucker and Polanco, 1997] </ref>. In the spirit of the Internet search engines, these systems provide a ranked list of items (Web sites, music recordings) where highest ranked items are predicted to be most preferred.
Reference: [Salton and McGill, 1983] <author> Salton, G. and McGill, M. </author> <year> (1983). </year> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <address> New York. </address>
Reference-contexts: for which both users a and i have recorded votes. 2.1.2 Vector Similarity In the field of information retrieval, the similarity between two documents is often measured by treating each document as a vector of word frequencies and computing the cosine of the angle formed by the two frequency vectors <ref> [Salton and McGill, 1983] </ref>. We can adopt this formalism to collaborative filtering, where users take the role of documents, titles take the role of words, and votes take the role of word frequencies. <p> only calculate weights for users who match the active user on at least one item. 1 In our experiments, we have used a value of 10,000 or k. 2.2.2 Inverse User Frequency In applications of vector similarity in information retrieval, word frequencies are typically modified by the inverse document frequency <ref> [Salton and McGill, 1983] </ref>. The idea is to reduce weights for commonly occurring words, capturing the intuition that they are not as useful in identifying the topic of a document, while words that occur less frequently are more indicative of topic.
Reference: [Thiesson et al., 1997] <author> Thiesson, B., Meek, C., Chick-ering, D., and Heckerman, D. </author> <month> (December, </month> <year> 1997). </year> <title> Learning mixtures of DAG models. </title> <type> Technical Report MSR-TR-97-30, </type> <institution> Microsoft Research, Red-mond, WA. </institution>
Reference-contexts: In our experiments, we assume each model structure (every possible number of classes) is equally likely, and use a uniform prior for model parameters. We initialize the EM algorithm using the marginal-plus-noise technique described in <ref> [Thiesson et al., 1997] </ref>. 2.3.2 Bayesian Network Model An alternative model formulation for probabilistic collaborative filtering is a Bayesian network with a node corresponding to each item in the domain. The states of each node correspond to the possible vote values for each item.
References-found: 12

