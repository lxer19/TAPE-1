URL: http://www.cs.toronto.edu/~mackay/inter.nc.ps.gz
Refering-URL: http://www.cs.toronto.edu/~mackay/README.html
Root-URL: http://www.cs.toronto.edu
Email: mackay@hope.caltech.edu  
Title: Bayesian Interpolation  
Author: David J.C. MacKay 
Address: Pasadena CA 91125  
Affiliation: Computation and Neural Systems California Institute of Technology 139-74  
Abstract: Although Bayesian analysis has been in use since Laplace, the Bayesian method of model-comparison has only recently been developed in depth. In this paper, the Bayesian approach to regularisation and model-comparison is demonstrated by studying the inference problem of interpolating noisy data. The concepts and methods described are quite general and can be applied to many other data modelling problems. Regularising constants are set by examining their posterior probability distribution. Alternative regularisers (priors) and alternative basis sets are objectively compared by evaluating the evidence for them. `Occam's razor' is automatically embodied by this process. The way in which Bayes infers the values of regularising constants and noise levels has an elegant interpretation in terms of the effective number of parameters determined by the data set. This framework is due to Gull and Skilling. 
Abstract-found: 1
Intro-found: 1
Reference: <author> H. </author> <title> Akaike (1970). `Statistical predictor identification', </title> <journal> Ann. Inst. Statist. Math. </journal> <volume> 22, </volume> <pages> 203-217 J. </pages> <month> Berger </month> <year> (1985). </year> <title> Statistical decision theory and Bayesian analysis, </title> <publisher> Springer. </publisher>
Reference: <author> G.L. </author> <title> Bretthorst (1990). `Bayesian Analysis. I. Parameter Estimation Using Quadrature NMR Models. II. Signal Detection and Model Selection. III. Applications to NMR.', </title> <journal> J. </journal> <volume> Magnetic Resonance 88 3, </volume> <pages> 533-595. </pages>
Reference-contexts: to get the variance on y (x), which for any x is a linear function 12 This approximation is valid when, in the spectrum of eigenvalues of fiB, the number of eigenvalues within e-fold of ^ff is O (1). 13 There are analytic methods for performing such integrals over fi <ref> (Bretthorst, 1990) </ref>. 18 of the parameters, y (x) = P h OE h (x)w h . The error bars at a single point x are given by var y (x) = OE T A 1 OE.
Reference: <editor> M.K. </editor> <title> Charter (1991). `Quantifying drug absorption', </title> <editor> in Grandy and Schick, eds. </editor> <year> (1991), </year> <pages> 245-252. </pages>
Reference: <author> R.T. </author> <title> Cox (1964). `Probability, frequency, and reasonable expectation', Am. J. Physics 14, 1-13 A.R. Davies and R.S. Anderssen (1986). `Optimization in the regularization of ill-posed problems', </title> <journal> J. Austral. Mat. Soc. Ser. </journal> <volume> B 28, </volume> <pages> 114-133. </pages>
Reference: <author> R.L. </author> <month> Eubank </month> <year> (1988). </year> <title> `Spline smoothing and non-parametric regression', </title> <publisher> Marcel Dekker. </publisher>
Reference-contexts: Gull (1989a) has demonstrated why the popular use of misfit criteria is incorrect and how Bayes sets these parameters. The use of test data may be an unreliable technique unless large quantities of data are available. Cross-validation, the orthodox `method of choice' <ref> (Eubank, 1988) </ref>, will be discussed more in section 6 and (MacKay, 1991b). I will explain the Bayesian method of inferring ff and fi after first reviewing some statistics of misfit.
Reference: <editor> W.T. Grandy, Jr. and L.H. Schick, eds. </editor> <year> (1991). </year> <title> Maximum entropy and Bayesian methods, Laramie 1990, </title> <publisher> Kluwer. </publisher>
Reference: <author> S.F. </author> <title> Gull (1988). `Bayesian inductive inference and maximum entropy', </title> <booktitle> in Maximum entropy and Bayesian methods in science and engineering, </booktitle> <volume> vol. 1: </volume> <booktitle> Foundations, </booktitle> <editor> G.J. Er-ickson and C.R. Smith, eds., </editor> <publisher> Kluwer. </publisher>
Reference-contexts: Bayesian methods automatically and quantitatively embody Occam's razor <ref> (Gull, 1988, Jeffreys, 1939) </ref>, without the introduction of ad hoc penalty terms. Complex models are automatically self-penalising under Bayes' rule. Figure 2 gives the basic intuition for why this should be expected; the rest of this paper will explore this property in depth. <p> jH i ) ' P (D jw MP ; H i ) P (w MP jH i ) w Evidence ' Best fit likelihood Occam factor (5) Thus the evidence is found by taking the best fit likelihood that the model can achieve and multiplying it by an `Occam factor' <ref> (Gull, 1988) </ref>, which is a term with magnitude less than one that penalises H i for having the parameter w. 6 w MP 0 w P (wjH i ) This figure shows the quantities that determine the Occam factor for a hypothesis H i having a single parameter w. <p> (w MP jH i ) = 1 0 w , and Occam factor = w ; i.e. the ratio of the posterior accessible volume of H i 's parameter space to the prior accessible volume, or the factor by which H i 's hypothesis space collapses when the data arrive <ref> (Gull, 1988, Jeffreys, 1939) </ref>. The model H i can be viewed as being composed of a certain number of equivalent submodels, of which only one survives when the data arrive. The Occam factor is the inverse of that number. <p> Rather, we invent as many priors (regularisers) as we want, and allow the data to tell us which prior is most probable. Having said this, I would still recommend that the `maximum entropy principle' and other respected guides should be consulted when inventing these priors <ref> (see Gull, 1988, for example) </ref>. Evaluating the evidence for A; R As ff and fi vary, a single evidence maximum is obtained, at ^ff; ^ fi (at least for quadratic E D and E W ). <p> We note in table 1 the value of the maximum evidence achieved by these models, and move on to alternative models. The choice of orthonormal Legendre polynomials described above was motivated by a maximum entropy argument <ref> (Gull, 1988) </ref>. Models using other polynomial basis sets have also been tried. For less well motivated basis sets such as Hermite polynomials, it was found that the Occam factors were far bigger and the evidence was substantially smaller.
Reference: <author> S.F. </author> <title> Gull (1989). `Developments in maximum entropy data analysis', </title> <editor> in J. Skilling, ed. </editor> <year> (1989a), </year> <pages> 53-71. </pages>
Reference: <author> S.F. </author> <title> Gull (1989). `Bayesian data analysis: straight-line fitting', </title> <editor> in J. Skilling, ed. </editor> <year> (1989a), </year> <pages> 511-518. </pages>
Reference: <author> S.F. Gull and J. </author> <title> Skilling (1991). Quantified maximum entropy. </title> <note> MemSys5 user's manual. </note>
Reference: <author> M.E.D.C., </author> <title> 33 North End, Royston, </title> <address> SG8 6NR, England. </address>
Reference: <author> R. Hanson, J. </author> <title> Stutz and P.Cheeseman (1991). `Bayesian classification theory', </title> <institution> NASA Ames TR FIA-90-12-7-01. </institution>
Reference-contexts: The same approach to regularisation has also been developed in part by Szeliski (1989). Bayesian model comparison is also discussed by Bretthorst (1990), who has used Bayesian methods to push back the limits of NMR signal detection. The same Bayesian theory underlies the unsupervised classification system, AutoClass <ref> (Hanson et. al., 1991) </ref>. <p> For more general statistical models we still expect the posterior to be dominated by locally gaussian peaks on account of the central limit theorem (Walker, 1967). Multiple maxima which arise in more complex models complicate the analysis, but Bayesian methods can still successfully be applied <ref> (Hanson et. al., 1991, MacKay, 1991b, Neal, 1991) </ref>. 5 Thing is negligible alongside any sensible model. Models like Sure Thing are rarely seriously proposed in real life, but if such models are developed then clearly we need to think about precisely what priors are appropriate.
Reference: <author> D. Haussler, M. Kearns and R. </author> <title> Schapire (1991). `Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension', </title> <type> preprint. </type>
Reference: <author> E.T. </author> <title> Jaynes (1986). `Bayesian methods: general background', in Maximum entropy and Bayesian methods in applied statistics, </title> <editor> ed. J.H. Justice, C.U.P. </editor> <booktitle> 26 H. Jeffreys (1939). Theory of probability, </booktitle> <publisher> Oxford Univ. Press. </publisher>
Reference-contexts: This is the way in which alternative regularisers are compared, for example. If we try one model and obtain awful predictions, we have learnt something. `A failure of Bayesian prediction is an opportunity to learn' <ref> (Jaynes, 1986) </ref>, and we are able to come back to the same data set with new models, using new priors for example. Evaluating the evidence Let us now explicitly study the evidence to gain insight into how the Bayesian Occam's razor works.
Reference: <author> R.L. </author> <title> Kashyap (1977). `A Bayesian comparison of different classes of dynamic models using empirical data', </title> <journal> IEEE Transactions on Automatic Control AC-22 5, </journal> <pages> 715-727. </pages>
Reference: <author> T.J. </author> <month> Loredo </month> <year> (1989). </year> <title> `From Laplace to supernova SN 1987A: Bayesian inference in astrophysics', in Maximum entropy and Bayesian methods, </title> <editor> ed. P. Fougere, </editor> <publisher> Kluwer. </publisher>
Reference: <author> D.J.C. </author> <title> MacKay (1991b) `A practical Bayesian framework for backprop networks', Neural Computation, this volume. </title>
Reference-contexts: I hope that this review will help to introduce these techniques to the `neural' modelling community. A companion paper <ref> (MacKay, 1991b) </ref> will demonstrate how these techniques can be fruitfully applied to backpropagation neural networks. <p> The use of test data may be an unreliable technique unless large quantities of data are available. Cross-validation, the orthodox `method of choice' (Eubank, 1988), will be discussed more in section 6 and <ref> (MacKay, 1991b) </ref>. I will explain the Bayesian method of inferring ff and fi after first reviewing some statistics of misfit.
Reference: <author> D.J.C. </author> <title> MacKay (1991c) `Information-based objective functions for active data selection', Neural Computation, this volume. </title>
Reference-contexts: A companion paper (MacKay, 1991b) will demonstrate how these techniques can be fruitfully applied to backpropagation neural networks. Another paper will show how this framework relates to the task of selecting where next to gather data so as to gain maximal information about our models <ref> (MacKay, 1991c) </ref>. 2 The evidence and the Occam factor Let us write down Bayes' rule for the two levels of inference described above, so as to see explicitly how Bayesian model comparison works. Each model H i (H stands for `hypothesis') is assumed to have a vector of parameters w.
Reference: <author> R.M. </author> <title> Neal (1991). `Bayesian mixture modeling by Monte Carlo simulation', </title> <type> Preprint. </type>
Reference-contexts: For more general statistical models we still expect the posterior to be dominated by locally gaussian peaks on account of the central limit theorem (Walker, 1967). Multiple maxima which arise in more complex models complicate the analysis, but Bayesian methods can still successfully be applied <ref> (Hanson et. al., 1991, MacKay, 1991b, Neal, 1991) </ref>. 5 Thing is negligible alongside any sensible model. Models like Sure Thing are rarely seriously proposed in real life, but if such models are developed then clearly we need to think about precisely what priors are appropriate.
Reference: <author> D.B. Osteyee and I.J. </author> <title> Good (1974). Information, weight of evidence, the singularity between probability measures and signal detection, </title> <publisher> Springer. </publisher>
Reference-contexts: To be precise, the expectation over possible data sets of the log evidence for the true model is greater than the expectation of the log evidence for any other fixed model <ref> (Osteyee and Good, 1974) </ref>. 14 Proof. Suppose that the truth is actually H 1 . A single data set arrives and we compare the evidences for H 1 and H 2 , a different fixed model. Both models may have free parameters, but this will be irrelevant to the argument.
Reference: <author> J.D. Patrick and C.S. </author> <title> Wallace (1982). `Stone circle geometries: an information theory approach', in Archaeoastronomy in the Old World, D.C. </title> <editor> Heggie, editor, </editor> <publisher> Cambridge Univ. Press. </publisher>
Reference-contexts: Akaike's (1970) criterion can be viewed as an approximation to MDL (Schwarz, 1978, Zellner, 1984). Any implementation of MDL necessitates approximations in evaluating the length of the ideal shortest message. Although some of the earliest work on complex model comparison involved the MDL framework <ref> (Patrick and Wallace, 1982) </ref>, I can see no advantage in MDL, and recommend that the evidence should be approximated directly. * It should be emphasised that the Occam factor has nothing to do with how compu-tationally complex it is to use a model.
Reference: <author> T. Poggio, V. Torre and C. </author> <title> Koch (1985). `Computational vision and regularization theory', </title> <booktitle> Nature 317 6035, </booktitle> <pages> 314-319. </pages>
Reference-contexts: Error bars on the best fit interpolant 8 can be obtained from the Hessian of M , A = rrM , evaluated at w MP . This is the well known Bayesian view of regularisation <ref> (Poggio et. al., 1985, Titterington, 1985) </ref>. Bayes can do a lot more than just provide an interpretation for regularisation.
Reference: <author> J. </author> <title> Rissanen (1978). `Modeling by shortest data description', </title> <type> Automatica 14, </type> <pages> 465-471. </pages>
Reference-contexts: In these cases, the right hand side of equation (6) should be multiplied by the degeneracy of w MP to give the correct estimate of the evidence. * `Minimum description length' (MDL) methods are closely related to this Bayesian framework <ref> (Rissanen, 1978, Wallace and Boulton, 1968, Wallace and Freeman, 1987) </ref>. The log evidence log 2 P (DjH i ) is the number of bits in the ideal shortest message that encodes the data D using model H i .
Reference: <author> G. </author> <title> Schwarz (1978). `Estimating the dimension of a model', </title> <journal> Ann. Stat. </journal> <volume> 6 2, </volume> <pages> 461-464. </pages>
Reference-contexts: The log evidence log 2 P (DjH i ) is the number of bits in the ideal shortest message that encodes the data D using model H i . Akaike's (1970) criterion can be viewed as an approximation to MDL <ref> (Schwarz, 1978, Zellner, 1984) </ref>. Any implementation of MDL necessitates approximations in evaluating the length of the ideal shortest message.
Reference: <author> S. </author> <month> Sibisi </month> <year> (1991). </year> <title> `Bayesian interpolation', </title> <editor> in Grandy and Schick, eds. </editor> <year> (1991), </year> <pages> 349-355. </pages>
Reference-contexts: value to P (^ff; ^ fi); I assume that it is a flat prior (flat over log ff and log fi, since ff and fi are scale parameters) which cancels out when we compare alternative interpolation models. 6 Demonstration These demonstrations will use two one-dimensional data sets, in imitation of <ref> (Sibisi, 1991) </ref>. The first data set, `X,' has discontinuities in derivative (figure 4), and the second is a smoother data set, `Y' (figure 8). In all the demonstrations, fi was not left as a free parameter, but was fixed to its known true value. <p> It is possible to visualise the joint error bars on the interpolant by making typical samples from the posterior distribution, performing a random walk around the posterior `bubble' in parameter space <ref> (Sibisi, 1991, Skilling et. al., 1991) </ref>. Figure 8 shows data set Y interpolated by three typical interpolants found by random sampling from the posterior distribution.
Reference: <author> J. Skilling, </author> <title> editor (1989a). Maximum entropy and Bayesian methods, </title> <address> Cambridge 1988, </address> <publisher> Kluwer. </publisher>
Reference: <author> J. </author> <title> Skilling (1989b). `The eigenvalues of mega-dimensional matrices', </title> <editor> in J. Skilling, ed. </editor> <year> (1989a), </year> <pages> 455-466. </pages>
Reference-contexts: There is not one significant `subjective prior' in this entire paper. (If you are interested to see problems where significant subjective priors do arise see <ref> (Gull, 1989b, Skilling, 1989b) </ref>.) The emphasis is that consistent degrees of preference for alternative hypotheses are represented by probabilities, and relative preferences for models are assigned by evaluating those probabilities.
Reference: <author> J. </author> <title> Skilling (1991). `On parameter estimation and quantified MaxEnt', </title> <editor> in Grandy and Schick, eds. </editor> <year> (1991), </year> <pages> 267-273. </pages>
Reference-contexts: It is possible to visualise the joint error bars on the interpolant by making typical samples from the posterior distribution, performing a random walk around the posterior `bubble' in parameter space <ref> (Sibisi, 1991, Skilling et. al., 1991) </ref>. Figure 8 shows data set Y interpolated by three typical interpolants found by random sampling from the posterior distribution.
Reference: <author> J. Skilling, D.R.T. Robinson, </author> <title> and S.F. Gull (1991). `Probabilistic displays', </title> <editor> in Grandy and Schick, eds. </editor> <year> (1991), </year> <pages> 365-368. </pages>
Reference-contexts: It is possible to visualise the joint error bars on the interpolant by making typical samples from the posterior distribution, performing a random walk around the posterior `bubble' in parameter space <ref> (Sibisi, 1991, Skilling et. al., 1991) </ref>. Figure 8 shows data set Y interpolated by three typical interpolants found by random sampling from the posterior distribution.
Reference: <author> S.M. </author> <title> Stigler (1986). `Laplace's 1774 memoir on inverse probability', </title> <journal> Stat. Sci. </journal> <volume> 1 3, </volume> <pages> 359-378. </pages>
Reference-contexts: It is the process of marginalisation that corrects the bias of maximum likelihood. 10 Since ff and fi are scale parameters, this prior should be understood as a flat prior over log ff and log fi. 11 It is remarkable that Laplace almost got this right in 1774 <ref> (Stigler, 1986) </ref>; when inferring the mean of a Laplacian distribution, he both inferred the posterior probability of a nuisance parameter like fi in (15), and then attempted to integrate out the nuisance parameter as in equation (17). 13 -200 -100 0 100 200 Alpha Log Evidence Data X^2 X^2_w Log Occam
Reference: <author> R. </author> <month> Szeliski </month> <year> (1989). </year> <title> `Bayesian modeling of uncertainty in low level vision', </title> <publisher> Kluwer. </publisher>
Reference: <author> D. </author> <month> Titterington </month> <year> (1985). </year> <title> `Common structure of smoothing techniques in statistics', </title> <journal> Int. Statist. Rev. </journal> <volume> 53, </volume> <pages> 141-170. </pages>
Reference-contexts: Error bars on the best fit interpolant 8 can be obtained from the Hessian of M , A = rrM , evaluated at w MP . This is the well known Bayesian view of regularisation <ref> (Poggio et. al., 1985, Titterington, 1985) </ref>. Bayes can do a lot more than just provide an interpretation for regularisation.
Reference: <author> A.M. </author> <title> Walker (1967). `On the asymptotic behaviour of posterior distributions', </title> <journal> J. R. Stat. Soc. </journal> <volume> B 31, </volume> <pages> 80-88. </pages>
Reference-contexts: For the interpolation models discussed in this paper, there is only a single maximum in the posterior distribution, and the gaussian approximation is exact. For more general statistical models we still expect the posterior to be dominated by locally gaussian peaks on account of the central limit theorem <ref> (Walker, 1967) </ref>. Multiple maxima which arise in more complex models complicate the analysis, but Bayesian methods can still successfully be applied (Hanson et. al., 1991, MacKay, 1991b, Neal, 1991). 5 Thing is negligible alongside any sensible model. <p> As the amount of data collected, N , increases, this gaussian approximation is expected to become increasingly accurate on account of the central limit theorem <ref> (Walker, 1967) </ref>. For the linear interpolation models discussed in this paper, this gaussian expression is exact for any N . Comments * Bayesian model selection is a simple extension of maximum likelihood model selection: the evidence is obtained by multiplying the best fit likelihood by the Occam factor.
Reference: <author> C. S. Wallace and D. M. </author> <title> Boulton (1968). `An information measure for classification', </title> <journal> Comput. J. </journal> <volume> 11 2, </volume> <pages> 185-194. </pages>
Reference: <author> C. S. Wallace and P. R. </author> <title> Freeman (1987). `Estimation and Inference by Compact Coding', </title> <journal> J. R. Statist. Soc. </journal> <volume> B 49 3, </volume> <pages> 240-265. </pages>
Reference: <author> N. </author> <title> Weir (1991). `Applications of maximum entropy techniques to HST data', </title> <booktitle> Proceedings of the ESO/ST-ECF data analysis workshop, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: was used (Gull, 1989a), the results obtained were most dissatisfactory, making clear what a poor regulariser was being used; this motivated an immediate search for alternative priors; the new, more probable priors discovered by this search are now at the heart of the state of the art in image deconvolution <ref> (Weir, 1991) </ref>. The similarity between regularisation and `early stopping' While an over-parameterised model is fitted to a data set using gradient descent on the data error, it is sometimes noted that the model's generalisation error passes through a minimum, rather than decreasing monotonically.
Reference: <author> A. </author> <title> Zellner (1984). Basic issues in econometrics, </title> <publisher> Chicago. </publisher>
Reference-contexts: Jeffreys applied this theory to simple model comparison problems in geophysics, for example testing whether a single additional parameter is justified by the data. Since the 1960s, Jeffreys' model comparison methods have been applied and extended in the economics literature <ref> (Zellner, 1984) </ref>. Only recently has this aspect of Bayesian analysis been further developed and applied to more complex problems in other fields. 3 This paper will review Bayesian model comparison, `regularisation,' and noise estima-tion, by studying the problem of interpolating noisy data.
Reference: <author> I thank Mike Lewicki, Nick Weir and David R.T. </author> <title> Robinson for helpful conversations, and Andreas Herz for comments on the manuscript. </title> <note> I am grateful to Dr. </note> <author> R. Goodman and Dr. P. </author> <title> Smyth for funding my trip to Maxent 90. This work was supported by a Caltech Fellowship and a Studentship from SERC, </title> <address> UK. </address> <month> 27 </month>
References-found: 38

