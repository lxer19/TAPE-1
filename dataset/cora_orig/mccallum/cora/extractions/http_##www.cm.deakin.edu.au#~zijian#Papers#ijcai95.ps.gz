URL: http://www.cm.deakin.edu.au/~zijian/Papers/ijcai95.ps.gz
Refering-URL: http://www.cm.deakin.edu.au/~zijian/publications.html
Root-URL: 
Email: (zijian@cs.su.oz.au)  
Title: Constructing Nominal Xof-N Attributes  
Author: Zijian Zheng 
Date: 1064-1070, 1995.  
Note: In Proceedings of the 14th IJCAI, Morgan Kaufmann,  
Address: Sydney, NSW 2006 Australia  
Affiliation: Basser Department of Computer Science The University of  
Abstract: Most constructive induction researchers focus only on new boolean attributes. This paper reports a new constructive induction algorithm, called XofN, that constructs new nominal attributes in the form of Xof-N representations. An Xof-N is a set containing one or more attribute-value pairs. For a given instance, its value corresponds to the number of its attribute-value pairs that are true. The promising preliminary experimental results, on both artificial and real-world domains, show that constructing new nominal attributes in the form of Xof-N representations can significantly improve the performance of selective induction in terms of both higher prediction accuracy and lower theory complexity.
Abstract-found: 1
Intro-found: 1
Reference: [ Bloedorn et al., 1993 ] <author> E. Bloedorn, R.S. Michalski, and J. Wnek, </author> <title> Multistrategy constructive induction: </title> <booktitle> AQ17-MCI. Proceedings of the Second International Workshop on Multistrategy Learning, </booktitle> <pages> 188-203, </pages> <year> 1993. </year>
Reference-contexts: To the best of the author's knowledge, few researchers have developed constructive induction systems that construct new nominal attributes. One exception is that INDUCE, AQ17-DCI, and AQ17-MCI construct attribute counting attributes for rule learning <ref> [Michalski, 1978; Bloedorn et al., 1993] </ref>. They have ordered discrete values, but are used more like continuous-valued attributes rather than nominal attributes (see section 6 for the differences from Xof-N). <p> There are published results for more than 25 different learning algorithms on them. They represent three different types of learning tasks with two binary and four nominal attributes. To make the Monks2 problem harder, especially for simple M-of-N learning methods, <ref> [Bloedorn et al., 1993] </ref> creates 8 There are fifty four English phonemes, but phonemes for "Word Boundary" and "Period" do not appear in the dataset. 9 There are six English stresses, but one for "Word Bound ary" does not appear in the dataset. 10 The number of all possible phoneme-stress pairs <p> To explore how noise and irrelevant attributes affect the performance of learning algorithms on the Monks2 domain, we give our results of C4.5, CI, ID2-of-3, and XofN, and the results of AQ17-DCI, AQ17-HCI, and AQ17-MCI from <ref> [ Bloedorn et al., 1993 ] </ref> in Table 4. Only XofN learns the correct concept. Because of noise, the learned concept is not the perfect representation. <p> When building a decision tree, both ID2-of-3 and XofN construct one new attribute for each decision node using the local training set. Instead of building trees, MoN [ Ting, 1994 ] creates M-of-N rules. The production rule learning algorithms INDUCE [ Michalski, 1978 ] , AQ17-DCI, and AQ17-MCI <ref> [ Bloedorn et al., 1993 ] </ref> use the counting operator 12 #VarEQ (x) to construct new attributes that count the number of attributes in an instance which take the value x.
Reference: [ Breiman et al., 1984 ] <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone, </author> <title> Classification And Regression Trees, </title> <address> Belmont, CA: </address> <publisher> Wadsworth, </publisher> <year> 1984. </year>
Reference-contexts: Following the method used in [Dietterich et al., 1990] we generate data sets by using a window of length 7, but use the 1000 most common English words. On each real-world domain, a 10-fold cross-validation <ref> [Breiman et al., 1984] </ref> is conducted. In all the experiments reported here, C4.5, CI, ID2-of-3, and XofN are run with their default option settings, and are run on the same partitions for all the domains. No parameter-tuning is done here.
Reference: [ Brodley and Utgoff, 1992 ] <author> C.E. Brodley and P.E. Ut-goff, </author> <title> Multivariate versus univariate decision trees. </title> <type> COINS Technical Report 92-8, </type> <institution> Department of Computer Science, University of Massachusetts, Amherst, Massachusetts, USA, </institution> <year> 1992. </year>
Reference-contexts: M-of-N representations are also boolean attributes. A few systems such as BACON [ Langley et al., 1987 ] and INDUCE [Michalski, 1978] explore methods to construct new continuous-valued attributes using mathematical operators such as multiplication and division. Systems such as LMDT <ref> [Brodley and Utgoff, 1992] </ref> and Swap1 [Indurkhya and Weiss, 1991] construct linear discriminant functions as new attributes. To the best of the author's knowledge, few researchers have developed constructive induction systems that construct new nominal attributes. <p> After growing a tree, XofN applies the pruning mechanism used by C4.5 [ Quinlan, 1993 ] . Decision trees built by conventional tree learning algorithms such as C4.5 use a test based on one attribute at each decision node. They are called univariate trees <ref> [Brodley and Utgoff, 1992] </ref>. By contrast, XofN creates multivariate trees in which tests can refer to multiple attributes. We call them Xof-N trees as Xof-N representations are used as multivariate tests. <p> It achieved quite high prediction accuracy on a couple of real-world domains such as Pima Indians Diabetes, but the problem is that it has a sensitive parameter "Lookahead Depth" which needs to be set when applied to a domain. Another multivariate tree learning algorithm is LMDT <ref> [ Brodley and Utgoff, 1992 ] </ref> that generates a linear machine at each decision node when building a tree. 7 Conclusions and Future Work Selective induction algorithms build their theories by selecting primitive attributes and are thus limited for hard tasks whose primitive attributes are not sufficient for, or directly relevant
Reference: [ Catlett, 1991 ] <author> J. Catlett, </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <booktitle> Proceedings of the European Working Session on Learning, </booktitle> <pages> 164-178, </pages> <publisher> Springer Verlag, </publisher> <year> 1991. </year>
Reference-contexts: The reason is that the accuracy of XofN is higher than that of CI in some folds. 5 Other discretization methods that can be used are multi--interval discretization methods <ref> [Catlett, 1991; Fayyad and Irani, 1993] </ref>, supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method [Ragavan and Rendell, 1993]. The current XofN discretizes continuous-valued attributes statically in the sense that discretiza-tion occurs before new attribute construction. An alternative is dynamic discretization, i.e. doing discretization while constructing new attributes.
Reference: [ Dietterich et al., 1990 ] <author> T.G. Dietterich, H. Hild, and G. Bakiri, </author> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> 24-31, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: Phoneme and Stress are two basic subproblems of the Nettalk domain. Letter is a combination of them. Their tasks are mapping a letter in an English word into a phoneme, a stress, and a phoneme-stress pair respectively. Following the method used in <ref> [Dietterich et al., 1990] </ref> we generate data sets by using a window of length 7, but use the 1000 most common English words. On each real-world domain, a 10-fold cross-validation [Breiman et al., 1984] is conducted.
Reference: [ Fayyad and Irani, 1993 ] <author> U.M. Fayyad and K.B. Irani, </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1022-1027, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The reason is that the accuracy of XofN is higher than that of CI in some folds. 5 Other discretization methods that can be used are multi--interval discretization methods <ref> [Catlett, 1991; Fayyad and Irani, 1993] </ref>, supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method [Ragavan and Rendell, 1993]. The current XofN discretizes continuous-valued attributes statically in the sense that discretiza-tion occurs before new attribute construction. An alternative is dynamic discretization, i.e. doing discretization while constructing new attributes.
Reference: [ Indurkhya and Weiss, 1991 ] <author> N. Indurkhya and S.M. Weiss, </author> <title> Iterative rule induction methods. </title> <journal> Journal of Applied Intelligence, </journal> <volume> 1, </volume> <pages> 43-54, </pages> <year> 1991. </year>
Reference-contexts: M-of-N representations are also boolean attributes. A few systems such as BACON [ Langley et al., 1987 ] and INDUCE [Michalski, 1978] explore methods to construct new continuous-valued attributes using mathematical operators such as multiplication and division. Systems such as LMDT [Brodley and Utgoff, 1992] and Swap1 <ref> [Indurkhya and Weiss, 1991] </ref> construct linear discriminant functions as new attributes. To the best of the author's knowledge, few researchers have developed constructive induction systems that construct new nominal attributes.
Reference: [ Langley et al., 1987 ] <author> P. Langley, H.A. Simon, G.L. Bradshaw, and J.M. Zytkow, </author> <title> Scientific Discovery: Computational Explorations of the Creative Processes, </title> <publisher> Cambridge: MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: ID2-of-3 [ Murphy and Pazzani, 1991 ] creates, as new attributes, M-of-N representations stating whether at least M of N conditions are true. M-of-N representations are also boolean attributes. A few systems such as BACON <ref> [ Langley et al., 1987 ] </ref> and INDUCE [Michalski, 1978] explore methods to construct new continuous-valued attributes using mathematical operators such as multiplication and division. Systems such as LMDT [Brodley and Utgoff, 1992] and Swap1 [Indurkhya and Weiss, 1991] construct linear discriminant functions as new attributes.
Reference: [ Matheus and Rendell, 1989 ] <author> C.J. Matheus and L.A. Rendell, </author> <title> Constructive induction on decision trees. </title> <booktitle> Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 645-650, </pages> <year> 1989. </year>
Reference-contexts: However, many existing 1 Some more sophisticated attributes such as structured attributes may be used, but here we talk about only these three most commonly used types of attributes. constructive induction algorithms such as FRINGE [ Pa-gallo, 1990 ] and CITRE <ref> [ Matheus and Rendell, 1989 ] </ref> construct new boolean attributes only by using logical operators such as ^, :, and _. ID2-of-3 [ Murphy and Pazzani, 1991 ] creates, as new attributes, M-of-N representations stating whether at least M of N conditions are true. M-of-N representations are also boolean attributes. <p> Most hypothesis-driven constructive induction algorithms such as FRINGE [ Pagallo, 1990 ] , CITRE <ref> [ Matheus and Rendell, 1989 ] </ref> , CI [ Zheng, 1992 ] , and AQ17-HCI [ Wnek and Michalski, 1994 ] construct and select a set of new attributes based on the entire training set. <p> than other unselected new attributes for a training subset after a 12 In INDUCE, it is called #v COND. 13 Generated rules have the form like (#VarEQ (1) &gt;= 3) [ Thrun et al., 1991, p11 ] . part of a decision tree or a rule set has been created <ref> [ Matheus and Rendell, 1989 ] </ref> . To overcome this, XofN constructs one new attribute using the local training set for each decision node. Therefore, the new attribute constructed by XofN at each decision node is the best one in XofN's search space in terms of the evaluation function.
Reference: [ Michalski, 1978 ] <author> R.S. Michalski, </author> <title> Pattern recognition as knowledge-guided computer induction. </title> <type> Technical Reports: 927, </type> <institution> Department of Computer Science, The University of Illinois, Urbana, </institution> <year> 1978. </year>
Reference-contexts: 1 Introduction A well-known elementary limitation of selective induction algorithms is that when task-supplied attributes are not adequate for describing hypotheses, their performance in terms of prediction accuracy and/or theory complexity is poor. To overcome this limitation, constructive induction algorithms <ref> [ Michalski, 1978 ] </ref> transform the original instance space into a more adequate space by creating new attributes. By contrast to new attributes, the task-supplied attributes are called primitive attributes. <p> ID2-of-3 [ Murphy and Pazzani, 1991 ] creates, as new attributes, M-of-N representations stating whether at least M of N conditions are true. M-of-N representations are also boolean attributes. A few systems such as BACON [ Langley et al., 1987 ] and INDUCE <ref> [Michalski, 1978] </ref> explore methods to construct new continuous-valued attributes using mathematical operators such as multiplication and division. Systems such as LMDT [Brodley and Utgoff, 1992] and Swap1 [Indurkhya and Weiss, 1991] construct linear discriminant functions as new attributes. <p> To the best of the author's knowledge, few researchers have developed constructive induction systems that construct new nominal attributes. One exception is that INDUCE, AQ17-DCI, and AQ17-MCI construct attribute counting attributes for rule learning <ref> [Michalski, 1978; Bloedorn et al., 1993] </ref>. They have ordered discrete values, but are used more like continuous-valued attributes rather than nominal attributes (see section 6 for the differences from Xof-N). <p> When building a decision tree, both ID2-of-3 and XofN construct one new attribute for each decision node using the local training set. Instead of building trees, MoN [ Ting, 1994 ] creates M-of-N rules. The production rule learning algorithms INDUCE <ref> [ Michalski, 1978 ] </ref> , AQ17-DCI, and AQ17-MCI [ Bloedorn et al., 1993 ] use the counting operator 12 #VarEQ (x) to construct new attributes that count the number of attributes in an instance which take the value x.
Reference: [ Murphy and Aha, 1994 ] <author> P.M. Murphy and D.W. Aha, </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <note> Available by anonymous ftp at ics.uci.edu in the pub/machine-learning-databases directory, </note> <year> 1994. </year>
Reference-contexts: They are five medical domains (Cleve-land Heart Disease, Hepatitis, Liver Disorders, Pima Indians Diabetes, and Wisconsin Breast Cancer), one molecular biology domain (Promoters), three linguistics domains (Nettalk (Phoneme), Nettalk (Stress), and Nettalk (Letter)), and one game domain (Tic-Tac-Toe). All are from the UCI repository of machine learning databases <ref> [Murphy and Aha, 1994] </ref>. Table 2 gives a brief summary of the domains, including the data set size, the number of binary (B), nominal (N), continuous-valued (C), total (T) attributes, and the number of classes. Phoneme and Stress are two basic subproblems of the Nettalk domain.
Reference: [ Murphy and Pazzani, 1991 ] <author> P.M. Murphy and M.J. Pazzani, ID2-of-3: </author> <title> Constructive induction of M-of-N concepts for discriminators in decision trees. </title> <booktitle> Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <pages> 183-187, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: ID2-of-3 <ref> [ Murphy and Pazzani, 1991 ] </ref> creates, as new attributes, M-of-N representations stating whether at least M of N conditions are true. M-of-N representations are also boolean attributes.
Reference: [ Oliver et al., 1992 ] <author> J.J. Oliver, D.L. Dowe, and C.S. Wallace, </author> <title> Inferring decision graphs using the minimum message length principle. </title> <booktitle> Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> 361-367, </pages> <publisher> World Scientific Publisher, </publisher> <year> 1992. </year>
Reference-contexts: To apply XofN to domains containing complex DNF concepts with long terms, some mechanisms are necessary to overcome the "fragmentation" problem. One approach is using subset-ting of C4.5 [ Quinlan, 1993 ] . Two other possible solutions are using subranging and building decision graphs <ref> [ Oliver et al., 1992 ] </ref> instead of decision trees. Subrang-ing is similar to subsetting but also considers the order of the values of Xof-N representations.
Reference: [ Pagallo, 1990 ] <author> G. Pagallo, </author> <title> Adaptive Decision Tree Algorithms for Learning from Examples, </title> <type> Ph.D. thesis, </type> <institution> University of California at Santa Cruz, </institution> <year> 1990. </year>
Reference-contexts: Most hypothesis-driven constructive induction algorithms such as FRINGE <ref> [ Pagallo, 1990 ] </ref> , CITRE [ Matheus and Rendell, 1989 ] , CI [ Zheng, 1992 ] , and AQ17-HCI [ Wnek and Michalski, 1994 ] construct and select a set of new attributes based on the entire training set.
Reference: [ Quinlan and Rivest, 1989 ] <author> J. Ross. Quinlan and R.L. Rivest, </author> <title> Inferring decision trees using the minimum description length principle. </title> <journal> Information and Computation, </journal> <volume> 80, </volume> <pages> 227-248, </pages> <year> 1989. </year>
Reference-contexts: By complex, we mean that an Xof-N representation has a large number of attribute-value pairs. 6 This kind of new attribute splits the training set into many subsets, so over-fitting is likely to occur. We use a similar coding method described in <ref> [ Quinlan and Rivest, 1989 ] </ref> . The coding cost of a new attribute includes two parts. One is for coding the new attribute itself. The other is for coding the exceptions when applying the new attribute as a classifier to the local training data at the current decision node.
Reference: [ Quinlan, 1993 ] <author> J.R. Quinlan, C4.5: </author> <title> Programs for Machine Learning, </title> <address> San Meteo: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: They have ordered discrete values, but are used more like continuous-valued attributes rather than nominal attributes (see section 6 for the differences from Xof-N). Subsetting used by C4.5 groups discrete values of a single primitive nominal attribute to form a new test <ref> [ Quinlan, 1993 ] </ref> . It can be thought as a method of constructing new nominal attributes. The author knows of no other decision tree algorithm that constructs new nominal attributes. <p> If target concepts are not complex or huge data sets are available, it is not a problem. 3 Constructing New Nominal Attributes for Decision Trees Now, let us describe how to create and use Xof-N representations in constructive induction. XofN uses the well-known decision tree learning algorithm C4.5 <ref> [ Quinlan, 1993 ] </ref> as its selective induction component. It consists of a single process. As shown in Table 1, XofN builds a decision tree by constructing, at each decision node, one new attribute based on primitive attributes using the local training set. <p> After growing a tree, XofN applies the pruning mechanism used by C4.5 <ref> [ Quinlan, 1993 ] </ref> . Decision trees built by conventional tree learning algorithms such as C4.5 use a test based on one attribute at each decision node. They are called univariate trees [Brodley and Utgoff, 1992]. By contrast, XofN creates multivariate trees in which tests can refer to multiple attributes. <p> For comparison, we also give the results of some other constructive induction algorithms: AQ17-HCI, AQ17-DCI, AQ17-MCI, ID2-of-3, and our algorithm CI [ Zheng, 1992 ] . CI creates new attributes for decision trees by using conjunctions of conditions from production rules generated by C4.5rules <ref> [ Quinlan, 1993 ] </ref> . The default option setting of CI is used here. To create a new attribute, CI chooses two conditions, which are near the root of a tree, from a rule. <p> To apply XofN to domains containing complex DNF concepts with long terms, some mechanisms are necessary to overcome the "fragmentation" problem. One approach is using subset-ting of C4.5 <ref> [ Quinlan, 1993 ] </ref> . Two other possible solutions are using subranging and building decision graphs [ Oliver et al., 1992 ] instead of decision trees. Subrang-ing is similar to subsetting but also considers the order of the values of Xof-N representations.
Reference: [ Ragavan and Rendell, 1993 ] <author> H. Ragavan and L. Ren-dell, </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proceedings of the 10th International Conference on Machine Learning, </booktitle> <pages> 252-259, </pages> <year> 1993. </year>
Reference-contexts: It uses only C4.5, CI, and ID2-of-3 as references, since very few directly comparable results of other constructive induction algorithms are available from publications. Each value given is the average of a ten-fold cross-validation. It is worth mentioning that LFC <ref> [Ragavan and Rendell, 1993] </ref> achieves higher prediction accuracy (78.8%) than XofN on Pima Indians Diabetes domain (see section 6). To compare accuracies of XofN, ID2-of-3, CI, and C4.5, a two-tailed pairwise t-test is used. <p> The reason is that the accuracy of XofN is higher than that of CI in some folds. 5 Other discretization methods that can be used are multi--interval discretization methods [Catlett, 1991; Fayyad and Irani, 1993], supervised/unsupervised methods [Van de Merckt, 1993], and an entropy method <ref> [Ragavan and Rendell, 1993] </ref>. The current XofN discretizes continuous-valued attributes statically in the sense that discretiza-tion occurs before new attribute construction. An alternative is dynamic discretization, i.e. doing discretization while constructing new attributes. This method might be able to create good discretizations but with an increased computational complexity. <p> Like ID2-of-3 and XofN, LFC <ref> [ Ragavan and Rendell, 1993 ] </ref> is also a data-driven constructive induction algorithm that builds multivariate trees, but it uses negation and conjunction as constructive operators. LFC creates one conjunction for each decision node by using a directed lookahead search.
Reference: [ Spackman, 1988 ] <author> K.A. Spackman, </author> <title> Learning categorical decision criteria in biomedical domains. </title> <booktitle> Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> 36-46, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Compared to ID2-of-3, XofN learns smaller trees on seven out of ten domains. 5 Discussion It has been found that XofN works quite well on a set of artificial and real-world domains. We expect that XofN can be applied to domains containing M-of-N concepts, such as biomedical domains <ref> [ Spackman, 1988 ] </ref> , linguistic domains, and domains containing parity concepts found for example in digital logic circuit design. To apply XofN to domains containing complex DNF concepts with long terms, some mechanisms are necessary to overcome the "fragmentation" problem.
Reference: [ Thrun et al., 1991 ] <author> S.B. Thrun, J. Bala, E. Bloedorn, I. Bratko, B. Cestnik, J. Cheng, K. De Jong, S. Dzeroski, S.E. Fahlman, D. Fisher, R. Hamann, K. Kaufman, S. Keller, I. Kononenko, J. Kreuziger, R.S. Michalski, T. Mitchell, P. Pachowicz, Y. Reich, H. Vafaie, W. Van de Welde, W. Wenzel, J. Wnek, and J. Zhang, </author> <title> The MONK's problems a performance comparison of different learning algorithms. </title> <type> Tech. Report: </type> <institution> CMU-CD-91-197, Carnegie Mellon University, </institution> <year> 1991. </year>
Reference-contexts: An attribute-value pair AV k (A i = V ij ) is true for an instance iff attribute A i of the instance has value V ij . Now, let us see an example of the Xof-N representation. The target concept of the Monks2 problem <ref> [ Thrun et al., 1991 ] </ref> is "exactly two of the six attributes have their first value". It can be represented using a tree as in Figure 1. The test at the root is a nominal Xof-N representation. <p> With this setting, CI constructs new attributes based on two primitive attributes, and identifies relevant attributes [ Zheng, 1992 ] . The results of C4.5 are given for reference. 4.1 Experimental domains and methods Three Monks domains <ref> [ Thrun et al., 1991 ] </ref> are chosen because they are well-studied. There are published results for more than 25 different learning algorithms on them. They represent three different types of learning tasks with two binary and four nominal attributes. <p> The Monks domains Table 3 summarizes the accuracy (Acc) and complexity (Com) of C4.5, AQ17-DCI, AQ17-HCI, CI, ID2-of-3, and XofN on three monks problems. The results of AQ17-DCI and AQ17-HCI are from <ref> [Thrun et al., 1991] </ref>. The Table shows that only XofN solves all three problems with correct representations. As we expected, XofN finds a perfect representation of the target concept on Monks2. <p> shortcoming: new attributes that have high values of the evaluation function for the entire training set might have lower values than other unselected new attributes for a training subset after a 12 In INDUCE, it is called #v COND. 13 Generated rules have the form like (#VarEQ (1) &gt;= 3) <ref> [ Thrun et al., 1991, p11 ] </ref> . part of a decision tree or a rule set has been created [ Matheus and Rendell, 1989 ] . To overcome this, XofN constructs one new attribute using the local training set for each decision node.
Reference: [ Ting, 1994 ] <author> K.M. Ting, </author> <title> An M-of-N rule induction algorithm and its application to DNA domain. </title> <booktitle> Proceedings of the 27th Annual Hawaii International Conference on System Sciences, Volume V: Biotechnology Computing, </booktitle> <pages> 133-140, </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Some algorithms also have _ as a constructive operator that makes it easier to create disjunctions. However, there are still some other concepts such as parity concepts, at-least, exactly, at-most M-of-N concepts, and their possible combinations, that cannot be effectively represented. ID2-of-3 and MoN <ref> [ Ting, 1994 ] </ref> can only create at-least M-of-N representations. From the definition, we can see that Xof-N can directly represent all the following types of concepts. 2 At the moment, the Xof-N representation is defined on binary and nominal attributes. <p> It constructs new binary attributes in the form of M-of-N representations, while XofN constructs Xof-N representations. When building a decision tree, both ID2-of-3 and XofN construct one new attribute for each decision node using the local training set. Instead of building trees, MoN <ref> [ Ting, 1994 ] </ref> creates M-of-N rules.
Reference: [ Van de Merckt, 1993 ] <author> Thierry Van de Merckt, </author> <title> Decision trees in numerical attribute spaces. </title> <booktitle> Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> 1016-1021, </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The reason is that the accuracy of XofN is higher than that of CI in some folds. 5 Other discretization methods that can be used are multi--interval discretization methods [Catlett, 1991; Fayyad and Irani, 1993], supervised/unsupervised methods <ref> [Van de Merckt, 1993] </ref>, and an entropy method [Ragavan and Rendell, 1993]. The current XofN discretizes continuous-valued attributes statically in the sense that discretiza-tion occurs before new attribute construction. An alternative is dynamic discretization, i.e. doing discretization while constructing new attributes.
Reference: [ Wnek and Michalski, 1994 ] <author> J. </author> <title> Wnek and R.S. Michal-ski, Hypothesis-driven constructive induction in AQ17-HCI: a method and experiments. </title> <journal> Machine Learning, </journal> <volume> 14:2, </volume> <pages> 139-168, </pages> <publisher> Kluwer Academic, </publisher> <year> 1994. </year>
Reference-contexts: Most hypothesis-driven constructive induction algorithms such as FRINGE [ Pagallo, 1990 ] , CITRE [ Matheus and Rendell, 1989 ] , CI [ Zheng, 1992 ] , and AQ17-HCI <ref> [ Wnek and Michalski, 1994 ] </ref> construct and select a set of new attributes based on the entire training set.
Reference: [ Zheng, 1992 ] <author> Z. Zheng, </author> <title> Constructing conjunctive tests for decision trees. </title> <booktitle> Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> 355-360, </pages> <publisher> World Scientific Publisher, </publisher> <year> 1992. </year>
Reference-contexts: As a nominal attribute, the Xof-N representation has one main advantage over conjunction, disjunction, and M-of-N representations: stronger expressive power without expanding the search space. Many constructive induction algorithms such as FRINGE, CITRE, and CI <ref> [ Zheng, 1992 ] </ref> use ^ and : as constructive operators. They can only indirectly represent disjunctive concepts by using the negation of a conjunction. This makes it harder to construct new attributes in the form of disjunctions, especially on domains with primitive nominal attributes. <p> For comparison, we also give the results of some other constructive induction algorithms: AQ17-HCI, AQ17-DCI, AQ17-MCI, ID2-of-3, and our algorithm CI <ref> [ Zheng, 1992 ] </ref> . CI creates new attributes for decision trees by using conjunctions of conditions from production rules generated by C4.5rules [ Quinlan, 1993 ] . The default option setting of CI is used here. <p> The default option setting of CI is used here. To create a new attribute, CI chooses two conditions, which are near the root of a tree, from a rule. With this setting, CI constructs new attributes based on two primitive attributes, and identifies relevant attributes <ref> [ Zheng, 1992 ] </ref> . The results of C4.5 are given for reference. 4.1 Experimental domains and methods Three Monks domains [ Thrun et al., 1991 ] are chosen because they are well-studied. There are published results for more than 25 different learning algorithms on them. <p> Most hypothesis-driven constructive induction algorithms such as FRINGE [ Pagallo, 1990 ] , CITRE [ Matheus and Rendell, 1989 ] , CI <ref> [ Zheng, 1992 ] </ref> , and AQ17-HCI [ Wnek and Michalski, 1994 ] construct and select a set of new attributes based on the entire training set.
Reference: [ Zheng, 1995 ] <author> Z. Zheng, </author> <title> Continuous-valued Xof-N Attributes Versus Nominal Xof-N Attributes for Constructive Induction: A Case Study. </title> <booktitle> Proceedings of the Fourth International Conference for Young Computer Scientists, </booktitle> <publisher> Peking University Press, </publisher> <year> 1995. </year> <month> 7 </month>
Reference-contexts: However, on domains requiring Xof-N representations with more than one cut point, the continuous-valued Xof-N has weaker expressive power than the nominal Xof-N. For details on this issue, please see <ref> [ Zheng, 1995 ] </ref> . 6 Related Work The closest related work is ID2-of-3 [ Murphy and Paz-zani, 1991 ] . It constructs new binary attributes in the form of M-of-N representations, while XofN constructs Xof-N representations.
References-found: 24

