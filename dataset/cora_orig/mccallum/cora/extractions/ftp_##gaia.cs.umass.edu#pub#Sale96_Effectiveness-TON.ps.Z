URL: ftp://gaia.cs.umass.edu/pub/Sale96:Effectiveness-TON.ps.Z
Refering-URL: http://www-net.cs.umass.edu/papers/papers.html
Root-URL: 
Title: The Effectiveness of Affinity-Based Scheduling in Multiprocessor Networking (Extended Version)  
Author: James D. Salehi, James F. Kurose, and Don Towsley 
Abstract: Techniques for avoiding the high memory overheads found on many modern shared-memory multiprocessors are of increasing importance in the development of high-performance multiprocessor protocol implementations. One such technique is processor-cache affinity scheduling, which can significantly lower packet latency and substantially increase protocol processing throughput [30]. In this paper, we evaluate several aspects of the effectiveness of affinity-based scheduling in multiprocessor network protocol processing, under packet-level and connection-level par-allelization approaches. Specifically, we evaluate the performance of the scheduling technique 1) when a large number of streams are concurrently supported, 2) when processing includes copying of uncached packet data, 3) as applied to send-side protocol processing, and 4) in the presence of stream burstiness and source locality, two well-known properties of network traffic. We find that affinity-based scheduling performs well under these conditions, emphasizing its robustness and general effectiveness in multiprocessor network processing. In addition, we explore a technique which improves the caching behavior and available packet-level concurrency under connection-level parallelism, and find performance improves dramatically. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Mark B. Abbott and Larry L. Peterson. </author> <title> Increasing network throughput by integrating protocol layers. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(5) </volume> <pages> 600-610, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: In response, researchers have examined the design of hardware network interfaces, low-level software drivers, and higher-level software protocol layers toward reducing the amount of data movement in network communication (e.g., <ref> [36, 7, 1, 18] </ref>). Other researchers have explored improvements to the "non-data-touching" components of protocol processing. These involve processing overheads which are independent of packet size, including protocol-specific functionality (e.g., demultiplex-ing, sequencing), protocol control operations, buffer management, operating system functions, and so on. <p> Although UDP supports checksumming, we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP and TCP [17]: * Checksumming can be combined with data copying in integrated layer processing (ILP) <ref> [4, 1] </ref>.
Reference: [2] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: To illustrate a performance lower bound, we consider a global organization managed either MRU or LRU, under which thread stacks tend to migrate among processors. (As an aside, note that while the load-balancing and synchronization benefits of per-processor thread pools have been explored <ref> [2] </ref>, the cache affinity bene fits have not previously been evaluated). * Stream affinity (i.e., avoidance of coherence-based misses on initial writes to per-stream data structures) is gained by "wiring" streams to processors|requiring all protocol processing for a given stream to be per 3 Not all read-only references are eligible. <p> Among them are how to load-balance the resources among the pools, how to find free resources when a local pool is empty, and how to efficiently synchronize pool access among processors <ref> [2] </ref>. An evaluation of these and related implementation issues would best be conducted within the context of an actual implementation and over real workloads. While such an evaluation is outside the scope of the present work, it clearly represents an interesting and important area of future study.
Reference: [3] <author> Mats Bjorkman and Per Gunningberg. </author> <title> Locking effects in multiprocessor implementations of protocols. </title> <booktitle> In Proc. ACM SIGCOMM, </booktitle> <pages> pages 74-83, </pages> <address> San Fran-cisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity.
Reference: [4] <author> D. D. Clark and D. L. Tennenhouse. </author> <title> Architectural considerations for a new generation of protocols. </title> <booktitle> In Proc. ACM SIGCOMM, </booktitle> <pages> pages 200-208, </pages> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Although UDP supports checksumming, we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP and TCP [17]: * Checksumming can be combined with data copying in integrated layer processing (ILP) <ref> [4, 1] </ref>.
Reference: [5] <author> Peter B. Danzig, Sugih Jamin, Ramon Caceres, Danny J. Mitzel, and Deborah Estrin. </author> <title> An empirical workload model for driving wide-area TCP/IP network simulations. </title> <journal> Journal of Internetworking: Research and Experience, </journal> <volume> 3(1) </volume> <pages> 1-26, </pages> <year> 1992. </year>
Reference-contexts: Although copying and checksumming (which scale with packet size) are expensive, the reason why non-data-touching overheads predominate is that typically in real environments most packets are small <ref> [5, 10, 17, 22] </ref>. 4 Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> In addition, they note the observed distribution matches the Ethernet traces reported in <ref> [5, 10, 22] </ref>. The packet size distribution, taken from Figure 5b of [17], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22].
Reference: [6] <author> Murthy Devarakonda and Arup Mukherjee. </author> <title> Issues in implementation of cache-affinity scheduling. </title> <booktitle> In Proc. Winter 1992 USENIX Conference, </booktitle> <pages> pages 345-357, </pages> <address> San Francicso, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> Four studies consider affinity-based scheduling at the process level <ref> [6, 9, 37, 39] </ref>, while 9 one considers the finer granularity of loop scheduling [21]. Vaswani and Zajorian [39] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone [38]. In <ref> [6] </ref>, Devarakonda and Mukherjee explore implementation issues in affinity-based scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited.
Reference: [7] <author> Peter Druschel, Mark B. Abbott, Michael A. Pagels, and Larry L. Peterson. </author> <title> Network subsystem design. </title> <journal> IEEE Network, </journal> <pages> pages 8-17, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In response, researchers have examined the design of hardware network interfaces, low-level software drivers, and higher-level software protocol layers toward reducing the amount of data movement in network communication (e.g., <ref> [36, 7, 1, 18] </ref>). Other researchers have explored improvements to the "non-data-touching" components of protocol processing. These involve processing overheads which are independent of packet size, including protocol-specific functionality (e.g., demultiplex-ing, sequencing), protocol control operations, buffer management, operating system functions, and so on. <p> Moreover, for the real-world workloads we consider, the benefits are high in comparison to techniques which aim to reduce data-movement overheads, such as single-copy or zero-copy protocol architectures <ref> [7] </ref>. In this section, we describe the setting in which we conduct our study.
Reference: [8] <author> Arun Garg. </author> <title> Parallel STREAMS: A multi-processor implementation. </title> <booktitle> In Proc. Winter 1990 USENIX Conference, </booktitle> <pages> pages 163-176, </pages> <address> Washington, D.C., </address> <month> January </month> <year> 1990. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level <ref> [8, 11, 28, 32, 34, 40] </ref> parallelisms enable concurrency at higher levels of granularity. In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor.
Reference: [9] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proc. ACM SIGMETRICS, </booktitle> <pages> pages 120-132, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue <ref> [9, 21, 37, 39] </ref>), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work [30, 31], we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. <p> Four studies consider affinity-based scheduling at the process level <ref> [6, 9, 37, 39] </ref>, while 9 one considers the finer granularity of loop scheduling [21]. Vaswani and Zajorian [39] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Although experimental measurements do not support advocacy of in-kernel affinity scheduling, within the user-level thread scheduler the authors find that affinity scheduling yields a 12% reduction in execution time for one of the two real applications. In a trace-driven simulation study, Gupta, Tucker and Urushibara <ref> [9] </ref> consider in-kernel affinity-based scheduling of parallel applications on a shared memory platform. Their approach is to simulate a multiprocessor system by interleaving the process execution traces obtained from a set of parallel scientific applications.
Reference: [10] <author> Riccardo Gusella. </author> <title> A measurement study of diskless workstation traffic on an Ethernet. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(9) </volume> <pages> 1557-1568, </pages> <month> Septem-ber </month> <year> 1990. </year>
Reference-contexts: Although copying and checksumming (which scale with packet size) are expensive, the reason why non-data-touching overheads predominate is that typically in real environments most packets are small <ref> [5, 10, 17, 22] </ref>. 4 Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> In addition, they note the observed distribution matches the Ethernet traces reported in <ref> [5, 10, 22] </ref>. The packet size distribution, taken from Figure 5b of [17], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22].
Reference: [11] <author> Ian Heavens. </author> <title> Experiences in parallelisation of streams-based communications drivers. </title> <booktitle> OpenForum Conference on Distributed Systems, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level <ref> [8, 11, 28, 32, 34, 40] </ref> parallelisms enable concurrency at higher levels of granularity. In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor.
Reference: [12] <author> Norman C. Hutchinson and Larry L. Peterson. </author> <title> The x-Kernel: An architecture for implementing network protocols. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(1) </volume> <pages> 64-76, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: While previous studies have explored affinity-based scheduling of non-network-related application processing, our work [30, 31] is the first to apply the technique to operating system network processing. To illustrate the motivation for considering affinity-based scheduling here, consider the following. In our experimental environment (consisting of a parallelized x-kernel <ref> [12, 25] </ref> running on a Silicon Graphics multiprocessor), packet processing times can vary by as much as a factor of four , depending on the state of the processor cache.
Reference: [13] <author> Mabo Ito, Len Takeuchi, and Gerald Neufeld. </author> <title> A multiprocessing approach for meeting the processing requirements for OSI. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 11(2) </volume> <pages> 220-227, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity.
Reference: [14] <author> Niraj Jain, Mischa Schwartz, and Theordore R. Bashkow. </author> <title> Transport protocol processing at Gbps rates. </title> <booktitle> In Proc. ACM SIGCOMM, </booktitle> <pages> pages 188-199, </pages> <address> 11 Philadelphia, PA, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity.
Reference: [15] <author> Raj Jain and Shawn Routhier. </author> <title> Packet trains: Measurements and a new model for computer network traffic. </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> 4(6) </volume> <pages> 986-995, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: We find that the benefit of affinity-based scheduling remains signif icant. * We evaluate performance as a function of stream bursti ness and source locality, two well-known properties of network traffic <ref> [15, 22] </ref>. To do so, we model individual streams with the Packet-Train source model [15]. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. * We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> We find that the benefit of affinity-based scheduling remains signif icant. * We evaluate performance as a function of stream bursti ness and source locality, two well-known properties of network traffic [15, 22]. To do so, we model individual streams with the Packet-Train source model <ref> [15] </ref>. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. * We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> Protocol processing does not include data-touching operations, such as copying or checksumming; these are considered in Section 5. Packet arrivals are modeled with an exponential interarrival time; the more realistic Packet-Train model <ref> [15] </ref>, which captures stream burstiness and source locality, is considered in Section 7. 4.1 PLP under PLP. In Figure 4a, the performance of code and stream affinity scheduling are shown, with mean packet delay plotted as a function of the number of admitted streams. <p> It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22]. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic <ref> [15, 22] </ref>. How do burstiness and source locality impact the performance of our affinity scheduling results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier [15]. See Figure 9 for an illustration. <p> How do burstiness and source locality impact the performance of our affinity scheduling results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier <ref> [15] </ref>. See Figure 9 for an illustration. Each stream is modeled as sending bursts of packets, where the burst size in packets has distribution B with mean B. <p> We find the scheduling technique performs well, in agree ment with the earlier receive-side results. * We have examined the impact of stream burstiness and source locality, as captured by the Packet-Train source model <ref> [15] </ref>. We find that the performance of the affinity-based policies is generally insensitive to these common source characteristics. In addition, we have explored stream-scaled CLP, which scales the number of independent stacks with the number of admitted streams.
Reference: [16] <author> Matthias Kaiserswerth. </author> <title> The parallel protocol engine. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 650-663, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Distributed applications, on the other hand, typically rely on very low-latency communication. Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., <ref> [16, 19] </ref>). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity.
Reference: [17] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The importance of non-data touching processing overheads in TCP/IP. </title> <booktitle> In Proc. ACM SIGCOMM, </booktitle> <pages> pages 259-268, </pages> <address> San Fran-cisco, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: These involve processing overheads which are independent of packet size, including protocol-specific functionality (e.g., demultiplex-ing, sequencing), protocol control operations, buffer management, operating system functions, and so on. In many real environments, packet sizes are generally small and these overheads constitute a significant fraction of overall end-system network processing time <ref> [17] </ref>. In such environments, techniques for optimizing non-data-touching performance (e.g., improved demultiplexing) gain importance. <p> For example, Kay and Pasquale <ref> [17] </ref> show that 84% of the processing time for TCP packets and 60% for UDP packets in their FDDI LAN environment is attributed to the non-data-touching operations of protocol-specific processing, buffer management, and OS overheads. <p> Although copying and checksumming (which scale with packet size) are expensive, the reason why non-data-touching overheads predominate is that typically in real environments most packets are small <ref> [5, 10, 17, 22] </ref>. 4 Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> To incorporate this per-byte overhead into our simulation model, we need a packet-size distribution. We use the empirical UDP/IP/FDDI distribution published by Kay and Pasquale <ref> [17] </ref>. This distribution was gathered from a traffic trace taken from the authors' departmental FDDI LAN, supporting mostly workstations and file servers. Ninety percent of the packets in this FDDI trace are UDP packets, primarily from NFS. Thus, the empirical data is well-matched to the protocols involved in our study. <p> In addition, they note the observed distribution matches the Ethernet traces reported in [5, 10, 22]. The packet size distribution, taken from Figure 5b of <ref> [17] </ref>, is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. We modified our multiprocessor simulation to sample the distribution independently for each packet sent. <p> Given the well-known high cost of data movement in protocol processing, and the resulting displacement of processor cache state, this is certainly somewhat surprising. The result is due to the fact that the packet sizes we consider are on average small. As a result, the non-data-touching protocol overheads predominate <ref> [17] </ref> and affinity-based scheduling, which seeks to minimize the memory overheads of these operations in a multiprocessor environment, remains effective. <p> Although UDP supports checksumming, we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP and TCP <ref> [17] </ref>: * Checksumming can be combined with data copying in integrated layer processing (ILP) [4, 1]. <p> Although it is a far more complex protocol than UDP, our results are likely to hold directly for TCP, for two reasons. First, the breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [17] </ref>). Second, TCP-specific processing generally accounts for a relatively small fraction of overall packet processing time (e.g., at most 13% for the DEC Ultrix protocol implementation studied in [17]), suggesting that the benefit of affinity scheduling for TCP packets would be quite similar to the benefit for UDP packets. <p> breakdowns of overall processing time overheads for TCP and UDP packets are very similar (compare for example graphs 3a and 3b in <ref> [17] </ref>). Second, TCP-specific processing generally accounts for a relatively small fraction of overall packet processing time (e.g., at most 13% for the DEC Ultrix protocol implementation studied in [17]), suggesting that the benefit of affinity scheduling for TCP packets would be quite similar to the benefit for UDP packets.
Reference: [18] <author> Jonathan Kay and Joseph Pasquale. </author> <title> Measurement, analysis, and improvement of UDP/IP throughput for the DECstation 5000. </title> <booktitle> In Proc. Winter 1993 USENIX Conference, </booktitle> <pages> pages 249-258, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: In response, researchers have examined the design of hardware network interfaces, low-level software drivers, and higher-level software protocol layers toward reducing the amount of data movement in network communication (e.g., <ref> [36, 7, 1, 18] </ref>). Other researchers have explored improvements to the "non-data-touching" components of protocol processing. These involve processing overheads which are independent of packet size, including protocol-specific functionality (e.g., demultiplex-ing, sequencing), protocol control operations, buffer management, operating system functions, and so on. <p> On our platform, given the relative overheads, it is likely that ILP would completely hide the checksumming overhead in the memory access latency of the uncached copy. * The hardware interface can support UDP or TCP check summing, as is generally done in Silicon Graphics in terfaces <ref> [18, 24] </ref>. * Checksumming can be disabled when network inter face's cyclic redundancy check (CRC) will suffice. This last point is developed in detail by Kay and Pasquale in [18]. <p> This last point is developed in detail by Kay and Pasquale in <ref> [18] </ref>. In that study, the authors argue that the UDP/TCP checksum operation is usually redundant: since most network interfaces (e.g., Ethernet, FDDI) support hardware CRC, and since traffic is usually local to the LAN (substantiated in [18] by traffic measurements), in the common case UDP and TCP packets are fully protected <p> This last point is developed in detail by Kay and Pasquale in <ref> [18] </ref>. In that study, the authors argue that the UDP/TCP checksum operation is usually redundant: since most network interfaces (e.g., Ethernet, FDDI) support hardware CRC, and since traffic is usually local to the LAN (substantiated in [18] by traffic measurements), in the common case UDP and TCP packets are fully protected by the interface CRC. Kay and Pasquale present algorithms for disabling UDP/TCP checksum under these conditions. In the UDP implementation, a flag is added per network interface, indicating whether hardware CRC is supported.
Reference: [19] <author> Thomas F. LaPorta and Mischa Schwartz. </author> <title> Performance analysis of MSP: Feature-rich high-speed transport protocol. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 1(6) </volume> <pages> 740-753, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Distributed applications, on the other hand, typically rely on very low-latency communication. Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., <ref> [16, 19] </ref>). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity.
Reference: [20] <author> Will E. Leland, Murad S. Taqqu, Walter Willinger, and Daniel V. Wilson. </author> <title> On the self-similar nature of Ethernet traffic (extended version). </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(1) </volume> <pages> 1-15, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22].
Reference: [21] <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue <ref> [9, 21, 37, 39] </ref>), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work [30, 31], we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. <p> Four studies consider affinity-based scheduling at the process level [6, 9, 37, 39], while 9 one considers the finer granularity of loop scheduling <ref> [21] </ref>. Vaswani and Zajorian [39] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Simulation results show that affinity-based scheduling yields a small but consistently positive impact across all applications, increasing processor utilization by an overall average of about 3%. Markatos and LeBlanc consider affinity-based scheduling of non-nested, completely parallelizable loops on several modern shared-memory multiprocessor platforms <ref> [21] </ref>. Previous work in loop scheduling has focused on load balancing of loops among processors and the overheads introduced by processor synchronization.
Reference: [22] <author> Jeffrey C. Mogul. </author> <title> Network locality at the scale of processes. </title> <booktitle> In Proc. ACM SIGCOMM, </booktitle> <pages> pages 273-284, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: We find that the benefit of affinity-based scheduling remains signif icant. * We evaluate performance as a function of stream bursti ness and source locality, two well-known properties of network traffic <ref> [15, 22] </ref>. To do so, we model individual streams with the Packet-Train source model [15]. We find that the performance of affinity-based scheduling is relatively insensitive to these source characteristics. * We explore a technique for improving the caching behavior and available packet-level concurrency under CLP. <p> Although copying and checksumming (which scale with packet size) are expensive, the reason why non-data-touching overheads predominate is that typically in real environments most packets are small <ref> [5, 10, 17, 22] </ref>. 4 Yet there are arguments for explicitly incorporating data-copying overheads. Most implementations copy packet data between user and kernel space. <p> In addition, they note the observed distribution matches the Ethernet traces reported in <ref> [5, 10, 22] </ref>. The packet size distribution, taken from Figure 5b of [17], is shown in Table 2. In formulating this distribution, we have assumed that each 8KB UDP packet is sent as two 4KB FDDI frames. <p> The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic <ref> [15, 22] </ref>. How do burstiness and source locality impact the performance of our affinity scheduling results? To capture these workload characteristics, we refine the per-stream arrival process to the Packet-Train model developed by Jain and Routhier [15]. See Figure 9 for an illustration.
Reference: [23] <author> Erich M. Nahum, David J. Yates, James F. Kurose, and Don Towsley. </author> <title> Performance issues in parallelized network protocols. </title> <booktitle> In Proc. First USENIX Symposium on Operating Systems Design and Implementation (OSDI), </booktitle> <pages> pages 125-137, </pages> <address> Monterey, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> Throughout the rest of the paper, the overhead of copying uncached data is incorporated into all results. 5.2 Checksumming The cost of checksumming cached data on our platform is 0:031s/byte <ref> [23] </ref>. Although UDP supports checksumming, we have not incorporated this overhead into our results. In fact, there are several techniques for reducing or avoiding the overhead of checksumming in UDP and TCP [17]: * Checksumming can be combined with data copying in integrated layer processing (ILP) [4, 1].
Reference: [24] <author> William Nowicki. </author> <type> Personal communication, </type> <month> April </month> <year> 1995. </year> <title> Silicon Graphics, </title> <publisher> Inc. </publisher>
Reference-contexts: Thus, the empirical data is well-matched to the protocols involved in our study. To estab 4 Some common applications do not touch the data at all. For example, the SGI implementation of the NFS server takes advantage of the fact that most SGI network interfaces support checksumming in firmware <ref> [24] </ref>. By DMA'ing data directly to the network interface, the server avoids bringing the data into the CPU cache. 6 lish the representativeness of the trace, Kay and Pasquale compare the empirical distribution with that of LAN traffic in another departmental environment, and find very similar results. <p> On our platform, given the relative overheads, it is likely that ILP would completely hide the checksumming overhead in the memory access latency of the uncached copy. * The hardware interface can support UDP or TCP check summing, as is generally done in Silicon Graphics in terfaces <ref> [18, 24] </ref>. * Checksumming can be disabled when network inter face's cyclic redundancy check (CRC) will suffice. This last point is developed in detail by Kay and Pasquale in [18].
Reference: [25] <author> Sean W. O'Malley and Larry L. Peterson. </author> <title> A dynamic network architecture. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(2) </volume> <pages> 110-143, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While previous studies have explored affinity-based scheduling of non-network-related application processing, our work [30, 31] is the first to apply the technique to operating system network processing. To illustrate the motivation for considering affinity-based scheduling here, consider the following. In our experimental environment (consisting of a parallelized x-kernel <ref> [12, 25] </ref> running on a Silicon Graphics multiprocessor), packet processing times can vary by as much as a factor of four , depending on the state of the processor cache.
Reference: [26] <author> Vern Paxson. </author> <title> Empirically-derived analytic models of wide-area TCP connections. </title> <journal> IEEE/ACM Transactions on Networking, </journal> <volume> 2(4) </volume> <pages> 316-336, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22].
Reference: [27] <author> Vern Paxson and Sally Floyd. </author> <title> Wide area traffic: The failure of Poisson modeling. </title> <booktitle> In Proc. ACM SIG-COMM, </booktitle> <pages> pages 257-268, </pages> <address> London, England UK, </address> <month> August </month> <year> 1994. </year>
Reference-contexts: It is well-known that network traffic is generally not Poisson <ref> [5, 10, 15, 20, 26, 27] </ref>. The Poisson process is not very bursty (its coefficient of variation, an informal measure of burstiness, is just 1), nor does the independent Poisson model capture the related property of source locality, a well-known characteristic of network traffic [15, 22].
Reference: [28] <author> David Presotto. </author> <title> Multiprocessor STREAMS for Plan 9. In United Kingdom UNIX User's Group, </title> <month> January </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level <ref> [8, 11, 28, 32, 34, 40] </ref> parallelisms enable concurrency at higher levels of granularity. In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor.
Reference: [29] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> Further results in affinity-based scheduling of parallel networking. </title> <type> Technical Report UM-CS-1995-046, </type> <institution> University of Massachusetts, Amherst, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The goal was to identify the protocol and stream-specific data structures written during a send operation, as a first step toward developing send-side PLP and CLP implementations. In contrast to the receive-side path, send-side UDP/IP/- FDDI processing references only read-only protocol-specific and stream-specific data structures (see <ref> [29] </ref> for discussion). Thus, there is no need to explicitly enable parallelism through software locks (PLP) or through data structure replication (CLP), and the "unparallelized" implementation immediately supports concurrency. Thus, there is no PLP/CLP distinction with regard to send-side UDP/- IP/FDDI processing. <p> The qualitative behaviors and relationships of the various scheduling policies are maintained. See <ref> [29] </ref> for the complete set of graphs. Overall, apart from the queueing effect seen under stream affinity scheduling, it is somewhat surprising that stream burstiness has such a small impact on the qualitative performance of the affinity-based scheduling policies.
Reference: [30] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> The performance impact of scheduling for cache affinity in parallel network processing. </title> <booktitle> In Fourth IEEE International Symposium on High-Performance Distributed Computing (IEEE HPDC), </booktitle> <pages> pages 66-77, </pages> <address> Pentagon City, VA, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor. While previous studies have explored affinity-based scheduling of non-network-related application processing, our work <ref> [30, 31] </ref> is the first to apply the technique to operating system network processing. To illustrate the motivation for considering affinity-based scheduling here, consider the following. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue [9, 21, 37, 39]), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work <ref> [30, 31] </ref>, we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. We found that the scheduling technique can significantly reduce packet latency, and in some cases substantially increase protocol processing throughput. <p> We found that the scheduling technique can significantly reduce packet latency, and in some cases substantially increase protocol processing throughput. We also compared the performance of packet-level parallelism (PLP) and connection-level parallelism (CLP). 1 PLP enables unrestricted packet-level concurrency, whereas CLP 1 In <ref> [30] </ref>, we referred to these approaches as "Locking" and "Independent Protocol Stacks (IPS)", respectively. <p> Data is not actually received from, or sent on, the actual network. To parallelize this x-kernel implementation, we first identified the non-read-only data structures referenced during protocol processing (see <ref> [30] </ref> for details). Consider the following two parallelization approaches. * Packet Level Parallelism (PLP) is achieved by protecting each non-read-only data structure with its own software lock. <p> In this section, we summarize the salient aspects of the approach; see <ref> [30] </ref> for details. To facilitate presentation, we focus throughout this section on receive-side processing. The same general methodology is employed in obtaining the send-side results (Section 6). 3.1 Simulation of multiprocessor network ing Consider the simulation of PLP. <p> See <ref> [30] </ref> for details. To acquire the t hot; hot , t cold; hot , and t cold; cold val ues, we conduct experiments on our multiprocessor in which we vary the scheduling of protocol threads and explicitly manipulate the processor caches. <p> To acquire the t hot; hot , t cold; hot , and t cold; cold val ues, we conduct experiments on our multiprocessor in which we vary the scheduling of protocol threads and explicitly manipulate the processor caches. The receive-side experimental design and resulting measurements are presented in <ref> [30] </ref>; the send-side experimental analysis appears in Section 6 of this paper. To summarize, we perform experiments specifically reflecting the parallelization alternative and particular combination of migration overheads, to determine t hot; hot , t cold; hot , and t cold; cold . <p> The computed packet processing time t (x) is then used in the multiprocessor simulation model. We now turn to the first set of performance results. 4 Large numbers of streams In <ref> [30] </ref>, we fixed the number of streams at eight, and examined the performance of receive-side UDP/IP/FDDI processing while varying the mean packet rate of the individual streams. <p> In fact, varying the number of streams has a relatively small impact on the benefits of affinity scheduling (i.e., in comparison to the case of a fixed number of streams with varying per-stream workloads, as in <ref> [30] </ref>). The reason is that under PLP, there is just a single set of protocol data structures, each protected with a software lock. These data structures and locks frequently migrate among processors, regardless of whether a small or large number of distinct streams are concurrently admitted. <p> To ensure the data is uncached, the referenced memory locations are written by another processor prior to protocol processing for the packet. We repeated the timing experiments, varying B from 1 to 4KB, and found that both the receive-side timings <ref> [30] </ref> as well as the send-side timings (Section 6) scale up nearly linearly at a rate of 0.04s per byte. To incorporate this per-byte overhead into our simulation model, we need a packet-size distribution. We use the empirical UDP/IP/FDDI distribution published by Kay and Pasquale [17]. <p> A sufficiently large number of independent runs (100 in our experiments) are performed to ensure that the 95% confidence interval half-widths do not exceed 1% of the overall mean, for all values. The resulting timing measurements are shown in Table 3, with corresponding receive-side numbers from <ref> [30] </ref> presented for comparison purposes. The timings suggest the importance of code affinity scheduling of send-side processing. <p> Free memory affinity scheduling increases the maximum number of supportable streams by about 25%. Overall, these results agree with the intuition that affinity-based scheduling should be effective on the send-side, given knowledge of its effectiveness on the receive-side <ref> [30] </ref> and the relative similarity of receive and send-side processing demands. 7 Stream burstiness and source lo cality We now refine the stream model. It is well-known that network traffic is generally not Poisson [5, 10, 15, 20, 26, 27]. <p> However, with B sufficiently large, code affinity scheduling under PLP offers lower mean packet delay than under CLP. If the mean inter-packet delay I were smaller (here it is 80s), the delay under CLP would rise above PLP even sooner with respect to increasing B. This is observed in <ref> [30] </ref>, where we considered Compound Poisson arrivals (i.e., the I = 0 case). 8 Stream-scaled CLP We now turn to our last set of performance results. So far, we have considered "processor-scaled" CLP, in which the number of independent protocol stacks matches the number of processors.
Reference: [31] <author> James Salehi, James Kurose, and Don Towsley. </author> <title> Scheduling for cache affinity in parallelized communication protocols (extended abstract). </title> <booktitle> In ACM International Conference on Measurement and Modeling of Computer Systems (ACM SIGMETRICS), </booktitle> <pages> pages 311-312, </pages> <address> Ottawa, Canada, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor. While previous studies have explored affinity-based scheduling of non-network-related application processing, our work <ref> [30, 31] </ref> is the first to apply the technique to operating system network processing. To illustrate the motivation for considering affinity-based scheduling here, consider the following. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue [9, 21, 37, 39]), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work <ref> [30, 31] </ref>, we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. We found that the scheduling technique can significantly reduce packet latency, and in some cases substantially increase protocol processing throughput.
Reference: [32] <author> Sunil Saxena, J. Kent Peacock, Fred Yang, Vijaya Verma, and Mohan Krishnan. </author> <title> Pitfalls in multithread-ing SVR4 STREAMS and other weightless processes. </title> <booktitle> In Proc. Winter 1993 USENIX Conference, </booktitle> <pages> pages 85-96, </pages> <address> San Diego, CA, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level <ref> [8, 11, 28, 32, 34, 40] </ref> parallelisms enable concurrency at higher levels of granularity. In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor.
Reference: [33] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the impact of alternative parallel process architectures on communication subsystem performance. </title> <booktitle> In Proc. 4 th International Workshop on Protocols for High-Speed Networks, </booktitle> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1994. </year> <pages> IFIP. </pages>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., <ref> [33] </ref>). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> Here, we adopt terms which reflect the level of protocol parallelism enabled. 1 allows concurrent processing only of packets from differ-ent streams. 2 These parallelization approaches are known to perform best on RISC-based shared-memory platforms <ref> [33, 34] </ref>, yet exhibit very different caching behaviors. CLP can deliver much lower message latency and significantly higher message throughput, due primarily to caching effects, but PLP exhibits better intra-stream scalability and more robust response to intra-stream burstiness, resulting from its greater packet-level concurrency.
Reference: [34] <author> Douglas C. Schmidt and Tatsuya Suda. </author> <title> Measuring the performance of parallel message-based process architectures. </title> <booktitle> In Proc. IEEE INFOCOM, </booktitle> <pages> pages 624-633, </pages> <address> Boston, MA, </address> <month> April </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: In this paper, we explore affinity-based scheduling of parallel network protocol processing, an area of research which has recently generated considerable interest (e.g., <ref> [3, 8, 19, 23, 28, 32, 33, 34, 40] </ref>). The use of parallelism An earlier version of this paper was presented at the IEEE Infocom '96 Conference. <p> Network parallelism in the host operating system|both within and among connections|can both increase the bandwidth and decrease the latency of multiprocessor communication. In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level <ref> [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] </ref> and connection-level [8, 11, 28, 32, 34, 40] parallelisms enable concurrency at higher levels of granularity. <p> In functional parallelism, an individual packet concurrently visits multiple processors (e.g., [16, 19]). In layer parallelism, packets visit multiple processors in a pipelined fashion (e.g., [33]). Packet-level [3, 8, 11, 13, 14, 23, 28, 32, 33, 34] and connection-level <ref> [8, 11, 28, 32, 34, 40] </ref> parallelisms enable concurrency at higher levels of granularity. In general, some form of network parallelism is generally necessary on multiprocessor machines, since the alternative would restrict aggregate network access to the bandwidth capacity of a single processor. <p> Here, we adopt terms which reflect the level of protocol parallelism enabled. 1 allows concurrent processing only of packets from differ-ent streams. 2 These parallelization approaches are known to perform best on RISC-based shared-memory platforms <ref> [33, 34] </ref>, yet exhibit very different caching behaviors. CLP can deliver much lower message latency and significantly higher message throughput, due primarily to caching effects, but PLP exhibits better intra-stream scalability and more robust response to intra-stream burstiness, resulting from its greater packet-level concurrency.
Reference: [35] <author> Jaswinder Pal Singh, Harold S. Stone, and Do-minique F. Thiebaut. </author> <title> A model of workloads and its use in miss-rate prediction for fully associative caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(7) </volume> <pages> 811-825, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: We start with a multiprocessor simulation model that closely follows the behavior of Figure 1. An analytic model of packet processing time, derived from established analytic results developed by other researchers <ref> [35, 38] </ref>, is used to capture the displacement of the cached protocol state by the background workload. This model is formulated to reflect the specific cache architecture and organization of our Silicon Graphics machine. <p> packet processing time, t (x), as t (x) = G 1 (x)t hot; hot + (1 G 1 (x))G 2 (x)t cold; hot + (1 G 1 (x))(1 G 2 (x))t cold; cold : (1) We formulate F 1 (x) and F 2 (x) from validated analytic results given in <ref> [35, 38] </ref>, in a way that reflects the cache organization (line size, cache size, associativity) of our SGI architecture. See [30] for details.
Reference: [36] <author> Jonathan M. Smith and C. Brendan S. Traw. </author> <title> Giving applications access to gb/s networking. </title> <journal> IEEE Network, </journal> <pages> pages 44-52, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: In response, researchers have examined the design of hardware network interfaces, low-level software drivers, and higher-level software protocol layers toward reducing the amount of data movement in network communication (e.g., <ref> [36, 7, 1, 18] </ref>). Other researchers have explored improvements to the "non-data-touching" components of protocol processing. These involve processing overheads which are independent of packet size, including protocol-specific functionality (e.g., demultiplex-ing, sequencing), protocol control operations, buffer management, operating system functions, and so on.
Reference: [37] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue <ref> [9, 21, 37, 39] </ref>), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work [30, 31], we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. <p> Four studies consider affinity-based scheduling at the process level <ref> [6, 9, 37, 39] </ref>, while 9 one considers the finer granularity of loop scheduling [21]. Vaswani and Zajorian [39] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> The explanation lies in the fact that among these applications, the upper bound on the time to completely reload the processor cache (about 1-2ms) is small in comparison to the processor reallocation interval (about 200-500ms). Squillante and Lazowska <ref> [37] </ref> conduct a modeling study designed to gain insight into the general class of scheduling policies which consider the state of processor caches. The study examines the performance of a range of in-kernel affinity-based scheduling policies on a multiprocessor system running multiple independent single-threaded processes.
Reference: [38] <author> Dominique F. Thiebaut and Harold S. Stone. </author> <title> Footprints in the cache. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(4) </volume> <pages> 305-329, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: We start with a multiprocessor simulation model that closely follows the behavior of Figure 1. An analytic model of packet processing time, derived from established analytic results developed by other researchers <ref> [35, 38] </ref>, is used to capture the displacement of the cached protocol state by the background workload. This model is formulated to reflect the specific cache architecture and organization of our Silicon Graphics machine. <p> packet processing time, t (x), as t (x) = G 1 (x)t hot; hot + (1 G 1 (x))G 2 (x)t cold; hot + (1 G 1 (x))(1 G 2 (x))t cold; cold : (1) We formulate F 1 (x) and F 2 (x) from validated analytic results given in <ref> [35, 38] </ref>, in a way that reflects the cache organization (line size, cache size, associativity) of our SGI architecture. See [30] for details. <p> This work motivated our own by demonstrating that when the cache reload time is large with respect to the task's inherent computing demands, affinity scheduling can have a significant impact. The authors use a variety of queueing-theoretic techniques and employ the analytic cache model developed by Thiebaut and Stone <ref> [38] </ref>. In [6], Devarakonda and Mukherjee explore implementation issues in affinity-based scheduling, both in-kernel and within a user-level thread scheduler, on an 8-processor Encore Multimax running Mach 2.5. A schedulable task is defined to have affinity strictly for the processor it most recently visited.
Reference: [39] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multipro-grammed, shared memory multiprocessors. </title> <booktitle> In Proc. Thirteenth Symposium on Operating Systems Principles, </booktitle> <pages> pages 26-40, </pages> <address> Pacific Grove, CA, </address> <month> October </month> <year> 1991. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction Processor-cache affinity scheduling is of growing interest as processor speeds continue to increase faster than memory speeds <ref> [6, 9, 21, 30, 37, 39] </ref>. On modern shared-memory machines, the time required to access an uncached memory location is typically much larger than the time to access one cached locally. <p> Given that processor speeds have tended to increase much faster than memory access times (a trend that is likely to continue <ref> [9, 21, 37, 39] </ref>), this empirical observation suggests that affinity-based scheduling represents a promising research opportunity in parallel networking. In our earlier work [30, 31], we evaluated the benefits of affinity-based scheduling of receive-side UDP/IP/FDDI processing, for a shared-memory multiprocessor concurrently executing a background workload of non-protocol activity. <p> Four studies consider affinity-based scheduling at the process level <ref> [6, 9, 37, 39] </ref>, while 9 one considers the finer granularity of loop scheduling [21]. Vaswani and Zajorian [39] show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. <p> Four studies consider affinity-based scheduling at the process level [6, 9, 37, 39], while 9 one considers the finer granularity of loop scheduling [21]. Vaswani and Zajorian <ref> [39] </ref> show experimentally that affinity-based scheduling within kernel-level processor space-sharing scheduling policies provides little benefit. Their workload is a mix of three types of parallel applications executing on a Sequent Symmetry multiprocessor. The measured reduction in task response time enabled by affinity-based scheduling does not exceed 1%.

References-found: 39

