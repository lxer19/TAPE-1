URL: ftp://ftp.cs.umd.edu/pub/sel/papers/otso_icse.ps.Z
Refering-URL: http://www.cs.umd.edu/users/jkontio/reuse.html
Root-URL: 
Title: A Case Study in Applying a Systematic Method for COTS Selection  
Author: Jyrki Kontio 
Keyword: software reuse, COTS, multiple criteria decision making  
Note: Version 0.14 1 This work has been supported by the Hughes Information Technology Corporation and the EOS Program. Abstract:  
Address: A.V.Williams Building College Park, MD 20742, U.S.A.  
Affiliation: University of Maryland Department of Computer Science  
Email: Email: jkontio@cs.umd.edu  
Web: http://www.cs.umd.edu/users/jkontio/  
Abstract: This paper describes a case study that used and evaluated key aspects of a method developed for systematic reusable off-the-shelf software selection. The paper presents a summary of the common problems in reusable off-the-shelf software selection, describes the method used and provides details about the case study carried out. The case study indicated that the evaluated aspects of the method are feasible, improve the quality and efficiency of reusable software selection and the decision makers have more confidence in the evaluation results, compared to traditional approaches. Furthermore, the case study also showed that the choice of evaluation data analysis method can influence the evaluation results. 
Abstract-found: 1
Intro-found: 1
Reference: [2] <author> V. R. Basili, G. Caldiera, and G. Cantone, </author> <title> A Reference Architecture for the Component Factory, </title> <journal> ACM Transactions on Software Engineering and Methodology , vol. </journal> <volume> 1, 1. </volume> <pages> pp. 53-80, </pages> <year> 1992. </year>
Reference: [3] <author> V. R. Basili, G. Caldiera, and H. D. Rombach. </author> <title> Goal Question Metric Paradigm. In: Ensyclopedia of Software Engineering , ed. </title> <editor> J. J. Marciniak. </editor> <address> New York: </address> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year> <pages> pp. 528-532. </pages>
Reference-contexts: Each branch in this hierarchy ends in an evaluation attribute: a well-defined measurement or a piece of information that will be determined during evaluation. This hierarchical decomposition principle is analogous to the GQM method <ref> [3] </ref>. The evaluation attributes should have clear operational definitions so that consistency can be maintained during evaluation.
Reference: [4] <author> V. R. Basili, G. Caldiera, and H. D. Rombach. </author> <title> The Experience Factory. In: Encyclopedia of Software Engineering , Anonymous New York: </title> <publisher> John Wiley & Sons, </publisher> <year> 1994. </year> <pages> pp. 470-476. </pages>
Reference: [5] <author> T. Birgerstaff and C. Richter, </author> <title> Reusability Framework, Assessment, and Directions, </title> <journal> IEEE Software , vol. </journal> <month> March. </month> <pages> pp. 41-49, </pages> <year> 1987. </year>
Reference: [6] <author> T. B. Bollinger and S. L. Pfleeger, </author> <title> Economics of Reuse: issues and alternatives, </title> <journal> Information and Software Technology , vol. </journal> <volume> 32, 10. </volume> <pages> pp. 643-652, </pages> <year> 1991. </year>
Reference: [7] <author> G. Boloix and P. N. Robillard, </author> <title> A Software System Evaluation Framework, </title> <journal> IEEE Computer , vol. </journal> <volume> 28, 12. </volume> <pages> pp. 17-26, </pages> <year> 1995. </year>
Reference-contexts: Some general criteria have been proposed to help in the search of potential reusable components [21,22]. Boloix and Robillard recently presented a general framework for assessing the software product, process and their impact on the organization <ref> [7] </ref>. However, none of this work is specific to COTS selection and the issues of how to define the evaluation criteria are not addressed. Copyright 1996 IEEE. Published in the Proceedings of the 18th International Conference on Software Engineering (ICSE-18), March 25-29, 1996, Berlin, Germany.
Reference: [8] <author> A. T. W. Chu and R. E. Kalaba, </author> <title> A Comparison of Two Methods for Determining the Weights Belonging to Fuzzy Sets, </title> <journal> Journal of Optimization Theory and Applications , vol. </journal> <volume> 27, 4. </volume> <pages> pp. 531-538, </pages> <year> 1979. </year>
Reference: [9] <author> W. E. </author> <title> Deming. Out of the Crisis , Cambridge: </title> <institution> Massachusetts Institute of Technology, </institution> <year> 1986. </year> <pages> 507 pages. </pages>
Reference-contexts: The criteria may be listed and documented with one or two words, e.g., memory usage, ease of use or reliability. This leaves the exact meaning of each criterion very open to each evaluator's own interpretations. This problem becomes more relevant when there are more than one evaluator. Operational definitions <ref> [9] </ref> for all criteria are required so that all COTS alternatives are compared against a common yardstick. Once the evaluation of COTS alternatives has been done, a common approach to consolidating evaluation results is to use some kind of weighted scoring method (WSM) to rank the alternatives.
Reference: [10] <author> G. R. Finnie, G. E. Wittig, and D. I. Petkov, </author> <title> Prioritizing Software Development Productivity Factors Using the Analytic Hierarchy Process, </title> <journal> Journal of Systems and Software , vol. </journal> <volume> 22, </volume> <pages> pp. 129-139, </pages> <year> 1995. </year>
Reference-contexts: The AHP technique was developed by Thomas Saaty for multiple criteria decision making situations [23,24]. The technique has been widely and successfully used in several fields [25], including software engineering <ref> [10] </ref> and software selection [13,18]. It has been reported to be an effective technique in multiple criteria decision making situations in several case studies and experiments [8,11,25,28]. Due to the hierarchical treatment of our criteria, AHP fits well into our evaluation process as well.
Reference: [11] <author> E. H. Forman, </author> <title> Facts and Fictions about the Analytic Hierarchy Process, </title> <journal> Mathematical and Computer Modelling , vol. </journal> <volume> 17, </volume> <pages> 4-5. pp. 19-26, </pages> <year> 1993. </year>
Reference: [12] <author> M. L. Griss, </author> <title> Software reuse: From library to factory, </title> <journal> IBM Systems Journal , vol. </journal> <volume> 32, 4. </volume> <pages> pp. 548-566, </pages> <year> 1993. </year>
Reference-contexts: It has been argued that an important characteristic of the infrastructure supporting reuse is the existence of a marketplace that both provides access to reuse producers and consumers as well as provides a mechanism to transfer benefits between the parties [6,19,20,34]. Many organizations have implemented systematic reuse programs <ref> [12] </ref> which have resulted in in-house libraries of reusable components. The increased commercial offering of embeddable software components, standardization of basic software environments (e.g., MS-Windows, UNIX), and popularization of Internet have resulted in a new situation for reusable software consumers: there are many more accessible, potential reuse candidates.
Reference: [13] <author> S. Hong and R. Nigam. </author> <title> Analytic Hierarchy Process Applied to Evaluation of Financial Modeling Software. </title> <booktitle> In: Proceedings of the 1st International Conference on Decision Support Systems, </booktitle> <address> Atlanta, GA , Anonymous1981. </address>
Reference: [14] <author> J. W. Hooper and R. O. Chester. </author> <title> Software Reuse: Guidelines and Methods, </title> <editor> R.A. Demillo (Ed). </editor> <address> New York: </address> <publisher> Plenum Press, </publisher> <year> 1991. </year>
Reference: [15] <author> ISO. </author> <title> Information technology - Software product evaluation - Quality characteristics and quidelines for their use, </title> <institution> ISO/IEC 9126:1991(E) , Geneve, Switzerland: International Standards Organization, </institution> <year> 1991. </year>
Reference-contexts: The criteria set is specific to each COTS selection case but most of the criteria can be categorized into four groups: functional requirements for the COTS; required quality characteristics, such as reliability, maintainability and portability <ref> [15] </ref>; business concerns, such as cost, reliability of the vendor, and future development prospects; and relevant software architecture, such as constraints presented by operating system, division of functionality in the system or specific communication mechanisms between modules.
Reference: [16] <author> J. Kontio, OTSO: </author> <title> A Systematic Process for Reusable Software Component Selection CS-TR-3478, 1995. </title> <institution> University of Maryland Technical Reports. University of Maryland. College Park, MD. </institution>
Reference-contexts: The method, called OTSO 3 , supports the search, evaluation and selection of reusable software and provides specific techniques for defining the evaluation criteria, comparing the costs and benefits of alternatives, and consolidating the evaluation results for decision making. The OTSO method has been documented separately <ref> [16] </ref> and this paper focuses in two specific aspects of the OTSO method: evaluation criteria definition and evaluation data analysis. These two aspects were evaluated in a case study in an active project with the Hughes Information Technology Corporation. The structure of this paper is as follows. <p> knowledge Design specification design and architecture constraints Cost models Value estimation models a model for comparing the costs and value associated with each alternative, making them comparable with each other use of appropriate decision making methods to analyze and summarize evaluation results The full OTSO method has been documented separately <ref> [16] </ref> and it is not explained in detail in this paper. However, the two specific aspects of the method - evaluation criteria definition and analysis of evaluation data - are explained in more detail as they were addressed in our case study. <p> This field is used for documenting which criteria were used in the screening phase. Baseline Baseline is the minimum required level of functionality and features that the application must satisfy when it is delivered. See reference <ref> [16] </ref> for details. Qualitative description Guidelines how additional information about the evaluation attribute should be documented. Source How the value for the evaluation attribute can be determined for each alternative. Priority Description of how important the particular evaluation attribute is. E.g. required, recommended or optional.
Reference: [17] <author> J. Kontio and S. Chen, </author> <title> Hypertext Document Viewing Tool Trade Study: Summary of Evaluation Results 441-TP-002 001, 1995. </title> <type> EOS project Technical Paper. </type> <institution> Hughes Corporation, EOS project. </institution>
Reference-contexts: In most cases only the high and low values were explicitly defined for the scores verbally before assigning a score. Note that the scores were, therefore, mostly based on ordinal scales. The qualitative differences between tools were documented separately <ref> [17] </ref>. This report represents the raw differences between the tools without any judgments of their importance or ranking to each other.
Reference: [18] <author> H. Min, </author> <title> Selection of Software: The Anal ytic Hierarchy Process, </title> <journal> International Journal of Physical Distribution & Logistics Management , vol. </journal> <volume> 22, 1. </volume> <pages> pp. 42-52, </pages> <year> 1992. </year>
Reference: [19] <author> S. L. Pfleeger and T. B. Bollinger, </author> <title> The Economics of Reuse: New Approaches to Modeling and Assessing Cost, </title> <booktitle> Information and Software Technology , vol. </booktitle> <year> 1994. </year>
Reference: [20] <author> J. S. Poulin, J. M. Caruso, and D. R. Hancock, </author> <title> The business case for software reuse, </title> <journal> IBM Systems Journal , vol. </journal> <volume> 32, 4. </volume> <pages> pp. 567-594, </pages> <year> 1993. </year>
Reference-contexts: It seems that there is a lot of potential for nonoptimal or inconsistent COTS reuse decisions. However, the issues and problems associated with the selection of suitable reusable components have not been addressed much in the software reuse community. Poulin et al. present an overall selection process <ref> [20] </ref> and include some general criteria for assessing the suitability of reuse candidate [29]. Some general criteria have been proposed to help in the search of potential reusable components [21,22].
Reference: [21] <author> R. Prieto-Daz, </author> <title> Implementing faceted classification for software reuse, </title> <journal> Communications of the ACM , vol. </journal> <volume> 34, </volume> <year> 5.1991. </year>
Reference: [22] <author> C. V. Ramamoorthy, V. Garg, and A. Prakash, </author> <booktitle> Support for Reusability in Genesis, </booktitle> <pages> pp. 299-305, </pages> <year> 1986. </year> <booktitle> Proceedings of Compsac 86. </booktitle> <address> Chicago. </address>
Reference: [23] <author> T. L. Saaty. </author> <title> Decision Making for Leaders , Belmont, California: Lifetime Learning Publications, </title> <booktitle> 1982. </booktitle> <pages> 291 pages. </pages>
Reference-contexts: Second, assigning of weights for the criteria is very difficult when the number of criteria is high. It seems that people are able to deal with less than ten alternatives at a time <ref> [23] </ref>. With large number of issues to keep in mind people have difficulties in mentally coping with the dependencies of individual factors. The assigning of scores instead of weights is even more limiting.
Reference: [24] <author> T. L. Saaty. </author> <title> The Analytic Hierarchy Process , New York: </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year> <pages> 287 pages. </pages>
Reference-contexts: Present the results of the evaluation, the alternative with the highest priority being the one that is recommended as the best alternative. The rankings obtained through paired comparisons between the alternatives are converted to normalized rankings using the eigenvalue method <ref> [24] </ref>, i.e., the relative rankings of alternatives are presented in ratio Item name Description Heading Heading for each evaluation attribute acts as a unique identifier. Definition A definition of the evaluation attribute. Rationale Description of the rationale for the evaluation attribute and how it relates to the evaluation criteria. <p> E.g. required, recommended or optional. Table 1: Evaluation attribute definition template scale values which total one. Saaty argues that hierarchies are a natural way for humans to organize their view of the world and they represent real world phenomena well <ref> [24] </ref>. Criteria and alternatives are compared in pairs, which results in more reliable comparison results. This way we are able to avoid the problem of having to assign absolute values to alternatives, only their relative preference or values are compared.
Reference: [25] <author> T. L. Saaty. </author> <title> Analytic Hierarchy. </title> <booktitle> In: Encyclopedia of Science & Technology , Anonymous McGraw-Hill, </booktitle> <year> 1992. </year> <pages> pp. 559-563. </pages>
Reference-contexts: The AHP technique was developed by Thomas Saaty for multiple criteria decision making situations [23,24]. The technique has been widely and successfully used in several fields <ref> [25] </ref>, including software engineering [10] and software selection [13,18]. It has been reported to be an effective technique in multiple criteria decision making situations in several case studies and experiments [8,11,25,28]. Due to the hierarchical treatment of our criteria, AHP fits well into our evaluation process as well.
Reference: [26] <author> T. L. Saaty, </author> <title> Expert Choice software 1995, </title> <type> ver. 9, rel. </type> <year> 1995. </year> <title> Expert Choice Inc. IBM. DOS. </title>
Reference-contexts: Due to the hierarchical treatment of our criteria, AHP fits well into our evaluation process as well. AHP is supported by a commercial tool that supports the entering of judgments and performs all the necessary calculations <ref> [26] </ref>. The AHP is based on the idea of decomposing a multiple criteria decision making problem into a criteria hierarchy. At each level in the hierarchy the relative importance of factors is assessed by comparing them in pairs. Finally, the alternatives are compared in pairs with respect to the criteria. <p> We deliberately want to separate the analysis of this data from producing the data. This allows the use of appropriate techniques in evaluation data analysis for decision making. The weighting of alternatives is done using the AHP method, preferably using a supporting tool <ref> [26] </ref>. Preferences are collected and consolidated to the level stakeholders prefer. The AHP allows the consolidation of all qualitative information and the financial information into a single ranking of alternatives. However, we believe that this would condense valuable information too much. <p> Note that we have only included five examples of the total of 38 criteria actually used. The AHP method was applied using the Expert Choice tool <ref> [26] </ref> in an online session. We first ranked the criteria hierarchically and then proceeded to rank the alternatives against each leaf-level criterion. The results of the AHP analysis method are presented in Figure 3. The total effort distribution for the evaluation process is presented in Table 3.
Reference: [27] <author> W. Schfer, R. Prieto-Daz, and M. Matsumoto. </author> <title> Software Reusability , W. </title> <editor> Schfer, R. Prieto-Daz, and M. Matsumoto ( Eds). Hemel Hempstead: </editor> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference: [28] <author> P. J. Schoemaker and C. C. Waid, </author> <title> An Experimental Comparison of Different Approaches to Determining Weights in Additive Utility Models, </title> <booktitle> Management Science , vol. </booktitle> <volume> 28, 2. </volume> <pages> pp. 182-196, </pages> <year> 1982. </year>
Reference-contexts: Third, it is rather difficult to define a set of criteria and their weights so that they are either independent from each other or, if they overlap, their weights are adjusted to compensate for the overlapping areas. The weighted scoring method has also been criticized in the DSS literature <ref> [28] </ref>. The problems in the COTS selection can be summarized in the following main points: lack of a defined, systematic, and repeatable process, potential disregard of the application requirements, and misuse of data consolidation methods in decision making.
Reference: [29] <author> W. Tracz, </author> <title> Reusability Comes of Age, </title> <journal> IEEE Software , vol. </journal> <month> July. </month> <pages> pp. 6-8, </pages> <year> 1987. </year>
Reference-contexts: However, the issues and problems associated with the selection of suitable reusable components have not been addressed much in the software reuse community. Poulin et al. present an overall selection process [20] and include some general criteria for assessing the suitability of reuse candidate <ref> [29] </ref>. Some general criteria have been proposed to help in the search of potential reusable components [21,22]. Boloix and Robillard recently presented a general framework for assessing the software product, process and their impact on the organization [7].
Reference: [30] <author> W. Tracz. </author> <title> Tutorial: Software Reuse: Emerging Technology , W. </title> <editor> Tracz (Ed). </editor> <address> Washington: </address> <publisher> IEEE Computer Society, </publisher> <year> 1988. </year>
Reference: [31] <author> W. Tracz. </author> <title> Software Reuse: Motivators and Inhibitors. In: Tutorial: Software Reuse: Emerging Technology, </title> <editor> ed. W. Tracz. </editor> <address> Washington: </address> <publisher> IEEE Computer Society, </publisher> <year> 1988. </year> <pages> pp. 62 67. </pages>
Reference: [32] <author> W. Tracz, </author> <title> Legal obligations for software reuse, </title> <journal> American Programmer, </journal> <volume> vol. </volume> <year> March.1991. </year>
Reference: [33] <author> M. Wasmund, </author> <title> Implementing Critical Success Factors in software reuse, </title> <journal> IBM Systems Journal , vol. </journal> <volume> 32, 4. </volume> <pages> pp. 595 611, </pages> <year> 1993. </year>
Reference: [34] <author> F. Wolff, </author> <title> Long-term controlling of software reuse, </title> <journal> Information and Software Technology, </journal> <volume> vol. 34, 3. </volume> <pages> pp. 178 184, </pages> <year> 1992. </year>
References-found: 33

