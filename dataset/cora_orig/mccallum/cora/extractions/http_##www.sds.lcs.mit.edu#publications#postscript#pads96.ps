URL: http://www.sds.lcs.mit.edu/publications/postscript/pads96.ps
Refering-URL: http://www.sds.lcs.mit.edu/publications/pads96.html
Root-URL: 
Title: Reducing Synchronization Overhead in Parallel Simulation  
Phone: 1996.  
Author: Ulana Legedza William E. Weihl 
Affiliation: Large-Scale Parallel Software Group MIT Laboratory for Computer Science  
Address: Philadelphia, PA,  Cambridge, MA 02139  
Note: Appears in the proceedings of the 10th ACM Workshop on Parallel and Distributed Simulation,  
Abstract: Synchronization is often the dominant cost in conservative parallel simulation, particularly in simulations of parallel computers, in which low-latency simulated communication requires frequent synchronization. We present and evaluate local barriers and predictive barrier scheduling, two techniques for reducing synchronization overhead in the simulation of message-passing multicomputers. Local barriers use nearest-neighbor synchronization to reduce waiting time at synchronization points. Predictive barrier scheduling, a novel technique that schedules synchronizations using both compile-time and runtime analysis, reduces the frequency of synchronization operations. In contrast to other work in this area, both techniques reduce synchronization overhead without decreasing the accuracy of network simulation. These techniques were evaluated by comparing their performance to that of periodic global synchronization. Experiments show that local barriers improve performance by up to 24% for communication-bound applications, while predictive barrier scheduling improves performance by up to 65% for applications with long local computation phases. Because the two techniques are complementary, we advocate a combined approach. This work was done in the context of Parallel Proteus, a new parallel simulator of message-passing multicomputers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk L. Johnson, David Kranz, John Kubiatowicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Local barriers do not perform as well because of high messaging overhead. On different host architectures, the relative performance of the two techniques will be different. Some multi-computers have support for fast fine-grain synchronization, but little or no support for global synchronization (e.g., Alewife <ref> [1] </ref>). Local barriers are likely to perform much better on such host machines, while predictive barrier scheduling will suffer from high overheads for its global operations. Therefore, the two techniques are also complementary with regard to the host architectures for which they are suited.
Reference: [2] <author> Alfred A. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: The minimum distances to nonlocal operations for all the basic blocks in the graph are computed iteratively using the algorithm of Figure 3 (based on algorithm 10.2 in <ref> [2] </ref>). 5.3.2 Code Instrumentation After the minimum distance to non-local operation has been determined for each block, each block is instrumented with code that records this value.
Reference: [3] <author> Robert C. Bedichek. Talisman: </author> <title> Fast and accurate multicom-puter simulation. </title> <booktitle> In Proceedings of SIGMETRICS '95, </booktitle> <year> 1995. </year>
Reference-contexts: Low overhead techniques for simulating the behavior of each target processor (direct execution as in Proteus and Tango [14] or threaded code <ref> [3] </ref>) cause synchronization overhead to outweigh the time spent doing simulation work. Large memory requirements (for simulator state or application data) for each target processor prevent the simulation of a large number of target processors by each host processor. Consequently, synchronization overhead dominates simulation time. <p> While the local barrier approach and predictive barrier scheduling have been evaluated in the context of a direct-execution simulator, they are certainly also applicable to simulators which simulate application code execution in a different way (e.g., threaded code <ref> [3] </ref>). First, local barriers do not depend on direct execution in any way. 8 Second, while our implementation of predictive barrier scheduling does rely on Parallel Proteus's support for direct execution, this is not essential to the technique.
Reference: [4] <author> Bertsekas and Tsitsiklis. </author> <title> Parallel and Distributed Computing: Numerical Methods. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1989. </year>
Reference-contexts: In contrast, radix represents applications in which communication is more frequent and irregular. By varying parameters of both applications, we were able to experiment with a wide range of computation-to-communication ratios. 6.2.1 SOR SOR is a stencil computation that iteratively solves PDE's <ref> [4] </ref>. Each iteration of the algorithm performs a relaxation function on a grid of points. This function requires only the value of the point to be updated and the values of adjacent points. Subblocks of this grid are distributed among the processors of the parallel machine.
Reference: [5] <author> Guy E. Blelloch, Charles E. Leiserson, Bruce M. Maggs, C. Greg Plaxton, Stephen J. Smith, and Marco Zagha. </author> <title> A 9 comparison of sorting algorithms for the Connection Machine CM-2. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <month> July </month> <year> 1991. </year>
Reference-contexts: We used grid sizes of 2 11 20 entries, and up to 625 target processors. This application alternates between communication phases and long computation phases. 6.2.2 Radix Sort Parallel radix sort sorts d-digit radix r numbers in d passes of counting sort <ref> [5] </ref>. The numbers to be sorted are distributed evenly among the processors. Each pass consists of 3 phases: counting, scanning and routing. The counting phase involves local computation only. The scanning phase consists of r parallel prefix scans.
Reference: [6] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. Proteus: </author> <title> A high-performance parallel architecture simulator. </title> <type> Technical Report MIT/LCS 516, </type> <institution> Massachusetts Institute of Technology, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Both techniques reduce synchronization overhead without decreasing network simulation accuracy. We implemented local barriers and predictive barrier scheduling as part of Parallel Proteus, an execution-driven conservative parallel discrete-event simulator of message-passing multicomputers that is based on Proteus <ref> [6] </ref>. Experiments with Parallel Proteus on a CM-5 show that local barriers are effective at reducing overhead in simulations involving frequent communication, while predictive barrier scheduling is most effective for simulations in which communication is infrequent.
Reference: [7] <author> Douglas C. Burger and David A. Wood. </author> <title> Accuracy vs. performance in parallel simulation of interconnection networks. </title> <booktitle> In Proceedings of the 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: They synchronize using periodic global barriers. They achieve good performance by decreasing network simulation accuracy, which allows them to synchronize infrequently. The WWT researchers have explored a range of network simulation models (from very accurate to not very accurate) <ref> [7] </ref>. All of these could easily be incorporated into Parallel Proteus, but would not solve the problem of reducing synchronization overhead while maintaining accurate network simulation. However, WWT and PTL cannot exploit lookahead in the same way as Parallel Proteus can, because they simulate shared memory architectures.
Reference: [8] <author> David R. Cheriton, Hendrik A. Goosen, Hugh Holbrook, and Philip Machanick. </author> <title> Restructuring a parallel simulation to improve behavior in a shared-memory multiprocessor: The value of distributed synchronization. </title> <booktitle> In Proceedings of the Seventh Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 159-162, </pages> <month> May </month> <year> 1993. </year>
Reference: [9] <institution> Thinking Machines Corporation. </institution> <type> CM-5 technical summary, </type> <year> 1992. </year>
Reference: [10] <author> Philip M. </author> <title> Dickens. </title> <type> Personal communication. </type> <month> January </month> <year> 1996. </year>
Reference-contexts: Also, all accesses to shared data (whether requiring communication or not) potentially affect other target processors, so it is not correct to allow application threads to run ahead until a communication operation is encountered. LAPSE is a conservative, direct execution-based parallel simulator of the message-passing Intel Paragon <ref> [10] </ref> [11] [12]. It achieves good performance by exploiting two sources of lookahead. First, like the runtime analysis in predictive barrier scheduling, LAPSE lets some application code execute in advance of the simulation of its timing. However, unlike Parallel Proteus, LAPSE does not augment this technique with compile-time analysis.
Reference: [11] <author> Philip M. Dickens, Philip Heidelberger, and David M. Nicol. </author> <title> Parallelized direct execution simulation of message-passing parallel programs. </title> <type> Technical Report 94-50, </type> <institution> ICASE, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Also, all accesses to shared data (whether requiring communication or not) potentially affect other target processors, so it is not correct to allow application threads to run ahead until a communication operation is encountered. LAPSE is a conservative, direct execution-based parallel simulator of the message-passing Intel Paragon [10] <ref> [11] </ref> [12]. It achieves good performance by exploiting two sources of lookahead. First, like the runtime analysis in predictive barrier scheduling, LAPSE lets some application code execute in advance of the simulation of its timing. However, unlike Parallel Proteus, LAPSE does not augment this technique with compile-time analysis.
Reference: [12] <author> Philip M. Dickens, Philip Heidelberger, and David M. Nicol. </author> <title> Parallelized network simulators for message-passing parallel programs. </title> <booktitle> In Proceedings of MASCOTS '95, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Also, all accesses to shared data (whether requiring communication or not) potentially affect other target processors, so it is not correct to allow application threads to run ahead until a communication operation is encountered. LAPSE is a conservative, direct execution-based parallel simulator of the message-passing Intel Paragon [10] [11] <ref> [12] </ref>. It achieves good performance by exploiting two sources of lookahead. First, like the runtime analysis in predictive barrier scheduling, LAPSE lets some application code execute in advance of the simulation of its timing. However, unlike Parallel Proteus, LAPSE does not augment this technique with compile-time analysis.
Reference: [13] <author> Richard M Fujimoto. </author> <title> Parallel discrete event simulation. </title> <journal> Com-muniations of the ACM, </journal> <volume> 33(10) </volume> <pages> 30-53, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Therefore, performance may be improved by eliminating the unnecessary periodic synchronizations performed during computation phases. Predictive barrier scheduling accomplishes this by predicting when communication is going to occur in the target system and scheduling synchronizations only during communication phases. In effect improving the lookahead <ref> [13] </ref> of simulations, this approach seems especially promising for applications with long local computation phases.
Reference: [14] <author> Stephen R. Goldschmidt. </author> <title> Simulation of Multiprocessors: Accuracy and Performance. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: In simulators of parallel computers with fast networks, accurate network simulation usually requires the value of Q to be very small and, consequently, synchronization to be very frequent. Low overhead techniques for simulating the behavior of each target processor (direct execution as in Proteus and Tango <ref> [14] </ref> or threaded code [3]) cause synchronization overhead to outweigh the time spent doing simulation work. Large memory requirements (for simulator state or application data) for each target processor prevent the simulation of a large number of target processors by each host processor. Consequently, synchronization overhead dominates simulation time. <p> In contrast, our work deals with the synchronization overhead that dominates when low overhead techniques are used to simulate processor behavior. The Wisconsin Wind Tunnel and Parallel Tango Lite are two direct execution-based parallel simulators of shared memory multi-computers [21] <ref> [14] </ref>. They synchronize using periodic global barriers. They achieve good performance by decreasing network simulation accuracy, which allows them to synchronize infrequently. The WWT researchers have explored a range of network simulation models (from very accurate to not very accurate) [7].
Reference: [15] <author> Kirk Johnson, David Chaiken, and Alan Mainwaring. NWO-P: </author> <title> Parallel simulation of the Alewife machine. </title> <booktitle> In Proceedings of the 1993 MIT Student Workshop on Supercomputing Technologies, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Several of these approaches will be described here as they relate to our work. Some simulators of parallel computers are extremely detailed (e.g., cycle-by-cycle simulators such as NWO-P, a detailed parallel simulator of the MIT Alewife machine <ref> [15] </ref>), with large overheads for simulating each cycle that far outweigh synchronization overhead. In contrast, our work deals with the synchronization overhead that dominates when low overhead techniques are used to simulate processor behavior.
Reference: [16] <author> Pavlos Konas and Pen-Chung Yew. </author> <title> Improved parallel architectural simulations on shared-memory multiprocessors. </title> <booktitle> In Proceedings of The 8th Workshop on Parallel and Distributed Simulation (PADS94), </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: Therefore, exploiting large amounts of lookahead in Parallel Proteus is much more difficult than it is in LAPSE. SPaDES is a conservative parallel simulation approach used for simulating symmetric multiprocessors on shared memory multicom-puter hosts <ref> [16] </ref>. It uses load balancing and split-phase (or fuzzy) barriers to avoid synchronization overhead. While effectively reducing waiting time for SPaDES simulations, balancing load in Parallel Proteus would involve too much overhead for transferring simulation state between host processors.
Reference: [17] <author> Ulana Legedza. </author> <title> Reducing synchronization overhead in parallel simulation. </title> <type> Technical Report MIT/LCS 655, </type> <institution> Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1995. </year>
Reference-contexts: Control then returns to the simulator and the nonlocal operation is delayed until the simulator has processed all earlier non-local events. This technique allows the simulator to achieve good performance without sacrificing accuracy (see appendix A in <ref> [17] </ref>). Predictive barrier scheduling uses the 1000-cycle window into the future provided by the simulator quantum mechanism to determine the future communication behavior of application threads. <p> Synchronization variables are an example of such shared data. Special routines simulate the functionality and concomitant cost of non-local operations. 3 Actually, the 1000 cycle simulator quantum does produce inaccuracy in sequential Proteus, but this has been fixed in Parallel Proteus <ref> [17] </ref>. 5.3 Compile-time Analysis Because the predictive barrier scheduling runtime analysis involves stopping at communication operations, those host processors simulating frequently-communicating threads will have less work to do at some synchronization points than those simulating threads that communicate infrequently.
Reference: [18] <author> David Nicol and Richard Fujimoto. </author> <title> Parallel simulation today. </title> <type> Technical Report 92-62, </type> <institution> ICASE, </institution> <year> 1992. </year>
Reference: [19] <author> David M. Nicol. </author> <title> Performance bounds on parallel self-initiating discrete-event simulations. </title> <journal> ACM Transactions on Modeling and Computer Simulations, </journal> <volume> 1(1) </volume> <pages> 24-50, </pages> <year> 1991. </year>
Reference: [20] <author> David M. Nicol. </author> <title> The cost of conservative synchronization in parallel discrete event simulations. </title> <journal> Journal of the ACM, </journal> <volume> 40(2) </volume> <pages> 304-333, </pages> <month> April </month> <year> 1993. </year>
Reference: [21] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C.Lewis, and David A. Wood. </author> <title> The Wisconsin Wind Tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: In contrast, our work deals with the synchronization overhead that dominates when low overhead techniques are used to simulate processor behavior. The Wisconsin Wind Tunnel and Parallel Tango Lite are two direct execution-based parallel simulators of shared memory multi-computers <ref> [21] </ref> [14]. They synchronize using periodic global barriers. They achieve good performance by decreasing network simulation accuracy, which allows them to synchronize infrequently. The WWT researchers have explored a range of network simulation models (from very accurate to not very accurate) [7].
References-found: 21

