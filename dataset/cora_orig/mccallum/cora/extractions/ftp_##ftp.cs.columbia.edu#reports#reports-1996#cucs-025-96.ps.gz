URL: ftp://ftp.cs.columbia.edu/reports/reports-1996/cucs-025-96.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1996.html
Root-URL: http://www.cs.columbia.edu
Title: Computational Complexity of Continuous Problems  
Author: Henryk Wozniakowski 
Date: May 20, 1996  
Affiliation: Columbia University Computer Science Department  University of Warsaw and Columbia University  
Pubnum: Report CUCS-025-96  
Abstract: Computational complexity studies the intrinsic difficulty of solving mathematically posed problems. Information-based complexity is a branch of computational complexity that deals with continuous problems defined on spaces of multivariate functions. For such problems only approximate solutions are possible to compute. The complexity is defined as the minimal cost needed to compute an approximation with error at most ". Error and cost can be defined in different settings such as the worst case, average case, probabilistic or randomized settings. In this paper, we survey recent results on complexity of linear multivariate problems and on path integration. In particular, we show that multivariate integration and approximation are (strongly) tractable in the average case setting for the class of continuous functions equipped with the Wiener sheet measure. This means that their complexity is a polynomial in " 1 . We consider path integration for the Wiener measure in the worst case and randomized settings. For the class of r times Frechet differentiable functions, the problem is intractable in the worst case setting, whereas it is tractable in the randomized setting and the classical Monte Carlo algorithm is optimal. On the other hand, for the specific class of entire functions, the problem is tractable in the worst case setting and its complexity is proportional to " 2=3 . 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. S. Bakhvalov, </author> <title> On approximate calculation of integrals (in Russian), </title> <journal> Vestnik MGU, Ser. Mat. Mekh. Astron. Fiz. Khim., </journal> <volume> 4, </volume> <pages> 3-18, </pages> <year> 1959. </year>
Reference-contexts: The approximation U can thus be identified with the pair (N; ), where N is an information operator and an algorithm that uses the information N . We illustrate these concepts by an example. Example: Integration Let F be a class of functions f : <ref> [0; 1] </ref> ! IR that satisfy a Lipschitz condition with constant q, jf (x) f (y)j q jx yj; 8 x; y 2 [0; 1]: S (f ) = 0 5 The class fl is a collection of L : F ! IR, such that for some x from [0; 1], <p> We illustrate these concepts by an example. Example: Integration Let F be a class of functions f : <ref> [0; 1] </ref> ! IR that satisfy a Lipschitz condition with constant q, jf (x) f (y)j q jx yj; 8 x; y 2 [0; 1]: S (f ) = 0 5 The class fl is a collection of L : F ! IR, such that for some x from [0; 1], L (f ) = f (x), 8f 2 F . <p> : <ref> [0; 1] </ref> ! IR that satisfy a Lipschitz condition with constant q, jf (x) f (y)j q jx yj; 8 x; y 2 [0; 1]: S (f ) = 0 5 The class fl is a collection of L : F ! IR, such that for some x from [0; 1], L (f ) = f (x), 8f 2 F . The information N is given by N (f ) = [f (x 1 ); f (x 2 ); : : : ; f (x n )] with the points x i and the number n adaptively chosen. <p> In the average case setting the error and cost of U are defined as e (U ) = F 1=2 cost (U ) = Z cost (U; f ) (df ): In the probabilistic setting, we assume that we are given a number ffi 2 <ref> [0; 1] </ref>, and the error and cost of U are defined as e (U ) = inf sup kS (f ) U (f )k : A such that (A) ffi ) cost (U ) = sup f2F cost (U; f ): We now discuss a randomized setting. <p> That is, t = [t 1 ; t 2 ; : : : ; t n ] 2 T = <ref> [0; 1] </ref> n and is the uniform distribution over the unit n dimensional cube. <p> By a linear multi-variate problem we mean an approximation of a linear operator defined on functions f of d variables. More precisely, let F d be a class of functions f : <ref> [0; 1] </ref> d ! IR, and let S d : F d ! G d ; where G d is a normed linear space. We wish to approximate S d (f ) for f 2 F d . <p> Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>. <p> We illustrate the results for multivariate approximation and integration for the class fl std in the average case setting for the class of continuous functions f : <ref> [0; 1] </ref> d ! IR equipped with the Wiener sheet measure. <p> we replace the Wiener sheet measure by the isotropic Wiener measure then the approximation problem is intractable since comp ("; d) = fi Consider now the integration problem S d f = R [0;1] d f (x)dx in the average case setting for the class of continuous functions f : <ref> [0; 1] </ref> d ! IR equipped with the Wiener sheet measure.
Reference: [2] <author> L. Blum, M. Shub and S. Smale, </author> <title> On a Theory of Computation and Complexity over the Real Numbers: NP-Completeness, Recursive Functions and Universal Machines Bull. </title> <journal> Amer. Math. Soc., </journal> <volume> 21, </volume> <pages> 1-46, </pages> <year> 1989. </year>
Reference-contexts: That is why, for continuous problems, we usually choose the real number model and study computational complexity in this model. For the precise definition of the real number model the reader is referred to <ref> [2, 12] </ref>. Continuous computational complexity may be split into two branches. The first branch deals with problems for which the information is complete. Informally, information may be complete for problems which are specified by a finite number of inputs. <p> The constant in the theta notation of the latter bound is, unfortunately, huge. We stress that problems with complete information may be very hard in the real number model. The first NP-complete problem over the reals was established in <ref> [2] </ref>. This is the problem of deciding whether a real polynomial of degree 4 in n variables has a real root. Hence, modulo the conjecture P 6= NP, but this time over the reals, the complexity of the latter problem is not polynomial in n.
Reference: [3] <author> D. Coppersmith and S. Winograd, </author> <title> Matrix multiplication via arithmetic progression, </title> <booktitle> in Proc. of the Nineteenth ACM Symp. on Theor. of Comp., </booktitle> <pages> 1-6, </pages> <address> New York, </address> <year> 1987. </year>
Reference-contexts: In 1969, Strassen [17] found an algorithm which computes the solution using fi (n log 2 7 ) arithmetic operations. Since log 2 7 = 2:81:::, this yields a better upper bound, at least for large n. Today, the best known upper bound is due to Coppersmith and Winograd <ref> [3] </ref> and it is fi (n 2:376 ). The constant in the theta notation of the latter bound is, unfortunately, huge. We stress that problems with complete information may be very hard in the real number model. The first NP-complete problem over the reals was established in [2].
Reference: [4] <author> M. R. Garey and D. S. Johnson, </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: The Turing machine model is used for discrete problems, and there is a deep theory culminating in the famous question whether P 6= NP, see e.g., <ref> [4] </ref>. In continuous computational complexity, we study continuous problems. Many scientific phenomena correspond to continuous problems. They are usually solved using fixed precision floating point arithmetic. The cost of floating point operations is independent of the size of the numbers. Furthermore, all arithmetic operations cost about the same to execute.
Reference: [5] <author> S. Heinrich, </author> <title> Random Approximation in Numerical Analysis, </title> <booktitle> in Proc. of the Functional Analysis Conference, Essen 1991, Lecture Notes in Pure and Applied Mathematics, </booktitle> <editor> ed. K. D. Bierstedt et al. </editor> , <volume> vol. 150, </volume> <publisher> Marcel Dekker, </publisher> <address> New York, 123-171, </address> <year> 1993. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues.
Reference: [6] <author> S. Heinrich, </author> <title> Complexity of integral equations and relations to s-numbers, </title> <journal> J. Complexity, </journal> <volume> 9, </volume> <pages> 141-153, </pages> <year> 1993. </year>
Reference-contexts: Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [7] <author> P. Mathe, </author> <title> Random approximation of Sobolev embedding, </title> <journal> J. Complexity, </journal> <volume> 7, </volume> <pages> 261-281, </pages> <year> 1993. </year>
Reference-contexts: In the average case and randomized settings, the curse of dimension is present for approximation over the class of functions with r continuous derivatives which is equipped with the folded isotropic Wiener measure, see [15, 22] for the average case, and <ref> [7, 10, 21] </ref> for the randomized setting. For some problems we can break the curse of dimension by switching to a different setting. For example, in the randomized setting, it is well known that the classical Monte Carlo algorithm breaks the curse of dimension for multivariate integration.
Reference: [8] <author> A. S. Nemirovsky and D. B. Yudin, </author> <title> Problem Complexity and Method Efficiency in Optimization, </title> <publisher> Wiley-Interscience, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [9] <author> E. Novak, </author> <title> Deterministic and Stochastic Error Bounds in Numerical Analysis, Lectures Notes in Math., </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <note> vol. 1349, </note> <year> 1988. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues. <p> 0 @ c @ ln (1=ffi) 1 1=2 1 A as " ! 0: Finally, in the randomized setting we have comp ran (") = fi " as " ! 0: The complexity of integration in different settings has been studied for various classes of functions by many researchers, see <ref> [9, 18] </ref> for a list of references. 2 One of the main goals of IBC is to find or estimate the "-complexity, and to find an "- complexity optimal U , or equivalently, an "-complexity optimal pair (N; ). <p> Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [10] <author> E. Novak, </author> <title> Optimal linear randomization methods for linear operators in Hilbert spaces, </title> <journal> J. Complexity, </journal> <volume> 8, </volume> <pages> 22-36, </pages> <year> 1992. </year>
Reference-contexts: In the average case and randomized settings, the curse of dimension is present for approximation over the class of functions with r continuous derivatives which is equipped with the folded isotropic Wiener measure, see [15, 22] for the average case, and <ref> [7, 10, 21] </ref> for the randomized setting. For some problems we can break the curse of dimension by switching to a different setting. For example, in the randomized setting, it is well known that the classical Monte Carlo algorithm breaks the curse of dimension for multivariate integration. <p> Roughly speaking, tractability and strong tractability hold if the singular values tend to zero sufficiently fast. Tractability and strong tractability in the randomized setting and the worst case setting are equivalent, and the corresponding complexities differ only by constants. This follows easily from <ref> [10] </ref>. Similarly, tractability and strong tractability in the probabilistic setting and the average case setting are equivalent due to relations between these two settings for linear problems, see [10]. <p> This follows easily from <ref> [10] </ref>. Similarly, tractability and strong tractability in the probabilistic setting and the average case setting are equivalent due to relations between these two settings for linear problems, see [10]. We stress that for the class fl all the construction of an "-approximation with minimal cost is easy since we know the optimal choice of linear functionals, and that linear algorithms are optimal. We now turn to the class fl std .
Reference: [11] <author> E. Novak, </author> <title> Algorithms and complexity for continuous problems, in Geometry, Analysis, and Mechanics, </title> <editor> ed. J. M. Rassias, </editor> <publisher> World Scientific, Singapore, </publisher> <pages> 96-128, </pages> <year> 1994. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues.
Reference: [12] <author> E. Novak, </author> <title> The real number model in numerical analysis, </title> <journal> J. Complexity, </journal> <volume> 11, </volume> <pages> 57-73, </pages> <year> 1995. </year>
Reference-contexts: That is why, for continuous problems, we usually choose the real number model and study computational complexity in this model. For the precise definition of the real number model the reader is referred to <ref> [2, 12] </ref>. Continuous computational complexity may be split into two branches. The first branch deals with problems for which the information is complete. Informally, information may be complete for problems which are specified by a finite number of inputs.
Reference: [13] <author> S. V. Pereverzev, </author> <title> On the complexity of the problem of finding solutions of Fredholm equations of the second kind with differentiable kernels (in Russian), Ukrain. </title> <journal> Mat. Sh., </journal> <volume> 41, </volume> <pages> 1422-1425, </pages> <year> 1989. </year>
Reference-contexts: Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [14] <author> L. Plaskota, </author> <title> Noisy Information and Computational Complexity, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1996. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues.
Reference: [15] <author> K. Ritter and G. W. Wasilkowski, </author> <title> Integration and L 2 -approximation: average case setting with isotropic Wiener measure for smooth functions, Rocky Mount. </title> <journal> J. Math., </journal> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: In the average case and randomized settings, the curse of dimension is present for approximation over the class of functions with r continuous derivatives which is equipped with the folded isotropic Wiener measure, see <ref> [15, 22] </ref> for the average case, and [7, 10, 21] for the randomized setting. For some problems we can break the curse of dimension by switching to a different setting. <p> For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see [23, 29], however, as already mentioned, it is not broken for the isotropic Wiener measure, see <ref> [15, 22] </ref>. It seems natural to characterize which multivariate problems are tractable or strongly tractable in various settings.
Reference: [16] <author> S. A. Smolyak, </author> <title> Quadrature and interpolation formulas for tensor products of certain classes of functions, </title> <journal> Dokl. Akad. Nauk SSSR, </journal> <volume> p. </volume> <pages> 240-243, </pages> <year> 1963. </year>
Reference-contexts: A construction is known for linear multivariate problems that are defined by tensor products, [23, 31]. For tractable tensor product problems and for the class fl std , we construct polynomial-time algorithms, see [23]. This construction is based on Smolyak's algorithm, see <ref> [16] </ref>.
Reference: [17] <author> V. Strassen, </author> <title> Gaussian elimination is not optimal, </title> <journal> Numer. Math., </journal> <volume> 13, </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: Examples of such algorithms include Gaussian elimination and Householder's 1 By fi (n 3 ) we mean a function which can be bounded from below and from above by a multiple of n 3 . 2 method. However, we can do better. In 1969, Strassen <ref> [17] </ref> found an algorithm which computes the solution using fi (n log 2 7 ) arithmetic operations. Since log 2 7 = 2:81:::, this yields a better upper bound, at least for large n.
Reference: [18] <author> J. F. Traub, G. W. Wasilkowski and H. Wozniakowski, </author> <title> Information-Based Complexity, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues. <p> 0 @ c @ ln (1=ffi) 1 1=2 1 A as " ! 0: Finally, in the randomized setting we have comp ran (") = fi " as " ! 0: The complexity of integration in different settings has been studied for various classes of functions by many researchers, see <ref> [9, 18] </ref> for a list of references. 2 One of the main goals of IBC is to find or estimate the "-complexity, and to find an "- complexity optimal U , or equivalently, an "-complexity optimal pair (N; ). <p> consisting of m (") information operations, and a mapping " such that the approximation U (f ) = " (N " (f )) has error at most " and U (f ) can be computed with cost at most (c + 2) m ("). (For examples of such problems see <ref> [18] </ref>, Chapter 5 and 7). <p> Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [19] <author> J. F. Traub and H. Wozniakowski, </author> <title> Theory and Applications of Information-Based Complexity, 1990 Lectures in Complex Systems, </title> <institution> Santa Fe Institute, Lect. </institution> <note> Vol. </note> <editor> III, Eds. L. Nadel and D. Stein, </editor> <publisher> Addison-Wesley, </publisher> <pages> 163-193, </pages> <year> 1991. </year>
Reference-contexts: A proof technique which leads to tight complexity bounds for some problems will also be indicated. Let S : F ! G; 2 This section is based on Section 2 of <ref> [19] </ref>. 4 where F is a subset of a linear space and G is a normed linear space over the real or complex field. We wish to approximate S (f ) for all f from F .
Reference: [20] <author> J. F. Traub and H. Wozniakowski, </author> <title> Recent Progress in Information-Based Complexity, </title> <journal> Bulletin of EATCS, </journal> <volume> 51, </volume> <pages> 141-154, </pages> <year> 1993. </year> <month> 17 </month>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues.
Reference: [21] <author> G. W. Wasilkowski, </author> <title> Randomization for Continuous Problems, </title> <journal> J. Complexity, </journal> <volume> 5, </volume> <pages> 195-218, </pages> <year> 1989. </year>
Reference-contexts: In the average case and randomized settings, the curse of dimension is present for approximation over the class of functions with r continuous derivatives which is equipped with the folded isotropic Wiener measure, see [15, 22] for the average case, and <ref> [7, 10, 21] </ref> for the randomized setting. For some problems we can break the curse of dimension by switching to a different setting. For example, in the randomized setting, it is well known that the classical Monte Carlo algorithm breaks the curse of dimension for multivariate integration.
Reference: [22] <author> G. W. Wasilkowski, </author> <title> Integration and approximation of multivariate functions: average case complexity with isotropic Wiener measure, </title> <journal> Bull. Amer. Math. Soc. (N.S), </journal> <volume> 28, </volume> <pages> 308-314, </pages> <year> 1993. </year>
Reference-contexts: In the average case and randomized settings, the curse of dimension is present for approximation over the class of functions with r continuous derivatives which is equipped with the folded isotropic Wiener measure, see <ref> [15, 22] </ref> for the average case, and [7, 10, 21] for the randomized setting. For some problems we can break the curse of dimension by switching to a different setting. <p> For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see [23, 29], however, as already mentioned, it is not broken for the isotropic Wiener measure, see <ref> [15, 22] </ref>. It seems natural to characterize which multivariate problems are tractable or strongly tractable in various settings. <p> We would like again to add that the choice of the Wiener sheet measure is essential. It is known, see <ref> [22] </ref>, that if we replace the Wiener sheet measure by the isotropic Wiener measure then the approximation problem is intractable since comp ("; d) = fi Consider now the integration problem S d f = R [0;1] d f (x)dx in the average case setting for the class of continuous functions
Reference: [23] <author> G. W. Wasilkowski, and H. Wozniakowski, </author> <title> Explicit cost bounds of algorithms for mul-tivariate tensor product problems, </title> <journal> J. Complexity, </journal> <volume> 11, </volume> <pages> p. 1-56, </pages> <year> 1995. </year>
Reference-contexts: However, in general, the proof is not constructive. For the Wiener sheet measure, the proof is constructive and we know almost optimal algorithms, see <ref> [23, 28] </ref>. For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see [23, 29], however, as already mentioned, it is not broken for the isotropic Wiener measure, see [15, 22]. <p> For the Wiener sheet measure, the proof is constructive and we know almost optimal algorithms, see [23, 28]. For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see <ref> [23, 29] </ref>, however, as already mentioned, it is not broken for the isotropic Wiener measure, see [15, 22]. It seems natural to characterize which multivariate problems are tractable or strongly tractable in various settings. <p> As before, the proof for the class fl std is not constructive. A construction is known for linear multivariate problems that are defined by tensor products, <ref> [23, 31] </ref>. For tractable tensor product problems and for the class fl std , we construct polynomial-time algorithms, see [23]. This construction is based on Smolyak's algorithm, see [16]. <p> As before, the proof for the class fl std is not constructive. A construction is known for linear multivariate problems that are defined by tensor products, [23, 31]. For tractable tensor product problems and for the class fl std , we construct polynomial-time algorithms, see <ref> [23] </ref>. This construction is based on Smolyak's algorithm, see [16]. <p> We illustrate the results for multivariate approximation and integration for the class fl std in the average case setting for the class of continuous functions f : [0; 1] d ! IR equipped with the Wiener sheet measure. For the approximation problem, we know a linear algorithm, see <ref> [23] </ref>, that computes an "-approximation with cost cost ("; d) c (d) 0:8489 0:9189 + ln 1=" ! 2 (d1) " : This algorithm has optimal powers of " 1 and ln 1=" since comp ("; d) = fi 2 (d1) ; see [29]. <p> Then we know a linear algorithm, see <ref> [23] </ref>, which computes an "-approximation with cost ("; d) bounded by cost ("; d) c (d) 3:304 1:12167 + ln 1=" ! 1:5 (d1) " The power of " 1 is optimal and the power of ln 1=" is too large since comp ("; d) = fi (d1)=2 ; see [28].
Reference: [24] <author> G. W. Wasilkowski, and H. Wozniakowski, </author> <title> On tractability of path integration, </title> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: In particular, multivariate integration and approximation are strongly tractable in the average case setting for the class of continuous functions equipped with the Wiener sheet measure. Specific complexity bounds are given in Section 3. The final section deals with path integration, see <ref> [24, 25] </ref>. Usually Monte Carlo algorithms are used to approximate path integrals. We study deterministic algorithms in the worst case setting. Then path integration is tractable (i.e., its complexity is polynomial in " 1 ) if the class of integrands consists of entire functions. <p> In this case, there exists no effective deterministic algorithm, and the use of randomized algorithms is reasonable. In fact, for this class of integrands, the classical Monte Carlo algorithm is optimal and the complexity in the randomized setting is proportional to " 2 , see <ref> [24] </ref>. On the other hand, for a particular class F of entire integrands, the worst case complexity of path integration is at most of order " p with p depending on the Gaussian measure . Hence, path integration is now tractable. <p> For the Wiener measure we have p = 2=3. For this class of entire integrands, we provide effective deterministic algorithms that solve the path 15 integration problem with (worst case) cost that is usually much less than the (randomized) cost of the classical Monte Carlo algorithm, see <ref> [24] </ref>. In [25] we consider a class of functions related to the Feynman-Kac formula. More precisely, this is the class of potential and initial conditions functions that define the heat equation.
Reference: [25] <author> G. W. Wasilkowski, and H. Wozniakowski, </author> <title> Worst case complexity of Feynman-Kac path integration, </title> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: In particular, multivariate integration and approximation are strongly tractable in the average case setting for the class of continuous functions equipped with the Wiener sheet measure. Specific complexity bounds are given in Section 3. The final section deals with path integration, see <ref> [24, 25] </ref>. Usually Monte Carlo algorithms are used to approximate path integrals. We study deterministic algorithms in the worst case setting. Then path integration is tractable (i.e., its complexity is polynomial in " 1 ) if the class of integrands consists of entire functions. <p> For the Wiener measure we have p = 2=3. For this class of entire integrands, we provide effective deterministic algorithms that solve the path 15 integration problem with (worst case) cost that is usually much less than the (randomized) cost of the classical Monte Carlo algorithm, see [24]. In <ref> [25] </ref> we consider a class of functions related to the Feynman-Kac formula. More precisely, this is the class of potential and initial conditions functions that define the heat equation.
Reference: [26] <author> G. W. Wasilkowski, and H. Wozniakowski, </author> <title> The exponent of discrepancy is at most 1:4778:::, </title> <note> to appear, </note> <year> 1995. </year>
Reference-contexts: This integration problem is strongly tractable since cost ("; d) c (d) 7:26 " 2:454 : 14 The exponent 2:454 is too high. There exists an algorithm with an exponent at most 1:4788:::, see <ref> [26] </ref>. The proof of this latter fact is, however, not constructive. This integration problem is related to discrepancy in the L 2 -norm, see [28].
Reference: [27] <author> A. G. Werschulz, </author> <title> The Computational Complexity of Differential and Integral Equations: An Information-Based Approach Oxford University Press, </title> <publisher> Oxford, </publisher> <year> 1991. </year>
Reference-contexts: The "-complexity is then defined as the minimal cost of computing an approximation with error at most ". The reader who wants to find more about IBC is referred to the books and recent surveys <ref> [5, 9, 11, 14, 18, 20, 27] </ref>. We believe that the readers of this proceedings are mainly interested in solving scientific problems for which only partial information is available. That is why we restrict ourselves in the rest of this paper to IBC issues. <p> Problems which suffer the curse of dimension in the worst case setting include integration, approximation, global optimization, integral and partial differential equations for classes of functions whose rth derivatives are uniformly bounded in L 1 , see <ref> [1, 6, 8, 9, 13, 18, 27] </ref>.
Reference: [28] <author> H. Wozniakowski, </author> <title> Average Case Complexity of Multivariate Integration, </title> <journal> Bull. Amer. Math. Soc. (N.S), </journal> <volume> 24, </volume> <pages> p. 185-194, </pages> <year> 1991. </year>
Reference-contexts: However, in general, the proof is not constructive. For the Wiener sheet measure, the proof is constructive and we know almost optimal algorithms, see <ref> [23, 28] </ref>. For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see [23, 29], however, as already mentioned, it is not broken for the isotropic Wiener measure, see [15, 22]. <p> [23], which computes an "-approximation with cost ("; d) bounded by cost ("; d) c (d) 3:304 1:12167 + ln 1=" ! 1:5 (d1) " The power of " 1 is optimal and the power of ln 1=" is too large since comp ("; d) = fi (d1)=2 ; see <ref> [28] </ref>. This integration problem is strongly tractable since cost ("; d) c (d) 7:26 " 2:454 : 14 The exponent 2:454 is too high. There exists an algorithm with an exponent at most 1:4788:::, see [26]. The proof of this latter fact is, however, not constructive. <p> There exists an algorithm with an exponent at most 1:4788:::, see [26]. The proof of this latter fact is, however, not constructive. This integration problem is related to discrepancy in the L 2 -norm, see <ref> [28] </ref>.
Reference: [29] <author> H. Wozniakowski, </author> <title> Average case complexity of linear multivariate problems: Part I. Theory, Part II. </title> <journal> Applications, J. Complexity, </journal> <volume> 8, </volume> <pages> 337-372, 373-392, </pages> <year> 1992. </year>
Reference-contexts: For the Wiener sheet measure, the proof is constructive and we know almost optimal algorithms, see [23, 28]. For multivariate approximation, the curse of dimension is broken only for some probability measures. For instance, it is broken for the Wiener sheet measure, see <ref> [23, 29] </ref>, however, as already mentioned, it is not broken for the isotropic Wiener measure, see [15, 22]. It seems natural to characterize which multivariate problems are tractable or strongly tractable in various settings. <p> problem, we know a linear algorithm, see [23], that computes an "-approximation with cost cost ("; d) c (d) 0:8489 0:9189 + ln 1=" ! 2 (d1) " : This algorithm has optimal powers of " 1 and ln 1=" since comp ("; d) = fi 2 (d1) ; see <ref> [29] </ref>. This approximation problem is strongly tractable since cost ("; d) c (d) 2:37632 " 5:672 : The exponent 5:672 seems to be too high; however, no smaller exponent has been found so far. We would like again to add that the choice of the Wiener sheet measure is essential.
Reference: [30] <author> H. Wozniakowski, </author> <title> Tractability and strong tractability of linear multivariate problems, </title> <journal> J. Complexity, </journal> <volume> 10, </volume> <pages> 96-128, </pages> <year> 1994. </year>
Reference-contexts: It is called strongly tractable if its complexity is independent of d and depends polynomially on " 1 . There are some general results characterizing which linear multivariate problems are tractable or strongly tractable, see, <ref> [30] </ref>. In particular, multivariate integration and approximation are strongly tractable in the average case setting for the class of continuous functions equipped with the Wiener sheet measure. Specific complexity bounds are given in Section 3. The final section deals with path integration, see [24, 25]. <p> For strongly tractable problems, the only dependence of the complexity on d is through the cost c (d). Tractability and strong tractability of linear multivariate problems have been studied in <ref> [30] </ref> for the information classes fl std and fl all . In the worst case and randomized settings we assume that the domain F d and the range of S d are Hilbert spaces. <p> We stress that for the class fl all the construction of an "-approximation with minimal cost is easy since we know the optimal choice of linear functionals, and that linear algorithms are optimal. We now turn to the class fl std . Under mild assumptions, we prove in <ref> [30] </ref> that tractability and strong tractability in the classes fl std and fl all are equivalent. In particular, we prove that the exponents in " 1 may differ by at most two. The proof of this equivalence is, however, not constructive. <p> a reproducing kernel Hilbert space and the linear problem is suitably normalized, then there exists a constant K such that comp ("; d) K c (d) " p ; where p = 2 for the class fl all , and p = 4 for the class fl std , see <ref> [30] </ref>. It is also known that p = 2 for the class fl all is sharp, whereas it is open whether p = 4 for the class fl std can be improved. As before, the proof for the class fl std is not constructive.
Reference: [31] <author> H. Wozniakowski, </author> <title> Tractability and strong tractability of multivariate tensor product problems, </title> <journal> J. Computing and Information, </journal> <volume> 4, </volume> <pages> 1-19, </pages> <year> 1994. </year>
Reference-contexts: As before, the proof for the class fl std is not constructive. A construction is known for linear multivariate problems that are defined by tensor products, <ref> [23, 31] </ref>. For tractable tensor product problems and for the class fl std , we construct polynomial-time algorithms, see [23]. This construction is based on Smolyak's algorithm, see [16].
Reference: [32] <author> H. Wozniakowski, </author> <title> Overview of information-based complexity, </title> <note> in Lectures in Applied Mathematics, </note> <author> eds. J. Renegar, M. Shub and S. Smale, </author> <note> to apper, </note> <year> 1995. </year> <title> Author's Address: </title> <type> 18 Henryk Wozniakowski, </type> <institution> Department of Computer Science, Columbia University, </institution> <address> New York, NY 10027, USA, </address> <institution> and Institute of Applied Mathematics, University of Warsaw, </institution> <address> ul. Banacha 2, 02-097 Warszawa, Poland, email: henryk@cs.columbia.edu 19 </address>
Reference-contexts: We are particularly interested in the complexity for large d and/or in large " 1 . To stress the dependence on the error parameter " and on the number of variables d, we denote the complexity by comp ("; d). 3 This section is based on Section 3 of <ref> [32] </ref>. 11 Many multivariate problems are intractable and their complexity grows exponentially with the number d of variables. This is sometimes called the curse of dimension.
References-found: 32

