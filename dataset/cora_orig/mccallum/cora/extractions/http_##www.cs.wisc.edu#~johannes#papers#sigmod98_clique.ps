URL: http://www.cs.wisc.edu/~johannes/papers/sigmod98_clique.ps
Refering-URL: http://www.cs.wisc.edu/~johannes/publications.html
Root-URL: 
Title: Automatic Subspace Clustering of High Dimensional Data for Data Mining Applications  
Author: Rakesh Agrawal Johannes Gehrke Dimitrios Gunopulos Prabhakar Raghavan 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: Data mining applications place special requirements on clustering algorithms including: the ability to find clusters embedded in subspaces of high dimensional data, scalability, end-user comprehensibility of the results, non-presumption of any canonical data distribution, and insensitivity to the order of input records. We present CLIQUE, a clustering algorithm that satisfies each of these requirements. CLIQUE identifies dense clusters in subspaces of maximum dimensionality. It generates cluster descriptions in the form of DNF expressions that are minimized for ease of comprehension. It produces identical results irrespective of the order in which input records are presented and does not presume any specific mathematical form for data distribution. Through experiments, we show that CLIQUE efficiently finds accurate clusters in large high dimensional datasets. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, H. Mannila, R. Srikant, H. Toivonen, and A. I. Verkamo. </author> <title> Fast Discovery of Association Rules. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthu-rusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 12, </booktitle> <pages> pages 307-328. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: This approach is infeasible for high dimensional data. We use a bottom-up algorithm that exploits the monotonicity of the clustering criterion with respect to dimensionality to prune the search space. This algorithm is similar to the Apriori algorithm for mining Association rules <ref> [1] </ref>. A somewhat similar bottom-up scheme was also used in [10] for determining modes in high dimensional histograms. Lemma 1 (Monotonicity): If a collection of points S is a cluster in a k-dimensional space, then S is also part of a cluster in any (k1)-dimensional projections of this space. <p> Thus, the algorithm can work with databases of any size. However, memory needs to be managed carefully as the candidates may swamp the available buffer. This situation is handled by employing a scheme used in <ref> [1] </ref>. As many candidates of C k are generated as will fit in the buffer and database is scanned to determine the selectivity of these candidates. Dense units resulting from these candidates are written to disk, while non-dense candidates are deleted. <p> The running time of our algorithm is therefore exponential in the highest dimensionality of any dense unit. As in <ref> [1] </ref> [20], it can be shown that the candidate generation procedure produces the minimal number of candidates that can guarantee that all dense units will be found. Let k be the highest dimensionality of any dense unit and m the number of the input points. <p> We assume therefore that the dense units in this and subsequent steps of CLIQUE can be stored in memory. We give asymptotic running times in terms of dense unit accesses; the dense units are stored in a main memory data structure (hash tree <ref> [1] </ref>) that allows efficient querying. For each dense unit visited, the algorithm checks its 2k neighbors to find connected units.
Reference: [2] <author> A. Aho, J. Hopcroft, and J. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <address> Addison-Welsley, </address> <year> 1974. </year>
Reference-contexts: On the other hand, units corresponding to vertices in different components cannot be connected, and therefore cannot be in the same cluster. We use a depth-first search algorithm <ref> [2] </ref> to find the connected components of the graph. We start with some unit u in D, assign it the first cluster number, and find all the units it is connected to.
Reference: [3] <author> P. Arabie and L. J. Hubert. </author> <title> An overview of combinatorial data analyis. </title> <editor> In P. Arabie, L. Hubert, and G. D. Soete, editors, </editor> <booktitle> Clustering and Classification, </booktitle> <pages> pages 5-63. </pages> <publisher> World Scientific Pub., </publisher> <address> New Jersey, </address> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] [25]. Clustering techniques have been studied extensively in statistics <ref> [3] </ref>, pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical.
Reference: [4] <institution> Arbor Software Corporation. </institution> <note> Application Manager User's Guide, Essbase Version 4.0 edition. </note>
Reference-contexts: To index OLAP data, for instance, the data space is first partitioned into dense and sparse regions [12]. Data in dense regions is stored in an array whereas a tree structure is used to store sparse regions. Currently, users are required to specify dense and sparse dimensions <ref> [4] </ref>. Similarly, the precomputation techniques for range queries over OLAP data cubes [21] require identification of dense regions in sparse data cubes. CLIQUE can be used for this purpose. In future work, we plan to address the problem of evaluating the quality of clusterings in different subspaces.
Reference: [5] <author> R. Bayardo. </author> <title> Efficiently mining long patterns from databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Seattle, Washington, </address> <year> 1998. </year>
Reference-contexts: Another area for future work is to try an alternative approach for finding dense units. If the user is only interested in clusters in the subspaces of highest dimensionality, we can use techniques based on recently proposed algorithms for discovering maximal itemsets <ref> [5] </ref> [26]. These techniques will allow CLIQUE to find dense units of high dimensionality without having to find all of their projections. Acknowledgment The code for CLIQUE builds on several components that Ramakrishnan Srikant wrote for quantitative association rules.
Reference: [6] <author> S. Berchtold, C. Bohm, D. Keim, and H.-P. Kriegel. </author> <title> A cost model for nearest neighbor search in high-dimensional data space. </title> <booktitle> In Proceedings of the 16th Symposium on Principles of Database Systems (PODS), </booktitle> <pages> pages 78-86, </pages> <year> 1997. </year>
Reference-contexts: It is not meaningful to look for clusters in such a high dimensional space as the average density of points anywhere in the data space is likely to be quite low <ref> [6] </ref>. Compounding this problem, many dimensions or combinations of dimensions can have noise or values that are uniformly distributed. Therefore, distance functions that use all the dimensions of the data may be ineffective. Moreover, several clusters may exist in different subspaces comprised of different combinations of attributes.
Reference: [7] <author> M. Berger and I. Regoutsos. </author> <title> An algorithm for point clustering and grid generation. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 21(5) </volume> <pages> 1278-86, </pages> <year> 1991. </year>
Reference-contexts: This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). Some clustering algorithms in image analysis (e.g. <ref> [7] </ref> [36] [42]) also find rectangular dense regions. In these domains, datasets are in low dimensional spaces and the techniques used are computationally too expensive for large datasets of high dimensionality. Our solution to the problem consists of two steps.
Reference: [8] <author> S. Brin, R. Motwani, J. D. Ullman, and S. Tsur. </author> <title> Dynamic itemset counting and implication rules for market basket data. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <month> May </month> <year> 1997. </year>
Reference-contexts: The algorithm makes k passes over the database. It follows that the running time of our algorithm is O (c k + m k) for a constant c. The number of database passes can be reduced by adapting ideas from [41] <ref> [8] </ref>. 3.1.2 Making the bottom-up algorithm faster While the procedure just described dramatically reduces the number of units that are tested for being dense, we still may have a computationally infeasible task at hand for high dimensional data.
Reference: [9] <author> P. Cheeseman and J. Stutz. </author> <title> Bayesian classification (auto-class): Theory and results. </title> <editor> In U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, chapter 6, </booktitle> <pages> pages 153-180. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning <ref> [9] </ref> [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical.
Reference: [10] <author> R. Chhikara and D. </author> <title> Register. A numerical classification method for partitioning of a large multidimensional mixed data set. </title> <journal> Technometrics, </journal> <volume> 21 </volume> <pages> 531-537, </pages> <year> 1979. </year>
Reference-contexts: We use a bottom-up algorithm that exploits the monotonicity of the clustering criterion with respect to dimensionality to prune the search space. This algorithm is similar to the Apriori algorithm for mining Association rules [1]. A somewhat similar bottom-up scheme was also used in <ref> [10] </ref> for determining modes in high dimensional histograms. Lemma 1 (Monotonicity): If a collection of points S is a cluster in a k-dimensional space, then S is also part of a cluster in any (k1)-dimensional projections of this space.
Reference: [11] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition <ref> [11] </ref> [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> However, user-identification of subspaces is quite error-prone. Another way to address high dimensionality is to apply a dimensionality reduction method to the dataset. Methods such as principal component analysis or Karhunen-Loeve transformation <ref> [11] </ref> [19] optimally transform the original data space into a lower dimensional space by forming dimensions that are linear combinations of given attributes. The new space has the property that distances between points remain approximately the same as before. <p> Table 2: DBSCAN experimental results. Dim. of Dim. of No. of Clusters True clusters data clusters clusters found identified 5 5 5 5 5 8 5 5 3 1 SVD: We also did Singular Value Decomposition (SVD) <ref> [11] </ref> [19] on the synthetic datasets to find if the dimensionality of the space can be reduced or if the subspaces that contain dense units can be deduced from the projections into the new space. Table 3: SVD decomposition experimental results. <p> and Raghu Ramakrishnan for providing access to DBSCAN, CLARANS, and BIRCH respectively. 6 Appendix: Dimensionality reduction The principal component analysis or Karhunen-Loeve (KL) transformation is the optimal way to project n-dimensional points to k-dimensional points such that the error of the projections (the sum of the squared distances) is minimal <ref> [11] </ref> [19]. This transformation gives a new set of orthogonal axes, each a linear combination of the original ones, sorted by the degree by which they preserve the distances of the points in the original space.
Reference: [12] <author> R. J. Earle. </author> <title> Method and apparatus for storing and retrieving multi-dimensional data in computer memory. </title> <type> U.S. Patent No. 5359724, </type> <month> October </month> <year> 1994. </year>
Reference-contexts: Automatic subspace clustering can be useful in other applications besides data mining. To index OLAP data, for instance, the data space is first partitioned into dense and sparse regions <ref> [12] </ref>. Data in dense regions is stored in an array whereas a tree structure is used to store sparse regions. Currently, users are required to specify dense and sparse dimensions [4].
Reference: [13] <author> M. Ester, H.-P. Kriegel, J. Sander, and X. Xu. </author> <title> A density-based algorithm for discovering clusters in large spatial databases with noise. </title> <booktitle> In Proc. of the 2nd Int'l Conference on Knowledge Discovery in Databases and Data Mining, </booktitle> <address> Port-land, Oregon, </address> <month> August </month> <year> 1996. </year>
Reference-contexts: Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN <ref> [13] </ref>. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> Mode-seeking clustering methods identify clusters by searching for regions in the data space in which the object density is large. DB-SCAN <ref> [13] </ref> finds dense regions that are separated by low density regions and clusters together the objects in the same dense region. A hierarchical clustering is a nested sequence of partitions.
Reference: [14] <author> M. Ester, H.-P. Kriegel, and X. Xu. </author> <title> A database interface for clustering in large spatial databases. </title> <booktitle> In Proc. of the 1st Int'l Conference on Knowledge Discovery in Databases and Data Mining, </booktitle> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year>
Reference-contexts: Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS <ref> [14] </ref>, BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> The popular K-means and K-medoid methods determine K cluster representatives and assign each object to the cluster with its representative closest to the object such that the sum of the distances squared between the objects and their representatives is minimized. CLARANS [33], Focused CLARANS <ref> [14] </ref>, and BIRCH [45] can be viewed as extensions of this fl Current Address: Department of Computer Science, University of Wisconsin, Madison, WI 53706. approach to work against large databases. Mode-seeking clustering methods identify clusters by searching for regions in the data space in which the object density is large.
Reference: [15] <editor> U. M. Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthu-rusamy, editors. </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Moreover, several clusters may exist in different subspaces comprised of different combinations of attributes. Interpretability of results: Data mining applications typically require cluster descriptions that can be easily assimilated by an end-user as insight and explanations are of critical importance <ref> [15] </ref>. It is particularly important to have simple representations because most visualization techniques do not work well in high dimensional spaces. Scalability and usability: The clustering technique should be fast and scale with the number of dimensions and the size of input. <p> Each of the original dimensions 1 For CLustering In QUEst, the data mining research project at IBM Almaden. typically has a real meaning to the user, while even a simple linear combination of many dimensions may be hard to interpret <ref> [15] </ref>. We use a density based approach to clustering: a cluster is a region that has a higher density of points than its surrounding region.
Reference: [16] <author> U. Feige. </author> <title> A threshold of ln n for approximating set cover. </title> <booktitle> In Proceedings of the Twenty-Eighth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 314-318, </pages> <year> 1996. </year>
Reference-contexts: Since this algorithm only works for the 2-dimensional case, it cannot be used in our setting. For the general set cover problem, the best known algorithm for approximating the smallest set cover gives an approximation factor of ln n where n is the size of the universe being covered <ref> [16] </ref> [28]. This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). <p> Thus it would appear that the addition heuristic, since its quality of approximation matches the negative results of <ref> [16] </ref> [28], would be the obvious choice. However, its implementation in our high dimensional geometric setting is too inefficient. The implementation requires the rather complex computation of the number of uncovered units a candidate maximal region will cover.
Reference: [17] <author> D. Franzblau. </author> <title> Performance guarantees on a sweep-line heuristic for covering rectilinear polygons with rectangles. </title> <journal> SIAM J. Disc. Math, </journal> <volume> 2 </volume> <pages> 307-321, </pages> <month> 3 </month> <year> (1989). </year>
Reference-contexts: The optimal cover is the cover with the minimal number of rectangles. The best approximate algorithm known for the special case of finding a cover of a 2-dimensional rectilinear polygon with no holes produces a cover of size bounded by a factor of 2 times the optimal <ref> [17] </ref>. Since this algorithm only works for the 2-dimensional case, it cannot be used in our setting.
Reference: [18] <author> J. Friedman. </author> <title> Optimizing a noisy function of many variables with application to data mining. </title> <booktitle> In UW/MSR Summer Research Institute in Data Mining, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: On the other hand, we automatically discover the interesting subspaces, and also generate minimal descriptions for the clusters. A different technique to find rectangular clusters of high density in a projection of the data space has been proposed by Friedman <ref> [18] </ref>. This algorithm works in a top down fashion. Starting from the full space, it greedily chooses which projection should be taken and reevaluates the solution after each step in order to get closer to an optimal solution.
Reference: [19] <author> K. Fukunaga. </author> <title> Introduction to Statistical Pattern Recognition. </title> <publisher> Academic Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] <ref> [19] </ref>, and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> However, user-identification of subspaces is quite error-prone. Another way to address high dimensionality is to apply a dimensionality reduction method to the dataset. Methods such as principal component analysis or Karhunen-Loeve transformation [11] <ref> [19] </ref> optimally transform the original data space into a lower dimensional space by forming dimensions that are linear combinations of given attributes. The new space has the property that distances between points remain approximately the same as before. <p> Table 2: DBSCAN experimental results. Dim. of Dim. of No. of Clusters True clusters data clusters clusters found identified 5 5 5 5 5 8 5 5 3 1 SVD: We also did Singular Value Decomposition (SVD) [11] <ref> [19] </ref> on the synthetic datasets to find if the dimensionality of the space can be reduced or if the subspaces that contain dense units can be deduced from the projections into the new space. Table 3: SVD decomposition experimental results. <p> Raghu Ramakrishnan for providing access to DBSCAN, CLARANS, and BIRCH respectively. 6 Appendix: Dimensionality reduction The principal component analysis or Karhunen-Loeve (KL) transformation is the optimal way to project n-dimensional points to k-dimensional points such that the error of the projections (the sum of the squared distances) is minimal [11] <ref> [19] </ref>. This transformation gives a new set of orthogonal axes, each a linear combination of the original ones, sorted by the degree by which they preserve the distances of the points in the original space.
Reference: [20] <author> D. Gunopulos, R. Khardon, H. Mannila, and S. Saluja. </author> <title> Data mining, hypergraph transversals, </title> <booktitle> and machine learning. In Proc. of the 16th ACM Symp. on Principles of Database Systems, </booktitle> <pages> pages 209-216, </pages> <year> 1997. </year>
Reference-contexts: The running time of our algorithm is therefore exponential in the highest dimensionality of any dense unit. As in [1] <ref> [20] </ref>, it can be shown that the candidate generation procedure produces the minimal number of candidates that can guarantee that all dense units will be found. Let k be the highest dimensionality of any dense unit and m the number of the input points.
Reference: [21] <author> C.-T. Ho, R. Agrawal, N. Megiddo, and R. Srikant. </author> <title> Range queries in OLAP data cubes. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Tucson, Arizona, </address> <month> May </month> <year> 1997. </year>
Reference-contexts: Data in dense regions is stored in an array whereas a tree structure is used to store sparse regions. Currently, users are required to specify dense and sparse dimensions [4]. Similarly, the precomputation techniques for range queries over OLAP data cubes <ref> [21] </ref> require identification of dense regions in sparse data cubes. CLIQUE can be used for this purpose. In future work, we plan to address the problem of evaluating the quality of clusterings in different subspaces.
Reference: [22] <author> S. J. Hong. </author> <title> MINI: A heuristic algorithm for two-level logic minimization. In R. Newton, editor, Selected Papers on Logic Synthesis for Integrated Circuit Design. </title> <publisher> IEEE Press, </publisher> <year> 1987. </year>
Reference-contexts: This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. <ref> [22] </ref>). Some clustering algorithms in image analysis (e.g. [7] [36] [42]) also find rectangular dense regions. In these domains, datasets are in low dimensional spaces and the techniques used are computationally too expensive for large datasets of high dimensionality. Our solution to the problem consists of two steps.
Reference: [23] <institution> Internationl Business Machines. </institution> <note> IBM Intelligent Miner User's Guide, Version 1 Release 1, SH12-6213-00 edition, </note> <month> July </month> <year> 1996. </year>
Reference-contexts: Current clustering techniques do not address all these points adequately, although considerable work has been done in addressing each point separately. The problem of high dimensionality is often tackled by requiring the user to specify the subspace (a subset of the dimensions) for cluster analysis (e.g. <ref> [23] </ref>) . However, user-identification of subspaces is quite error-prone. Another way to address high dimensionality is to apply a dimensionality reduction method to the dataset. <p> Having demonstrated the computational feasibility of automatic subspace clustering, we believe it should be considered a basic data mining operation along with other operations such as associations and sequential-patterns discovery, time-series clustering, and classification <ref> [23] </ref>. Automatic subspace clustering can be useful in other applications besides data mining. To index OLAP data, for instance, the data space is first partitioned into dense and sparse regions [12]. Data in dense regions is stored in an array whereas a tree structure is used to store sparse regions.
Reference: [24] <author> A. K. Jain and R. C. Dubes. </author> <title> Algorithms for Clustering Data. </title> <publisher> Prentice Hall, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) <ref> [24] </ref> [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and <p> their attributes (dimensions) <ref> [24] </ref> [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. Given a set of objects and a clustering criterion [39], parti-tional clustering obtains a partition of the objects into clusters such that the objects in a cluster are more similar to each other than to objects in different clusters. <p> Divisive, hierarchical clustering reverses the process by starting with all objects in cluster and subdividing into smaller pieces <ref> [24] </ref>. 1.1 Desiderata from the data mining perspective Emerging data mining applications place the following special requirements on clustering techniques, motivating the need for developing new algorithms: Effective treatment of high dimensionality: An object (data record) typically has dozens of attributes and the domain for each attribute can be large.
Reference: [25] <author> L. Kaufman and P. Rousseeuw. </author> <title> Finding Groups in Data: An Introduction to Cluster Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] <ref> [25] </ref>. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> attributes (dimensions) [24] <ref> [25] </ref>. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. Given a set of objects and a clustering criterion [39], parti-tional clustering obtains a partition of the objects into clusters such that the objects in a cluster are more similar to each other than to objects in different clusters.
Reference: [26] <author> D.-I. Lin and Z. M. Kedem. </author> <title> Pincer search: A new algorithm for discovering the maximum frequent sets. </title> <booktitle> In Proc. of the 6th Int'l Conference on Extending Database Technology (EDBT), </booktitle> <address> Valencia, Spain, </address> <year> 1998. </year>
Reference-contexts: Another area for future work is to try an alternative approach for finding dense units. If the user is only interested in clusters in the subspaces of highest dimensionality, we can use techniques based on recently proposed algorithms for discovering maximal itemsets [5] <ref> [26] </ref>. These techniques will allow CLIQUE to find dense units of high dimensionality without having to find all of their projections. Acknowledgment The code for CLIQUE builds on several components that Ramakrishnan Srikant wrote for quantitative association rules.
Reference: [27] <author> L. Lovasz. </author> <title> On the ratio of the optimal integral and fractional covers. </title> <journal> Discrete Mathematics, </journal> <volume> 13 </volume> <pages> 383-390, </pages> <year> 1975. </year>
Reference-contexts: Break ties arbitrarily. Repeat the procedure until the whole cluster is covered. For general set cover, the addition heuristic is known to give a cover within a factor ln n of the optimum where n is the number of units to be covered <ref> [27] </ref>. Thus it would appear that the addition heuristic, since its quality of approximation matches the negative results of [16] [28], would be the obvious choice. However, its implementation in our high dimensional geometric setting is too inefficient.
Reference: [28] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <booktitle> In Proceedings of the ACM Symposium on Theory of Computing, </booktitle> <pages> pages 286-293, </pages> <year> 1993. </year>
Reference-contexts: For the general set cover problem, the best known algorithm for approximating the smallest set cover gives an approximation factor of ln n where n is the size of the universe being covered [16] <ref> [28] </ref>. This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). <p> Thus it would appear that the addition heuristic, since its quality of approximation matches the negative results of [16] <ref> [28] </ref>, would be the obvious choice. However, its implementation in our high dimensional geometric setting is too inefficient. The implementation requires the rather complex computation of the number of uncovered units a candidate maximal region will cover.
Reference: [29] <author> W. Masek. </author> <title> Some NP-complete set covering problems. M.S. </title> <type> Thesis, </type> <institution> MIT, </institution> <year> 1978. </year>
Reference-contexts: Computing the optimal cover is known to be NP-hard, even in the 2-dimensional case <ref> [29] </ref> [34]. The optimal cover is the cover with the minimal number of rectangles. The best approximate algorithm known for the special case of finding a cover of a 2-dimensional rectilinear polygon with no holes produces a cover of size bounded by a factor of 2 times the optimal [17].
Reference: [30] <author> M. Mehta, R. Agrawal, and J. Rissanen. SLIQ: </author> <title> A fast scalable classifier for data mining. </title> <booktitle> In Proc. of the Fifth Int'l Conference on Extending Database Technology (EDBT), </booktitle> <address> Avi-gnon, France, </address> <month> March </month> <year> 1996. </year>
Reference-contexts: The subspace identification problem is related to the problem of finding quantitative association rules that also identify interesting regions of various attributes [40] [32]. However, the techniques proposed are quite different. One can also imagine adapting a tree-classifier designed for data mining (e.g. <ref> [30] </ref> [37]) for subspace clustering. In the tree-growth phase, the splitting criterion will have to be changed so that some clustering criterion (e.g. average cluster diameter) is optimized.
Reference: [31] <author> R. S. Michalski and R. E. Stepp. </author> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> volume I, </volume> <pages> pages 331-363. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: 1 Introduction Clustering is a descriptive task that seeks to identify homogeneous groups of objects based on the values of their attributes (dimensions) [24] [25]. Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] <ref> [31] </ref>. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical.
Reference: [32] <author> R. Miller and Y. Yang. </author> <title> Association rules over interval data. </title> <booktitle> In Proc. ACM SIGMOD International Conf. on Management of Data, </booktitle> <pages> pages 452-461, </pages> <year> 1997. </year>
Reference-contexts: The subspace identification problem is related to the problem of finding quantitative association rules that also identify interesting regions of various attributes [40] <ref> [32] </ref>. However, the techniques proposed are quite different. One can also imagine adapting a tree-classifier designed for data mining (e.g. [30] [37]) for subspace clustering. In the tree-growth phase, the splitting criterion will have to be changed so that some clustering criterion (e.g. average cluster diameter) is optimized.
Reference: [33] <author> R. T. Ng and J. Han. </author> <title> Efficient and effective clustering methods for spatial data mining. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <address> Santiago, Chile, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS <ref> [33] </ref>, Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> The popular K-means and K-medoid methods determine K cluster representatives and assign each object to the cluster with its representative closest to the object such that the sum of the distances squared between the objects and their representatives is minimized. CLARANS <ref> [33] </ref>, Focused CLARANS [14], and BIRCH [45] can be viewed as extensions of this fl Current Address: Department of Computer Science, University of Wisconsin, Madison, WI 53706. approach to work against large databases. <p> We further discuss these point in the Appendix and Section 4. Clustering algorithms developed in the database community like BIRCH, CLARANS, and DBSCAN are designed to be scalable, an emphasis not present in the earlier work in the statistics and machine learning literature <ref> [33] </ref> [45]. However, these techniques were developed to discover clusters in the full dimensional space. It is not surprising therefore that they are not effective in identifying clusters that exist in the subspaces of the original data space.
Reference: [34] <author> R. A. Reckhow and J. Culberson. </author> <title> Covering simple orthogonal polygon with a minimum number of orthogonally convex polygons. </title> <booktitle> In Proc. of the ACM 3rd Annual Computational Geometry Conference, </booktitle> <pages> pages 268-277, </pages> <year> 1987. </year>
Reference-contexts: Computing the optimal cover is known to be NP-hard, even in the 2-dimensional case [29] <ref> [34] </ref>. The optimal cover is the cover with the minimal number of rectangles. The best approximate algorithm known for the special case of finding a cover of a 2-dimensional rectilinear polygon with no holes produces a cover of size bounded by a factor of 2 times the optimal [17].
Reference: [35] <author> J. Rissanen. </author> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific Publ. Co., </publisher> <year> 1989. </year>
Reference-contexts: MDL-based pruning: To decide which subspaces (and the corresponding dense units) are interesting, we apply the MDL (Minimal Description Length) principle. The basic idea underlying the MDL principle is to encode the input data under a given model and select the encoding that minimizes the code length <ref> [35] </ref>. Assume we have the subspaces S 1 ; S 2 ; : : : ; S n . Our pruning technique first groups together the dense units that lie in the same subspace.
Reference: [36] <author> P. Schroeter and J. Bigun. </author> <title> Hierarchical image segmentation by multi-dimensional clustering and orientation-adaptive boundary refinement. </title> <journal> Pattern Recognition, </journal> <volume> 25(5) </volume> <pages> 695-709, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). Some clustering algorithms in image analysis (e.g. [7] <ref> [36] </ref> [42]) also find rectangular dense regions. In these domains, datasets are in low dimensional spaces and the techniques used are computationally too expensive for large datasets of high dimensionality. Our solution to the problem consists of two steps.
Reference: [37] <author> J. Shafer, R. Agrawal, and M. Mehta. SPRINT: </author> <title> A scalable parallel classifier for data mining. </title> <booktitle> In Proc. of the 22nd Int'l Conference on Very Large Databases, </booktitle> <address> Bombay, India, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: The subspace identification problem is related to the problem of finding quantitative association rules that also identify interesting regions of various attributes [40] [32]. However, the techniques proposed are quite different. One can also imagine adapting a tree-classifier designed for data mining (e.g. [30] <ref> [37] </ref>) for subspace clustering. In the tree-growth phase, the splitting criterion will have to be changed so that some clustering criterion (e.g. average cluster diameter) is optimized.
Reference: [38] <author> A. Shoshani. </author> <type> Personal communication. </type> <year> 1997. </year>
Reference-contexts: Consequently, if this dimension is chosen for clustering, the clusters will have the same value in this dimension. Related Work: A similar approach to clustering high dimensional data has been proposed by Shoshani <ref> [38] </ref>. The technique computes an approximation of the density function in a user-specified subspace using a grid and then uses this function to cluster the data. On the other hand, we automatically discover the interesting subspaces, and also generate minimal descriptions for the clusters.
Reference: [39] <author> P. Sneath and R. Sokal. </author> <title> Numerical Taxonomy. </title> <publisher> Freeman, </publisher> <year> 1973. </year>
Reference-contexts: Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH [45], and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. Given a set of objects and a clustering criterion <ref> [39] </ref>, parti-tional clustering obtains a partition of the objects into clusters such that the objects in a cluster are more similar to each other than to objects in different clusters.
Reference: [40] <author> R. Srikant and R. Agrawal. </author> <title> Mining Quantitative Association Rules in Large Relational Tables. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The subspace identification problem is related to the problem of finding quantitative association rules that also identify interesting regions of various attributes <ref> [40] </ref> [32]. However, the techniques proposed are quite different. One can also imagine adapting a tree-classifier designed for data mining (e.g. [30] [37]) for subspace clustering. In the tree-growth phase, the splitting criterion will have to be changed so that some clustering criterion (e.g. average cluster diameter) is optimized.
Reference: [41] <author> H. Toivonen. </author> <title> Sampling large databases for association rules. </title> <booktitle> In Proc. of the 22nd Int'l Conference on Very Large Databases, </booktitle> <pages> pages 134-145, </pages> <address> Mumbai (Bombay), India, </address> <month> September </month> <year> 1996. </year>
Reference-contexts: The algorithm makes k passes over the database. It follows that the running time of our algorithm is O (c k + m k) for a constant c. The number of database passes can be reduced by adapting ideas from <ref> [41] </ref> [8]. 3.1.2 Making the bottom-up algorithm faster While the procedure just described dramatically reduces the number of units that are tested for being dense, we still may have a computationally infeasible task at hand for high dimensional data.
Reference: [42] <author> S. Wharton. </author> <title> A generalized histogram clustering for multidimensional image data. </title> <journal> Pattern Recognition, </journal> <volume> 16(2) </volume> <pages> 193-199, </pages> <year> 1983. </year>
Reference-contexts: This problem is similar to the problem of constructive solid geometry formulae in solid-modeling [44]. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). Some clustering algorithms in image analysis (e.g. [7] [36] <ref> [42] </ref>) also find rectangular dense regions. In these domains, datasets are in low dimensional spaces and the techniques used are computationally too expensive for large datasets of high dimensionality. Our solution to the problem consists of two steps.
Reference: [43] <author> M. Zait and H. Messatfa. </author> <title> A comparative study of clustering methods. </title> <booktitle> Future Generation Computer Systems, </booktitle> <address> 13(2-3):149-159, </address> <month> November </month> <year> 1997. </year>
Reference-contexts: The experiments were run on a 133-MHz IBM RS/6000 Model 43P workstation. The data resided in the AIX file system and was stored on a 2GB SCSI drive with sequential throughput of about 2 MB/second. 4.1 Synthetic data generation We use the synthetic data generator from <ref> [43] </ref> to produce datasets with clusters of high density in specific subspaces. The data generator allows control over the structure and the size of datasets through parameters such as the number of records, the number of attributes, and the range of values for each attribute.
Reference: [44] <author> D. Zhang and A. Bowyer. </author> <title> CSG set-theoretic solid modelling and NC machining of blend surfaces. </title> <booktitle> In Proceedings of the Second Annual ACM Symposium on Computational Geometry, </booktitle> <pages> pages 314-318, </pages> <year> 1986. </year>
Reference-contexts: For the general set cover problem, the best known algorithm for approximating the smallest set cover gives an approximation factor of ln n where n is the size of the universe being covered [16] [28]. This problem is similar to the problem of constructive solid geometry formulae in solid-modeling <ref> [44] </ref>. It is also related to the problem of covering marked boxes in a grid with rectangles in logic minimization (e.g. [22]). Some clustering algorithms in image analysis (e.g. [7] [36] [42]) also find rectangular dense regions.
Reference: [45] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> BIRCH: An efficient data clustering method for very large databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Clustering techniques have been studied extensively in statistics [3], pattern recognition [11] [19], and machine learning [9] [31]. Recent work in the database community includes CLARANS [33], Focused CLARANS [14], BIRCH <ref> [45] </ref>, and DBSCAN [13]. Current clustering techniques can be broadly classified into two categories [24] [25]: partitional and hierarchical. <p> The popular K-means and K-medoid methods determine K cluster representatives and assign each object to the cluster with its representative closest to the object such that the sum of the distances squared between the objects and their representatives is minimized. CLARANS [33], Focused CLARANS [14], and BIRCH <ref> [45] </ref> can be viewed as extensions of this fl Current Address: Department of Computer Science, University of Wisconsin, Madison, WI 53706. approach to work against large databases. Mode-seeking clustering methods identify clusters by searching for regions in the data space in which the object density is large. <p> We further discuss these point in the Appendix and Section 4. Clustering algorithms developed in the database community like BIRCH, CLARANS, and DBSCAN are designed to be scalable, an emphasis not present in the earlier work in the statistics and machine learning literature [33] <ref> [45] </ref>. However, these techniques were developed to discover clusters in the full dimensional space. It is not surprising therefore that they are not effective in identifying clusters that exist in the subspaces of the original data space.
References-found: 45

