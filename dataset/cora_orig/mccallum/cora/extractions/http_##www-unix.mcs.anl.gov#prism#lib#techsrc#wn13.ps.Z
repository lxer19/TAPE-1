URL: http://www-unix.mcs.anl.gov/prism/lib/techsrc/wn13.ps.Z
Refering-URL: http://www-unix.mcs.anl.gov/prism/lib/tech.html
Root-URL: http://www.mcs.anl.gov
Title: Comparison of Scalable Parallel Matrix Multiplication Libraries  
Author: Steven Huss-Lederman Elaine M. Jacobson Anna Tsao 
Address: 17100 Science Drive, Bowie, MD 20715  
Affiliation: Supercomputing Research Center,  
Abstract: This paper compares two general library routines for performing parallel distributed matrix multiplication. The PUMMA algorithm utilizes block scattered data layout, whereas BiMMeR utilizes virtual 2-D torus wrap. The algorithmic differences resulting from these different layouts are discussed as well as the general issues associated with different data layouts for library routines. Results on the Intel Delta for the two matrix multiplication algorithms are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification Version 1.0, </title> <year> 1993. </year>
Reference-contexts: are now required so all the data for B on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] <ref> [ 1 ] </ref> [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . <p> Finally, data layouts are starting to be incorporated into high level languages. At the current time the High Performance Fortran (HPF) standard only fully and directly supports the block scattered data layout <ref> [1] </ref>. Many applications users may only utilize the data layouts supported in high level languages to achieve ease of usage. Choosing which data layouts to support before the alternatives are investigated may discourage such investigation and limit the number of potential users of worthy alternative data layouts. 5.
Reference: [2] <author> Anderson, E., C. Bischof, J. Demmel, J. Dongarra, J. DuCroz, S. Hammarling, & W. Kahan, </author> <title> Prospectus for an extension to LAPACK: A portable linear algebra library for high-performance computers, </title> <institution> CS-90-118, U of Tennessee (1990). </institution>
Reference-contexts: 1. Introduction Matrix multiplication is a standard algorithm that is an important computational kernel in many applications including eigensolvers [3] and LU factorization [15]. Utilizing matrix multiplication is one of the principal ways of achieving high efficiency block algorithms in packages such as LAPACK <ref> [2] </ref>. The BLAS 3 routines were added to achieve this block performance on computers, and optimized versions are available on most serial machines [10]. <p> on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] <ref> [ 2 ] </ref> [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [3] <author> Auslander, L. & A. Tsao, </author> <title> A divide and conquer algorithm for the eigenproblem via complementary invariant subspace decomposition, </title> <type> Technical Report SRC-TR-89-003, </type> <institution> Supercomputing Research Center (1989). </institution>
Reference-contexts: 1. Introduction Matrix multiplication is a standard algorithm that is an important computational kernel in many applications including eigensolvers <ref> [3] </ref> and LU factorization [15]. Utilizing matrix multiplication is one of the principal ways of achieving high efficiency block algorithms in packages such as LAPACK [2]. The BLAS 3 routines were added to achieve this block performance on computers, and optimized versions are available on most serial machines [10]. <p> Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] <ref> [ 3 ] </ref> [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [4] <author> Bischof, C. H., S. Huss-Lederman, E. M. Jacobson, X. Sun, & A. Tsao, </author> <title> On the impact of HPF data layout on the design of efficient and maintainable parallel linear algebra libraries, </title> <type> (private communication). </type>
Reference-contexts: The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements <ref> [4; 8] </ref>; [5; 8]; : : : ; [4; 9]; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . <p> The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements [4; 8]; [5; 8]; : : : ; <ref> [4; 9] </ref>; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . . . . . . . . . <p> mesh case, new rows of B are now required so all the data for B on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] <ref> [ 4 ] </ref> [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . <p> BiMMeR utilizes the virtual 2-D torus wrap data layout shown in Figure 4 <ref> [4] </ref>. In this data layout the 4 fi 3 mesh is virtualized to a 12 fi 12 [LC M fi LC M ] mesh by placing 3 fi 4 virtual processors on each physical node. The virtual processors are separated by thin solid lines in Figure 4. <p> The example of the previous section and this discussion only touch on some of the properties of virtual 2-D torus wrap; a more detailed discussion can be found in <ref> [4] </ref>. It is clear that there are several potential data layouts for distributed memory algorithms. Each layout will have its own advantages and disadvantages, and more investigation is necessary to determine which alternatives have enough merit to be worthy of support.
Reference: [5] <author> Cannon, L.E., </author> <title> A Cellular Computer to Implement the Kalman Filter Algorithm, </title> <type> Ph.D. Thesis (1969), </type> <institution> Mon-tana State University. </institution>
Reference-contexts: Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic [15], 2D-systolic or Cannon's algorithm <ref> [5, 15] </ref>, Broadcast-Multiply-Roll [12, 13, 14] and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120. <p> The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements [4; 8]; <ref> [5; 8] </ref>; : : : ; [4; 9]; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . . . <p> so all the data for B on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] <ref> [ 5 ] </ref> [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [6] <author> Choi, J., J. J. Dongarra, R. Pozo, & D. W. Walker, </author> <title> ScaLAPACK: A scalable linear algebra library for distributed memory concurrent computers, </title> <booktitle> Proceedings, Fourth Symposium on the Frontiers of Massively Parallel Computation (McLean, </booktitle> <address> Virginia, </address> <month> October 19-21, </month> <title> 1992), </title> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> California, </address> <year> 1992, </year> <pages> pp. 120-127. </pages>
Reference-contexts: For matrix multiplication, the BLAS 3 routine XGEMM is available to compute C = fiC + ff op (A) op (B), where op (X) is either the matrix or its transpose. For analogous reasons, utilization of matrix multiplication-based algorithms on parallel computers also yields efficient implementations <ref> [6] </ref>. Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. <p> is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] <ref> [ 6 ] </ref> [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [7] <author> Choi, J., J. J. Dongarra, & D. W. Walker, </author> <title> Level 3 BLAS for distributed memory concurrent computers, </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing (Saint Hilaire du Tou-vet, </booktitle> <address> France, September 7-8, 1992), </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: Unfortunately, the routines implemented to date are often not general, and even the general routines have only achieved high performance by using machine-specific implementations. This paper will focus on two efforts, PUMMA <ref> [7, 8] </ref> and BiMMeR [17], both aimed at providing a library-quality implementation of distributed matrix multiplication that retains the functionality of XGEMM on serial computers. Since these implementations are for distributed memory computers, the data distribution (decomposition) is critical to the implementation. <p> matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] <ref> [ 7 ] </ref> [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [8] <author> Choi, J., J. J. Dongarra, & D. W. Walker, PUMMA: </author> <title> Parallel Universal Matrix Multiplication Algorithms on distributed memory concurrent computers, </title> <institution> ORNL/TM-12252, Oak Ridge National Laboratory (April 1993). </institution>
Reference-contexts: Unfortunately, the routines implemented to date are often not general, and even the general routines have only achieved high performance by using machine-specific implementations. This paper will focus on two efforts, PUMMA <ref> [7, 8] </ref> and BiMMeR [17], both aimed at providing a library-quality implementation of distributed matrix multiplication that retains the functionality of XGEMM on serial computers. Since these implementations are for distributed memory computers, the data distribution (decomposition) is critical to the implementation. <p> The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements <ref> [4; 8] </ref>; [5; 8]; : : : ; [4; 9]; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . <p> The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements [4; 8]; <ref> [5; 8] </ref>; : : : ; [4; 9]; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . . . <p> More details can be found in [9]. It can be seen that the indices of the blocks and the indices within each block align in the same manner as for the single row/column case <ref> [8] </ref>. Thus, the matrix multiplication algorithm remains the same for the block scattered data layout on square meshes; only the specific elements multiplied on each processor change. Since a scattered data layout is desirable for many algorithms, it is used in both the PUMMA and BiMMeR matrix multiplication routines. <p> rows of B are now required so all the data for B on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] <ref> [ 8 ] </ref> [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . <p> Repetition for all 4 [P ] stages completes the matrix multiplication. Details of the PUMMA algorithm and other cases are given in <ref> [8] </ref>. <p> Showing different nonsquare meshes would show other algorithmic choices made between PUMMA and BiMMeR. The detailed comparison between these two algorithms can be derived from the information contained in the papers on PUMMA <ref> [8] </ref> and BiMMeR [17] and is beyond the scope of this paper. 4.
Reference: [9] <author> Choi, J., J. J. Dongarra, & D. W. Walker, </author> <title> The Design of Scalable Software Libraries for Distributed Memory Concurrent Computers, </title> <booktitle> CNRS-NSF Workshop on Environments and Tools for Parallel Scientific Computing (Saint Hilaire du Touvet, </booktitle> <address> France, September 7-8, 1992), </address> <publisher> Elsevier Science Publishers, </publisher> <year> 1992. </year>
Reference-contexts: The numbers down (across) the rows (columns) represent the row (column) entry of the matrix. The values contained in each box represent the (x; y) processor location in the 2D mesh. For example, processor (1; 2) has matrix elements [4; 8]; [5; 8]; : : : ; <ref> [4; 9] </ref>; : : :; each processor starts with the same elements for matrices A, B, and C. (2,0) (2,1) (2,2) (0,0) (0,1) (0,2) 0 1 2 3 4 Roll B 5 6 7 8 9 10 11 . . . . . . . . . . . . . <p> The algorithm has so far been presented for a contiguous data layout that is sufficient for matrix multiplication alone. However, many other algorithms utilize a scattered data layout to get better load balancing properties by placing nearby rows/columns of the matrix on different processors <ref> [9] </ref>. Figure 2 illustrates the block scattered data layout for the same set of pa rameters as in Figure 1. <p> Note that the contiguous block case can still be achieved when b r = b c = N=P . More details can be found in <ref> [9] </ref>. It can be seen that the indices of the blocks and the indices within each block align in the same manner as for the single row/column case [8]. <p> data for B on each node is rolled upward. (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] <ref> [ 9 ] </ref> [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [10] <author> Dongarra, J. J., J. Du Croz, S. Hammarling, & I. Duff, </author> <title> A set of level 3 basic linear algebra subprograms, </title> <journal> ACM Transactions on Mathematical Software 16 (1990), </journal> <pages> 1-17. </pages>
Reference-contexts: Utilizing matrix multiplication is one of the principal ways of achieving high efficiency block algorithms in packages such as LAPACK [2]. The BLAS 3 routines were added to achieve this block performance on computers, and optimized versions are available on most serial machines <ref> [10] </ref>. For matrix multiplication, the BLAS 3 routine XGEMM is available to compute C = fiC + ff op (A) op (B), where op (X) is either the matrix or its transpose. For analogous reasons, utilization of matrix multiplication-based algorithms on parallel computers also yields efficient implementations [6]. <p> (Since P &gt; Q and the matrices are square, the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] <ref> [ 10 ] </ref> [ 3 ] [ 7 ] [ 11 ] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Reference: [11] <author> Falgout, R. D., A. Skjellum, S. G. Smith, & C. H. </author> <title> Still, The Multicomputer Toolbox approach to concurrent BLAS and LACS, </title> <booktitle> Proceedings, Scalable High Performance Computing Conference (Williamsburg, </booktitle> <address> VA, April 26-29, 1992), </address> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 121-28. </pages>
Reference-contexts: the number of columns of A is greater than the number (3,0) (3,1) (3,2) (1,0) (1,1) (1,2) [ 0 ] [ 4 ] [ 8 ] [ 1 ] [ 5 ] [ 9 ] [ 2 ] [ 6 ] [ 10 ] [ 3 ] [ 7 ] <ref> [ 11 ] </ref> . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . <p> The properties of the possible data lay outs lead to different tradeoffs and advantages when it comes to algorithm design. These two data layouts represent only some of the many possible permutations of the rows and columns that could be utilized <ref> [11] </ref>. Each of these possibilities has potential advantages and disadvantages associated with them. BiMMeR uses a data layout whose properties differ from block scattered in a number of important ways.
Reference: [12] <author> Fox, G., </author> <title> Domain decomposition in distributed and shared memory environments, </title> <booktitle> Lecture Notes in Com-put. Sci. 297: Proc., 1987 Intl. Conf. Supercomputing, </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1987, </year> <pages> pp. 1042-1073. </pages>
Reference-contexts: Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic [15], 2D-systolic or Cannon's algorithm [5, 15], Broadcast-Multiply-Roll <ref> [12, 13, 14] </ref> and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120.
Reference: [13] <author> Fox, G., M. Johnson, G. Lyzenga, S. Otto, J. Salmon, & D. Walker, </author> <title> Solving Problems on Concurrent Processors, Vol. I, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1988. </year>
Reference-contexts: Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic [15], 2D-systolic or Cannon's algorithm [5, 15], Broadcast-Multiply-Roll <ref> [12, 13, 14] </ref> and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120.
Reference: [14] <author> Fox, G., S. Otto, & A. Hey, </author> <title> Matrix algorithms on a hypercube I: matrix multiplication, </title> <booktitle> Parallel Computing 4 (1987), </booktitle> <pages> 17-31. </pages>
Reference-contexts: Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic [15], 2D-systolic or Cannon's algorithm [5, 15], Broadcast-Multiply-Roll <ref> [12, 13, 14] </ref> and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120. <p> Section 4 elaborates on the general data distribution question for libraries and application codes. Finally, Section 5 presents results and Section 6 gives a summary and conclusions. 2. Square Meshes Both PUMMA and BiMMeR use variants of the broadcast-multiply-roll algorithm <ref> [14] </ref>. This paper will present the algorithms on a 2D toroidal mesh, since this is the logical topology desired as noted in [14]. <p> Finally, Section 5 presents results and Section 6 gives a summary and conclusions. 2. Square Meshes Both PUMMA and BiMMeR use variants of the broadcast-multiply-roll algorithm <ref> [14] </ref>. This paper will present the algorithms on a 2D toroidal mesh, since this is the logical topology desired as noted in [14]. If the physical machine topology differs from the algorithmic topology, then one needs to embed the 2D toroidal mesh into the physical machine topology (for example, a hypercube). <p> This completes one stage of the algorithm. Each processor row now contains new columns of B. The algorithm continues with the broadcast of a different block of A followed by a matrix multiplication, and thus the algorithm continues. A more detailed presentation of broadcast-multiply-roll can be found in <ref> [14] </ref>. The broadcast-multiply-roll algorithm has several desirable properties, some of which are given below. * A local matrix multiplication is performed on each node.
Reference: [15] <author> Golub, G. & C. F. Van Loan, </author> <title> Matrix Computations, 2nd ed., </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, </address> <year> 1989. </year>
Reference-contexts: 1. Introduction Matrix multiplication is a standard algorithm that is an important computational kernel in many applications including eigensolvers [3] and LU factorization <ref> [15] </ref>. Utilizing matrix multiplication is one of the principal ways of achieving high efficiency block algorithms in packages such as LAPACK [2]. The BLAS 3 routines were added to achieve this block performance on computers, and optimized versions are available on most serial machines [10]. <p> For analogous reasons, utilization of matrix multiplication-based algorithms on parallel computers also yields efficient implementations [6]. Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic <ref> [15] </ref>, 2D-systolic or Cannon's algorithm [5, 15], Broadcast-Multiply-Roll [12, 13, 14] and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120. <p> Thus, there is a desire to have a widely available library-quality implementation of XGEMM on parallel machines. Several algorithms have been proposed for parallel, distributed memory matrix multiplication including 1D-systolic [15], 2D-systolic or Cannon's algorithm <ref> [5, 15] </ref>, Broadcast-Multiply-Roll [12, 13, 14] and fl This paper is PRISM Working Note #13, available via anonymous ftp to ftp.super.org in the directory pub/prism. This work was partially supported by the Applied and Computational Mathematics Program, Advanced Research Projects Agency, under Contract DM28E04120.
Reference: [16] <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Optimizing Communication Primitives on the Intel Touchstone Delta, </title> <type> Technical Report, </type> <note> Supercomputing Research Center (1993) (to appear). </note>
Reference-contexts: Thus, in communicating A, BiMMeR transmits more data in its broadcast than PUMMA does in its rolls. In addition, the broadcasting algorithm used on the Intel Delta takes dlog 2 (Q)e stages, whereas the roll can be done in 2 stages <ref> [16] </ref>. (On some other systems, it is possible to do both operations in a single stage of comparable time.) Except for very small meshes, dlog 2 (Q)e is larger than 2.
Reference: [17] <author> Huss-Lederman, S., E. M. Jacobson, A. Tsao, & G. Zhang, </author> <title> Matrix Multiplication on the Intel Touchstone Delta, </title> <booktitle> Proceedings, Sixth SIAM Conference on Parallel Processing for Scientific Computing (Nor-folk, </booktitle> <address> Virginia, </address> <month> March 22-24, </month> <editor> 1993) (R. F. Sincovec, eds.), </editor> <publisher> SIAM, </publisher> <address> Philadelphia, </address> <year> 1993, </year> <note> (also PRISM Working Note #7). </note>
Reference-contexts: Unfortunately, the routines implemented to date are often not general, and even the general routines have only achieved high performance by using machine-specific implementations. This paper will focus on two efforts, PUMMA [7, 8] and BiMMeR <ref> [17] </ref>, both aimed at providing a library-quality implementation of distributed matrix multiplication that retains the functionality of XGEMM on serial computers. Since these implementations are for distributed memory computers, the data distribution (decomposition) is critical to the implementation. <p> Each node can now do a matrix multiplication of the broadcast A with B. Finally, B is rolled up. This completes one stage of the BiMMeR algorithm. Details of the BiMMeR algorithm can be found in <ref> [17] </ref>. A summary of the algorithmic changes made in BiMMeR to deal with this nonsquare mesh is as follows. * The columns of A to be broadcast can lie in at most two processors, sometimes necessitating two smaller broadcasts. <p> Showing different nonsquare meshes would show other algorithmic choices made between PUMMA and BiMMeR. The detailed comparison between these two algorithms can be derived from the information contained in the papers on PUMMA [8] and BiMMeR <ref> [17] </ref> and is beyond the scope of this paper. 4.
Reference: [18] <author> Lin, C. & L. Snyder, </author> <title> A Matrix Product Algorithm and its Comparative Performance on Hypercubes, </title> <booktitle> Proceedings, Scalable High Performance Computing Conference (Williamsburg, </booktitle> <address> VA, </address> <month> April 26-29, </month> <note> 1992) (Stout, </note> <editor> Q. & M. Wolfe, eds.), </editor> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1992, </year> <pages> pp. 190-3. </pages>
Reference-contexts: Access to the Intel Touchstone Delta System operated by Caltech on behalf of the Concurrent Supercomputing Consortium was provided by NSF. Typeset by A M S-T E X the Transpose algorithm <ref> [18] </ref>. These algorithms have been implemented on a variety of machines. However, a single, standard, library-quality parallel version of XGEMM is not yet available. To satisfy the diverse needs of users, a library version must be able to deal with arbitrary matrix sizes and numbers of processors.
References-found: 18

