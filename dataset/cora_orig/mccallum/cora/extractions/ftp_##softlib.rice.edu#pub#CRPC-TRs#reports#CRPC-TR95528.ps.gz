URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95528.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Title: Loop Transformations to Prevent False Sharing  
Author: Elana D. Granston Thierry Montaut Fran~cois Bodin 
Keyword: Key words: false sharing, page-level affinity scheduling, loop transformations, shared virtual mem ory.  
Note: To appear in the International Journal of Parallel Programming  
Address: 6100 South Main Street 35042 Rennes, Cedex 35042 Rennes, Cedex Houston, Texas 77005, USA France France  
Affiliation: Rice University IRISA IRISA Center for Research on Parallel Computation Campus de Beaulieu Campus de Beaulieu  
Email: granston@cs.rice.edu montaut@irisa.fr bodin@irisa.fr  
Date: May 16, 1995  
Abstract: To date, page management in shared virtual memory (SVM) systems has been primarily the responsibility of the run-time system. However, there are some problems that are difficult to resolve efficiently at run time. Chief among these is false sharing. In this paper, a loop transformation theory is developed for identifying and eliminating potential sources of multiple-writer false sharing and other sources of page migration resulting from regular references in numerical applications. Loop nests of one and two dimensions (before blocking) with single-level, DOALL-style parallelism are covered. The potential of these transformations is demonstrated experimentally. 
Abstract-found: 1
Intro-found: 1
Reference: [AALT94] <author> Saman P. Ammarsinghe, Jennifer M. Anderson, Monica S. Lam, and Chau-Wen Tseng. </author> <title> Design and Evaluation of Compiler Optimizations for Scalable Address Space Machines, </title> <note> 1994. To be published. </note>
Reference-contexts: Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. <p> However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing. False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed <ref> [AALT94, AHD93, MHS94, OKB, BGM95] </ref>. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF [KLS + 94] typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion. <p> Interestingly, our decision to leave the data layout intact also gives us this same advantage over most other research on compiling (either data-parallel or sequential language programs) for global address space architectures <ref> [AL93, AALT94, MHS94] </ref>. Leaving the data layout intact has the additional advantage that data is mapped directly to memory, which is linear. Thus, we only have to deal with straightforward, one-dimensional distributions.
Reference: [ACIK93] <author> Corinne Ancourt, Fabien Coelho, Fran~cois Irigoin, and Ronan Keryell. </author> <title> A Linear Algebra Framework for Static HPF Code Distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: The problem of partitioning computation according to a general block-cyclic owner computes rule has been studied by several groups of researchers <ref> [CGL + 93, ACIK93, KNS94, AFMP95] </ref>. The solution that we propose transforms the problem of eliminating false sharing into a similar problem: we assign pages to processors in a block-cyclic fashion and then partition computation accordingly.
Reference: [AFMP95] <author> A. Andre, M. Le Fur, Y. Maheo, and J.-L. Pazat. </author> <title> The Pandore Data Parallel Compiler and its Portable Runtime. </title> <booktitle> In HPCN Europe'95, </booktitle> <address> Milan, Italy, </address> <month> May </month> <year> 1995. </year> <note> To appear in LNCS, Springer Verlag. </note>
Reference-contexts: The problem of partitioning computation according to a general block-cyclic owner computes rule has been studied by several groups of researchers <ref> [CGL + 93, ACIK93, KNS94, AFMP95] </ref>. The solution that we propose transforms the problem of eliminating false sharing into a similar problem: we assign pages to processors in a block-cyclic fashion and then partition computation accordingly.
Reference: [AHD93] <author> Bill Appelbe, Charles Hardnett, and Sri Doddapaneni. </author> <title> Program Transformation for Locality Using Affinity Regions. </title> <booktitle> In the Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Portland, Oregon, </address> <month> August </month> <year> 1993. </year> <title> Published in Languages and Compilers for Parallel Computing, </title> <editor> Banerjee et al. (Eds.), </editor> <publisher> LNCS 768, Springer-Verlag, </publisher> <year> 1994, </year> <pages> pages 290-300. </pages>
Reference-contexts: These include transformations such as loop interchanging that increase locality within an individual loop nest [BEJW92, KM92, WL91] as well as optimizations that increase locality across loop nests, for example <ref> [HA90, AHD93] </ref>. Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only [BGM94, Mon95]. <p> However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing. False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed <ref> [AALT94, AHD93, MHS94, OKB, BGM95] </ref>. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF [KLS + 94] typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion.
Reference: [AL93] <author> Jennifer Anderson and Monica Lam. </author> <title> Global Optimizations for Parallelism and Locality on Scalable Parallel Machines. </title> <booktitle> In Proceedings of the SIGPLAN '93 Conference on Programming Languages Design and Implementation. </booktitle> <publisher> ACM Press, </publisher> <month> June </month> <year> 1993. </year>
Reference-contexts: Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. <p> Interestingly, our decision to leave the data layout intact also gives us this same advantage over most other research on compiling (either data-parallel or sequential language programs) for global address space architectures <ref> [AL93, AALT94, MHS94] </ref>. Leaving the data layout intact has the additional advantage that data is mapped directly to memory, which is linear. Thus, we only have to deal with straightforward, one-dimensional distributions.
Reference: [BEJW92] <author> Fran~cois Bodin, Christine Eisenbeis, William Jalby, and Daniel Windheiser. </author> <title> A Quantitative Algorithm for Data Locality Optimization. In Code Generation-Concepts, Tools, Techniques. </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. These include transformations such as loop interchanging that increase locality within an individual loop nest <ref> [BEJW92, KM92, WL91] </ref> as well as optimizations that increase locality across loop nests, for example [HA90, AHD93]. Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only [BGM94, Mon95].
Reference: [BFS89] <author> William J. Bolosky, Robert P. Fitzgerald, and Michael L. Scott. </author> <title> Simple But Effective Techniques for NUMA Memory Management. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 19-31. </pages> <publisher> ACM Press, </publisher> <month> December </month> <year> 1989. </year>
Reference-contexts: Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem.
Reference: [BGM94] <author> Fran~cois Bodin, Elana D. Granston, and Thierry Montaut. </author> <title> Evaluating Two Loop Transformations for Reducing Multiple-Writer False Sharing. </title> <booktitle> In the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Ithaca, New York, </address> <month> August </month> <year> 1994. </year> <note> Published as LNCS 892, pages 423-439, </note> <editor> Pingali et al. (Eds.), </editor> <booktitle> 1995. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, Heidelberg. </address>
Reference-contexts: When the third step is not legal, another alternative exists: we can prevent page-level sharing completely within some reference group. For the other reference groups, we can reduce the number of page migrations that page-level sharing causes by combining this approach with our ping-pong reduction transformation 22 described in <ref> [BGM94] </ref>. This hybrid strategy has been shown to work well in practice [BGM94]. 8 Experimental Results FS-OPT has been implemented in the Fortran-S compiler [BKP93], which generates code that runs on the iPSC/2 under the KOAN SVM [LP92]. <p> For the other reference groups, we can reduce the number of page migrations that page-level sharing causes by combining this approach with our ping-pong reduction transformation 22 described in <ref> [BGM94] </ref>. This hybrid strategy has been shown to work well in practice [BGM94]. 8 Experimental Results FS-OPT has been implemented in the Fortran-S compiler [BKP93], which generates code that runs on the iPSC/2 under the KOAN SVM [LP92]. The KOAN SVM system is embedded in the operating system of the iPSC/2. <p> Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only <ref> [BGM94, Mon95] </ref>. This approach alleviates ping-pong effects by batching up write requests which encourages processors to perform multiple writes to a page before relinquishing the page.
Reference: [BGM95] <author> Fran~cois Bodin, Elana D. Granston, and Thierry Montaut. </author> <title> Page-level Affinity Scheduling for Eliminating False Sharing. </title> <booktitle> In Fifth Workshop on Compilers for Parallel Computers, Malaga, </booktitle> <address> Spain, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: These two steps alone are sufficient for preventing false sharing. However, unless the third step can also be applied, we are trading false sharing for "true" sharing, namely the introduction of additional synchronization. The third step is only legal when there are no fusion-preventing flow or anti dependences <ref> [BGM95] </ref>. For this work, we decided only to prevent page-level sharing when we could do so without introducing additional synchronization overhead. When the third step is not legal, another alternative exists: we can prevent page-level sharing completely within some reference group. <p> However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing. False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed <ref> [AALT94, AHD93, MHS94, OKB, BGM95] </ref>. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF [KLS + 94] typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion. <p> Many commonly occurring cases of loops containing multiple references can also be handled. We are currently working on techniques to generalize these optimizations even further. Our transformation has several beneficial side effects. First, applying our transformation may expose opportunities to eliminate barrier synchronization between loops <ref> [BGM95] </ref>. Second, the transformation generally increases locality and reduces the working set of pages, thereby reducing thrashing as well.
Reference: [BKP93] <author> F. Bodin, L. Kervella, and T. Priol. Fortran-S: </author> <title> A Fortran Interface for Shared Virtual Memory Architectures. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 274-283. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: This hybrid strategy has been shown to work well in practice [BGM94]. 8 Experimental Results FS-OPT has been implemented in the Fortran-S compiler <ref> [BKP93] </ref>, which generates code that runs on the iPSC/2 under the KOAN SVM [LP92]. The KOAN SVM system is embedded in the operating system of the iPSC/2. Pages of size 4 KB are physically distributed across processors' local memories.
Reference: [BLSS93] <author> Mauricio Breternitz, Jr., Michael Lai, Vivek Sarkar, and Barbara Simons. </author> <title> Compiler Solutions for the Stale-Data and False-Sharing Problems. </title> <type> Technical Report 03.466, </type> <institution> IBM Santa Teresa Laboratory, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Therefore, we did not consider any transformations that would require the insertion of additional synchronization. In contrast, on systems where no hardware or run-time support for coherence is provided, false sharing must be prevented to ensure correctness. Breternitz et al. <ref> [BLSS93] </ref> study this problem. They insert additional 28 synchronization as needed so that the compiler can maintain coherence. Consequently, their techniques are more general than ours. However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing.
Reference: [CF78] <author> L.M. Censier and P. Feautrier. </author> <title> A New Solution to Coherence Problems in Multicache Systems. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1112-1118, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: The KOAN SVM system is embedded in the operating system of the iPSC/2. Pages of size 4 KB are physically distributed across processors' local memories. KOAN uses a distributed-manager algorithm based on [Li86], with an invalidation-based protocol that ensures that the shared memory is coherent at all times <ref> [CF78] </ref>. Under this protocol, pages can have one of three access modes: read-only , write-exclusive and invalid. Multiple copies of a page are permitted only when all copies are in read-only mode.
Reference: [CGL + 93] <author> Siddhartha Chatterjee, John R. Gilbert, Fred J. E. Long, Robert Schreiber, and Shun-Hua Teng. </author> <title> Generating Local Address Communication Sets for Data-Parallel Programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice Of Parallel Programming, </booktitle> <pages> pages 149-158, </pages> <address> San Diego, California, </address> <year> 1993. </year>
Reference-contexts: The problem of partitioning computation according to a general block-cyclic owner computes rule has been studied by several groups of researchers <ref> [CGL + 93, ACIK93, KNS94, AFMP95] </ref>. The solution that we propose transforms the problem of eliminating false sharing into a similar problem: we assign pages to processors in a block-cyclic fashion and then partition computation accordingly.
Reference: [DBMS79] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart. </author> <title> LINPACK User's Guide, </title> <booktitle> 1979. </booktitle> <pages> 31 </pages>
Reference-contexts: For both versions, the innermost DOALL loop was parallelized. To maximize the grain of parallelism, loop interchanging was then applied when legal. 8.1 DMXPY Loop Nest 17 depicts the Fortran kernel DMXPY from LINPACKD <ref> [DBMS79] </ref> which performs matrix-vector multiplication.
Reference: [EJ91] <author> Susan J. Eggers and Tor E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 377-381. </pages> <publisher> CRC Press, Inc., </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem.
Reference: [GW92] <author> Elana D. Granston and Harry A. G. Wishoff. </author> <title> Managing Pages in Shared Virtual Memory Systems: Getting the Compiler into the Game. </title> <type> Technical Report 92-19, </type> <institution> Computer Science Department, Leiden University, </institution> <month> December </month> <year> 1992. </year> <month> Revised July </month> <year> 1993. </year>
Reference-contexts: The proof follows directly from Lemmas 7-9. Note that the scheduling of DOALL loop iterations in Loop Nest 10 is static. Alternatively, a consistent surjective mapping could be effected while allowing DOALL loop iterations to be dynamically sched uled <ref> [GW92] </ref>. 5 Parallelizing the Outer Loop Suppose instead that we parallelize the outer loop.
Reference: [HA90] <author> David E. Hudak and Santosh G. Abraham. </author> <title> Compiler Techniques for Data Partitioning of Sequentially Iterated Loops. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 187-200. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1990. </year>
Reference-contexts: These include transformations such as loop interchanging that increase locality within an individual loop nest [BEJW92, KM92, WL91] as well as optimizations that increase locality across loop nests, for example <ref> [HA90, AHD93] </ref>. Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only [BGM94, Mon95].
Reference: [HHKT92] <author> Mary W. Hall, Seema Hiranandani, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Interprocedural Compilation of Fortran D for MIMD Distributed Memory Machines. </title> <booktitle> In Supercomputing '92, </booktitle> <pages> pages 522-524. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1992. </year>
Reference-contexts: Thus, we only have to deal with straightforward, one-dimensional distributions. Com 29 pilers for data-parallel languages such as HPF must support multi-dimensional distributions, as well as complicated alignment specifications and processor mappings. This leads to the "reaching distribution" problem <ref> [HHKT92] </ref>. In general, a compiler must know what distribution an array might have. This is particularly problematic at procedure call boundaries or when redistribution statements are present, both of which can lead to situations where variables are associated with different distributions at different points of execution.
Reference: [KDCZ94] <author> P. Keleher, S. Dwarkadas, A. Cox, and W. Zwaenepoel. Treadmarks: </author> <title> Distributed Shared Memory On Standard Workstations and and Operating Systems. </title> <booktitle> In Winter Usenix Conference, </booktitle> <year> 1994. </year>
Reference-contexts: Run-time solutions for preventing ping-pong effects have also been proposed. One approach is to relax the consistency model. For example, systems such as Treadmarks <ref> [KDCZ94] </ref> (by default) and KOAN [LP92] (as an option) allow multiple copies of writable pages to exist and merge modifications only at synchronization points.
Reference: [KLS + 94] <author> Charles H. Koelbel, David B. Loveman, Robert S. Schreiber, Guy L. Steele Jr., and Mary E. Zosel. </author> <title> The High Performance Fortran handbook. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed [AALT94, AHD93, MHS94, OKB, BGM95]. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF <ref> [KLS + 94] </ref> typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion. The processor where data is physically located is the owner of that data.
Reference: [KM92] <author> Ken Kennedy and Katheryn S. McKinley. </author> <title> Optimizing for Parallelism and Data Locality. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <pages> pages 323-334. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1992. </year>
Reference-contexts: In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. These include transformations such as loop interchanging that increase locality within an individual loop nest <ref> [BEJW92, KM92, WL91] </ref> as well as optimizations that increase locality across loop nests, for example [HA90, AHD93]. Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only [BGM94, Mon95].
Reference: [KNS94] <author> Ken Kennedy, Nenad Nedeljkovic, and Ajay Sethi. </author> <title> Efficient Address Generation for Block-Cyclic Distributions. </title> <type> Technical report, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <type> Technical Report No. </type> <institution> CRPC-TR94487-S, Houston, Texas, </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: The problem of partitioning computation according to a general block-cyclic owner computes rule has been studied by several groups of researchers <ref> [CGL + 93, ACIK93, KNS94, AFMP95] </ref>. The solution that we propose transforms the problem of eliminating false sharing into a similar problem: we assign pages to processors in a block-cyclic fashion and then partition computation accordingly.
Reference: [Li86] <author> Kai Li. </author> <title> Shared Virtual Memory on Loosely Coupled Multiprocessors. </title> <type> PhD thesis, </type> <institution> Yale University, </institution> <month> September </month> <year> 1986. </year>
Reference-contexts: Writing correct compilers, let alone good compilers, is not easy either. Typically, compiler technology lags years behind architectural innovations. To simplify the programming of and compiling for such systems, much research effort has been directed toward the implementation of shared virtual memory (SVM) interfaces <ref> [Li86] </ref> which provide the programmer with the illusion of a global address space. In such a system, the unit of data to which coherency is applied fl Supported by a Postdoctoral Research Associateship in Computational Science and Engineering under National Science Foundation Grant No. <p> The KOAN SVM system is embedded in the operating system of the iPSC/2. Pages of size 4 KB are physically distributed across processors' local memories. KOAN uses a distributed-manager algorithm based on <ref> [Li86] </ref>, with an invalidation-based protocol that ensures that the shared memory is coherent at all times [CF78]. Under this protocol, pages can have one of three access modes: read-only , write-exclusive and invalid. Multiple copies of a page are permitted only when all copies are in read-only mode.
Reference: [LP92] <author> Z. Lajormi and T. Priol. KOAN: </author> <title> A Shared-Memory for the iPSC/2 Hypercube. In CON-PAR/VAPP92, </title> <publisher> LNCS 634. Springer-Verlag, </publisher> <month> September </month> <year> 1992. </year>
Reference-contexts: Both versions were executed on 16 processors of a 32-processor iPSC/2 under the KOAN SVM system <ref> [LP92] </ref>, 2 which supports the aforementioned invalidation-based coherence protocol and employs a page size of 4 KB (512 double-precision numbers). The problem size was N 1 = N 2 = 10 3 . <p> This hybrid strategy has been shown to work well in practice [BGM94]. 8 Experimental Results FS-OPT has been implemented in the Fortran-S compiler [BKP93], which generates code that runs on the iPSC/2 under the KOAN SVM <ref> [LP92] </ref>. The KOAN SVM system is embedded in the operating system of the iPSC/2. Pages of size 4 KB are physically distributed across processors' local memories. KOAN uses a distributed-manager algorithm based on [Li86], with an invalidation-based protocol that ensures that the shared memory is coherent at all times [CF78]. <p> Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. <p> Run-time solutions for preventing ping-pong effects have also been proposed. One approach is to relax the consistency model. For example, systems such as Treadmarks [KDCZ94] (by default) and KOAN <ref> [LP92] </ref> (as an option) allow multiple copies of writable pages to exist and merge modifications only at synchronization points.
Reference: [MHS94] <author> Ravi Michandaney, Seema Hiranandani, and Ajay Sethi. </author> <title> Improving the Performance of DSM Systems via Compiler Involvement. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year>
Reference-contexts: However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing. False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed <ref> [AALT94, AHD93, MHS94, OKB, BGM95] </ref>. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF [KLS + 94] typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion. <p> Interestingly, our decision to leave the data layout intact also gives us this same advantage over most other research on compiling (either data-parallel or sequential language programs) for global address space architectures <ref> [AL93, AALT94, MHS94] </ref>. Leaving the data layout intact has the additional advantage that data is mapped directly to memory, which is linear. Thus, we only have to deal with straightforward, one-dimensional distributions.
Reference: [Mon95] <institution> Thierry Montaut. Methodes pour l'elimination du faux-partage et l'optimisation de la localite pour memoire virtuelle partagee. </institution> <type> PhD thesis, </type> <institution> IRISA, Campus de Beaulieu, </institution> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: In this case, in the steady state, each block is composed of 4096 iterations, which may be prohibitively large. 19 If the block size, t iterations, is prohibitively large, array padding can be used to reduce t. This option is addressed in <ref> [Mon95] </ref>. In the special case where c 2 = 0, R is invariant with respect to the inner loop. <p> Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. <p> Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only <ref> [BGM94, Mon95] </ref>. This approach alleviates ping-pong effects by batching up write requests which encourages processors to perform multiple writes to a page before relinquishing the page.
Reference: [OKB] <author> M.F.P. O'Boyle, L. Kervella, and F. Bodin. </author> <title> Synchronization Minimization in a SPMD Execution Model. </title> <note> To appear in the Journal of Parallel and Distributed Computing. </note>
Reference-contexts: However, on page-coherent systems, the overhead for the additional synchronization may outweigh the performance gain from preventing false sharing. False sharing prevention is only one optimization that a good SVM compiler should perform. Techniques to reduce synchronization and hide memory access latencies are also needed <ref> [AALT94, AHD93, MHS94, OKB, BGM95] </ref>. 9.2 Relation to Research On Compiling Data Parallel Languages Data parallel languages like HPF [KLS + 94] typically support a block-cyclic data distribution for regularly distributed data. When the programmer that selects a block-cyclic distribution, data is physically distributed across processors in a block-cyclic fashion.
Reference: [THK93] <author> Chau-Wen Tseng, Seema Hiranandani, and Ken Kennedy. </author> <title> Preliminary Experiences with the Fortran D Compiler. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 338-350. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus, we avoid the complexities associated with implementing global to local address translation at the compiler level which can significantly complicate the design of data parallel compilers <ref> [THK93] </ref>. Interestingly, our decision to leave the data layout intact also gives us this same advantage over most other research on compiling (either data-parallel or sequential language programs) for global address space architectures [AL93, AALT94, MHS94].
Reference: [TLH92] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches, </title> <month> August </month> <year> 1992. </year> <note> Submitted to IEEE Transactions on Computers. </note>
Reference-contexts: Based on this research, data layout optimizations <ref> [BFS89, EJ91, LP92, TLH92, AL93, AALT94, Mon95] </ref> have been proposed. In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem.
Reference: [WL91] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the SIGPLAN '91 Conference on Programming Languages Design and Implementation, </booktitle> <pages> pages 30-44. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1991. </year> <month> 32 </month>
Reference-contexts: In many cases, when coherency units are small, compiler-directed program transformations that increase temporal and spatial locality without directly considering the size of the coherency unit alleviate much of the problem. These include transformations such as loop interchanging that increase locality within an individual loop nest <ref> [BEJW92, KM92, WL91] </ref> as well as optimizations that increase locality across loop nests, for example [HA90, AHD93]. Unfortunately, when the coherency unit becomes larger, such techniques no longer suffice. An alternate compile-time approach that we explored attacks ping-pong effects only [BGM94, Mon95].
References-found: 30

