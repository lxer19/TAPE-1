URL: http://www.cs.utexas.edu/users/dmcl/projects/trellis/papers/ps/ICDE96.ps
Refering-URL: http://www.cs.utexas.edu/users/dmcl/projects/trellis/papers.html
Root-URL: http://www.cs.utexas.edu
Title: High Availability in Clustered Multimedia Servers  
Author: Renu Tewari Daniel M. Dias Rajat Mukherjee Harrick M. Vin T. J. 
Address: NY 10532 Austin, TX 78712  
Affiliation: IBM Research Division Department of Computer Science  Watson Research Center University of Texas at Austin Hawthorne,  
Abstract: Clustered multimedia servers, consisting of interconnected nodes and disks, have been proposed for large scale servers, that are capable of supporting multiple concurrent streams which access the video objects stored in the server. As the number of disks and nodes in the cluster increases, so does the probability of a failure. With data striped across all disks in a cluster, the failure of a single disk or node results in the disruption of many or all streams in the system. Guaranteeing high availability in such a cluster becomes a primary requirement to ensure continuous service. In this paper, we study mirroring and software RAID schemes with different placement strategies, that guarantee high availability in the event of disk and node failures, while satisfying the real-time requirements of the streams. We examine various declustering techniques for spreading the redundant information across disks and nodes and show that random declustering has good real-time performance. Finally, we compare the overall cost per stream for different system configurations. We derive the parameter space where mirroring and software RAID apply, and determine optimal parity group sizes. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C.R. Attanasio, M. Butrico, C.A. Polyzois, S.E. Smith, and J.L. Peterson. </author> <title> Design and implementation of a recoverable virtual shared disk. </title> <institution> IBM Research Report RC-19843, </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: The block is stored in a stream buffer (includes read-ahead buffers and a play-out buffer), at the front-end (Figure 1). We assume an underlying software layer (e.g., virtual shared disk) makes the interconnection architecture transparent to the nodes <ref> [1] </ref>. The blocks from the playout buffer are transmitted over the external network at regular intervals, based on the playout rate of the stream. The blocks required later for playout are pre-fetched and stored in the read-ahead buffers at the front-end node.

Reference: [3] <author> Steven Berson, Leana Golubchik, and Richard R. Muntz. </author> <title> Fault tolerant design of multimedia servers. </title> <booktitle> SIGMOD, </booktitle> <year> 1995. </year>
Reference-contexts: The focus of this paper is the real-time performance and price-performance tradeoffs between methods for providing high availability in clustered multimedia servers. 1.1 Related Work All the previous work on high availability in multimedia servers that we are aware of, examine single node servers and specifically focus on RAID <ref> [2, 3, 20, 21] </ref> schemes across disks within a node. The streaming RAID proposed by Tobagi et al. [20], divides the disks into parity groups 1 . In their scheme, entire parity groups are read out in one cycle and sent on the network during the next cycle. <p> The streaming RAID proposed by Tobagi et al. [20], divides the disks into parity groups 1 . In their scheme, entire parity groups are read out in one cycle and sent on the network during the next cycle. Berson et. al. <ref> [3] </ref>, generalize the streaming RAID approach. Their staggered stripe scheme, reads a parity group in a single cycle, but plays it out over n cycles, roughly halving the memory requirements. Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. <p> With node failure (Figure 8 (b)), the operational disk utilization can be increased from about 65% to 70% by increasing the read-ahead buffering. Alternatively <ref> [3, 21] </ref>, when a request is made for a block on the failed disk, the parity group for the next block on the failed disk could be pre-fetched and stored in the parity-group buffer.
Reference: [4] <author> John Chandy and Narasimha A.L. Reddy. </author> <title> Failure evaluation of disk array organizations. </title> <booktitle> Proceedings of ICDCS, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes <ref> [4, 7, 17, 13, 14] </ref>, but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin [11].
Reference: [5] <author> Ram Chillarege, S. Biyani and J. Rosenthal. </author> <title> Measurement of Failure Rate in Commerical Software. </title> <institution> IBM Research Report RC-19889, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: A node failure makes all the disks attached to the failed node become inaccessible, leading to a larger outage. Typical MTTF (mean time to failure) values for a disk is 300,000 hours, while for a node (including software failures) is around 18,000 hours <ref> [5] </ref>. Handling node failures becomes increasingly important as they not only occur more frequently but also cause more disruption to service. Most of the techniques that guarantee continuous service store redundant data, which can be used during failure to compute the lost or inaccessible information on the failed resource.
Reference: [6] <author> E. Chang and A. Zakhor. </author> <title> Scalable video data placement on parallel disk arrays. Proceedings of storage and retrieval for image and video databases II, </title> <month> February </month> <year> 1994. </year>
Reference-contexts: Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. In a previous paper [19], we studied the real-time issues in a clustered multimedia server. There are numerous other papers on disk layout for multimedia applications <ref> [6, 8, 15, 22] </ref>, but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes [4, 7, 17, 13, 14], but these do not focus on multimedia applications.
Reference: [7] <author> Peter M. Chen, Edward K. Lee, Garth A. Gib son, Randy H. Katz, and David A. Patterson. </author> <title> RAID: High performance, reliable secondary storage. </title> <journal> ACM Computing Surverys, </journal> <year> 1994. </year>
Reference-contexts: There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes <ref> [4, 7, 17, 13, 14] </ref>, but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin [11].
Reference: [8] <author> Craig Freedman and David DeWitt. </author> <title> The SPIFFI scalable video-on-demand server. </title> <booktitle> SIGMOD, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Clustered multimedia servers consisting of a set of processing nodes, each with a local disk array, connected by a high bandwidth switch or network, have been proposed <ref> [8, 12, 19] </ref> to provide a scalable solution that meets the real-time requirements of multimedia streams. The basic requirement for any continuous media server or a multimedia server in general, is the delivery of isochronous streams. <p> Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. In a previous paper [19], we studied the real-time issues in a clustered multimedia server. There are numerous other papers on disk layout for multimedia applications <ref> [6, 8, 15, 22] </ref>, but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes [4, 7, 17, 13, 14], but these do not focus on multimedia applications.
Reference: [9] <author> A.H. Frey and R.C. Mosteller. </author> <title> File-based redundant parity protection in a parallel computing sys tem. </title> <type> U.S. Patent, (5130992), </type> <year> 1990. </year>
Reference-contexts: Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes [4, 7, 17, 13, 14], but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey <ref> [9] </ref> and the work of Haskin [11]. Frey et. al. describe a round-robin layout across nodes and disks, but they do not consider real-time applications.
Reference: [10] <author> Donald Gross and Carl M Harris. </author> <title> Fundamentals of Queueing Theory. </title> <publisher> John Wiley and Sons, </publisher> <year> 1985. </year>
Reference-contexts: This assumption can be made because the variation in disk service time due to seek delays is relatively small compared to the total service time, when the block size is large. Each disk thus behaves like an M/D/1 queue. From the results for an M/D/1 queue <ref> [10] </ref> we know the queue length distribution, p n , which is the probability that there are n requests at a disk, from which the delay distribution, W d (t) (probability that a request has a disk delay equal to t) can be derived. <p> The maximum delay T max , that a block can encounter at the disk and still meets its deadline is, (k=) ffi net ffi p . The loss probability is then computed from the tail of the queue length distribution, p i , of the disk queue <ref> [10] </ref>, using Equation 1.
Reference: [11] <author> Roger Haskin. </author> <type> Personal Communication, </type> <year> 1994. </year>
Reference-contexts: Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin <ref> [11] </ref>. Frey et. al. describe a round-robin layout across nodes and disks, but they do not consider real-time applications.
Reference: [12] <author> Roger Haskin and Frank L. Stein. </author> <title> A system for delivery of interactive television programming. </title> <booktitle> COMPCON, </booktitle> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Clustered multimedia servers consisting of a set of processing nodes, each with a local disk array, connected by a high bandwidth switch or network, have been proposed <ref> [8, 12, 19] </ref> to provide a scalable solution that meets the real-time requirements of multimedia streams. The basic requirement for any continuous media server or a multimedia server in general, is the delivery of isochronous streams.
Reference: [13] <author> Mark Holland and Garth A. Gibson. </author> <title> Parity declustering for continuous operation in redundant disk arrays. </title> <booktitle> ASPLOS, </booktitle> <year> 1992. </year>
Reference-contexts: There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes <ref> [4, 7, 17, 13, 14] </ref>, but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin [11]. <p> The block numbering notation a.b, for a video object refers to the b th block within the a th parity group of the video object. In RAID based schemes, if a disk is used to store only parity information its bandwidth is wasted during normal operation <ref> [13] </ref>. To utilize the bandwidth of all the disks in the system under normal operation, the data and parity blocks associated with a video object, need to be distributed across all the disks in the system.
Reference: [14] <author> Mark Holland, Garth A. Gibson, and Daniel P. Siewiorek. </author> <title> Fast, on -line failure recovery in redundant disk arrays. </title> <address> FTCS, </address> <year> 1993. </year>
Reference-contexts: There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes <ref> [4, 7, 17, 13, 14] </ref>, but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin [11].
Reference: [15] <author> J. Hsieh and et.al. </author> <title> Performance of a mass storage system for video-on-demand. </title> <journal> Journal of parallel and distributed computing, </journal> <note> submitted. </note>
Reference-contexts: Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. In a previous paper [19], we studied the real-time issues in a clustered multimedia server. There are numerous other papers on disk layout for multimedia applications <ref> [6, 8, 15, 22] </ref>, but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes [4, 7, 17, 13, 14], but these do not focus on multimedia applications.
Reference: [16] <author> C.L. Liu and J.W. Layland. </author> <title> Scheduling Algo rithms for Multiprogramming in a Hard Real-Time Environment. </title> <journal> Journal of the ACM, </journal> <volume> 20(1):46 - 61, </volume> <month> January </month> <year> 1973. </year>
Reference-contexts: The blocks from the playout buffer are transmitted over the external network at regular intervals, based on the playout rate of the stream. The blocks required later for playout are pre-fetched and stored in the read-ahead buffers at the front-end node. We use EDF (Earliest Deadline First <ref> [16, 19] </ref>) scheduling policy at the disk. The deadline of a block request is a function of the read-ahead buffer size and the playout rate of the stream. In case of disk failures, both architectures behave similarly. With node failure, the behavior differs.
Reference: [17] <author> D. Patterson, G. Gibson, and R. Katz. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of the ACM-SIGMOD , Chicago, </booktitle> <month> May </month> <year> 1988. </year>
Reference-contexts: There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes <ref> [4, 7, 17, 13, 14] </ref>, but these do not focus on multimedia applications. Previous work that considers high availability in a clustered environment that we know of is by Frey [9] and the work of Haskin [11]. <p> Hardware RAID schemes <ref> [17] </ref> do not offer a solution for node failure. Moreover, if the stripe group is across disks on different nodes, hardware RAID is not feasible. In the software RAID scheme, blocks are placed on disks across nodes in order to support both node and disk failures.
Reference: [18] <author> L.A. Rowe and B.C. Smith. </author> <title> A continous media player. </title> <booktitle> Proceedings of the 3rd intl NOSDAV wk-shp., </booktitle> <month> November </month> <year> 1992. </year>
Reference: [19] <author> Renu Tewari, Dan Dias, Rajat Mukherjee, and Harrick Vin. </author> <title> Real-time issues for clustered multimedia servers. </title> <institution> IBM Research Report- RC 20020, </institution> <year> 1995. </year>
Reference-contexts: Clustered multimedia servers consisting of a set of processing nodes, each with a local disk array, connected by a high bandwidth switch or network, have been proposed <ref> [8, 12, 19] </ref> to provide a scalable solution that meets the real-time requirements of multimedia streams. The basic requirement for any continuous media server or a multimedia server in general, is the delivery of isochronous streams. <p> Their staggered stripe scheme, reads a parity group in a single cycle, but plays it out over n cycles, roughly halving the memory requirements. Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. In a previous paper <ref> [19] </ref>, we studied the real-time issues in a clustered multimedia server. There are numerous other papers on disk layout for multimedia applications [6, 8, 15, 22], but these do not consider high availability. <p> Details of the architectural alternatives and real-time issues can be found in an earlier paper <ref> [19] </ref>. Here we briefly outline the architectural aspects required in the subsequent sections of this paper. Each node has a set of local disks attached to it. <p> The blocks from the playout buffer are transmitted over the external network at regular intervals, based on the playout rate of the stream. The blocks required later for playout are pre-fetched and stored in the read-ahead buffers at the front-end node. We use EDF (Earliest Deadline First <ref> [16, 19] </ref>) scheduling policy at the disk. The deadline of a block request is a function of the read-ahead buffer size and the playout rate of the stream. In case of disk failures, both architectures behave similarly. With node failure, the behavior differs. <p> Number of Streams 500 2. Stream Data Rate 0.5 MB/sec MPEG-2 rates 3. Block Size 256 KB 4. Processor Speed 100 MHz 5. Node Utilization 70% 6. Front-end s/w overhd. 6.2 cyc./byte Refer <ref> [19] </ref> 7. Back-end s/w overhd. 3.5 cyc./byte Refer [19] 8. XOR Delay (3 way) 0.012 sec. RS/6000-580 9. Mean Seek Time 10 msec. 10. Disk RPM 7200 11. Rotational Latency 4.17 msec. <p> Number of Streams 500 2. Stream Data Rate 0.5 MB/sec MPEG-2 rates 3. Block Size 256 KB 4. Processor Speed 100 MHz 5. Node Utilization 70% 6. Front-end s/w overhd. 6.2 cyc./byte Refer <ref> [19] </ref> 7. Back-end s/w overhd. 3.5 cyc./byte Refer [19] 8. XOR Delay (3 way) 0.012 sec. RS/6000-580 9. Mean Seek Time 10 msec. 10. Disk RPM 7200 11. Rotational Latency 4.17 msec. <p> The effective disk bandwidth is in turn based on the mean seek delay, the rotational latency and the transfer time of a data block. More details of the configuration can be found in <ref> [19] </ref>. Table 1 shows the default parameters used in the simulation. 4.2 Mirroring The real-time performance of block-level mirroring during failure is better than either mirrored disk or software RAID, and can be used as a basis to compare the other schemes. Mirroring doubles the disk-space requirement per video object. <p> Let us assume that the deadline of a request 4 is the time period between block requests of a stream, 1=. The deadline will also depend on the read-ahead 3 derived from Table 1 and results in <ref> [19] </ref> 4 The model can be easily generalized to set the deadline to any desired value based on the system requirements. buffer size, k. The maximum delay T max , that a block can encounter at the disk and still meets its deadline is, (k=) ffi net ffi p .
Reference: [20] <author> F. Tobagi, J. Pang, R. Baird, and M. Gang. </author> <title> Streaming RAID- a disk array management system for video files. </title> <booktitle> ACM Multimedia, </booktitle> <year> 1993. </year>
Reference-contexts: The focus of this paper is the real-time performance and price-performance tradeoffs between methods for providing high availability in clustered multimedia servers. 1.1 Related Work All the previous work on high availability in multimedia servers that we are aware of, examine single node servers and specifically focus on RAID <ref> [2, 3, 20, 21] </ref> schemes across disks within a node. The streaming RAID proposed by Tobagi et al. [20], divides the disks into parity groups 1 . In their scheme, entire parity groups are read out in one cycle and sent on the network during the next cycle. <p> The streaming RAID proposed by Tobagi et al. <ref> [20] </ref>, divides the disks into parity groups 1 . In their scheme, entire parity groups are read out in one cycle and sent on the network during the next cycle. Berson et. al. [3], generalize the streaming RAID approach.
Reference: [21] <author> Harrick M. Vin, P.J. Shenoy, and Sriram Rao. </author> <title> Efficient failure recovery in multi-disk multimedia servers. </title> <address> FTCS, </address> <year> 1995. </year>
Reference-contexts: The focus of this paper is the real-time performance and price-performance tradeoffs between methods for providing high availability in clustered multimedia servers. 1.1 Related Work All the previous work on high availability in multimedia servers that we are aware of, examine single node servers and specifically focus on RAID <ref> [2, 3, 20, 21] </ref> schemes across disks within a node. The streaming RAID proposed by Tobagi et al. [20], divides the disks into parity groups 1 . In their scheme, entire parity groups are read out in one cycle and sent on the network during the next cycle. <p> With node failure (Figure 8 (b)), the operational disk utilization can be increased from about 65% to 70% by increasing the read-ahead buffering. Alternatively <ref> [3, 21] </ref>, when a request is made for a block on the failed disk, the parity group for the next block on the failed disk could be pre-fetched and stored in the parity-group buffer.
Reference: [22] <author> H.M. Vin and P.V. Rangan. </author> <title> Designing a multiuser HDTV storage server. </title> <journal> IEEE journal on selected areas of communications, </journal> <month> January </month> <year> 1993. </year>
Reference-contexts: Their non-clustered scheme has the entire stripe group read in a cycle, only under failure conditions. In a previous paper [19], we studied the real-time issues in a clustered multimedia server. There are numerous other papers on disk layout for multimedia applications <ref> [6, 8, 15, 22] </ref>, but these do not consider high availability. Similarly, there are numerous other papers on RAID, mirrored data layout, and decluster-ing schemes [4, 7, 17, 13, 14], but these do not focus on multimedia applications.
Reference: [23] <author> Asit Dan, Dan Dias, Rajat Mukherjee, Dinkar Sitaram, and Renu Tewari. </author> <title> Buffering and caching in large scale video servers. </title> <booktitle> COMPCON, </booktitle> <year> 1995. </year>
Reference-contexts: Simulation 5.2 Cost-Performance Comparisons The system cost includes the cost of the nodes, disks and the memory cost. The switching cost is amortized over the set of nodes. Table 2 <ref> [23] </ref> gives the cost parameters used in the comparison. Equation 3 gives the system cost for mirroring C mir , and the corresponding cost for software-RAID, C RAID .
References-found: 22

