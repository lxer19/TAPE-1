URL: http://www.cs.arizona.edu/people/bkmoon/papers/ipps95.ps.gz
Refering-URL: http://www.cs.arizona.edu/people/bkmoon/papers.html
Root-URL: http://www.cs.arizona.edu
Email: fbkmoon, uysal, saltzg@cs.umd.edu  
Title: Index Translation Schemes for Adaptive Computations on Distributed Memory Multicomputers  
Author: Bongki Moon Mustafa Uysal Joel Saltz 
Affiliation: UMIACS and Dept. of Computer Science University of Maryland  
Date: April 1995, pages 812-819.  
Address: Santa Barbara, CA,  College Park, MD 20742  
Note: Proceedings of the 9th International Parallel Processing Symposium,  
Abstract: Current research in parallel programming is focused on closing the gap between globally indexed algorithms and the separate address spaces of processors on distributed memory multicomputers. A set of index translation schemes have been implemented as a part of CHAOS runtime support library, so that the library functions can be used for implementing a global index space across a collection of separate local index spaces. These schemes include two software-cached translation schemes aimed at adaptive irregular problems as well as a distributed translation table technique for statically irregular problems. To evaluate and demonstrate the efficiency of the software-cached translation schemes, experiments have been performed with an adaptively irregular loop kernel and a full-fledged 3D DSMC code from NASA Langley on the Intel Paragon and Cray T3D. This paper also discusses and analyzes the operational conditions under which each scheme can produce optimal performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ray Barriuso and Allan Knies. </author> <title> Shmem user's guide. </title> <type> Report, </type> <institution> Cray Research, Inc., </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: The current implementation has used vendor-supplied message passing libraries. It should be noted, though, that the translation schemes have been further optimized using the low latency shared memory functions on the Cray T3D <ref> [1] </ref>. The shared memory functions copy blocks of data directly from one processor's memory to another. These shared memory functions remove a substantial amount of overhead for synchronization. The last two columns in Table 1 demonstrate the optimized performance obtained from Cray T3D over that from Intel Paragon.
Reference: [2] <author> T. J. Bartel and S. J. Plimpton. </author> <title> DSMC simulation of rarefied gas dynamics on a large hypercube supercomputer, </title> <booktitle> AIAA-92-2860. In Proceedings of the 27th AIAA Thermophysics Conference, </booktitle> <address> Nashville, TN, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: It has been widely used in aerospace applications such as upper-atmosphere flows for hypersonic cruise vehicles and rocket plumes, and in vacuum-related technologies for the semiconductor industry modelling plasma etching or chemical vapor deposition <ref> [2] </ref>. The DSMC method includes movement and collision handling of simulated particles on a spatial flow field domain overlaid by a Cartesian mesh. The spatial location of each particle is associated with a Cartesian mesh cell.
Reference: [3] <author> Graeme A. Bird. </author> <title> Molecular Gas Dynamics and the Direct Simulation of Gas Flows. </title> <publisher> Clarendon Press, Oxford, </publisher> <year> 1994. </year>
Reference-contexts: Then, since the index translation information stored in local memory can not be reused, the globally indexed data items should be dereferenced whenever the access patterns change. In adaptive applications such as DSMC <ref> [3] </ref> and CHARMM [5], data access patterns change frequently and irregular data distribution is preferred for better performance over regular data distribution. Thus, minimization of the dereferencing cost is crucial for efficient processing of such applications on distributed memory multicomput-ers. <p> DSMC is a well-established technique for modelling rarefied gas dynam (R = 0.2 and S = 16) ics via direct particle simulation on a grid <ref> [3] </ref>. It has been widely used in aerospace applications such as upper-atmosphere flows for hypersonic cruise vehicles and rocket plumes, and in vacuum-related technologies for the semiconductor industry modelling plasma etching or chemical vapor deposition [2].
Reference: [4] <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesa-van, A. Malony, and B. Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel system. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 588-597. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Thus, current research in parallel programming is focused on closing the gap between globally indexed algorithms independent of the underlying distribution of data and the separate address spaces of processors. Compilers for various languages such as Fortran [8] and C++ <ref> [4] </ref> have been developed to give the illusion of a shared address space on distributed memory multicom-puters. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computa fl This work was supported by NASA under contract No. NAG-11560, by ONR under contract No.
Reference: [5] <author> B. R. Brooks and M. Hodoscek. </author> <title> Parallelization of CHARMM for MIMD machines. </title> <journal> Chemical Design Automation News, </journal> <volume> 7, </volume> <year> 1992. </year>
Reference-contexts: Then, since the index translation information stored in local memory can not be reused, the globally indexed data items should be dereferenced whenever the access patterns change. In adaptive applications such as DSMC [3] and CHARMM <ref> [5] </ref>, data access patterns change frequently and irregular data distribution is preferred for better performance over regular data distribution. Thus, minimization of the dereferencing cost is crucial for efficient processing of such applications on distributed memory multicomput-ers.
Reference: [6] <author> J. Lawrence Carter and Mark N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18 </volume> <pages> 143-154, </pages> <year> 1979. </year>
Reference-contexts: For this purpose, the current implementation of the hashed translation scheme allows the option of choosing a hash function from a universal 2 class of hash functions H 1 defined in <ref> [6] </ref>. It is experimentally shown that for a given set of keys, by choosing functions at random from the class H 1 , the theoretically predicted performance of the hash functions can be achieved in practice, independent of the key distribution [11].
Reference: [7] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Briefly outlined is how the distributed translation table can be used in the preprocessing stage of the inspector/executor model of paral-lelization <ref> [15, 7] </ref>. On distributed memory machines, large data arrays may not fit in a single-processor's memory, hence they are divided among processors. Also computational work is divided among individual processors to achieve parallelism.
Reference: [8] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Technical Report TR90-141, Rice University, </institution> <month> Decem-ber </month> <year> 1990. </year>
Reference-contexts: Thus, current research in parallel programming is focused on closing the gap between globally indexed algorithms independent of the underlying distribution of data and the separate address spaces of processors. Compilers for various languages such as Fortran <ref> [8] </ref> and C++ [4] have been developed to give the illusion of a shared address space on distributed memory multicom-puters. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computa fl This work was supported by NASA under contract No. <p> Compilers for various languages such as Fortran <ref> [8] </ref> and C++ [4] have been developed to give the illusion of a shared address space on distributed memory multicom-puters. For structured problems, such compilers as Fortran D [8] use distribution directives to partition computa fl This work was supported by NASA under contract No. NAG-11560, by ONR under contract No. SC 292-1-22913 and by ARPA under contract No. NAG-11485. The authors assume all responsibility for the contents of the paper.
Reference: [9] <author> Milan Milenkovic. </author> <title> Operating Systems Concepts and Design. </title> <publisher> McGraw-Hill, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Since implementation of the well-known page replacement algorithm LRU (Least-Recently-Used) imposes too much overhead to be handled by software alone, implemented here is the NRU (Not-Recently-Used) page replacement algorithm, one of the approximations of LRU, using the reference counters in the page table <ref> [9] </ref>. 3.2 Hashed Translation Table The structure of a hashed translation table differs from that of a paged translation table in that a hashed translation table consists of a hash table and a set of hash nodes instead of a page table and a set of page frames.
Reference: [10] <author> Bongki Moon and Joel Saltz. </author> <title> Adaptive runtime support for direct simulation Monte Carlo methods on distributed memory architectures. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-94), </booktitle> <pages> pages 176-183, </pages> <address> Knoxville, TN, May 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Furthermore, since the computations associated with performing probabilistic chemistry and collisions can be distributed across processors cell by cell, the DSMC method in principle is a good match for parallel processing on distributed memory multicomput-ers <ref> [16, 10] </ref>. Changes in position coordinates may cause the particles to move across cell boundaries. In the particular corner flow DSMC code presented here, about 30 percent of the particles change their cell locations every time step. However, particle movements are local enough that particles only move between neighboring cells.
Reference: [11] <author> M. V. Ramakrishna. </author> <title> Hashing in practice, analysis of hashing and universal hashing. </title> <booktitle> In Proceedings of the 1988 ACM SIGMOD, </booktitle> <pages> pages 191-199, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: It is experimentally shown that for a given set of keys, by choosing functions at random from the class H 1 , the theoretically predicted performance of the hash functions can be achieved in practice, independent of the key distribution <ref> [11] </ref>. These translation schemes have been implemented as a part of the CHAOS runtime support library on various distributed memory multicomputers such as Intel Paragon, IBM SP-1/2, Thinking Machine CM-5 and Cray T3D. The current implementation has used vendor-supplied message passing libraries.
Reference: [12] <author> Steven K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-level shared memory. </title> <booktitle> In Proceedings of the 21th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325-336. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> April </month> <year> 1994. </year>
Reference-contexts: The compilers can then generate message passing calls to directly pass this value from the owner processor to the processor that needs it. Another approach called Distributed Shared Memory (DSM) enables an application's user-level code to support shared memory and message passing efficiently <ref> [12] </ref>. Distributed shared memory is typically supported by the processor's address translation hardware. This paper describes and evaluates a set of index translation schemes for implementing a global index space across a collection of distributed memories. <p> These schemes can also be incorporated into distributed shared memory systems such as the one used in the Wis-consin Wind Tunnel project to support user-level shared memory <ref> [12] </ref>. To illustrate the need of runtime support for address translation, consider the Jacobi iterative method for solving a partial differential equation on an irregular numerical grid, which arises in molecular dynamics codes and sparse linear solvers.
Reference: [13] <editor> J. Saltz et al. </editor> <title> A manual for the CHAOS runtime library. </title> <type> Technical report, </type> <institution> University of Maryland, Department of Computer Science and UMIACS, </institution> <year> 1993. </year>
Reference-contexts: A dereferencing operation using the distributed translation table requires communication between processors to exchange the information stored in each processor's portion of the distributed translation table. parallelized with the CHAOS runtime library <ref> [13] </ref>. Each processor passes the procedure build translation table a list of global indices of array elements for which it will be responsible.
Reference: [14] <author> Joel Saltz, Harry Berryman, and Janet Wu. </author> <title> Multiprocessors and runtime compilation. </title> <type> Technical Report 90-59, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: These schemes can be incorporated into a runtime support library so that calls to the library functions can be invoked by manually par-allelized programs or can be generated by compilers <ref> [14] </ref>. These schemes can also be incorporated into distributed shared memory systems such as the one used in the Wis-consin Wind Tunnel project to support user-level shared memory [12].
Reference: [15] <author> Joel Saltz, Kathleen Crowley, Ravi Mirchandaney, and Harry Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 303-312, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Briefly outlined is how the distributed translation table can be used in the preprocessing stage of the inspector/executor model of paral-lelization <ref> [15, 7] </ref>. On distributed memory machines, large data arrays may not fit in a single-processor's memory, hence they are divided among processors. Also computational work is divided among individual processors to achieve parallelism.
Reference: [16] <author> Richard G. Wilmoth. </author> <title> Direct simulation Monte Carlo analysis of rarefied flows on parallel processors. </title> <journal> AIAA Journal of Thermophysics and Heat Transfer, </journal> <volume> 5(3) </volume> <pages> 292-300, </pages> <month> July-Sept. </month> <year> 1991. </year>
Reference-contexts: Furthermore, since the computations associated with performing probabilistic chemistry and collisions can be distributed across processors cell by cell, the DSMC method in principle is a good match for parallel processing on distributed memory multicomput-ers <ref> [16, 10] </ref>. Changes in position coordinates may cause the particles to move across cell boundaries. In the particular corner flow DSMC code presented here, about 30 percent of the particles change their cell locations every time step. However, particle movements are local enough that particles only move between neighboring cells.
References-found: 16

