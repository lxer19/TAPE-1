URL: http://www.cs.huji.ac.il/labs/learning/Papers/psa_colt94.ps.gz
Refering-URL: http://www.cs.huji.ac.il/labs/learning/Papers/MLT_list.html
Root-URL: http://www.cs.huji.ac.il
Email: Email: fdanar,singer,tishbyg@cs.huji.ac.il  
Title: Learning Probabilistic Automata with Variable Memory Length  
Author: Dana Ron Yoram Singer Naftali Tishby 
Address: Jerusalem 91904, Israel  
Affiliation: Institute of Computer Science and Center for Neural Computation Hebrew University,  
Abstract: We propose and analyze a distribution learning algorithm for variable memory length Markov processes. These processes can be described by a subclass of probabilistic finite automata which we name Probabilistic Finite Suffix Automata. The learning algorithm is motivated by real applications in man-machine interaction such as handwriting and speech recognition. Conventionally used fixed memory Markov and hidden Markov models have either severe practical or theoretical drawbacks. Though general hardness results are known for learning distributions generated by sources with similar structure, we prove that our algorithm can indeed efficiently learn distributions generated by our more restricted sources. In Particular, we show that the KL-divergence between the distribution generated by the target source and the distribution generated by our hypothesis can be made small with high confidence in polynomial time and sample complexity. We demonstrate the applicability of our algorithm by learning the structure of natural English text and using our hy pothesis for the correction of corrupted text.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Abe and M. Warmuth. </author> <title> On the computational complexity of approximating distributions by probabilistic automata. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 205-260, </pages> <year> 1992. </year>
Reference-contexts: The most powerful of these models is the Hidden Markov Model (HMM) [13], for which there exists a maximum likelihood estimation procedure which is widely used in many applications [12]. From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth <ref> [1] </ref> study the problem of training HMMs. The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs. They prove that HMMs are not trainable in time polynomial in the alphabet size, unless RP = NP. <p> Finite Automata and Prediction Suffix Trees 2.2.1 Probabilistic Finite Automata A Probabilistic Finite Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Q fi ! Q is the transition function, fl : Qfi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. The functions fl and must satisfy the following requirements: for every q 2 Q, P 2 fl (q; ) = 1, and q2Q (q) = 1. <p> Finite Automaton (PFA) M is a 5-tuple (Q; ; t; fl; ), where Q is a finite set of states, is a finite alphabet, t : Q fi ! Q is the transition function, fl : Qfi ! <ref> [0; 1] </ref> is the next symbol probability function, and : Q ! [0; 1] is the initial probability distribution over the starting states. The functions fl and must satisfy the following requirements: for every q 2 Q, P 2 fl (q; ) = 1, and q2Q (q) = 1. <p> The nodes of the tree are labeled by pairs (s; fl s ) where s is the string associated with the `walk' starting from that node and ending in the root of the tree, and fl s : ! <ref> [0; 1] </ref> is the next symbol probability function related with s.
Reference: [2] <author> Lewis Carroll. </author> <note> Alice's adventures in wonderland. The Millennium Fulcrum edition 2.9. </note>
Reference: [3] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Hence, to obtain a measure independent of that length it is necessary to 1 Similar definitions can be considered for other distance measures such as the variation and the quadratic distances. Note that the KL-divergence bounds the variation distance as follows <ref> [3] </ref>: D KL [P 1 jjP 2 ] 1 2 jjP 1 P 2 jj 2 1 . Since the L 1 norm bounds the L 2 norm, the last bound holds for the quadratic distance as well. divide the KL-divergence by the length of the strings, N .
Reference: [4] <author> J.A. Fill. </author> <title> Eigenvalue bounds on convergence to stationary for nonreversible Markov chains, with an application to exclusion process. </title> <journal> Annals of Applied Probability, </journal> <volume> 1 </volume> <pages> 62-87, </pages> <year> 1991. </year>
Reference-contexts: This convergence rate can be bounded using expansion properties of a weighted graph related to U M [10] or more generally, using algebraic properties of U M , namely, its second largest eigenvalue <ref> [4] </ref>. 4 Emulation of PFSAs by PSTs In this section we show that for every PFSA there exists an equivalent PST which is not much larger. This allows us to consider the PST equivalent to our target PFSA, whenever it is convenient.
Reference: [5] <author> Y. Freund, M. Kearns, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> In Proceedings of the 24th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 315-324, </pages> <year> 1993. </year>
Reference-contexts: Hoffgen [6] studies related families of distributions, but his algorithms depend exponentially and not polynomi-ally on the order, or memory length, of the distributions. Freund et. al. <ref> [5] </ref> point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard.
Reference: [6] <author> K.-U. Hoffgen. </author> <title> Learning and robust learning of product distributions. </title> <booktitle> In Proceedings of the Sixth Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-106, </pages> <year> 1993. </year>
Reference-contexts: Furthermore, such a hypothesis can be obtained from a single sample sequence if the sequence length is also polynomial in a parameter related to the mixing rate (or the correlation length) of the target ma chine. Hoffgen <ref> [6] </ref> studies related families of distributions, but his algorithms depend exponentially and not polynomi-ally on the order, or memory length, of the distributions.
Reference: [7] <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <type> Technical report, </type> <institution> IBM T.J. Watson Research Center, </institution> <year> 1985. </year>
Reference-contexts: 1 Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language and speech <ref> [7, 11] </ref>, and statistical models of biological sequences such as DNA and proteins [9]. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function. This observation suggests modeling such sequences by Markov and hidden Markov models.
Reference: [8] <author> M. Kearns, Y.Mansour, D. Ron, R. Rubinfeld, R.E. Schapire, and L. Sellie. </author> <title> On the learnability of discrete distributions. </title> <booktitle> In The 25th Annual ACM Symposium on Theory of Computing (to appear), </booktitle> <year> 1994. </year>
Reference-contexts: Freund et. al. [5] point out that their result for learning typical deterministic finite automata from random walks without membership queries, can be extended to learning typical PFAs. Unfortunately, there is strong evidence indicating that the problem of learning general PFAs is hard. Kearns et. al. <ref> [8] </ref> show that PFAs are not efficiently learnable under the assumption that there is no efficient algorithm for learning noisy parity functions in the PAC model. Despite the intractability result mentioned above, our restricted model can be learned in a PAC-like sense efficiently. <p> The transition probabilities are depicted on the edges. Bottom: A prediction suffix tree. The prediction probabilities of the symbols `0' and `1', respectively, are depicted beside the nodes, in parentheses. 3 The Learning Model The learning model described in this paper is similar in spirit to that introduced in <ref> [8] </ref> in the sense that our approach is also motivated by the PAC model for learning boolean concepts from labeled examples. The two models differ slightly in their assumptions on the input to the learning algorithm and in the requirements from its output.
Reference: [9] <author> A. Krogh, S.I. Mian, and D. Haussler. </author> <title> A hidden markov model that finds genes in E. coli DNA. </title> <type> Technical Report UCSC-CRL-93-16, </type> <institution> University of California at Santa-Cruz, </institution> <year> 1993. </year>
Reference-contexts: The most noticeable examples of such applications are statistical models in human communication such as natural language and speech [7, 11], and statistical models of biological sequences such as DNA and proteins <ref> [9] </ref>. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function. This observation suggests modeling such sequences by Markov and hidden Markov models.
Reference: [10] <author> M. Mihail. </author> <title> Conductance and convergence of Markov chains A combinatorial treatment of expanders. </title> <booktitle> In Proceedings 30th Annual Conference on Foundations of Computer Science, </booktitle> <year> 1989. </year>
Reference-contexts: When given one sample string, the given string must be long enough so as to ensure convergence of the probability of visiting a state to the station ary probability. This convergence rate can be bounded using expansion properties of a weighted graph related to U M <ref> [10] </ref> or more generally, using algebraic properties of U M , namely, its second largest eigenvalue [4]. 4 Emulation of PFSAs by PSTs In this section we show that for every PFSA there exists an equivalent PST which is not much larger.
Reference: [11] <author> A. Nadas. </author> <title> Estimation of probabilities in the language model of the IBM speech recognition system. </title> <journal> IEEE Trans. on ASSP, </journal> <volume> 32(4) </volume> <pages> 859-861, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction Statistical modeling of complex sequences is a fundamental goal of machine learning due to its wide variety of natural applications. The most noticeable examples of such applications are statistical models in human communication such as natural language and speech <ref> [7, 11] </ref>, and statistical models of biological sequences such as DNA and proteins [9]. These kinds of complex sequences generally do not have any simple underlying statistical source, but they typically exhibit an exponentially decaying autocorrelation function. This observation suggests modeling such sequences by Markov and hidden Markov models.
Reference: [12] <author> L.R. Rabiner. </author> <title> A tutorial on hidden markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <year> 1989. </year>
Reference-contexts: The most powerful of these models is the Hidden Markov Model (HMM) [13], for which there exists a maximum likelihood estimation procedure which is widely used in many applications <ref> [12] </ref>. From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth [1] study the problem of training HMMs. The HMM training problem is the problem of approximating an arbitrary, unknown source distribution by distributions generated by HMMs.
Reference: [13] <author> L.R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <volume> 3(1) </volume> <pages> 4-16, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: These statistical models cap ture a rich family of sequence distributions and moreover, they give efficient procedures both for generating sequences and for computing their probabilities. The most powerful of these models is the Hidden Markov Model (HMM) <ref> [13] </ref>, for which there exists a maximum likelihood estimation procedure which is widely used in many applications [12]. From the computational learning theory point of view however, the HMM has severe drawbacks. Abe and Warmuth [1] study the problem of training HMMs.
Reference: [14] <author> J. Rissanen. </author> <title> A universal data compression system. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 29(5) </volume> <pages> 656-664, </pages> <year> 1983. </year>
Reference-contexts: Moreover, the estimation of the full order L Markov model requires data length and time exponential in L. As our hypothesis class we choose a slightly different family of probabilistic machines named Probabilistic Suffix Trees (PST). Similar tree machines have been used before for universal data compression <ref> [14, 17, 18] </ref>. In the learning algorithm we `grow' such a tree starting from a single root node, and adaptively add nodes (strings) for which there is strong evidence in the sample that they significantly affect the prediction properties of the tree. <p> If the PST does not have that property then we can prove a slightly weaker claim. A similar version of these trees was introduced in <ref> [14] </ref> and has been used for other tasks such as compression and prediction (for examples see [17, 18]). A PST T , over an alphabet , is a tree of degree jj.
Reference: [15] <author> D. Ron, Y. Singer, and N. Tishby. </author> <title> The power of amnesia. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 6. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference: [16] <author> H. Schutze and Y. Singer. </author> <title> Part-of-Speech tagging using a variable memory Markov model. </title> <booktitle> In Proceedings of ACL 32'nd, </booktitle> <year> 1994. </year>
Reference: [17] <author> M.J. Weinberger, A. Lempel, and J. Ziv. </author> <title> A sequential algorithm for the universal coding of finite-memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38 </volume> <pages> 1002-1014, </pages> <month> May </month> <year> 1982. </year>
Reference-contexts: Moreover, the estimation of the full order L Markov model requires data length and time exponential in L. As our hypothesis class we choose a slightly different family of probabilistic machines named Probabilistic Suffix Trees (PST). Similar tree machines have been used before for universal data compression <ref> [14, 17, 18] </ref>. In the learning algorithm we `grow' such a tree starting from a single root node, and adaptively add nodes (strings) for which there is strong evidence in the sample that they significantly affect the prediction properties of the tree. <p> If the PST does not have that property then we can prove a slightly weaker claim. A similar version of these trees was introduced in [14] and has been used for other tasks such as compression and prediction (for examples see <ref> [17, 18] </ref>). A PST T , over an alphabet , is a tree of degree jj. Each edge in the tree is labeled by a single symbol in , such that from every internal node there is exactly one edge labeled by each symbol.
Reference: [18] <author> M.J. Weinberger, J. Rissanen, and M. Feder. </author> <title> A universal finite memory source. </title> <note> Submitted for publication. </note>
Reference-contexts: Moreover, the estimation of the full order L Markov model requires data length and time exponential in L. As our hypothesis class we choose a slightly different family of probabilistic machines named Probabilistic Suffix Trees (PST). Similar tree machines have been used before for universal data compression <ref> [14, 17, 18] </ref>. In the learning algorithm we `grow' such a tree starting from a single root node, and adaptively add nodes (strings) for which there is strong evidence in the sample that they significantly affect the prediction properties of the tree. <p> If the PST does not have that property then we can prove a slightly weaker claim. A similar version of these trees was introduced in [14] and has been used for other tasks such as compression and prediction (for examples see <ref> [17, 18] </ref>). A PST T , over an alphabet , is a tree of degree jj. Each edge in the tree is labeled by a single symbol in , such that from every internal node there is exactly one edge labeled by each symbol.
References-found: 18

