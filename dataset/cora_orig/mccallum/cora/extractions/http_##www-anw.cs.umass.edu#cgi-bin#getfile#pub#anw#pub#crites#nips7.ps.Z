URL: http://www-anw.cs.umass.edu/cgi-bin/getfile/pub/anw/pub/crites/nips7.ps.Z
Refering-URL: http://www-anw.cs.umass.edu/People/crites/crites.html
Root-URL: 
Email: crites@cs.umass.edu  barto@cs.umass.edu  
Title: Equivalent to Q-Learning  
Author: G. Tesauro, D. S. Touretzky and T. K. Leen, Robert H. Crites Andrew G. Barto 
Address: Amherst, MA 01003  Amherst, MA 01003  
Affiliation: Computer Science Department University of Massachusetts  Computer Science Department University of Massachusetts  
Note: To appear in:  eds., Advances in Neural Information Processing Systems 7, MIT Press, Cambridge MA, 1995. An Actor/Critic Algorithm that is  
Abstract: We prove the convergence of an actor/critic algorithm that is equivalent to Q-learning by construction. Its equivalence is achieved by encoding Q-values within the policy and value function of the actor and critic. The resultant actor/critic algorithm is novel in two ways: it updates the critic only when the most probable action is executed from any given state, and it rewards the actor using criteria that depend on the relative probability of the action that was executed.
Abstract-found: 1
Intro-found: 1
Reference: <author> A. G. Barto, S. J. Bradtke & S. P. Singh. </author> <title> (1993) Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <note> Accepted. </note>
Reference: <author> A. G. Barto, R. S. Sutton & C. W. Anderson. </author> <title> (1983) Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics 13 </journal> <pages> 835-846. </pages>
Reference: <author> A. G. Barto, R. S. Sutton & C. J. C. H. Watkins. </author> <title> (1990) Learning and sequential decision making. </title> <editor> In M. Gabriel & J. Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> D. P. Bertsekas & J. N. Tsitsiklis. </author> <title> (1989) Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> T. Jaakkola, M. I. Jordan & S. P. Singh. </author> <title> (1993) On the convergence of stochastic iterative dynamic programming algorithms. </title> <institution> MIT Computational Cognitive Science Technical Report 9307. </institution>
Reference: <author> L. Lin. </author> <title> (1993) Reinforcement Learning for Robots Using Neural Networks. </title> <type> PhD Thesis, </type> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> A. L. Samuel. </author> <title> (1963) Some studies in machine learning using the game of checkers. </title>
Reference-contexts: The result is two interacting adaptive processes: the actor adapts to the critic, while the critic adapts to the actor. The foundations of actor/critic learning systems date back at least to Samuel's checker program in the late 1950s <ref> (Samuel,1963) </ref>. Examples of actor/critic systems include Barto, Sutton, & Anderson's (1983) ASE/ACE architecture and Sutton's (1990) Dyna-PI architecture. Sutton (1988) notes that the critic in these systems performs temporal credit assignment using what he calls temporal difference (TD) methods.
Reference: <editor> In E. Feigenbaum & J. Feldman, editors, </editor> <booktitle> Computers and Thought. </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference: <author> R. S. Sutton. </author> <title> (1988) Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-44. </pages>
Reference: <author> R. S. Sutton. </author> <title> (1990) Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning. </booktitle>
Reference-contexts: Sutton (1990) modifies the generic algorithm so that these parameters can be interpreted as action value estimates. He redefines " in step 2 as follows: " = [r + fl ^ V (y)] w (x; a): For this reason, the Dyna-PI architecture <ref> (Sutton, 1990) </ref> and the modified actor/critic algorithm we present below both reward less probable actions more readily because of their lower estimated values.
Reference: <author> J. N. Tsitsiklis. </author> <title> (1994) Asynchronous stochastic approximation and Q-learning. </title> <booktitle> Machine Learning 16 </booktitle> <pages> 185-202. </pages>
Reference: <author> C. J. C. H. Watkins. </author> <title> (1989) Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cam-bridge University. </institution>
Reference: <author> C. J. C. H. Watkins & P. </author> <title> Dayan. </title> <booktitle> (1992) Q-learning. Machine Learning 8 </booktitle> <pages> 279-292. </pages>
Reference: <author> R. J. Williams & L. C. Baird. </author> <title> (1993) Analysis of some incremental variants of policy iteration: first steps toward understanding actor-critic learning systems. </title> <type> Technical Report NU-CCS-93-11. </type> <institution> Northeastern University College of Computer Science. </institution>
References-found: 14

