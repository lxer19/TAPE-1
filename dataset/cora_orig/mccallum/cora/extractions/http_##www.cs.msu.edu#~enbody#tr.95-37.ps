URL: http://www.cs.msu.edu/~enbody/tr.95-37.ps
Refering-URL: http://www.cs.msu.edu/~enbody/
Root-URL: http://www.cs.msu.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Barrett R, </author> <title> "Getting the Most Out of PVM", </title> <address> HPCwire July 28th, </address> <year> 1995. </year>
Reference-contexts: Further improvements to communications were made by optimizing PVM options. For example, PvmDirect was used which sets up TCP sockets for the communications allowing the applications to directly communicate with each other; versus going through the PVM daemon for every message <ref> [1] </ref>. Thus, communications improved by about another 10%. A last optimization uses PvmDataRaw for packing PVM buffers. This option specifies to PVM that it should not use XDR to encode messages, and should directly copy the data to the send buffers [1]. <p> versus going through the PVM daemon for every message <ref> [1] </ref>. Thus, communications improved by about another 10%. A last optimization uses PvmDataRaw for packing PVM buffers. This option specifies to PVM that it should not use XDR to encode messages, and should directly copy the data to the send buffers [1]. Performance was slightly faster than the default which uses XDR. An unsuccessful PVM optimization specifies PvmDataInPlace which means that PVM should directly copy from application memory to system memory, and skip the copy to PVM space [1]. <p> to encode messages, and should directly copy the data to the send buffers <ref> [1] </ref>. Performance was slightly faster than the default which uses XDR. An unsuccessful PVM optimization specifies PvmDataInPlace which means that PVM should directly copy from application memory to system memory, and skip the copy to PVM space [1]. However, this optimization resulted in much slower communication because the data in memory may not have been stored in a contiguous enough manner for efficient copying [1]. Overall, the optimizations and contention management improved the communication code to over 60% faster than the original naive approach. <p> optimization specifies PvmDataInPlace which means that PVM should directly copy from application memory to system memory, and skip the copy to PVM space <ref> [1] </ref>. However, this optimization resulted in much slower communication because the data in memory may not have been stored in a contiguous enough manner for efficient copying [1]. Overall, the optimizations and contention management improved the communication code to over 60% faster than the original naive approach. Performance Results impact of load balancing.
Reference: [2] <author> Bilan T, Doom T, Muras B, </author> <title> "Issues in Workstation Cluster Distributed Processing" Term Project. </title> <institution> Computer Science 812. Michigan State University. </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: Although the machines were idle except for this application, there can still be substantial differences in the amount of processing capacity available even across homogenous machines as shown in <ref> [2] </ref>. The result was that it was very difficult to help a cell three consecutive times, and therefore very difficult to acquire new cells. Thus, we turned load sharing or helping off which resulted in rapid growth of the local neighbor metric (see figure 6) .
Reference: [3] <author> Liu J, Saletore V, </author> <title> "Self Scheduling on Distributed-Memory Machines", </title> <booktitle> Proceedings Supercomputing 1993, </booktitle> <publisher> IEEE Computer Society Press, </publisher> <pages> pp 814-823. </pages>
Reference-contexts: All task pools overlap with at least part of another processor's pool, and most likely with parts of several processor's pools. This overlapping is similar to partial duplication described in <ref> [3] </ref> where a given task is only replicated across a certain subset of the processors. Within these overlapping pools, processors grab tasks based on their relative contiguity as 6 described above. Tasks are awarded to processors on a first come basis.
Reference: [4] <author> Plimpton S, Heffelfinger G, </author> <title> "Scalable Parallel Molecular Dynamics on MIMD Supercomputers", </title> <booktitle> 7th IEEE Scalable High Performance Computer Conference, </booktitle> <year> 1992, </year> <pages> pp 246-251. </pages>
Reference-contexts: This spacial partitioning maps naturally into a parallel processing environment where each processor can work on a given subsection of the simulation space. <ref> [4] </ref> has shown that a spacial partitioning is most effective for particle interactions of this type. The Lennard-Jones simulation was chosen to test the evolutionary load balancing algorithm because the physics force calculations are very fast, and ineffective load balancing 2 would be very apparent.
Reference: [5] <author> Severance C, Enbody R, </author> <title> "Evolutionary Load Balancing on NUMA Parallel Processors", </title> <address> http://clunix.cl.msu.edu:~crs/papers/super_95 </address>
Reference-contexts: Thus, one or more processors are not sitting idle while the remaining processors are still computing. This project explores a load balancing implementation motivated by results described in <ref> [5] </ref>. In [5], an evolutionary load balancing scheme is implemented whereby the system moves to a balanced state without any explicit load balancing state information being used. <p> Thus, one or more processors are not sitting idle while the remaining processors are still computing. This project explores a load balancing implementation motivated by results described in <ref> [5] </ref>. In [5], an evolutionary load balancing scheme is implemented whereby the system moves to a balanced state without any explicit load balancing state information being used. <p> The general pattern of figure 5 is that as the load is balanced, the execution time is faster and less cell migration is necessary. The original shared memory evolutionary algorithm in <ref> [5] </ref> must help a particular cell a threshold number of times in a row before it takes ownership of the cell. In the shared memory implementation the threshold is set at three. However, helping or load sharing was not as effective for this application as pure load balancing. <p> Thus, in the fastest version of the program, the threshold is set to one and no helping takes place. Cells are simply migrated to the processor which performed the work on it. 13 In <ref> [5] </ref>, the local neighbor metric with a helping threshold of three grew much faster than in this application due to the more loosely coupled environment that this application was running in.
Reference: [6] <author> Severance C, Enbody R, </author> <title> "Real Valued Arrays - An Initial Look", </title> <booktitle> Proc. 1992 SIAM Conference of Parallel Processing, </booktitle> <pages> pp 979-982. </pages> <address> http://clunix.cl.msu.edu:80/~crs/papers/siam_93 </address>
Reference-contexts: The particular platform that we have used is Parallel Virtual Machine (PVM) running on a network of SPARC workstations over a traditional ethernet network. The implementation uses a Real Value Indexed (RVI) Arrays package to manage the particles of the simulation which is described further in <ref> [6] </ref>. Lennard-Jones Simulation The Lennard-Jones simulation models the dynamic interactions of free-floating particles such as gas molecules in some given area.
References-found: 6

