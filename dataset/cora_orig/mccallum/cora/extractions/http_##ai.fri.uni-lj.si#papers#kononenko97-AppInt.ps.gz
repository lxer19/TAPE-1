URL: http://ai.fri.uni-lj.si/papers/kononenko97-AppInt.ps.gz
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Title: Overcoming the myopia of inductive learning algorithms with RELIEFF  
Author: IGOR KONONENKO, EDVARD SIMEC, MARKO ROBNIK- SIKONJA 
Keyword: learning from examples, estimating attributes, impurity function, RELIEFF, empirical evaluation  
Address: Trzaska 25, SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of electrical engineering computer science  
Note: [Journal Name], [Volumn Number], 1-17 ([Volumn Year]) c [Volumn Year] Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: igor.kononenko@fer.uni-lj.si  
Phone: tel: +386-61-1768390; fax: +386-61-1264630  
Abstract: Current inductive machine learning algorithms typically use greedy search with limited looka-head. This prevents them to detect significant conditional dependencies between the attributes that describe training objects. Instead of myopic impurity functions and lookahead, we propose to use RELI-EFF, an extension of RELIEF developed by Kira and Rendell [10], [11], for heuristic guidance of inductive learning algorithms. We have reimplemented Assistant, a system for top down induction of decision trees, using RELIEFF as an estimator of attributes at each selection step. The algorithm is tested on several artificial and several real world problems and the results are compared with some other well known machine learning algorithms. Excellent results on artificial data sets and two real world problems show the advantage of the presented approach to inductive learning. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> L. Breiman, J.H. Friedman, R.A. Olshen, and C.J. Stone. </author> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <year> 1984. </year>
Reference-contexts: The heuristic function that estimates the potential successors of the current state in the search space has a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio [25], gini-index <ref> [1] </ref>, distance measure [16], j-measure [30], and MDL [14]. However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. <p> Function diff (Attribute,Instance1,Instance2) calculates the difference between the values of Attribute for two instances. For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval <ref> [0; 1] </ref>. Normalization with n guarantees all weights W [A] to be in the interval [1; 1], however, normalization with n is an unnecessary step if W [A] is to be used for relative comparison among attributes. The weights are estimates of the quality of attributes. <p> For discrete attributes the difference is either 1 (the values are different) or 0 (the values are equal), while for continuous attributes the difference is the actual difference normalized to the interval [0; 1]. Normalization with n guarantees all weights W [A] to be in the interval <ref> [1; 1] </ref>, however, normalization with n is an unnecessary step if W [A] is to be used for relative comparison among attributes. The weights are estimates of the quality of attributes. <p> W 0 [A] = P eqval fi Ginigain 0 (A) P samecl (1 P samecl ) = const fi X P (V ) 2 fi Ginigain 0 (A) (3) where Ginigain 0 (A) = X P X P (CjV ) 2 C (4) is highly correlated with the gini-index gain <ref> [1] </ref> for classes C and values V of attribute A. The difference is that instead of factor P (V ) 2 V P (V ) 2 the gini-index gain uses P (V ) V P (V ) Equation (3) shows strong relation of RELIEF's weights with the gini-index gain. <p> data set and the estimated quality of attributes function A1 A2 A3 Class 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain <ref> [1] </ref> 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 <p> 1 1 0 RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain <ref> [1] </ref> 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant.
Reference: 2. <author> B. Cestnik. </author> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> Proc. European Conference on Artificial Intelligence. </booktitle> <address> Stockholm, </address> <month> August </month> <year> 1990, </year> <month> pp.147-149. </month>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that RELI-EFF is used for attribute selection. In addition, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms <ref> [2] </ref>, [3]. For prior probabilities Laplace's law of succession is used: P a (X) = N + # of possible outcomes (6) where N is the number of all trials and N (X) the number of trials with the outcome X. <p> In our experiments, the parameter m was set to 2 (this setting is usually used as default and, empirically, gives satisfactory results <ref> [2] </ref>, [3] although THE TITLE??? 7 with tuning in some problem domains better results may be expected). The m-estimate is used in the naive Bayesian formula (5), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko [3], and for RELIEFF's estimates of probabilities. <p> Cestnik <ref> [2] </ref> has shown that the m-estimate significantly increases the performance of the naive Bayesian classifier which is also confirmed with our experiments. Both versions of Assistant perform the same on all data sets except on the SAT data set where Assistant-R and LFC achieve significantly better result (99.95% confidence level).
Reference: 3. <author> B. Cestnik and I. Bratko. </author> <title> On estimating probabilities in tree pruning. </title> <booktitle> Proc. European Working Session on Learning. (Porto, </booktitle> <month> March </month> <year> 1991), </year> <editor> Y.Kodratoff (ed.), </editor> <publisher> Springer Verlag. </publisher> <address> pp.138-150, </address> <year> 1991. </year>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that RELI-EFF is used for attribute selection. In addition, wherever appropriate, instead of the relative frequency, Assistant-R uses the m-estimate of probabilities, which was shown to often significantly increase the performance of machine learning algorithms [2], <ref> [3] </ref>. For prior probabilities Laplace's law of succession is used: P a (X) = N + # of possible outcomes (6) where N is the number of all trials and N (X) the number of trials with the outcome X. <p> In our experiments, the parameter m was set to 2 (this setting is usually used as default and, empirically, gives satisfactory results [2], <ref> [3] </ref> although THE TITLE??? 7 with tuning in some problem domains better results may be expected). The m-estimate is used in the naive Bayesian formula (5), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko [3], and for RELIEFF's estimates of probabilities. <p> usually used as default and, empirically, gives satisfactory results [2], <ref> [3] </ref> although THE TITLE??? 7 with tuning in some problem domains better results may be expected). The m-estimate is used in the naive Bayesian formula (5), for postpruning instead of Laplace's law of succession as proposed by Cestnik and Bratko [3], and for RELIEFF's estimates of probabilities.
Reference: 4. <author> B. Cestnik, I. Kononenko, and I. Bratko. </author> <title> ASSISTANT 86 : A knowledge elicitation tool for sophisticated users. </title> <editor> In: I. Bratko and N. Lavrac (eds.). </editor> <booktitle> Progress in Machine Learning. </booktitle> <address> Wilmslow, England: </address> <publisher> Sigma Press, </publisher> <year> 1987. </year>
Reference-contexts: Kira and Rendell used RELIEF as a preprocessor to eliminate irrelevant attributes from data description before learning. RELIEFF is general, relatively efficient, and reliable enough to guide the search in the learning process. In this paper a reimplementation of Assistant learning algorithm for top down induction of decision trees <ref> [4] </ref> is described, named Assistant-R. Instead of information gain, Assistant-R uses RELIEFF as a heuristic function for estimating the attributes' quality at each step during the tree generation. <p> Impurity functions tend to overestimate multi-valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio [25], distance measure [16], and bina-rization of attributes <ref> [4] </ref>). Equation (3) shows that RELIEF exhibits an implicit normalization effect. Another deficiency of gini-index gain is that its values tend to decrease with the increasing number of classes [14]. <p> Information gain and gini-index overestimate one attribute with 3 values (by the opinion of physicians specialists). RELIEFF and normalized versions of impurity functions correctly estimate this attribute as less important. 3. Assistant-R Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees <ref> [4] </ref>. The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [9] and reimplemented by several authors (see [25] for an overview). In the following we describe the main features of Assistant. Binarization of attributes: The algorithm generates binary decision trees.
Reference: 5. <author> W. Chase and F. Brown. </author> <title> General Statistics. </title> <publisher> John Wiley & Sons, </publisher> <year> 1986. </year>
Reference-contexts: Each system used the same subsets of instances for learning and for testing in order to provide the same experimental conditions. To verify the significance of differences we used the one-tailed t-test with ff = 0:0005 (99.95% confidence level) and the null hypothesis stating that the difference is zero <ref> [5] </ref>. All the differences in results having the value of statistic t above the threshold (t &gt; 3:66) are considered significant.
Reference: 6. <author> B. Dolsak and S. Muggleton. </author> <title> The application of inductive logic programming to finite element mesh design. </title> <editor> In: Muggleton S. (ed.). </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: IRIS: The well known Fisher's problem of deter mining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> [6] </ref>. There are five objects for which experts have constructed appropriate meshes. In each of five experiments one object is used for testing and the other four for learning and the results are averaged.
Reference: 7. <author> S. Dzeroski. </author> <title> Handling noise in inductive logic programming. M.SC. </title> <type> Thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering & computer science, Ljubljana, Slovenia, </institution> <year> 1991. </year>
Reference-contexts: The attributes describe the relevant relations between pieces, such as "same rank" and "adjacent file". Originally the data included five sets of 1000 examples (1000 for learning and 4000 for testing) and was used to test Inductive Logic Programming algorithms <ref> [7] </ref>. The reported classification accuracy is 99.70.1 %. We used only one set of 1000 examples (i.e. 700 instances for training). KRK2: Same as KRK1 except that the only available attributes are the coordinates of pieces. The same data set was used by Mladenic [19]. <p> There are five objects for which experts have constructed appropriate meshes. In each of five experiments one object is used for testing and the other four for learning and the results are averaged. The results reported by Dzeroski <ref> [7] </ref> for various ILP systems are 12% classification accuracy for FOIL, 22% for mFOIL and 29% for GOLEM and the result reported by Pompe et al. [23] is 28% for SFOIL. The description of the MESH problem is appropriate for ILP systems. <p> The most interesting results appear in the MESH domains. Although attribute learners in MESH3 have less information than ILP systems, they all outperform the results by ILP systems as reported by Dzeroski <ref> [7] </ref> and Pompe et al. [23]. With 12 additional attributes in MESH15, the results of inductive learners are significantly improved. All inductive learning systems significantly outperform the naive Bayesian classifier and the k-NN algorithm.
Reference: 8. <author> S.J. Hong. </author> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical report, </type> <institution> IBM RC19664, </institution> <note> 7/94, 1994. (to appear in IEEE Trans. on Knowledge and Data Engineering). </note>
Reference-contexts: Hong <ref> [8] </ref> developed a procedure similar to RELIEF for estimating the quality of attributes, where he directly emphasizes the use of contextual information. The difference to RELIEF is that his approach uses only information from nearest misses and ignores nearest hits.
Reference: 9. <author> E. Hunt, J. Martin and P. Stone. </author> <title> Experiments in Induction, </title> <address> New York, </address> <publisher> Academic Press, </publisher> <year> 1966. </year>
Reference-contexts: of attributes function A1 A2 A3 Class 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain <ref> [9] </ref> 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. <p> RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain <ref> [9] </ref> 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. Hong [8] developed a procedure similar to RELIEF for estimating the quality of attributes, where he directly emphasizes the use of contextual information. <p> RELIEFF and normalized versions of impurity functions correctly estimate this attribute as less important. 3. Assistant-R Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees [4]. The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. <ref> [9] </ref> and reimplemented by several authors (see [25] for an overview). In the following we describe the main features of Assistant. Binarization of attributes: The algorithm generates binary decision trees. At each decision step the binarized version of each attribute is selected that maximizes the information gain of the attribute.
Reference: 10. <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <editor> July 1992) D.Sleeman and P.Edwards (eds.), </editor> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference-contexts: However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. Kira and Rendell <ref> [10] </ref>, [11] developed an algorithm called RELIEF, which seems to be very powerful in estimating the quality of attributes. <p> W [A] := W [A] - diff (A,R,H)/n 8. + diff (A,R,M)/n; 9. end; searches for its two nearest neighbors: one from the same class (called nearest hit) and the other from a different class (called nearest miss). The original algorithm of RELIEF <ref> [10] </ref>, [11] randomly selects n training instances, where n is the user-defined parameter. The algorithm is given in Figure 1. Function diff (Attribute,Instance1,Instance2) calculates the difference between the values of Attribute for two instances. <p> This approach assumes that conditional probabilities of attribute-values given the class are applicable without the context of any other attribute. This may in some cases be too naive, however including the context of other atributes is far too inefficient. Multi-class problems: Kira and Rendell <ref> [10] </ref>, [11] claim that RELIEF can be used to estimate the attributes' qualities in data sets with more than two classes by splitting the problem into a series of 2-class problems.
Reference: 11. <author> K. Kira and L. Rendell. </author> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference-contexts: However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. Kira and Rendell [10], <ref> [11] </ref> developed an algorithm called RELIEF, which seems to be very powerful in estimating the quality of attributes. <p> W [A] := W [A] - diff (A,R,H)/n 8. + diff (A,R,M)/n; 9. end; searches for its two nearest neighbors: one from the same class (called nearest hit) and the other from a different class (called nearest miss). The original algorithm of RELIEF [10], <ref> [11] </ref> randomly selects n training instances, where n is the user-defined parameter. The algorithm is given in Figure 1. Function diff (Attribute,Instance1,Instance2) calculates the difference between the values of Attribute for two instances. <p> This approach assumes that conditional probabilities of attribute-values given the class are applicable without the context of any other attribute. This may in some cases be too naive, however including the context of other atributes is far too inefficient. Multi-class problems: Kira and Rendell [10], <ref> [11] </ref> claim that RELIEF can be used to estimate the attributes' qualities in data sets with more than two classes by splitting the problem into a series of 2-class problems.
Reference: 12. <author> I. Kononenko. </author> <title> Inductive and Bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 317-337, </pages> <year> 1993. </year>
Reference-contexts: The results of experiments on these data sets are provided in Tables 9 and 10. In medical data sets, attributes are typically conditionally independent given the class . Therefore, it is not surprising that the naive Bayesian classifier shows clear advantage on these data sets <ref> [12] </ref>. It is interesting that the performance of the k-NN algorithm is good in these domains, although worse than the performance of the naive Bayesian classifier. The information score (Table 10) for BREA data set indicates that no learning algorithm was able to solve this problem.
Reference: 13. <author> I. Kononenko. </author> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. European Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994). </year> <editor> L. De Raedt & F.Bergadano (eds.), </editor> <publisher> Springer Verlag, </publisher> <pages> pp. 171-182, </pages> <year> 1994. </year> <note> THE TITLE??? 17 </note>
Reference-contexts: To increase the reliability of the probability approximation RELIEFF searches for k nearest hits/misses instead of only one near hit/miss and averages the contribution of all k nearest hits/misses. It was shown that this extension significantly improves the reliability of estimates of attributes' qualities <ref> [13] </ref>. To overcome the problem of parameter tuning, in all our experiments k was set to 10 which, empirically, gives satisfactory results. In some problems significantly better results can be obtained with tuning (as is typical for the majority of machine learning algorithms). <p> Note that the time complexity of RELIEFF is O (N 2 fi #attributes), where N is the number of training instances. 2.4. RELIEFF's estimates and attribute's qual ity To estimate the contribution of parameter k (# nearest hits/misses) on RELIEFF's estimates of attribute's quality Kononenko <ref> [13] </ref> compared the intended information gain of attributes with the estimates, generated by RELIEFF, by calculating the standard linear correlation coefficient. The correlation coefficient can show how is the intended quality and the estimated quality of attributes related.
Reference: 14. <author> I. Kononenko. </author> <title> On biases when estimating multivalued attributes. </title> <booktitle> Proc. IJCAI-95 (Montreal, </booktitle> <month> August </month> <year> 1995). </year> <editor> C.Mellish (ed.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 1034-1040, </pages> <year> 1995. </year>
Reference-contexts: The heuristic function that estimates the potential successors of the current state in the search space has a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio [25], gini-index [1], distance measure [16], j-measure [30], and MDL <ref> [14] </ref>. However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. <p> Equation (3) shows that RELIEF exhibits an implicit normalization effect. Another deficiency of gini-index gain is that its values tend to decrease with the increasing number of classes <ref> [14] </ref>. Denominator which is constant factor in equation (3) for a given attribute again serves as a kind of normalization and therefore RELIEF's estimates do not exhibit such strange behavior as gini-index gain does. The above derivation eliminated the "nearest instance" condition from the probabilities. <p> With increasing the number (k) of nearest hits/misses the correlation of RELIEFF's estimates with other impurity functions also increases unless k is greater than the number of instances in the same peak of the instance space. The study reported in <ref> [14] </ref> showed that RELIEFF has an acceptable bias with respect to other measures when estimating attributes with different number of values. The myopia of current inductive learning systems can be partially overcome by replacing the existing heuristic functions with RELIEFF.
Reference: 15. <author> I. Kononenko and I. Bratko. </author> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80, </pages> <year> 1991. </year>
Reference-contexts: The exception from the above methodology were the experiments in the finite element mesh design problem, where the experimental methodology was dictated by previous published results, as described in Section 5.4. Besides the classification accuracy, we measured also the average information score <ref> [15] </ref>. This measure eliminates the influence of prior probabilities and appropriately treats probabilistic answers of the classifier.
Reference: 16. <author> R.L. Mantaras. </author> <title> ID3 Revisited: A distance based criterion for attribute selection. </title> <booktitle> Proc. Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Car-olina, U.S.A., </address> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: The heuristic function that estimates the potential successors of the current state in the search space has a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio [25], gini-index [1], distance measure <ref> [16] </ref>, j-measure [30], and MDL [14]. However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. <p> Impurity functions tend to overestimate multi-valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio [25], distance measure <ref> [16] </ref>, and bina-rization of attributes [4]). Equation (3) shows that RELIEF exhibits an implicit normalization effect. Another deficiency of gini-index gain is that its values tend to decrease with the increasing number of classes [14]. <p> 1 0 1 0 1 0 0 0 0 1 1 1 0 RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance <ref> [16] </ref> 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. <p> -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio [25] 0.000 0.000 0.051 distance <ref> [16] </ref> 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. Hong [8] developed a procedure similar to RELIEF for estimating the quality of attributes, where he directly emphasizes the use of contextual information.
Reference: 17. <author> R.S. Michalski and R.L. Chilausky. </author> <title> Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in the context of developing an expert system for soybean disease diagnosis. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4 </volume> <pages> 125-161, </pages> <year> 1980. </year>
Reference-contexts: Non-medical real-world data sets We compared the performance of the algorithms also on the following non-medical real world data sets (SOYB, IRIS, and VOTE are obtained from the Irvine database [21], SAT is obtained from the StatLog database [18]): SOYB: The famous soybean data set used by Michalski & Chilausky <ref> [17] </ref>. IRIS: The well known Fisher's problem of deter mining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem [6].
Reference: 18. <editor> D. Michie, D.J. Spiegelhalter, and C.C. Taylor (eds.). </editor> <title> Machine learning, neural and statistical classification. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1994. </year>
Reference-contexts: The data was provided by Gail Gong from Carnegie-Mellon University. * Data sets obtained from the StatLog database <ref> [18] </ref>: diagnosis of diabetes (DIAB) and diagnosis of heart diseases (HEART). For the DIAB data set, Ragavan & Rendell [27]report 78.8% classification accuracy with their LFC algorithm. <p> However, our results (see below) and results of the StatLog project <ref> [18] </ref> show that the poor results of the other algorithms in this domain are not due to the lack of constructive induction. In our experiments, on DIAB dataset, all classifiers perform equally well, with the exception of the naive Bayesian classifier which is significantly better. <p> Non-medical real-world data sets We compared the performance of the algorithms also on the following non-medical real world data sets (SOYB, IRIS, and VOTE are obtained from the Irvine database [21], SAT is obtained from the StatLog database <ref> [18] </ref>): SOYB: The famous soybean data set used by Michalski & Chilausky [17]. IRIS: The well known Fisher's problem of deter mining the type of iris flower. <p> SAT: The database consists of multi-class spectral values of pixels in 3 fi 3 neighborhoods in a satellite image, and the classification of the central pixel in each neighborhood. The results of the StatLog project <ref> [18] </ref> are 90.6% classification accuracy for the k-NN algorithm, 86.1% for backpropagation, 85.0% for C4.5, 84.8% for CN2 and 69.3% for the naive Bayesian classifier (using relative frequencies and not the m estimate of probabilities). VOTE: The voting records are from a session of the 1984 United States Congress. <p> The results of the naive Bayesian classifier indicate that the attributes are conditionally relatively independent in these data sets, which is in agreement with previously published results. On the SAT data set, k-NN significantly outperforms other algorithms which is in agreement with the results of the StatLog project <ref> [18] </ref>. However, the naive Bayesian classifier with the m-estimate of probabilities reaches the classification accuracy of inductive learning algorithms.
Reference: 19. <author> D. Mladenic. </author> <title> Combinatorial optimization in inductive concept learning. </title> <booktitle> Proc. 10th Intern. Conf. on Machine Learning. </booktitle> <address> (Amherst, June 1993), </address> <publisher> Morgan Kauf-mann, </publisher> <pages> pp. 205-211, </pages> <year> 1993. </year>
Reference-contexts: The reported classification accuracy is 99.70.1 %. We used only one set of 1000 examples (i.e. 700 instances for training). KRK2: Same as KRK1 except that the only available attributes are the coordinates of pieces. The same data set was used by Mladenic <ref> [19] </ref>. The reported results are about 69% accuracy for her ATRIS system and 64% for Assistant. The basic description of data sets is provided in Table 5 and results are given in Tables 6 and 7.
Reference: 20. <author> S. Muggleton (ed.). </author> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: In such domains, the naive Bayesian classifier is able to reliably estimate the conditional probabilities and is also able to use all attributes, i.e all available information. It would be interesting to appropriately combine the power of RELIEFF and the naive Bayesian classifier. Current ILP systems <ref> [20] </ref> are not able to use the attributes appropriately. This was demonstrated in the MESH3 domain where all attribute learn 16 THE AUTHORS??? ers outperformed existing ILP systems. To enable ILP systems to deal with the attribute-value representation, a combination with the (semi) naive Bayesian classifier could be useful.
Reference: 21. <author> P.M. Murphy and D.W. Aha. </author> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science, </institution> <year> 1991. </year>
Reference-contexts: The optimal recognition rate is estimated to be 74%. Smyth et al. [31] report 68.11.7% of the classification accu racy for naive Bayes, 64.63.5 for backpropagation, and 72.71.3 for their rule-based classifier. This data set can be obtained from Irvine database <ref> [21] </ref>. KRK1: The problem of legality of King-Rook-King chess endgame positions. The attributes describe the relevant relations between pieces, such as "same rank" and "adjacent file". <p> Non-medical real-world data sets We compared the performance of the algorithms also on the following non-medical real world data sets (SOYB, IRIS, and VOTE are obtained from the Irvine database <ref> [21] </ref>, SAT is obtained from the StatLog database [18]): SOYB: The famous soybean data set used by Michalski & Chilausky [17]. IRIS: The well known Fisher's problem of deter mining the type of iris flower.
Reference: 22. <author> T. Niblett and I. Bratko. </author> <title> Learning decision rules in noisy domains. </title> <booktitle> Proc. Expert Systems 86, </booktitle> <address> Brighton, UK, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: For preprun-ing, three user-defined thresholds are provided: minimal number of training instances, minimal attributes information gain and maximal probability of majority class in the current node. For postpruning, the method developed by Niblett and Bratko <ref> [22] </ref> is used that uses Laplace's law of succession for estimating the expected classification error of the current node commited by pruning/not pruning its subtree.
Reference: 23. <author> U. Pompe, M. Kovacic, and I. Kononenko. SFOIL: </author> <title> Stochastic approach to inductive logic programming. </title> <booktitle> Proc. Slovenian Conf. on Electrical Engineering and Computer Science. Portoroz, Slovenia, </booktitle> , <pages> pp. </pages> <month> 189-192 Sept. </month> <year> 1993. </year>
Reference-contexts: The results reported by Dzeroski [7] for various ILP systems are 12% classification accuracy for FOIL, 22% for mFOIL and 29% for GOLEM and the result reported by Pompe et al. <ref> [23] </ref> is 28% for SFOIL. The description of the MESH problem is appropriate for ILP systems. For attribute learners only relations with arity 1 (i.e. attributes) can be used to describe the problem. Note that in this domain the training/testing splits are the same for all algorithms. <p> The most interesting results appear in the MESH domains. Although attribute learners in MESH3 have less information than ILP systems, they all outperform the results by ILP systems as reported by Dzeroski [7] and Pompe et al. <ref> [23] </ref>. With 12 additional attributes in MESH15, the results of inductive learners are significantly improved. All inductive learning systems significantly outperform the naive Bayesian classifier and the k-NN algorithm.
Reference: 24. <author> U. Pompe and I. Kononenko. </author> <title> Linear space induction in first order logic with RELIEFF, </title> <editor> In: G.Della Riccia, R.Kruse, R.Viertl, (eds.). </editor> <booktitle> Mathematical and statistical methods in artificial intelligence. CISM Lecture Notes, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: To enable ILP systems to deal with the attribute-value representation, a combination with the (semi) naive Bayesian classifier could be useful. On the other hand, current ILP systems use greedy search techniques and the heuristics that guide the search are myopic. Pompe and Kononenko <ref> [24] </ref> implemented an adapted version of RELIEFF in the FOIL like ILP system called ILP-R and prelemi-nary experiments show similar advantages of this system over other ILP systems as Assistant-R has over Assistant-I. 7.
Reference: 25. <author> R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: The heuristic function that estimates the potential successors of the current state in the search space has a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio <ref> [25] </ref>, gini-index [1], distance measure [16], j-measure [30], and MDL [14]. However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis. <p> Impurity functions tend to overestimate multi-valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio <ref> [25] </ref>, distance measure [16], and bina-rization of attributes [4]). Equation (3) shows that RELIEF exhibits an implicit normalization effect. Another deficiency of gini-index gain is that its values tend to decrease with the increasing number of classes [14]. <p> A3 Class 1 0 1 1 0 1 0 1 0 0 0 0 1 1 1 0 RELIEF = W [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio <ref> [25] </ref> 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. <p> [A] (1) 0.542 0.458 -0.750 W'[A] (3) 0.000 0.000 0.063 Ginigain' (4) 0.000 0.000 0.029 gini index gain [1] 0.000 0.000 0.033 information gain [9] 0.000 0.000 0.049 gain-ratio <ref> [25] </ref> 0.000 0.000 0.051 distance [16] 0.000 0.000 0.026 Ginigain' (equation (4)), original gini-index gain [1], information gain [9], gain ratio [25], and distance measure [16] estimate that the contribution of A3 is the highest while attributes A1 and A2 are estimated as completely irrelevant. Hong [8] developed a procedure similar to RELIEF for estimating the quality of attributes, where he directly emphasizes the use of contextual information. <p> Assistant-R Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees [4]. The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. [9] and reimplemented by several authors (see <ref> [25] </ref> for an overview). In the following we describe the main features of Assistant. Binarization of attributes: The algorithm generates binary decision trees. At each decision step the binarized version of each attribute is selected that maximizes the information gain of the attribute.
Reference: 26. <author> R. Quinlan. </author> <title> The minimum description length principle and categorical theories. </title> <booktitle> Proc. 11th Int. Conf. on Machine Learning, </booktitle> <address> (Ruthers University, New Brunswick, </address> <month> July </month> <year> 1994), </year> <editor> Cohen W & Hirsh H. (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 233-241, </pages> <year> 1994. </year>
Reference-contexts: Note that in this domain the training/testing splits are the same for all algorithms. The testing methodology is a special case of leave-one-out, therefore, the results in the tables for this problem have no standard deviations. Quinlan <ref> [26] </ref> reports results of some ILP systems that achieved over 90% in that domain testing on positive and negative instances. However, those results are misleading. Each positive instance has ten negative instances in average.
Reference: 27. <author> H. Ragavan and L. Rendell. </author> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> Proc. 10th Intern. Conf. on Machine Learning. </booktitle> <address> (Amherst, June 1993), </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 252-259, </pages> <year> 1993. </year>
Reference-contexts: The following approaches are compared: * the use of information gain as a selection crite rion; * LFC <ref> [27] </ref>, [28] that tries to overcome the myopia of information gain with a limited lookahead; * the naive Bayesian classifier, that assumes con ditional independence of attributes; * the k-nearest neighbors algorithm. The paper is organized as follows. <p> However, the other differences to Assistant remain (m-estimate of probabilities). This algorithm enables us to evaluate the contribution of RE-LIEFF. The parameters for Assistant-I and Assistant-R were fixed throughout the experiments (no prepruning, postpruning with m = 2). LFC: Ragavan et al. <ref> [27] </ref>, [28] use limited looka-head in their LFC (Lookahead Feature Construction) algorithm for top down induction of decision trees to detect significant conditional dependencies between attributes for constructive induction. They show interesting results on some data sets. We reimplemented their algorithm [29] and tested its performance.
Reference: 28. <author> H. Ragavan, L. Rendell, M. Shaw, and A. Tessmer. </author> <title> Learning complex real-world concepts through feature construction. </title> <type> Technical Report UIUC-BI-AI-93-03. </type> <institution> The Beckman Institute, University of Illinois, </institution> <year> 1993. </year>
Reference-contexts: The following approaches are compared: * the use of information gain as a selection crite rion; * LFC [27], <ref> [28] </ref> that tries to overcome the myopia of information gain with a limited lookahead; * the naive Bayesian classifier, that assumes con ditional independence of attributes; * the k-nearest neighbors algorithm. The paper is organized as follows. <p> However, the other differences to Assistant remain (m-estimate of probabilities). This algorithm enables us to evaluate the contribution of RE-LIEFF. The parameters for Assistant-I and Assistant-R were fixed throughout the experiments (no prepruning, postpruning with m = 2). LFC: Ragavan et al. [27], <ref> [28] </ref> use limited looka-head in their LFC (Lookahead Feature Construction) algorithm for top down induction of decision trees to detect significant conditional dependencies between attributes for constructive induction. They show interesting results on some data sets. We reimplemented their algorithm [29] and tested its performance.
Reference: 29. <author> M. Robnik. </author> <title> Constructive induction with decision trees. </title> <booktitle> B.Sc. Thesis (in Slovene), </booktitle> <institution> University of Ljubl-jana, Faculty of electrical engineering & computer science, Ljubljana, Slovenia, </institution> <year> 1993. </year>
Reference-contexts: LFC: Ragavan et al. [27], [28] use limited looka-head in their LFC (Lookahead Feature Construction) algorithm for top down induction of decision trees to detect significant conditional dependencies between attributes for constructive induction. They show interesting results on some data sets. We reimplemented their algorithm <ref> [29] </ref> and tested its performance. Our results, presented in this paper, show some drawbacks of the experimental comparison described by Ragavan and Rendell and confirm the advantage of the limited lookahead for constructive induction. LFC generates binary decision trees.
Reference: 30. <author> P. Smyth and R.M. Goodman. </author> <title> Rule induction using information theory. </title> <editor> In. G.Piatetsky-Shapiro and W.Frawley (eds.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: The heuristic function that estimates the potential successors of the current state in the search space has a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio [25], gini-index [1], distance measure [16], j-measure <ref> [30] </ref>, and MDL [14]. However, all these measures assume that attributes are conditionally independent given the class and therefore in domains with strong conditional dependencies between attributes the greedy search has poor chances of revealing a good hypothesis.
Reference: 31. <author> P. Smyth, R.M. Goodman R.M., and C. Higgins. </author> <title> A hybrid Rule-based Bayesian Classifier. </title> <booktitle> Proc.European Conf. on Artificial Intelligence, </booktitle> <address> Stockholm, </address> <month> August, </month> <year> 1990, </year> <pages> pp. 610-615, </pages> <year> 1990. </year>
Reference-contexts: set was used by Smyth et al. <ref> [31] </ref>and they report 67.21.7% of the classification accuracy for naive Bayes, 82.51.1% for backpropagation, and 85.90.9% for their rule based classifier. LED: LED-digits problem with 10% of noise in attribute values. The optimal recognition rate is estimated to be 74%. Smyth et al. [31] report 68.11.7% of the classification accu racy for naive Bayes, 64.63.5 for backpropagation, and 72.71.3 for their rule-based classifier. This data set can be obtained from Irvine database [21]. KRK1: The problem of legality of King-Rook-King chess endgame positions. <p> VOTE: The voting records are from a session of the 1984 United States Congress. Smyth et al. <ref> [31] </ref> report 88.9% of classification accuracy for the naive Bayesian classifier, 93.0% for backpropagation and 94.9% for their rule-based classifier. The basic characteristics of non-medical real world data sets are presented in Table 11. Tables 12 and 13 give the results. <p> This result confirms that RELIEFF estimates the quality of attributes better than the information gain. On the VOTE data set the naive Bayesian classifier is the worst, while both versions of Assistant are comparable to the rule based classifier by Smyth et al. <ref> [31] </ref>. The most interesting results appear in the MESH domains. Although attribute learners in MESH3 have less information than ILP systems, they all outperform the results by ILP systems as reported by Dzeroski [7] and Pompe et al. [23].
References-found: 31

