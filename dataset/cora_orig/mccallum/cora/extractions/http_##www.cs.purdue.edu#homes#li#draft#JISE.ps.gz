URL: http://www.cs.purdue.edu/homes/li/draft/JISE.ps.gz
Refering-URL: http://www.cs.purdue.edu/homes/li/publications.html
Root-URL: http://www.cs.purdue.edu
Title: Integrating Parallelizing Compilation Technology and Processor Architecture for Cost-Effective Concurrent Multithreading  
Author: Jenn-Yuan Tsai Zhenzhen Jiang Zhiyuan Li flfl David J. Lilja Xin Wang Pen-Chung Yew Bixia Zheng Stephen J. Schwinn 
Date: November 26, 1997  
Affiliation: Department of Computer Science and Engineering Department of Electrical and Computer Engineering University of Minnesota flfl Department of Computer Science Purdue University  
Abstract: As the number of transistors on a single chip continues to grow, it is important to think beyond the traditional approaches of compiler optimizations for deeper pipelines and wider instruction issue units to improve performance. This single-threaded execution model limits these approaches to exploiting only the relatively small amount of instruction-level parallelism available in application programs. While integrating an entire multiprocessor onto a single chip is feasible, this architecture is limited to exploiting only relatively coarse-grained parallelism. We propose a concurrent multi-threaded architecture, called the superthreaded architecture, as an alternative. As a hybrid of a wide-issue superscalar processor and a multiprocessor-on-a-chip, this new concurrent multithread-ing architecture can leverage the best of existing and future parallel hardware and compilation technologies. By combining compiler-directed thread-level speculation for control and data dependences with run-time checking of data dependences, the superthreaded architecture can exploit the multiple granularities of parallelism available in general-purpose application programs to reduce the execution time of a single program. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Randy Allen and Steve Johnson. </author> <title> Compiling C for vectorization, parallelization, and inline expansion. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 241-249, </pages> <month> June 22-24, </month> <year> 1988. </year>
Reference-contexts: These conversions allow many traditional parallelization and analysis techniques to be applied directly to these loops. These conversions also introduce many new compilation challenges, such as the need for more advanced induction variable recognition and elimination techniques, more detailed 8 flow analysis, and so on <ref> [1] </ref>. Program transformation techniques that can improve the overlap between loop iterations can be applied here with additional consideration for thread pipelining. Knowing the extent of thread overlap could help determine whether a loop should be speculated.
Reference: [2] <author> Michael Butler, Tse-Yu Yeh, Yale Patt, Mitch Alsup, Hunter Scales, and Michael Shebanow. </author> <title> Single instruction stream parallelism is greater than two. </title> <booktitle> In Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 276-286, </pages> <month> May 27-30, </month> <year> 1991. </year>
Reference-contexts: However, it has been shown to be very difficult and expensive to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. <p> to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. Hence, even without considering data dependences within a basic block, a machine capable of issuing 10 instructions per machine cycle would issue all of the instructions in a typical basic block in only 1 or 2 cycles.
Reference: [3] <author> Pradeep K. Dubey, Kevin O'Brien, Kathryn O'Brien, and Charles Barton. </author> <title> Single-program speculative multithreading (SPSM) architecture: Compiler-assisted fine-grained multithread-ing. </title> <booktitle> In Proceedings of the IFIP WG 10.3 Working Conference on Parallel Architectures and Compilation Techniques, PACT '95, </booktitle> <pages> pages 109-121, </pages> <month> June 27-29, </month> <year> 1995. </year>
Reference-contexts: To increase the instruction issue rate in a "single-processor" architecture, several other multi-threaded architectures have also been proposed, including the XIMD [15], the Elementary Mul-tithreading architecture [5], the M-machine [4], the Simultaneous Multithreading architecture [13], the Multiscalar [11], and the SPSM <ref> [3] </ref>. The key features of the different concurrent multithreaded architectures are compared in Figure 7.
Reference: [4] <author> Marco Fillo, Stephen W. Keckler, Dally William J, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146-156, </pages> <month> November 29-December 1, </month> <year> 1995. </year>
Reference-contexts: To increase the instruction issue rate in a "single-processor" architecture, several other multi-threaded architectures have also been proposed, including the XIMD [15], the Elementary Mul-tithreading architecture [5], the M-machine <ref> [4] </ref>, the Simultaneous Multithreading architecture [13], the Multiscalar [11], and the SPSM [3]. The key features of the different concurrent multithreaded architectures are compared in Figure 7.
Reference: [5] <author> Hiroaki Hirata, Kozo Kimura, Satoshi Nagamine, Yoshiyuki Mochizuki, Akio Nishimura, Yoshimori Nakase, and Teiji Nishizawa. </author> <title> An elementary processor architecture with simultaneous instruction issuing from multiple threads. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 136-145, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: To increase the instruction issue rate in a "single-processor" architecture, several other multi-threaded architectures have also been proposed, including the XIMD [15], the Elementary Mul-tithreading architecture <ref> [5] </ref>, the M-machine [4], the Simultaneous Multithreading architecture [13], the Multiscalar [11], and the SPSM [3]. The key features of the different concurrent multithreaded architectures are compared in Figure 7.
Reference: [6] <author> Norman P. Jouppi and David W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 272-282, </pages> <month> April 3-6, </month> <year> 1989. </year>
Reference-contexts: However, it has been shown to be very difficult and expensive to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. <p> to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. Hence, even without considering data dependences within a basic block, a machine capable of issuing 10 instructions per machine cycle would issue all of the instructions in a typical basic block in only 1 or 2 cycles.
Reference: [7] <author> Monica Lam. </author> <title> Software pipelining: An effective scheduling technique for VLIW machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <month> June 22-24, </month> <year> 1988. </year>
Reference-contexts: However, the accuracy of branch prediction drops quickly after crossing only a few branch instructions, and the profiling information that is often used to select the most frequently executed paths may be input sensitive. Software pipelining <ref> [7] </ref> currently is a common technique used to exploit parallelism in loops. In theory, similar techniques could be applied to outer loops as well, but most likely with a substantial loss of efficiency. As a result, most existing software pipelining techniques typically deal only with innermost loops.
Reference: [8] <author> Monica S. Lam and Robert P. Wilson. </author> <title> Limits of control flow on parallelism. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 46-57, </pages> <month> May 19-21, </month> <year> 1992. </year>
Reference-contexts: However, it has been shown to be very difficult and expensive to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. <p> to extract enough parallelism with a single thread of 2 control to efficiently utilize even a small number of functional units <ref> [2, 6, 8] </ref>. To make matters worse, in most general-purpose applications, there are typically only around 5-10 instructions between two branches, which is the size of a basic block [2, 6, 8]. Hence, even without considering data dependences within a basic block, a machine capable of issuing 10 instructions per machine cycle would issue all of the instructions in a typical basic block in only 1 or 2 cycles.
Reference: [9] <author> Zhiyuan Li, Jenn-Yuan Tsai, Xin Wang, Pen-Chung Yew, and Bixia Zheng. </author> <title> Compiler techniques for concurrent multithreading with hardware speculation support. </title> <booktitle> In Proceedings of the 9th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August 8-10, </month> <year> 1996. </year>
Reference-contexts: Some simple thread synchronization instructions are also provided. 3 Compilation Issues for Concurrent Multithreaded Architectures The superthreaded architecture requires more compiler support than is typically provided by traditional front-end parallelizing compilers for multiprocessors since the superthreaded architecture exploits both loop-level and instruction-level parallelism <ref> [9] </ref>. Existing parallelizing compilers are designed primarily to exploit loop-level parallelism only, and are most appropriate for scientific programs written in Fortran. They usually produce annotated source programs and rely on back-end compilers to perform instruction level optimizations. Currently, these two processes are rarely integrated.
Reference: [10] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural 14 Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October 12-15, </month> <year> 1992. </year>
Reference-contexts: Furthermore, because there will be no need for cache coherence enforcement, there will be no false-sharing problems. In fact, sharing of the cache lines among threads should be encouraged to improve locality. Prefetching one iteration ahead <ref> [10] </ref> will not work properly since several iterations will be active concurrently. Also, because of the shared cache memory, a cache block prefetched by one thread can be used by all other threads as well.
Reference: [11] <author> Gurindar S. Sohi, Scott E. Breach, and T. N. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 414-425, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: To increase the instruction issue rate in a "single-processor" architecture, several other multi-threaded architectures have also been proposed, including the XIMD [15], the Elementary Mul-tithreading architecture [5], the M-machine [4], the Simultaneous Multithreading architecture [13], the Multiscalar <ref> [11] </ref>, and the SPSM [3]. The key features of the different concurrent multithreaded architectures are compared in Figure 7.
Reference: [12] <author> Jenn-Yuan Tsai and Pen-Chung Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In Proceedings of International Conference on Parallel Architectures and Compilation Techniques, PACT '96, </booktitle> <pages> pages 35-46, </pages> <month> October 20-23, </month> <year> 1996. </year>
Reference-contexts: These concurrent multithreaded architectures (CMAs) are similar to having very tightly-coupled multiprocessors on a single chip, yet with very different design goals, architectural supports, and compilation techniques. We propose a new concurrent multithreading architecture, called the su-perthreaded architecture <ref> [12] </ref>, that is a hybrid of current superscalar processors and a multiprocessor-on-a-chip. <p> Characteristics (3) and (4) further suggest that completely new compiler techniques and architectural supports are necessary to develop a reasonable and sensible approach for extracting parallelism from these irregular applications. 2 The Superthreaded Architecture The superthreaded architecture <ref> [12] </ref> addresses these challenges in parallelizing general-purpose applications by tightly integrating a parallelizing compiler with a concurrent multithreaded architecture. This tight integration allows the superthreaded architecture to exploit both instruction- and thread-level parallelism using multiple threads of control. <p> if ( minclk &gt; funct_units [i].busy ) - minclk = funct_units [i].busy; j = i; if ( minclk == 0 ) break; - i++; 4 Preliminary Performance Evaluation We have developed a simulator for a superthreaded processor to make some preliminary comparisons of its performance to a traditional single-threaded processor <ref> [12] </ref>. The superthreaded processor consisted of either 4 or 8 thread processing units where each unit can issue a single instruction per cycle, has in-order instruction execution, can forward one target store entry per cycle, and assumes a one-cycle delay for all loads and branches.
Reference: [13] <author> Dean M. Tullsen, Susan J. Eggers, and Henry M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 392-403, </pages> <month> June 22-24, </month> <year> 1995. </year>
Reference-contexts: To increase the instruction issue rate in a "single-processor" architecture, several other multi-threaded architectures have also been proposed, including the XIMD [15], the Elementary Mul-tithreading architecture [5], the M-machine [4], the Simultaneous Multithreading architecture <ref> [13] </ref>, the Multiscalar [11], and the SPSM [3]. The key features of the different concurrent multithreaded architectures are compared in Figure 7.
Reference: [14] <author> Steven VanderWiel and David J. Lilja. </author> <title> When caches are not enough: Data prefetching techniques. </title> <journal> In IEEE Computer, </journal> <volume> volume 30, </volume> <pages> pages 23-30, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: analysis and more flexible code optimizations. 3.4 Memory Latency and Locality Issues Since all of the threads in a superthreaded processor will access the same data and instruction caches, traditional memory optimizations, such as locality enhancement through loop tiling and loop unrolling, and latency hiding strategies, such as data prefetching <ref> [14] </ref>, can be applied. However, these compiler optimization techniques must consider the unique characteristics of the su-perthreaded architecture. For example, with multiple threads sharing the same cache memory, 9 there may be more conflict misses requiring a more careful data layout by the compiler.

References-found: 14

