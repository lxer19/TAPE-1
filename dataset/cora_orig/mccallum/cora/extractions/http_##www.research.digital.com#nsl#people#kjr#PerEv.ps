URL: http://www.research.digital.com/nsl/people/kjr/PerEv.ps
Refering-URL: http://www.research.digital.com/nsl/people/kjr/bio.html
Root-URL: http://www.research.digital.com
Email: faward,glynng@leland.stanford.edu kjr@pa.dec.com  
Title: Internet Service Performance Failure Detection  
Author: Amy Ward Peter Glynn Kathy Richardson 
Address: Stanford, CA 94305 Palo Alto, CA 94301  
Affiliation: Engineering Economic Systems Western Research Labs Operations Research Department Stanford University Digital Equipment Corporation  
Abstract: The increasing complexity of computer networks and our increasing dependence on them means enforcing reliability requirements is both more challenging and more critical. The expansion of network services to include both traditional interconnect services and user-oriented services such as the web and email has guaranteed both the increased complexity of networks and the increased importance of their performance. The first step toward increasing reliability is early detection of network performance failures. Here we consider the applicability of statistical model frameworks under the most general assumptions possible. Using measurements from corporate proxy servers, we test the framework against real world failures. The results of these experiments show we can detect failures, but with some tradeoff questions. The pull is in the warning time: either we miss early warning signs or we report some false warnings. Finally, we offer insight into the problem of failure diagnosis. 
Abstract-found: 1
Intro-found: 1
Reference: [Bil68] <author> Patrick Billingsley. </author> <title> Convergence of Probability Measures. </title> <publisher> Wiley, </publisher> <year> 1968. </year>
Reference-contexts: However, in the case that the measurements record N (t), the total number of counts up to time t, and the arrival distribution for these counts are independent and identically distributed, Donsker's Theorem for renewal processes <ref> [Bil68] </ref> states that for large enough time scales, N (t) is approximately normal. In our case, N (t) counts the number of processed connection requests during a 15 minute time interval.
Reference: [Gla94] <author> Steve Glassman. </author> <title> A caching relay for the world wide web. </title> <booktitle> In Proceedings of the 1st International Word Wide Web Conference, </booktitle> <pages> pages 69-76. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: We hypothesize the cause to be request service time. Overall network performance has a large effect on request service time and remains relatively stable across short time periods. Request service times then affect request rates <ref> [Gla94] </ref>, accounting for the data correlations. Usually, the faster requests are serviced, the faster users make them. For example, consider browsing web pages. The more quickly these pages are requested depends directly upon how quickly the pages are received after a request is made.
Reference: [HJ94] <author> C.S. Hood and C. Ji. </author> <title> Automated proactive anomaly detection. </title> <booktitle> In Proceedings of the 5th IFIP/IEEE International Symposium on Integrated Network Management, pages Session 15 Fault Management II. IFIP and IEEE ComSoc, IEEE, </booktitle> <year> 1994. </year>
Reference-contexts: Part of the difficulty inherent in failure detection is the definition of a performance failure. In order that Quality of Service requirements can be placed on network applications, we must have a reliable mechanism for fault management <ref> [HJ94] </ref>. Here we define a failure as a user-problematic departure from expected operating conditions [MO90]. In our environment, we use the number of requests processed at our proxy servers as a mechanism for tracking performance.
Reference: [Max90] <author> Roy A. Maxion. </author> <title> Anomaly detection for network diagnosis. </title> <booktitle> In 20th International Symposium on Fault Tolerant Computing. IEEE Computer Society, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: However, for a mature network, it is reasonable to assume that for a limited time span we can model regular behavior patterns and use this model to spot deviant behaviors as an indication of performance failures <ref> [Max90] </ref>. Here we present methodology for identifying and tracking a process that reflects network performance and give results for the process we tracked on our network.
Reference: [MO90] <author> Roy A. Maxion and Robert T. Olszewski. </author> <title> Detection and discrimination of injected network faults. </title> <booktitle> In 23th International Symposium on Fault Tolerant Computing. IEEE Computer Society, IEEE, </booktitle> <year> 1990. </year>
Reference-contexts: In order that Quality of Service requirements can be placed on network applications, we must have a reliable mechanism for fault management [HJ94]. Here we define a failure as a user-problematic departure from expected operating conditions <ref> [MO90] </ref>. In our environment, we use the number of requests processed at our proxy servers as a mechanism for tracking performance. Noticeable deviations in the number of requests processed for a specific time interval often indicate undesirable operating conditions.
Reference: [Pet77] <author> A.N. Pettitt. </author> <title> Testing the normality of several independent samples using the anderson-darling statistic. </title> <journal> Applied Statistics, </journal> <volume> 26(2) </volume> <pages> 156-161, </pages> <year> 1977. </year>
Reference-contexts: Rather we observe that it is likely our observations could be normally distributed and perform statistical tests to confirm this supposition. The statistical test we performed was the test developed by Anderson and Darling <ref> [Pet77] </ref>, and the test confirms the normality assumption. The mean and variance of the process at each time period can now be used to identify deviant observations; i.e., observations from the tail of the distribution.
Reference: [TMW97] <author> Kevin Thompson, Gregory J. Miller, and Rick Wilder. </author> <title> Wide-area internet traffic patterns and characteristics. </title> <journal> IEEE Network, </journal> <month> November/December </month> <year> 1997. </year>
Reference-contexts: These patterns are general network traffic patterns and the heuristics are supported by measurements made by the vBNS engineering department at MCI within Internet MCI's backbone <ref> [TMW97] </ref>. Abnormalities in this pattern usually correspond to some type of failure. For example, a sudden drop in the number of connection requests is cause for alarm.
References-found: 7

