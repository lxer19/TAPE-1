URL: http://www.cs.berkeley.edu/~russell/papers/aaai94-mdp.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Email: tash@math.berkeley.edu  russell@cs.berkeley.edu  
Title: Control Strategies for a Stochastic Planner  
Author: Jonathan Tash Stuart Russell 
Address: Berkeley, CA 94720  Berkeley, CA 94720  
Affiliation: Group in Logic and the Methodology of Science University of California  Computer Science Division University of California  
Abstract: We present new algorithms for local planning over Markov decision processes. The base-level algorithm possesses several interesting features for control of computation, based on selecting computations according to their expected benefit to decision quality. The algorithms are shown to expand the agent's knowledge where the world warrants it, with appropriate responsiveness to time pressure and randomness. We then develop an introspective algorithm, using an internal representation of what computational work has already been done. This strategy extends the agent's knowledge base where warranted by the agent's world model and the agent's knowledge of the work already put into various parts of this model. It also enables the agent to act so as to take advantage of the computational savings inherent in staying in known parts of the state space. The control flexibility provided by this strategy, by incorporating natural problem-solving methods, directs computational effort towards where it's needed better than previous approaches, providing greater hopes for scalability to large domains. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Baum, E. B. </author> <year> 1992. </year> <title> On optimal game tree propagation for imperfect players. </title> <booktitle> In AAAI-92, </booktitle> <pages> 507-512. </pages>
Reference-contexts: This captures an important distinction, one used in <ref> (Baum 1992) </ref> to improve game playing performance. (Of course, over repeated trials, these distances will converge.) This change in action choice criteria should be reflected in the criteria used for choosing computations as well.
Reference: <author> Dean, T., and Boddy, M. </author> <year> 1988. </year> <title> An analysis of time-dependent planning. </title> <booktitle> In AAAI-88, </booktitle> <pages> 49-54. </pages>
Reference-contexts: The methodology also provides for concise storage (in the form of two heuristic values for each state) of the results of previous planning efforts for use in future encounters with the same state space region, and the planner has the anytime property of <ref> (Dean & Boddy 1988) </ref> in that its policy improves as a function of the time it has had to compute.
Reference: <author> Dean, T.; Kaelbling, L. P.; Kirman, J.; and Nicholson, A. </author> <year> 1993. </year> <title> Deliberation scheduling for time-critical sequential decision making. </title> <booktitle> In UAI-93, </booktitle> <pages> 309-316. </pages>
Reference-contexts: This enrichment requires modification of the classical methods of operations research when faced with large domains. The next section elaborates a planning system which accounts for the expense of computing by making tradeoffs between computation time and policy quality. The system uses local computation techniques resembling ones presented in <ref> (Dean et al. 1993) </ref> and (Thiebaux et al. 1994), but allows for more extensive use of heuristic knowledge and is amenable to a proof of convergence to optimality. <p> This strategy is similar to (but was developed independently of) those described in the papers of <ref> (Dean et al. 1993) </ref> and (Thiebaux et al. 1994); however, its use of estimated values on non-envelope states provides important additional information for determining envelope action choices, allowing for meaningful action choices to be made before a path to the goal is found, and summarizing previous computational effort outside the current <p> Both <ref> (Dean et al. 1993) </ref> and (Thiebaux et al. 1994) therefore choose the fringe state most likely to be reached first from the current state as the best candidate for further envelope expansion. <p> The definition of G fx could also be generalized by integrating over a distribution for f instead of using a point estimate.) We thus have an analytic strategy for choosing envelope expansions (unlike the trial and error learning approach of <ref> (Dean et al. 1993) </ref>), and expand the envelope according to impact on action choice quality. <p> The method bears a general resemblance to other localized extensions of dynamic programming, such as Incremental Dynamic Programming (Sutton 1991) and the work of <ref> (Dean et al. 1993) </ref> and (Thiebaux et al. 1994), but adds a new analysis of the factors appropriate for envelope determination: computational effort is put where it will make the most difference to quality of action choice, and is expended only so long as it will make a significant difference to
Reference: <author> Howard, R. A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference: <author> Koenig, S. </author> <year> 1992. </year> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Technical Report UCB/CSD 92/685, </type> <institution> UC Berkeley, Calif. </institution>
Reference-contexts: This paper presents a planning methodology suitable for domains representable as a Markov decision process (as in <ref> (Koenig 1992) </ref>), and shows how such a methodology can incorporate several natural control strategies. In this model, the problem-solving agent is in a world consisting of a finite, discrete set of states, and can identify the current one.
Reference: <author> Russell, S., and Wefald, E. </author> <year> 1991. </year> <title> Do the Right Thing. </title> <address> Cambridge, Mass: </address> <publisher> MIT Press. </publisher>
Reference-contexts: This leads to consideration of the potential utility gain afforded by further computation, which is measured by the amount one expects one's final action choice to be better than one's current choice (as elucidated in <ref> (Russell & Wefald 1991) </ref>). <p> Introspective planning techniques are an extension of common metalevel control techniques (discussed in, for example, <ref> (Russell & Wefald 1991) </ref>). Computations such as those performed by the base-level system described above are controlled somewhat like physical actions by a higher level controller. This architecture can be iterated, getting higher level controllers allowing for deeper forms of introspection.
Reference: <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to predict by the methods of temporal differences. </title> <booktitle> Machine Learning 3 </booktitle> <pages> 9-43. </pages>
Reference-contexts: All runs were conducted with random initial states of distance about 10 from the goal. An initial set of 80 runs was conducted to learn good heuristics near the goal and to estimate the numerical coefficients (using the temporal dif ference learning methods of <ref> (Sutton 1988) </ref>) in the equations for variance and total value: f = 2 (10k + 1) T (k; p) = p 3k (8) (The time cost term of T (k; p) was assumed to grow linearly with p at k = 0, and @T =@kj k=0 = 1 so computational time
Reference: <author> Sutton, R. S. </author> <year> 1991. </year> <title> Planning by incremental dynamic programming. </title> <booktitle> In 8th Intl Wkshp on Machine Learning. </booktitle>
Reference-contexts: The method bears a general resemblance to other localized extensions of dynamic programming, such as Incremental Dynamic Programming <ref> (Sutton 1991) </ref> and the work of (Dean et al. 1993) and (Thiebaux et al. 1994), but adds a new analysis of the factors appropriate for envelope determination: computational effort is put where it will make the most difference to quality of action choice, and is expended only so long as it
Reference: <author> Thiebaux, S.; Hertzberg, J.; Shoaff, W.; and Schneider, M. </author> <year> 1994. </year> <title> A stochastic model of actions and plans for anytime planning under uncertainty. </title> <journal> Int. J. of Intelligent Systems. </journal>
Reference-contexts: The next section elaborates a planning system which accounts for the expense of computing by making tradeoffs between computation time and policy quality. The system uses local computation techniques resembling ones presented in (Dean et al. 1993) and <ref> (Thiebaux et al. 1994) </ref>, but allows for more extensive use of heuristic knowledge and is amenable to a proof of convergence to optimality. <p> This strategy is similar to (but was developed independently of) those described in the papers of (Dean et al. 1993) and <ref> (Thiebaux et al. 1994) </ref>; however, its use of estimated values on non-envelope states provides important additional information for determining envelope action choices, allowing for meaningful action choices to be made before a path to the goal is found, and summarizing previous computational effort outside the current envelope. <p> Both (Dean et al. 1993) and <ref> (Thiebaux et al. 1994) </ref> therefore choose the fringe state most likely to be reached first from the current state as the best candidate for further envelope expansion. <p> The method bears a general resemblance to other localized extensions of dynamic programming, such as Incremental Dynamic Programming (Sutton 1991) and the work of (Dean et al. 1993) and <ref> (Thiebaux et al. 1994) </ref>, but adds a new analysis of the factors appropriate for envelope determination: computational effort is put where it will make the most difference to quality of action choice, and is expended only so long as it will make a significant difference to the next action choice.
References-found: 9

