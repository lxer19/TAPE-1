URL: http://www.demo.cs.brandeis.edu/papers/sab94.ps.gz
Refering-URL: http://www.demo.cs.brandeis.edu/papers/long.html
Root-URL: http://www.cs.brandeis.edu
Title: Abstract  
Abstract: From the many possible perspectives in which an agent may be viewed, behavior-based AI selects observable actions as a particularly useful level of description. Yet behavior is clearly not structure, and anyone using behavior-based constraints to construct an agent still faces many implementa-tional roadblocks. Such obstacles are typically avoided by adopting a finite state automaton (FSA) as a base representation. As a result, potential benefits from alternative formalisms are ignored. To explore these benefits, our work adopts a multilevel view of an agent: behaviors and FSAs are but two of many levels of description. We still focus on behaviors for the expression of design constraints, but we avoid using FSAs as an implementation. Our particular agent, Addam, is comprised of a set of connectionist networks, a substrate which promotes the automatic design of subsumptive systems. Moreover, the implementational choice has important behavioral consequences some complex behaviors emerge due to interactions among networks and need not be specified explicitly. In this way, the underlying layers leak into one another, each affecting the others in subtle and desirable ways. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Agre, P. E. </author> <year> (1993). </year> <title> The symbolic worldview: Reply to Vera and Simon. </title> <journal> Cognitive Science, 17(1):6169. </journal>
Reference-contexts: is the set of beliefs of the agent, a goal is a desired state (of the world, for instance), and the principle of rationality stipulates that an agent will use its knowledge to accomplish its goals. different directions: e.g., connectionism (Hinton et al., 1986; McClelland et al., 1986), situated action <ref> (compare Vera and Simon, 1993, with Agre, 1993) </ref>, the observers paradox (Kolen and Pollack, 1993, to appear), and others (e.g., Searle, 1993).
Reference: <author> Angeline, P. J. </author> <year> (1994). </year> <title> Evolutionary Algorithms and Emergent Intelligence. </title> <type> Ph.D. thesis, </type> <institution> The Ohio State University, Columbus, Ohio. </institution>
Reference-contexts: Angeline (1994) has explored how modularization can arise without a behavioral decomposition, and elsewhere, we have explored how the structure of a module (i.e., number of hidden units and network connectivity) can arise from an evolutionary program <ref> (Saunders, Angeline, and Pollack, 1994) </ref>. Many of the problems of training behavior-based systems stem from the failure to recognize the multiplicity of levels in agents. We whole-heartedly agree with Brooks that the level of behaviors is particularly useful for the expression of design constraints.
Reference: <author> Beer, R. D. and Gallagher, J. C. </author> <year> (1992). </year> <title> Evolving dynamical neural networks for adaptive behavior. Adaptive Behavior, </title> <publisher> 1(1):91122. </publisher>
Reference-contexts: The first set of sensors is tactile, the second olfactory, and the third visual (implemented as sonar that passes through transparent objects). Unlike other attempts at learning that focus on a single behavior such as walking <ref> (Beer and Gallagher, 1992) </ref>, we chose to focus on the subsumptive interaction of several behaviors; hence, Addams actuators are a level of abstraction above leg controllers (similar to Brooks, 1986). Thus, Addam moves by simply specifying dx and dy.
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, 2(1):1423. </journal>
Reference-contexts: Unlike other attempts at learning that focus on a single behavior such as walking (Beer and Gallagher, 1992), we chose to focus on the subsumptive interaction of several behaviors; hence, Addams actuators are a level of abstraction above leg controllers <ref> (similar to Brooks, 1986) </ref>. Thus, Addam moves by simply specifying dx and dy. Internally, Addam consists of a set of feedforward connectionist networks, connected as shown in Figure 1.
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without representations. </title> <journal> Artificial Intelligence, 47:139159. </journal>
Reference-contexts: Unfortunately, Brooks does not go far enough. After performing a behavioral decomposition to define the functionality of a layer, he then proceeds to design a set of finite state automata (FSAs) to implement that layer. Yet, this is precisely the type of functional decomposition he warns against <ref> (Brooks, 1991, p. 146) </ref>. One might appeal to learning to avoid performing this functional decomposition by hand, but current work in automating behavior-based design focuses instead on learning the interactions between preexisting behavioral modules (e.g., Maes, 1991).
Reference: <author> Cariani, P. </author> <year> (1989). </year> <title> On the Design of Devices with Emergent Semantic Properties. </title> <type> Ph.D. thesis, </type> <institution> State University of New York at Binghamton. </institution>
Reference: <author> Chandrasekaran, B. and Josephson, S. G. </author> <year> (1993). </year> <title> Architecture of intelligence: The problems and current approaches to solutions. </title> <booktitle> Current Science, </booktitle> <address> 64(6):366380. </address>
Reference-contexts: For evolving sub-sumptive systems, however, such design-space freedom must be limited. In this paper, we present an alternative approach to sub-sumptive learning. Recognizing the multitude of formalisms with which to describe behaviors <ref> (Chandrasekaran and Josephson, 1993) </ref>, we explore the merits and drawbacks of adopting a connectionist implementation for our layers. 2 As will be discussed below, our version of subsumption replaces Brooks FSAs with feedforward networks and additional circuitry, combined so that each module in a hierarchy respects 2. <p> The activity of this layer peaks at about t=190, and then decays to 0 as Addam reaches the center of its room and the visual sensors balance. 5 Remarks The behavior of Chandrasekaran and Josephsons coin sorter is best described by appealing to multiple levels of behavior <ref> (Chandrasekaran and Josephson, 1993) </ref>. Addam is best described in a similar way. At one level, it is an agent which exhibits only three behaviors: avoid ice, go to food, and avoid blocks.
Reference: <author> Cliff, D. </author> <year> (1991). </year> <title> Computational neuroethology: A provisional manifesto. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 2939, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Collins, R. J. and Jefferson, D. R. </author> <year> (1991). </year> <title> Representations for artificial organisms. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 382390, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation architecture. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Structures 2, </booktitle> <pages> pages 524 532. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. </author> <year> (1986). </year> <title> Distributed representations. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pages 77109. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Jacobs, R. A., Jordan, M. I., and Barto, A. G. </author> <year> (1990). </year> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <journal> Cognitive Science, 15:219250. </journal>
Reference-contexts: In fact, the work of Beer and Gallagher (1992) or Maes and Brooks (1990) is really complementary to ours, for although Addams modules were instantiated with feedfor-ward networks trained by backpropagation, they could have 3. This also illustrates how our work differs from other methods of connectionist modular control <ref> (e.g., Jacobs, Jordan, and Barto, 1990) </ref>, which adopt a negative view of the interactions between modules. In fact, some work along these lines explicitly focuses on training away such interactions (Nowlan and Hinton, 1991). just as easily been trained by either genetic or correlation algorithms.
Reference: <author> Kolen, J. F. </author> <title> (In press). The observers paradox: The apparent computational complexity of physical systems. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence. </journal>
Reference: <author> Kolen, J. F. and Pollack, J. B. </author> <year> (1993). </year> <title> The apparent computational complexity of physical systems. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 617622, </pages> <address> Hillsdale, NJ. </address> <publisher> Erlbaum Associates. </publisher>
Reference-contexts: a desired state (of the world, for instance), and the principle of rationality stipulates that an agent will use its knowledge to accomplish its goals. different directions: e.g., connectionism (Hinton et al., 1986; McClelland et al., 1986), situated action (compare Vera and Simon, 1993, with Agre, 1993), the observers paradox <ref> (Kolen and Pollack, 1993, to appear) </ref>, and others (e.g., Searle, 1993).
Reference: <author> Maes, P. </author> <year> (1991). </year> <title> The agent network architecture. </title> <booktitle> In AAAI Spring Symposium on Integrated Intelligent Architectures, </booktitle> <month> March. </month>
Reference-contexts: Yet, this is precisely the type of functional decomposition he warns against (Brooks, 1991, p. 146). One might appeal to learning to avoid performing this functional decomposition by hand, but current work in automating behavior-based design focuses instead on learning the interactions between preexisting behavioral modules <ref> (e.g., Maes, 1991) </ref>. We feel that the reliance upon designed modules arises from choosing FSAs as the level in which to implement sub-sumptive systems; in particular, from the arbitrary ways in which FSAs interact. Brooks achieves modularity through task-based decomposition of complex behavior into a set of simpler behaviors.
Reference: <author> Maes, P. and Brooks, R. A. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of the Eighth National Conferences on AI, </booktitle> <pages> pages 769802. </pages>
Reference: <author> McClelland, J. L., Rumelhart, D. E., </author> <title> and The PDP Research Group (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 2: Psychological and Biological Models. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Meyer, J. A. and Guillot, A. </author> <year> (1991). </year> <title> Simulation of adaptive behavior in animats: Review and prospect. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 214, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Newell, A. </author> <year> (1982). </year> <title> The knowledge level. </title> <journal> Artificial Intelligence, 18:87127. </journal>
Reference-contexts: 1 Introduction Historically, AI has viewed agents from the Knowledge Level <ref> (Newell, 1982) </ref>, in which an individual is characterized by its knowledge, goals, and rationality. 1 The abstract nature of this level has been called into question from many 1.
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1991). </year> <title> Evaluation of adaptive mixtures of competing experts. </title> <editor> In Lippmann, R., Moody, J., and Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 774780. </pages> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: This also illustrates how our work differs from other methods of connectionist modular control (e.g., Jacobs, Jordan, and Barto, 1990), which adopt a negative view of the interactions between modules. In fact, some work along these lines explicitly focuses on training away such interactions <ref> (Nowlan and Hinton, 1991) </ref>. just as easily been trained by either genetic or correlation algorithms. Our work also sheds light on the issue of neural network representations for agents.
Reference: <author> Saunders, G. M., Angeline, P. J., and Pollack, J. B. </author> <year> (1994). </year> <title> Structural and behavioral evolution of recurrent networks. </title> <booktitle> In Advances in Neural Information Processing 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Angeline (1994) has explored how modularization can arise without a behavioral decomposition, and elsewhere, we have explored how the structure of a module (i.e., number of hidden units and network connectivity) can arise from an evolutionary program <ref> (Saunders, Angeline, and Pollack, 1994) </ref>. Many of the problems of training behavior-based systems stem from the failure to recognize the multiplicity of levels in agents. We whole-heartedly agree with Brooks that the level of behaviors is particularly useful for the expression of design constraints.
Reference: <author> Searle, J. </author> <year> (1992). </year> <title> Rediscovery of the Mind. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Simon, H. A. </author> <year> (1969). </year> <booktitle> Sciences of the Artificial. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Vera, A. H. and Simon, H. A. </author> <year> (1993). </year> <title> Situated action: A symbolic interpretation. </title> <journal> Cognitive Science, 17(1):748. </journal>
Reference-contexts: is the set of beliefs of the agent, a goal is a desired state (of the world, for instance), and the principle of rationality stipulates that an agent will use its knowledge to accomplish its goals. different directions: e.g., connectionism (Hinton et al., 1986; McClelland et al., 1986), situated action <ref> (compare Vera and Simon, 1993, with Agre, 1993) </ref>, the observers paradox (Kolen and Pollack, 1993, to appear), and others (e.g., Searle, 1993).

Reference: <author> Angeline, P. J. </author> <year> (1994). </year> <title> Evolutionary Algorithms and Emergent Intelligence. </title> <type> Ph.D. thesis, </type> <institution> The Ohio State University, Columbus, Ohio. </institution>
Reference-contexts: Angeline (1994) has explored how modularization can arise without a behavioral decomposition, and elsewhere, we have explored how the structure of a module (i.e., number of hidden units and network connectivity) can arise from an evolutionary program <ref> (Saunders, Angeline, and Pollack, 1994) </ref>. Many of the problems of training behavior-based systems stem from the failure to recognize the multiplicity of levels in agents. We whole-heartedly agree with Brooks that the level of behaviors is particularly useful for the expression of design constraints.
Reference: <author> Beer, R. D. and Gallagher, J. C. </author> <year> (1992). </year> <title> Evolving dynamical neural networks for adaptive behavior. Adaptive Behavior, </title> <publisher> 1(1):91122. </publisher>
Reference-contexts: The first set of sensors is tactile, the second olfactory, and the third visual (implemented as sonar that passes through transparent objects). Unlike other attempts at learning that focus on a single behavior such as walking <ref> (Beer and Gallagher, 1992) </ref>, we chose to focus on the subsumptive interaction of several behaviors; hence, Addams actuators are a level of abstraction above leg controllers (similar to Brooks, 1986). Thus, Addam moves by simply specifying dx and dy.
Reference: <author> Brooks, R. A. </author> <year> (1986). </year> <title> A robust layered control system for a mobile robot. </title> <journal> IEEE Journal of Robotics and Automation, 2(1):1423. </journal>
Reference-contexts: Unlike other attempts at learning that focus on a single behavior such as walking (Beer and Gallagher, 1992), we chose to focus on the subsumptive interaction of several behaviors; hence, Addams actuators are a level of abstraction above leg controllers <ref> (similar to Brooks, 1986) </ref>. Thus, Addam moves by simply specifying dx and dy. Internally, Addam consists of a set of feedforward connectionist networks, connected as shown in Figure 1.
Reference: <author> Brooks, R. A. </author> <year> (1991). </year> <title> Intelligence without representations. </title> <journal> Artificial Intelligence, 47:139159. </journal>
Reference-contexts: Unfortunately, Brooks does not go far enough. After performing a behavioral decomposition to define the functionality of a layer, he then proceeds to design a set of finite state automata (FSAs) to implement that layer. Yet, this is precisely the type of functional decomposition he warns against <ref> (Brooks, 1991, p. 146) </ref>. One might appeal to learning to avoid performing this functional decomposition by hand, but current work in automating behavior-based design focuses instead on learning the interactions between preexisting behavioral modules (e.g., Maes, 1991).
Reference: <author> Cariani, P. </author> <year> (1989). </year> <title> On the Design of Devices with Emergent Semantic Properties. </title> <type> Ph.D. thesis, </type> <institution> State University of New York at Binghamton. </institution>
Reference: <author> Chandrasekaran, B. and Josephson, S. G. </author> <year> (1993). </year> <title> Architecture of intelligence: The problems and current approaches to solutions. </title> <booktitle> Current Science, </booktitle> <address> 64(6):366380. </address>
Reference-contexts: For evolving sub-sumptive systems, however, such design-space freedom must be limited. In this paper, we present an alternative approach to sub-sumptive learning. Recognizing the multitude of formalisms with which to describe behaviors <ref> (Chandrasekaran and Josephson, 1993) </ref>, we explore the merits and drawbacks of adopting a connectionist implementation for our layers. 2 As will be discussed below, our version of subsumption replaces Brooks FSAs with feedforward networks and additional circuitry, combined so that each module in a hierarchy respects 2. <p> The activity of this layer peaks at about t=190, and then decays to 0 as Addam reaches the center of its room and the visual sensors balance. 5 Remarks The behavior of Chandrasekaran and Josephsons coin sorter is best described by appealing to multiple levels of behavior <ref> (Chandrasekaran and Josephson, 1993) </ref>. Addam is best described in a similar way. At one level, it is an agent which exhibits only three behaviors: avoid ice, go to food, and avoid blocks.
Reference: <author> Cliff, D. </author> <year> (1991). </year> <title> Computational neuroethology: A provisional manifesto. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 2939, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Collins, R. J. and Jefferson, D. R. </author> <year> (1991). </year> <title> Representations for artificial organisms. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 382390, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Fahlman, S. E. and Lebiere, C. </author> <year> (1990). </year> <title> The cascade-correlation architecture. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Structures 2, </booktitle> <pages> pages 524 532. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Hinton, G. E., McClelland, J. L., and Rumelhart, D. E. </author> <year> (1986). </year> <title> Distributed representations. </title> <editor> In Rumelhart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <pages> pages 77109. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Jacobs, R. A., Jordan, M. I., and Barto, A. G. </author> <year> (1990). </year> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <journal> Cognitive Science, 15:219250. </journal>
Reference-contexts: In fact, the work of Beer and Gallagher (1992) or Maes and Brooks (1990) is really complementary to ours, for although Addams modules were instantiated with feedfor-ward networks trained by backpropagation, they could have 3. This also illustrates how our work differs from other methods of connectionist modular control <ref> (e.g., Jacobs, Jordan, and Barto, 1990) </ref>, which adopt a negative view of the interactions between modules. In fact, some work along these lines explicitly focuses on training away such interactions (Nowlan and Hinton, 1991). just as easily been trained by either genetic or correlation algorithms.
Reference: <author> Kolen, J. F. </author> <title> (In press). The observers paradox: The apparent computational complexity of physical systems. </title> <journal> Journal of Experimental and Theoretical Artificial Intelligence. </journal>
Reference: <author> Kolen, J. F. and Pollack, J. B. </author> <year> (1993). </year> <title> The apparent computational complexity of physical systems. </title> <booktitle> In Proceedings of the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 617622, </pages> <address> Hillsdale, NJ. </address> <publisher> Erlbaum Associates. </publisher>
Reference-contexts: a desired state (of the world, for instance), and the principle of rationality stipulates that an agent will use its knowledge to accomplish its goals. different directions: e.g., connectionism (Hinton et al., 1986; McClelland et al., 1986), situated action (compare Vera and Simon, 1993, with Agre, 1993), the observers paradox <ref> (Kolen and Pollack, 1993, to appear) </ref>, and others (e.g., Searle, 1993).
Reference: <author> Maes, P. </author> <year> (1991). </year> <title> The agent network architecture. </title> <booktitle> In AAAI Spring Symposium on Integrated Intelligent Architectures, </booktitle> <month> March. </month>
Reference-contexts: Yet, this is precisely the type of functional decomposition he warns against (Brooks, 1991, p. 146). One might appeal to learning to avoid performing this functional decomposition by hand, but current work in automating behavior-based design focuses instead on learning the interactions between preexisting behavioral modules <ref> (e.g., Maes, 1991) </ref>. We feel that the reliance upon designed modules arises from choosing FSAs as the level in which to implement sub-sumptive systems; in particular, from the arbitrary ways in which FSAs interact. Brooks achieves modularity through task-based decomposition of complex behavior into a set of simpler behaviors.
Reference: <author> Maes, P. and Brooks, R. A. </author> <year> (1990). </year> <title> Learning to coordinate behaviors. </title> <booktitle> In Proceedings of the Eighth National Conferences on AI, </booktitle> <pages> pages 769802. </pages>
Reference: <author> McClelland, J. L., Rumelhart, D. E., </author> <title> and The PDP Research Group (1986). Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 2: Psychological and Biological Models. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Meyer, J. A. and Guillot, A. </author> <year> (1991). </year> <title> Simulation of adaptive behavior in animats: Review and prospect. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 214, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
Reference: <author> Newell, A. </author> <year> (1982). </year> <title> The knowledge level. </title> <journal> Artificial Intelligence, 18:87127. </journal>
Reference-contexts: 1 Introduction Historically, AI has viewed agents from the Knowledge Level <ref> (Newell, 1982) </ref>, in which an individual is characterized by its knowledge, goals, and rationality. 1 The abstract nature of this level has been called into question from many 1.
Reference: <author> Nowlan, S. J. and Hinton, G. E. </author> <year> (1991). </year> <title> Evaluation of adaptive mixtures of competing experts. </title> <editor> In Lippmann, R., Moody, J., and Touretzky, D., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 774780. </pages> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: This also illustrates how our work differs from other methods of connectionist modular control (e.g., Jacobs, Jordan, and Barto, 1990), which adopt a negative view of the interactions between modules. In fact, some work along these lines explicitly focuses on training away such interactions <ref> (Nowlan and Hinton, 1991) </ref>. just as easily been trained by either genetic or correlation algorithms. Our work also sheds light on the issue of neural network representations for agents.
Reference: <author> Saunders, G. M., Angeline, P. J., and Pollack, J. B. </author> <year> (1994). </year> <title> Structural and behavioral evolution of recurrent networks. </title> <booktitle> In Advances in Neural Information Processing 7. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Angeline (1994) has explored how modularization can arise without a behavioral decomposition, and elsewhere, we have explored how the structure of a module (i.e., number of hidden units and network connectivity) can arise from an evolutionary program <ref> (Saunders, Angeline, and Pollack, 1994) </ref>. Many of the problems of training behavior-based systems stem from the failure to recognize the multiplicity of levels in agents. We whole-heartedly agree with Brooks that the level of behaviors is particularly useful for the expression of design constraints.
Reference: <author> Searle, J. </author> <year> (1992). </year> <title> Rediscovery of the Mind. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Simon, H. A. </author> <year> (1969). </year> <booktitle> Sciences of the Artificial. </booktitle> <publisher> MIT Press. </publisher>
Reference: <author> Vera, A. H. and Simon, H. A. </author> <year> (1993). </year> <title> Situated action: A symbolic interpretation. </title> <journal> Cognitive Science, 17(1):748. </journal>
Reference-contexts: is the set of beliefs of the agent, a goal is a desired state (of the world, for instance), and the principle of rationality stipulates that an agent will use its knowledge to accomplish its goals. different directions: e.g., connectionism (Hinton et al., 1986; McClelland et al., 1986), situated action <ref> (compare Vera and Simon, 1993, with Agre, 1993) </ref>, the observers paradox (Kolen and Pollack, 1993, to appear), and others (e.g., Searle, 1993).
Reference: <author> Wilson, S. W. </author> <year> (1991). </year> <title> The animat path to AI. </title> <editor> In Meyer, J. A. and Wilson, S. W., editors, </editor> <booktitle> From Animals to Animats: Proceedings of the First International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 1521, </pages> <address> Cambridge. </address> <publisher> MIT Press. </publisher>
References-found: 48

