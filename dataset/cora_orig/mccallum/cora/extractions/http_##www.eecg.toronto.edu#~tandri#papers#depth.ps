URL: http://www.eecg.toronto.edu/~tandri/papers/depth.ps
Refering-URL: http://www.cs.toronto.edu/~tandri/
Root-URL: 
Title: Software Support for Parallel Computing A Perspective  
Author: Sudarsan Tandri 
Date: May 8, 1992  
Abstract: Although performance enhancement in parallel programs is conceptually simple, it has been complicated because of the abstractions provided by programming languages. The more abstract a parallel programming language is, the simpler it is to use. With the abstraction, the information required for enhancing performance is generally lost. This paper analyzes the parameters required for performance and the information hidden because of the use of programming languages. 
Abstract-found: 1
Intro-found: 1
Reference: [ABB + 86] <author> Mike Accetta, Robert Baron, William Bolosky, David Golub, et al. </author> <title> MACH:A new kernel foundation for UNIX development. </title> <booktitle> In Proceedings of the summer USENIX conference, </booktitle> <pages> pages 93-112, </pages> <month> July </month> <year> 1986. </year>
Reference-contexts: The operating system must be designed in such a way that it provides a good machine interface without degrading the performance of the architecture. In the operating system domain, there are two main variations for creating processes - light-weight processes using a Mach-type operating system <ref> [ABB + 86] </ref> or Unix-compatible heavy-weight processes with some provision for accommodating the shared address space [RT74]. The scheduler's decision to properly allocate tasks to processors also plays a major role in obtaining good speedup. <p> Performance was not motivating factor in the design of many of these languages. 4.5 Independent Threads of Control in a Shared Address Space COOL, PCN, Presto, and C-Threads provide a facility to create independent threads of control in a shared address space <ref> [CGH90, FT91b, BLL88, ABB + 86] </ref>. The programmer is responsible for proper synchronization, and access restrictions for the shared data. The threads can create their own private copies of data to provide locality on NUMA machines. The user is responsible for load balancing and granularity of tasks.
Reference: [AOC + 88] <author> Gregory R. Andrews, Ronald A. Olsson, Michael Coffin, Irving Elshoff, Kelvin Nilsen, Titus Purdin, and Gregg Townsend. </author> <title> An overview of the SR language and implementation. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 51-86, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Lack of data partitioning primitives in the parallel Fortran dialects and Chores prevent it from being ported effectively to NUMA and NORMA machines. 10 4.4 New Parallel Programming Languages Many new parallel programming languages like Occam, Hermes, Orca, Synchronizing Resources (SR), and Argus have been designed for distributed memory machines <ref> [Bal90, Hoa78, Per87, AOC + 88] </ref>. Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines [FT91b, FT91a, CG89] 5 .
Reference: [Bab88] <author> Robert G. Babb II. </author> <title> Programming Parallel Processors. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1988. </year>
Reference-contexts: In languages developed for parallel programming, the parallelism is explicit. A compromise between performance parameters using an automated technique is hard to arrive at. 4.1 Sequential Programming Languages Parafrase-2, KAP, and PTRAN are popular restructuring compilers <ref> [PGH + 90, Bab88, Sar91] </ref>. Restructuring compilers take the sequential code and transform it to parallel form, maintaining the dependencies dictated by the sequential execution. <p> Locality and load balancing are conflicting performance measures in a NUMA multiprocessor. When should locality be sacrificed for load balancing? The data parallel programming languages are restricted to the data partitioning and cannot support recursive parallelism. All Fortran parallel programming dialects <ref> [Bab88] </ref> can also be classified as data parallel. But the iterations need not be completely independent of each other. Altering the threads of control, and providing monitor style access to variables gives these languages a control parallel flavour.
Reference: [Bal90] <author> H. E. Bal. </author> <title> Programming Distributed Systems. </title> <publisher> Silicon Press, </publisher> <address> NJ, </address> <year> 1990. </year>
Reference-contexts: Lack of data partitioning primitives in the parallel Fortran dialects and Chores prevent it from being ported effectively to NUMA and NORMA machines. 10 4.4 New Parallel Programming Languages Many new parallel programming languages like Occam, Hermes, Orca, Synchronizing Resources (SR), and Argus have been designed for distributed memory machines <ref> [Bal90, Hoa78, Per87, AOC + 88] </ref>. Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines [FT91b, FT91a, CG89] 5 . <p> Only a few languages have focussed their attention on scheduling policies. The table does not contain many distributed programming languages because they have been extensively surveyed in <ref> [Bal90] </ref>. Table 1.
Reference: [BLL88] <author> Brian N. Bershad, Edward D. Lazowska, and Henry M. Levy. </author> <title> Presto: A system for object-oriented parallel programming. </title> <journal> Software Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Performance was not motivating factor in the design of many of these languages. 4.5 Independent Threads of Control in a Shared Address Space COOL, PCN, Presto, and C-Threads provide a facility to create independent threads of control in a shared address space <ref> [CGH90, FT91b, BLL88, ABB + 86] </ref>. The programmer is responsible for proper synchronization, and access restrictions for the shared data. The threads can create their own private copies of data to provide locality on NUMA machines. The user is responsible for load balancing and granularity of tasks.
Reference: [CB90] <author> David Callahan and Smith Burton. </author> <title> A future-based parallel language for a general-purpose highly-parallel computer. </title> <editor> In David Gelernter, Alexandru Nico-lau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 95-113. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year> <month> 15 </month>
Reference-contexts: In Presto, COOL, C-Threads, and PCN the programmer is responsible for maintaining the dependencies and introducing proper synchronizations among 5 Details about some of these programming languages can be found in the Appendix. 6 Tera Computer Companys proposed machine has this hardware bit <ref> [CB90] </ref>. 11 tasks. In Schedule the programmer specifies the dependencies among the tasks and the run--time system is responsible for following the ordering. Ada, and Turing+ are concurrent programming languages. These languages were designed for creating and managing multiple processes in a uniprocessor environment.
Reference: [CG89] <author> N. Carriero and David Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines <ref> [FT91b, FT91a, CG89] </ref> 5 . In distributed memory machines the choice of synchronous message passing, asynchronous message passing, naming of the communication ports, remote procedure call specification, and asynchronous process invocation has given rise to a spate of programming languages.
Reference: [CGH90] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> COOL: A language for parallel programming. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 126-148. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: Performance was not motivating factor in the design of many of these languages. 4.5 Independent Threads of Control in a Shared Address Space COOL, PCN, Presto, and C-Threads provide a facility to create independent threads of control in a shared address space <ref> [CGH90, FT91b, BLL88, ABB + 86] </ref>. The programmer is responsible for proper synchronization, and access restrictions for the shared data. The threads can create their own private copies of data to provide locality on NUMA machines. The user is responsible for load balancing and granularity of tasks.
Reference: [DDdC + 87] <author> James Demmel, Jack J. Dongarra, Jeremy du Croz, Anne Greenbaum, Suen Hammering, and Danny Sorenson. </author> <title> Lapack working note 1: Prospectus for the development of a linear algebra library for high-performance computers. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <month> September </month> <year> 1987. </year>
Reference-contexts: LAPACK is the only parallel library package that is used. LAPACK provides interfaces to parallel Basic Linear Algebra Subroutines. Efficient implementation of the BLAS routines is required to achieve good performance <ref> [DDdC + 87] </ref>. 7 8 Sequential programming languages are easy to program in and the analysis is left to the compiler. The performance obtained may be poor. Programming using concurrent programming languages is hard but the task of the compiler is reduced. The performance obtained is good.
Reference: [DS87] <author> Jack J. Dongarra and Danny C. Sorenson. </author> <title> Schedule: Tools for developing and analyzing parallel Fortran programs. </title> <editor> In Leah H. Jamieson, Dennis B. Gannon, and Robert J. Douglass, editors, </editor> <booktitle> The Characteristics of Parallel Programs, </booktitle> <pages> pages 363-394. </pages> <publisher> The MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: The concept of future is similar to the idea of single assignment variables in SISAL, a data flow language [FC90]. Schedule is an extension to Fortran to specify the task graphs <ref> [DS87] </ref>. The control process executes the program and based on the information provided by the user, identifies the task dependence structure. All the executable processes are placed on a central queue. All the processes that are created are called worker processes.
Reference: [EZ90] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared-memory parallel computing. </title> <type> Draft, </type> <institution> Dept. of Computational Science, University of Saskatchewan, </institution> <year> 1990. </year>
Reference-contexts: But the iterations need not be completely independent of each other. Altering the threads of control, and providing monitor style access to variables gives these languages a control parallel flavour. Chores is another data parallel programming extension derived from experiments using Presto <ref> [EZ90] </ref>. It is very similar to the the parallel Fortran dialects and specifies loop level data parallelism.
Reference: [F + 90] <author> Geoffery Fox et al. </author> <title> Fortran D specification. </title> <type> Technical report, </type> <institution> Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Because of the strong leaning toward sequential semantics, these languages are restricted to shared memory multiprocessors. 4.3 Data Parallel Programming Fortran-D is a data parallel programming language and is an extension to Fortran 77 <ref> [F + 90] </ref>. Data Parallel C (DPC) and Dino also belong to the data parallel language class and are extensions to C [HQ + 91, QHS91, RSW88]. In Fortran-D, only data partitioning is specified, whereas in DPC and Dino code is also partitioned.
Reference: [FC90] <author> John T. Feo and David C. Cann. </author> <title> A report on the SISAL language project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10 </volume> <pages> 349-366, </pages> <year> 1990. </year>
Reference-contexts: It can also be implemented by associating one of the values that the variable takes as unassigned but this requires change to the language semantics. The concept of future is similar to the idea of single assignment variables in SISAL, a data flow language <ref> [FC90] </ref>. Schedule is an extension to Fortran to specify the task graphs [DS87]. The control process executes the program and based on the information provided by the user, identifies the task dependence structure. All the executable processes are placed on a central queue.
Reference: [FT91a] <author> Ian Foster and Stephen Taylor. </author> <title> Strand New Concepts in Parallel Programming. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference-contexts: Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines <ref> [FT91b, FT91a, CG89] </ref> 5 . In distributed memory machines the choice of synchronous message passing, asynchronous message passing, naming of the communication ports, remote procedure call specification, and asynchronous process invocation has given rise to a spate of programming languages.
Reference: [FT91b] <author> Ian Foster and Steven Tuecke. </author> <title> Parallel programming with PCN. </title> <type> Technical report, </type> <institution> Argonne National Laboratory, </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines <ref> [FT91b, FT91a, CG89] </ref> 5 . In distributed memory machines the choice of synchronous message passing, asynchronous message passing, naming of the communication ports, remote procedure call specification, and asynchronous process invocation has given rise to a spate of programming languages. <p> Performance was not motivating factor in the design of many of these languages. 4.5 Independent Threads of Control in a Shared Address Space COOL, PCN, Presto, and C-Threads provide a facility to create independent threads of control in a shared address space <ref> [CGH90, FT91b, BLL88, ABB + 86] </ref>. The programmer is responsible for proper synchronization, and access restrictions for the shared data. The threads can create their own private copies of data to provide locality on NUMA machines. The user is responsible for load balancing and granularity of tasks.
Reference: [Gab89] <author> Eran Gabber. </author> <title> VMMP: A virtual machine for the development of portable and efficient programs for multiprocessors. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <year> 1989. </year>
Reference-contexts: The programmer is responsible for managing all the performance parameters. 4.6 VMMP Virtual Machine for MultiProcessors (VMMP) tries to provide services for a limited set of algorithms and can be classified as restricted independent threads of control and message passing <ref> [Gab89, Gab90] </ref>. VMMP is limited to two types of computations, message passing and recursive computations. Message passing computations use the VMMP routines for sending and receiving information. Recursive computations share a limited amount of information. This shared data space is explicitly created and handled by VMMP.
Reference: [Gab90] <author> Eran Gabber. </author> <title> VMMP: A practical tool for the development of portable and efficient programs for multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The programmer is responsible for managing all the performance parameters. 4.6 VMMP Virtual Machine for MultiProcessors (VMMP) tries to provide services for a limited set of algorithms and can be classified as restricted independent threads of control and message passing <ref> [Gab89, Gab90] </ref>. VMMP is limited to two types of computations, message passing and recursive computations. Message passing computations use the VMMP routines for sending and receiving information. Recursive computations share a limited amount of information. This shared data space is explicitly created and handled by VMMP.
Reference: [GB92] <author> Manish Gupta and Prithviraj Banerjee. </author> <title> Demonstration of automatic data partitioning techiniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year> <month> 16 </month>
Reference-contexts: Restructuring compilers partition the code and Data partitioning compilers partition data. Restructuring is the first step in data partitioning compilers. Researchers are currently working on finding heuristics for data partitioning to minimize communication <ref> [GB92, RS91] </ref>. The problem of finding optimal data partitioning has been shown to be NP-complete [GB92]. The restructuring compilers were primarily designed for FORTRAN and the code comprising loops. These compilers identify the loops that can be executed in parallel based on the data dependencies in the loop. <p> Restructuring compilers partition the code and Data partitioning compilers partition data. Restructuring is the first step in data partitioning compilers. Researchers are currently working on finding heuristics for data partitioning to minimize communication [GB92, RS91]. The problem of finding optimal data partitioning has been shown to be NP-complete <ref> [GB92] </ref>. The restructuring compilers were primarily designed for FORTRAN and the code comprising loops. These compilers identify the loops that can be executed in parallel based on the data dependencies in the loop. Because of the presence of global data structures, locality information is not available.
Reference: [Hoa78] <author> C. A. R. Hoare. </author> <title> Communicating sequential processes. </title> <journal> Communications of the ACM, </journal> <volume> 21(8) </volume> <pages> 666-677, </pages> <month> August </month> <year> 1978. </year>
Reference-contexts: Lack of data partitioning primitives in the parallel Fortran dialects and Chores prevent it from being ported effectively to NUMA and NORMA machines. 10 4.4 New Parallel Programming Languages Many new parallel programming languages like Occam, Hermes, Orca, Synchronizing Resources (SR), and Argus have been designed for distributed memory machines <ref> [Bal90, Hoa78, Per87, AOC + 88] </ref>. Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines [FT91b, FT91a, CG89] 5 .
Reference: [HQ + 91] <author> Philip J. Hatcher, Michael J. Quinn, et al. </author> <title> Data-parallel programming on mimd computers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 377-383, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Data Parallel C (DPC) and Dino also belong to the data parallel language class and are extensions to C <ref> [HQ + 91, QHS91, RSW88] </ref>. In Fortran-D, only data partitioning is specified, whereas in DPC and Dino code is also partitioned. The Fortran-D compiler is responsible for generating code that corresponds to the particular group of data elements on that processor.
Reference: [Kar87] <author> Alan H. Karp. </author> <title> Programming for parallelism. </title> <journal> Computer, </journal> <volume> 20(5) </volume> <pages> 43-57, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: When performance becomes critical, programmers have used their knowledge of paging, cache-size, vector lengths, and so forth to fine tune their programs <ref> [Kar87] </ref>. The desire to obtain the best possible performance for a problem requires the fine-tuning of the program based on the knowledge of the machine. Software development environment includes the compiler, debugger, editor, performance monitor, and other associated tools.
Reference: [KKK90] <author> David Klappholz, Apostolos D. Kallis, and Xiangyun Kong. </author> <title> Refined C an update. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 331-357. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: The data partitioning compilers are limited to restricted programs and architectures 4 . 4.2 Limited Programmer Specified Parallelism Programming languages like Jade and Refined C try to maintain the sequential semantics and try to identify parallel segments of code with the help of the compiler and/or the run-time system <ref> [KKK90, LR91] </ref>. In Jade the programmer identifies segments of code that can potentially be executed in parallel and provides the run-time system with information about the data elements that are used in that segment.
Reference: [LR91] <author> Monica S. Lam and Martin C. Rinard. </author> <title> Coarse-grain parallel programming in Jade. </title> <booktitle> In Proceedings of the third ACM SIGPLAN symposium on PPOPP, </booktitle> <pages> pages 94-105, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The data partitioning compilers are limited to restricted programs and architectures 4 . 4.2 Limited Programmer Specified Parallelism Programming languages like Jade and Refined C try to maintain the sequential semantics and try to identify parallel segments of code with the help of the compiler and/or the run-time system <ref> [KKK90, LR91] </ref>. In Jade the programmer identifies segments of code that can potentially be executed in parallel and provides the run-time system with information about the data elements that are used in that segment.
Reference: [MR90] <author> Piyush Mehrotra and John Van Rosendale. </author> <title> Programming distributed memory architectures using kali. </title> <type> Technical report, </type> <institution> NASA Langley Research Center, Hamp-ton, </institution> <month> October </month> <year> 1990. </year>
Reference: [Pan91] <author> Cherri M. Pancake. </author> <title> Software support for parallel computing: Where are we headed? Communications of the ACM, </title> <booktitle> 34(11) </booktitle> <pages> 52-64, </pages> <month> November </month> <year> 1991. </year>
Reference-contexts: 1 Introduction "Nobody wants parallelism. What we want is performance. It is the fact that going to parallelism is the only way to continue to enhance performance, that makes parallelism a necessity" <ref> [Pan91] </ref> Applications such as weather forecasting, 3-D modeling, fluid dynamics, and real-time image processing require computational capability far beyond even our fastest sequential machines. Parallel machines seem to be the only viable alternative. A parallel computer consists of a number of conventional processors collected together into a single device [Sch84].
Reference: [PB90] <author> Cherri M. Pancake and Donna Bergmark. </author> <title> Do parallel languages respond to the needs of scientific programmers. </title> <journal> Computer, </journal> <volume> 23(12) </volume> <pages> 13-23, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Efficient algorithm design also plays a major role in obtaining good performance on a given system. Numerical methods are being designed to exploit multiprocessing capabilities <ref> [PB90] </ref>. High performance computing users are exploring ways to handle non-determinism and improve performance. Although algorithm design is also dictated by the language, it is in the user's domain. We will not analyze algorithm design issues further in this paper. <p> Implicit parallelism requires extensive analysis of the dependencies among the data items. Parallelism becomes explicit when the programmer specifies the nature and the extent of concurrent activities through language constructs. In the current literature the mechanisms to support parallel programming have been classified into three categories <ref> [PB90] </ref>: * adding extensions to existing sequential languages * providing high-level interfaces to parallel routines stored in a system library, and * incorporating parallel features as integral parts of a language's design. The classification schemes proposed in the literature classify programming languages without regard to how parallelism is expressed.
Reference: [Per87] <author> Ronald H. Perrott. </author> <title> Parallel Programming. </title> <publisher> Addison Wesley Publishing Company, Inc., </publisher> <address> Reading, Massachusetts, </address> <year> 1987. </year>
Reference-contexts: Lack of data partitioning primitives in the parallel Fortran dialects and Chores prevent it from being ported effectively to NUMA and NORMA machines. 10 4.4 New Parallel Programming Languages Many new parallel programming languages like Occam, Hermes, Orca, Synchronizing Resources (SR), and Argus have been designed for distributed memory machines <ref> [Bal90, Hoa78, Per87, AOC + 88] </ref>. Languages like Program Composition Notation (PCN), Strand, and Linda are targeted for both shared and distributed memory machines [FT91b, FT91a, CG89] 5 .
Reference: [PGH + 90] <author> Constantine D. Polychronopolis, Milind B. Girkar, Mohammed R. Haghighat, Chia L. Lee, Bruce P. Leung, and Dale A. Schouten. </author> <title> The structure of parafrase-2: an advanced parallelizing compiler for C and FORTRAN. </title> <editor> In David Gelernter, Alexandru Nicolau, and David Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 423-453. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: In languages developed for parallel programming, the parallelism is explicit. A compromise between performance parameters using an automated technique is hard to arrive at. 4.1 Sequential Programming Languages Parafrase-2, KAP, and PTRAN are popular restructuring compilers <ref> [PGH + 90, Bab88, Sar91] </ref>. Restructuring compilers take the sequential code and transform it to parallel form, maintaining the dependencies dictated by the sequential execution.
Reference: [QHS91] <author> Michael J. Quinn, P. Hatcher, and B. Seevers. </author> <title> Implementing a data parallel language on a tightly coupled multiprocessor. </title> <editor> In Alexander Nicolau, David Gel-ernter, Thomas Gross, and David Padua, editors, </editor> <booktitle> Advances in Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 385-401. </pages> <publisher> Pitman, </publisher> <address> London, </address> <year> 1991. </year>
Reference-contexts: Data Parallel C (DPC) and Dino also belong to the data parallel language class and are extensions to C <ref> [HQ + 91, QHS91, RSW88] </ref>. In Fortran-D, only data partitioning is specified, whereas in DPC and Dino code is also partitioned. The Fortran-D compiler is responsible for generating code that corresponds to the particular group of data elements on that processor.
Reference: [RS91] <author> J Ramanujam and P Sadayappan. </author> <title> Compile time techniques for data distribution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Restructuring compilers partition the code and Data partitioning compilers partition data. Restructuring is the first step in data partitioning compilers. Researchers are currently working on finding heuristics for data partitioning to minimize communication <ref> [GB92, RS91] </ref>. The problem of finding optimal data partitioning has been shown to be NP-complete [GB92]. The restructuring compilers were primarily designed for FORTRAN and the code comprising loops. These compilers identify the loops that can be executed in parallel based on the data dependencies in the loop.
Reference: [RSW88] <author> Matthew Rosing, Robert B. Schnabel, and Robert Weaver. Dino: </author> <title> Summary and examples. </title> <booktitle> In Proceedings of Third conference on Hypercube Concurrent computers and applications, </booktitle> <pages> pages 472-481, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Data Parallel C (DPC) and Dino also belong to the data parallel language class and are extensions to C <ref> [HQ + 91, QHS91, RSW88] </ref>. In Fortran-D, only data partitioning is specified, whereas in DPC and Dino code is also partitioned. The Fortran-D compiler is responsible for generating code that corresponds to the particular group of data elements on that processor.
Reference: [RT74] <author> Dennis M Ritchie and Ken Thompson. </author> <title> The UNIX time-sharing system. </title> <journal> Communications of the ACM, </journal> <volume> 17(7) </volume> <pages> 365-375, </pages> <month> July </month> <year> 1974. </year>
Reference-contexts: In the operating system domain, there are two main variations for creating processes - light-weight processes using a Mach-type operating system [ABB + 86] or Unix-compatible heavy-weight processes with some provision for accommodating the shared address space <ref> [RT74] </ref>. The scheduler's decision to properly allocate tasks to processors also plays a major role in obtaining good speedup. A parallel machine does the programmer no good without a means of describing the parallelism to the system.
Reference: [Sar91] <author> Vivek Sarkar. </author> <title> PTRAN: The IBM parallel translation system. </title> <editor> In Boleslaw K. Szymanki, editor, </editor> <booktitle> Parallel Functional Languages and Compilers, </booktitle> <pages> pages 309-392. </pages> <publisher> ACM Press, </publisher> <address> NY, </address> <year> 1991. </year>
Reference-contexts: In languages developed for parallel programming, the parallelism is explicit. A compromise between performance parameters using an automated technique is hard to arrive at. 4.1 Sequential Programming Languages Parafrase-2, KAP, and PTRAN are popular restructuring compilers <ref> [PGH + 90, Bab88, Sar91] </ref>. Restructuring compilers take the sequential code and transform it to parallel form, maintaining the dependencies dictated by the sequential execution.
Reference: [SBG + 91] <author> Robert E. Storm, David F. Bacon, Arthur P. Goldberg, Andy Lowry, Daniel M. Yellin, and Shaula A. Yemini. </author> <title> Hermes: A Language for Parallel Distributed Computing. </title> <publisher> Prentice Hall, </publisher> <year> 1991. </year>
Reference: [Sch84] <editor> U. Schendel. </editor> <title> Introduction to Numerical Methods for Parallel Computers. </title> <publisher> Ellis Horwood Limited, </publisher> <year> 1984. </year>
Reference-contexts: Parallel machines seem to be the only viable alternative. A parallel computer consists of a number of conventional processors collected together into a single device <ref> [Sch84] </ref>. A typical parallel computing environment is shown in Figure 1. The performance of a sequential machine is dictated by the architecture, the operating system, and the software development environment. In parallel computing, these aspects play an even greater role.
Reference: [VR88] <author> Mark T Vandevoorde and Eric S Roberts. Workcrews: </author> <title> An abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <year> 1988. </year>
Reference-contexts: All the executable processes are placed on a central queue. All the processes that are created are called worker processes. Worker processes remove the work from the central task queue and complete it. This model is called the the work-crew model <ref> [VR88] </ref> of parallel programming. In Presto, COOL, C-Threads, and PCN the programmer is responsible for maintaining the dependencies and introducing proper synchronizations among 5 Details about some of these programming languages can be found in the Appendix. 6 Tera Computer Companys proposed machine has this hardware bit [CB90]. 11 tasks.
References-found: 36

