URL: http://www.ai.mit.edu/projects/transit/dpga/dpga-fpga94.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/transit/dpga_prototype_documents.html
Root-URL: 
Email: misha@ai.mit.edu andre@ai.mit.edu tk@ai.mit.edu  
Phone: (617) 253-8170 (617) 253-5868 (617) 253-7807 545  FAX: (617) 253-5060  
Title: Unifying FPGAs and SIMD Arrays  
Author: Michael Bolotski Andre DeHon Thomas F. Knight, Jr. 
Date: March 7, 1994  
Address: Cambridge, MA 02139  
Affiliation: Technology Sq.  
Note: FPGA '94 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA  
Abstract: Field-Programmable Gate Arrays (FPGAs) and Single-Instruction Multiple-Data (SIMD) processing arrays share many architectural features. In both architectures, an array of simple, fine-grained logic elements is employed to provide high-speed, customizable, bit-wise computation. In this paper, we present a unified computational array model which encompasses both FPGAs and SIMD arrays. Within this framework, we examine the differences and similarities between these array structures and touch upon techniques and lessons which can be transfered between the architectures. The unified model also exposes promising prospects for hybrid array architectures. We introduce the Dynamically Programmable Gate Array (DPGA) which combines the best features from FPGAs and SIMD arrays into a single array architecture. 
Abstract-found: 1
Intro-found: 1
Reference: [Act90] <author> Actel Corporation, </author> <title> 955 East Arques Avenue, Sunnyvale, CA 94086. ACT Family Field Programmable Gate Array DATABOOK, </title> <month> April </month> <year> 1990. </year>
Reference-contexts: FPGAs are in wide use today for system customization and glue logic, low-volume application-specific designs, and IC prototyping. A variety of FPGAs are commercially available from a number of vendors (e.g. Xilinx [Inc91], Actel <ref> [Act90] </ref>, At-mel [Fur93], Lattice [Qui92]). Single-Instruction Multiple-Data (SIMD) arrays are employed to realize high throughput on many regular, computationally intensive data processing applications. An array of simple, fine-grained computational units make up most SIMD arrays. Typically, the computational units are wired together through local, nearest-neighbor communications. <p> During a slow programming phase, each AE is configured with its operation, which remain fixed during subsequent normal operation. In SRAM-programmable FPGAs (e.g. Xilinx LCA [Inc91], Atmel [Fur93]) this programming phase normally occurs once each time the system is powered on. In fuse or anti-fuse based FPGAs (e.g. Actel <ref> [Act90] </ref> ) a device is programmed exactly once during its lifetime. SIMD arrays weaken the model by distributing the same instruction to every AE (See Figure 4).
Reference: [ALKK90] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubia-towicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1990. </year>
Reference-contexts: This has two benefits. First, the array can support multiple configurations for a single computation thread. Second, configuration contexts allow DPGA co-processors to support multiple threads. This capability will become more important for fine-grained, multithreaded microprocessors which support fast context switching (e.g. April <ref> [ALKK90] </ref>, *T [NPA92]). 4.2 Costs From both the FPGA and SIMD array standpoint, the primary additional cost for the DPGA array is the area for the instruction store lookup table (See Figure 6).
Reference: [BBLC93] <author> M. Bolotski, R. Barman, J. J. Little, and D. Camporese. SILT: </author> <title> A Distributed Bit-Parallel Architecture for 9 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA Early Vision. </title> <journal> International Journal of Computer Vision, </journal> <volume> IJCV-11:63-74, </volume> <year> 1993. </year>
Reference-contexts: Increased silicon area along with advanced 1 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA packaging trends allow production of very high-performance, highly-integrated, SIMD arrays at reasonable costs <ref> [BBLC93] </ref>, [Lea91]. Viewed at an abstract level, these two array architectures are very similar. Both employ a moderately fine-grained array of logic elements.
Reference: [BDHR88] <author> D. W. Blevins, E. W. Davis, R. A. Heaton, and J. H. Reif. Blitzen: </author> <title> A Highly Integrated Massively Parallel Machine. </title> <booktitle> In Frontiers of Parallel Computation '88, </booktitle> <year> 1988. </year>
Reference-contexts: The translation process is typically very simple: operational autonomy is achieved by potentially inverting the ALU output; addressing autonomy by modifying a few low order bits of the address <ref> [BDHR88] </ref>; connection autonomy by configuring a network crossbar at each AE site [ML89]. All of these techniques can be viewed as resource-limited implementations of the DPGA architecture.
Reference: [Bla90] <author> Tom Blank. </author> <title> The MasPar MP-1 Architecture. </title> <booktitle> In International Conference on Computer Architecture, </booktitle> <year> 1990. </year>
Reference-contexts: Typical applications for SIMD arrays include low-level vision and image processing, discrete particle simulation, database searches, and genetic sequence matching. NASA's MPP [Pot85], Thinking Machines' CM-2 [Hil85], and MasPar's MP-1 <ref> [Bla90] </ref> are early examples of large-scale SIMD arrays. Increased silicon area along with advanced 1 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA packaging trends allow production of very high-performance, highly-integrated, SIMD arrays at reasonable costs [BBLC93], [Lea91].
Reference: [Bol93] <author> Michael Bolotski. </author> <title> Abacus: A High-Performance Software Bit-Parallel Architecture. AI memo, in preparation, </title> <publisher> MIT, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction FPGA-based custom computing engines and massively parallel SIMD arrays <ref> [Gea91, Bol93, BRV93] </ref> have been demonstrated to provide supercomputer-class performance on some tasks at a tiny fraction of supercomputer cost. Both of these architectures consist of arrays of small yet numerous processing elements. <p> The instruction store can be implemented with a single-port, SRAM-style memory array which can be implemented very compactly in a full-custom design. For example, 70% of each Abacus AE <ref> [Bol93] </ref> comprises roughly 80 bits of SRAM memory. The Abacus AE requires 16 instruction bits to specify the operation performed by the ALU on each cycle. We could add another 80-bit SRAM to serve as a 5-entry context cache. <p> For example, the Abacus machine, a SIMD architecture designed for bit-parallel operation, has a four-input/two-output logical unit, two wires, and only 64 state bits in each AE <ref> [Bol93] </ref>. In the opposite direction, the Concurrent Logic FPGA AE, targeted specifically for computing rather than implementing random logic, has reduced routing resources, a simple logical unit, and is intended to be clocked at high rates [Fur93]. <p> Multiple Networks All SIMD array architectures to date have used a grid with a single wire between AEs, while FPGA AEs typically have access to at least four wires. The performance of SIMD machines, especially of the software bit-parallel <ref> [Bol93] </ref> variety, can be increased by adding extra intra-chip grid wires. Multiple wires are possible in many SIMD implementations because local routing channels to route additional signals between AEs take up little or no additional area in the SIMD array.
Reference: [BRV93] <author> Patrice Bertin, Didier Roncin, and Jean Vuillemin. </author> <title> Programmable Active Memories: A Performance Assessment. </title> <booktitle> In Research on Integrated Systems Symposium, </booktitle> <year> 1993. </year>
Reference-contexts: 1 Introduction FPGA-based custom computing engines and massively parallel SIMD arrays <ref> [Gea91, Bol93, BRV93] </ref> have been demonstrated to provide supercomputer-class performance on some tasks at a tiny fraction of supercomputer cost. Both of these architectures consist of arrays of small yet numerous processing elements.
Reference: [BTA93] <author> Jonathan Babb, Russell Tessier, and Anant Agarwal. </author> <title> Virtual Wires: Overcoming Pin Limitations in FPGA-based Logic Emulators. </title> <booktitle> In FPGA '93, </booktitle> <year> 1993. </year>
Reference-contexts: As a result of a limited number of I/O pins, only a small fraction of the AEs in each FPGA component is generally usable. A recent technique called virtual wires <ref> [BTA93] </ref>, has been used to overcome this limitation by sending different signals through the same pin at different points in time. This is another name for time-multiplexing I/O pins, an approach employed by SIMD computers for many years.
Reference: [Fur93] <author> F. Furtek. </author> <title> A Field-Programmable Gate Array for Systolic Computing. </title> <booktitle> In Research on Integrated Systems Symposium, </booktitle> <year> 1993. </year>
Reference-contexts: FPGAs are in wide use today for system customization and glue logic, low-volume application-specific designs, and IC prototyping. A variety of FPGAs are commercially available from a number of vendors (e.g. Xilinx [Inc91], Actel [Act90], At-mel <ref> [Fur93] </ref>, Lattice [Qui92]). Single-Instruction Multiple-Data (SIMD) arrays are employed to realize high throughput on many regular, computationally intensive data processing applications. An array of simple, fine-grained computational units make up most SIMD arrays. Typically, the computational units are wired together through local, nearest-neighbor communications. <p> Different AEs, however, can be executing different operations. During a slow programming phase, each AE is configured with its operation, which remain fixed during subsequent normal operation. In SRAM-programmable FPGAs (e.g. Xilinx LCA [Inc91], Atmel <ref> [Fur93] </ref>) this programming phase normally occurs once each time the system is powered on. In fuse or anti-fuse based FPGAs (e.g. Actel [Act90] ) a device is programmed exactly once during its lifetime. SIMD arrays weaken the model by distributing the same instruction to every AE (See Figure 4). <p> In the opposite direction, the Concurrent Logic FPGA AE, targeted specifically for computing rather than implementing random logic, has reduced routing resources, a simple logical unit, and is intended to be clocked at high rates <ref> [Fur93] </ref>. In the remainder of this section, we discuss these three differences in further detail. We note how the available technology influences implementations.
Reference: [Gea91] <author> Maya Gokhale and et al. </author> <title> Building and Using a Highly Parallel Programmable Logic Array. </title> <journal> IEEE Computer, </journal> <volume> 24(1) </volume> <pages> 81-89, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: 1 Introduction FPGA-based custom computing engines and massively parallel SIMD arrays <ref> [Gea91, Bol93, BRV93] </ref> have been demonstrated to provide supercomputer-class performance on some tasks at a tiny fraction of supercomputer cost. Both of these architectures consist of arrays of small yet numerous processing elements.
Reference: [HCC + 93] <author> Mark Horowitz, Andy Chan, Joe Co-brunson, Jim Gasbarro, Thomas Lee, Wing Leung, Wayne Richardson, Tim Thrush, and Yasuhiro Fujii. </author> <title> PLL Design for a 500MB/s Interface. </title> <booktitle> In ISSCC Digest of Technical Papers, </booktitle> <pages> pages 160-161. </pages> <publisher> IEEE, </publisher> <month> February </month> <year> 1993. </year>
Reference-contexts: 8 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA Characteristic FPGA SIMD Wires/direction 4-8 1 Routing Static Dynamic Logical unit Logical function 1-bit adder of 4-8 inputs Clock Slow Fast Arithmetic Bit-parallel Bit-serial Local state 1-2 bits 64-256 bits loop from a slow external clock <ref> [HCC + 93] </ref>. Also, a clocked system allows the use of low-power, clocked logic families such as dynamic logic. 5.4 Local State As discussed earlier, SIMD machines require considerable local state to store intermediate results, while FPGAs store these results on the wires connecting combinational blocks.
Reference: [Hil85] <author> D. Hillis. </author> <title> The Connection Machine. </title> <publisher> Distinguished Dissertations. The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: SIMD arrays are commonly used for algorithms requiring regular, data-parallel computations where identical operations must be performed on a large set of data. Typical applications for SIMD arrays include low-level vision and image processing, discrete particle simulation, database searches, and genetic sequence matching. NASA's MPP [Pot85], Thinking Machines' CM-2 <ref> [Hil85] </ref>, and MasPar's MP-1 [Bla90] are early examples of large-scale SIMD arrays. Increased silicon area along with advanced 1 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA packaging trends allow production of very high-performance, highly-integrated, SIMD arrays at reasonable costs [BBLC93], [Lea91]. <p> The multiple loaded contexts allow each DPGA AE to be reused to perform different functions during each time slice. Virtual Cells and Embedded Systems In analogy to the Virtual Processor concept used in the SIMD Connection Machine <ref> [Hil85] </ref> to map a large computation onto a small number of processors, we can also treat the DPGA as having many Virtual Cells per physical AE. At a given point in time one function is active at each AE.
Reference: [Inc91] <author> Xilinx Inc. </author> <title> The Programmable Gate Array Data Book. </title> <publisher> Xilinx, Inc., </publisher> <year> 1991. </year>
Reference-contexts: FPGAs are in wide use today for system customization and glue logic, low-volume application-specific designs, and IC prototyping. A variety of FPGAs are commercially available from a number of vendors (e.g. Xilinx <ref> [Inc91] </ref>, Actel [Act90], At-mel [Fur93], Lattice [Qui92]). Single-Instruction Multiple-Data (SIMD) arrays are employed to realize high throughput on many regular, computationally intensive data processing applications. An array of simple, fine-grained computational units make up most SIMD arrays. Typically, the computational units are wired together through local, nearest-neighbor communications. <p> Different AEs, however, can be executing different operations. During a slow programming phase, each AE is configured with its operation, which remain fixed during subsequent normal operation. In SRAM-programmable FPGAs (e.g. Xilinx LCA <ref> [Inc91] </ref>, Atmel [Fur93]) this programming phase normally occurs once each time the system is powered on. In fuse or anti-fuse based FPGAs (e.g. Actel [Act90] ) a device is programmed exactly once during its lifetime.
Reference: [Lea91] <author> R. M. Lea. </author> <title> Wafer-Scale Massively Parallel Computing Modules For Fault-Tolerant Signal and Data Processing. </title> <booktitle> In International Conference on Computer Architecture, </booktitle> <year> 1991. </year>
Reference-contexts: Increased silicon area along with advanced 1 FPGA '94 -- 2nd International ACM/SIGDA Workshop on FPGAs February 13-15, Berkeley, CA packaging trends allow production of very high-performance, highly-integrated, SIMD arrays at reasonable costs [BBLC93], <ref> [Lea91] </ref>. Viewed at an abstract level, these two array architectures are very similar. Both employ a moderately fine-grained array of logic elements.
Reference: [ML89] <author> M. Maresca and H. Li. </author> <title> Toward connection autonomy of fine-grain SIMD parallel architecture. </title> <booktitle> In Parallel Processing for Computer Vision and Display, </booktitle> <pages> pages 77-86. </pages> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: Several architectures allow local state bits to modify the transmitted instruction, the transmitted address, or the local network connections. This local modification is referred to as operational, address, and connection autonomy where autonomy indicates that processors have the ability to not perform identical operations <ref> [ML89] </ref>. The translation process is typically very simple: operational autonomy is achieved by potentially inverting the ALU output; addressing autonomy by modifying a few low order bits of the address [BDHR88]; connection autonomy by configuring a network crossbar at each AE site [ML89]. <p> have the ability to not perform identical operations <ref> [ML89] </ref>. The translation process is typically very simple: operational autonomy is achieved by potentially inverting the ALU output; addressing autonomy by modifying a few low order bits of the address [BDHR88]; connection autonomy by configuring a network crossbar at each AE site [ML89]. All of these techniques can be viewed as resource-limited implementations of the DPGA architecture. Each can be derived by starting with a small number of contexts and observing that the instructions in each context are almost identical except for a few bits.
Reference: [NPA92] <author> Rishiyur S. Nikhil, Gregory M. Pa-padopoulous, and Arvind. </author> <title> *T: A Mul-tithreaded Massively Parallel Architecture. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture. ACM, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: This has two benefits. First, the array can support multiple configurations for a single computation thread. Second, configuration contexts allow DPGA co-processors to support multiple threads. This capability will become more important for fine-grained, multithreaded microprocessors which support fast context switching (e.g. April [ALKK90], *T <ref> [NPA92] </ref>). 4.2 Costs From both the FPGA and SIMD array standpoint, the primary additional cost for the DPGA array is the area for the instruction store lookup table (See Figure 6).
Reference: [Pot85] <author> Jerry L. Potter, </author> <title> editor. The Massively Parallel Processor. </title> <publisher> MIT Press Series in Scientific Computation. The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1985. </year>
Reference-contexts: SIMD arrays are commonly used for algorithms requiring regular, data-parallel computations where identical operations must be performed on a large set of data. Typical applications for SIMD arrays include low-level vision and image processing, discrete particle simulation, database searches, and genetic sequence matching. NASA's MPP <ref> [Pot85] </ref>, Thinking Machines' CM-2 [Hil85], and MasPar's MP-1 [Bla90] are early examples of large-scale SIMD arrays.
Reference: [Qui92] <institution> QuickLogic Corporation, </institution> <address> 2933 Bunker Hill Lane, Santa Clara, CA 95054. </address> <month> QuickLogic: </month> <title> Very High Speed FPGAs, 1992. QuickLogic is now a subsidiary of Lattice Semiconductor Corporation. </title>
Reference-contexts: FPGAs are in wide use today for system customization and glue logic, low-volume application-specific designs, and IC prototyping. A variety of FPGAs are commercially available from a number of vendors (e.g. Xilinx [Inc91], Actel [Act90], At-mel [Fur93], Lattice <ref> [Qui92] </ref>). Single-Instruction Multiple-Data (SIMD) arrays are employed to realize high throughput on many regular, computationally intensive data processing applications. An array of simple, fine-grained computational units make up most SIMD arrays. Typically, the computational units are wired together through local, nearest-neighbor communications.
Reference: [Sil93] <author> Harvey F. Silverman. </author> <title> Processor Reconfiguration Through Instruction-Set Metamorphosis. </title> <journal> IEEE Computer, </journal> <volume> 26(3) </volume> <pages> 11-18, </pages> <month> March </month> <year> 1993. </year> <month> 10 </month>
Reference-contexts: Processor Assistance FPGAs also promise to be useful as tightly coupled co-processors for conventional microprocessors. The FPGA can be configured to perform some application-specific calculations more quickly than the processor <ref> [Sil93] </ref>. With the internal configuration cache, the DPGA array can switch between operations rapidly. This has two benefits. First, the array can support multiple configurations for a single computation thread. Second, configuration contexts allow DPGA co-processors to support multiple threads.
References-found: 19

