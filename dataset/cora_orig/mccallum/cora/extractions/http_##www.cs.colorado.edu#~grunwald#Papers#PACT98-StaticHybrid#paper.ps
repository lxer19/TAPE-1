URL: http://www.cs.colorado.edu/~grunwald/Papers/PACT98-StaticHybrid/paper.ps
Refering-URL: http://www.cs.colorado.edu/~grunwald/Papers/PACT98-StaticHybrid/
Root-URL: http://www.cs.colorado.edu
Title: Static Methods in Hybrid Branch Prediction  
Author: Dirk Grunwald, Donald Lindsay, and Benjamin Zorn 
Date: 1998  
Note: to appear in Proceedings of the International Conference on Parallel Architectures and Compilation Techniques (PACT)  
Address: Campus Box 430  Boulder, CO 803090430 USA  
Affiliation: Department of Computer Science  University of Colorado  
Abstract: Hybrid branch predictors combine the predictions of multiple single-level or two-level branch predictors. The prediction-combining hardware the meta-predictor may itself be large, complex and slow. We show that the combination function is better performed statically, using prediction hints in the branch instructions. The hints are set by profiling or static analysis. Although the meta-predictor is static, the actual predictions remain dynamic, so there is little risk of worst-case performance. An important advantage of our approach is that a branch site only causes interference within a single component predictor, reducing capacity demands. We argue that our proposal is implementable, and that it addresses the scaling issues currently facing hardware designers. We show that the static hybrid method we propose is more effective than existing techniques based on dynamic selection, and requires less hardware. For example, one result shows a conventional 4096-bit dynamic selection mechanism getting a 4.7% average miss rate, while our static approach gets 3.6%. These results are obtained with the Instruction Benchmark Suite (IBS), a realistic whole-system benchmark, and the SPECint95 suite, using realistic hardware sizes. All the results we present are based on a cross-validation methodology, in which the profile data used for static selection are based on training inputs that are entirely different from the inputs used to evaluate the performance of the technique. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P.-Y. Chang, E. Hao, and Y. Patt. </author> <title> Alternative implementations of hybrid branch predictors. </title> <booktitle> In 28th Intl. Symp. on Mi-croarchitecture, </booktitle> <pages> pages 252257, </pages> <year> 1995. </year>
Reference-contexts: The selection threshold was set to a 95% prediction accuracy. In other work, Chang and Banerjee [3] investigate the use of static hybrid methods in choosing between a new component predictor they propose (AVG) and another 2-level predictor. In later work, Chang, Hao and Patt <ref> [1] </ref> also search for good combinations of components. To reduce the design space, they used components of equal size and determined the effectiveness of each component for each branch site. This allowed a static computation of the miss rate of an idealized hybrid predictor. <p> Component Configurations In Section 2 we discussed several proposed dynamic hybrid predictors. From these, we chose to compare our static hybrid predictor with an instance of the McFarling dynamic hybrid predictor because the McFarling predictor is well known and published experimental results show that it works well <ref> [11, 1, 8] </ref> . Chang, Hao and Patt [1] reported that their best predictor combination was PAs [19] plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters. <p> From these, we chose to compare our static hybrid predictor with an instance of the McFarling dynamic hybrid predictor because the McFarling predictor is well known and published experimental results show that it works well [11, 1, 8] . Chang, Hao and Patt <ref> [1] </ref> reported that their best predictor combination was PAs [19] plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters.
Reference: [2] <author> P.-Y. Chang, E. Hao, T.-Y. Yeh, and Y. Patt. </author> <title> Branch classification: a new mechanism for improving branch predictor performance. </title> <booktitle> In 27th Intl. Symp. on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: In general, multiple component predictors can be used, and different designs can be used for the meta-predictor. Chang, Hao, Yeh and Patt proposed branch classification <ref> [2] </ref>, which used a static meta-predictor and a static prediction component along with a dynamic component. The static meta-predictor is used to select either the dynamic or static component. The static predictor component was set using an edge-profile, and the prediction was set to the most likely branch outcome. <p> With the static hybrid predictor, we propose that the dynamic selection mechanism used in dynamic hybrid branch predictors, such as McFar-ling's, be replaced by a static per-branch selection bit, much as in Branch Classification <ref> [2] </ref>. Like branch classification, we investigate statically choosing between different component predictors. However, in our model, both components are fully dynamic, and not as susceptible to worst-case effects caused by poor training inputs. <p> Above that size, the performance of the predictors converges, as we would expect it to. Comparison with Branch Classification We also investigated how a branch classification approach <ref> [2] </ref> might compare with the static hybrid we propose. With branch classification (specifically, the proposed PG+gshare technique), which also uses profiles, the branches that are observed to be statically predictable to a given threshold are predicted statically, while the rest are predicted dynamically. <p> With branch classification (specifically, the proposed PG+gshare technique), which also uses profiles, the branches that are observed to be statically predictable to a given threshold are predicted statically, while the rest are predicted dynamically. We used the same assignment threshold of 95% prediction accuracy used in <ref> [2] </ref>. Since one of their two components (the static hint) has no hardware cost, we made the gshare component used by the Branch Classification scheme twice as large as that used in our static-hybrid predictor.
Reference: [3] <author> Pohua Chang and Utpal Banerjee. </author> <title> Profile-guided multi-heuristic branch prediction. </title> <booktitle> In 1995 Intl. Conf. on Parallel Processing, </booktitle> <pages> pages 215218, </pages> <year> 1995. </year>
Reference-contexts: We compare our static-hybrid method to branch classification using one of the most successful hybrid predictors proposed, PG+gshare, which combines a static prediction and a gshare predictor. The selection threshold was set to a 95% prediction accuracy. In other work, Chang and Banerjee <ref> [3] </ref> investigate the use of static hybrid methods in choosing between a new component predictor they propose (AVG) and another 2-level predictor. In later work, Chang, Hao and Patt [1] also search for good combinations of components. <p> Furthermore, as results in Section 5 suggest, use of branch classification requires establishing a careful balance between the dynamic component and the threshold for the static component. While Chang and Bannerjee <ref> [3] </ref> mentioned the static selection of components in their paper, the focus of their analysis was on the advantage of their AVG branch predictor, and they did not indicate any advantage to the use of a static meta-predictor.
Reference: [4] <author> Thomas M. Conte, Burzin A. Patel, and J. Stan Cox. </author> <title> Using branch handling hardware to support profile driven optimization. </title> <booktitle> In 27th Intl. Symp. on Microarchitecture, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: This profile operation is somewhat more complex than standard basic block or instruction count profiles, since a simulation of all the alternative component predictors must be performed. However, this information could easily be gathered using on-line performance tools <ref> [5, 4] </ref> that provide stochastic instruction sampling, or, as done for this paper, using binary instrumentation [15]. Since the component predictors are selected statically, we only need to update the component predictor that is actually used. This reduces the interference from capacity misses or aliasing in the different components.
Reference: [5] <author> Jeffrey Dean, James E. Hicks, Carl A. Waldspurger, William E. Weihl, and George Chrysos. ProfileMe: </author> <title> Hardware support for instruction-level profiling on out-of-order processors. </title> <booktitle> In 30th Annual Intl. Symp. on Microarchitecture. IEEE, </booktitle> <month> December </month> <year> 1997. </year>
Reference-contexts: This profile operation is somewhat more complex than standard basic block or instruction count profiles, since a simulation of all the alternative component predictors must be performed. However, this information could easily be gathered using on-line performance tools <ref> [5, 4] </ref> that provide stochastic instruction sampling, or, as done for this paper, using binary instrumentation [15]. Since the component predictors are selected statically, we only need to update the component predictor that is actually used. This reduces the interference from capacity misses or aliasing in the different components. <p> When using binary instrumentation, we can use multiple iterations of the training process to refine the static assignment. We found this improved performance slightly, but was not essential. With online tools, such as ProfileMe <ref> [5] </ref>, we would expect that continuous sampling and re-optimization would have a similar effect. All of our results are shown without this improvement.
Reference: [6] <author> Joel Emer and Nikolas Gloy. </author> <title> A language for describing predictors and its application to automatic synthesis. </title> <booktitle> In 24th Annual Intl. Symp. of Computer Architecture, </booktitle> <pages> pages 304314, </pages> <year> 1997. </year>
Reference-contexts: Recently there has been interest in hybrid branch predictors. The fundamental idea here is that different predictor schemes have different advantages. Hence, a combination of basic schemes might (at some cost) obtain all of their advantages. The upcoming Alpha 21264 <ref> [6, 9] </ref> uses a large hybrid predictor. This paper proposes that the prediction-combining hardware, or selection mechanism, be replaced by a static choice encoded in the branch instruction. We feel this paper makes the following contributions. <p> The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 [18] uses a bimodal [11] predictor of 512 counters. The Alpha 21164 [7] uses 2048 counters in the same organization: the Alpha 21264 <ref> [9, 6] </ref> contains four arrays, with 1024 or 4096 entries each.
Reference: [7] <author> John H. Edmondson et al. </author> <title> Internal organization of the Alpha 21164, a 300 Mhz 64-bit quad-issue CMOS RISC microprocessor. </title> <journal> Digital Technical Journal, </journal> <volume> 7(1):119135, </volume> <year> 1995. </year>
Reference-contexts: The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 [18] uses a bimodal [11] predictor of 512 counters. The Alpha 21164 <ref> [7] </ref> uses 2048 counters in the same organization: the Alpha 21264 [9, 6] contains four arrays, with 1024 or 4096 entries each.
Reference: [8] <author> M. Evers, P.-Y. Chang, and Y. Patt. </author> <title> Using hybrid branch predictors to improve branch prediction accuracy in the presence of context switches. </title> <booktitle> In 23nd Annual Intl. Symp. of Computer Architecture, </booktitle> <pages> pages 311, </pages> <year> 1996. </year>
Reference-contexts: They also suggest that some predictors have a quicker warm-up time (i.e., a smaller training cost). In a more recent paper Evers, Chang and Patt <ref> [8] </ref> consider a larger selection of component predictors, and propose a hybrid predictor with six, rather than two, components. This multi-hybrid con tains unequal sized elements, in the expectation that some will warm-up or train more quickly after context switches. <p> Component Configurations In Section 2 we discussed several proposed dynamic hybrid predictors. From these, we chose to compare our static hybrid predictor with an instance of the McFarling dynamic hybrid predictor because the McFarling predictor is well known and published experimental results show that it works well <ref> [11, 1, 8] </ref> . Chang, Hao and Patt [1] reported that their best predictor combination was PAs [19] plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters.
Reference: [9] <author> Linley Gwennap. </author> <title> Digital 21264 sets new standard. </title> <type> Microprocessor Report, </type> <year> 1997. </year> <note> Article available online at http://www.digital.com/semiconductor/microrep/digital2.htm. </note>
Reference-contexts: Recently there has been interest in hybrid branch predictors. The fundamental idea here is that different predictor schemes have different advantages. Hence, a combination of basic schemes might (at some cost) obtain all of their advantages. The upcoming Alpha 21264 <ref> [6, 9] </ref> uses a large hybrid predictor. This paper proposes that the prediction-combining hardware, or selection mechanism, be replaced by a static choice encoded in the branch instruction. We feel this paper makes the following contributions. <p> The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 [18] uses a bimodal [11] predictor of 512 counters. The Alpha 21164 [7] uses 2048 counters in the same organization: the Alpha 21264 <ref> [9, 6] </ref> contains four arrays, with 1024 or 4096 entries each.
Reference: [10] <author> Chih-Chieh Lee, I-Cheng K. Chen, and Trevor N. Mudge. </author> <title> The bi-mode branch predictor. </title> <booktitle> In 30th Intl. Symp. on Mi-croarchitecture. IEEE, </booktitle> <year> 1997. </year> <note> (to appear). </note>
Reference-contexts: Because our method allows components to be updated selectively, we outperform the McFarling predictor in almost all cases because the partial update reduces the load on each of the dynamic components, effectively increasing their capacity and thus performance. A similar effect was noted in the bimode predictor <ref> [10] </ref>. 4 Experimental Design In this section, we discuss the experimental design that we used to obtain the results in the following section.
Reference: [11] <author> Scott McFarling. </author> <title> Combining branch predictors. </title> <type> Technical Report TN-36, </type> <institution> Digital Equipment Corporation, Western Research Lab, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: A BHT is usually organized as a 2-way or 4-way associative structure. Throughout this paper, we will use a PAs branch history table that uses tags. McFarling <ref> [11] </ref> introduced the idea of hybrid predictors. He describes a series of simple predictors, most notably gshare. In the gshare predictor the BHR and program counter are mixed using an exclusive-or to index the pattern history table (PHT), a table of 2-bit saturating counters. <p> Component Configurations In Section 2 we discussed several proposed dynamic hybrid predictors. From these, we chose to compare our static hybrid predictor with an instance of the McFarling dynamic hybrid predictor because the McFarling predictor is well known and published experimental results show that it works well <ref> [11, 1, 8] </ref> . Chang, Hao and Patt [1] reported that their best predictor combination was PAs [19] plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters. <p> Chang, Hao and Patt [1] reported that their best predictor combination was PAs [19] plus gshare <ref> [11] </ref>. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters. Based on this prior work, we chose a PAs predictor with a 256-entry BHT, plus a one-dimensional array of 512 2-bit SPECint95 Suites counters. <p> We chose a gshare predictor with 2 11 counters, in a 2 7 fi2 4 organization, meaning that the 2-level predictor is organized to use 7 bits from the program counter and have a 4-bit BHR. Following McFarling's suggestions <ref> [11] </ref>, the McFarling combining predictor has the same number of counters as the gshare component (2048 counters). The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 [18] uses a bimodal [11] predictor of 512 counters. <p> Following McFarling's suggestions <ref> [11] </ref>, the McFarling combining predictor has the same number of counters as the gshare component (2048 counters). The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 [18] uses a bimodal [11] predictor of 512 counters. The Alpha 21164 [7] uses 2048 counters in the same organization: the Alpha 21264 [9, 6] contains four arrays, with 1024 or 4096 entries each.
Reference: [12] <author> Pierre Michaud, Andre Seznec, and Richard Uhlig. </author> <title> Trading conflict and capacity aliasing in conditional branch predictors. </title> <booktitle> In 24th Annual Intl. Symp. of Computer Architecture, </booktitle> <pages> pages 292303, </pages> <year> 1997. </year>
Reference-contexts: Due to the complexity of the design, the selection mechanism used may introduce timing constraints, although the authors suggest ways to ameliorate such effects. Recently, a different approach to dynamic hybrids has been proposed by Michaud, Seznec and Uhlig <ref> [12] </ref>. Their gskew predictor uses three component predictors, but does not have a meta-predictor. Instead, selection is done by majority vote. The components use different hash functions to combine the branch history register and program counter address.
Reference: [13] <author> Parthasarathy Ranganathan and Norman Jouppi. </author> <title> The relative impact of memory latency, bandwidth and branch limits to microprosssor performance. Presented at 1st. Workshop on Mixing Logic and DRAM: Chips that Compute and Remember, </title> <booktitle> held in conjunction with the 1997 Intl. Symp. on Computer Architecture, </booktitle> <address> Denver, Colorado, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Pipeline stalls on a modern wide issue processor introduce a large lost-opportunity cost, meaning that several instructions were not issued, and branch recovery mechanisms are difficult to implement because they typically require checkpointing of considerable resources. Ranganathan and Jouppi <ref> [13] </ref> argue that branch prediction will be the most limiting aspect of computer architectures, surpassing the limitations of memory systems by 2010.
Reference: [14] <author> S. Sechrest, C.-C. Lee, and T. Mudge. </author> <title> Correlation and aliasing in dynamic branch predictors. </title> <booktitle> In 23nd Annual Intl. Symp. of Computer Architecture, </booktitle> <pages> pages 2232, </pages> <year> 1996. </year>
Reference-contexts: This allowed a static computation of the miss rate of an idealized hybrid predictor. They argued that this ideal is not perfect since it does not adapt to program dynamics. However, the method allowed them to choose the best pairing, which they determined to usually be gshare plus PAs <ref> [14, 19] </ref>. Like McFarling, they argue that the pairing is successful because gshare does well on site correlation, and PAs does well on successive uses of one site. They also suggest that some predictors have a quicker warm-up time (i.e., a smaller training cost). <p> Chang, Hao and Patt [1] reported that their best predictor combination was PAs [19] plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge <ref> [14] </ref> report the most effective ratios between the parameters. Based on this prior work, we chose a PAs predictor with a 256-entry BHT, plus a one-dimensional array of 512 2-bit SPECint95 Suites counters. Further, we chose a BHT with 4-way associativity, and an update on taken update policy.
Reference: [15] <author> Amitabh Srivastava and Alan Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings of the SIGPLAN'94 Conf. on Programming Language Design and Implementation, pages 196205. ACM, </booktitle> <year> 1994. </year>
Reference-contexts: However, this information could easily be gathered using on-line performance tools [5, 4] that provide stochastic instruction sampling, or, as done for this paper, using binary instrumentation <ref> [15] </ref>. Since the component predictors are selected statically, we only need to update the component predictor that is actually used. This reduces the interference from capacity misses or aliasing in the different components.
Reference: [16] <institution> Standard Performance Evaluation Corportation, Manassas, VA. </institution> <note> SPEC95 Technical Manual, </note> <month> August </month> <year> 1995. </year>
Reference-contexts: Benchmark Suites To evaluate and compare the performance of our static hybrid predictor with that of the McFar-ling predictor, we chose two benchmark suites often used in branch prediction studies: the SPECint95 suite <ref> [16] </ref> and the the Ultrix portion of the IBS traces [17]. We chose to focus on the integer portion of SPEC95 in our evaluation because many of the branches in the SPECfp95 benchmarks are highly predictable and less interesting for the purpose of evaluating branch prediction hardware.
Reference: [17] <author> R. Uhlig, D. Nagle, T. Mudge, S. Sechrest, and J. Emer. </author> <title> Instruction fetching: Coping with code bloat. </title> <booktitle> In 22nd Annual Intl. Symp. of Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: Benchmark Suites To evaluate and compare the performance of our static hybrid predictor with that of the McFar-ling predictor, we chose two benchmark suites often used in branch prediction studies: the SPECint95 suite [16] and the the Ultrix portion of the IBS traces <ref> [17] </ref>. We chose to focus on the integer portion of SPEC95 in our evaluation because many of the branches in the SPECfp95 benchmarks are highly predictable and less interesting for the purpose of evaluating branch prediction hardware.
Reference: [18] <author> Kenneth C. Yeager. </author> <title> The MIPS R10000 superscalar processor. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 2840, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: Following McFarling's suggestions [11], the McFarling combining predictor has the same number of counters as the gshare component (2048 counters). The hardware configurations we have chosen for evaluation purposes are similar to current practice in production microprocessors. For instance, the MIPS R10000 <ref> [18] </ref> uses a bimodal [11] predictor of 512 counters. The Alpha 21164 [7] uses 2048 counters in the same organization: the Alpha 21264 [9, 6] contains four arrays, with 1024 or 4096 entries each.
Reference: [19] <author> Tse-Yu Yeh and Yale N. Patt. </author> <title> Alternative implementations of two-level adaptive branch prediction. </title> <booktitle> In Proc. 19th Annual Symp. on Computer Architecture, </booktitle> <pages> pages 124134, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In Section 6 we summarize and discuss some possibilities for future work. 2 Previous Work Because branch prediction is such an important part of modern architectures, there is an active and growing body of research in the area, to the point where a 1992 taxonomy <ref> [19] </ref> is already becoming dated. Branch predictors generally use one or more of the program counter, recent local branch history, and recent global branch history, to index into an array of up/down saturating counters. The actual prediction is based on whether the counter contains a high or low value. <p> A branch history register (BHR) is a shift register representing recent branch outcomes where one bit is updated as each branch commits. The gshare predictor uses a single global branch history, but other techniques, such as PAs or SAg predictors <ref> [19] </ref> use a number of branch history registers organized as a branch history table (BHT), and subdivide the branch stream to these different BHR's. <p> This allowed a static computation of the miss rate of an idealized hybrid predictor. They argued that this ideal is not perfect since it does not adapt to program dynamics. However, the method allowed them to choose the best pairing, which they determined to usually be gshare plus PAs <ref> [14, 19] </ref>. Like McFarling, they argue that the pairing is successful because gshare does well on site correlation, and PAs does well on successive uses of one site. They also suggest that some predictors have a quicker warm-up time (i.e., a smaller training cost). <p> Chang, Hao and Patt [1] reported that their best predictor combination was PAs <ref> [19] </ref> plus gshare [11]. Both of these predictors have several parameters, but papers such as Sechrest, Lee and Mudge [14] report the most effective ratios between the parameters.
References-found: 19

