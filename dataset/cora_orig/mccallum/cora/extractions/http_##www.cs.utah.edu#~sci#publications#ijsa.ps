URL: http://www.cs.utah.edu/~sci/publications/ijsa.ps
Refering-URL: http://www.cs.utah.edu/~sci/projects/comp_apps.html
Root-URL: 
Email: E-Mail: (crj,beazley,ylivnat,sparker,jas,hwshen,dweinste)@cs.utah.edu  
Title: Applications of Large-Scale Computing and Scientific Visualization in Medicine  
Author: C.R. Johnson, D.M. Beazley, Y. Livnat, S.G. Parker, J.A. Schmidt, H.W. Shen, and D.M. Weinstein 
Web: Web: http://www.cs.utah.edu/  
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science, University of Utah,  
Note: The authors are with the  ~sci/ November 6, 1995 DRAFT  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> C.E. Miller and C.S. Henriquez. </author> <title> Finite element analysis of bioelectric phenomena. </title> <journal> Crit. Rev. in Biomed. Eng., </journal> <volume> 18 </volume> <pages> 181-205, </pages> <year> 1990. </year>
Reference: [2] <author> T.C. Pilkington, B. Loftis, J.F. Thompson, S.L-Y. Woo, T.C. Palmer, and T.F. Budinger. </author> <title> High-Performance Computing in Biomedical Research. </title> <publisher> CRC Press, </publisher> <address> Boca Raton, </address> <year> 1993. </year>
Reference: [3] <author> H.W. Shen and C.R. Johnson. </author> <title> Semi-automatic image segmentation: A bimodel thresholding approach. </title> <type> Technical report, </type> <institution> UUCS-94-019, Dept. of Computer Science, Univ. of Utah, </institution> <year> 1994. </year>
Reference: [4] <author> D. Weinstein and C.R. Johnson. </author> <title> Hierarchical data structures for interactive volume visualization. </title> <journal> IEEE Visualization `95, </journal> <note> 1995 (submitted). </note>
Reference: [5] <author> J.A. Schmidt, C.R. Johnson, J.C. Eason, </author> <title> and R.S. MacLeod. Applications of automatic mesh generation and adaptive methods in computational medicine. </title> <editor> In I. Babuska, J.E. Flaherty, W.D. Henshaw, J.E. Hopcroft, J.E. Oliger, and T. Tezduyar, editors, </editor> <title> Modeling, Mesh Generation, and Adaptive Methods for Partial Differential Equations, </title> <address> pages 367-390. </address> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: To classify a tetrahedron we had to localize the position of its centroid relative to the surfaces separating regions of different conductivity. To localize the element, we utilized a ray tracing approach <ref> [5] </ref> that projects a ray from the centroid of an element to a point at infinity and counts the instances the ray intersects with a triangulated surface. <p> in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], <ref> [5] </ref>, [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. Because our work involves large-scale problems, much of the software we have developed has been designed to take advantage of parallel processing via high performance architectures.
Reference: [6] <author> D.F. Watson. </author> <title> Computing the n-dimensional Delaunay tesselation with applications to Voronoi polytopes. </title> <journal> Computer Journal, </journal> <volume> 24(2) </volume> <pages> 167-172, </pages> <year> 1981. </year>
Reference: [7] <editor> J. Thompson and N.P. Weatherill. Structed and unstructed grid generation. In T.C. Pilk-ington, B. Loftis, J.F. Thompson, S.L-Y. Woo, T.C. Palmer, and T.F. Budinger, editors, </editor> <booktitle> High-Performance Computing in Biomedical Research, </booktitle> <pages> pages 63-112. </pages> <publisher> CRC Press, </publisher> <address> Boca Raton, </address> <year> 1993. </year>
Reference: [8] <author> W. Wells, R. Kikinis, W. Grimson, and F. Jolesz. </author> <title> Statistical intensity correlation and segmentation of magnetic resonance image data. </title> <booktitle> In Proceedings of the Third Conference on Visualization in Biomedical Computing. SPIE, </booktitle> <year> 1994. </year>
Reference: [9] <author> H.W. Shen and C.R. Johnson. Sweeping simplices: </author> <title> A fast isosurface extraction algorithm for unstructured grids. In Visualization `95. </title> <note> IEEE Press, 1995 (to appear). </note>
Reference: [10] <author> C.R. Johnson, R.S. MacLeod, and P.R. Ershler. </author> <title> A computer model for the study of electrical current flow in the human thorax. </title> <journal> Computers in Biology and Medicine, </journal> <volume> 22(3) </volume> <pages> 305-323, </pages> <year> 1992. </year>
Reference-contexts: November 6, 1995 DRAFT 8 III. Computation Over the past several years we have developed several programs to solve computationally intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes <ref> [10] </ref>, [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> computationally intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes <ref> [10] </ref>, [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> surface and volumetric meshes <ref> [10] </ref>, [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. Because our work involves large-scale problems, much of the software we have developed has been designed to take advantage of parallel processing via high performance architectures. One area of recent research involves adaptively refining large-scale unstructured finite element meshes to improve solution accuracy. A. <p> We have developed a defibrillation modeling system that allows a researcher to interactively design and place electrodes into a high-resolution finite element model of the human thorax (the Utah Torso Model <ref> [10] </ref>, [14]). The advantages of this system are that different electrode combinations can be tested computationally, reserving only the most promising candidates for animal studies. Reducing the number of experiments, could result in tremendous savings in device design and development costs. <p> The advantages of this system are that different electrode combinations can be tested computationally, reserving only the most promising candidates for animal studies. Reducing the number of experiments, could result in tremendous savings in device design and development costs. B.1 Model Construction and Electrode Design The Utah Torso Model <ref> [10] </ref>, [11] was derived from MRI scans and digitized to provide the boundary points defining the major electrically relevant anatomical regions including subcutaneous fat, skeletal muscle, lungs, ribs (clavicle and sternum), epicardium, epicardial fat pads, blood cavities (atria and ventricles), and the major blood vessels (pulmonary artery and vein, aorta, superior
Reference: [11] <author> C.R. Johnson, R.S. MacLeod, and M.A. Matheson. </author> <title> Computational medicine: Bioelectric field problems. </title> <booktitle> IEEE COMPUTER, </booktitle> <pages> pages 59-67, </pages> <month> Oct., </month> <year> 1993. </year>
Reference-contexts: November 6, 1995 DRAFT 8 III. Computation Over the past several years we have developed several programs to solve computationally intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], <ref> [11] </ref>, [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> November 6, 1995 DRAFT 11 IV. Visualization Visualization systems play an integral role in all phases of computational modeling, and we have developed a variety of application programs based on several common visualization paradigms <ref> [11] </ref>, [12], [13], [23]. For high quality ray-traced images, we have developed graphics systems which exploit parallel processing and/or clusters of workstations. <p> Reducing the number of experiments, could result in tremendous savings in device design and development costs. B.1 Model Construction and Electrode Design The Utah Torso Model [10], <ref> [11] </ref> was derived from MRI scans and digitized to provide the boundary points defining the major electrically relevant anatomical regions including subcutaneous fat, skeletal muscle, lungs, ribs (clavicle and sternum), epicardium, epicardial fat pads, blood cavities (atria and ventricles), and the major blood vessels (pulmonary artery and vein, aorta, superior and
Reference: [12] <author> R.S. MacLeod, C.R. Johnson, and M.A. Matheson. </author> <title> Visualization tools for computational electrocardiography. </title> <booktitle> In Visualization in Biomedical Computing, </booktitle> <pages> pages 433-444, </pages> <year> 1992. </year>
Reference-contexts: November 6, 1995 DRAFT 8 III. Computation Over the past several years we have developed several programs to solve computationally intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], <ref> [12] </ref>, [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> November 6, 1995 DRAFT 11 IV. Visualization Visualization systems play an integral role in all phases of computational modeling, and we have developed a variety of application programs based on several common visualization paradigms [11], <ref> [12] </ref>, [13], [23]. For high quality ray-traced images, we have developed graphics systems which exploit parallel processing and/or clusters of workstations.
Reference: [13] <author> R.S. MacLeod, C.R. Johnson, and M.A. Matheson. </author> <title> Visualization of cardiac bioelectricity | a case study. </title> <booktitle> In IEEE Visualization `92, </booktitle> <pages> pages 411-418, </pages> <year> 1992. </year>
Reference-contexts: Computation Over the past several years we have developed several programs to solve computationally intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], [12], <ref> [13] </ref>; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> November 6, 1995 DRAFT 11 IV. Visualization Visualization systems play an integral role in all phases of computational modeling, and we have developed a variety of application programs based on several common visualization paradigms [11], [12], <ref> [13] </ref>, [23]. For high quality ray-traced images, we have developed graphics systems which exploit parallel processing and/or clusters of workstations.
Reference: [14] <author> C.R. Johnson, R.S. MacLeod, and M.A. Matheson. </author> <title> Computer simulations reveal complexity of electrical activity in the human thorax. </title> <journal> Comp. in Physics, </journal> <volume> 6(3) </volume> <pages> 230-237, </pages> <month> May/June </month> <year> 1992. </year>
Reference-contexts: intensive problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], <ref> [14] </ref>, [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. <p> We have developed a defibrillation modeling system that allows a researcher to interactively design and place electrodes into a high-resolution finite element model of the human thorax (the Utah Torso Model [10], <ref> [14] </ref>). The advantages of this system are that different electrode combinations can be tested computationally, reserving only the most promising candidates for animal studies. Reducing the number of experiments, could result in tremendous savings in device design and development costs.
Reference: [15] <author> C.R. Johnson and R.S. MacLeod. </author> <title> Nonuniform spatial mesh adaption using a posteriori error estimates: applications to forward and inverse problems. </title> <journal> Applied Numerical Mathematics, </journal> <volume> 14 </volume> <pages> 311-326, </pages> <year> 1994. </year>
Reference-contexts: problems in medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], <ref> [15] </ref>, [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. Because our work involves large-scale problems, much of the software we have developed has been designed to take advantage of parallel processing via high performance architectures.
Reference: [16] <author> J.A. Schmidt, C.R. Johnson, J.C. Eason, </author> <title> and Macleod R.S. Modeling, Mesh Generation, and Adaptive Methods for Partial Differential Equations, chapter Applications of Automatic Mesh Generation and Adaptive Methods in Computational Medicine. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year> <month> November 6, </month> <note> 1995 DRAFT 32 </note>
Reference-contexts: medicine (focusing primarily on bioelectric field problems), such as (1) interactive programs to construct, manipulate, and display large scale, three-dimensional surface and volumetric meshes [10], [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], <ref> [16] </ref>; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], [17]. Because our work involves large-scale problems, much of the software we have developed has been designed to take advantage of parallel processing via high performance architectures.
Reference: [17] <author> C.R. Johnson and R.S. MacLeod. </author> <title> Inverse solutions for electric and potential field imaging. In R.L. </title> <editor> Barbour and M.J. Carvlin, editors, </editor> <title> Physiological Imaging, Spectroscopy, and Early Detection Diagnostic Methods, </title> <booktitle> volume 1887, </booktitle> <pages> pages 130-139. SPIE, </pages> <year> 1993. </year>
Reference-contexts: and volumetric meshes [10], [11], [12], [13]; (2) three-dimensional, adaptive, finite element programs for solving partial differential equations with general boundary conditions and source terms [10], [14], [15], [5], [16]; and (3) programs that solve the resulting large system of equations and simultaneously apply regularization to ill-posed inverse problems [10], <ref> [17] </ref>. Because our work involves large-scale problems, much of the software we have developed has been designed to take advantage of parallel processing via high performance architectures. One area of recent research involves adaptively refining large-scale unstructured finite element meshes to improve solution accuracy. A. <p> Regularization schemes work by finding an approximate vector, ~ C" , such that ~ C" ! ~ H as " ! 0, where " is the error between the approximate and true values. As introduced in a previous work <ref> [17] </ref>, we found that it was possible to reformulate the K matrix into an expression involving sub-matrices of K such that regularization can then be applied at a local level. Researchers have long noticed that the discontinuities in the inverse solution appear irregularly distributed throughout the data.
Reference: [18] <author> P.G. Ciarlet and J.L Lions. </author> <title> Handbook of Numerical Analysis: Finite Element Methods, volume 1. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: The novel aspect of our approach is that it uses local approximations of the error in the numerical solutions to drive an automatic adaptive mesh refinement. The basis of the error estimations comes from recent research on the finite element method <ref> [18] </ref>. The essential feature of the finite element method is that the approximate forms of the scalar field satisfies the governing equation in each element in some weighted sense.
Reference: [19] <author> D. Burnett. </author> <title> Finite Element Analysis- From Concepts to Applications. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1987. </year>
Reference-contexts: If we reduce the size of the elements, h-refinement, or increase the order of the basis function, p-refinement, we increase the accuracy of the approximation <ref> [19] </ref>. While either approach can be applied globally, computational limits make it more efficient to apply refinement locally, to regions where it is deemed most beneficial.
Reference: [20] <author> R. Lewis, H. Huang, A. Usmani, and J. </author> <title> Cross. Finite element analysis of heat transfer and flow problems using adaptive remeshing including application to solidification problems. </title> <journal> International Journal of Numerical Methods in Engineering, </journal> <volume> 32 </volume> <pages> 767-781, </pages> <year> 1991. </year>
Reference-contexts: Improvements via an adaptive h-refinement technique can be impelmented by using an estimate of the element energy error, such as the one described here derived from methods suggested by Lewis <ref> [20] </ref>. <p> The smoothed current densities are now defined at the same nodal locations as the potentials and are continuous across element boundaries. The smoothing process uses the Galerkin technique <ref> [20] </ref> and involves minimizing the difference between the two current densities, (r ~ r ^ ) (8) where r ~ is the smoothed gradient representing the exact gradient and r ^ is the constant gradient from the finite element solution. <p> Once the energy error has been computed, the mesh refinement can begin. The error must be related to some parameter which guides the mesh refinement. We utilized a spacing function, h e , which controls the size and number of elements <ref> [20] </ref>. This spacing function, h e is defined to be the linear interpolation of the spacing values at the four nodes of the tetrahedron.
Reference: [21] <author> O. Zienkiewicz and J. Zhu. </author> <title> A simple error estimator and adaptive procedure for practical engineering analysis. </title> <journal> International Journal of Numerical Methods in Engineering, </journal> <volume> 24 </volume> <pages> 337-357, </pages> <year> 1987. </year>
Reference-contexts: The error norm is defined as: ke q k = ( 1 Zienkiewicz <ref> [21] </ref>, [22] has shown that Z (r) T (r ^ )d = Z (r ^ ) T (r ^ )d: (4) Using this result, (3) becomes ke q k 2 = Z (r ^ ) T (r ^ )d (5) or equivalently, ke q k 2 = kqk 2 k^qk 2
Reference: [22] <author> O. Zienkiewicz and J. Zhu. </author> <title> Adaptivity and mesh generation. </title> <journal> International Journal of Numerical Methods in Engineering, </journal> <volume> 32 </volume> <pages> 783-810, </pages> <year> 1991. </year>
Reference-contexts: The error norm is defined as: ke q k = ( 1 Zienkiewicz [21], <ref> [22] </ref> has shown that Z (r) T (r ^ )d = Z (r ^ ) T (r ^ )d: (4) Using this result, (3) becomes ke q k 2 = Z (r ^ ) T (r ^ )d (5) or equivalently, ke q k 2 = kqk 2 k^qk 2 (6) <p> Zienkiewicz showed that when the current densities arising from the gradients are globally smoothed, they provide a more November 6, 1995 DRAFT 10 accurate estimate of the energy than the energy using the constant current densities.[21], <ref> [22] </ref> Hence, (7) can be calculated using the energy due to the smoothed currents and the energy due to the constant currents. The smoothed current densities are now defined at the same nodal locations as the potentials and are continuous across element boundaries.
Reference: [23] <author> R.S. MacLeod, C.R. Johnson, and M.A. Matheson. </author> <title> Visualizing bioelectric fields. </title> <journal> IEEE Computer Graphics and Applications, </journal> <pages> pages 10-12, </pages> <year> 1993. </year>
Reference-contexts: November 6, 1995 DRAFT 11 IV. Visualization Visualization systems play an integral role in all phases of computational modeling, and we have developed a variety of application programs based on several common visualization paradigms [11], [12], [13], <ref> [23] </ref>. For high quality ray-traced images, we have developed graphics systems which exploit parallel processing and/or clusters of workstations.
Reference: [24] <author> H.W. Shen and C.R. Johnson. </author> <title> Differential volume rendering: A fast alogrithm for scalar field animation. </title> <booktitle> In Visualization `94, </booktitle> <pages> pages 180-187. </pages> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: We have recently developed differential volume rendering and isosurface extraction techniques that allows nearly real-time rendering of three-dimensional flows and isosurface extraction for models with hundreds of thousands or millions of elements <ref> [24] </ref>, [25], [26]. A. Isosurface extraction Isosurface extraction is a powerful tool for investigating volumetric scalar fields. The position of an isosurface, as well as its relation to other neighboring isosurfaces, can provide clues to the underlying structure of the scalar field.
Reference: [25] <author> H. Shen and C. R. Johnson. Sweeping simplicies: </author> <title> A fast iso-surface extraction algorithm for unstructured grids. </title> <booktitle> Proceedings of Visualization '95, </booktitle> <year> 1995. </year> <note> (to appear). </note>
Reference-contexts: We have recently developed differential volume rendering and isosurface extraction techniques that allows nearly real-time rendering of three-dimensional flows and isosurface extraction for models with hundreds of thousands or millions of elements [24], <ref> [25] </ref>, [26]. A. Isosurface extraction Isosurface extraction is a powerful tool for investigating volumetric scalar fields. The position of an isosurface, as well as its relation to other neighboring isosurfaces, can provide clues to the underlying structure of the scalar field. <p> November 6, 1995 DRAFT 12 Our research primarily involves unstructured geometry and often requires interactive investigation of large data sets. Toward this goal we have developed two rapid isosurface extraction algorithms that work for both structured and unstructured grids. The Sweeping Simplices <ref> [25] </ref> method accelerates the search by utilizing coherence between isosurfaces with close isovalues.
Reference: [26] <author> Y Livnat, H. Shen, and C. R. Johnson. </author> <title> A near optimal isosurface extraction algorithm for structured and unstructured grids. </title> <journal> IEEE Trans. Vis. Comp. Graphics, </journal> <year> 1995. </year> <month> (submited). </month>
Reference-contexts: We have recently developed differential volume rendering and isosurface extraction techniques that allows nearly real-time rendering of three-dimensional flows and isosurface extraction for models with hundreds of thousands or millions of elements [24], [25], <ref> [26] </ref>. A. Isosurface extraction Isosurface extraction is a powerful tool for investigating volumetric scalar fields. The position of an isosurface, as well as its relation to other neighboring isosurfaces, can provide clues to the underlying structure of the scalar field. <p> A neighboring isosurface can be found quickly by consulting the two sorted lists and sweeping through these flags. Recently we developed a new representation for the underlying domain of the isosurface extraction problem, which we termed the Span Space <ref> [26] </ref>. Traditionally, the data cells of unstructured grids were viewed as intervals over the real line, R, where the extrema of the interval are the minimum and maximum values attained in the cell. Isosurface extraction was then viewed as locating those intervals that include the given isovalue. <p> The isosurface extraction is achieved by locating all the points which are confined to the semi-infinite rectangle (1; v) fi (v; 1), where v is the given isovalue. Posing the isosurface extraction problem over the Span Space, we developed a Near Optimal IsoSurface Extraction algorithm (NOISE) <ref> [26] </ref>, which has a worst case performance of only O ( p n + k). Considering the fact that a typical isosurface intersects O (n 2 3 ) cells, this algorithm is clearly optimal in the general case.
Reference: [27] <author> W.E. Lorensen and H. E. Cline. </author> <title> Marching cubes: A high resolution 3d surface construction algorithm. </title> <journal> Computer Graphics, </journal> <volume> 21(4) </volume> <pages> 163-169, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: While medical imaging data is provided at structured grid positions, scientific data sets frequently consist of geometry represented by unstructured finite element grids. Originally, isosurface extraction methods were restricted to structured grid geometry and earlier effort focused on extracting a single isosurface <ref> [27] </ref>. Recently, in an effort to speed up isosurface extraction, several methods were developed that could be adapted to multiple isosurface extraction from structured [28], [29] as well as unstructured geometry [30], [31]. Nevertheless, these methods do not provide interactive speed, especially for unstructured grids.
Reference: [28] <author> J. Wilhelms and A. Van Gelder. </author> <title> Octrees for faster isosurface generation. </title> <journal> ACM Transactions on Graphics, </journal> <volume> 11(3) </volume> <pages> 201-227, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Originally, isosurface extraction methods were restricted to structured grid geometry and earlier effort focused on extracting a single isosurface [27]. Recently, in an effort to speed up isosurface extraction, several methods were developed that could be adapted to multiple isosurface extraction from structured <ref> [28] </ref>, [29] as well as unstructured geometry [30], [31]. Nevertheless, these methods do not provide interactive speed, especially for unstructured grids. Defining n as the number of data cells and k as the number of cells intersecting a given isosurface, the above algorithms usually have time complexity of O (n). <p> Nevertheless, these methods do not provide interactive speed, especially for unstructured grids. Defining n as the number of data cells and k as the number of cells intersecting a given isosurface, the above algorithms usually have time complexity of O (n). Although <ref> [28] </ref> has an improved time complexity of O (k log ( n k )), it is only suitable for rectangular grids. Current isosurface extraction methods are based on locating data cells intersecting the isosurface and then approximating the isosurface inside these cells.
Reference: [29] <author> T. Itoh and K. Koyyamada. </author> <title> Isosurface generation by using extrema graphs. </title> <booktitle> In Proceedings of Visualization '94, </booktitle> <pages> pages 77-83. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1994. </year>
Reference-contexts: Originally, isosurface extraction methods were restricted to structured grid geometry and earlier effort focused on extracting a single isosurface [27]. Recently, in an effort to speed up isosurface extraction, several methods were developed that could be adapted to multiple isosurface extraction from structured [28], <ref> [29] </ref> as well as unstructured geometry [30], [31]. Nevertheless, these methods do not provide interactive speed, especially for unstructured grids. Defining n as the number of data cells and k as the number of cells intersecting a given isosurface, the above algorithms usually have time complexity of O (n).
Reference: [30] <author> M. Giles and R. Haimes. </author> <title> Advanced interactive visualization for CFD. </title> <journal> Computing Systems in Engineering, </journal> <volume> 1(1) </volume> <pages> 51-62, </pages> <year> 1990. </year>
Reference-contexts: Recently, in an effort to speed up isosurface extraction, several methods were developed that could be adapted to multiple isosurface extraction from structured [28], [29] as well as unstructured geometry <ref> [30] </ref>, [31]. Nevertheless, these methods do not provide interactive speed, especially for unstructured grids. Defining n as the number of data cells and k as the number of cells intersecting a given isosurface, the above algorithms usually have time complexity of O (n).
Reference: [31] <author> R. S. Gallagher. </author> <title> Span filter: An optimization scheme for volume visualization of large finite element models. </title> <booktitle> In Proceedings of Visualization '91, </booktitle> <pages> pages 68-75. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1991. </year>
Reference-contexts: Recently, in an effort to speed up isosurface extraction, several methods were developed that could be adapted to multiple isosurface extraction from structured [28], [29] as well as unstructured geometry [30], <ref> [31] </ref>. Nevertheless, these methods do not provide interactive speed, especially for unstructured grids. Defining n as the number of data cells and k as the number of cells intersecting a given isosurface, the above algorithms usually have time complexity of O (n).
Reference: [32] <author> P. Ghapure and C.R. Johnson. </author> <title> A 3d cellular automata model of the heart. </title> <booktitle> In Proceedings of 15th Annual IEEE Engineering in Medicine and Biology Society International Conference. </booktitle> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: We were motivated to develop a more efficient way to visualize scalar fields by our attempts to visualize simulation data from a model of electrical wave propagation within the complex geometry of the heart and from large scale models of unsteady compressible fluid flow <ref> [32] </ref>, [33]. Because of the regular structures used to characterize this particular set of simulation data, we can use direct volume-rendering techniques to visualize the states at each time step. We characterize the states within the model by assigning different colors and opacities.
Reference: [33] <author> K.-L. Ma and K. Sikorski. </author> <title> A distributed algorithm for the three-dimensional compressible navier-stokes equations. </title> <journal> Transputer Research and Applications, </journal> <volume> 4, </volume> <year> 1990. </year>
Reference-contexts: We were motivated to develop a more efficient way to visualize scalar fields by our attempts to visualize simulation data from a model of electrical wave propagation within the complex geometry of the heart and from large scale models of unsteady compressible fluid flow [32], <ref> [33] </ref>. Because of the regular structures used to characterize this particular set of simulation data, we can use direct volume-rendering techniques to visualize the states at each time step. We characterize the states within the model by assigning different colors and opacities.
Reference: [34] <author> R Yagel and Z. Shi. </author> <title> Accelerating volume animation by space-leaping. </title> <booktitle> In Proceedings of Visualization '91, </booktitle> <pages> pages 62-69. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <month> October </month> <year> 1993. </year> <month> November 6, </month> <note> 1995 DRAFT 33 </note>
Reference-contexts: The method is independent of specific volume-rendering techniques and can be adapted into a variety of ray casting paradigms which can be used to further accelerate the visualization process <ref> [34] </ref>, [35], [36]. During preliminary studies of our electrical wave propagation simulations in the heart, we noticed that the only elements that changed values between consecutive time steps, when the time steps were small, were the activated cells and their neighbors.
Reference: [35] <author> R Yagel and A. Kaufman. </author> <title> Template-based volume viewing. </title> <booktitle> In Proceedings of EUROGRAPH-ICS '92, </booktitle> <pages> pages 153-157. </pages> <publisher> Blackwell, </publisher> <address> Cambridge, England, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: The method is independent of specific volume-rendering techniques and can be adapted into a variety of ray casting paradigms which can be used to further accelerate the visualization process [34], <ref> [35] </ref>, [36]. During preliminary studies of our electrical wave propagation simulations in the heart, we noticed that the only elements that changed values between consecutive time steps, when the time steps were small, were the activated cells and their neighbors.
Reference: [36] <author> M.F. Cohen K.-L. Ma and J.S. Painter. </author> <title> Volume seeds: A volume exploration technique. </title> <journal> The Journal of Visualization and Computer Animation, </journal> <volume> 2 </volume> <pages> 135-140, </pages> <year> 1991. </year>
Reference-contexts: The method is independent of specific volume-rendering techniques and can be adapted into a variety of ray casting paradigms which can be used to further accelerate the visualization process [34], [35], <ref> [36] </ref>. During preliminary studies of our electrical wave propagation simulations in the heart, we noticed that the only elements that changed values between consecutive time steps, when the time steps were small, were the activated cells and their neighbors.
Reference: [37] <author> H.W. Shen and C.R. Johnson. </author> <title> Differential volume rendering: A fast volume visualization technique for flow animation. </title> <note> In IEEE Visualization `94, 1994 (to appear). </note>
Reference-contexts: Because the variation between consecutive time steps is small, the differential information file, which replaces the whole sequence of volume data, can yield tremendous savings in terms of disk space (often saving more than 90% over other volume rendering methods) <ref> [37] </ref>. An example of the differential rendering method for visualizing electrical wave propagation simulations in the heart is shown in Figure 5. In addition to the differential volume rendering algorithm, we have also developed a data parallel volume rendering method.
Reference: [38] <author> K.-L. Ma J.S. Painter, C.D. Hansen and M.F. Krogh. </author> <title> A data distributed, parallel algorithm for ray-traced volume rendering. </title> <booktitle> In Parallel Rendering Symposium, </booktitle> <pages> pages 15-22. </pages> <publisher> IEEE Computer Society Press, Los Alamitos, </publisher> <address> CA, </address> <year> 1993. </year>
Reference-contexts: An example of the differential rendering method for visualizing electrical wave propagation simulations in the heart is shown in Figure 5. In addition to the differential volume rendering algorithm, we have also developed a data parallel volume rendering method. Ma et al. <ref> [38] </ref> have proposed a divide-and-conquer ray casting algorithm November 6, 1995 DRAFT 16 and a binary swap image composition method to achieve fast parallel volume rendering. Although the binary swap image composition operation is very efficient, the performance of the ray casting process is not very satisfactory.
Reference: [39] <author> J. Schmidt. </author> <title> Mesh generation with applications in computational electrophysiology. </title> <type> PhD thesis, </type> <institution> Duke University, Durham, NC, </institution> <year> 1993. </year>
Reference-contexts: Now, however, computer models have recently become sophisticated enough to assist in studying the defibrillation problem. Early November 6, 1995 DRAFT 18 computer models usually required such significant amounts of time to construct and change model parameters such that relatively few electrode configurations have been simulated <ref> [39] </ref>. We have developed a defibrillation modeling system that allows a researcher to interactively design and place electrodes into a high-resolution finite element model of the human thorax (the Utah Torso Model [10], [14]).
Reference: [40] <author> Y. Yamashita. </author> <title> Theoretical studies on the inverse problem in electrocardiography and the uniqueness of the solution. </title> <journal> IEEE Trans Biomed. Eng., </journal> <volume> BME-29:719-725, </volume> <year> 1982. </year>
Reference-contexts: Thus, instead of solving Poisson's equation, we solve a generalized Laplace's equation with Cauchy boundary conditions: r r = 0 (14) with boundary conditions = 0 on T and r n = 0 on T : (15) While this version of the inverse problem has a unique solution <ref> [40] </ref>, the problem is still mathematically ill-posed in the Hadamard sense; i.e., because the solution does not depend continuously on the data, small errors in the measurement of the voltages on the torso (scalp) can yield unbounded errors in the solution.
Reference: [41] <author> R.S. MacLeod, C.R. Johnson, M.J. Gardner, and B.M. Horacek. </author> <title> Localization of ischemia during coronary angioplasty using body surface potential mapping and an electrocardiographic inverse solution. </title> <booktitle> In IEEE Computers in Cardiology, </booktitle> <pages> pages 251-254. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: An accurate solution to the inverse problem in electrocardiography would provide a non-invasive procedure for the evaluation for myocardial ischemia <ref> [41] </ref>, the localization of ven-tricular arrhythmias and the site of accessory pathways in Wolff-Parkinson-White (WPW) syndrome, and, more generally, the determination of patterns of excitation and recovery of excitability. Similarly, an accurate solution to the inverse problem in electroencephalography would provide a non-invasive procedure for functionally mapping the cortex.
Reference: [42] <author> Y. Yan, P.L. Nunez, and R.T. Hart. </author> <title> Finite-element model of the human head: scalp potentials due to dipole sources. </title> <journal> Medical & Biological Engineering & Computing, </journal> <volume> 29 </volume> <pages> 475-481, </pages> <year> 1991. </year>
Reference-contexts: While these methods yield a good first approximation to the true cortical potentials for simple cases involving a single dipole source, they are not accurate November 6, 1995 DRAFT 22 enough for many clinical applications <ref> [42] </ref>. An accurate inverse solution would also provide a non-invasive means for localizing and analyzing the activity of ectopic foci in epileptic patients [43], thereby providing neurologists reliable data on which to diagnose and treat patients.
Reference: [43] <author> F.H. Lopes da Silva. </author> <title> A critical review of clinical applications of topographic mapping of brain potentials. </title> <journal> Journal of Clinical Neurophysiology, </journal> <volume> 7(4) </volume> <pages> 535-551, </pages> <year> 1990. </year>
Reference-contexts: An accurate inverse solution would also provide a non-invasive means for localizing and analyzing the activity of ectopic foci in epileptic patients <ref> [43] </ref>, thereby providing neurologists reliable data on which to diagnose and treat patients.
Reference: [44] <author> F.P. Colli Franzone, G. Gassaniga, L. Guerri, B. Taccardi, and C. Viganotti. </author> <title> Accuracy evaluation in direct and inverse electrocardiology. </title> <editor> In P.W. Macfarlane, editor, </editor> <booktitle> Progress in Electrocardiography, </booktitle> <pages> pages 83-87. </pages> <publisher> Pitman Medical, </publisher> <year> 1979. </year>
Reference-contexts: The application of the local regularization technique recovered the voltages to within 12.6% RMS error. Previous studies in three-dimensional problems have reported the recovery of epicardial potentials with errors in the range of 20-40%, <ref> [44] </ref>, [45], [46], [47]. Figure 4 shows the inverse solution calculated using the local regularization technique compared with the recorded heart voltages as a function of position on the epicardium. The global solution tended to be smoother, not able to follow the extrema as well as the local solution could.
Reference: [45] <author> P. Colli Franzone, L. Guerri, S. Tentonia, C. Viganotti, S. Spaggiari, and B. Taccardi. </author> <title> A numerical procedure for solving the inverse problem of electrocardiography. Analysis of the time-space accuracy from in vitro experimental data. </title> <journal> Math. Biosci., </journal> <volume> 77:353, </volume> <year> 1985. </year>
Reference-contexts: The application of the local regularization technique recovered the voltages to within 12.6% RMS error. Previous studies in three-dimensional problems have reported the recovery of epicardial potentials with errors in the range of 20-40%, [44], <ref> [45] </ref>, [46], [47]. Figure 4 shows the inverse solution calculated using the local regularization technique compared with the recorded heart voltages as a function of position on the epicardium. The global solution tended to be smoother, not able to follow the extrema as well as the local solution could.
Reference: [46] <author> B.J. Messinger-Rapport and Y. Rudy. </author> <title> Regularization of the inverse problem in electrocardiography: A model study. </title> <journal> Math. Biosci., </journal> <volume> 89 </volume> <pages> 79-118, </pages> <year> 1988. </year>
Reference-contexts: The application of the local regularization technique recovered the voltages to within 12.6% RMS error. Previous studies in three-dimensional problems have reported the recovery of epicardial potentials with errors in the range of 20-40%, [44], [45], <ref> [46] </ref>, [47]. Figure 4 shows the inverse solution calculated using the local regularization technique compared with the recorded heart voltages as a function of position on the epicardium. The global solution tended to be smoother, not able to follow the extrema as well as the local solution could.
Reference: [47] <author> P.C. Stanley, </author> <title> T.C. Pilkington, and M.N. Morrow. The effects of thoracic inhomogeneities on the relationship between epicardial and torso potentials. </title> <journal> IEEE Trans Biomed. Eng., </journal> <volume> BME-33:273-284, </volume> <year> 1986. </year>
Reference-contexts: The application of the local regularization technique recovered the voltages to within 12.6% RMS error. Previous studies in three-dimensional problems have reported the recovery of epicardial potentials with errors in the range of 20-40%, [44], [45], [46], <ref> [47] </ref>. Figure 4 shows the inverse solution calculated using the local regularization technique compared with the recorded heart voltages as a function of position on the epicardium. The global solution tended to be smoother, not able to follow the extrema as well as the local solution could.
Reference: [48] <author> C.R. Johnson and S.G. Parker. </author> <title> A computational steering model for problems in medicine. </title> <booktitle> In Supercomputing `94, </booktitle> <pages> pages 540-549. </pages> <publisher> IEEE Press, </publisher> <year> 1994. </year>
Reference-contexts: Local Regularization Technique tailor it to the particular requirements of a specific research problem. A. SCIRun To reduce the time the scientist spends in using this standard modeling paradigm and to provide a tool for exploration of computational science and engineering problems, we have developed SCIRun 2 <ref> [48] </ref>, [49]. SCIRun is a framework in which large scale computer simulations can be composed, executed, controlled and tuned interactively. Composing the simulation is accomplished via a visual programming interface to a dataflow network.
Reference: [49] <author> S.G. Parker and C.R. Johnson. Scirun: </author> <title> A scientific programming environment for computational steering. </title> <booktitle> In Supercomputing `95. </booktitle> <publisher> IEEE Press, </publisher> <year> 1995. </year> <note> (submitted). </note>
Reference-contexts: Local Regularization Technique tailor it to the particular requirements of a specific research problem. A. SCIRun To reduce the time the scientist spends in using this standard modeling paradigm and to provide a tool for exploration of computational science and engineering problems, we have developed SCIRun 2 [48], <ref> [49] </ref>. SCIRun is a framework in which large scale computer simulations can be composed, executed, controlled and tuned interactively. Composing the simulation is accomplished via a visual programming interface to a dataflow network.
Reference: [50] <author> W. Gu, J. Vetter, and K. Schwan. </author> <title> An annotated bibliography of interactive program steering. </title> <institution> Georgia Institute of Technology Technical Report, </institution> <year> 1994. </year>
Reference-contexts: To execute the program, one specifies parameters with a graphical user interface rather than with the traditional text-based datafile. Controlling a simulation involves steering the simulation interactively as it progresses. Computational steering has been defined as "the capacity to control the execution of long-running, resource-intensive programs" <ref> [50] </ref>. In the field of computational science, we apply this concept to link visualization with geometric design and computational phases to interactively explore (steer) a simulation in time and/or space. As knowledge is gained, it can be used to change the input conditions and/or other parameters of the simulation.
Reference: [51] <author> C. Upson and et al. </author> <title> The application visualization system: A computational environment for scientific visualization. </title> <journal> IEEE Computer Graphics & Applications, </journal> <volume> 9(4) </volume> <pages> 30-42, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: SCIRun is a framework in which large scale computer simulations can be composed, executed, controlled and tuned interactively. Composing the simulation is accomplished via a visual programming interface to a dataflow network. Software systems such as AVS from Application Visualization Systems Inc. <ref> [51] </ref>, Iris Explorer from Silicon Graphics, and Visualization Data Explorer from IBM [52] have made this archetype popular for scientific visualization [53]. Our work has extended this paradigm into the realm of scientific computation.
Reference: [52] <author> B. Lucas and et al. </author> <title> An architecture for a scientific visualization system. </title> <booktitle> In Proceedings of Visualization '92, </booktitle> <pages> pages 107-114. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year> <month> November 6, </month> <note> 1995 DRAFT 34 </note>
Reference-contexts: Composing the simulation is accomplished via a visual programming interface to a dataflow network. Software systems such as AVS from Application Visualization Systems Inc. [51], Iris Explorer from Silicon Graphics, and Visualization Data Explorer from IBM <ref> [52] </ref> have made this archetype popular for scientific visualization [53]. Our work has extended this paradigm into the realm of scientific computation.
Reference: [53] <author> C. Willams, J. Rasure, and C. Hansen. </author> <title> The state of the art of visual languages for visualization. </title> <booktitle> In Proceedings of Visualization '92, </booktitle> <pages> pages 202-209. </pages> <publisher> IEEE Press, </publisher> <year> 1992. </year>
Reference-contexts: Composing the simulation is accomplished via a visual programming interface to a dataflow network. Software systems such as AVS from Application Visualization Systems Inc. [51], Iris Explorer from Silicon Graphics, and Visualization Data Explorer from IBM [52] have made this archetype popular for scientific visualization <ref> [53] </ref>. Our work has extended this paradigm into the realm of scientific computation.
Reference: [54] <institution> Communications The High Performance Computing, Information Technology (HPCCIT) Subcommittee of the National Science, and Technology Council. High performance computing and communications: Foundations for america's information future, </institution> <year> 1995. </year> <month> November 6, </month> <note> 1995 DRAFT </note>
Reference-contexts: VII. Future Directions One of the goals of the HPCC is to "Extend U.S. technological leadership in high performance computing and computer communications <ref> [54] </ref>". Unfortunately, it not economically feasible to put a high performance computer on the desk of every scientist in the coun November 6, 1995 DRAFT 28 try.
References-found: 54

