URL: http://networks.cs.ucdavis.edu/~ghosal/globecom/globecom_paper.ps
Refering-URL: http://networks.cs.ucdavis.edu/~ghosal/Globecom.html
Root-URL: http://www.cs.ucdavis.edu
Email: Email: feng@@item.ntnu.no  Email: ghosal@@cs.ucdavis.edu  Email: narana@@cisco.com  
Title: Impact of ATM ABR Control on the Performance of TCP-Tahoe and  
Author: TCP-Reno Boning Feng Dipak Ghosal Narana Kannappan 
Address: N-7034 Trondheim, Norway.  Davis.  
Affiliation: Department of Telematics, Norwegian University of Science,  Department of Computer Science, University of California,  Enterprise Line of Business Unit, Cisco Systems Inc.  
Abstract: We study the effect of ATM ABR control on the throughput and fairness of running large unidirectional file transfer applications on TCP-Tahoe and TCP-Reno with a single bottleneck ATM link. The various ATM ABR control are characterized by different switch queue threshold(Qth) values which define when the bottleneck link declares congestion and starts marking RM cells and, the Reduction Factor(RDF) values which define how quickly the source reduces its cell transmission rate. The key results of this study are the following. The key results of this study are the following. First, our results show that TCP-Reno is slightly better than TCP-Tahoe in terms of the throughput when the round-trip delay is short, specially with a large packet size. Second, with a medium round-trip delay(1ms), TCP-Reno gives a better throughput with long packets, while TCP-Tahoe performs better with short packets. Third, when the round-trip delay is long(30ms), TCP-Tahoe clearly offers higher throughput than TCP-Reno. TCP-Tahoe consistently outperforms TCP-Reno in terms of fairness. Fourth, for a short or medium round-trip delay, tight ABR control (specially with lower RDF) results in higher throughput than loose control; while the opposite is true for a long round-trip delay. Finally, a strong positive correlation between poor throughput and poor fairness is only valid for short & medium round-trip delays for TCP-Tahoe. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> "Traffic management specifications version 4.0," ATM Forum Technical Committee, AT-TM-0056.000, </institution> <month> April </month> <year> 1996. </year>
Reference-contexts: 1 Introduction In this paper we consider running TCP over and end-to-end ATM network. ATM networks provide four classes of services <ref> [1] </ref>. Constant Bit Rate (CBR) and Variable Bit Rate (VBR) address applicatons which have precisely defined re fl This work was done during sabbatical leave at University of Cali-fornia, Davis. y This work was done during the author's graduate study at University of California, Davis. quirements of throughputs and delays. <p> The ATM Forum Traffic Management group has standardized a rate-based closed-loop flow control model for the ABR class of traffic <ref> [1] </ref>. In this model, the ATM switches give feedback (explicit rate (ER), or binary (Explicit Forward Congestion Indication, EFCI)) in Resource Management (RM) cells and the sources adjust their transmission rates appropriately.
Reference: [2] <author> A. Romanow and S. Floyd, </author> <title> "Dynamics of TCP Traffic over ATM Networks," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 13, </volume> <pages> pp. 633-641, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: ABR and UBR are the main candidates for transporting data traffic, e.g. file transfer, which have fairly loose bounds on the QoS requirements and can tolerate variations in throughput and delay. Several studies on the performance of UBR <ref> [2, 3, 4] </ref> show that, since UBR provides no congestion control, indiscriminate dropping of ATM cells under congestion could lead to the collapse of throughput for data applications [2]. <p> Several studies on the performance of UBR [2, 3, 4] show that, since UBR provides no congestion control, indiscriminate dropping of ATM cells under congestion could lead to the collapse of throughput for data applications <ref> [2] </ref>. Enhancements to UBR like intelligent cell drop policies at the switch have also been proposed [5] ABR economically supports data traffic by minimizing cell loss at the expense of delay [6]. <p> The parame ters chosen are 0.25, 0.75, and 0.90. The simulator is based on the one used by Allyn Romanov and Sally Floyd [7]. The switch architecture is of a sufficient general design that the performance results are not architecturally dependent <ref> [2] </ref>. The performance is measured by the TCP throughput and fairness. * Throughput is defined as the total number of bytes successfully delivered and acknowledged. The TCP throughputs are measured at the sender TCP layer. After the transient period, the sender records the acknowledgment with the largest sequence number.
Reference: [3] <author> R. Jain, R. Goyal, S. Kalyanaraman, and F. S., </author> <title> "Performance of TCP over UBR and buffer requirements," </title> <journal> ATM Forum, </journal> <volume> vol. </volume> <pages> AF-TM 96-0518, </pages> <month> April </month> <year> 1996. </year>
Reference-contexts: ABR and UBR are the main candidates for transporting data traffic, e.g. file transfer, which have fairly loose bounds on the QoS requirements and can tolerate variations in throughput and delay. Several studies on the performance of UBR <ref> [2, 3, 4] </ref> show that, since UBR provides no congestion control, indiscriminate dropping of ATM cells under congestion could lead to the collapse of throughput for data applications [2].
Reference: [4] <author> R. Jain, R. Goyal, S. Kalyanaraman, F. S., and S. Kim, </author> <title> "Performance of TCP over UBR+," </title>
Reference-contexts: ABR and UBR are the main candidates for transporting data traffic, e.g. file transfer, which have fairly loose bounds on the QoS requirements and can tolerate variations in throughput and delay. Several studies on the performance of UBR <ref> [2, 3, 4] </ref> show that, since UBR provides no congestion control, indiscriminate dropping of ATM cells under congestion could lead to the collapse of throughput for data applications [2]. <p> is over, it measures throughput for all the segments correctly received in sequence during the simulation period by calculating the difference between the largest ACK at that time and the largest ACK number recorded right after the transient period. * Fairness is expressed by the Fairness Index (hereafter called FI) <ref> [4] </ref> which is defined as follows: F airness Index = ( x i ) 2 P i where x i = throughput of the i th TCP sender, and n is the number of TCP senders. 3 Simulation Results and Discussions The total capacity of the system is 155 Mbits/s which
Reference: [5] <author> R. Jain, R. Goyal, S. Kalyanaraman, S. Fahmy, and F. o. T. o. U. Lu, </author> <title> "Performance of TCP/IP over UBR," </title>
Reference-contexts: Enhancements to UBR like intelligent cell drop policies at the switch have also been proposed <ref> [5] </ref> ABR economically supports data traffic by minimizing cell loss at the expense of delay [6]. The ATM Forum Traffic Management group has standardized a rate-based closed-loop flow control model for the ABR class of traffic [1].
Reference: [6] <author> F. Bonomi and K. Fendick, </author> <title> "The rate-based flow control framework for the available bit rate ATM service," </title> <journal> IEEE Network, </journal> <pages> pp. 25-39, </pages> <month> March/April </month> <year> 1995. </year> <title> [7] "The ns simulator, </title> <institution> lawrence berkeley laboratories: </institution> <note> http://www-nrg.ee.lbl.gov/ns." </note>
Reference-contexts: Enhancements to UBR like intelligent cell drop policies at the switch have also been proposed [5] ABR economically supports data traffic by minimizing cell loss at the expense of delay <ref> [6] </ref>. The ATM Forum Traffic Management group has standardized a rate-based closed-loop flow control model for the ABR class of traffic [1].
Reference: [8] <author> W. R. Stevens, </author> <title> TCP/IP Illustrated Volume 1. </title> <publisher> Addison-Wesley Professional Computing Series, </publisher> <month> November </month> <year> 1994. </year>
Reference-contexts: The TCP buffer stores all the segments that are already sent by TCP (and IP) but not yet processed by the ATM SAR (segmentation and reassembly) unit which divides the packet into cells and re-groups them. The TCP versions simulated are 4.3BSD Tahoe and 4.3BSD Reno <ref> [8] </ref>. In TCP, the retransmission timer is set as a function of the round-trip time. The minimum value for the retransmit timer is twice the TCP clock tick (which refers to the granularity of the clock used for measuring the TCP round-trip time).
Reference: [9] <author> C. Douglas, </author> <title> Internetworking with TCP/IP, </title> <booktitle> Vol 1: Principles,Protocols, and Architecture. </booktitle> <publisher> Prentice Hall Inc.,, </publisher> <year> 1995. </year>
Reference-contexts: Packet sizes were chosen according to common link level maximum transfer units (MTUs); - 512 bytes, which is the minimum MTU that must be supported by IP; - 9180 bytes, which is the maximum size for IP over ATM as defined in the TCP/IP standards <ref> [9] </ref>. 4. Switch buffer sizes of 1932 (100 Kbytes) and 7728 cells (400 Kbytes) are used. 5. TCP maximum window size is 128 Kbytes. 6. TCP delay ACK timer is not set. Segments are acked as soon as they are received. 7.
Reference: [10] <author> K. Fall and S. Floyd, </author> <title> "Simulation-based comparisons of tahoe, renoe, and SACK TCP," </title> <type> tech. rep., </type> <institution> Lawrence Berkeley Laboratory, </institution> <year> 1996. </year>
Reference-contexts: While TCP-Tahoe and TCP-Reno give almost the same throughput in the short packet cases, TCP-Reno gives slightly better throughput in most of the long packet cases. A possible explanation is that TCP-Reno is optimized to single packet losses <ref> [10] </ref>. Our results show that with long packets most of the losses are indeed single losses. For medium round-trip delay, TCP-Tahoe has better throughput when packets are short, while TCP-Reno performs better when packets are long. However, for most of the cases the differences are small.
References-found: 9

