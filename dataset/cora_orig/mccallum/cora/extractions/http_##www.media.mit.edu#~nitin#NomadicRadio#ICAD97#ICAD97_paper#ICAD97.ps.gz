URL: http://www.media.mit.edu/~nitin/NomadicRadio/ICAD97/ICAD97_paper/ICAD97.ps.gz
Refering-URL: http://vismod.www.media.mit.edu/vismod/demos/smartclothes/mitres.html
Root-URL: http://www.media.mit.edu
Email: nitin, geek-@media.mit.edu  
Title: Design of Spatialized Audio in Nomadic Environments  
Author: Nitin Sawhney and Chris Schmandt 
Address: 20 Ames Street, Cambridge, MA 02139  
Affiliation: Speech Interface Group MIT Media Laboratory  
Abstract: This paper describes an ongoing research project at the MIT Media Lab, exploring the use of audio as a primary modality for nomadic computing applications. We are developing a framework for use on a wearable audio platform, Nomadic Radio, that presents dynamic information within a spatialized audio environment. The contextual state of the nomadic listener indicated by time of day, physical positioning, scheduled tasks, message content, and level of interruption is used to present relevant information in the users listening space. In this paper we will consider issues related to auditory presentation and spatial techniques for awareness and browsing of audio messages on wearable computing. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Schmandt, Chris. </author> <title> Multimedia Nomadic Services on Todays Hardware. </title> <journal> IEEE Network, </journal> <note> September/October 1994, pp12-21. </note>
Reference-contexts: Such services should be made available on-demand in a passive and unobtrusive manner, based on the users level of attention and interruptability. The MIT Media Labs Nomadic Computing Environment <ref> [1] </ref> enables subscribers to manage personal information via fax, pagers and telephony access using digitized audio and synthesized speech. Pagers and cellular phones provide remote access to information, yet such devices offer extremely low-bandwidth for communication and the interface does not afford rich delivery of information content. <p> The current architecture relies on server processes (written in C and Perl) running on Sun SPARCstations that utilize the telephony infrastructure in the Media Labs Speech Interface group <ref> [1] </ref>. The servers extract information from live sources including voice-mail, email, hourly updates of ABC News, weather and traffic reports. The clients, when notified, download the appropriate text/audio files stored on the web server.
Reference: 2. <author> Stifelman, Lisa J., Barry Arons, Chris Schmandt, Eric A. Hulteen. VoiceNotes: </author> <title> A Speech Interface for Hand Held Voice Notetaker. </title> <booktitle> Proceedings of INTERCHI93, </booktitle> <month> April </month> <year> 1993. </year>
Reference-contexts: Personal Digital Assistants (PDAs) offer personal applications in a smaller size, yet they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy such as while driving. Handheld <ref> [2, 3] </ref> and mobile [4, 5] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [6]. <p> If wearable computers are expected to become as natural as clothing, we must consider the role of audio and tactile interaction as the primary interface for wearable computing. Speech input is a natural means of interaction, yet the user interface must be carefully devised to permit recognition and recording <ref> [2] </ref> on a wearable device. Speech and button input can be combined to provide users more flexibility for interaction in different situations. Button input allows operation of the WAC when background noise or social protocol constrains the use of speech recognition.
Reference: 3. <author> Roy, Deb K. and Chris Schmandt. NewsComm: </author> <title> A HandHeld Interface for Interactive Access to Structured Audio. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996, </year> <pages> pp. 173-180. </pages>
Reference-contexts: Personal Digital Assistants (PDAs) offer personal applications in a smaller size, yet they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy such as while driving. Handheld <ref> [2, 3] </ref> and mobile [4, 5] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [6].
Reference: 4. <author> Stifelman, Lisa J. </author> <title> Augmenting Real-World Objects: A Paper-Based Audio Notebook. </title> <booktitle> Proceedings of CHI 96, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: Personal Digital Assistants (PDAs) offer personal applications in a smaller size, yet they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy such as while driving. Handheld [2, 3] and mobile <ref> [4, 5] </ref> audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [6].
Reference: 5. <author> Wilcox, Lynn D., Bill N. Schilit, Nitin Sawhney. Dynomite: </author> <title> A Dynamically Organized Ink and Audio Notebook. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997, </year> <pages> pp. 186-193. </pages>
Reference-contexts: Personal Digital Assistants (PDAs) offer personal applications in a smaller size, yet they generally utilize pen-based graphical user interfaces, which are not ideal when the users hands and eyes are busy such as while driving. Handheld [2, 3] and mobile <ref> [4, 5] </ref> audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) [6].
Reference: 6. <author> Roy, Deb K., Nitin Sawhney, Chris Schmandt, Alex Pentland. </author> <title> Wearable Audio Computing: A Survey of Interaction Techniques Technical Report, </title> <publisher> MIT Media Lab, </publisher> <month> April </month> <year> 1997. </year>
Reference-contexts: Handheld [2, 3] and mobile [4, 5] audio devices with localized computing and richer interaction mechanisms certainly point towards audio interfaces and networked applications for a new personal information platform, Wearable Audio Computing (WAC) <ref> [6] </ref>. Auditory displays can be used to enhance an environment with timely information and provide a sense of peripheral awareness [7] of people and background events. A combination of wearable auditory and tactile interaction can provide users a means of unobtrusively augmenting a physical environment.
Reference: 7. <author> Mynatt, E.D., Back, M., Want, R. and Frederick, R. </author> <title> Audio Aura: LightWeight Audio Augmented Reality. </title> <booktitle> Proceedings of UIST '97 User Interface Software and Technology Symposium, </booktitle> <address> Banff, Canada, Oct 15-17, </address> <year> 1997. </year>
Reference-contexts: Auditory displays can be used to enhance an environment with timely information and provide a sense of peripheral awareness <ref> [7] </ref> of people and background events. A combination of wearable auditory and tactile interaction can provide users a means of unobtrusively augmenting a physical environment.
Reference: 8. <author> Sawhney, Nitin and Chris Schmandt. </author> <title> Nomadic Radio: A Spatialized Audio Environment for Wearable Computing. </title> <booktitle> Proceedings of the International Symposium on Wearable Computing, IEEE, </booktitle> <address> Cambridge, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: Hence techniques based on time-compressed speech , simultaneous audio and spatialization must be considered for browsing audio-based messages more efficiently. NOMADIC RADIO: WEARABLE AUDIO MESSAGING Nomadic Radio <ref> [8] </ref> is being developed as a unified messaging system that utilizes spatialized audio, speech synthesis and recognition, on a wearable audio platform. Messages such as hourly news broadcasts, voice mail , email and weather reports are downloaded to the device throughout the day.
Reference: 9. <author> Marx, Matthew and Chris Schmandt. </author> <title> CLUES: Dynamic Personalized Message Filtering. </title> <booktitle> Proceedings of CSCW 96, </booktitle> <month> November </month> <year> 1996, </year> <pages> pp. 113-121. </pages>
Reference-contexts: Messages such as hourly news broadcasts, voice mail , email and weather reports are downloaded to the device throughout the day. Selected messages are presented in the users listening space, based on her context and desired level of awareness or interruptability. We are incorporating timely message filtering <ref> [9] </ref> to selectively present the appropriate messages, coupled with user location and environmental context. The current system operates primarily as a wearable audio-only interface, although a visual interface is used for development purposes. A combination of speech and button inputs are used to control the interface.
Reference: 10. <author> Cohen, J. </author> <title> Monitoring background activities. Auditory Display: Sonification, Audification, and Auditory Interfaces. </title> <address> Reading MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Speech and music in the background and peripheral auditory cues can provide an awareness of messages or signify events, without requiring ones full attention or disrupting their foreground activity. Audio easily fades into the background, but users are alerted when it changes <ref> [10] </ref>. It is possible for listeners to attend to multiple background processes via the audio channel as long as the sounds representing each process are distinguishable.
Reference: 11. <author> Handel, S. </author> <title> Listening: An Introduction to the Perception of Auditory Events. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: Audio easily fades into the background, but users are alerted when it changes [10]. It is possible for listeners to attend to multiple background processes via the audio channel as long as the sounds representing each process are distinguishable. This well known cognitive phenomenon, called the Cocktail Party Effect <ref> [11] </ref>, provides the justification that humans can in fact monitor several audio streams simultaneously, selectively focusing on any one and placing the rest in the background. A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [12].
Reference: 12. <author> Wenzel, E.M.. </author> <title> Localization in virtual acoustic displays, </title> <journal> Presence, </journal> <volume> 1, 80, </volume> <year> 1992. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources <ref> [12] </ref>. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [13]. Bregman claims that stream segregation is better when frequency separation is greater between sound streams [14].
Reference: 13. <author> Stifelman, Lisa J. </author> <title> The Cocktail Party Effect in Auditory Interfaces: A Study of Simultaneous Presentation. </title> <type> Technical Report, </type> <institution> MIT Media Lab, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: A good model of the head-related transfer functions (HRTF) permits effective localization and externalization of sound sources [12]. Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension <ref> [13] </ref>. Bregman claims that stream segregation is better when frequency separation is greater between sound streams [14].
Reference: 14. <author> Bregman, Albert S. </author> <title> Auditory Scene Analysis: The Perceptual Organization of Sound. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Yet the cognitive load of listening to simultaneous channels increases with the number of channels. Experiments show that increasing the number of channels beyond three causes a degradation in comprehension [13]. Bregman claims that stream segregation is better when frequency separation is greater between sound streams <ref> [14] </ref>. Arons suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels (providing perceptual handles on each channel) and pull an audio stream into focus as well as allowing sufficient time to fully fuse the audio streams [15].
Reference: 15. <author> Arons, Barry. </author> <title> A Review of the Cocktail Party Effect. </title> <journal> Journal of the American Voice I/O Society , Vol. </journal> <volume> 12, </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: Arons suggests that the effect of spatialization can be improved by allowing listeners to easily switch between channels (providing perceptual handles on each channel) and pull an audio stream into focus as well as allowing sufficient time to fully fuse the audio streams <ref> [15] </ref>. A spatial sound system can provide a strong metaphor by placing individual voices in particular spatial locations. The effective use of spatial layout can be used to aid auditory memory. <p> As the message is pulled in, its direction is maintained allowing the user to retain a sense of message arrival time. This spatial continuity is important in discriminating and holding the auditory streams together <ref> [15] </ref>. Several levels of awareness are used to determine the form of message delivery. Since users can be actively engaged in conversations or meetings, they may not wish to be distracted by audio playing at certain times of the day. Instead auditory cues or message previews can be provided.
Reference: 16. <author> Schmandt, Chris and Atty Mullins. AudioStreamer: </author> <title> Exploiting Simultaneity for Listening. </title> <booktitle> Proceedings of CHI 95 , May 1995, </booktitle> <pages> pp. 218-219. </pages>
Reference-contexts: A spatial sound system can provide a strong metaphor by placing individual voices in particular spatial locations. The effective use of spatial layout can be used to aid auditory memory. The AudioStreamer <ref> [16] </ref> detects the gesture of head movement towards spatialized audio-based news sources to increase the relative gain of the source, allowing simultaneous browsing and listening of several news articles.
Reference: 17. <author> Kobayashi, Minoru and Chris Schmandt. </author> <title> Dynamic Soundscape: Mapping Time to Space for Audio Browsing. </title> <booktitle> Proceedings of CHI 97, </booktitle> <month> March </month> <year> 1997. </year>
Reference-contexts: Kobayashi introduced a technique for browsing audio by allowing listeners to switch their attention between moving sound sources that play multiple portions of a single audio recording <ref> [17] </ref>. An audio landscape with directional sound sources and overlapping auditory streams ( audio-braiding) can also provide a listening environment for browsing multiple audio sources easily [18]. On a wearable device, spatial audio requires the use of headphones or shoulder mounted directional speakers.
Reference: 18. <author> Maher, Brenden. </author> <title> Navigating a Spatialized Speech Environment through Simultaneous Listening and Tangible Interactions. M.S. </title> <type> Thesis, </type> <institution> Media Arts and Sciences, MIT Media Lab, </institution> <month> Fall </month> <year> 1997. </year>
Reference-contexts: An audio landscape with directional sound sources and overlapping auditory streams ( audio-braiding) can also provide a listening environment for browsing multiple audio sources easily <ref> [18] </ref>. On a wearable device, spatial audio requires the use of headphones or shoulder mounted directional speakers. In noisy environments there will be a greater cognitive load to effectively use spatial audio, yet it can help segregate simultaneous audio streams more easily.
Reference: 19. <author> Roy, Deb K. and Alex Pentland. </author> <title> Adaptive Multimodal Interfaces. </title> <booktitle> Proceedings of the Workshop on Perceptual User Interfaces , Banff, </booktitle> <address> Canada, </address> <month> October </month> <year> 1997. </year>
Reference-contexts: It consists of two directional speakers, mounted on the users shoulders, and a directional microphone placed on the chest. The system uses speaker-independent speech recognition based on AT&Ts Watson API. We are currently incorporating an adaptive speaker dependent speech recognizer, developed at the Media Lab <ref> [19] </ref>. It will allow creation and use of different contexts, each containing a unique lexicon of recognizable words. A button on the Neckset can activate speech recognition or deactivate it in noisy environments.
References-found: 19

