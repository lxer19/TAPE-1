URL: http://www.cs.brown.edu/people/mph/tocs95.ps
Refering-URL: http://www.cs.brown.edu/people/mph/tocs95.html
Root-URL: http://www.cs.brown.edu/
Title: Scalable Concurrent Counting  
Author: Maurice Herlihy Beng-Hong Lim Nir Shavit 
Address: Cambridge, MA 02139  Cambridge, MA 02139  Tel-Aviv, Israel 69978  
Affiliation: Lab,  Laboratory for Computer Science, MIT,  Department of Computer Science, Tel-Aviv University,  
Note: Digital Equipment Corporation, Cambridge Research  
Date: August 12, 1994  
Abstract: The notion of counting is central to a number of basic multiprocessor coordination problems, such as dynamic load balancing, barrier synchronization, and concurrent data structure design. In this paper, we investigate the scalability of a variety of counting techniques for large-scale multiprocessors. We compare counting techniques based on: (1) spin locks, (2) message passing, (3) distributed queues, (4) software combining trees, and (5) counting networks. Our comparison is based on a series of simple benchmarks on a simulated 64-processor Alewife machine, a distributed-memory multiprocessor currently under development at MIT. Although locking techniques are known to perform well on small-scale, bus-based multiprocessors, serialization limits performance and contention can degrade performance. Both counting networks and combining trees substantially outperform the other methods by avoiding serialization and alleviating contention, although combining tree throughput is more sensitive to variations in load. A comparison of shared-memory and message-passing implementations of counting networks and combining trees shows that message-passing implementations have substantially higher throughput. A preliminary version of this report appeared in the Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, July 1992, San Diego, CA [16]. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal and M. Cherian. </author> <title> Adaptive backoff synchronization techniques. </title> <booktitle> In Proceedings of the 16th international symposium on computer architecture, </booktitle> <month> June </month> <year> 1989. </year> <month> 28 </month>
Reference-contexts: In this paper, we present the results of an experimental investigation of the scalability of a variety of software counting techniques. We consider five basic techniques: 1. Lock-based counters, encompassing both test-and-test-and-set [21] locks with exponential backoff <ref> [1, 4, 15] </ref>, and a version of the MCS queue lock that relies only on atomic swaps [19]. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3.
Reference: [2] <author> A. Agarwal et al. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <booktitle> In Proceedings of Workshop on Scalable Shared Memory Multiprocessors. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year> <note> An extended version of this paper has been submitted for publication, and appears as MIT/LCS Memo TM-454, </note> <year> 1991. </year>
Reference-contexts: A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees [12, 25]. 5. Counting networks [5]. For each technique, we ran a series of simple benchmarks on a simulated 64-processor Alewife machine <ref> [2] </ref>, a cache-coherent distributed-memory machine currently under development at MIT. Our experiments were done on the ASIM simulator, an accurate cycle-by-cycle simulator for the Alewife architecture. ASIM is the principal simulator used by the Alewife research group. <p> In this paper, we use their Bitonic counting network, whose layout is isomorphic to the Bitonic sorting network of Batcher [6]. Henceforth, we use "counting network" to mean "Bitonic counting network." 3 Experimental Methodology The MIT Alewife multiprocessor <ref> [2] </ref> is a cache-coherent, distributed-memory multiprocessor that supports the shared-memory programming abstraction. node consists of a Sparcle processor [3], an FPU, 64KB of cache memory, a 4MB portion of globally-addressable memory, the Caltech MRC network router, and the Alewife Communications and Memory Management Unit (CMMU) [17].
Reference: [3] <author> Anant Agarwal, John Kubiatowicz, David Kranz, Beng-Hong Lim, Donald Yeung, Godfrey D'Souza, and Mike Parkin. Sparcle: </author> <title> An Evolutionary Processor Design for Multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-61, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Henceforth, we use "counting network" to mean "Bitonic counting network." 3 Experimental Methodology The MIT Alewife multiprocessor [2] is a cache-coherent, distributed-memory multiprocessor that supports the shared-memory programming abstraction. node consists of a Sparcle processor <ref> [3] </ref>, an FPU, 64KB of cache memory, a 4MB portion of globally-addressable memory, the Caltech MRC network router, and the Alewife Communications and Memory Management Unit (CMMU) [17]. The CMMU implements a cache-coherent globally-shared address space 13 with the LimitLESS cache-coherence protocol [8].
Reference: [4] <author> T.E. Anderson. </author> <title> The performance of spin lock alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: In this paper, we present the results of an experimental investigation of the scalability of a variety of software counting techniques. We consider five basic techniques: 1. Lock-based counters, encompassing both test-and-test-and-set [21] locks with exponential backoff <ref> [1, 4, 15] </ref>, and a version of the MCS queue lock that relies only on atomic swaps [19]. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3. <p> Earlier experimental work on small-scale multiprocessors has shown that spin locks with exponential backoff and queue locks both perform well for certain kinds of problems on bus-based architectures <ref> [4, 15, 19] </ref>. Nevertheless, our results indicate that these techniques do not scale well to large-scale distributed memory multiprocessors. <p> A preliminary version of some of these results appeared in [16]. This paper extends the earlier paper in the following ways. * We revise the queue-lock-based counter to use the MCS queue lock instead of the Anderson queue lock <ref> [4] </ref>. * We add an analysis of a centralized message-based counter. * We add message-passing implementations of combining trees and count ing networks, which we have found to be the most scalable of all the techniques considered. * We show the importance of parallelism for scalable performance of shared data structures. <p> To increment the counter, a processor must acquire the lock, read and increment the memory location, and release the lock. We consider two spin lock algorithms: test-and-test-and-set with exponential backoff <ref> [4, 15] </ref>, and a version of the MCS queue lock that relies only on atomic swaps [19]. Message-based counter In this technique, the shared counter is represented by a private memory location owned by a unique processor.
Reference: [5] <author> J. Aspnes, M.P. Herlihy, and N. Shavit. </author> <title> Counting networks and multiprocessor coordination. </title> <booktitle> In Proceedings of the 23rd Annual Symposium on Theory of Computing, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: A message-based counter, in which a single processor increments the counter in response to messages. 3. A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees [12, 25]. 5. Counting networks <ref> [5] </ref>. For each technique, we ran a series of simple benchmarks on a simulated 64-processor Alewife machine [2], a cache-coherent distributed-memory machine currently under development at MIT. Our experiments were done on the ASIM simulator, an accurate cycle-by-cycle simulator for the Alewife architecture. <p> Counting network A counting network <ref> [5] </ref> is a highly concurrent data structure used to implement a counter. An abstract counting network, like a sorting network [9], is a directed graph whose nodes are simple computing elements called balancers, and whose edges are called wires. <p> Counting networks achieve a high level of throughput by decomposing interactions among processors into pieces that can be performed in parallel, effectively reducing memory contention. Aspnes, Herlihy, and Shavit <ref> [5] </ref> give two O (log 2 n) depth counting networks. In this paper, we use their Bitonic counting network, whose layout is isomorphic to the Bitonic sorting network of Batcher [6].
Reference: [6] <author> K.E. Batcher. </author> <title> Sorting networks and their applications. </title> <booktitle> In Proceedings of AFIPS Joint Computer Conference, </booktitle> <pages> pages 334-338, </pages> <year> 1968. </year>
Reference-contexts: Aspnes, Herlihy, and Shavit [5] give two O (log 2 n) depth counting networks. In this paper, we use their Bitonic counting network, whose layout is isomorphic to the Bitonic sorting network of Batcher <ref> [6] </ref>.
Reference: [7] <author> B. Bershad. </author> <title> Practical considerations for lock-free concurrent objects. </title> <type> Technical Report CMU-CS-91-183, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: The increment models a process taking an index, and the random pause represents the execution of the loop iteration for that index. This benchmark is similar to Bershad's benchmark for lock-free synchronization <ref> [7] </ref>. 3.3 Job Queue Benchmark A job queue is a load balancing technique in which processes dynamically insert and remove jobs from a shared queue. Each process alternates de-queuing a job, working on the job for some duration, and enqueuing a job.
Reference: [8] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: The CMMU implements a cache-coherent globally-shared address space 13 with the LimitLESS cache-coherence protocol <ref> [8] </ref>. The LimitLESS cache--coherence protocol maintains a small, fixed number of directory pointers in hardware, and relies on software trap handlers to handle cache-coherence actions when the number of read copies of a cache block exceeds the limited number of hardware directory pointers.
Reference: [9] <author> T.H. Cormen, C.E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1990. </year>
Reference-contexts: Counting network A counting network [5] is a highly concurrent data structure used to implement a counter. An abstract counting network, like a sorting network <ref> [9] </ref>, is a directed graph whose nodes are simple computing elements called balancers, and whose edges are called wires. Each token (input item) enters on one of the network's w n input wires, traverses a sequence of balancers, and leaves on an output wire.
Reference: [10] <author> Thorsten von Eicken, David Culler, Seth Goldstein, and Klaus Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In 19th International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1992. </year>
Reference-contexts: The message interface also allows us to use message-passing operations to implement the synchronization operations. An incoming message traps the processor and invokes a user-defined message handler. The message handler can be atomic with respect to other message handlers in the style of Active Messages <ref> [10] </ref>. Our experiments were done on the ASIM simulator, an accurate cycle-by-cycle simulator for the Alewife architecture. This is the principal simulator used by the Alewife research group.
Reference: [11] <author> E. Freudenthal and A. Gottlieb. </author> <title> Processor coordination with fetch-and-increment. </title> <booktitle> In Proceedings of the 4th ASPLOS, </booktitle> <month> April 91, </month> <pages> pages 260-268. 29 </pages>
Reference-contexts: 1 Introduction The notion of counting is central to a number of basic multiprocessor coordination problems, such as dynamic load balancing, barrier synchronization, and concurrent data structure design. (See Freudenthal and Gottlieb <ref> [11] </ref> for further examples.) For our purposes, a counter is an object that holds an integer value, and provides a fetch-and-increment operation that increments the counter's value and returns its previous value.
Reference: [12] <author> J.R. Goodman, M.K. Vernon, and P.J. Woest. </author> <title> Efficient synchronization primitives for large-scale cache-coherent multiprocessors. </title> <booktitle> In Proceedings of the 3rd ASPLOS, </booktitle> <pages> pages 64-75. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1989. </year>
Reference-contexts: A message-based counter, in which a single processor increments the counter in response to messages. 3. A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees <ref> [12, 25] </ref>. 5. Counting networks [5]. For each technique, we ran a series of simple benchmarks on a simulated 64-processor Alewife machine [2], a cache-coherent distributed-memory machine currently under development at MIT. Our experiments were done on the ASIM simulator, an accurate cycle-by-cycle simulator for the Alewife architecture. <p> Hardware combining trees were first proposed as a feature of the NYU Ultracomputer [13]. For our experiments, we implemented the software combining tree algorithm proposed by Goodman et al. in <ref> [12] </ref>. This algorithm can compute a general Fetch-and- operation, although we use it for the special case of Fetch-and-Increment. <p> Because Alewife does not have a QOSB primitive, we have omitted all calls to QOSB. We also mark in comments a change to enhance performance of the algorithm on Alewife, and a fix to a bug in the original code. (The reader is referred to the original paper <ref> [12] </ref> for a more complete description of the algorithm.) An earlier software combining tree algorithm proposed by Yew et al. [25] is not suitable for implementing a shared counter because it disallows asynchronous combining of requests. We investigated two ways to implement combining trees.
Reference: [13] <author> A. Gottlieb, R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, and M. Snir. </author> <title> The NYU Ultracomputer designing an MIMD parallel computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-32(2):175-189, </volume> <month> Febru-ary </month> <year> 1984. </year>
Reference-contexts: The combined requests are applied to the counter when they reach the root, and the results are sent back down the tree and distributed to the waiting processes. Hardware combining trees were first proposed as a feature of the NYU Ultracomputer <ref> [13] </ref>. For our experiments, we implemented the software combining tree algorithm proposed by Goodman et al. in [12]. This algorithm can compute a general Fetch-and- operation, although we use it for the special case of Fetch-and-Increment.
Reference: [14] <author> A. Gottlieb, B.D. Lubachevsky, and L. Rudolph. </author> <title> Basic techniques for the efficient coordination of very large numbers of cooperating sequential processors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(2) </volume> <pages> 164-189, </pages> <month> April </month> <year> 1983. </year>
Reference: [15] <author> G. Graunke and S. Thakkar. </author> <title> Synchronization algorithms for shared-memory multiprocessors. </title> <journal> IEEE Computer, </journal> <volume> 23(6) </volume> <pages> 60-70, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: In this paper, we present the results of an experimental investigation of the scalability of a variety of software counting techniques. We consider five basic techniques: 1. Lock-based counters, encompassing both test-and-test-and-set [21] locks with exponential backoff <ref> [1, 4, 15] </ref>, and a version of the MCS queue lock that relies only on atomic swaps [19]. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3. <p> Earlier experimental work on small-scale multiprocessors has shown that spin locks with exponential backoff and queue locks both perform well for certain kinds of problems on bus-based architectures <ref> [4, 15, 19] </ref>. Nevertheless, our results indicate that these techniques do not scale well to large-scale distributed memory multiprocessors. <p> To increment the counter, a processor must acquire the lock, read and increment the memory location, and release the lock. We consider two spin lock algorithms: test-and-test-and-set with exponential backoff <ref> [4, 15] </ref>, and a version of the MCS queue lock that relies only on atomic swaps [19]. Message-based counter In this technique, the shared counter is represented by a private memory location owned by a unique processor.
Reference: [16] <author> M.P. Herlihy, B-H. Lim, and N. </author> <title> Shavit Low Contention Load Balancing on Large-Scale Multiprocessors. </title> <booktitle> In Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <address> July 1992, San Diego, CA. </address>
Reference-contexts: In this respect, counting networks have a substantial advantage over combining trees in systems where individual processes might incur arbitrary delays, an important property for concurrent data structure design. A preliminary version of some of these results appeared in <ref> [16] </ref>.
Reference: [17] <author> John Kubiatowicz, David Chaiken, and Anant Agarwal. </author> <title> The Alewife CMMU: Addressing the Multiprocessor Communications Gap. </title> <booktitle> In HOTCHIPS, </booktitle> <month> August </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: Methodology The MIT Alewife multiprocessor [2] is a cache-coherent, distributed-memory multiprocessor that supports the shared-memory programming abstraction. node consists of a Sparcle processor [3], an FPU, 64KB of cache memory, a 4MB portion of globally-addressable memory, the Caltech MRC network router, and the Alewife Communications and Memory Management Unit (CMMU) <ref> [17] </ref>. The CMMU implements a cache-coherent globally-shared address space 13 with the LimitLESS cache-coherence protocol [8].
Reference: [18] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In International Supercomputing Conference (ICS) 1993, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: The current implementation of the Alewife CMMU has 5 hardware directory pointers per cache line. The CMMU also interfaces the Sparcle processor to the interconnection network, allowing the use of an efficient message-passing interface for communication <ref> [18] </ref>. The LimitLESS protocol relies on this interface to handle coherence operations in software. The message interface also allows us to use message-passing operations to implement the synchronization operations. An incoming message traps the processor and invokes a user-defined message handler.
Reference: [19] <author> J.M. Mellor-Crummey and M.L. Scott. </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: We consider five basic techniques: 1. Lock-based counters, encompassing both test-and-test-and-set [21] locks with exponential backoff [1, 4, 15], and a version of the MCS queue lock that relies only on atomic swaps <ref> [19] </ref>. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3. A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees [12, 25]. 5. Counting networks [5]. <p> with exponential backoff [1, 4, 15], and a version of the MCS queue lock that relies only on atomic swaps <ref> [19] </ref>. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3. A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees [12, 25]. 5. Counting networks [5]. For each technique, we ran a series of simple benchmarks on a simulated 64-processor Alewife machine [2], a cache-coherent distributed-memory machine currently under development at MIT. <p> Earlier experimental work on small-scale multiprocessors has shown that spin locks with exponential backoff and queue locks both perform well for certain kinds of problems on bus-based architectures <ref> [4, 15, 19] </ref>. Nevertheless, our results indicate that these techniques do not scale well to large-scale distributed memory multiprocessors. <p> To increment the counter, a processor must acquire the lock, read and increment the memory location, and release the lock. We consider two spin lock algorithms: test-and-test-and-set with exponential backoff [4, 15], and a version of the MCS queue lock that relies only on atomic swaps <ref> [19] </ref>. Message-based counter In this technique, the shared counter is represented by a private memory location owned by a unique processor. To increment the counter, a processor sends a request message to that unique processor and waits for a reply. <p> If the queue was empty, the process owns the lock; otherwise it waits for a signal from its predecessor. To release a lock, a process checks to see if it has a waiting successor. If so, it signals that 5 successor, otherwise it empties the queue. See <ref> [19] </ref> for further details. The queue-based counter improves on a simple lock-based counter in the following way. Instead of keeping the counter value in a fixed memory location, it is kept at the processor that currently holds the lock. <p> If there is no next processor, the current value is stored in the lock. This technique combines synchronization with data transfer and reduces communication requirements. Figure 1 shows the pseudocode for this counter following the style of <ref> [19] </ref>. Software combining tree In a combining tree, increment requests enter at a leaf of the tree. When two requests simultaneously arrive at the same node, they are combined; one process advances up the tree with the combined request, while the other waits for the result.
Reference: [20] <author> J.M. Mellor-Crummey and T.J. LeBlanc. </author> <title> A software instruction counter. </title> <booktitle> In Proceedings of the 3rd ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 78-86, </pages> <month> April 89. </month>
Reference-contexts: Each loop iteration covers a rectangle in the screen. Because rectangles are independent of one another, they can be rendered in parallel, but because some rectangles take unpredictably longer than others, dynamic load balancing is important for performance.) A similar application is a software instruction counter <ref> [20] </ref>. 14 procedure do_counting (C : ^counter, iters : int) i : int := 0 repeat fetch_and_increment (counter) i := i + 1 while (i &lt; iters) procedure do_index (C : ^counter, iters : int, w : int) repeat i := fetch_and_increment (counter) delay (random () mod w) while (i &lt;
Reference: [21] <author> L. Rudolph and Z. Segall. </author> <title> Dynamic decentralized cache schemes for MIMD parallel processors. </title> <booktitle> In 11th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 340-347, </pages> <month> June </month> <year> 1984. </year> <month> 30 </month>
Reference-contexts: In this paper, we present the results of an experimental investigation of the scalability of a variety of software counting techniques. We consider five basic techniques: 1. Lock-based counters, encompassing both test-and-test-and-set <ref> [21] </ref> locks with exponential backoff [1, 4, 15], and a version of the MCS queue lock that relies only on atomic swaps [19]. 2. A message-based counter, in which a single processor increments the counter in response to messages. 3.
Reference: [22] <author> B.J. Smith. </author> <title> Architecture and Applications of the HEP Multiprocessor Computer System, </title> <journal> Society of Photooptical Instrumentation Engineers, 1981, </journal> <volume> Vol 298, </volume> <pages> pages 241-248. </pages>
Reference-contexts: Each process alternates de-queuing a job, working on the job for some duration, and enqueuing a job. The queue itself consists of an array with a flag on each element that signifies if the element is present or not. We use full/empty bits <ref> [22] </ref> on Alewife to implement this flag. A head counter indicates the first full element, and a tail counter indicating the first empty element. The elements of the array are distributed across the machine.
Reference: [23] <author> M.P. Herlihy, N. Shavit, and O. Waarts. </author> <booktitle> Linearizable Counting Networks In Proceedings of the 32 nd Annual Symposium on Foundations of Computer Science, </booktitle> <address> San Juan, Puerto Rico, </address> <month> October </month> <year> 1991, </year> <pages> pp. 526-535. </pages>
Reference-contexts: To illustrate the relative importance of these two properties, we now investigate a counter implementation that has low contention, but does not attain a high degree of parallelism. A counter is linearizable <ref> [23] </ref> if the values it returns are consistent with the real-time order of the matching requests.
Reference: [24] <author> M.P. Herlihy and J.M. Wing. </author> <title> Linearizability: A correctness condition for concurrent objects. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(3) </volume> <pages> 463-492, </pages> <month> July </month> <year> 1990. </year>
Reference: [25] <author> P.C Yew, N.F. Tzeng, and D.H. Lawrie. </author> <title> Distributing hot-spot addressing in large-scale multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 388-395, </pages> <month> April </month> <year> 1987. </year> <month> 31 </month>
Reference-contexts: A message-based counter, in which a single processor increments the counter in response to messages. 3. A queue-based counter, which is a version of the MCS queue lock [19] optimized for distributed counting. 4. Software combining trees <ref> [12, 25] </ref>. 5. Counting networks [5]. For each technique, we ran a series of simple benchmarks on a simulated 64-processor Alewife machine [2], a cache-coherent distributed-memory machine currently under development at MIT. Our experiments were done on the ASIM simulator, an accurate cycle-by-cycle simulator for the Alewife architecture. <p> in comments a change to enhance performance of the algorithm on Alewife, and a fix to a bug in the original code. (The reader is referred to the original paper [12] for a more complete description of the algorithm.) An earlier software combining tree algorithm proposed by Yew et al. <ref> [25] </ref> is not suitable for implementing a shared counter because it disallows asynchronous combining of requests. We investigated two ways to implement combining trees. In a shared-memory implementation, each tree node is represented as a data structure in shared memory. Simple test-and-set locks are used for atomically updating the nodes.
References-found: 25

