URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/jagota/vol2_2.ps.gz
Refering-URL: http://http.icsi.berkeley.edu/ftp/global/pub/ai/jagota/
Root-URL: http://http.icsi.berkeley.edu
Title: Weightless Neural Models: A Review of Current and Past Works the challenges and future directions
Author: Teresa B. Ludermir Andre de Carvalho Antonio P. Braga and Marcilio C. P. de Souto 
Note: The paper describes the most important works in WNNs found in the literature, pointing out  is also presented.  
Address: S~ao Paulo, S~ao Carlos, Brazil  College, London, UK  
Affiliation: Depto. de Informatica, Univ. Federal de Pernambuco, Recife, Brazil Laboratorio de Intelig^encia Computacional, Univ. de  Depto. de Engenharia Eletronica, Univ. Federal de Minas Gerais, Belo Horizonte, Brazil Dept. of Electrical Engineering, Imperial  
Abstract: This paper presents a survey of a class of neural models known as Weightless Neural Networks (WNNs). As the name suggests, these models do not use weighted connections between nodes. Instead, a different kind of neuron model, usually based on RAM memory devices, is used. In the literature, the terms "RAM-based" and "n-tuple based" systems are also commonly used to refer to WNNs. WNNs are being widely investigated, motivating relevant applications and two international workshops in the last few years. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. J. L. Adeodato and J. Taylor. </author> <title> Storage capacity of RAM-based neural networks: pyramids. </title> <booktitle> Neural Networks World, </booktitle> <volume> 6(3) </volume> <pages> 241-249, </pages> <year> 1996. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 56 </note>
Reference-contexts: Nevertheless, it was also shown that a constrained class of pyramids, called multi-pyramidal architectures (a set of disjoint tress with bounded depth), can be trained in polynomial time. Studies on the storage capacity of pyramidal architectures were presented in <ref> [1, 86] </ref>. In [86], such a problem was analysed into the the context of information theory. <p> 00...0 C [0] C [ ]100...1 00...0 C [0] C [ ]100...1 00...0 C [0] C [ ]100...1 00...0 input weightless output C [0] C [ ]100...1 00...0 C [0] C [ ]100...1 00...0 C [0] C [ ]100...1 00...0 nodes layer 1 layer 2 layer 3 vector analysis in <ref> [1] </ref> showed that, in pyramids, the reduction in storage capacity is bigger than the reduction in cost (the total amount of sites in a network) when the node fan-in is diminished. The use of three-logic values and pyramid structures by PLN networks represented a breakthrough in the WNN research. <p> The evolution of such a model was shown to be equivalent to that of networks of noisy (probabilistic) RAMs or pRAMs [49]. The pRAM is also an extension of the PLN like the MPLN, but in which continuous probabilities can be stored, that is, value in the range <ref> [0; 1] </ref>. This kind of node outputs 1 (spike) with frequency related to the memory content being addressed. The pRAM was further extended, so as to be able to map continuous input to binary outputs. Such an extension is called the integrating pRAM (i-pRAM) [51]. <p> Hence, in this binary input case, the mean value y of the pRAM output is directly given by the memory content C [I] being accessed, that is, y = C [I]. Now, let x = fx 1 ; x 2 ; :::; x N g (x 2 <ref> [0; 1] </ref> N ) be a real-valued input for A. <p> With T &gt; 1 and X a (x) 2 <ref> [0; 1] </ref>, the pRAM model is much more complex, and this is the main difference with the MPLN: the pRAM can output frequencies truly dependent on the probabilities stored in the memory contents. Also, in this continuous version the pRAM exhibits generalisation properties. <p> On the other hand, cognitive computing needs efficient storage for "experience". To solve this problem, ways to decompose "experience" need to be considered. Studies about storage capacity of RAM-based networks are presented in <ref> [1, 33, 31, 86, 98] </ref>. 2. Planning. One of the major differences between rule-based and neural computing is the notion that a neural net can find the solution for a planning problem directly, whereas rule-based systems have to search quasi-exhaustively. Planning is a somewhat neglected area of neurocomputing.
Reference: [2] <author> P. J. L. Adeodato and J. G. Taylor. </author> <title> Analysis of the storage capacity of RAM-based neural networks. </title> <editor> In D. L. Bisset, editor, </editor> <booktitle> Proceedings of the Weightless Neural Network Workshop (WNNW95), </booktitle> <pages> pages 103-110, </pages> <institution> University of Kant at Canterbury, UK, </institution> <year> 1995. </year>
Reference-contexts: The attractor properties of GNUs were also studied by Braga, who also demonstrated that the GNU's retrieval time is a function of the number of associations stored [32]. Recently, Adeodato & Taylor <ref> [2] </ref> developed another approach for the storage capacity assessment of WNNs which can be applied to different nodes and topologies, including GNUs. In such an approach, the probability of occurrence of collision is expressed as a function of the probability distribution of the training set. <p> Differently, in Hopfield networks the cost of storage is only O (k 2 ). Nevertheless, weightless networks such as GNUs do not need to be fully connected as Hopfield networks. In <ref> [2, 32] </ref>, two different methods to design partially connected GNUs with high confidence in the storage process are proposed.
Reference: [3] <author> R. Al-Alawi and T. J. Stonham. </author> <title> A training strategy and functionality analisys of digital multi-layer neural networks. </title> <journal> Journal of Intelligent Systems, </journal> <volume> 2 </volume> <pages> 53-93, </pages> <year> 1992. </year>
Reference-contexts: A generic gradient descendent learning algorithm, proposed by Myers, [83], uses several presentations of the training set to teach pyramids of PLNs by using a reward and a punish phase. In <ref> [3] </ref>, Al-Alawi & Stonham introduced a learning procedure, the back-propagation search algorithm, for multi-layer WNNs and applied it to pyramids of PLNs. <p> For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel. A training algorithm for this task can be found in <ref> [3] </ref>. Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [4] <author> I. Aleksander. </author> <title> Self-adaptive universal logic circuits. </title> <journal> IEE Electronics Letters, </journal> <volume> 2:231, </volume> <year> 1966. </year>
Reference-contexts: WNNs have their origin on the N -tuple sampling machines described by Bledsoe and Browning in the late 50s [27]. Aleksander took a great deal of interest in adaptive learning networks using N -tuple sampling machines; he suggested a universal logic circuit as the node of a learning network <ref> [4] </ref>. He also introduced the RAM node and the SLAM (Stored Logic Adaptive Microcircuit), which were designed for research purposes before the availability of integrated circuit memories.
Reference: [5] <author> I. Aleksander. </author> <title> Emergent intelligent properties of progressively structured pattern recognition nets. </title> <journal> Pattern Recognition Letters, </journal> <volume> 1 </volume> <pages> 375-384, </pages> <year> 1983. </year>
Reference-contexts: Generalisation may be understood as the ability to provide the correct output for patterns not seen in the training phase. Conventional RAM nodes produce the correct output only for those input patterns stored in the training phase. However, there is generalisation in networks composed of RAM nodes <ref> [5] </ref>. Generalisation can be introduced by considering the hamming distance from training patterns or masks. Although RAM nodes deal only with binary values, by preprocessing the input data, these nodes can be extended to handle scalar values.
Reference: [6] <author> I. Aleksander. </author> <title> Canonical neural nets based on logic nodes. </title> <booktitle> In Proceedings of the IEE International Conference on Artificial Neural Network, </booktitle> <pages> pages 110-114, </pages> <address> London, </address> <year> 1989. </year>
Reference-contexts: Before the PLN, all the RAM-based neural models which learnt by updating the values stored in the memory contents were single-layer architectures. Aleksander <ref> [6] </ref> proposed the use of PLN nodes in multilayer architectures, called pyramids, which can be seen in Figure 3. A pyramid is a tree structure in which each layer usually has a fixed number of neurons with a fan-out of 1 and low fan-in.
Reference: [7] <author> I. Aleksander. </author> <title> Ideal neurons for neural computers. </title> <editor> In R. Eckmiller, G. Hartmann, and G. Hauske, editors, </editor> <booktitle> Parallel Processing in Neural Systems and Computers, </booktitle> <pages> pages 225-228. </pages> <publisher> Elsevier Science, </publisher> <year> 1989. </year>
Reference-contexts: This is because too many entries are left in the u-state. Another characteristic of PLNs and RAM-based neurons mentioned earlier, with exception of the pRAM, is that they only generalise at the network level: individual nodes do not have generalisation properties. To improve WNNs models, Aleksander <ref> [7] </ref> suggested a modification in the PLN learning algorithm by including a spreading phase after patterns are stored. The effect of spreading is to create classes or clusters having the stored patterns as their centroids. <p> If a contradictory region is accessed the output is randomly selected between 0 or 1. Since individual nodes with such learning scheme do generalise (implicitly, due to the spreading algorithm), they are known in the literature as Generalising Random Access Memories (GRAMs) <ref> [7] </ref>. A new spreading algorithm for GRAMs called Combined Generalisation Algorithm (CGA) was proposed in [11]. Such an algorithm combines the properties of weightless neural systems and those of weighted Hebbian learning systems [94]. <p> In contrast, in the weighted neural models, individual neurons can be sensitive to particular input variables. Nevertheless, generalisation at the node level can also be obtained with some weightless nodes such as GRAM <ref> [7] </ref> and pRAM [51]. * Content addressable systems: WNNs with the same kind of configuration of Hopfield networks, that is, fully connected auto-associative networks, were analysed by Lucy in [70].
Reference: [8] <author> I. Aleksander. </author> <title> An introduction to neural computing. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1990. </year>
Reference-contexts: The construction of the WISARD (Wilkie, Stonham & Aleksander's Recognition Device) was then possible, with its first prototype completed in 1981 [14]. The machine was patented and produced commercially from 1984. After WISARD, other weightless models were proposed in the literature, such as PLNs, pRAMs, GSNs, and GRAMs <ref> [8, 18, 22] </ref>. Also, in the literature, the terms "RAM-based" and "n-tuple based" have been used to refer to WNNs. <p> A conventional RAM node by itself does not generalise. That is, in WNNs, generalisation is sensitive to similarities to entire patterns instead of their constituent variables <ref> [8] </ref>. Thus, generalisation in WNNs is affected mostly by the node fan-in and the connectivity pattern of the network. In contrast, in the weighted neural models, individual neurons can be sensitive to particular input variables.
Reference: [9] <author> I. Aleksander. </author> <title> Connectionism or weightless neurocomputing. </title> <editor> In T. Kohonen, K. Makisara, O. Sim-ula, and J. Kangas, editors, </editor> <booktitle> Proceedings of International Conference on Artificial Neural Networks (ICANN91), </booktitle> <volume> volume 2, </volume> <pages> pages 991-1000, </pages> <address> Helsinki, </address> <year> 1991. </year>
Reference-contexts: Output threshold 8 Comparative Studies Two types of comparative study associating weighted and weightless networks have been presented in the literature. The first focuses on learning, generalisation, content addressable systems, and hardware implementation. The second was presented in <ref> [9] </ref>. Such a study was directed specifically at the Parallel Distributed Processing (PDP) framework [94]. In these comparative studies, the details of the weightless systems are given while many details of the weighted are omitted for being well known. <p> Now, definitions of all the major concepts concerned with the PDP framework (set of processing units, state of activation, output functions, pattern of connectivity, propagation rule, learning rule and environ ment) will be given to WNNs. These definitions are briefly described below for WNNs <ref> [9] </ref>: 1. Set of processing units: the weightless nodes. 2. State of activation: a vector of N elements where each element is the decimal representation of the input of the correspondent node. 3.
Reference: [10] <author> I. Aleksander and R. C. Albrow. </author> <title> Pattern recognition with adaptive logic elements. </title> <booktitle> In Proceedings of IEE-NPL Conference Pattern Recognition, </booktitle> <year> 1968. </year>
Reference-contexts: Generalisation can be introduced by considering the hamming distance from training patterns or masks. Although RAM nodes deal only with binary values, by preprocessing the input data, these nodes can be extended to handle scalar values. The thermometer coding <ref> [10] </ref>, the rank coding [16], the interval coding Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 43 or Minchinton cells [26], and more recently the CMAC coding [69], are examples of methods which can be used to process scalar data for RAM networks.
Reference: [11] <author> I. Aleksander, T. J. Clarke, and A. P. Braga. </author> <title> Binary neural systems: combining weighted and weightless properties. </title> <journal> IEE Intelligent Systems Engineering, </journal> <volume> 3 </volume> <pages> 211-221, </pages> <year> 1994. </year>
Reference-contexts: Since individual nodes with such learning scheme do generalise (implicitly, due to the spreading algorithm), they are known in the literature as Generalising Random Access Memories (GRAMs) [7]. A new spreading algorithm for GRAMs called Combined Generalisation Algorithm (CGA) was proposed in <ref> [11] </ref>. Such an algorithm combines the properties of weightless neural systems and those of weighted Hebbian learning systems [94]. A new metric measure called Discriminating Distance was described, which takes into consideration the discrimination power of the input variables by assigning a discrimination coefficient to each one of them.
Reference: [12] <author> I. Aleksander and H. Morton. </author> <title> General neural unit: retrieval performance. </title> <journal> IEE Electronics Letters, </journal> <volume> 27 </volume> <pages> 1776-1778, </pages> <year> 1991. </year>
Reference-contexts: GRAMs have been often used in a recurrent architecture called GNUs, which will be described next. Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 51 6.1 GNU The General Neural Unit (GNU) <ref> [12] </ref> is a single layer recurrent WNNs that uses GRAMs as nodes. The topology of GNUs differs from the classical Hopfield recurrent model [57] by having an internal feedback field as well as external feedforward connections. <p> Aleksander and Morton studied the retrieval properties of GNUs with external inputs and demonstrated that the optimum retrieval performance is obtained when there is perfect balance between the feedforward and feedback fields <ref> [12] </ref>. Braga [33], by using a probabilistic approach, obtained a set of expressions that enable the prediction of contradictions happening during the storage process of partially connected GNUs. <p> The main results of the comparison are listed below: * Learning: In contrast to the weighted neural models, there are several one-shot learning algorithms for WNNs, where training takes only one epoch <ref> [12, 42, 47, 90] </ref>. Also, like for the weighted neural systems, there exist many iterative learning schemes for WNNs. For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel.
Reference: [13] <author> I. Aleksander and T. J. Stonham. </author> <title> Guide to pattern recognition using random-acess memories. </title> <booktitle> Computers and Digital Techniques, </booktitle> <volume> 2 </volume> <pages> 29-40, </pages> <year> 1979. </year>
Reference-contexts: This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems [90, 91]. The results obtained, together with previous ones <ref> [13, 88, 98] </ref>, show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework [31, 72, 92]. <p> The way that the network is connected to its input is called the input mapping, or the connectivity pattern. Although the input mapping is chosen at random, such a mapping is often a fixed parameter of the network <ref> [13, 90] </ref>. At the discriminator's output, an adder sums the outputs of the RAMs, producing what is called Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 44 the discriminator's response r. Before training takes place, all the k2 N one-bit words are set to zero, and training consists in their modification. <p> The pattern is assigned the class corresponding to the discriminator with the highest response r. The performance of the N -tuple method depends mainly on the size of the N-tuple chosen <ref> [13, 89, 100, 101] </ref>. Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications [89, 90].
Reference: [14] <author> I. Aleksander, W. V Thomas, and P. A. Bowden. WISARD: </author> <title> a radical step forward in image recognition. </title> <journal> Sensor Review, </journal> <volume> 4(3) </volume> <pages> 120-124, </pages> <year> 1984. </year>
Reference-contexts: The construction of the WISARD (Wilkie, Stonham & Aleksander's Recognition Device) was then possible, with its first prototype completed in 1981 <ref> [14] </ref>. The machine was patented and produced commercially from 1984. After WISARD, other weightless models were proposed in the literature, such as PLNs, pRAMs, GSNs, and GRAMs [8, 18, 22]. Also, in the literature, the terms "RAM-based" and "n-tuple based" have been used to refer to WNNs. <p> These networks are modified so that instead of having a logic gate to combine the RAMs' outputs, the decision is left to the maximum response detector. These modified RAM networks, which are used both in SLAMs and WISARD <ref> [14] </ref>, are called discriminators (Figure 2) 2.1 WISARD The advances in integrated circuit technology during the 70s led Aleksander and Stonham to implement N-tuple machines using conventional RAM devices. This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. <p> modified RAM networks, which are used both in SLAMs and WISARD <ref> [14] </ref>, are called discriminators (Figure 2) 2.1 WISARD The advances in integrated circuit technology during the 70s led Aleksander and Stonham to implement N-tuple machines using conventional RAM devices. This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems [90, 91]. The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. <p> In general, nodes such as GRAMs, which require the spreading of information to neighbouring memory locations, are not straightforwardly implementable in hardware such as the WISARD system. So, they might require an amount of circuitry comparable to that of weighted neural systems. In <ref> [14, 41] </ref>, respectively, examples of hardware implementation of the WISARD system (discrete components) and pRAM networks (VLSI) are presented, as well as their learning algorithm. In [96], a technique for hardware and software implementation of trained RAM and GSN networks is introduced.
Reference: [15] <author> N. M. Allison and M. J. Johnson. </author> <title> Realisation of self-organising neural maps in f0; 1g n -space. </title> <editor> In J. G. Taylor and C. L. T. Mannion, editors, </editor> <booktitle> New Developments in Neural Computing, </booktitle> <pages> pages 79-86. </pages> <publisher> IOP Publishing Ltd, </publisher> <year> 1989. </year>
Reference-contexts: Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised [23, 47, 52, 58, 63] and unsupervised <ref> [15, 38, 42, 85] </ref> learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in <ref> [15, 38, 85] </ref>. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [16] <author> J. Austin. </author> <title> Grey scale N-tuple processing. </title> <editor> In J. Kittler, editor, </editor> <booktitle> Proceedings of the IV International Conference on Pattern Recognition, </booktitle> <pages> pages 110-119, </pages> <address> Cambridge, </address> <year> 1988. </year>
Reference-contexts: Generalisation can be introduced by considering the hamming distance from training patterns or masks. Although RAM nodes deal only with binary values, by preprocessing the input data, these nodes can be extended to handle scalar values. The thermometer coding [10], the rank coding <ref> [16] </ref>, the interval coding Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 43 or Minchinton cells [26], and more recently the CMAC coding [69], are examples of methods which can be used to process scalar data for RAM networks. The method in [69] has been shown suitable for different applications [90].
Reference: [17] <author> J. Austin. </author> <title> A review of the advanced Distributed Associative Memory (ADAM). </title> <booktitle> In Proceedings of the Weightless Neural Network Workshop (WNNW93), </booktitle> <pages> pages 24-28, </pages> <address> University of York, York, UK, </address> <year> 1993. </year>
Reference-contexts: This architecture is called ADAM (Advanced Distributed Associative Memory) [23]. ADAM uses an N -tuple technique as a pre-processor (a RAM network) to cope with non-linearities and to reduce the saturation of the matrix memories. Such a model has been shown to have a large storage capacity <ref> [17] </ref> and to be tolerant to binary errors, particularly when small sized tuple processors are used [29].
Reference: [18] <author> J. </author> <title> Austin A Review of RAM-based Neural Networks. </title> <booktitle> In Proceedings of the Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pages 58-66, </pages> <address> 1994. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: The construction of the WISARD (Wilkie, Stonham & Aleksander's Recognition Device) was then possible, with its first prototype completed in 1981 [14]. The machine was patented and produced commercially from 1984. After WISARD, other weightless models were proposed in the literature, such as PLNs, pRAMs, GSNs, and GRAMs <ref> [8, 18, 22] </ref>. Also, in the literature, the terms "RAM-based" and "n-tuple based" have been used to refer to WNNs.
Reference: [19] <author> J. Austin. </author> <title> Associative memories and application of neural networks to vision. </title> <editor> In R. Beale, editor, </editor> <booktitle> Handbook of Neural Computation. </booktitle> <publisher> Oxford University Press, </publisher> <year> 1995. </year>
Reference-contexts: Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications <ref> [19] </ref>.
Reference: [20] <author> J. Austin. </author> <title> Distributed associative memories for high-speed symbolic reasoning. </title> <journal> Fuzzy Sets and Systems, </journal> <volume> 82(2) </volume> <pages> 223-233, </pages> <year> 1996. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 57 </note>
Reference-contexts: Planning is a somewhat neglected area of neurocomputing. Learning by exploration, generalisation to new cases and the use of several levels of abstraction simultaneously are promising areas for research. Mrsic-Flogel ([79]) described a weightless system for planning. In <ref> [20] </ref>, a novel symbolic reasoning system based upon ADAM is presented. 3. Temporal Computation. Many aspects of temporal processing demand attention. Ludermir ([71]), developed weightless techniques for the recognition of the syntactic structure of sequences.
Reference: [21] <author> J. Austin. </author> <title> RAM-Based Neural Networks. </title> <publisher> World Scientific, </publisher> <address> York, UK, </address> <year> 1998. </year>
Reference-contexts: The features of these recently described WNN models support the weightless approach as very promising to overcome the challenges in the field of Artificial Neural Networks. A comprehensive collection of recent papers on RAM-based networks can be found in <ref> [21] </ref>. Acknowledgements The authors would like to thank the anonymous reviewers for their constructive comments and valuable suggestions.
Reference: [22] <author> J. Austin. </author> <title> RAM-Based Neural Networks, a Short History. </title> <editor> In J. Austim, editor, </editor> <booktitle> RAM-Based Neural Networks, </booktitle> <pages> pages 3-17, </pages> <address> York, UK, </address> <year> 1998. </year>
Reference-contexts: The construction of the WISARD (Wilkie, Stonham & Aleksander's Recognition Device) was then possible, with its first prototype completed in 1981 [14]. The machine was patented and produced commercially from 1984. After WISARD, other weightless models were proposed in the literature, such as PLNs, pRAMs, GSNs, and GRAMs <ref> [8, 18, 22] </ref>. Also, in the literature, the terms "RAM-based" and "n-tuple based" have been used to refer to WNNs.
Reference: [23] <author> J. Austin and T. J. Stonham. </author> <title> An associative memory for use in image recognition and occlusion analysis. </title> <journal> Image and Vision Computing, </journal> <volume> 5 </volume> <pages> 251-261, </pages> <year> 1987. </year>
Reference-contexts: Neuron functions are stored into look-up tables, which can be implemented using commercially available Random Access Memories (RAMs). Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised <ref> [23, 47, 52, 58, 63] </ref> and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> Finally, it is important to point out that, although these modified methods described above, could achieve a better performance, they often reduce the speed and simplicity advantages of the original algorithm. The use of RAM-based networks incorporating N -tuple methods in associative memories was also investigated in <ref> [23] </ref>. This work was motivated by the generalisation problems faced by Willshaw nets [106]. Such problems were reduced by the development of an associative architecture which combines an N -tuple front-end processor with two-stages of Willshaw net. This architecture is called ADAM (Advanced Distributed Associative Memory) [23]. <p> was also investigated in <ref> [23] </ref>. This work was motivated by the generalisation problems faced by Willshaw nets [106]. Such problems were reduced by the development of an associative architecture which combines an N -tuple front-end processor with two-stages of Willshaw net. This architecture is called ADAM (Advanced Distributed Associative Memory) [23]. ADAM uses an N -tuple technique as a pre-processor (a RAM network) to cope with non-linearities and to reduce the saturation of the matrix memories.
Reference: [24] <author> A. Badr. </author> <title> N-tuple classifier for ECG signals. </title> <editor> In N. M. Allinson, editor, </editor> <booktitle> Proceedings of the Weightless Neural Network Workshop (WNNW93), </booktitle> <pages> pages 29-32, </pages> <address> University of York, York, UK, </address> <year> 1993. </year>
Reference-contexts: This is known as the saturation problem [100]. So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood <ref> [28, 24, 97, 100] </ref>. More recently, in order to overcome the saturation problem, Tarling and Rohwer [98] proposed a simple modification in the original N-tuple training algorithm. <p> Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods <ref> [24, 31, 72, 98, 92] </ref>, where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise.
Reference: [25] <author> A. G. Barto. </author> <title> Learning by statistical cooperation of self-interested neuron-like computing elements. </title> <booktitle> Human Neurobiol., </booktitle> <volume> 4 </volume> <pages> 229-256, </pages> <year> 1985. </year>
Reference-contexts: A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures <ref> [25] </ref> for the WNNs were introduced in [52, 84, 81].
Reference: [26] <author> J. M. Bishop, P. R. Minchinton, and R. J. Mitchell. </author> <title> Real time invariant grey level image processing using digital neural networks. </title> <booktitle> In Proceedings of IMechE Conference, EuroTech Direct'91 Computers in the Engineering Industry, </booktitle> <pages> pages 187-199, </pages> <address> Birmingham, </address> <year> 1991. </year>
Reference-contexts: Although RAM nodes deal only with binary values, by preprocessing the input data, these nodes can be extended to handle scalar values. The thermometer coding [10], the rank coding [16], the interval coding Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 43 or Minchinton cells <ref> [26] </ref>, and more recently the CMAC coding [69], are examples of methods which can be used to process scalar data for RAM networks. The method in [69] has been shown suitable for different applications [90].
Reference: [27] <author> W. Bledsoe and I. Browning. </author> <title> Pattern recognition and reading by machine. </title> <booktitle> In Proceedings of Eastern Joint Computer Conference, </booktitle> <pages> pages 225-232, </pages> <address> Boston, </address> <year> 1959. </year>
Reference-contexts: As an example, Hopfield networks [57] are BNNs, but not WNNs. WNNs have their origin on the N -tuple sampling machines described by Bledsoe and Browning in the late 50s <ref> [27] </ref>. Aleksander took a great deal of interest in adaptive learning networks using N -tuple sampling machines; he suggested a universal logic circuit as the node of a learning network [4].
Reference: [28] <author> W. W. Bledsoe and C. L. Bisson. </author> <title> Improved memory matrices for the n-tuple recognition method. </title> <booktitle> In Proceedings of IRE Joint Computer Conference,11, </booktitle> <pages> pages 414-415, </pages> <year> 1962. </year>
Reference-contexts: This is known as the saturation problem [100]. So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood <ref> [28, 24, 97, 100] </ref>. More recently, in order to overcome the saturation problem, Tarling and Rohwer [98] proposed a simple modification in the original N-tuple training algorithm.
Reference: [29] <author> G. </author> <title> Bolt. Fault tolerance in binary neural networks. </title> <booktitle> In Proceedings of Weightless Neural Network Workshop (WNNW93), </booktitle> <pages> pages 64-69, </pages> <address> University of York, York, UK, </address> <year> 1993. </year>
Reference-contexts: Such a model has been shown to have a large storage capacity [17] and to be tolerant to binary errors, particularly when small sized tuple processors are used <ref> [29] </ref>. An important aspect about RAM networks is that although there is no ambiguous information concerning RAM locations which contain 1, the same does not hold for those that are left in the 0 state.
Reference: [30] <author> R. G. Bowmaker and G. G. Coghill. </author> <title> Improved recognition capabilities for goal seeking neuron. </title> <journal> IEE Electronics Letters, </journal> <volume> 28 </volume> <pages> 220-221, </pages> <year> 1992. </year>
Reference-contexts: In the learning phase, the contents of the locations which led the pyramids to produce the desired response are modified to allow the network to learn the current association. Different algorithms have been proposed to teach GSN neural networks <ref> [30, 43] </ref>. The basic characteristic of these algorithms is that the training set is often presented in a single epoch, that is, one-shot learning algorithms.
Reference: [31] <author> N. P. Bradshaw. </author> <title> Improving the generalisation of the N-tuple classifier using the effective VC dimension. </title> <journal> IEE Electronics Letters, </journal> <volume> 32(20) </volume> <pages> 1904-1905, </pages> <year> 1996. </year>
Reference-contexts: The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework <ref> [31, 72, 92] </ref>. <p> Their experiments showed that the modification brought a decrease in the level of saturation and increase in generalisation accuracy over an identical experiment performed with the original algorithm. A further training algorithm, called Stochastic Minimisation Algorithm (SMA), was proposed in <ref> [31] </ref>. The SMA was used for estimation of the Vapnik-Chervonenkis (VC) dimension [102] of the N-tuple classifier, but can also be applied to real-world data. A different kind of learning algorithm, involving changes in the connectivity pattern, was introduced by Jorgensen et al. in [63]. <p> Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods <ref> [24, 31, 72, 98, 92] </ref>, where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise. <p> Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods [24, 31, 72, 98, 92], where some of them, such as the ones in <ref> [31, 98] </ref>, are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise. That is, in WNNs, generalisation is sensitive to similarities to entire patterns instead of their constituent variables [8]. <p> On the other hand, cognitive computing needs efficient storage for "experience". To solve this problem, ways to decompose "experience" need to be considered. Studies about storage capacity of RAM-based networks are presented in <ref> [1, 33, 31, 86, 98] </ref>. 2. Planning. One of the major differences between rule-based and neural computing is the notion that a neural net can find the solution for a planning problem directly, whereas rule-based systems have to search quasi-exhaustively. Planning is a somewhat neglected area of neurocomputing.
Reference: [32] <author> A. P. Braga. </author> <title> Attractor properties of recurrent networks with generalising Boolean nodes. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (ICANN94), </booktitle> <pages> pages 421-424. </pages> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: The attractor properties of GNUs were also studied by Braga, who also demonstrated that the GNU's retrieval time is a function of the number of associations stored <ref> [32] </ref>. Recently, Adeodato & Taylor [2] developed another approach for the storage capacity assessment of WNNs which can be applied to different nodes and topologies, including GNUs. In such an approach, the probability of occurrence of collision is expressed as a function of the probability distribution of the training set. <p> Differently, in Hopfield networks the cost of storage is only O (k 2 ). Nevertheless, weightless networks such as GNUs do not need to be fully connected as Hopfield networks. In <ref> [2, 32] </ref>, two different methods to design partially connected GNUs with high confidence in the storage process are proposed. <p> Nevertheless, weightless networks such as GNUs do not need to be fully connected as Hopfield networks. In [2, 32], two different methods to design partially connected GNUs with high confidence in the storage process are proposed. In <ref> [32] </ref>, it was also shown that partially connected GNUs could have better retrieval performance than the fully connected ones, provided that the training set size is small enough to avoid saturation and overwriting in the storage process. * Issues on hardware implementation: The possibility of implementing real networks in hardware is
Reference: [33] <author> A. P. Braga. </author> <title> Predicting contradictions in the storage process of diluted recurrent Boolean neural networks. </title> <journal> IEE Electronics Letters, </journal> <volume> 30 </volume> <pages> 55-56, </pages> <year> 1994. </year>
Reference-contexts: Aleksander and Morton studied the retrieval properties of GNUs with external inputs and demonstrated that the optimum retrieval performance is obtained when there is perfect balance between the feedforward and feedback fields [12]. Braga <ref> [33] </ref>, by using a probabilistic approach, obtained a set of expressions that enable the prediction of contradictions happening during the storage process of partially connected GNUs. <p> On the other hand, cognitive computing needs efficient storage for "experience". To solve this problem, ways to decompose "experience" need to be considered. Studies about storage capacity of RAM-based networks are presented in <ref> [1, 33, 31, 86, 98] </ref>. 2. Planning. One of the major differences between rule-based and neural computing is the notion that a neural net can find the solution for a planning problem directly, whereas rule-based systems have to search quasi-exhaustively. Planning is a somewhat neglected area of neurocomputing.
Reference: [34] <author> A. P. Braga and I. Aleksander. </author> <title> Geometrical treatment and statistical modelling of the the distribution of patterns in the n-dimensional Boolean space. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 </volume> <pages> 507-515, </pages> <year> 1994. </year>
Reference-contexts: The expansion by spreading of each one of the classes defined by the training set patterns is limited by the expansion of the others. Crosstalk between two classes happens when their boundaries meet each other due to successive spreading phases. In <ref> [34] </ref>, Braga and Aleksander developed a model to predict the percentage of the space that is in the overlap between two classes in N -dimensional Boolean space as well as the probability of each one of the two classes occurring.
Reference: [35] <author> G. A. Carpenter and S. Grossberg. </author> <title> Adaptive resonance theory: neural networks architectures for self-organizing pattern recognition. </title> <editor> In R. Eckmiller, G. Hartmann, and G. Hauske, editors, </editor> <booktitle> Proceedings of Parallel Processing in Neural Systems and Computers, </booktitle> <year> 1990. </year>
Reference-contexts: A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory <ref> [35] </ref>, implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [36] <author> J. Castro. </author> <title> Semantic knowledge in general neural units: </title> <booktitle> issues of representation. In Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <pages> pages 674-679, </pages> <address> Australia, </address> <year> 1995. </year>
Reference-contexts: The topology of GNUs differs from the classical Hopfield recurrent model [57] by having an internal feedback field as well as external feedforward connections. For instance, in <ref> [36] </ref> auto-associative (without external inputs) GNUs are shown to be able to learn reasoning structures found in semantic representation. In contrast, Sales et al. [95] carried out some linguistic experiments, concerning symbol grounding, by using heter-associative (with external inputs) GNUs.
Reference: [37] <author> S. S. Christensen, A. W. Andersen, T. M. Jorgensen, and C. Liisberg. </author> <title> Visual guidance of a pig evisceration robot using neural networks. </title> <journal> Pattern Recognition Letters, </journal> <volume> 17(4) </volume> <pages> 345-355, </pages> <year> 1996. </year>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons.
Reference: [38] <author> T. G. Clarkson, D. Gorse, and J. G. Taylor. </author> <title> Applications of the pRAM. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks (IJCNN91), </booktitle> <pages> pages 2618-2623, </pages> <address> Singapore, </address> <year> 1991. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 58 </note>
Reference-contexts: Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised [23, 47, 52, 58, 63] and unsupervised <ref> [15, 38, 42, 85] </ref> learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in <ref> [15, 38, 85] </ref>. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [39] <author> T. G. Clarkson, D. Gorse, and J. G. Taylor. </author> <title> Biologically plausible learning in hardware realiziable nets. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN91), </booktitle> <pages> pages 195-199, </pages> <address> Helsinki, Finland, </address> <year> 1991. </year>
Reference-contexts: Weightless does not imply distance from biological reality. On the contrary, the methodology has more flexible modelling capabilities which enables it to accommodate new discoveries in living systems <ref> [39, 81] </ref>. Some fundamental issues will be considered below: Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 55 1. The Scaling Problem. Most of the neural configurations (autoassociators in particular) have the problem of poor growth of storage capacity with the size of the system.
Reference: [40] <author> T. G. Clarkson, Y. Guan, and J. G. Taylor. </author> <title> Generalization in probabilistic RAM nets. </title> <journal> IEEE Tran-scactions on Computers, </journal> <volume> 4(2) </volume> <pages> 360-363, </pages> <year> 1993. </year>
Reference-contexts: In general, pRAMs have been used in pyramids [54, 87] and trained by means of reinforcement procedures [50, 51, 53]. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in <ref> [40] </ref>. An important aspect about pRAMs and their reinforcement training algorithm is the fact that they are hardware implementable, and chips are commercially available [41].
Reference: [41] <author> T. G. Clarkson, C. K. Ng, D. Gorse, and J. G. Taylor. </author> <title> Learning probabilistic RAM nets using VLSI structures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(12) </volume> <pages> 1552-1561, </pages> <year> 1992. </year>
Reference-contexts: The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40]. An important aspect about pRAMs and their reinforcement training algorithm is the fact that they are hardware implementable, and chips are commercially available <ref> [41] </ref>. More recently, Gorse et al. [48] proposed a spike-based reinforcement learning algorithm which allows continuous functions to be learned by neural networks and applied it to pRAMs networks. <p> In general, nodes such as GRAMs, which require the spreading of information to neighbouring memory locations, are not straightforwardly implementable in hardware such as the WISARD system. So, they might require an amount of circuitry comparable to that of weighted neural systems. In <ref> [14, 41] </ref>, respectively, examples of hardware implementation of the WISARD system (discrete components) and pRAM networks (VLSI) are presented, as well as their learning algorithm. In [96], a technique for hardware and software implementation of trained RAM and GSN networks is introduced.
Reference: [42] <author> A. de Carvalho, M. C. Fairhurst, and D. L. Bisset. </author> <title> SOFT ABoolean self organising feature extractor. </title> <editor> In I. Aleksander and J. G. Taylor, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN92), </booktitle> <pages> pages 669-672, </pages> <address> Brighton, UK, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised [23, 47, 52, 58, 63] and unsupervised <ref> [15, 38, 42, 85] </ref> learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> In the case when both 0 and 1 have occurred with the same frequency, the output of the node will be the undefined value u. GSN-based architectures form the basis for recently developed architectures such as the Self Organising Feature exTractor (SOFT) <ref> [42] </ref> and the Boolean Convergence Network (BCN) [58]. SOFT was designed to be used as a feature extractor for GSN-based neural classifiers [44]. <p> The main results of the comparison are listed below: * Learning: In contrast to the weighted neural models, there are several one-shot learning algorithms for WNNs, where training takes only one epoch <ref> [12, 42, 47, 90] </ref>. Also, like for the weighted neural systems, there exist many iterative learning schemes for WNNs. For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel.
Reference: [43] <author> A. de Carvalho, M. C. Fairhurst, and D. L. Bisset. </author> <title> Progressive learning algorithm for GSN feedforward neural architectures. </title> <journal> IEE Electronics Letters, </journal> <volume> 30(6) </volume> <pages> 506-507, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: In the learning phase, the contents of the locations which led the pyramids to produce the desired response are modified to allow the network to learn the current association. Different algorithms have been proposed to teach GSN neural networks <ref> [30, 43] </ref>. The basic characteristic of these algorithms is that the training set is often presented in a single epoch, that is, one-shot learning algorithms.
Reference: [44] <author> A. de Carvalho, M. C. Fairhurst, and D. L. Bisset. </author> <title> Combining Boolean Neural Architectures for Image Recognition. </title> <journal> Connection Science, </journal> <volume> 9(4) </volume> <pages> 506-507, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: GSN-based architectures form the basis for recently developed architectures such as the Self Organising Feature exTractor (SOFT) [42] and the Boolean Convergence Network (BCN) [58]. SOFT was designed to be used as a feature extractor for GSN-based neural classifiers <ref> [44] </ref>. BCNs were proposed to overcome Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 50 problems such as the lack of cross connection faced by the previous GSN neural architectures (pyramids).
Reference: [45] <author> M. C. P. de Souto, K. S. Guimar~aes, and T. B Ludermir. </author> <title> On the intractability of loading pyramidal architectures. </title> <booktitle> In Proceedings of the IEE International Conference on Artificial Neural Networks, </booktitle> <pages> pages 189-194, </pages> <address> UK, </address> <year> 1995. </year>
Reference-contexts: A more general approach for the computational complexity of training pyramids was presented in <ref> [45] </ref>, where this problem was shown to be an NP-complete problem. Nevertheless, it was also shown that a constrained class of pyramids, called multi-pyramidal architectures (a set of disjoint tress with bounded depth), can be trained in polynomial time.
Reference: [46] <author> M. C. P. de Souto, T. B. Ludermir, and W. R. de Oliveira. </author> <title> Synthesis of probabilistic automata in pRAM neural networks. </title> <booktitle> In Proceedings of the International Conference on Artificial Neural Networks (ICANN98), </booktitle> <pages> pages 603-608, </pages> <address> Sweden, </address> <year> 1998. </year>
Reference-contexts: She also developed algorithms using cut-point nodes which generate production rules for weightless neural networks and generate weightless neural networks for production rules. Recently, such a work was extended for the case of single-layer pRAM networks <ref> [46] </ref>. But work needs to be done on semantics. In order to achieve the appropriate balance between "understanding" of language imparted by syntax and that imparted by semantics, the linguistic behaviour of temporal neural structures needs to be studied.
Reference: [47] <author> E. Filho, M. C. Fairhurst, and D. L. Bisset. </author> <title> Adaptive pattern recognition using goal-seeking neurons. </title> <journal> Pattern Recognition Letters, </journal> <volume> 12 </volume> <pages> 131-138, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Neuron functions are stored into look-up tables, which can be implemented using commercially available Random Access Memories (RAMs). Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised <ref> [23, 47, 52, 58, 63] </ref> and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> GSN neurons have been used in three different architectures, each one suitable for a different range of applications. These classes of architectures are GSN feedforward (GSN f ), GSN feedback (GSN fb ), and GSN Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 49 self-organising (GSN s ) <ref> [47] </ref>. Such architectures are usually formed by a fixed number of pyramids, known as multi-pyramidal networks (Figure 6), where each pyramid covers a subset in the pixels of the input field. The use of genetic algorithms to optimise the topology of GSN-based multi-pyramidal networks was proposed by Martins in [73]. <p> The main results of the comparison are listed below: * Learning: In contrast to the weighted neural models, there are several one-shot learning algorithms for WNNs, where training takes only one epoch <ref> [12, 42, 47, 90] </ref>. Also, like for the weighted neural systems, there exist many iterative learning schemes for WNNs. For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel. <p> A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in <ref> [47, 67] </ref>. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [48] <author> D. Gorse, D. A. Romano-Critchley, and J. G. Taylor. </author> <title> A pulse-based reinforcement algorithm for learning continuous functions. </title> <journal> Neurocomputing, </journal> <volume> 14(4) </volume> <pages> 319-344, </pages> <year> 1997. </year>
Reference-contexts: This output function can be seen as a polynomial containing products up to N input signals. Thus, in the PDP terminology [94], these pRAM nodes are sigma-pi units. Networks with pRAM nodes can also have the ability to approximate universal function <ref> [48] </ref>. <p> An important aspect about pRAMs and their reinforcement training algorithm is the fact that they are hardware implementable, and chips are commercially available [41]. More recently, Gorse et al. <ref> [48] </ref> proposed a spike-based reinforcement learning algorithm which allows continuous functions to be learned by neural networks and applied it to pRAMs networks.
Reference: [49] <author> D. Gorse and J. G. Taylor. </author> <title> On the equivalence properties of noisy neural and probabilistic RAM nets. </title> <journal> Physics Letters A, </journal> <volume> 131(6) </volume> <pages> 326-332, </pages> <year> 1988. </year>
Reference-contexts: Other WNNs models were soon created incorporating these ideas, such as the MPLN and pRAM. These nodes, which are similar in many ways, were simultaneously and independently developed in [82, 83] and <ref> [49, 51] </ref>, respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. <p> In [99], Taylor proposed a model of noisy neurons that presents equations which incorporate and formalise many known properties of living neurons. The evolution of such a model was shown to be equivalent to that of networks of noisy (probabilistic) RAMs or pRAMs <ref> [49] </ref>. The pRAM is also an extension of the PLN like the MPLN, but in which continuous probabilities can be stored, that is, value in the range [0; 1]. This kind of node outputs 1 (spike) with frequency related to the memory content being addressed.
Reference: [50] <author> D. Gorse and J. G. Taylor. </author> <title> Reinforcement training strategies for probabilistic RAMs. </title> <editor> In M. Novak and E. Pelikan, editors, </editor> <booktitle> Proceedings of the International Symposium on Neural Networks and Neuro-computing (NEURONET90), </booktitle> <pages> pages 180-184, </pages> <year> 1990. </year>
Reference-contexts: Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids [54, 87] and trained by means of reinforcement procedures <ref> [50, 51, 53] </ref>. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40].
Reference: [51] <author> D. Gorse and J. G. Taylor. </author> <title> A continuous input RAM-based stochastic neural model. </title> <booktitle> Neural Networks, </booktitle> <volume> 4 </volume> <pages> 657-665, </pages> <year> 1991. </year>
Reference-contexts: Other WNNs models were soon created incorporating these ideas, such as the MPLN and pRAM. These nodes, which are similar in many ways, were simultaneously and independently developed in [82, 83] and <ref> [49, 51] </ref>, respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. <p> This kind of node outputs 1 (spike) with frequency related to the memory content being addressed. The pRAM was further extended, so as to be able to map continuous input to binary outputs. Such an extension is called the integrating pRAM (i-pRAM) <ref> [51] </ref>. Another Boolean unit with the capability of dealing with continuous inputs, called cubic node, was also proposed in [55]. <p> Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids [54, 87] and trained by means of reinforcement procedures <ref> [50, 51, 53] </ref>. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40]. <p> In contrast, in the weighted neural models, individual neurons can be sensitive to particular input variables. Nevertheless, generalisation at the node level can also be obtained with some weightless nodes such as GRAM [7] and pRAM <ref> [51] </ref>. * Content addressable systems: WNNs with the same kind of configuration of Hopfield networks, that is, fully connected auto-associative networks, were analysed by Lucy in [70].
Reference: [52] <author> D. Gorse and J. G. Taylor. </author> <title> Universal associative stochastic learning automata. </title> <booktitle> In Neural Networks World 1, </booktitle> <pages> pages 193-202, </pages> <year> 1991. </year>
Reference-contexts: Neuron functions are stored into look-up tables, which can be implemented using commercially available Random Access Memories (RAMs). Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised <ref> [23, 47, 52, 58, 63] </ref> and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids [54, 87] and trained by means of reinforcement procedures [50, 51, 53]. The convergence of the reinforcement procedure for pRAM networks was investigated in <ref> [52] </ref>, and the generalisation properties of these networks were addressed in [40]. An important aspect about pRAMs and their reinforcement training algorithm is the fact that they are hardware implementable, and chips are commercially available [41]. <p> Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in <ref> [52, 84, 81] </ref>. Additionally, there are algorithms based on standard statistical methods [24, 31, 72, 98, 92], where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole.
Reference: [53] <author> D. Gorse and J. G. Taylor. </author> <title> Review of the theory of pRAMs. </title> <booktitle> In Proceedings of the Weightless Neural Network Workshop (WNNW93), </booktitle> <address> University of York, UK, </address> <year> 1993. </year>
Reference-contexts: Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids [54, 87] and trained by means of reinforcement procedures <ref> [50, 51, 53] </ref>. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40].
Reference: [54] <author> Y. Guan, T. G. Clarkson, D. Gorse, and J. G. Taylor. </author> <title> Noisy reinforcement training for pRAM nets. </title> <booktitle> Neural Networks, </booktitle> <volume> 7(3) </volume> <pages> 523-538, </pages> <year> 1994. </year>
Reference-contexts: Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids <ref> [54, 87] </ref> and trained by means of reinforcement procedures [50, 51, 53]. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40].
Reference: [55] <author> K N. Gurney. </author> <title> Training nets of hardware realizable sigma-pi units. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 289-303, </pages> <year> 1992. </year>
Reference-contexts: The pRAM was further extended, so as to be able to map continuous input to binary outputs. Such an extension is called the integrating pRAM (i-pRAM) [51]. Another Boolean unit with the capability of dealing with continuous inputs, called cubic node, was also proposed in <ref> [55] </ref>.
Reference: [56] <author> L. Hepplewhite and T. J. Stonham. </author> <title> N-tuple texture recognition and the zero crossing sketch. </title> <journal> IEE Electronics Letters, </journal> <volume> 33(1) </volume> <pages> 45-46, </pages> <year> 1997. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 59 </note>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons.
Reference: [57] <author> J. J. </author> <title> Hopfield. Neural networks and physical systems with emergent collective computational properties. </title> <booktitle> In Proceedings of the National Academy of Sciences (USA), </booktitle> <volume> volume 79, </volume> <pages> pages 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: Updates, corrections, and comments should be sent to Teresa B. Ludermir at tbl@di.ufpe.br. Neural Computing Surveys 2, 41-61, 1999, http ://www.icsi.berkeley.edu/~ jagota/NCS 41 Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 42 not true. As an example, Hopfield networks <ref> [57] </ref> are BNNs, but not WNNs. WNNs have their origin on the N -tuple sampling machines described by Bledsoe and Browning in the late 50s [27]. <p> Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 51 6.1 GNU The General Neural Unit (GNU) [12] is a single layer recurrent WNNs that uses GRAMs as nodes. The topology of GNUs differs from the classical Hopfield recurrent model <ref> [57] </ref> by having an internal feedback field as well as external feedforward connections. For instance, in [36] auto-associative (without external inputs) GNUs are shown to be able to learn reasoning structures found in semantic representation. <p> Spreading occurs in each one of the GRAMs after all the input/output associations are presented. This procedure is similar to the one used to create fixed points in the state space of Hopfield networks <ref> [57] </ref>. Figure 7 illustrates the process of creating an association in GNUs. . . . . . . . . . . . . . . . . . . . . .
Reference: [58] <author> G. Howells, M. Fairhurst, and D. Bisset. BCN: </author> <title> A novel network architecture for RAM-based neurons. </title> <journal> Pattern Recognition Letters, </journal> <volume> 16 </volume> <pages> 297-303, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: Neuron functions are stored into look-up tables, which can be implemented using commercially available Random Access Memories (RAMs). Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised <ref> [23, 47, 52, 58, 63] </ref> and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> In the case when both 0 and 1 have occurred with the same frequency, the output of the node will be the undefined value u. GSN-based architectures form the basis for recently developed architectures such as the Self Organising Feature exTractor (SOFT) [42] and the Boolean Convergence Network (BCN) <ref> [58] </ref>. SOFT was designed to be used as a feature extractor for GSN-based neural classifiers [44]. BCNs were proposed to overcome Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 50 problems such as the lack of cross connection faced by the previous GSN neural architectures (pyramids).
Reference: [59] <author> G. Howells, M. C. Fairhurst, and D. L. Bisset. GCN: </author> <title> The Generalised Convergent Network. </title> <booktitle> In Proceedings of the IEE International Conference on Image Processing and its Application, </booktitle> <year> 1995. </year>
Reference-contexts: BCNs were proposed to overcome Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 50 problems such as the lack of cross connection faced by the previous GSN neural architectures (pyramids). BCN architectures were further extended and led to the development of the Generalised Convergent Network (GCN) <ref> [59] </ref>, which can be used with any kind of weightless node. Finally, based on both BCNs and GCNs, the Probabilistic Convergent Network (PCN) was proposed [60].
Reference: [60] <author> G. Howells, M. C. Fairhurst, and D. L. Bisset. </author> <title> PCN: The Probabilistic Convergent Network. </title> <booktitle> In Proceedings of the IEEE International Conference on Neural Networks (ICNN95), </booktitle> <pages> pages 1211-1214, </pages> <address> Perth, Australia, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: BCN architectures were further extended and led to the development of the Generalised Convergent Network (GCN) [59], which can be used with any kind of weightless node. Finally, based on both BCNs and GCNs, the Probabilistic Convergent Network (PCN) was proposed <ref> [60] </ref>. The following section presents the GRAM model, which aims to increase the generalisation of WNNs at the node level. 6 GRAM Since only the locations accessed by the trained patterns are modified during learning, a PLN suffers from hesitation when the training set is small.
Reference: [61] <author> A. Jagota, G. Narasimhan, and K. Regan. </author> <title> Information Capacity of Binary Weights Associative Memories. </title> <journal> Neurocomputing, </journal> <pages> 19(1-3), </pages> <month> April </month> <year> 1998. </year>
Reference-contexts: Rule of propagation: the responsibility for learning lies in the input-output mappings found in the units themselves. 6. Activation rule: a mapping from the input into the output of the net. 7. Learning rule: changes in the contents of the truth table. In <ref> [61] </ref> is presented a comparative study regarding the information capacity of two models with binary weights: the Willshaw model and the Inverted Neural Network. 9 Future Work and Research Issues As in the neurocomputing field in general, there are also two salient directions for research in weightless systems: new insights into
Reference: [62] <author> T. M. Jorgensen. </author> <title> Classification of handwritten digits using a RAM neural net architecture. </title> <journal> International Journal of Neural Systems, </journal> <volume> 8(1) </volume> <pages> 17-25, </pages> <year> 1997. </year>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons. <p> They proposed a measure, which combines the concept of cross-validation and Shannon information theory, to be used during training to select the best connections needed to achieve a given performance. An example of the application of this technique can be found in <ref> [62] </ref>. A similar problem, but more concerned with storage cost, was approached in [78]. Such a study described several ways in which memory requirements of N-tuple systems can be reduced.
Reference: [63] <author> T. M. Jorgensen, S. S. Christensen, and C. Liisberg. </author> <title> Crossvalidation and information measures for RAM based neural networks. </title> <editor> In D. Bisset, editor, </editor> <booktitle> Proceedings of the Weightless Neural Networks Workshop (WNNW95), </booktitle> <pages> pages 87-92, </pages> <institution> University of Kent at Canterbury, UK, </institution> <year> 1995. </year>
Reference-contexts: Neuron functions are stored into look-up tables, which can be implemented using commercially available Random Access Memories (RAMs). Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised <ref> [23, 47, 52, 58, 63] </ref> and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> The SMA was used for estimation of the Vapnik-Chervonenkis (VC) dimension [102] of the N-tuple classifier, but can also be applied to real-world data. A different kind of learning algorithm, involving changes in the connectivity pattern, was introduced by Jorgensen et al. in <ref> [63] </ref>. They proposed a measure, which combines the concept of cross-validation and Shannon information theory, to be used during training to select the best connections needed to achieve a given performance. An example of the application of this technique can be found in [62].
Reference: [64] <author> W. K Kan and I. Aleksander. </author> <title> A probabilistic logic neuron network for associative learning. </title> <booktitle> In Proceedings of the IEEE First International Conference on Neural Networks (ICNN87), </booktitle> <volume> volume II, </volume> <pages> pages 541-548, </pages> <address> San Diego, California, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: Another discriminator holding a value 1 at the same location indicates a counter-example. In the second case, the vector could be an example of the class, but since no information was given, the RAM will output 0 instead of 1. To overcome this problem, Kan and Aleksander <ref> [64] </ref> developed the word-addressable logic node called Probabilistic Logic Node (PLN), which will be described in the next section. 3 PLN A PLN node differs from a RAM node in that a 2-bit number (rather than a single bit) is now stored at the addressed memory location. <p> Pyramids have lower functionality than a single neuron covering the same input pixels. On the other hand, if compared to this single neuron, a pyramid reduces the size of memory needed and increases generalisation <ref> [64] </ref>. Training PLN networks becomes a process of replacing u's with 0's and 1's so that the network consistently produces the correct output pattern in response to training pattern inputs.
Reference: [65] <author> P. Kanerva. </author> <title> Sparse Distributed Memory. </title> <publisher> MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: The failure in the retrieval process is a consequence of learning associations beyond the network's capacity, which fills the state space with fixed points to which the network can be erroneously attracted. Successful retrieval of all the associations learned is expected if there is no overloading. 7 SDM Kanerva <ref> [65] </ref> developed a method of associative storage, the Sparse Distributed Memory (SDM), that is physiologically plausible and has characteristics that make its hardware implementation attractive. <p> Since such a model has many features similar to that of WNNs, in this section, the basic principle on which this system is based will be reviewed. A more detailed description of SDMs can be found in <ref> [65] </ref>. The essence of SDMs is to use sparsely distributed decoders in a high dimensional Boolean space so that any sparse decoder or hard storage location is accessed from anywhere in the space that is at a Hamming distance within r p bits from its base address.
Reference: [66] <author> J. V. Kennedy and J. Austin. </author> <title> A hardware implementation of a binary neural image processor. </title> <booktitle> In Proceedings of the IV International Conference on Microeletronics for Neural Networks and Fuzzy Systems, </booktitle> <pages> pages 178-185, </pages> <address> Torino. Italy, </address> <year> 1994. </year>
Reference-contexts: In [14, 41], respectively, examples of hardware implementation of the WISARD system (discrete components) and pRAM networks (VLSI) are presented, as well as their learning algorithm. In [96], a technique for hardware and software implementation of trained RAM and GSN networks is introduced. Kennedy et al. <ref> [66] </ref> described the SAT processor; a dedicated hardware implementation of a binary neural image processor. The SAT processor is aimed specifically at supporting the ADAM networks.
Reference: [67] <author> M. A. Kerin and T. J. Stonham. </author> <title> Face recognition using digital neural network with self-organising capabilities. </title> <booktitle> In Proceedings of the X International Conference on Pattern Recognition, </booktitle> <pages> pages 738-741, </pages> <year> 1990. </year>
Reference-contexts: A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in <ref> [47, 67] </ref>. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [68] <author> T. Kohonen. </author> <title> Self-Organising and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> Heidelberg, </address> <year> 1988. </year>
Reference-contexts: For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel. A training algorithm for this task can be found in [3]. Implementations of Kohonen maps <ref> [68] </ref> with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [69] <author> A. Kolcz and N. M. Allinson. </author> <title> Application of the CMAC input encoding scheme in the N-tuple approximation network. </title> <journal> IEE Proceedings-E Computers and Digital Techniques, </journal> <volume> 141(3) </volume> <pages> 177-183, </pages> <year> 1994. </year>
Reference-contexts: The thermometer coding [10], the rank coding [16], the interval coding Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 43 or Minchinton cells [26], and more recently the CMAC coding <ref> [69] </ref>, are examples of methods which can be used to process scalar data for RAM networks. The method in [69] has been shown suitable for different applications [90]. <p> The thermometer coding [10], the rank coding [16], the interval coding Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 43 or Minchinton cells [26], and more recently the CMAC coding <ref> [69] </ref>, are examples of methods which can be used to process scalar data for RAM networks. The method in [69] has been shown suitable for different applications [90]. In a typical RAM network, that is, a single-layer architecture, RAMs are taught to respond with a 1 for those patterns in the training set, and only for those patterns.
Reference: [70] <author> J. Lucy. </author> <title> Perfect auto-associators using RAM-type nodes. </title> <journal> IEE Electronics Letters, </journal> <volume> 27 </volume> <pages> 799-800, </pages> <year> 1991. </year>
Reference-contexts: For instance, in [36] auto-associative (without external inputs) GNUs are shown to be able to learn reasoning structures found in semantic representation. In contrast, Sales et al. [95] carried out some linguistic experiments, concerning symbol grounding, by using heter-associative (with external inputs) GNUs. In <ref> [70] </ref>, auto-associative GNUs were shown to be able to store up to 2 N patterns and that given an initial vector, or initial state, the network retrieves the nearest stored one. <p> Nevertheless, generalisation at the node level can also be obtained with some weightless nodes such as GRAM [7] and pRAM [51]. * Content addressable systems: WNNs with the same kind of configuration of Hopfield networks, that is, fully connected auto-associative networks, were analysed by Lucy in <ref> [70] </ref>. Lucy's work showed that the storage capacity of fully connected auto-associative GNUs grows exponentially with the number k of nodes, that is, O (2 k ). In contrast, in Hopfield networks such a capacity scales only linearly with the number of nodes, that is, O (k= ln (k)) [76].
Reference: [71] <author> T. B. Ludermir. </author> <title> Computability of logical neural networks. </title> <journal> Journal of Intelligent Systems, </journal> <volume> 2 </volume> <pages> 261-290, </pages> <year> 1992. </year>
Reference: [72] <author> S. P. Luttrell. </author> <title> Gibbs distribution theory of adaptive n-tuple networks. </title> <editor> In I. Aleksander and J. Taylor, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN92), </booktitle> <pages> pages 313-316, </pages> <address> Brighton, UK, </address> <year> 1992. </year>
Reference-contexts: The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework <ref> [31, 72, 92] </ref>. <p> Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods <ref> [24, 31, 72, 98, 92] </ref>, where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise.
Reference: [73] <author> W. Martins and N. M. Allinson. </author> <title> Two improvements for GSN neural networks. </title> <editor> In N. M. Allison, editor, </editor> <booktitle> Proceedings of the Weightless Neural Network Workshop (WNNW93), </booktitle> <pages> pages 58-63, </pages> <address> University of York, York, UK, </address> <year> 1993. </year>
Reference-contexts: Such architectures are usually formed by a fixed number of pyramids, known as multi-pyramidal networks (Figure 6), where each pyramid covers a subset in the pixels of the input field. The use of genetic algorithms to optimise the topology of GSN-based multi-pyramidal networks was proposed by Martins in <ref> [73] </ref>. These multi-pyramidal GSN networks often have three operation phases, each associated with a specific goal: a validating phase, a learning phase and a recall phase. Before the network training, all of its memory contents in the network are filled with u values.
Reference: [74] <author> J. D. McCauley, B. R. Thabe, and A. D. Whittaker. </author> <title> Fat estimation in beef ultrasound images using texture and adaptive logic networks. </title> <journal> Transactions of ASAE, </journal> <volume> 37(3) </volume> <pages> 997-102, </pages> <month> June </month> <year> 1994. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 60 </note>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons.
Reference: [75] <author> W. S. McCulloch and W. Pitts. </author> <title> A Logical Calculus of the Ideas Imminent in Nervous Activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1930. </year>
Reference-contexts: There is some misunderstanding in the literature about the definitions of Boolean Neural Networks (BNN) and WNNs. BNN are those networks that handle only Boolean values at their inputs and outputs. They may have nodes based on the model proposed by McCulloch and Pitts <ref> [75] </ref>, with weights associated to their connections. WNNs, as the name suggests, do not have adaptive weights associated to their connections. They can also be seen as having fixed binary weights.
Reference: [76] <author> R. J. McEliece, E. C. Posner, E. R. Rodemich, and S. S. Venkatesh. </author> <title> The capacity of the Hopfield associative memory. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 33 </volume> <pages> 461-482, </pages> <year> 1987. </year>
Reference-contexts: Lucy's work showed that the storage capacity of fully connected auto-associative GNUs grows exponentially with the number k of nodes, that is, O (2 k ). In contrast, in Hopfield networks such a capacity scales only linearly with the number of nodes, that is, O (k= ln (k)) <ref> [76] </ref>. However, the cost of storage in GNUs also grows Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 54 exponentially with the number of nodes. Differently, in Hopfield networks the cost of storage is only O (k 2 ).
Reference: [77] <author> D. K. Milligan and M. J. D. Wilson. </author> <title> The behaviour of affine Boolean sequential networks. </title> <journal> Connection Science, </journal> <volume> 5(2) </volume> <pages> 153-167, </pages> <year> 1993. </year>
Reference-contexts: Sales et al. [95] present a neural state machine, constructed from WNNs, to "ground" visual and linguistic representation. An interesting analysis of the dynamic of temporal neural structures can be found in <ref> [77, 103, 105, 107] </ref>. 4. Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications [19].
Reference: [78] <author> R. J. Mitchell, J. M. Bishop, and P. R. Minchinton. </author> <title> Optimizing memory usage in N-tuple neural networks. </title> <booktitle> Mathematics and Computers in Simulation, </booktitle> <address> 40(5-6):549-563, </address> <year> 1996. </year>
Reference-contexts: An example of the application of this technique can be found in [62]. A similar problem, but more concerned with storage cost, was approached in <ref> [78] </ref>. Such a study described several ways in which memory requirements of N-tuple systems can be reduced. Finally, it is important to point out that, although these modified methods described above, could achieve a better performance, they often reduce the speed and simplicity advantages of the original algorithm.
Reference: [79] <author> J. Mrsic-Flogel. </author> <title> Approaching cognitive system design. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN91), </booktitle> <volume> volume 1, </volume> <pages> pages 879-883, </pages> <address> Helsinki, </address> <year> 1991. </year>
Reference-contexts: The discrimination coefficient is proportional to the input's ability to lead the node to fire, having a similar function to the synapses in the Hebbian learning [94]. GRAMs are often implemented as Virtual GRAMs (VGRAM) <ref> [79] </ref>. This is because the cost of storage in WNNs scales exponentially with the node fan-in (2 N ). A VGRAM is a GRAM in which space is not allocated for patterns which are never found in the training phase.
Reference: [80] <author> S. Muroga. </author> <title> Lower bounds of the number of threshold functions and a maximum weight. </title> <journal> IEEE Transactions on Eletronic Computers, </journal> <pages> pages 136-148, </pages> <year> 1965. </year>
Reference-contexts: They also studied the functionality of pyramidal structures. Such a functionality was shown to be greater than that of weighted-sum-and-threshold neurons in <ref> [80] </ref>. Zhang et al. [108] presented both a training algorithm that can be used with pyramids of PLNs and a formula for computing the average number of steps that the algorithm converges (when there is a solution).
Reference: [81] <author> C. Myers. </author> <title> Delay learning in artificial neural networks. </title> <publisher> Chapman & Hall, </publisher> <year> 1992. </year>
Reference-contexts: Similarly, new information is only acquired after a series of experiences indicates its validity. So, the reinforcement learning procedure for PLNs was extended to take this into account and was applied to model delay learning in invertebrates <ref> [81] </ref>. In [99], Taylor proposed a model of noisy neurons that presents equations which incorporate and formalise many known properties of living neurons. The evolution of such a model was shown to be equivalent to that of networks of noisy (probabilistic) RAMs or pRAMs [49]. <p> Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in <ref> [52, 84, 81] </ref>. Additionally, there are algorithms based on standard statistical methods [24, 31, 72, 98, 92], where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. <p> Weightless does not imply distance from biological reality. On the contrary, the methodology has more flexible modelling capabilities which enables it to accommodate new discoveries in living systems <ref> [39, 81] </ref>. Some fundamental issues will be considered below: Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 55 1. The Scaling Problem. Most of the neural configurations (autoassociators in particular) have the problem of poor growth of storage capacity with the size of the system.
Reference: [82] <author> C. Myers and I. Aleksander. </author> <title> Learning algorithms for probabilistic logic nodes. </title> <booktitle> In Abstracts of the I Annual International Neural Networks Society Meeting (INNS88), </booktitle> <pages> page 205, </pages> <address> Boston, </address> <year> 1988. </year>
Reference-contexts: The use of three-logic values and pyramid structures by PLN networks represented a breakthrough in the WNN research. Other WNNs models were soon created incorporating these ideas, such as the MPLN and pRAM. These nodes, which are similar in many ways, were simultaneously and independently developed in <ref> [82, 83] </ref> and [49, 51], respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. <p> These nodes, which are similar in many ways, were simultaneously and independently developed in <ref> [82, 83] </ref> and [49, 51], respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. The main difference between the MPLN and the PLN is that the first allows a wider, but still discrete, range of probabilities to be stored at each memory content.
Reference: [83] <author> C. Myers and I. Aleksander. </author> <title> Output functions for probabilistic logic nodes. </title> <booktitle> In Proceedings of the IEE International Conference on Neural Networks, </booktitle> <pages> pages 310-314, </pages> <address> London, </address> <year> 1989. </year>
Reference-contexts: At the beginning of training, all stored values in all nodes are set to u, and thus the net's behaviour is completely unbiased. A generic gradient descendent learning algorithm, proposed by Myers, <ref> [83] </ref>, uses several presentations of the training set to teach pyramids of PLNs by using a reward and a punish phase. In [3], Al-Alawi & Stonham introduced a learning procedure, the back-propagation search algorithm, for multi-layer WNNs and applied it to pyramids of PLNs. <p> The use of three-logic values and pyramid structures by PLN networks represented a breakthrough in the WNN research. Other WNNs models were soon created incorporating these ideas, such as the MPLN and pRAM. These nodes, which are similar in many ways, were simultaneously and independently developed in <ref> [82, 83] </ref> and [49, 51], respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. <p> These nodes, which are similar in many ways, were simultaneously and independently developed in <ref> [82, 83] </ref> and [49, 51], respectively. 4 MPLN and pRAM The development of the PLN led to the definition of the m-state PLN or MPLN [82, 83]. The main difference between the MPLN and the PLN is that the first allows a wider, but still discrete, range of probabilities to be stored at each memory content. <p> Also, the output function for an MPLN could be, in addition to the probabilistic function, a linear function, or the sigmoid function <ref> [83] </ref>. One result of extending the PLN to MPLN is that the node locations may now store output probabilities which are more finely graded than in the PLN. An MPLN, for instance, could output 1 with 15% probability under a certain input.
Reference: [84] <author> R. S. Neville and T. J. Stonham. </author> <title> Adaptive reward-penalty for probabilistic logic nodes. </title> <editor> In I. Aleksander and J. G. Taylor, editors, </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks (ICANN92), </booktitle> <volume> volume 1, </volume> <pages> pages 631-634, </pages> <address> Brighton, UK, </address> <year> 1992. </year>
Reference-contexts: Implementations of Kohonen maps [68] with weightless nodes can be found in [15, 38, 85]. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in <ref> [52, 84, 81] </ref>. Additionally, there are algorithms based on standard statistical methods [24, 31, 72, 98, 92], where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole.
Reference: [85] <author> P. Ntourntoufis. </author> <title> Self-organisation properties of a discriminator-based neural network. </title> <editor> In M. Caudill, editor, </editor> <booktitle> Proceedings of the International Joint Conference on Neural Networks (IJCNN90), </booktitle> <volume> volume 2, </volume> <pages> pages 319-324, </pages> <address> Washington, USA, </address> <year> 1990. </year>
Reference-contexts: Instead of adjusting weights, learning on Weightless Neural Networks (WNNs) generally consists of changing the contents of look-up table entries, which results in highly flexible and fast learning algorithms. Both supervised [23, 47, 52, 58, 63] and unsupervised <ref> [15, 38, 42, 85] </ref> learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies [37, 56, 62, 74, 87, 90, 104]. <p> A training algorithm for this task can be found in [3]. Implementations of Kohonen maps [68] with weightless nodes can be found in <ref> [15, 38, 85] </ref>. Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81].
Reference: [86] <author> W. Penny and T. J. Stonham. </author> <title> Storage capacity of multilayer Boolean neural networks. </title> <journal> IEE Electronics Letters, </journal> <volume> 29 </volume> <pages> 1340-1341, </pages> <year> 1993. </year>
Reference-contexts: Nevertheless, it was also shown that a constrained class of pyramids, called multi-pyramidal architectures (a set of disjoint tress with bounded depth), can be trained in polynomial time. Studies on the storage capacity of pyramidal architectures were presented in <ref> [1, 86] </ref>. In [86], such a problem was analysed into the the context of information theory. <p> Nevertheless, it was also shown that a constrained class of pyramids, called multi-pyramidal architectures (a set of disjoint tress with bounded depth), can be trained in polynomial time. Studies on the storage capacity of pyramidal architectures were presented in [1, 86]. In <ref> [86] </ref>, such a problem was analysed into the the context of information theory. <p> On the other hand, cognitive computing needs efficient storage for "experience". To solve this problem, ways to decompose "experience" need to be considered. Studies about storage capacity of RAM-based networks are presented in <ref> [1, 33, 31, 86, 98] </ref>. 2. Planning. One of the major differences between rule-based and neural computing is the notion that a neural net can find the solution for a planning problem directly, whereas rule-based systems have to search quasi-exhaustively. Planning is a somewhat neglected area of neurocomputing.
Reference: [87] <author> S. Ramanan, R. S. Petersen, T. G. Clarkson, and J. G. Taylor. </author> <title> pRAM nets for detection of small targets in sequence of infra-red images. </title> <booktitle> Neural Networks, </booktitle> <pages> pages 1227-1237, </pages> <year> 1995. </year>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons. <p> Also, in this continuous version the pRAM exhibits generalisation properties. In general, pRAMs have been used in pyramids <ref> [54, 87] </ref> and trained by means of reinforcement procedures [50, 51, 53]. The convergence of the reinforcement procedure for pRAM networks was investigated in [52], and the generalisation properties of these networks were addressed in [40]. <p> Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications [19]. In <ref> [87] </ref>, a pRAM network for the classification of objects in a video sequence of FLIR (forward looking infra-red) images is described. 10 Conclusions It has been shown in the paper that WNN models define an important paradigm in the field of Artificial Neural Networks, by providing flexibility, fast learning algorithms and
Reference: [88] <author> R. Rohwer and D. Cressy. </author> <title> Phoneme classification by Boolean networks. </title> <editor> In J. P. Tubach and J. J. Mariani, editors, </editor> <booktitle> Proceedings of the European Conference on Speech Communication and Technology, </booktitle> <pages> pages 557-560, </pages> <address> Edinburgh, </address> <year> 1989. </year>
Reference-contexts: This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems [90, 91]. The results obtained, together with previous ones <ref> [13, 88, 98] </ref>, show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework [31, 72, 92].
Reference: [89] <author> R. Rohwer and A. Lamb. </author> <title> An exploration of the effect of super large n-tuple on single-layer RAMnets. </title> <editor> In N. M. Allison, editor, </editor> <booktitle> Proceedings of the Weightless Neural Network Workshop (WNNW93), </booktitle> <pages> pages 33-37, </pages> <address> Univeristy of York, York, UK, </address> <year> 1993. </year>
Reference-contexts: The pattern is assigned the class corresponding to the discriminator with the highest response r. The performance of the N -tuple method depends mainly on the size of the N-tuple chosen <ref> [13, 89, 100, 101] </ref>. Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications [89, 90]. <p> Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications <ref> [89, 90] </ref>. However, in general, the value of N is constrained by many considerations, where some of them might not be simultaneously satisfied in all situations. In such cases, a simple adjusting in N alone will not significantly improve the network performance [90]. Another important point is the training process.
Reference: [90] <author> R. Rohwer and M. Morciniec. </author> <title> A theoretical and experimental account of n-tuple classifier performance. </title> <journal> Neural Computing, </journal> <volume> 8 </volume> <pages> 629-642, </pages> <year> 1995. </year>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons. <p> The method in [69] has been shown suitable for different applications <ref> [90] </ref>. In a typical RAM network, that is, a single-layer architecture, RAMs are taught to respond with a 1 for those patterns in the training set, and only for those patterns. An unseen pattern is classified in the same class of the training set if all RAMs output 1s. <p> This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems <ref> [90, 91] </ref>. The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework [31, 72, 92]. <p> The way that the network is connected to its input is called the input mapping, or the connectivity pattern. Although the input mapping is chosen at random, such a mapping is often a fixed parameter of the network <ref> [13, 90] </ref>. At the discriminator's output, an adder sums the outputs of the RAMs, producing what is called Neural Computing Surveys 2, 41-61, 1999, http://www.icsi.berkeley.edu/~jagota/NCS 44 the discriminator's response r. Before training takes place, all the k2 N one-bit words are set to zero, and training consists in their modification. <p> Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications <ref> [89, 90] </ref>. However, in general, the value of N is constrained by many considerations, where some of them might not be simultaneously satisfied in all situations. In such cases, a simple adjusting in N alone will not significantly improve the network performance [90]. Another important point is the training process. <p> However, in general, the value of N is constrained by many considerations, where some of them might not be simultaneously satisfied in all situations. In such cases, a simple adjusting in N alone will not significantly improve the network performance <ref> [90] </ref>. Another important point is the training process. For example, the method of training described above ensures that all the node functions output one when presented with the specific N-tuple in the training pattern. <p> The main results of the comparison are listed below: * Learning: In contrast to the weighted neural models, there are several one-shot learning algorithms for WNNs, where training takes only one epoch <ref> [12, 42, 47, 90] </ref>. Also, like for the weighted neural systems, there exist many iterative learning schemes for WNNs. For instance, in WNNs the tasks carried out by error back-propagation [93]can be achieved by feeding error information to all the hidden layers of a feedforward system in parallel.
Reference: [91] <author> R. Rohwer and M. Morciniec. </author> <title> The theoretical and experimental status of the n-tuple classifier. </title> <booktitle> Neural Networks, </booktitle> <volume> 11(1) </volume> <pages> 1-14, </pages> <year> 1998. </year>
Reference-contexts: This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems <ref> [90, 91] </ref>. The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework [31, 72, 92].
Reference: [92] <author> R. J. Rohwer. </author> <title> Two bayesian treatments of the n-tuple recognition method. </title> <booktitle> In Proceedings of the IEE International Conference on Neural Networks, </booktitle> <pages> pages 171-176, </pages> <address> UK, </address> <year> 1995. </year> <booktitle> Neural Computing Surveys 2, </booktitle> <pages> 41-61, </pages> <year> 1999, </year> <note> http://www.icsi.berkeley.edu/~jagota/NCS 61 </note>
Reference-contexts: The results obtained, together with previous ones [13, 88, 98], show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework <ref> [31, 72, 92] </ref>. <p> Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods <ref> [24, 31, 72, 98, 92] </ref>, where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise.
Reference: [93] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and the PDP Research Group, editors, </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference: [94] <editor> D. E. Rumelhart and J. L.; McClelland. </editor> <booktitle> Parallel Distributed Processing, volume 1: Foundations. </booktitle> <publisher> The MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Such a probability is computed based on probability distribution functions defined by x. This output function can be seen as a polynomial containing products up to N input signals. Thus, in the PDP terminology <ref> [94] </ref>, these pRAM nodes are sigma-pi units. Networks with pRAM nodes can also have the ability to approximate universal function [48]. <p> A new spreading algorithm for GRAMs called Combined Generalisation Algorithm (CGA) was proposed in [11]. Such an algorithm combines the properties of weightless neural systems and those of weighted Hebbian learning systems <ref> [94] </ref>. A new metric measure called Discriminating Distance was described, which takes into consideration the discrimination power of the input variables by assigning a discrimination coefficient to each one of them. <p> The discrimination coefficient is proportional to the input's ability to lead the node to fire, having a similar function to the synapses in the Hebbian learning <ref> [94] </ref>. GRAMs are often implemented as Virtual GRAMs (VGRAM) [79]. This is because the cost of storage in WNNs scales exponentially with the node fan-in (2 N ). A VGRAM is a GRAM in which space is not allocated for patterns which are never found in the training phase. <p> The first focuses on learning, generalisation, content addressable systems, and hardware implementation. The second was presented in [9]. Such a study was directed specifically at the Parallel Distributed Processing (PDP) framework <ref> [94] </ref>. In these comparative studies, the details of the weightless systems are given while many details of the weighted are omitted for being well known.
Reference: [95] <author> N. Sales, R. Evans, and I. Aleksander. </author> <title> Succesful naive representation grounding. </title> <journal> Artificial Intelligence Review, </journal> <volume> 10(1-2):83-102, </volume> <year> 1996. </year>
Reference-contexts: For instance, in [36] auto-associative (without external inputs) GNUs are shown to be able to learn reasoning structures found in semantic representation. In contrast, Sales et al. <ref> [95] </ref> carried out some linguistic experiments, concerning symbol grounding, by using heter-associative (with external inputs) GNUs. In [70], auto-associative GNUs were shown to be able to store up to 2 N patterns and that given an initial vector, or initial state, the network retrieves the nearest stored one. <p> But work needs to be done on semantics. In order to achieve the appropriate balance between "understanding" of language imparted by syntax and that imparted by semantics, the linguistic behaviour of temporal neural structures needs to be studied. Sales et al. <ref> [95] </ref> present a neural state machine, constructed from WNNs, to "ground" visual and linguistic representation. An interesting analysis of the dynamic of temporal neural structures can be found in [77, 103, 105, 107]. 4. Applications.
Reference: [96] <author> E. V. Sim~oes, L. F. Uebel, and D. A. C. Barone. </author> <title> Hardware implementation of RAM neural networks. </title> <journal> Pattern Recognition Letters, </journal> <volume> 17(4) </volume> <pages> 421-429, </pages> <year> 1996. </year>
Reference-contexts: So, they might require an amount of circuitry comparable to that of weighted neural systems. In [14, 41], respectively, examples of hardware implementation of the WISARD system (discrete components) and pRAM networks (VLSI) are presented, as well as their learning algorithm. In <ref> [96] </ref>, a technique for hardware and software implementation of trained RAM and GSN networks is introduced. Kennedy et al. [66] described the SAT processor; a dedicated hardware implementation of a binary neural image processor. The SAT processor is aimed specifically at supporting the ADAM networks.
Reference: [97] <author> M. J. Sixsmith, G. D. Tattersall, and J. M. Rollet. </author> <title> Speech recognition using n-tuple techniques. </title> <journal> British Telecom Technology Journal, </journal> <volume> 8(2) </volume> <pages> 50-60, </pages> <year> 1990. </year>
Reference-contexts: This is known as the saturation problem [100]. So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood <ref> [28, 24, 97, 100] </ref>. More recently, in order to overcome the saturation problem, Tarling and Rohwer [98] proposed a simple modification in the original N-tuple training algorithm.
Reference: [98] <author> R. Tarling and R. Rohwer. </author> <title> Efficient use of training data in the n-tuple recognition method. </title> <journal> IEE Electronics Letters, </journal> <volume> 29(24) </volume> <pages> 2093-2094, </pages> <year> 1993. </year>
Reference-contexts: This approach led to the description and construction of WISARD [14], a general purpose pattern recognition machine. Recently, a comparison between WISARD and other classification methods (including neural networks) was performed for various classification problems [90, 91]. The results obtained, together with previous ones <ref> [13, 88, 98] </ref>, show that the underlying principle of RAM networks (WISARD) is a powerful method. This method was also analysed into a probabilistic framework [31, 72, 92]. <p> So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood [28, 24, 97, 100]. More recently, in order to overcome the saturation problem, Tarling and Rohwer <ref> [98] </ref> proposed a simple modification in the original N-tuple training algorithm. Their experiments showed that the modification brought a decrease in the level of saturation and increase in generalisation accuracy over an identical experiment performed with the original algorithm. <p> Learning schemes similar to the Adaptive Resonance Theory [35], implemented with WNNs, were presented in [47, 67]. Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods <ref> [24, 31, 72, 98, 92] </ref>, where some of them, such as the ones in [31, 98], are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise. <p> Reinforcement learning procedures [25] for the WNNs were introduced in [52, 84, 81]. Additionally, there are algorithms based on standard statistical methods [24, 31, 72, 98, 92], where some of them, such as the ones in <ref> [31, 98] </ref>, are peculiar to the weightless neural models. * Generalisation: generalisation in WNNs is mainly obtained from the network as a whole. A conventional RAM node by itself does not generalise. That is, in WNNs, generalisation is sensitive to similarities to entire patterns instead of their constituent variables [8]. <p> On the other hand, cognitive computing needs efficient storage for "experience". To solve this problem, ways to decompose "experience" need to be considered. Studies about storage capacity of RAM-based networks are presented in <ref> [1, 33, 31, 86, 98] </ref>. 2. Planning. One of the major differences between rule-based and neural computing is the notion that a neural net can find the solution for a planning problem directly, whereas rule-based systems have to search quasi-exhaustively. Planning is a somewhat neglected area of neurocomputing.
Reference: [99] <author> J. G. Taylor. </author> <title> Spontaneous behaviour in neural networks. </title> <journal> Journal of Theoretical Biology, </journal> <volume> 36 </volume> <pages> 513-528, </pages> <year> 1972. </year>
Reference-contexts: Similarly, new information is only acquired after a series of experiences indicates its validity. So, the reinforcement learning procedure for PLNs was extended to take this into account and was applied to model delay learning in invertebrates [81]. In <ref> [99] </ref>, Taylor proposed a model of noisy neurons that presents equations which incorporate and formalise many known properties of living neurons. The evolution of such a model was shown to be equivalent to that of networks of noisy (probabilistic) RAMs or pRAMs [49].
Reference: [100] <author> J. R. Ullman. </author> <title> Experiments with the n-tuple method of pattern recognition. </title> <journal> IEEE Transactions on computers, </journal> <pages> pages 1135-1137, </pages> <year> 1969. </year>
Reference-contexts: The pattern is assigned the class corresponding to the discriminator with the highest response r. The performance of the N -tuple method depends mainly on the size of the N-tuple chosen <ref> [13, 89, 100, 101] </ref>. Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications [89, 90]. <p> Thus, problems arise when most memory locations are set to 1 so that a pattern also yields r = k in other discriminators. This is known as the saturation problem <ref> [100] </ref>. So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood [28, 24, 97, 100]. <p> This is known as the saturation problem [100]. So, in order to improve the performance of the N-tuple method, others learning algorithms have been proposed. These methods were based initially on the idea of maximum likelihood <ref> [28, 24, 97, 100] </ref>. More recently, in order to overcome the saturation problem, Tarling and Rohwer [98] proposed a simple modification in the original N-tuple training algorithm.
Reference: [101] <author> J. R. Ullman and P. A. </author> <title> Kidd. Recognition experiments with typed numeral from envelopes in the mail. </title> <journal> Pattern Recognition, </journal> (1):273-289, 1969. 
Reference-contexts: The pattern is assigned the class corresponding to the discriminator with the highest response r. The performance of the N -tuple method depends mainly on the size of the N-tuple chosen <ref> [13, 89, 100, 101] </ref>. Experimental results, for instance, showed that a bigger size of N is better, until it approaches an impractical large size, though a value of N = 8 turned out to be enough for many applications [89, 90].
Reference: [102] <author> V. N. Vapnik and A. Ja Chervonenkis. </author> <title> The necessary and sufficient conditions for consistency of the method of empical risk minimisation. </title> <journal> Pattern Recognition and Image Analysis, </journal> <volume> 1(3) </volume> <pages> 284-305, </pages> <year> 1991. </year>
Reference-contexts: A further training algorithm, called Stochastic Minimisation Algorithm (SMA), was proposed in [31]. The SMA was used for estimation of the Vapnik-Chervonenkis (VC) dimension <ref> [102] </ref> of the N-tuple classifier, but can also be applied to real-world data. A different kind of learning algorithm, involving changes in the connectivity pattern, was introduced by Jorgensen et al. in [63].
Reference: [103] <author> G. Vasconcelos and C. Fernandes. </author> <title> Error recovery behaviour of feedback RAM-networks. </title> <booktitle> In Proceedings of the IEE International Conference on Artificial Neural Networks, </booktitle> <volume> volume 349, </volume> <pages> pages 304-308, </pages> <address> UK, </address> <year> 1991. </year>
Reference-contexts: Sales et al. [95] present a neural state machine, constructed from WNNs, to "ground" visual and linguistic representation. An interesting analysis of the dynamic of temporal neural structures can be found in <ref> [77, 103, 105, 107] </ref>. 4. Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications [19].
Reference: [104] <author> Y. S. Wang, B. J. Griffiths, and B. A. Wilkie. </author> <title> A novel system for coloured object recognition. </title> <booktitle> Computers in Industry, </booktitle> <volume> 32(1) </volume> <pages> 69-77, </pages> <year> 1996. </year>
Reference-contexts: Both supervised [23, 47, 52, 58, 63] and unsupervised [15, 38, 42, 85] learning techniques have been used with WNNs. The effectiveness of WNN models for real-world applications has been demonstrated in a number of experimental studies <ref> [37, 56, 62, 74, 87, 90, 104] </ref>. The high speed of the learning process in WNNs is due to the existence of mutual independence between nodes when entries are changed, which contrasts with conventional weighted-sum-and-threshold neurons.
Reference: [105] <author> T. L. H. Watkin and D. Sherrington. </author> <title> Sequences of memories in Boolean networks. </title> <journal> Physica A, </journal> <volume> 185 </volume> <pages> 449-452, </pages> <year> 1992. </year>
Reference-contexts: Sales et al. [95] present a neural state machine, constructed from WNNs, to "ground" visual and linguistic representation. An interesting analysis of the dynamic of temporal neural structures can be found in <ref> [77, 103, 105, 107] </ref>. 4. Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications [19].
Reference: [106] <author> D. J. Willshaw, O P. Buneman, and H. C. Longuet-Higgins. </author> <title> Non-holographic associative memory. </title> <journal> Nature, </journal> <volume> 222 </volume> <pages> 960-962, </pages> <year> 1969. </year>
Reference-contexts: The use of RAM-based networks incorporating N -tuple methods in associative memories was also investigated in [23]. This work was motivated by the generalisation problems faced by Willshaw nets <ref> [106] </ref>. Such problems were reduced by the development of an associative architecture which combines an N -tuple front-end processor with two-stages of Willshaw net. This architecture is called ADAM (Advanced Distributed Associative Memory) [23].
Reference: [107] <author> M. J. D. Wilson and D. K. Milligan. </author> <title> Cyclic behaviour of autonomous, synchronous Boolean networks: some theorems and conjectures. </title> <journal> Connection Science, </journal> <volume> 4(2) </volume> <pages> 143-154, </pages> <year> 1992. </year>
Reference-contexts: Sales et al. [95] present a neural state machine, constructed from WNNs, to "ground" visual and linguistic representation. An interesting analysis of the dynamic of temporal neural structures can be found in <ref> [77, 103, 105, 107] </ref>. 4. Applications. There are many applications, such as scene and language understanding, where conventional computing fails to provide efficient solutions, and where neurocomputing offers the potential of making computational machinery more competent. The ADAM network has been used in scene analysis applications [19].
Reference: [108] <author> B. Zhang, L. Zhang, and H. Zhang. </author> <title> The complexity of learning in PLN networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 8(2) </volume> <pages> 221-228, </pages> <year> 1995. </year>
Reference-contexts: They also studied the functionality of pyramidal structures. Such a functionality was shown to be greater than that of weighted-sum-and-threshold neurons in [80]. Zhang et al. <ref> [108] </ref> presented both a training algorithm that can be used with pyramids of PLNs and a formula for computing the average number of steps that the algorithm converges (when there is a solution).
References-found: 108

