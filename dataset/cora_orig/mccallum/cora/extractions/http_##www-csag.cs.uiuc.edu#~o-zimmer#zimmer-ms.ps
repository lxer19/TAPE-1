URL: http://www-csag.cs.uiuc.edu/~o-zimmer/zimmer-ms.ps
Refering-URL: http://www-csag.cs.uiuc.edu/~o-zimmer/
Root-URL: http://www.cs.uiuc.edu
Note: c Copyright by Oolan M. Zimmer, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [Ast97] <author> Greg Astfalk. </author> <title> Personal conversation about scalability of applications, </title> <month> September </month> <year> 1997. </year>
Reference-contexts: However, shared-memory applications usually leave communication management to the compiler or the hardware, with less than optimal results. Some application designers have found that their message-passing applications on shared-memory machines get higher speedups because of the greater control of locality management <ref> [Ast97] </ref>.
Reference: [BBVvE95] <author> Anandya Basu, Vineet Buch, Werner Volgels, and Thorsten von Eicken. U-Net: </author> <title> A user-level network interface for parallel and distributed computing. </title> <booktitle> In Proceedings of the 15th ACM Symposium on Operating Systems Principles (SOSP), </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: We found that the greatest improvements in FMADI-T 2 performance came from exploitation of shared-memory semantics in FM-T, using FM-T only as a synchronization layer. 1.4 Related Work FM-T and FMADI-T add to a large body of research in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>. In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. <p> In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. Fast messages [PKC97], Active messages [vECGS92], U-Net <ref> [BBVvE95] </ref>, and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and ADI interfaces as well as <p> As expected, the increased queue costs for processing the acknowledgments were slight to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>, as well as work in communication issues on shared-memory machines [BCL + 95, HLW95].
Reference: [BCL + 95] <author> Eric A. Brewer, Frederic T. Chong, Lok T. Liu, John Kubiatowicz, and Shamik D. Sharma. </author> <title> Remote queues: Exposing network queues for atomicity and optimization. </title> <booktitle> In Proceedings of SPAA, </booktitle> <year> 1995. </year>
Reference-contexts: In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. Fast messages [PKC97], Active messages [vECGS92], U-Net [BBVvE95], and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues <ref> [BCL + 95] </ref> exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and ADI interfaces as well as the architecture of the HP/Convex Exemplar. <p> There are other interfaces, such as FM 2.0 [PKC97], Remote Queues <ref> [BCL + 95] </ref>, or Active Messages [vECGS92], that would also satisfy this goal. In particular, we chose to use FM 1.1 over FM 2.0 because the implementation of streaming would require a payload copy in FM-T, something we wanted to avoid. <p> to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication [BBVvE95, vECGS92, PKC97, THI96, LBD + 96], as well as work in communication issues on shared-memory machines <ref> [BCL + 95, HLW95] </ref>. Since our work approached shared-memory communication from the perspective of lightweight messaging, we focused on the design and use of lightweight message interfaces. 6.1 Fast Messages As mentioned in Section 2.1, there are a number of other Fast Messages implementations. <p> AM has since been implemented on Sun symmetric multiprocessors. 6.4 Remote Queues Remote Queues <ref> [BCL + 95] </ref>, has a receive-queue architecture that closely resembles the shared-memory receive queues that underly FM-T. However, Remote Queues explicitly allows control of the queues, while FM-T provides a messaging interface.
Reference: [Bia95] <author> Ricardo Bianchini. </author> <title> Exploiting bandwidth to reduce average memory access time in scalable multiprocessors. </title> <type> Tr 582 and ph.d. thesis, </type> <institution> University of Rochester, Computer Science Department, </institution> <month> April </month> <year> 1995. </year> <note> Thu, 17 Jul 97 09:00:00 GMT. </note>
Reference: [Cena] <institution> Ohio Supercomputing Center. </institution> <note> The LAM Release. World Wide Web. http://www.osc.edu/Lam/lam.html. </note>
Reference-contexts: It is much more complex than PVM and most other message passing interfaces, including support for complex user-defined datatypes for messages and global communication. There are several vendor implementations of MPI, as well as three research implementations. LAM <ref> [Cena] </ref> and CHIMP [Cenb] are research MPI implementations for networks of workstations, and MPICH [GLDS96] is a portable MPI implementation. 2.2.1 MPICH MPICH is an ongoing research project at Argonne National Labs; its goal is to produce and maintain a reference implementation for the MPI standard.
Reference: [Cenb] <institution> Edinborough Parallel Computing Centre. </institution> <note> The CHIMP Release. World Wide Web. http://www.epcc.ed.ac.uk/epcc-projects/CHIMP/. </note>
Reference-contexts: It is much more complex than PVM and most other message passing interfaces, including support for complex user-defined datatypes for messages and global communication. There are several vendor implementations of MPI, as well as three research implementations. LAM [Cena] and CHIMP <ref> [Cenb] </ref> are research MPI implementations for networks of workstations, and MPICH [GLDS96] is a portable MPI implementation. 2.2.1 MPICH MPICH is an ongoing research project at Argonne National Labs; its goal is to produce and maintain a reference implementation for the MPI standard.
Reference: [CKP + 93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subra-monian, and T. von Eicken. </author> <title> LogP: Toward a realistic model of parallel computation. </title> <booktitle> In ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Versions of these benchmarks were implemented to use FM-T or FMADI-T. The benchmarks were then run on a moderately-loaded HP/Convex S-Class with 16 processors. The resulting times were used to determine latency, bandwidth, and gap as described in <ref> [CKP + 93] </ref>. These tests do not directly determine overhead; the Multi-Send test, however, can be modified so that it measures overhead. 5.1.1 Benchmark Descriptions The Ping-pong / Latency Test The Ping-Pong benchmark is a standard networking test. It consists of two similar threads.
Reference: [Cor93] <institution> Convex Computer Corporation. Convex Exemplar Architecture, </institution> <month> November </month> <year> 1993. </year> <month> 84 </month>
Reference: [GBD + 94] <author> Al Geist, Adam Beguelin, Jack Dongarra, Weicheng Jiang, Bob Manchek, and Vaidy Sunderam. </author> <title> PVM: Parallel Virtual Machine|A User's Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: It was determined that low-level implementations of these guarantees is more efficient than in higher-level implementations, thus allowing greater potential application performance. 2.2 MPICH and ADI The Message Passing Interface (MPI) [GL95] is a portable standard for message passing. It is designed as a successor to PVM <ref> [GBD + 94] </ref> and vendor-specific message-passing libraries. It establishes a standard interface that is rich enough to provide sufficient flexibility for most or all applications, yet still is able to offer high performance.
Reference: [GGHL + 96] <author> A. Geist, W. Gropp, S. Huss-Lederman, A. Lumsdaine, E. Lusk, W. Saphir, T. Skjel-lum, and M. Snir. </author> <title> MPI-2: extending the Message-Passing Interface. </title> <editor> In Luc Bouge, P. Fraigniaud, A. Mignotte, and Y. Robert, editors, </editor> <booktitle> Euro-Par '96 parallel processing: second International Euro-Par Conference, </booktitle> <address> Lyon, France, </address> <month> August 26-29, </month> <year> 1996: </year> <booktitle> proceedings, volume 1123-1124 of Lecture notes in computer science, </booktitle> <pages> pages 128-135, </pages> <address> Berlin, Germany / Heidelberg, Germany / London, UK / etc., 1996. </address> <publisher> Springer-Verlag. </publisher>
Reference: [GL93] <author> William Gropp and Ewing Lusk. </author> <title> An abstract device definition to support the implementation of a high-level message-passing interface. </title> <type> Technical Report MCS-P342-1193, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <year> 1993. </year>
Reference-contexts: Some special-purpose collections of routines (such as device-independent support functions that may be called by the MPID layer, interface routines that are not part of the MPI standard, and routines for profiling) have separate name groupings. 2.2.2 ADI The Abstract Device Interface (ADI) <ref> [GL93] </ref> is the lowest layer of MPICH. This layer abstracts an architecture's native communication mechanisms into a standard interface. This interface is what makes MPICH portable. The version of ADI that we studied specifies the following interface functions: MPID Init () This function is called to initialize the MPID layer. <p> In addition to FM-T, we built FMADI-T, an implementation of the Abstract Device Interface <ref> [GL93] </ref> from MPICH [GLDS96] using FM-T, and measured the performance degradation of this more heavyweight layer. We implemented three optimizations for FMADI-T and analyzed the performance improvements of those optimizations. 4.1 Overview The ADI functions use send and receive handles from the MPIR layer of MPICH.
Reference: [GL95] <author> W. Gropp and E. Lusk. </author> <title> The MPI message-passing interface standard: Overview and status. </title> <editor> In J. J. Dongarra et al., editors, </editor> <booktitle> High performance computing: technology, methods, and applications (Advanced workshop, </booktitle> <month> June </month> <year> 1994, </year> <title> Cetraro, Italy), </title> <booktitle> volume 10 of Advances in Parallel Computing, </booktitle> <pages> pages 265-270, </pages> <address> Amsterdam, The Netherlands, 1995. </address> <publisher> Elsevier. </publisher>
Reference-contexts: The FM interface guarantees reliable, in-order delivery of messages. It was determined that low-level implementations of these guarantees is more efficient than in higher-level implementations, thus allowing greater potential application performance. 2.2 MPICH and ADI The Message Passing Interface (MPI) <ref> [GL95] </ref> is a portable standard for message passing. It is designed as a successor to PVM [GBD + 94] and vendor-specific message-passing libraries. It establishes a standard interface that is rich enough to provide sufficient flexibility for most or all applications, yet still is able to offer high performance.
Reference: [GLDS96] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementation of the MPI message passing interface standard. </title> <journal> Parallel Computing, </journal> <volume> 22(6) </volume> <pages> 789-828, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: There are several vendor implementations of MPI, as well as three research implementations. LAM [Cena] and CHIMP [Cenb] are research MPI implementations for networks of workstations, and MPICH <ref> [GLDS96] </ref> is a portable MPI implementation. 2.2.1 MPICH MPICH is an ongoing research project at Argonne National Labs; its goal is to produce and maintain a reference implementation for the MPI standard. MPICH is designed for easy porting rather than high performance. <p> In addition to FM-T, we built FMADI-T, an implementation of the Abstract Device Interface [GL93] from MPICH <ref> [GLDS96] </ref> using FM-T, and measured the performance degradation of this more heavyweight layer. We implemented three optimizations for FMADI-T and analyzed the performance improvements of those optimizations. 4.1 Overview The ADI functions use send and receive handles from the MPIR layer of MPICH.
Reference: [GLS95] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Programming with the Message Passing Interface. </title> <publisher> MIT Press, </publisher> <year> 1995. </year>
Reference: [HLW95] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <title> Tempest: A substrate for portable parallel programs. </title> <booktitle> In Compcon, </booktitle> <month> March </month> <year> 1995. </year> <note> Available from ftp://ftp.cs.wisc.edu/wwt/compcon95 tempest.ps. </note>
Reference-contexts: Fast messages [PKC97], Active messages [vECGS92], U-Net [BBVvE95], and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest <ref> [HLW95] </ref> supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and ADI interfaces as well as the architecture of the HP/Convex Exemplar. In Chapters 3 and 4 we present the design of FM-T and FMADI-T. <p> to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication [BBVvE95, vECGS92, PKC97, THI96, LBD + 96], as well as work in communication issues on shared-memory machines <ref> [BCL + 95, HLW95] </ref>. Since our work approached shared-memory communication from the perspective of lightweight messaging, we focused on the design and use of lightweight message interfaces. 6.1 Fast Messages As mentioned in Section 2.1, there are a number of other Fast Messages implementations. <p> This implies that the queue entries have variable size. Our use of a single pointer as the only user-supplied parameter permits a faster implementation without much loss of flexibility. 6.5 Tempest The Tempest Project <ref> [HLW95] </ref>, a product of the Wisconsin Wind Tunnel [RHL + 93], is an attempt to provide an interface for hybrid message-passing and shared-memory programs. It implements distributed shared memory with fine-grained access, and provides an active message [vECGS92] interface for synchronization. This meets many of the goals of FM-T.
Reference: [HP97] <institution> Convex Technology Division Hewlett-Packard. Exemplar Architecture: S-Class and X-Class Servers, </institution> <month> January </month> <year> 1997. </year> <month> 85 </month>
Reference: [Lab] <institution> Argonne National Laboratory. </institution> <note> The MPICH Release. World Wide Web. http://www.mcs.anl.gov/mpi/mpich. </note>
Reference: [LBD + 96] <author> J. V. Lawton, J. J. Brosnan, M. P. Doyle, S. D. O. Riordain, and T. G. Reddin. </author> <title> Building a high-performance message-passing system for MEMORY CHANNEL clusters. </title> <journal> Digital Technical Journal of Digital Equipment Corporation, </journal> <volume> 8(2) </volume> <pages> 96-116, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: We found that the greatest improvements in FMADI-T 2 performance came from exploitation of shared-memory semantics in FM-T, using FM-T only as a synchronization layer. 1.4 Related Work FM-T and FMADI-T add to a large body of research in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>. In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. <p> As expected, the increased queue costs for processing the acknowledgments were slight to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>, as well as work in communication issues on shared-memory machines [BCL + 95, HLW95].
Reference: [LC97] <author> Mario Lauria and Andrew Chien. </author> <title> MPI-FM: High performance MPI on workstation clusters. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 40(1) </volume> <pages> 4-18, </pages> <month> January </month> <year> 1997. </year>
Reference: [PKC97] <author> Scott Pakin, Vijay Karamcheti, and Andrew Chien. </author> <title> Fast messages (FM): Efficient, portable communication for workstation clusters and massively-parallel processors. </title> <journal> IEEE Concurrency, </journal> <year> 1997. </year>
Reference-contexts: We found that the greatest improvements in FMADI-T 2 performance came from exploitation of shared-memory semantics in FM-T, using FM-T only as a synchronization layer. 1.4 Related Work FM-T and FMADI-T add to a large body of research in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>. In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. <p> In addition to other FM and ADI over FM implementations <ref> [PKC97] </ref>, there are other lightweight message interfaces. Fast messages [PKC97], Active messages [vECGS92], U-Net [BBVvE95], and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of <p> In addition to other FM and ADI over FM implementations <ref> [PKC97] </ref>, there are other lightweight message interfaces. Fast messages [PKC97], Active messages [vECGS92], U-Net [BBVvE95], and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and <p> There are other interfaces, such as FM 2.0 <ref> [PKC97] </ref>, Remote Queues [BCL + 95], or Active Messages [vECGS92], that would also satisfy this goal. In particular, we chose to use FM 1.1 over FM 2.0 because the implementation of streaming would require a payload copy in FM-T, something we wanted to avoid. <p> As expected, the increased queue costs for processing the acknowledgments were slight to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>, as well as work in communication issues on shared-memory machines [BCL + 95, HLW95].
Reference: [RHL + 93] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood. </author> <title> The wisconsin wind tunnel: Virtual prototyping of parallel computers. </title> <booktitle> In ACM SIGMETRICS, </booktitle> <year> 1993. </year>
Reference-contexts: This implies that the queue entries have variable size. Our use of a single pointer as the only user-supplied parameter permits a faster implementation without much loss of flexibility. 6.5 Tempest The Tempest Project [HLW95], a product of the Wisconsin Wind Tunnel <ref> [RHL + 93] </ref>, is an attempt to provide an interface for hybrid message-passing and shared-memory programs. It implements distributed shared memory with fine-grained access, and provides an active message [vECGS92] interface for synchronization. This meets many of the goals of FM-T.
Reference: [THI96] <author> Hiroshi Tezuka, Atsushi Hori, and Yutaka Ishikawa. </author> <title> Design and Implementation of PM: A Communication Library for Workstation Cluster. </title> <booktitle> In JSPP, </booktitle> <pages> pages 41-48. </pages> <booktitle> Information Processing Society of Japan, </booktitle> <month> June </month> <year> 1996. </year> <title> (In Japanese). </title>
Reference-contexts: We found that the greatest improvements in FMADI-T 2 performance came from exploitation of shared-memory semantics in FM-T, using FM-T only as a synchronization layer. 1.4 Related Work FM-T and FMADI-T add to a large body of research in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>. In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. <p> In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. Fast messages [PKC97], Active messages [vECGS92], U-Net [BBVvE95], and PM <ref> [THI96] </ref> provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and ADI interfaces as well as the architecture of <p> As expected, the increased queue costs for processing the acknowledgments were slight to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>, as well as work in communication issues on shared-memory machines [BCL + 95, HLW95].
Reference: [vECGS92] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <type> Technical Report UCB//CSD-92-675, </type> <institution> EECS Computer Science Division, University of Cal-ifornia, Berkeley, </institution> <month> March </month> <year> 1992. </year> <month> 86 </month>
Reference-contexts: We found that the greatest improvements in FMADI-T 2 performance came from exploitation of shared-memory semantics in FM-T, using FM-T only as a synchronization layer. 1.4 Related Work FM-T and FMADI-T add to a large body of research in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>. In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. <p> In addition to other FM and ADI over FM implementations [PKC97], there are other lightweight message interfaces. Fast messages [PKC97], Active messages <ref> [vECGS92] </ref>, U-Net [BBVvE95], and PM [THI96] provides messaging interfaces of varying complexity for communication between processes, Remote Queues [BCL + 95] exposes receive queues to the user, and Tempest [HLW95] supports both message-passing and shared-memory communication. 1.5 Overview of Thesis In Chapter 2 we explain the FM and ADI interfaces as <p> There are other interfaces, such as FM 2.0 [PKC97], Remote Queues [BCL + 95], or Active Messages <ref> [vECGS92] </ref>, that would also satisfy this goal. In particular, we chose to use FM 1.1 over FM 2.0 because the implementation of streaming would require a payload copy in FM-T, something we wanted to avoid. <p> As expected, the increased queue costs for processing the acknowledgments were slight to nonexistent because the ping-pong benchmark never has more than one entry in the send or receive queues. 70 Chapter 6 Related Work There has been a lot of work in high-performance communication <ref> [BBVvE95, vECGS92, PKC97, THI96, LBD + 96] </ref>, as well as work in communication issues on shared-memory machines [BCL + 95, HLW95]. <p> It is tuned for an explicit shared-memory environment, using FM-T for synchronization and communication about shared memory regions. 6.3 Active Messages Active Messages <ref> [vECGS92] </ref> is a high-performance low-level message layer similar to Fast Messages. The original version of Active Messages has an API very similar to the FM 1.1 interface used by FM-T. <p> It implements distributed shared memory with fine-grained access, and provides an active message <ref> [vECGS92] </ref> interface for synchronization. This meets many of the goals of FM-T. Since FM-T is implemented on a coherent shared memory machine, FM-T applications have access to fine-grained shared memory and efficient bulk transfer (via memcpy ()), the elements described by the Tempest interface but not implemented in FM-T.
References-found: 23

