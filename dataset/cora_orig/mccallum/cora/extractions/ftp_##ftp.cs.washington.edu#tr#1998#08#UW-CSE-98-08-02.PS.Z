URL: ftp://ftp.cs.washington.edu/tr/1998/08/UW-CSE-98-08-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: Cascaded Execution: Speeding Up Unparallelized Execution on Shared-Memory Multiprocessors  
Author: Ruth E. Anderson, Thu D. Nguyen, and John Zahorjan 
Note: This work was supported in part by the National Science Foundation (Grant CCR-9704503), the Intel Corporation,  
Date: August 18, 1998 Revised September 26, 1998  
Address: Box 352350  Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  Microsoft Corporation, and Apple Computer, Inc.  
Pubnum: Technical Report UW-CSE-98-08-02  
Abstract: Both inherently sequential code and limitations of analysis techniques prevent full parallelization of many applications by parallelizing compilers. Amdahls Law tells us that as parallelization becomes increasingly effective, any unparallelized loop becomes an increasingly dominant performance bottleneck. We present a technique for speeding up the execution of unparallelized loops by cascading their sequential execution across multiple processors: only a single processor executes the loop body at any one time, and each processor executes only a portion of the loop body before passing control to another. Cascaded execution allows otherwise idle processors to optimize their memory state for the eventual execution of their next portion of the loop, resulting in significantly reduced overall loop body execution times. We evaluate cascaded execution using loop nests from wave5, a Spec95fp benchmark application, and a synthetic benchmark meant to assess the impact of the increasingly dominant memory access times of future processors. Running on a PC with 4 Pentium Pro processors and an SGI Power Onyx with 8 R10000 processors, we observe an overall speedup of 1.35 and 1.7, respectively, for the wave5 loops we examined, and speedups as high as 4.5 for individual loops. Our extrapolated results using the synthetic benchmark show a potential for speedups as large as 16 on future machines 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: However, hardware prefetching is usually limited to detecting only constant stride access patterns and can require a nontrivial amount of hardware support both to detect memory accesses that can be prefetched and to retrieve and hold the prefetched values. Multithreading <ref> [1, 27] </ref> tolerates latency by switching threads when a cache miss occurs. This technique can handle arbitrarily complex access patterns, but must be implemented in hardware to be effective.
Reference: 2. <author> J.M. Anderson, S.P. Amarasinghe, and M.S. Lam. </author> <title> Data and Computation Transformations for Multiprocessors. </title> <booktitle> In Proceedings of the Symposium on the Principles and Practice of Parallel Programming, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: Many techniques have been proposed for data restructuring <ref> [2, 14] </ref>. While any of these could be applied here, we adopt the sequential buffer technique.
Reference: 3. <author> D.F. Bacon, S.L. Graham, and O.J. Sharp. </author> <title> Compiler Transformations for High-Performance Computing. </title> <journal> ACM Computing Sureveys, </journal> <volume> 26(4), </volume> <year> 1994. </year>
Reference-contexts: 1 Introduction The focus of most of the work on parallelizing compilers has been on finding efficient, legal parallel executions of loops expressed using sequential semantics <ref> [3, 5] </ref>. This paper addresses a complementary issue, how to most efficiently execute loops for which the compiler cannot find a legal or efficient parallel realization.
Reference: 4. <author> J.-L. Baer and T.-F. Chen. </author> <title> An Effective On-Chip Preloading Scheme to Reduce Data Access Penalty. </title> <booktitle> In Proceedings of Supercomputing '91, </booktitle> <month> November </month> <year> 1991. </year>
Reference-contexts: Accurate analysis is crucial because prefetching can displace useful values in the cache, increase memory traffic, and increase the total number of instructions that must be executed. Hardware prefetching <ref> [4] </ref> is able to make use of dynamic information not available at compile time, and avoids the instruction overhead that software techniques incur.
Reference: 5. <author> U. Banerjee, R. Eigenmann, A. Nicolau, and D. Padua. </author> <title> Automatic Program Parallelization. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(2), </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction The focus of most of the work on parallelizing compilers has been on finding efficient, legal parallel executions of loops expressed using sequential semantics <ref> [3, 5] </ref>. This paper addresses a complementary issue, how to most efficiently execute loops for which the compiler cannot find a legal or efficient parallel realization.
Reference: 6. <author> D. Bhandarkar and J. Ding. </author> <title> Performance Characterization of the Pentium Pro Processor. </title> <booktitle> In Proceedings of the Third International Symposium on High Performance Computer Architecture, </booktitle> <month> Feb. </month> <year> 1997. </year>
Reference-contexts: Timing and cache miss measurements on the Pentium Pro were obtained using hardware counters that can be accessed in about 60 cycles with the RDTSC and RDPMC instructions. We measured DCU_LINES_IN and L2_LINES_IN for L1 data cache and L2 cache misses, respectively <ref> [6] </ref>. Timings on the R10000 were obtained using a 21 ns. resolution counter that can be accessed in about 100 cycles.
Reference: 7. <author> D. Burger, S. Kaxiras, and J.R. Goodman. </author> <title> DataScalar Architectures. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: In the dense case, cascaded execution provides speedups of around 4 for both systems. Speedups are even more impressive for the more memory-intensive sparse case: 16 for the Pentium Pro and close to 14 for the R10000. 4 Related Work Numerous hardware <ref> [7, 30] </ref> and software [9, 10, 20] techniques have been proposed to tolerate memory latency in sequential programs. The approaches most relevant to our work are prefetching and multithreading.
Reference: 8. <author> R. Eigenmann, J. Hoeflinger, and D. Padua. </author> <title> On the Automatic Parallelization of the Perfect Benchmarks. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 9(1), </volume> <year> 1998. </year>
Reference-contexts: The importance of such unparallelized loops is evidenced indirectly by the continuing intense interest in parallelization techniques, and directly by empirical studies of the effectiveness of current techniques <ref> [8, 11, 17, 23, 25] </ref>. Whether loops are left unparallelized because of inherently sequential semantics or limitations of the parallelization techniques, Amdahl's law tells us that these sequential code segments are a limiting factor to performance.
Reference: 9. <author> E.H. Gornish, E.D. Granston, and A.V. Veidenbaum. </author> <title> Compiler-directed Data Prefetching in Multiprocessors with Memory Hierarchies. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: In the dense case, cascaded execution provides speedups of around 4 for both systems. Speedups are even more impressive for the more memory-intensive sparse case: 16 for the Pentium Pro and close to 14 for the R10000. 4 Related Work Numerous hardware [7, 30] and software <ref> [9, 10, 20] </ref> techniques have been proposed to tolerate memory latency in sequential programs. The approaches most relevant to our work are prefetching and multithreading. Prefetching may be done in hardware or in software, and may involve prefetching values into cache or into a separate prefetch buffer. <p> The approaches most relevant to our work are prefetching and multithreading. Prefetching may be done in hardware or in software, and may involve prefetching values into cache or into a separate prefetch buffer. Cascaded execution is most similar to software-controlled prefetching <ref> [9, 20] </ref>. In this approach, the compiler analyzes the program and inserts prefetch instructions for accesses that will most likely result in cache misses.
Reference: 10. <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber. </author> <title> Comparative Evaluation of Latency Reducing and Tolerating Techniques. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1991. </year>
Reference-contexts: In the dense case, cascaded execution provides speedups of around 4 for both systems. Speedups are even more impressive for the more memory-intensive sparse case: 16 for the Pentium Pro and close to 14 for the R10000. 4 Related Work Numerous hardware [7, 30] and software <ref> [9, 10, 20] </ref> techniques have been proposed to tolerate memory latency in sequential programs. The approaches most relevant to our work are prefetching and multithreading. Prefetching may be done in hardware or in software, and may involve prefetching values into cache or into a separate prefetch buffer.
Reference: 11. <author> M.W. Hall, J. Anderson, S.P. Amarasinghe, B.R. Murphy, S.-W. Liao, E. Bugnion, and M.S. Lam. </author> <title> Maximizing Multiprocessor Performance with the SUIF Compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12), </volume> <year> 1996. </year>
Reference-contexts: The importance of such unparallelized loops is evidenced indirectly by the continuing intense interest in parallelization techniques, and directly by empirical studies of the effectiveness of current techniques <ref> [8, 11, 17, 23, 25] </ref>. Whether loops are left unparallelized because of inherently sequential semantics or limitations of the parallelization techniques, Amdahl's law tells us that these sequential code segments are a limiting factor to performance. <p> PARMVR is called approximately 5000 times and consists of 15 loops. Previous examination of these loops, including our own experience, showed difficulty with parallelization and no effective speedup in this application <ref> [11] </ref>. The original reference data set provided with wave5 is sized inappropriately for the caches on todays machines: the data set processed by each call to PARMVR is less than 300 KB.
Reference: 12. <author> Intel Corporation. </author> <title> Intel Architecture Software Developer's Manual. Order Number 243190. Vol. </title> <type> 1. </type> <institution> Intel Corporation, </institution> <year> 1997. </year>
Reference-contexts: Associativity Line Size Other info Pentium Pro L2 Memory 3 ~58 512 KB 2 - 32 bytes 32 bytes - On-chip, write-back On package, write-back, unified - L1 Memory 3 100-200 2 MB 2 - 32 bytes 128 bytes - On-chip, write-back External, write-back, unified - Table 1: Pentium Pro <ref> [12, 13] </ref> and R10000 [18] memory characteristics On the Pentium Pro, we use the Microsoft PowerStation Fortran compiler and NT threads to support cascaded execution. On the R10000, we used the MIPSpro 7.2 Fortran compiler and the m_fork command used by the SGI parallelizing compiler to spawn lightweight processes.
Reference: 13. <author> K. Keeton, D.A. Patterson, Y.Q. He, R.C. Raphael, and W.E. Baker. </author> <title> Performance Characterization of a Quad Pentium Pro SMP Using OLTP Workloads. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: Associativity Line Size Other info Pentium Pro L2 Memory 3 ~58 512 KB 2 - 32 bytes 32 bytes - On-chip, write-back On package, write-back, unified - L1 Memory 3 100-200 2 MB 2 - 32 bytes 128 bytes - On-chip, write-back External, write-back, unified - Table 1: Pentium Pro <ref> [12, 13] </ref> and R10000 [18] memory characteristics On the Pentium Pro, we use the Microsoft PowerStation Fortran compiler and NT threads to support cascaded execution. On the R10000, we used the MIPSpro 7.2 Fortran compiler and the m_fork command used by the SGI parallelizing compiler to spawn lightweight processes.
Reference: 14. <author> S.-T. Leung. </author> <title> Array Restructuring for Cache Locality. </title> <type> Technical Report UW-CSE-96-08-01, </type> <institution> Department of Computer Science, University of Washington, </institution> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Many techniques have been proposed for data restructuring <ref> [2, 14] </ref>. While any of these could be applied here, we adopt the sequential buffer technique.
Reference: 15. <author> S.-T. Leung and J. Zahorjan. </author> <title> Improving the Performance of Runtime Parallelization. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: At the same time, cascaded execution may enhance the performance of these techniques by simplifying and improving the memory reference behavior. Several speculative and runtime parallelization methods have been proposed to attempt parallel execution of loops that cannot be analyzed sufficiently accurately at compile time <ref> [15, 21, 22, 31] </ref>. Like cascaded execution, these techniques make use of processors that would otherwise be idle if the compiler resorted to simple, sequential execution. The two approaches involve different tradeoffs, however.
Reference: 16. <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Policy for Multiprogrammed Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2), </volume> <year> 1993. </year>
Reference-contexts: In Section 4, we describe related work on hiding memory latency and making use of otherwise idle processors during sequential code segments. Section 5 concludes the paper. 1 While it is possible for the system scheduler to reallocate the idle processors to other applications during these intervals <ref> [16, 19] </ref>, this can entail considerable overhead and is not commonly done. 3 2 Cascaded Execution a system with three processors. Note that processors 2 and 3 are idle while processor 1 executes the sequential section.
Reference: 17. <author> K.S. McKinley. </author> <title> Evaluating Automatic Parallelization for Efficient Execution on Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: The importance of such unparallelized loops is evidenced indirectly by the continuing intense interest in parallelization techniques, and directly by empirical studies of the effectiveness of current techniques <ref> [8, 11, 17, 23, 25] </ref>. Whether loops are left unparallelized because of inherently sequential semantics or limitations of the parallelization techniques, Amdahl's law tells us that these sequential code segments are a limiting factor to performance.
Reference: 18. <institution> MIPS Technologies Inc. R10000 Microprocessor User's Manual-Version 2.0. MIPS Technologies Inc., </institution> <year> 1997. </year>
Reference-contexts: info Pentium Pro L2 Memory 3 ~58 512 KB 2 - 32 bytes 32 bytes - On-chip, write-back On package, write-back, unified - L1 Memory 3 100-200 2 MB 2 - 32 bytes 128 bytes - On-chip, write-back External, write-back, unified - Table 1: Pentium Pro [12, 13] and R10000 <ref> [18] </ref> memory characteristics On the Pentium Pro, we use the Microsoft PowerStation Fortran compiler and NT threads to support cascaded execution. On the R10000, we used the MIPSpro 7.2 Fortran compiler and the m_fork command used by the SGI parallelizing compiler to spawn lightweight processes.
Reference: 19. <author> J.E. Moreira and V.K. Naik. </author> <title> Dynamic Resource Management on Distributed Systems Using Reconfigurable Applications. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 41(3), </volume> <year> 1997. </year>
Reference-contexts: In Section 4, we describe related work on hiding memory latency and making use of otherwise idle processors during sequential code segments. Section 5 concludes the paper. 1 While it is possible for the system scheduler to reallocate the idle processors to other applications during these intervals <ref> [16, 19] </ref>, this can entail considerable overhead and is not commonly done. 3 2 Cascaded Execution a system with three processors. Note that processors 2 and 3 are idle while processor 1 executes the sequential section.
Reference: 20. <author> T.C. Mowry, M.S. Lam, and A. Gupta. </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching. </title> <booktitle> In Proceedings of the International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> Sept. </month> <year> 1992. </year> <month> 14 </month>
Reference-contexts: In the dense case, cascaded execution provides speedups of around 4 for both systems. Speedups are even more impressive for the more memory-intensive sparse case: 16 for the Pentium Pro and close to 14 for the R10000. 4 Related Work Numerous hardware [7, 30] and software <ref> [9, 10, 20] </ref> techniques have been proposed to tolerate memory latency in sequential programs. The approaches most relevant to our work are prefetching and multithreading. Prefetching may be done in hardware or in software, and may involve prefetching values into cache or into a separate prefetch buffer. <p> The approaches most relevant to our work are prefetching and multithreading. Prefetching may be done in hardware or in software, and may involve prefetching values into cache or into a separate prefetch buffer. Cascaded execution is most similar to software-controlled prefetching <ref> [9, 20] </ref>. In this approach, the compiler analyzes the program and inserts prefetch instructions for accesses that will most likely result in cache misses.
Reference: 21. <author> L. Rauchwerger, N.M. Amato, and D.A. Padua. </author> <title> RunTime Methods for Parallelizing Partially Parallel Loops. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: At the same time, cascaded execution may enhance the performance of these techniques by simplifying and improving the memory reference behavior. Several speculative and runtime parallelization methods have been proposed to attempt parallel execution of loops that cannot be analyzed sufficiently accurately at compile time <ref> [15, 21, 22, 31] </ref>. Like cascaded execution, these techniques make use of processors that would otherwise be idle if the compiler resorted to simple, sequential execution. The two approaches involve different tradeoffs, however.
Reference: 22. <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative RunTime Parallelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In Proceedings of the Conference on Programming Language Design and Implementation, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: At the same time, cascaded execution may enhance the performance of these techniques by simplifying and improving the memory reference behavior. Several speculative and runtime parallelization methods have been proposed to attempt parallel execution of loops that cannot be analyzed sufficiently accurately at compile time <ref> [15, 21, 22, 31] </ref>. Like cascaded execution, these techniques make use of processors that would otherwise be idle if the compiler resorted to simple, sequential execution. The two approaches involve different tradeoffs, however.
Reference: 23. <author> J.P. Singh and J.L. Hennessy. </author> <title> An Empirical Investigation of the Effectiveness and Limitations of Automatic Parallelization. </title> <booktitle> In Proceedings of the International Symposium on Shared Memory Multiprocessing, </booktitle> <month> April </month> <year> 1991. </year>
Reference-contexts: The importance of such unparallelized loops is evidenced indirectly by the continuing intense interest in parallelization techniques, and directly by empirical studies of the effectiveness of current techniques <ref> [8, 11, 17, 23, 25] </ref>. Whether loops are left unparallelized because of inherently sequential semantics or limitations of the parallelization techniques, Amdahl's law tells us that these sequential code segments are a limiting factor to performance.
Reference: 24. <author> J.P. Singh, J.L. Hennessy, and A. Gupta. </author> <title> Scaling Parallel Programs for Multiprocessors: Methodology and Examples. </title> <journal> Computer, </journal> <volume> 26(7), </volume> <year> 1993. </year>
Reference-contexts: The original reference data set provided with wave5 is sized inappropriately for the caches on todays machines: the data set processed by each call to PARMVR is less than 300 KB. Larger problem sizes provided with the benchmark grow along the time dimension but not in the space dimension <ref> [24] </ref>. Since the original data set was too small to be representative of problems likely to be run on todays parallel machines, we enlarged the problem by increasing the amount of data accessed in each loop.
Reference: 25. <author> B. So, S. Moon, and M.W. Hall. </author> <title> Measuring the Effectiveness of Automatic Parallelization in SUIF. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1998. </year>
Reference-contexts: The importance of such unparallelized loops is evidenced indirectly by the continuing intense interest in parallelization techniques, and directly by empirical studies of the effectiveness of current techniques <ref> [8, 11, 17, 23, 25] </ref>. Whether loops are left unparallelized because of inherently sequential semantics or limitations of the parallelization techniques, Amdahl's law tells us that these sequential code segments are a limiting factor to performance.
Reference: 26. <author> E. Torrie, M. Martonosi, C.-W. Tseng, and M.W. Hall. </author> <title> Characterizing the Memory Behavior of Compiler-Parallelized Applications. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(12), </volume> <year> 1996. </year>
Reference-contexts: We focus on reducing the execution times of sequential code segments by reducing the number of cache misses that occur. In programs that manipulate large in-memory structures, a characteristic typical of many compiler-parallelized applications, memory access costs can be a substantial fraction of execution times <ref> [26] </ref>. It is well known that cache miss penalties are becoming increasingly costly as memory access time fails to keep up with increasing processor speed and instruction-level parallelism.
Reference: 27. <author> D.M. Tullsen, S.J. Eggers, J.S. Emer, H.M. Levy, J.L. Lo, and R.L. Stamm. </author> <title> Exploiting Choice: Instruction Fetch and Issue on an Implementable Simultaneous Multithreading Processor. </title> <booktitle> In Proceedings of the International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: However, hardware prefetching is usually limited to detecting only constant stride access patterns and can require a nontrivial amount of hardware support both to detect memory accesses that can be prefetched and to retrieve and hold the prefetched values. Multithreading <ref> [1, 27] </ref> tolerates latency by switching threads when a cache miss occurs. This technique can handle arbitrarily complex access patterns, but must be implemented in hardware to be effective.
Reference: 28. <author> R.Y. Wang, A. Krishnamurthy, R.P. Martin, T.E. Anderson, and D.E. Culler. </author> <title> Modeling Communication Pipeline Latency. </title> <booktitle> In Proceedings of the SIGMETRICS '98/PERFORMANCE '98 Conference, </booktitle> <month> June </month> <year> 1998. </year>
Reference-contexts: This initial execution phase is unoptimized, and so each iteration in it will take longer than iterations in later execution phases. For this reason, ramping up the chunk sizes at the beginning of the cascaded execution of a loop may be necessary to obtain optimum performance <ref> [28] </ref>. However, thus far we have considered only constant chunk sizes. We choose the chunk size based on an estimate of the number of bytes of data that each iteration of the execution loop will touch.
Reference: 29. <author> M. Wolfe. </author> <title> High Performance Compilers for Parallel Computing. </title> <publisher> Addison-Wesley, </publisher> <year> 1996. </year>
Reference-contexts: This removes the addition, as well as the extra load instruction and cache space for accessing both operands from the execute loop. This form of loop distribution <ref> [29] </ref>, while possible in general, fits naturally in the cascaded execution framework. 3 For this to be legal, it must be certain that the value of the restructured data is up-to-date at the time the helper function is executed.
Reference: 30. <author> Y. Yamada, T.L. Johnson, G.E. Haab, J.C. Gyllenhaal, W.-m.W. Hwu, and J. Torrellas. </author> <title> Reducing Cache Misses in Numerical Applications Using Data Relocation and Prefetching. </title> <type> Technical Report CRHC-95-04, </type> <institution> Center for Reliable and High Performance Computing, </institution> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: In the dense case, cascaded execution provides speedups of around 4 for both systems. Speedups are even more impressive for the more memory-intensive sparse case: 16 for the Pentium Pro and close to 14 for the R10000. 4 Related Work Numerous hardware <ref> [7, 30] </ref> and software [9, 10, 20] techniques have been proposed to tolerate memory latency in sequential programs. The approaches most relevant to our work are prefetching and multithreading.
Reference: 31. <author> Z. Ye, L. Rauchwerger, and J. Torrellas. </author> <title> Hardware for Speculative RunTime Parallelization in Distributed Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the International Symposium on High Performance Computer Architecture, </booktitle> <month> Feb </month> <year> 1998. </year>
Reference-contexts: At the same time, cascaded execution may enhance the performance of these techniques by simplifying and improving the memory reference behavior. Several speculative and runtime parallelization methods have been proposed to attempt parallel execution of loops that cannot be analyzed sufficiently accurately at compile time <ref> [15, 21, 22, 31] </ref>. Like cascaded execution, these techniques make use of processors that would otherwise be idle if the compiler resorted to simple, sequential execution. The two approaches involve different tradeoffs, however.
Reference: 32. <author> M. Zagha, B. Larson, S. Turner, and M. Itzkowitz. </author> <title> Performance Analysis Using the MIPS R10000 Performance Counters. </title> <booktitle> In Proceedings of Supercomputing '96, </booktitle> <month> November </month> <year> 1996. </year>
Reference-contexts: We measured DCU_LINES_IN and L2_LINES_IN for L1 data cache and L2 cache misses, respectively [6]. Timings on the R10000 were obtained using a 21 ns. resolution counter that can be accessed in about 100 cycles. Cache misses were obtained from hardware counters via a more expensive system call <ref> [32] </ref>. 3.3 Current Performance 4 of the PARMVR subroutine of the wave5 benchmark when run under cascaded execution with 64KB chunks (which was found to perform best on both platforms among the chunk sizes we evaluated).
References-found: 32

