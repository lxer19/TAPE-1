URL: http://www.eecs.umich.edu/PPP/CSE-TR-277-96.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Title: Characterizing Shared Memory and Communication Performance: A Case Study of the Convex SPP-1000  
Author: Gheith A. Abandah Edward S. Davidson 
Keyword: Shared-memory Multiprocessor, SPP-1000, Memory Performance, Communication Performance, and Performance Evaluation.  
Note: January 8, 1996  
Address: 1301 Beal Avenue, Ann Arbor, MI 48109-2122  
Affiliation: Advanced Computer Architecture Laboratory, Department of EECS University of Michigan  
Email: gabandah,davidson@eecs.umich.edu  
Phone: TEL: (313) 936-2917, FAX: (313) 763-4617  
Abstract: The objective of this paper is to develop models that characterize the memory and communication performance of shared-memory multiprocessors which is crucial for developing efficient parallel applications for them. The Convex SPP-1000, a modern scalable distributed-memory multiprocessor that supports the shared-memory programming paradigm, is used throughout as a case study. The paper evaluates and models four aspects of SPP-1000 performance: scheduling, local-memory, shared-memory, and synchronization. Our evaluation and modeling are in tended to supply useful information for application and compiler development.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Hwang, </author> <title> Advanced computer architecture: parallelism, scalability, programmability. </title> <publisher> McGraw-Hill, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction A distributed-memory multiprocessor is a scalable shared-memory parallel processor that uses a high-bandwidth, low-latency interconnection network to connect processing nodes which contain processors and memory <ref> [1] </ref>. The interconnection network provides the communication channels through which nodes exchange data and coordinate their work in solving a parallel application. Different types of interconnection networks vary in throughput, number of communication links per node, and topology.
Reference: [2] <author> D. Lenoski et al., </author> <title> The Stanford DASH Multiprocessor, </title> <journal> Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 63-79, </pages> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: Different types of interconnection networks vary in throughput, number of communication links per node, and topology. Mesh, ring, and multistage interconnection network (MIN) are three of the commonly used topologies <ref> [2, 3, 4, 5] </ref>. fl The University of Michigan Center for Parallel Computing, site of the SPP-1000, is partially funded by NSF grant CDA-92-14296. In a distributed-memory multiprocessor, the physical memory is distributed among the nodes but forms one global address space.
Reference: [3] <author> J. Kuskin et al., </author> <title> The Stanford FLASH Multiprocessor, </title> <booktitle> in International Symposium on Com--puter Architecture, </booktitle> <pages> pp. 302-313, </pages> <year> 1994. </year>
Reference-contexts: Different types of interconnection networks vary in throughput, number of communication links per node, and topology. Mesh, ring, and multistage interconnection network (MIN) are three of the commonly used topologies <ref> [2, 3, 4, 5] </ref>. fl The University of Michigan Center for Parallel Computing, site of the SPP-1000, is partially funded by NSF grant CDA-92-14296. In a distributed-memory multiprocessor, the physical memory is distributed among the nodes but forms one global address space.
Reference: [4] <author> E. Boyd and E. Davidson, </author> <title> Communication in the KSR1 MPP: performance evaluation using synthetic workload experiments, </title> <booktitle> in International Conference on Supercomputing, </booktitle> <pages> pp. 166-175, </pages> <year> 1994. </year>
Reference-contexts: Different types of interconnection networks vary in throughput, number of communication links per node, and topology. Mesh, ring, and multistage interconnection network (MIN) are three of the commonly used topologies <ref> [2, 3, 4, 5] </ref>. fl The University of Michigan Center for Parallel Computing, site of the SPP-1000, is partially funded by NSF grant CDA-92-14296. In a distributed-memory multiprocessor, the physical memory is distributed among the nodes but forms one global address space.
Reference: [5] <author> T. Agerwala et al., </author> <title> SP2 system architecture, </title> <journal> IBM Systems Journal, </journal> <volume> vol. 34, no. 2, </volume> <pages> pp. 152-184, </pages> <year> 1995. </year>
Reference-contexts: Different types of interconnection networks vary in throughput, number of communication links per node, and topology. Mesh, ring, and multistage interconnection network (MIN) are three of the commonly used topologies <ref> [2, 3, 4, 5] </ref>. fl The University of Michigan Center for Parallel Computing, site of the SPP-1000, is partially funded by NSF grant CDA-92-14296. In a distributed-memory multiprocessor, the physical memory is distributed among the nodes but forms one global address space.
Reference: [6] <institution> Convex Computer Corporation, </institution> <address> P.O. Box 833851, Richardson, TX 75083-3851, </address> <note> Convex Exemplar Programming Guide, </note> <editor> x3.0.0.2 ed., </editor> <month> June </month> <year> 1994. </year>
Reference-contexts: Convex SPP-1000 The Convex Exemplar SPP-1000 consists of 1 to 16 Hypernodes <ref> [6] </ref>. Each Hypernode contains 4 Functional Blocks and an I/O Interface interconnected by a 5-port crossbar, rather than a bus to achieve higher throughput. Each Functional Block contains 2 Hewlett-Packard PA-RISC 7100 processors, Memory, and some control devices (see Figures 2 and 3).
Reference: [7] <institution> Scalable Coherent Interface (SCI). </institution> <address> ANSI/IEEE Std 1596, </address> <year> 1992. </year>
Reference-contexts: Each Functional Block contains 2 Hewlett-Packard PA-RISC 7100 processors, Memory, and some control devices (see Figures 2 and 3). The Functional Blocks communicate across the Hypernodes via four CTI (Coherent Toroidal Interconnect) Rings. The CTI supports an extended version of the Scalable Coherent Interface (SCI) standard <ref> [7] </ref>. The CTI supports global memory accesses by providing or invalidating one cache line in response to a global memory access. Each CTI Ring is a pair of unidirectional links with a peak transfer rate of 600 MB/sec for each link.
Reference: [8] <author> A. Geist et al., </author> <title> PVM 3 User's Guide and Reference Manual. </title> <institution> Oak Ridge National Laboratory, Oak Ridge, Tennessee 37831, </institution> <month> Sept. </month> <year> 1994. </year> <month> ORNL/TM-12187. </month>
Reference-contexts: The SPP-1000 supports the shared-memory programming model. Its Fortran and C compilers can automatically parallelize simple loops. The compilers feature some directives that enable programmers to assist in parallelizing more difficult loops and to exploit task parallelism. The SPP-1000 also supports the PVM <ref> [8] </ref> and MPI [9] message-passing libraries. 2 Scheduling Overhead The scheduling time is the time needed by the parallel environment to start and finish parallel tasks on p processors.
Reference: [9] <author> E. Anderson et al., </author> <title> MPI: a message-passing interface standard, Technical Report, Message Passing Interface Forum, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: The SPP-1000 supports the shared-memory programming model. Its Fortran and C compilers can automatically parallelize simple loops. The compilers feature some directives that enable programmers to assist in parallelizing more difficult loops and to exploit task parallelism. The SPP-1000 also supports the PVM [8] and MPI <ref> [9] </ref> message-passing libraries. 2 Scheduling Overhead The scheduling time is the time needed by the parallel environment to start and finish parallel tasks on p processors.
Reference: [10] <author> G. Abandah and E. Davidson, </author> <title> Modeling the communication performance of the IBM SP2, </title> <booktitle> in 10th International Parallel Processing symposium (IPPS'96), </booktitle> <address> (Honolulu, Hawaii), </address> <month> April </month> <year> 1996. </year>
Reference-contexts: Measuring the execution wall time is a good approximation for the SSO. Figure 4 shows the range and average of the SSO for 10 runs. Using curve fitting, the SSO in seconds can be roughly approximated by: SSO (p) = 1:2 + 0:22p Compared with multicomputers <ref> [10] </ref>, the SPP-1000 has relatively short SSO. The SPP-1000 advantage stems from having one operating system image with central control that swiftly allocates and starts parallel tasks. Moreover, multiple processors can share the same executable binaries. The Convex Fortran and C parallelizing compilers enable parallelizing loops. <p> We suggest that the methodology presented in this paper can be applied to other shared-memory multiprocessors and that the resulting characterization is useful for developing and tuning shared-memory applications and compilers. We have shown that the corresponding characterization of message-passing multicomputer communication performance <ref> [10, 15] </ref> can also be systematically carried out.
Reference: [11] <author> K. Gallivan, D. Gannon, W. Jalby, A. Malony, and H. Wijshoff, </author> <title> Experimentally characterizing the behavior of multiprocessor memory systems: A case study, </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 16, </volume> <pages> pp. 216-223, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: The PLSO is approximately proportional to the number of processors and has sudden increases when the additional processor is from a new Hypernode. Using curve fitting, the PLSO can be roughly approximated by: PLSO (p) = 14:3p 3 Local-memory Performance We have used Load/Store kernels <ref> [11, 12, 13] </ref> to characterize the performance of the local-memory. is a serial program with an inner loop that loads double-precision (8 Bytes) 1-dimensional array elements into the floating-point registers. The array size is varied from 1 KBytes to 3 times the data cache size.
Reference: [12] <author> W. H. Mangione-Smith, T. P. Shih, S. G. Abraham, and E. S. Davidson, </author> <title> Approaching a machine-application bound in delivered performance on scientific code, </title> <journal> IEEE Proceedings, </journal> <volume> vol. 81, </volume> <pages> pp. 1166-1178, </pages> <month> Aug. </month> <year> 1993. </year>
Reference-contexts: The PLSO is approximately proportional to the number of processors and has sudden increases when the additional processor is from a new Hypernode. Using curve fitting, the PLSO can be roughly approximated by: PLSO (p) = 14:3p 3 Local-memory Performance We have used Load/Store kernels <ref> [11, 12, 13] </ref> to characterize the performance of the local-memory. is a serial program with an inner loop that loads double-precision (8 Bytes) 1-dimensional array elements into the floating-point registers. The array size is varied from 1 KBytes to 3 times the data cache size.
Reference: [13] <author> R. Saavedra, R. Gaines, and M. Carlton, </author> <title> Micro benchmark analysis of the KSR1, </title> <booktitle> in Supercomputing, </booktitle> <pages> pp. 202-213, </pages> <year> 1993. </year>
Reference-contexts: The PLSO is approximately proportional to the number of processors and has sudden increases when the additional processor is from a new Hypernode. Using curve fitting, the PLSO can be roughly approximated by: PLSO (p) = 14:3p 3 Local-memory Performance We have used Load/Store kernels <ref> [11, 12, 13] </ref> to characterize the performance of the local-memory. is a serial program with an inner loop that loads double-precision (8 Bytes) 1-dimensional array elements into the floating-point registers. The array size is varied from 1 KBytes to 3 times the data cache size.
Reference: [14] <author> J. Mellor-Crummey and M. Scott, </author> <title> Algorithms for scalable synchronization on shared-memory multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 9, no. 1, </volume> <pages> pp. 21-65, </pages> <year> 1991. </year>
Reference-contexts: The big difference between near and far access performance, as presented in this paper, sheds light on the performance gains that can be achieved by localizing the data structures of SPP-1000 applications. The implementation of the synchronization barrier subroutines for the SPP-1000 is inefficient and better algorithms are available <ref> [14] </ref>. We suggest that the methodology presented in this paper can be applied to other shared-memory multiprocessors and that the resulting characterization is useful for developing and tuning shared-memory applications and compilers.
Reference: [15] <author> G. A. Abandah and E. S. Davidson, </author> <title> Modeling computation and communication performance of the IBM SP2, </title> <type> Technical Report CSE-TR-258-95, </type> <institution> University of Michigan, </institution> <type> Rm. 3402, 1301 Beal Ave., </type> <address> Ann Arbor, MI 48109, </address> <month> May </month> <year> 1995. </year> <month> 16 </month>
Reference-contexts: We suggest that the methodology presented in this paper can be applied to other shared-memory multiprocessors and that the resulting characterization is useful for developing and tuning shared-memory applications and compilers. We have shown that the corresponding characterization of message-passing multicomputer communication performance <ref> [10, 15] </ref> can also be systematically carried out.
References-found: 15

