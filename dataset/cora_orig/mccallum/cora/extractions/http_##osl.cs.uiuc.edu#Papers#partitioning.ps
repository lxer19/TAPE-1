URL: http://osl.cs.uiuc.edu/Papers/partitioning.ps
Refering-URL: http://osl.cs.uiuc.edu/Papers/Parallel.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: fpanwar j agha g@cs.uiuc.edu  
Title: Partitioning and Distribution Strategies as First Class Objects resource management policies used. Such policies determine,
Author: Rajendra Panwar and Gul Agha 
Note: on the  can be  
Address: 1304 W. Springfield Avenue  Urbana, IL 61801, USA  
Affiliation: Open Systems Laboratory Department of Computer Science  University of Illinois at Urbana-Champaign  
Abstract: fl The research described has been made possible by support from the Office of Naval Research (ONR contract numbers N00014-90-J-1899 and N00014-93-1-0273), by an Incentives for Excellence Award from the Digital Equipment Corporation Faculty Program, and by joint support from the Defense Advanced Research Projects Agency and the National Science Foundation (NSF CCR 90-07195). y The authors thank the referees for providing several constructive suggestions. The authors thank Daniel Sturman for his development of Broadway. Svend Frtlund, Daniel Sturman and Wooyoung Kim and other members of the Open Systems Laboratory have provided helpful suggestions and discussions. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Actors are self-contained, interactive components of a computing system that communicate by asynchronous message passing <ref> [1] </ref>. Each actor has a mail address and a behavior. Mail addresses may be communicated, thereby providing 12 a dynamic topology.
Reference: [2] <author> G. Agha, C. Houck, and R. Panwar. </author> <title> Distributed execution of actor systems. </title> <editor> In D. Gelernter, T. Gross, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 1-17. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <note> Lecture Notes in Computer Science 589. </note>
Reference-contexts: Actors do not introduce unnecessary sequentiality. In particular, actor programs naturally represent all the parallelism in an ideal algorithm including the available overlap of computation and communication <ref> [2] </ref>. The experiments discussed in this paper are implemented using an actor kernel called Broadway. The kernel provides a minimal set of primitive operations that may be used to efficiently construct more complex abstractions. Broadway is written in C++ and facilitates the specialization and customization of system components.
Reference: [3] <editor> G. Agha, P. Wegner, and A. Yonezawa, editors. </editor> <booktitle> Research Directions in Object-Oriented Programming. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachussets, </address> <year> 1993. </year> <note> (to be published). </note>
Reference-contexts: The matrix-coordinator actor may not get the row sizes in order of their indices. The following synchronization constraint can be used by the matrix-coordinator to enforce the order of indices of rows (for a discussion of synchronization constraints, see <ref> [3] </ref>). enable (index (row) = counter) A more concurrent implementation of such a strategy is developed as follows. An array R is used to store the number of elements of the i th row as R [i]. The row-coordinator s can write the sizes of different rows concurrently in R.
Reference: [4] <author> G. Agha and K. Wooyoung. </author> <title> Compilation of a highly parallel actor-based language. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proceedings of the Workshop on Languages and Compilers for Parallel Computing. </booktitle> <publisher> Yale University, Springer-Verlag, </publisher> <year> 1992. </year> <note> LNCS, to be published. </note>
Reference-contexts: We are investigating compiler optimizations for generating efficient code for the high level language constructs <ref> [4] </ref>. We will define representative prototypes which encapsulate details of the architecture dependent PDS's for a given abstract data type and use delegation [25] to access those representations. Delegation allows the representation to be shared without exposing the methods used in the prototype.
Reference: [5] <author> G. A. Agha, S. Frtlund, W. Kim, R. Panwar, A. Patterson, and D. Sturman. </author> <title> Abstraction and modularity mechanisms for concurrent computing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 3-14, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The PDS abstractions may be reused with different applications that use the same data structure. Our methodology allows composition of PDS's with ideal algorithms as shown in figure 2, (see ref. <ref> [5] </ref>). Although the specification of the ideal algorithms is different, the applications may use the same PDS. Note that while a PDS may be architecture dependent, the ideal algorithm is not.
Reference: [6] <author> W. Athas and C. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 9-23, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor <ref> [6] </ref>, HAL [17], ABCL [30], Mentat [15], ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28].
Reference: [7] <author> J. Bruno and P. R. Cappello. </author> <title> Implementing the beam and warming method on the hypercube. </title> <booktitle> In Proceedings of the Third Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 1073-1087, </pages> <address> Pasadena, CA, </address> <month> January </month> <year> 1988. </year>
Reference-contexts: In such cases, objects need to be migrated during execution in order to improve performance. For example, using 32 processors to implement the Beam-Warming algorithm, an implicit time-marching scheme for Navier-Stokes, the execution efficiency is 28% with static mapping <ref> [7] </ref> and 80% using migration [27]. Scalability affects the choice of PDS in two ways. First, the best PDS may be a function of the architecture and problem sizes. For some parallel algorithms, when the architecture size is scaled up, different PDS's may become more efficient.
Reference: [8] <author> P. A. Buhr, G. Ditchfield, R. A. Stroobosscher, B. M. Younger, and C. R. Zaranke. </author> <title> C++: Concurrency in the object-oriented language C++. </title> <journal> Software Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 137-172, </pages> <month> February </month> <year> 1992. </year> <month> 27 </month>
Reference-contexts: Some approaches extend the sequential object-oriented language C++ with constructs for parallel computing. In particular, Chandy and Kesselman have extended C++ with parallel programming constructs such as par , parfor and spawn [11]. Another effort whose goal is to introduce concurrency in C++ is <ref> [8] </ref>. They present C++, a dialect of C++ which adds features such as coroutines, monitors and threads. An object-based model called Distributed Collection Model for parallel programming is presented in [24] along with an experimental language PC++.
Reference: [9] <author> P. Mehrotra C. Koelbel. </author> <title> Compiling global name-space parallel loops for distibuted exe-cution. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 440-451, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The final section discusses future work. 3 Previous Work In this section we present a brief review of some of the existing programming languages that provide support for data partitioning and distribution of parallel programs. Support for partitioning and distributing arrays is provided in languages such as Kali <ref> [9] </ref>, Vienna 6 Fortran [13] and Fortran D [22]. The languages Kali and Vienna Fortran allow the pro-grammer to declare a processor array and specify distribution of each dimension of a data array onto the processor array.
Reference: [10] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A guide to the perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> 21(3) </volume> <pages> 323-357, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The constructs provided for specifying PDS's in languages such as Kali, Vienna Fortran and Fortran D are well suited for static data structures and also do not utilize the features provided by object-oriented technology such as encapsulation and inheritance. Some language models, such as Linda <ref> [10] </ref>, abstract away from the underlying architecture to such an extent that specification of placement strategies may not be feasible. Other languages use annotations for deciding the placement of tasks on processors (for example, [29, 18]).
Reference: [11] <author> K. M. Chandy and C. Kesselman. </author> <title> Compositional c++: Compositional parallel programming. </title> <type> Technical Report Caltech-CS-TR-92-13, </type> <institution> California Institute of Technology, Computer Science Department, </institution> <year> 1992. </year>
Reference-contexts: For example, an object-based approach using parallel sets of a given types is presented in [23]. Some approaches extend the sequential object-oriented language C++ with constructs for parallel computing. In particular, Chandy and Kesselman have extended C++ with parallel programming constructs such as par , parfor and spawn <ref> [11] </ref>. Another effort whose goal is to introduce concurrency in C++ is [8]. They present C++, a dialect of C++ which adds features such as coroutines, monitors and threads. An object-based model called Distributed Collection Model for parallel programming is presented in [24] along with an experimental language PC++.
Reference: [12] <author> K. M. Chandy and Stephen Taylor. </author> <title> An Introduction to Parallel Programming. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, </address> <year> 1992. </year>
Reference-contexts: Other examples of dynamic data structures which require input history to express a PDS include trees and sparse graphs and matrices. Partitioning strategies for dynamic trees are presented in <ref> [12] </ref>. Assume that the rows of a sparse matrix are being read from a file. The 10 the i th element that arrives from the source. matrix is stored using pairs of column indices and element values and rows are separated by a special character.
Reference: [13] <author> B. M. Chapman, P. Mehrotra, and H. P. Zima. </author> <title> Vienna fortran a fortran language extension for distributed memory multiprocessors. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Compilers and Runtime Software for Scalable Multiprocessors. </title> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: Support for partitioning and distributing arrays is provided in languages such as Kali [9], Vienna 6 Fortran <ref> [13] </ref> and Fortran D [22]. The languages Kali and Vienna Fortran allow the pro-grammer to declare a processor array and specify distribution of each dimension of a data array onto the processor array.
Reference: [14] <author> S. Frtlund and G. Agha. </author> <title> A language framework for multi-object coordination. </title> <booktitle> In Proceedings of ECOOP 1993. </booktitle> <publisher> Springer Verlag, </publisher> <month> July </month> <year> 1993. </year> <note> LNCS 627. </note>
Reference-contexts: We are experimenting with sample ideal algorithms and PDS's. We are also adding performance monitors to Broadway and studying optimizations to the kernel. Finally, research on specification techniques for multi-object coordination are being developed these techniques will allow modular and reusable abstractions for PDS's requiring coordination <ref> [14] </ref>.
Reference: [15] <author> A. Grimshaw, W. T. Strayer, and P. Narayan. </author> <title> Dynamic object-oriented parallel processing. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2) </volume> <pages> 33-47, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor [6], HAL [17], ABCL [30], Mentat <ref> [15] </ref>, ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28]. Some of these concurrent object-oriented languages allow the programmer to create a new object at a specific processor by annotating the create command with the target processor address. <p> In fact, on standard benchmarks, the absolute performance of the current implementation is not competitive with implementations of other more restrictive programming models. For example, the performance figures for Mentat <ref> [15] </ref> are several fold better. However, the network we used in our experiments was not a dedicated network for running Broadway but was shared with other users.
Reference: [16] <author> Michael T. Heath, Esmond Ng, and Barry W. Peyton. </author> <title> Parallel algorithms for sparse linear systems. </title> <journal> SIAM Review, </journal> <volume> 33(3) </volume> <pages> 420-460, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Another example of a parallel sparse matrix computation that requires coordination between multiple objects is matrix reordering using parallel nested dissection algorithm <ref> [16] </ref>. The reordered matrix can be considered as a new matrix being created and the parallel reordering algorithm acts as a concurrent source of its elements. 5 The Actor Kernel In our computational model, data and the processes manipulating it are unified into message driven objects called actors.
Reference: [17] <author> C. Houck and G. Agha. HAL: </author> <title> A high-level actor language and its distributed implementation. </title> <booktitle> In Proceedings of th 21st International Conference on Parallel Processing (ICPP '92), </booktitle> <volume> volume II, </volume> <pages> pages 158-165, </pages> <address> St. Charles, IL, </address> <month> August </month> <year> 1992. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor [6], HAL <ref> [17] </ref>, ABCL [30], Mentat [15], ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28].
Reference: [18] <author> P. Hudak. </author> <title> Para-functional programming. </title> <booktitle> IEEE Computer, </booktitle> <pages> pages 60-70, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: Some language models, such as Linda [10], abstract away from the underlying architecture to such an extent that specification of placement strategies may not be feasible. Other languages use annotations for deciding the placement of tasks on processors (for example, <ref> [29, 18] </ref>). Languages that use annotations for specifying PDS's mix the concurrency specif-cation with that of the architecture specific PDS code thereby making the programs less portable across architectures. 7 A number of researchers are experimenting with a data parallel programming paradigm.
Reference: [19] <author> L. H. Jamieson. </author> <title> Characterizing parallel algorithms. </title> <editor> In R. J. Douglass L.H. Jamieson, D.B. Gannon, editor, </editor> <booktitle> The Characteristics of Parallel Algorithms, </booktitle> <pages> pages 65-100. </pages> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: times faster than the processors on the mesh. 2 Our Approach Current programming methods for concurrent computers intermix specification of resource management policies with the code specifying the ideal algorithm - i.e., the set of operations in the algorithm and the partial order in which they may be carried out <ref> [19] </ref>. The confla-tion of design goals resulting from such intermixing complicates the code and reduces its reusability. This paper discusses an object-oriented methodology to support the separate specification of an ideal algorithm and the policies used to partition and distribute it onto a concurrent architecture.
Reference: [20] <author> D. Kafura. </author> <title> Concurrent object-oriented real-time systems research. </title> <journal> SIGPLAN Notices, </journal> <volume> 24(4) </volume> <pages> 203-205, </pages> <month> April </month> <year> 1989. </year> <booktitle> (Proceedings of the Workshop on Object-Based Concurrent Programming, </booktitle> <year> 1988). </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor [6], HAL [17], ABCL [30], Mentat [15], ACT++ <ref> [20] </ref> and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28]. Some of these concurrent object-oriented languages allow the programmer to create a new object at a specific processor by annotating the create command with the target processor address.
Reference: [21] <author> L. Kale. </author> <title> The CHARM(3.0) Programming Language Manual. </title> <institution> University of Illinois, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor [6], HAL [17], ABCL [30], Mentat [15], ACT++ [20] and Charm <ref> [21] </ref>. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28]. Some of these concurrent object-oriented languages allow the programmer to create a new object at a specific processor by annotating the create command with the target processor address.
Reference: [22] <author> Ken Kennedy and Ulrich Kremer. </author> <title> Automatic data alignment and distribution for loosely synchronous problems in an interactive programming environment. </title> <institution> Technical Report Rice COMP TR91-155, Rice University, </institution> <year> 1991. </year>
Reference-contexts: Support for partitioning and distributing arrays is provided in languages such as Kali [9], Vienna 6 Fortran [13] and Fortran D <ref> [22] </ref>. The languages Kali and Vienna Fortran allow the pro-grammer to declare a processor array and specify distribution of each dimension of a data array onto the processor array. In Fortran D, [22] data distribution is specified using abstract structures called decompositions which provide a frame of reference for inter-array alignment. <p> and distributing arrays is provided in languages such as Kali [9], Vienna 6 Fortran [13] and Fortran D <ref> [22] </ref>. The languages Kali and Vienna Fortran allow the pro-grammer to declare a processor array and specify distribution of each dimension of a data array onto the processor array. In Fortran D, [22] data distribution is specified using abstract structures called decompositions which provide a frame of reference for inter-array alignment. The ALIGN statement is used to map arrays onto decompositions. The decompositions are mapped to the physical machine by using the DISTRIBUTE statement.
Reference: [23] <author> M. J. Kilian. </author> <title> Object-oriented programming for massively parallel machines. </title> <type> Technical Report TR-13-91, </type> <institution> Center for Research in Computing Technology, Harvard University, Aiken Computation Laboratory, </institution> <address> 33 Oxford Street, Cambridge, MA 02138, </address> <year> 1991. </year> <month> 28 </month>
Reference-contexts: For example, an object-based approach using parallel sets of a given types is presented in <ref> [23] </ref>. Some approaches extend the sequential object-oriented language C++ with constructs for parallel computing. In particular, Chandy and Kesselman have extended C++ with parallel programming constructs such as par , parfor and spawn [11]. Another effort whose goal is to introduce concurrency in C++ is [8].
Reference: [24] <author> Jenq Kuen Lee and Dennis Gannon. </author> <title> Object-oriented parallel programming experiments and results. </title> <booktitle> In Proceedings Supercomputing 91, </booktitle> <pages> pages 273-282, </pages> <year> 1991. </year>
Reference-contexts: Another effort whose goal is to introduce concurrency in C++ is [8]. They present C++, a dialect of C++ which adds features such as coroutines, monitors and threads. An object-based model called Distributed Collection Model for parallel programming is presented in <ref> [24] </ref> along with an experimental language PC++. Although the language allows features such as inheritance and data-encapsulation, the language constructs used for partitioning and distribution are similar to Kali and the main distributed data-structure assumed is static array.
Reference: [25] <author> H. Lieberman. </author> <title> Using prototypical objects to implement shared behavior in object oriented systems. </title> <booktitle> In OOPSLA Proceedings, </booktitle> <pages> pages 214-223, </pages> <year> 1986. </year>
Reference-contexts: We are investigating compiler optimizations for generating efficient code for the high level language constructs [4]. We will define representative prototypes which encapsulate details of the architecture dependent PDS's for a given abstract data type and use delegation <ref> [25] </ref> to access those representations. Delegation allows the representation to be shared without exposing the methods used in the prototype.
Reference: [26] <author> Carl Manning. Acore: </author> <title> The design of a core actor language and its compiler. </title> <type> Master's thesis, </type> <institution> MIT, Artificial Intelligence Laboratory, </institution> <month> August </month> <year> 1987. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore <ref> [26] </ref>, Cantor [6], HAL [17], ABCL [30], Mentat [15], ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28].
Reference: [27] <author> P. Porta. </author> <title> Implicit finite-difference simulation of an internal flow on hypercube. </title> <institution> Research Report YALEU/DCS/RR-594, Yale University, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: In such cases, objects need to be migrated during execution in order to improve performance. For example, using 32 processors to implement the Beam-Warming algorithm, an implicit time-marching scheme for Navier-Stokes, the execution efficiency is 28% with static mapping [7] and 80% using migration <ref> [27] </ref>. Scalability affects the choice of PDS in two ways. First, the best PDS may be a function of the architecture and problem sizes. For some parallel algorithms, when the architecture size is scaled up, different PDS's may become more efficient.
Reference: [28] <author> M. Di Santo, G. Iannello, and W. Russo. </author> <title> Ask: A transputer implementation of the actor model. </title> <editor> In M. Valero, E. Onate, M. Jane, J. L. Larriba, and B. Suarez, editors, </editor> <booktitle> Parallel Computing and Transputer Applications, </booktitle> <pages> pages 611-620. </pages> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in <ref> [28] </ref>. Some of these concurrent object-oriented languages allow the programmer to create a new object at a specific processor by annotating the create command with the target processor address. Such annotations result in mixing of the ideal algorithm specification with the PDS specification.
Reference: [29] <author> E. Shapiro, </author> <title> editor. Concurrent Prolog. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachussets, </address> <year> 1987. </year>
Reference-contexts: Some language models, such as Linda [10], abstract away from the underlying architecture to such an extent that specification of placement strategies may not be feasible. Other languages use annotations for deciding the placement of tasks on processors (for example, <ref> [29, 18] </ref>). Languages that use annotations for specifying PDS's mix the concurrency specif-cation with that of the architecture specific PDS code thereby making the programs less portable across architectures. 7 A number of researchers are experimenting with a data parallel programming paradigm.
Reference: [30] <author> K. Taura, S. Matsuoka, and A. Yonezawa. </author> <title> An efficient implementation scheme of concurrent object-oriented languages on stock multicomputers. </title> <booktitle> In Fourth ACM SIG-PLAN Symposium on Principles and Practice of Parallel Programming PPOPP, </booktitle> <pages> pages 218-228, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette [31], Acore [26], Cantor [6], HAL [17], ABCL <ref> [30] </ref>, Mentat [15], ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28].
Reference: [31] <author> C. Tomlinson, P. Cannata, G. Meredith, and D. Woelk. </author> <title> The extensible services switch in carnot. </title> <journal> IEEE Parallel and Distributed Technology: Systems and Applications, </journal> <volume> 1(2), </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: A number of concurrent object-oriented languages and programming environments have been developed and used for parallel programming (for example, Rosette <ref> [31] </ref>, Acore [26], Cantor [6], HAL [17], ABCL [30], Mentat [15], ACT++ [20] and Charm [21]. A run-time kernel for concurrent object-oriented programming specialized to the transputer architecture is presented in [28].
Reference: [32] <author> E. S. Wu, R. Wesley, and D. Calahan. </author> <title> Performance analysis and projections for a massively-parallel Navier-Stokes implementation. </title> <booktitle> In Proceedings of the Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, CA, </address> <month> March </month> <year> 1989. </year>
Reference-contexts: For example, an implementation of the 3D Navier-Stokes equation based on the explicit MacCormack scheme on a 1024 processor NCUBE uses Gray code mapping for ensuring 2 nearest neighbor communication <ref> [32] </ref>. Without the use of Gray code mapping, the efficiency falls from 90% to 68%. Static partitioning and distribution strategies (PDS's) are not always efficient. An optimal mapping at one stage of the algorithm may be non-optimal for another.
References-found: 32

