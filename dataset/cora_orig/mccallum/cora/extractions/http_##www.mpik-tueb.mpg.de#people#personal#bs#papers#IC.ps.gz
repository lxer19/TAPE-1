URL: http://www.mpik-tueb.mpg.de/people/personal/bs/papers/IC.ps.gz
Refering-URL: http://www.mpik-tueb.mpg.de/people/personal/bs/svm.html
Root-URL: 
Phone: 3  
Title: Predicting Time Series with Support Vector Machines  
Author: K.-R. Muller A. J. Smola G. Ratsch B. Scholkopf J. Kohlmorgen V. Vapnik 
Note: 2 Max-Planck-Institut f. biol. Kybernetik, Spemannstr. 38,  
Address: FIRST, Rudower Chaussee 5, 12489 Berlin, Germany  72076 Tubingen, Germany  101 Crawfords Corner Rd, PO 3030, Holmdel 07733, NJ, USA  
Affiliation: 1 GMD  AT&T Research,  
Abstract: Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an * insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 37%.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. A. Aizerman, E. M. Braverman, and L. I. </author> <title> Rozonoer (1964), Theoretical foundations of the potential function method in pattern recognition learning. </title> <journal> Automation and Remote Control, </journal> <volume> 25 </volume> <pages> 821-837. </pages>
Reference-contexts: Taking (3) and (1) into account, we are able to rewrite the whole problem in terms of dot products in the low dimensional input space (a concept introduced in <ref> [1] </ref>) f (x) = i=1 i )((x i ) (x)) + b = i=1 i )k (x i ; x) + b: (4) In Eq.(4) we introduced a kernel function k (x i ; x j ) = ((x i ) (x j )).
Reference: 2. <author> C.M. </author> <title> Bishop (1995) , Neural networks for pattern recognition, </title> <publisher> Oxford U. Press. </publisher>
Reference-contexts: However, we do not only adjust the output weights by backpropagation (on squared loss with regularization), but also the RBF centers and variances. In this way, the networks fine-tune themselves to the data after the clustering step, yet of course overfitting has to be avoided (cf. <ref> [2] </ref>). We fix the following experimental setup for our comparison: (a) RBF nets and (b) SVR are trained using a simple cross validation technique. We stop training the RBF networks at the minimum of the one step prediction error measured on a validation set.
Reference: 3. <author> B. E. Boser, I .M. Guyon, and V. N. Vapnik. </author> <year> (1992), </year> <title> A training algorithm for optimal margin classifiers. </title> <editor> In D. Haussler, editor, </editor> <booktitle> Proc. of COLT'92, </booktitle> <pages> 144. </pages>
Reference-contexts: 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. <ref> [3, 13] </ref>). They have been applied successfully to classification tasks as OCR [13, 10] and more recently also to regression [4, 14]. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance. <p> A brief discussion concludes the paper. 2 Support Vector Regression In SVR the basic idea is to map the data x into a high dimensional feature space F via a nonlinear mapping and to do linear regression in this space (cf. <ref> [3, 13] </ref>) f (x) = (! (x)) + b with : R n ! F ; ! 2 F ; (1) where b is a threshold. Thus, linear regression in a high dimensional (feature) space corresponds to nonlinear regression in the low dimensional input space R n . <p> It can be shown that any symmetric kernel function k satisfying Mercer's condition cor responds to a dot product in some feature space (see <ref> [3] </ref> for details). A common kernel is e.g. a RBF kernel k (x; y) = exp (jjx yjj 2 =(2 2 )). Vapnik's *-insensitive Loss Function.
Reference: 4. <author> H. Drucker, C. Burges, L. Kaufman, A. Smola, V. </author> <month> Vapnik </month> <year> (1997), </year> <title> Linear support vector regression machines, </title> <publisher> NIPS'96. </publisher>
Reference-contexts: 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. [3, 13]). They have been applied successfully to classification tasks as OCR [13, 10] and more recently also to regression <ref> [4, 14] </ref>. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance.
Reference: 5. <author> P. J. </author> <title> Huber (1972), Robust statistics: a review. </title> <journal> Ann. Statist., 43:1041. </journal>
Reference-contexts: Other cost functions like the robust loss function in the sense of <ref> [5] </ref> can also be utilized (cf. Fig. 1a) [11]. <p> Table 1 confirms this intuition largely. For uniform noise the whole scenario should be reversed, since *-insensitive loss is the more appropriate noise model (cf. <ref> [5] </ref>). This is again confirmed in the experiment. The use of a validation set to assess the proper parameters and *, however, is suboptimal and so the low resolution with which the (; *) space is scanned is partly responsible for table entries that do not match the above intuition.
Reference: 6. <author> M. C. Mackey and L. </author> <title> Glass (1977), </title> <journal> Science, </journal> <volume> 197 </volume> <pages> 287-289. </pages>
Reference-contexts: Our first application is a high-dimensional chaotic system generated by the Mackey-Glass delay differential equation dx (t) = 0:1x (t) + 1 + x (t t d ) 10 ; (9) with delay t d = 17. Eq. (9) was originally introduced as a model of blood cell regulation <ref> [6] </ref> and became quite common as artificial forecasting benchmark. After integrating (9), we added noise to the time series.
Reference: 7. <editor> J. Moody and C. </editor> <booktitle> Darken (1989), Neural Computation, </booktitle> <volume> 1(2) </volume> <pages> 281-294. </pages>
Reference-contexts: no prediction available. "Full set" means, that the full training set of set D was used, whereas "segmented set" means that a prior segmentation according to [8] was done as preprocessing. 3 Experiments The RBF nets used in the experiments are based on the the method of Moody and Darken <ref> [7] </ref>. However, we do not only adjust the output weights by backpropagation (on squared loss with regularization), but also the RBF centers and variances. In this way, the networks fine-tune themselves to the data after the clustering step, yet of course overfitting has to be avoided (cf. [2]).
Reference: 8. <author> K. Pawelzik, J. Kohlmorgen, and K.-R. </author> <title> Muller (1996), Annealed competition of experts for a segmentation and classification of switching dynamics. </title> <journal> Neural Computation, </journal> <volume> 8(2) </volume> <pages> 342-358. </pages>
Reference-contexts: For benchmark data from the Santa Fe Competition (data set D) we get the best result achieved so far which is 37% better than the winning approach during the competition [15] and still 29% better than our previous result <ref> [8] </ref>. <p> Huber RBF ZH [15] PKM <ref> [8] </ref> full set 0.0639 0.0653 0.0677 0.0665 - segmented set 0.0418 0.0425 0.0569 - 0.0596 Table 1. <p> Second Table: Comparison of 25 step iterated predictions (root mean squared errors) on Data set D. "-" denotes: no prediction available. "Full set" means, that the full training set of set D was used, whereas "segmented set" means that a prior segmentation according to <ref> [8] </ref> was done as preprocessing. 3 Experiments The RBF nets used in the experiments are based on the the method of Moody and Darken [7]. However, we do not only adjust the output weights by backpropagation (on squared loss with regularization), but also the RBF centers and variances. <p> As embedding 20 consecutive points were used. Since the time series is non-stationary, we first segment into regimes of approximately stationary dynamics with competing predictors <ref> [8] </ref>. We use only the subset for training (327 patterns) which was tagged by the predictor responsible for the data points at the end of the full training set. <p> This allows us to train the RBF networks and the SVR on quasi stationary data and we avoid to predict the average over all dynamical modes hidden in the full training set (see also <ref> [8] </ref> for further discussion), however at the same time we are left with a rather small training set requiring careful regularization. As in the previous section we use a validation set (50 patterns) to determine the stopping point and (; *) respectively. <p> Table 1 shows that our 25 step iterated prediction of the SVR is 37% better than the one achieved by Zhang et al. [15], who assumed a stationary model. It is still 29% better than our previous result <ref> [8] </ref> that used the same preprocessing as above and simple RBF nets with non-adaptive centers and variances. <p> For the data set D benchmark we ob-tained excellent results for SVR - 37% above the best result achieved during the Santa Fe competition [15]. Clearly, this remarkable difference is mostly due to the segmentation used as preprocessing step to get stationary data <ref> [8] </ref>, nevertheless still 29% improvement remain compared to a previous result using the same preprocessing step [8]. <p> Clearly, this remarkable difference is mostly due to the segmentation used as preprocessing step to get stationary data <ref> [8] </ref>, nevertheless still 29% improvement remain compared to a previous result using the same preprocessing step [8]. This underlines that we need to consider possible non-stationarities or even mixings in the time series before the actual prediction, for which we used SVR or RBF nets (see also [8, 9] for discussion). <p> This underlines that we need to consider possible non-stationarities or even mixings in the time series before the actual prediction, for which we used SVR or RBF nets (see also <ref> [8, 9] </ref> for discussion). Our experiments show that SVR methods work particularly well if the data is sparse (i.e. we have little data in a high dimensional space). This is due to their good inherent regularization properties.
Reference: 9. <author> K. Pawelzik, K.-R. Muller, J. </author> <month> Kohlmorgen </month> <year> (1996), </year> <title> Prediction of Mixtures, </title> <booktitle> in ICANN '96, </booktitle> <publisher> LNCS 1112, Springer Berlin, </publisher> <address> 127-132 and GMD Tech.Rep. </address> <month> 1069. </month>
Reference-contexts: This underlines that we need to consider possible non-stationarities or even mixings in the time series before the actual prediction, for which we used SVR or RBF nets (see also <ref> [8, 9] </ref> for discussion). Our experiments show that SVR methods work particularly well if the data is sparse (i.e. we have little data in a high dimensional space). This is due to their good inherent regularization properties.
Reference: 10. <author> B. Scholkopf, C. Burges, V. </author> <month> Vapnik </month> <year> (1995), </year> <title> Extracting support data for a given task. </title> <editor> In: U. M. Fayyad and R. Uthurusamy (eds.), </editor> <booktitle> Proceedings, First International Conference on Knowledge Discovery & Data Mining, </booktitle> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA. </address>
Reference-contexts: 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. [3, 13]). They have been applied successfully to classification tasks as OCR <ref> [13, 10] </ref> and more recently also to regression [4, 14]. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance.
Reference: 11. <author> A. J. Smola, B. </author> <title> Scholkopf (1997) On a kernel-based method for pattern recognition, regression, approximation and operator inversion. </title> <note> Algorithmica to appear and GMD Tech.Rep. 1064. </note>
Reference-contexts: For this special cost function the La-grange multipliers ff i ; ff fl i are sparse, i.e. they result in non-zero values after the optimization (2) only if they are on the boundary (see Fig. 1b), which means that they fulfill the Karush-Kuhn-Tucker conditions (for more details see <ref> [13, 11] </ref>). The *-insensitive cost function is given by C (f (x) y) = jf (x) yj * for jf (x) yj * 0 otherwise (5) (cf. <p> problem is defined as minimize 1 l X (ff fl j ff j )k (x i ; x j ) i=1 i (y i *) ff i (y i + *) (a) (b) Fig. 1. (a) *-insensitive and Huber's loss for * = 1. (b) The shown regression (kernel: B-splines <ref> [11] </ref>) of the sinc function is the flattest within the * tube around the data. ff; ff fl are drawn as positive and negative forces respectively. <p> Other cost functions like the robust loss function in the sense of [5] can also be utilized (cf. Fig. 1a) <ref> [11] </ref>.
Reference: 12. <author> A.S. Weigend, N.A. Gershenfeld (Eds.) </author> <year> (1994), </year> <title> Time Series Prediction: Forecasting the Future and Understanding the Past, </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Data Set D from the Santa Fe Competition. Data set D from the Santa Fe competition is artificial data generated from a nine-dimensional periodically driven dissipative dynamical system with an asymmetrical four-well potential and a drift on the parameters <ref> [12] </ref>. As embedding 20 consecutive points were used. Since the time series is non-stationary, we first segment into regimes of approximately stationary dynamics with competing predictors [8].
Reference: 13. <author> V. </author> <month> Vapnik </month> <year> (1995), </year> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer NY. </publisher>
Reference-contexts: 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. <ref> [3, 13] </ref>). They have been applied successfully to classification tasks as OCR [13, 10] and more recently also to regression [4, 14]. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance. <p> 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. [3, 13]). They have been applied successfully to classification tasks as OCR <ref> [13, 10] </ref> and more recently also to regression [4, 14]. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance. <p> A brief discussion concludes the paper. 2 Support Vector Regression In SVR the basic idea is to map the data x into a high dimensional feature space F via a nonlinear mapping and to do linear regression in this space (cf. <ref> [3, 13] </ref>) f (x) = (! (x)) + b with : R n ! F ; ! 2 F ; (1) where b is a threshold. Thus, linear regression in a high dimensional (feature) space corresponds to nonlinear regression in the low dimensional input space R n . <p> It can be shown that the vector ! can be written in terms of the data points ! = i=1 i )(x i ) (3) with ff i ; ff fl i being the solution of the aforementioned quadratic programming problem <ref> [13] </ref>. ff i ; ff fl i have an intuitive interpretation (see Fig. 1b) as forces pushing and pulling the estimate f (x i ) towards the measurements y i . <p> For this special cost function the La-grange multipliers ff i ; ff fl i are sparse, i.e. they result in non-zero values after the optimization (2) only if they are on the boundary (see Fig. 1b), which means that they fulfill the Karush-Kuhn-Tucker conditions (for more details see <ref> [13, 11] </ref>). The *-insensitive cost function is given by C (f (x) y) = jf (x) yj * for jf (x) yj * 0 otherwise (5) (cf.
Reference: 14. <author> V. Vapnik, S. Golowich, A. </author> <month> Smola </month> <year> (1997), </year> <title> Support vector method for function approximation, regression estimation, and signal processing, </title> <publisher> NIPS'96. </publisher>
Reference-contexts: 1 Introduction Support Vector Machines have become a subject of intensive study (see e.g. [3, 13]). They have been applied successfully to classification tasks as OCR [13, 10] and more recently also to regression <ref> [4, 14] </ref>. In this contribution we use Support Vector Machines in the field of time series prediction and we find that they show an excellent performance.
Reference: 15. <author> X. Zhang, J. </author> <title> Hutchinson (1994). Simple architectures on fast machines: practical issues in nonlinear time series prediction in [12]. This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: For benchmark data from the Santa Fe Competition (data set D) we get the best result achieved so far which is 37% better than the winning approach during the competition <ref> [15] </ref> and still 29% better than our previous result [8]. <p> Huber RBF ZH <ref> [15] </ref> PKM [8] full set 0.0639 0.0653 0.0677 0.0665 - segmented set 0.0418 0.0425 0.0569 - 0.0596 Table 1. <p> As in the previous section we use a validation set (50 patterns) to determine the stopping point and (; *) respectively. Table 1 shows that our 25 step iterated prediction of the SVR is 37% better than the one achieved by Zhang et al. <ref> [15] </ref>, who assumed a stationary model. It is still 29% better than our previous result [8] that used the same preprocessing as above and simple RBF nets with non-adaptive centers and variances. <p> Note that a stable prediction is a difficult problem since the noise level applied to the chaotic dynamics was rather high. For the data set D benchmark we ob-tained excellent results for SVR - 37% above the best result achieved during the Santa Fe competition <ref> [15] </ref>. Clearly, this remarkable difference is mostly due to the segmentation used as preprocessing step to get stationary data [8], nevertheless still 29% improvement remain compared to a previous result using the same preprocessing step [8].
References-found: 15

