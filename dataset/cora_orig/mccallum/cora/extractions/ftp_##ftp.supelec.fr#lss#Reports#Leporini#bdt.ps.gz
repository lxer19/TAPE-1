URL: ftp://ftp.supelec.fr/lss/Reports/Leporini/bdt.ps.gz
Refering-URL: http://www.stats.bris.ac.uk/MCMC/pages/list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: OPTIMIZED REPRESENTATIONS OF SIGNALS USING BAYESIAN DECOMPOSITION TREES  
Author: D. Leporini 
Keyword: Wavelet Packets, Malvar's wavelets, Reversible jump Markov chain Monte Carlo methods, Nonlinear estimation, Denoising.  
Note: Technical Report Submitted for Publication  
Address: GDR ISIS, ESE, 91192 Gif-sur-Yvette Cedex, France,  
Affiliation: Laboratoire des Signaux et Systemes, CNRS/UPS and  
Email: E-mail leporini@lss.supelec.fr  
Date: April, 1998  
Abstract: Tree-structured dictionaries of orthonormal bases (wavelet packets/Malvar's wavelets) provide a natural framework to answer the problem of finding a "best representation" of both deterministic and stochastic signals. These classical dictionaries, in turn, may be extended to more general decomposition trees by considering node-varying decomposition operators. In this paper, we reformulate the "best basis" search in these generalized dictionaries as a model selection problem and present a Bayesian approach where the sequence of optimal operators, and subsequently the optimal basis, are considered as model stochastic parameters. Non-homogeneous priors are introduced on the signal transformations and make it compelling to resort to the recently developed reversible jump Markov chain Monte Carlo algorithms. Applications to signal denoising in non-necessarily Gaussian i.i.d. situations are subsequently presented. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> F. Abramovich, T. Sapatinas, and B.W. Silverman. </author> <title> Wavelet Tresholding via a Bayesian Approach. </title> <type> Preprint, </type> <institution> Univ. of Bristol, </institution> <year> 1996. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by <ref> [31, 4, 5, 1, 10] </ref> in the case of wavelet decompositions. In [24], this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> hidden vector Q j;m of independent random variables defining the following conditional densities p Y B j;m [k] (y B j;m [k] j Q j;m [k] = 0) = g (y B j;m [k] j 2 ) j;m ); with P (Q j;m [k] = 1) = " j;m 2 <ref> [0; 1] </ref>. Note however that, as in [10], the simplifying independent assumption may be alleviated by considering a Markov chain/field for the hidden vector. <p> In the former case, an attempt to replace the basis B j;m B fl (n) and the parameter vector fl (n) ( j+1;2m ; j+1;2m+1 ) respectively is made, increasing the dimension of the model by 3 ( j;m 2 <ref> [0; 1] </ref> fi R 2 + ). <p> " j;m ; (1 " j+1;2m ) 2 j;m ; j+1;2m+1 = 2 (1 u 2 )(1 " j;m ) 2 " j+1;2m ~ 2 j;m ; j+1;2m+1 = 2 (1 u 3 )" j;m ~ 2 12 subject to the model constraints (" j+1;2m ; " j+1;2m+1 ) 2 <ref> [0; 1] </ref> 2 , ~ 2 j+1;2m 2 ~ 2 j+1;2m+1 .
Reference: [2] <author> J. B. Buckheit and D. L. </author> <type> Donoho. </type> <institution> Wavelab and Reproducible Research. Stanford University, </institution> <year> 1995. </year>
Reference-contexts: See the Appendix. 10 5.1.3 A simulation example For illustration, consider the classical Blocks signal from the Stanford database <ref> [2] </ref> presented in Fig. 1. A Gaussian i.i.d. noise with variance 2 = 3 was added to the original signal resulting in a SNR of 0:5 dB.
Reference: [3] <author> H.A. Chipman, E.I. George, and R.E. McCulloch. </author> <title> Bayesian CART Model Search. </title> <type> Preprint, </type> <institution> University of Chicago, </institution> <year> 1997. </year>
Reference-contexts: We point out that this Bayesian tree idea was recently proposed in the different context of classification and regression trees (Bayesian CART) <ref> [11, 3] </ref>, where binary trees are used to partition a predictor space. The paper is organized as follows.
Reference: [4] <author> H.A. Chipman, E.D. Kolaczyck, and R.E. McCulloch. </author> <title> Adaptive Bayesian Wavelet Shrinkage. </title> <type> Preprint, </type> <institution> University of Chicago, </institution> <year> 1996. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by <ref> [31, 4, 5, 1, 10] </ref> in the case of wavelet decompositions. In [24], this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> Note also that, interestingly, the optimization of (2) is essentially a minimization of a weighted L 1 -norm. Example 2. A classical model for signal denoising applications is given by the mixture of Gaussian distributions <ref> [24, 4, 5] </ref> p B fl y B fl Y 2 j K Y j;m )+(1" j;m )g (y B j;m [k] j 2 4 where g ( j s 2 ) denotes the Gaussian probability density function with zero mean and variance s 2 . <p> representation of p B j;m (x B j;m j " j;m ; ( 2 2 j K Y h x ) j;m ) : Note that, using the unitary transform property under the Gaussian i.i.d. noise assump-tion, we are back to the well-known mixture of Gaussians model for the observation <ref> [24, 4, 5] </ref> Y j;m ; 2 ) = k=1 (1 " j;m )g (y B j;m [k] j 2 ) + " j;m g (y B j;m [k] j ~ 2 i (5) where 2 denotes the noise variance and ~ 2 j;m = ( 2 x ) j;m +
Reference: [5] <author> M. Clyde, G. Parmigiani, and B. Vidakovic. </author> <title> Multiple Shrinkage and Subset Selection in Wavelets. </title> <type> Preprint, </type> <institution> Duke University, </institution> <year> 1996. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by <ref> [31, 4, 5, 1, 10] </ref> in the case of wavelet decompositions. In [24], this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> Note also that, interestingly, the optimization of (2) is essentially a minimization of a weighted L 1 -norm. Example 2. A classical model for signal denoising applications is given by the mixture of Gaussian distributions <ref> [24, 4, 5] </ref> p B fl y B fl Y 2 j K Y j;m )+(1" j;m )g (y B j;m [k] j 2 4 where g ( j s 2 ) denotes the Gaussian probability density function with zero mean and variance s 2 . <p> representation of p B j;m (x B j;m j " j;m ; ( 2 2 j K Y h x ) j;m ) : Note that, using the unitary transform property under the Gaussian i.i.d. noise assump-tion, we are back to the well-known mixture of Gaussians model for the observation <ref> [24, 4, 5] </ref> Y j;m ; 2 ) = k=1 (1 " j;m )g (y B j;m [k] j 2 ) + " j;m g (y B j;m [k] j ~ 2 i (5) where 2 denotes the noise variance and ~ 2 j;m = ( 2 x ) j;m +
Reference: [6] <author> A. Cohen. </author> <title> Non-stationary multiscale analysis. </title> <editor> In C. K. Chui, L. Montefusco, and L. Puccio, editors, </editor> <title> Wavelets. Theory, algorithms and applications. </title> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1994. </year>
Reference-contexts: The paper is organized as follows. In Section 2, we introduce generalized representation dictionaries called non-stationary decomposition trees in reference to the so-called non 1 stationary wavelet packets <ref> [6] </ref>, and present a Bayesian model for signal representation in Section 3. In Section 4, a reversible jump sampler is proposed to deal with the variable-dimension problem. In Section 5, we substantiate the proposed methodology by way of simulations in various denoising applications including underwater acoustic signals. <p> It is worth noticing that this general framework encompasses, but is not restricted to, possibly non-stationary wavelet packets <ref> [6] </ref>, M - band wavelet packets [28], and local trigonometric bases [21]. Given a particular tree-structured dictionary, the optimization of representation classically involves an additive cost function, and is developed through the well-known BB selection algorithm [9].
Reference: [7] <author> A. Cohen, I. Daubechies, and P. Vial. </author> <title> Wavelets on the interval and fast wavelet algorithms. </title> <journal> Applied and Computational Harmonic Analysis, </journal> <volume> 1 </volume> <pages> 54-81, </pages> <month> Dec. </month> <year> 1993. </year>
Reference-contexts: We point out that this binary tree approach may straightforwardly be extended up to any order M &gt; 2, by considering alternative operator sets of the form S = H (1) ; ; H (M) N ; 1 Discrete decompositions on the interval <ref> [7, 15] </ref> are used in this paper. 2 The symbol bc denotes the greatest integer lower than or equal to its argument. 2 along with the corresponding partitions of identity M X H j;m H j;m = I; (1) (M) 2 S.
Reference: [8] <editor> R. Coifman and D. Donoho. Translation-invariant de-noising. In A. Antoniadis, editor, </editor> <booktitle> Wavelets and statistics. Lecture Notes in Statistics, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Exploring the posterior p (B fl ; fl j y) is alternatively particularly useful to derive a posteriori mean estimates, by averaging signal estimates on distinctive bases. In a sense, this latter approach can be viewed as a generalization of translation-invariant wavelet denoising algorithms <ref> [8, 22, 23] </ref>. <p> Recall first that, due to the decimation operations, the wavelet transform is not shift-invariant. Depending on the position of signal irregularities, its wavelet expansion may vary considerably which represents a major drawback for signal representation and denoising. The classical alternative in denoising applications <ref> [8] </ref>, consists in estimating the underlying signal as the mean of the estimates obtained by considering all possible shiftings of the original noisy signal. <p> The contribution of a particular translation P n , given by 1=K in <ref> [8] </ref>, may be approximated using the generated Markov chain 1 L X P (P n j fl (l) ; y) = L l 0 l=l 0 +1 Y K1 X p B fl (P n 0 ) (y B fl (P n 0 ) j fl (l) ; q fl (l)
Reference: [9] <author> R. R. Coifman and M. V. Wickerhauser. </author> <title> Entropy-based algorithms for best basis selection. </title> <journal> IEEE Trans. Informat. Theory, </journal> <volume> IT-38:713-718, </volume> <month> Mar. </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Particular emphasis has recently been on optimizing multiscale representations of signals embedded in random noise based on statistical approaches <ref> [9, 13, 17] </ref>. The optimization of representation typically takes place over a dictionary of orthonormal bases (wavelet packets/Malvar's wavelets) whose binary tree structure allows a computationally efficient search of the "Best Basis" (BB) [9]. <p> The optimization of representation typically takes place over a dictionary of orthonormal bases (wavelet packets/Malvar's wavelets) whose binary tree structure allows a computationally efficient search of the "Best Basis" (BB) <ref> [9] </ref>. This optimal representation, in turn, is used to nonlinearly estimate (e.g. via thresholding) the underlying (deterministic) signal of interest. We recall that this stochastic framework successfully utilizes the orthonormal-ity property of the decompositions to statistically distinguish the (deterministic) signal components from those of the (stochastic) noise. <p> Given a particular tree-structured dictionary, the optimization of representation classically involves an additive cost function, and is developed through the well-known BB selection algorithm <ref> [9] </ref>. However, without sufficient a priori information on the signal under study and/or due to its possible complexity, optimizing the dictionary itself (via the choice of the decomposition operators) may be of particular interest. In the sequel, we reformulate the BB selection as a model determination problem.
Reference: [10] <author> M.S. Crouse, R.G. Baraniuk, and R.D. Nowak. </author> <title> Signal Estimation Using Wavelet-Markov Models. </title> <booktitle> In Proc. IEEE Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pages 3429-3432, </pages> <year> 1997. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by <ref> [31, 4, 5, 1, 10] </ref> in the case of wavelet decompositions. In [24], this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> Note however that, as in <ref> [10] </ref>, the simplifying independent assumption may be alleviated by considering a Markov chain/field for the hidden vector.
Reference: [11] <author> D. Denison, B. Mallick, and A.F.M. Smith. </author> <title> A Bayesian CART Algorithm. </title> <type> Preprint, </type> <institution> Department of Mathematics, Imperial College, </institution> <address> London, </address> <year> 1996. </year>
Reference-contexts: We point out that this Bayesian tree idea was recently proposed in the different context of classification and regression trees (Bayesian CART) <ref> [11, 3] </ref>, where binary trees are used to partition a predictor space. The paper is organized as follows.
Reference: [12] <author> J. Diebolt and C. P. Robert. </author> <title> Estimation of Finite Mixture Distributions through Bayesian Sampling. </title> <journal> J. Roy. Stat. Soc. B, </journal> <volume> 56 </volume> <pages> 363-375, </pages> <year> 1994. </year>
Reference-contexts: + N j ) Letting = 1 1 X min K P n 0 q fl (l1) ; q fl (l) , we finally conclude that the Markov chain satisfies [27] X jP l (q) P (q j y)j 2 l : Applying the duality principle of Robert and Diebolt <ref> [12] </ref>, we now show that the uniform geometric convergence of the chain is transfered to the continuous state space Markov chain fi To prove this result, first note that p l (q; ) = P (q j ; y) q 0 2f0;1g K Thus, we obtain Z jp l () p
Reference: [13] <author> D. L. Donoho and I. M. Johnstone. </author> <title> Ideal denoising in an orthogonal basis chosen from a library of bases. </title> <journal> C. R. Acad. Sci. Paris, </journal> <volume> 319 </volume> <pages> 1317-1322, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Particular emphasis has recently been on optimizing multiscale representations of signals embedded in random noise based on statistical approaches <ref> [9, 13, 17] </ref>. The optimization of representation typically takes place over a dictionary of orthonormal bases (wavelet packets/Malvar's wavelets) whose binary tree structure allows a computationally efficient search of the "Best Basis" (BB) [9].
Reference: [14] <author> P. J. Green. </author> <title> Reversible jump Markov chain Monte Carlo computation and Bayesian model determination. </title> <journal> Biometrika, </journal> <volume> 82 </volume> <pages> 711-732, </pages> <year> 1995. </year> <month> 26 </month>
Reference-contexts: Non-homogeneous prior distributions, which induce variable-dimension models, are introduced on the decomposition coefficients of the observed process, and make it compelling to resort to reversible jump Markov Chain Monte Carlo (MCMC) methods <ref> [14, 25] </ref>. Depending on the application, this Bayesian framework may be useful to obtain optimal decomposition bases reflecting parsimonious representations, or provide a complete statistical description of the BB upon which inference may be carried out.
Reference: [15] <author> C. Herley and M. Vetterli. </author> <title> Orthogonal time-varying filter banks and wavelet pack-ets. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> 42 </volume> <pages> 2650-2663, </pages> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: We point out that this binary tree approach may straightforwardly be extended up to any order M &gt; 2, by considering alternative operator sets of the form S = H (1) ; ; H (M) N ; 1 Discrete decompositions on the interval <ref> [7, 15] </ref> are used in this paper. 2 The symbol bc denotes the greatest integer lower than or equal to its argument. 2 along with the corresponding partitions of identity M X H j;m H j;m = I; (1) (M) 2 S.
Reference: [16] <author> I.M. Johnstone and B.W. Silverman. </author> <title> Wavelet threshold estimators for data with correlated noise. </title> <type> Technical Report, </type> <institution> Department of Statistics, Stanford University, </institution> <year> 1994. </year>
Reference-contexts: Note that this noise model is implicitly used in wavelet denoising applications with correlated noises <ref> [16] </ref>. Without further information, our prior distribution p (B) will subsequently be uniform over a fixed wavelet packet dictionary, i.e. no particular decomposition basis is privileged. <p> We now focus on non-Gaussian noise environments, and consider a transient doppler signal embedded in an i.i.d. uniform U [0:5; 0:5] noise. Our approach is then compared to the level-dependent wavelet thresholding method developed in <ref> [16] </ref>.
Reference: [17] <author> H. Krim and J.-C. Pesquet. </author> <title> On the statistics of best bases criteria. </title> <editor> In A. Antoniadis, editor, </editor> <booktitle> Wavelets and statistics. Lecture Notes in Statistics, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: 1 Introduction Particular emphasis has recently been on optimizing multiscale representations of signals embedded in random noise based on statistical approaches <ref> [9, 13, 17] </ref>. The optimization of representation typically takes place over a dictionary of orthonormal bases (wavelet packets/Malvar's wavelets) whose binary tree structure allows a computationally efficient search of the "Best Basis" (BB) [9].
Reference: [18] <author> D. </author> <month> Leporini. </month> <institution> Methodes MCMC pour la Decomposition en Paquets d'Ondelettes de Signaux Transitoires. In Seizieme Colloque GRETSI (Grenoble, </institution> <address> FRANCE), </address> <pages> pages 1455-1458, </pages> <year> 1997. </year>
Reference-contexts: For illustration, examples of estimates are presented in Fig. 1. 11 5.2 Denoising in a fixed dictionary We now consider the problem of optimizing the representation of an observed noisy signal given a particular dictionary. Our approach first reported in <ref> [18] </ref> and further extended herein, will focus on wavelet packets decompositions of transient signals embedded in non-necessarily Gaussian and possibly correlated noises. Using the following property [19] Proposition 2.
Reference: [19] <author> D. Leporini and J.-C. Pesquet. </author> <title> High-order Wavelet Packets and Cumulant Field Analysis. </title> <note> submitted to IEEE Tr. on Information Theory, </note> <year> 1998. </year>
Reference-contexts: Our approach first reported in [18] and further extended herein, will focus on wavelet packets decompositions of transient signals embedded in non-necessarily Gaussian and possibly correlated noises. Using the following property <ref> [19] </ref> Proposition 2.
Reference: [20] <author> S. Mallat. </author> <title> A theory for multiresolution signal decomposition: the wavelet representation. </title> <journal> IEEE Trans. Patt. Anal. Mach. Intell., </journal> <volume> PAMI-11:674-693, </volume> <month> Jul. </month> <year> 1989. </year>
Reference-contexts: We recall that double-exponential laws are a special case of exponential power distributions which were proposed by Mallat <ref> [20] </ref> for wavelet coefficient representations of signals and images. Note also that, interestingly, the optimization of (2) is essentially a minimization of a weighted L 1 -norm. Example 2.
Reference: [21] <author> H. Malvar. </author> <title> Lapped transforms for efficient transform subband coding. </title> <journal> IEEE Trans. Acoust., Speech, Signal Processing, </journal> <volume> ASSP-38:969-978, </volume> <month> Jun. 90. </month>
Reference-contexts: It is worth noticing that this general framework encompasses, but is not restricted to, possibly non-stationary wavelet packets [6], M - band wavelet packets [28], and local trigonometric bases <ref> [21] </ref>. Given a particular tree-structured dictionary, the optimization of representation classically involves an additive cost function, and is developed through the well-known BB selection algorithm [9].
Reference: [22] <author> G. P. Nason and B. W. Silverman. </author> <title> The stationary wavelet transform and some statistical applications. </title> <editor> In A. Antoniadis, editor, </editor> <booktitle> Wavelets and statistics. Lecture Notes in Statistics, </booktitle> <publisher> Springer Verlag, </publisher> <year> 1995. </year>
Reference-contexts: Exploring the posterior p (B fl ; fl j y) is alternatively particularly useful to derive a posteriori mean estimates, by averaging signal estimates on distinctive bases. In a sense, this latter approach can be viewed as a generalization of translation-invariant wavelet denoising algorithms <ref> [8, 22, 23] </ref>.
Reference: [23] <author> J.-C. Pesquet, H. Krim, and H. Carfantan. </author> <title> Time invariant orthonormal wavelet representations. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> SP-44:1964-1970, </volume> <month> Aug. </month> <year> 1996. </year>
Reference-contexts: Exploring the posterior p (B fl ; fl j y) is alternatively particularly useful to derive a posteriori mean estimates, by averaging signal estimates on distinctive bases. In a sense, this latter approach can be viewed as a generalization of translation-invariant wavelet denoising algorithms <ref> [8, 22, 23] </ref>.
Reference: [24] <author> J.-C. Pesquet, H. Krim, D. Leporini, and E. Hamman. </author> <title> Bayesian Approach to Best Basis Selection. </title> <booktitle> In Proc. IEEE Conf. Acoust., Speech, Signal Processing, </booktitle> <pages> pages 2634-2638, </pages> <address> Atlanta, USA, </address> <month> May 7-9 </month> <year> 1996. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by [31, 4, 5, 1, 10] in the case of wavelet decompositions. In <ref> [24] </ref>, this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> Note also that, interestingly, the optimization of (2) is essentially a minimization of a weighted L 1 -norm. Example 2. A classical model for signal denoising applications is given by the mixture of Gaussian distributions <ref> [24, 4, 5] </ref> p B fl y B fl Y 2 j K Y j;m )+(1" j;m )g (y B j;m [k] j 2 4 where g ( j s 2 ) denotes the Gaussian probability density function with zero mean and variance s 2 . <p> We however point out that, in denoising applications, such algorithms (along with the classical BB search techniques), only concentrate on the mode of a particular basis function (e.g. the posterior distribution in the previous example, the profile likelihood function in <ref> [24] </ref>, ...) upon which a (nonlinear) estimate of the underlying signal is obtained. Exploring the posterior p (B fl ; fl j y) is alternatively particularly useful to derive a posteriori mean estimates, by averaging signal estimates on distinctive bases. <p> representation of p B j;m (x B j;m j " j;m ; ( 2 2 j K Y h x ) j;m ) : Note that, using the unitary transform property under the Gaussian i.i.d. noise assump-tion, we are back to the well-known mixture of Gaussians model for the observation <ref> [24, 4, 5] </ref> Y j;m ; 2 ) = k=1 (1 " j;m )g (y B j;m [k] j 2 ) + " j;m g (y B j;m [k] j ~ 2 i (5) where 2 denotes the noise variance and ~ 2 j;m = ( 2 x ) j;m + <p> Our approach is then compared to the two possible best basis selection algorithms associated to wavelet packets and Malvar's wavelets introduced in <ref> [24] </ref> using the same statistical model (5). Note however that, in [24], the noise level is assumed to be known. The first process corresponds to a biological signal while the second one is artificial. In both cases, the noise level was set to ensure a Signal-to-Noise Ratio of 0 dB. <p> Our approach is then compared to the two possible best basis selection algorithms associated to wavelet packets and Malvar's wavelets introduced in <ref> [24] </ref> using the same statistical model (5). Note however that, in [24], the noise level is assumed to be known. The first process corresponds to a biological signal while the second one is artificial. In both cases, the noise level was set to ensure a Signal-to-Noise Ratio of 0 dB.
Reference: [25] <author> S. Richardson and P. J. Green. </author> <title> On Bayesian analysis of mixtures with an unknown number of components. </title> <journal> Journal of the Roy. Stat. Soc. B, </journal> <volume> 59 </volume> <pages> 731-792, </pages> <year> 1997. </year>
Reference-contexts: Non-homogeneous prior distributions, which induce variable-dimension models, are introduced on the decomposition coefficients of the observed process, and make it compelling to resort to reversible jump Markov Chain Monte Carlo (MCMC) methods <ref> [14, 25] </ref>. Depending on the application, this Bayesian framework may be useful to obtain optimal decomposition bases reflecting parsimonious representations, or provide a complete statistical description of the BB upon which inference may be carried out. <p> j+1;2m ; j+1;2m+1 ) is given, after some algebra, by J = fi fi fi @( j;m ; u) fi fi fi fi fi fi j;m ~ 2 u 1 (1 u 1 )(1 2u 1 " j;m )[1 2 (1 u 1 )" j;m ] fi fi : Following <ref> [25] </ref>, a proposal for (q j+1;2m ; q j+1;2m+1 ) is obtained using the full conditional distribution given the proposed parameter vector ( j+1;2m ; j+1;2m+1 ).
Reference: [26] <author> C. P. Robert. </author> <title> Mixtures of distributions: inference and estimation. Chapman and Hall, in Practical Markov chain Monte Carlo, </title> <journal> pp. </journal> <pages> 441-464, </pages> <address> London, </address> <year> 1996. </year>
Reference-contexts: Note that identifiability is in this case directly obtained by the variance order imposed by the model assumptions. To implement the Data Augmentation algorithm <ref> [29, 26] </ref>, this model is used in tan dem with a hidden vector Q j;m of independent random variables defining the following conditional densities p Y B j;m [k] (y B j;m [k] j Q j;m [k] = 0) = g (y B j;m [k] j 2 ) j;m ); with
Reference: [27] <author> J. S. Rosenthal. </author> <title> Convergence Rates of Markov Chains. </title> <journal> SIAM Review, </journal> <year> 1995. </year>
Reference-contexts: (a j;m + b j;m + 2N j )(2N j + ~ff j;m )(a j;m + N j )(b j;m + N j ) Letting = 1 1 X min K P n 0 q fl (l1) ; q fl (l) , we finally conclude that the Markov chain satisfies <ref> [27] </ref> X jP l (q) P (q j y)j 2 l : Applying the duality principle of Robert and Diebolt [12], we now show that the uniform geometric convergence of the chain is transfered to the continuous state space Markov chain fi To prove this result, first note that p l
Reference: [28] <author> P. Stephen, P. N. Heller, R. A. Gospinath, and G. B. Burrus. </author> <title> Theory of regular M - band wavelet bases. </title> <journal> IEEE Trans. Signal Processing, </journal> <volume> SP-41:3497-3511, </volume> <month> Dec. </month> <year> 1993. </year> <month> 27 </month>
Reference-contexts: It is worth noticing that this general framework encompasses, but is not restricted to, possibly non-stationary wavelet packets [6], M - band wavelet packets <ref> [28] </ref>, and local trigonometric bases [21]. Given a particular tree-structured dictionary, the optimization of representation classically involves an additive cost function, and is developed through the well-known BB selection algorithm [9].
Reference: [29] <author> M. Tanner and W. Wong. </author> <title> The calculation of posterior distributions. </title> <journal> J.Amer. Statist. Assoc., </journal> <volume> 82 </volume> <pages> 528-550, </pages> <year> 1987. </year>
Reference-contexts: Note that identifiability is in this case directly obtained by the variance order imposed by the model assumptions. To implement the Data Augmentation algorithm <ref> [29, 26] </ref>, this model is used in tan dem with a hidden vector Q j;m of independent random variables defining the following conditional densities p Y B j;m [k] (y B j;m [k] j Q j;m [k] = 0) = g (y B j;m [k] j 2 ) j;m ); with
Reference: [30] <author> L. Tierney. </author> <title> Markov chains for exploring posterior distributions. </title> <journal> Annals of Statistics, </journal> <volume> 22 </volume> <pages> 1701-1762, </pages> <year> 1994. </year>
Reference-contexts: decomposition operators, may also reinforce the mixing property of the generated chain. 4.2 Convergence of distributions The previously exhibited sampler being randomly scanned, a sufficient condition to obtain the desired convergence properties of the Markov chain is that one of the transition kernels in the mixture is irreducible and aperiodic <ref> [30] </ref>. Consider the transition kernel obtained from the mixture of moves (a), (c), and (d). Aperiodicity is clear since in transition (c) (and (d)) there is a positive probability for the chain to reject the proposal and stay in its previous state.
Reference: [31] <author> B. Vidakovic. </author> <title> Nonlinear wavelet shrinkage with Bayes rules and Bayes factors. </title> <type> Internal Report, </type> <institution> Duke University, </institution> <year> 1994. </year>
Reference-contexts: Then, the desired parsimonious representation of the signal in its BB is expressed in terms of a prior distribution on its decomposition coefficients. We should point out that this Bayesian idea was independently used by <ref> [31, 4, 5, 1, 10] </ref> in the case of wavelet decompositions. In [24], this Bayesian approach was further extended to deal with wavelet packet/Malvar's wavelet decompositions using maximum likelihood/maximum generalized likelihood criteria to determine the BB. <p> ~ Be (0:2; 1), 2 ~ IG (3; 1=4V y B (P 0 ) ) and ~ 2 j;m ~ IG (1; 1=2V y B (P 0 ) ), subject to ~ 2 j;m 2 , where V x denotes a robust estimate of the variance of x given by <ref> [31] </ref> V x = median k jx [k]j 0:6745 : Note that the prior distribution for " j;m naturally expresses the desired parsimony of signal representations, while the modes of the variance prior distributions are precisely given by V y B (P 0 ) . classical shift-invariant wavelet denoising algorithm (NMSE
Reference: [32] <author> M. V. Wickerhauser. </author> <title> INRIA lectures on wavelet packet algorithms. </title> <booktitle> In Ondelettes et paquets d'ondelettes, </booktitle> <pages> pages 31-99, </pages> <address> Roquencourt, France, </address> <month> Jun. 17-21 </month> <year> 1991. </year> <month> 28 </month>
Reference-contexts: 1g. 2 An orthonormal basis of R K is subsequently obtained according to B I = [ (j;m)=I j;m I B j;m where I is a partition of [0; 1 [ in intervals I j;m = [2 j m; 2 j (m + 1)[, similarly to the wavelet packet case <ref> [32] </ref>.
References-found: 32

