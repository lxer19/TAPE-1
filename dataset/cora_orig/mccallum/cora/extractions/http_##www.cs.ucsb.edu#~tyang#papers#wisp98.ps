URL: http://www.cs.ucsb.edu/~tyang/papers/wisp98.ps
Refering-URL: http://www.cs.ucsb.edu/Research/rapid_sweb/SWEB.html
Root-URL: http://www.cs.ucsb.edu
Email: dan@cis.ksu.edu  tyang@cs.ucsb.edu  
Title: SWEB++: Partitioning and Scheduling for Adaptive Client-Server Computing on WWW  
Author: Daniel Andresen Tao Yang 
Address: 234 Nichols Hall,  Manhattan, KS 66506  Santa Barbara, CA 93106  
Affiliation: Department of Computing and Information Sciences  Kansas State University  Department of Computer Science University of California  
Abstract: The SWEB++ project studies runtime partitioning, scheduling and load balancing techniques for improving the performance of on-line WWW-based information systems such as digital libraries. The main performance bottlenecks of such a system are caused by the server computing capability and Internet bandwidth. Our observations and solutions are based on our experience with the Alexandria Digital Library (ADL) testbed at UCSB, which provides on-line browsing and processing of documents, digitized maps and other geo-spatially mapped data via WWW. A proper partitioning and scheduling of computation and communication in processing a user request on a multi-processor server and transferring some computation to client-site machines can reduce network traffic and substantially improve system response time. We have proposed a partitioning and scheduling mechanism that adapts to resource changes and optimizes resource utilization. We have developed a software tool which implements and supports the use of our scheduling strategies when programming WWW applications, and have demonstrated the application of this system for on-line DL information browsing. We have conducted a set of experiments to examine the system performance on the Meiko parallel machine and a cluster of workstations.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Andresen, </author> <title> Distributed Scheduling and Software Support for High Performance WWW Applications. </title> <type> PhD Thesis, </type> <institution> University of California at Santa Barbara, </institution> <month> Oct, </month> <year> 1997. </year>
Reference-contexts: The value given to an attribute can also take the form of a C++ function, providing additional flexibility. The details can be found in <ref> [1] </ref>. 3.3 Application integration To integrate an application with SWEB++ after creating a TDL specification and compiling it via the Composer, several steps need to be completed. <p> The overhead for monitoring and scheduling is quite small for all experiments. Analyzing a request takes about 2-4ms, and monitoring takes about 0.1% of CPU resources [4]. A complete set of experiments is reported in <ref> [1] </ref>. The impact of adding multiple servers. We examine how average response times decrease when the number of server nodes (p) increases for a test period of 30 seconds, and at each second R requests are launched from clients (RP S = R). RPS stands for requests per second.
Reference: [2] <author> D. Andresen, L. Carver, R. Dolin, C. Fischer, J. Frew, M. Goodchild, O. Ibarra, R. Kothuri, M. Larsgaard, B. Manjunath, D. Nebert, J. Simpson, T. Smith, T. Yang, and Q. Zheng, </author> <title> "The WWW Prototype of the Alexandria Digital Library", </title> <booktitle> Proc. of ISDL'95: International Symposium on Digital Libraries, </booktitle> <address> Japan, </address> <year> 1995. </year>
Reference-contexts: 1 Motivations The number of digital library (DL) projects is increasing rapidly at both the national and the international levels (see, for example, <ref> [2, 11] </ref>) and they are moving rapidly towards supporting on-line retrieval and processing of major collections of digitized documents over the Internet via the WWW. Performance and scalability issues are especially important for DLs.
Reference: [3] <author> D. Andresen and T. Yang, </author> <title> Multiprocessor Scheduling with Client Resources to Improve the Response Time of WWW Applications, </title> <booktitle> Proceedings of the 11th ACM/SIGARCH Conference on Supercomputing (ICS'97), </booktitle> <address> Vienna, Austria, </address> <month> July, </month> <year> 1997. </year>
Reference-contexts: Section 4 presents experimental results and verifies our analytical results. Section 5 discusses related work and conclusions. An analysis of our scheduling model in a homogeneous environment is reported in <ref> [3] </ref>. 2 WWW request processing We first present a model for WWW request processing, give two applications to demonstrate the use of this model, and then briefly discuss our partitioning and scheduling scheme. 2.1 The model of client-server computing on WWW Our WWW server cluster consists of a set of nodes
Reference: [4] <author> D.Andresen, T.Yang, V.Holmedahl, O.Ibarra, "SWEB: </author> <title> Towards a Scalable World Wide Web Server on Multi-computers", </title> <booktitle> Proc. of 10th IEEE International Symp. on Parallel Processing (IPPS'96), </booktitle> <pages> pp. 850-856. </pages> <month> April, </month> <year> 1996. </year> <month> 11 </month>
Reference-contexts: Our research is motivated by the above situation and develops solutions addressing performance issues of WWW-based applications. In <ref> [4, 5] </ref>, we have studied issues in developing a WWW server cluster dealing with this bottleneck using networked workstations connected with inexpensive disks. As the WWW develops and Web browsers achieve the ability to download executable content (e.g. <p> User requests are first evenly routed to processors via DNS rotation <ref> [4, 12] </ref>. Each server node may have its local disk, which is accessible to other nodes via remote file service in the OS. <p> No requests are allowed to be re-directed more than once, to avoid the ping-pong effect. 1 A file fetch can also be modeled as a chain <ref> [4] </ref>. 4 The predicted cost for processing a request on a node is: t s = t redirection + t data + t server + t net + t client : (1) t redirection is the cost to redirect the request to another processor, if required. t data is the server <p> Clients are located within the campus network to avoid Internet bandwidth fluctuations over multiple experiments. The overhead for monitoring and scheduling is quite small for all experiments. Analyzing a request takes about 2-4ms, and monitoring takes about 0.1% of CPU resources <ref> [4] </ref>. A complete set of experiments is reported in [1]. The impact of adding multiple servers. <p> Addressing client configuration variation is discussed in [10] for filtering multi-media data but it does not consider the use of client resources for integrated computing. Load balancing for Web servers is addressed in <ref> [4, 6] </ref>, the main contributions of our SWEB++ work are adaptive partitioning and scheduling for processing requests by utilizing both client and multiprocessor server resources and a software tool prototype for WWW programmers to incorporate this model in developing their applications.
Reference: [5] <author> D. Andresen, T. Yang, O. Egecioglu, O.H. Ibarra, T.R. Smith, </author> <title> "Scalability Issues for High Performance Digital Libraries on the World Wide Web", </title> <booktitle> Proc. of the 3rd IEEE ADL'96 (Advances in Digital Libraries), </booktitle> <pages> pp. 139-148, </pages> <month> May, </month> <year> 1996. </year>
Reference-contexts: Our research is motivated by the above situation and develops solutions addressing performance issues of WWW-based applications. In <ref> [4, 5] </ref>, we have studied issues in developing a WWW server cluster dealing with this bottleneck using networked workstations connected with inexpensive disks. As the WWW develops and Web browsers achieve the ability to download executable content (e.g. <p> To support these features, the ADL system is using a wavelet-based hierarchical data representation for multi-resolution images <ref> [5, 7] </ref>. Figure 3 depicts a task chain for processing a user request to access an image or subimage at a higher resolution, based on the implementation in [14].
Reference: [6] <author> M. Colajanni, P. Yu, and D. Dias, </author> <title> Scheduling algorithms for distributed Web Servers, </title> <booktitle> Proc. of Inter. Conf on Distributed Computing Systems, </booktitle> <year> 1997. </year> <pages> pp. 169-176. </pages>
Reference-contexts: Addressing client configuration variation is discussed in [10] for filtering multi-media data but it does not consider the use of client resources for integrated computing. Load balancing for Web servers is addressed in <ref> [4, 6] </ref>, the main contributions of our SWEB++ work are adaptive partitioning and scheduling for processing requests by utilizing both client and multiprocessor server resources and a software tool prototype for WWW programmers to incorporate this model in developing their applications.
Reference: [7] <author> E.C.K. Chui, </author> <title> Wavelets: A Tutorial in Theory and Applications, </title> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: To support these features, the ADL system is using a wavelet-based hierarchical data representation for multi-resolution images <ref> [5, 7] </ref>. Figure 3 depicts a task chain for processing a user request to access an image or subimage at a higher resolution, based on the implementation in [14].
Reference: [8] <author> H. Casanova, J. Dongarra, "NetSolve: </author> <title> A network server for solving computation science problems", </title> <booktitle> Proc. of Supercomputing'96, </booktitle> <address> ACM/IEEE, </address> <month> Nov., </month> <year> 1996. </year>
Reference-contexts: The lower curves show the average response time with presence of our scheduler. 10 (a) Postscript text extraction (Meiko). (b) 512 fi 512 wavelet subimage (SUN cluster). sec., 4 RPS. 5 Related work and conclusions Several projects are related to our work. Projects in <ref> [8, 9] </ref> are working on global computing software infrastructures. The above work deals with an integration of different machines as one server and does not have the division of client and server.
Reference: [9] <author> K. Dincer, and G. C. Fox, </author> <title> "Building a world-wide virtual machine based on Web and HPCC technologies", </title> <booktitle> Proc. of Supercomputing'96, </booktitle> <address> ACM/IEEE, </address> <month> Nov., </month> <year> 1996. </year>
Reference-contexts: The lower curves show the average response time with presence of our scheduler. 10 (a) Postscript text extraction (Meiko). (b) 512 fi 512 wavelet subimage (SUN cluster). sec., 4 RPS. 5 Related work and conclusions Several projects are related to our work. Projects in <ref> [8, 9] </ref> are working on global computing software infrastructures. The above work deals with an integration of different machines as one server and does not have the division of client and server.
Reference: [10] <author> A. Fox, E. Brewer, </author> <title> "Reducing WWW Latency and Bandwidth Requirements by Real-Time Distillation", </title> <journal> Computer Networks and ISDN Systems, </journal> <volume> Volume 28, issues 711, </volume> <editor> p. </editor> <volume> 1445. </volume> <month> May, </month> <year> 1996. </year>
Reference-contexts: The extraction of the plain text from a Postscript-formatted document is an application which can benefit strongly from client resources, but requires dynamic scheduling. This application is useful for content replication, information retrieval for non-Postscript enabled browsers, and other tasks <ref> [10] </ref>. pages. The chain has two tasks that can be performed either at server or at client. 1) Select the pages needed. Eliminating unnecessary postscript pages reduces the computation needed for Step 2. <p> Our current project focuses on the optimization between a server and clients and currently uses tightly coupled server nodes for a WWW server, but results could be generalized for loosely coupled server nodes. Addressing client configuration variation is discussed in <ref> [10] </ref> for filtering multi-media data but it does not consider the use of client resources for integrated computing.
Reference: [11] <author> E. Fox, Akscyn, R., Furuta, R. and Leggett, J. </author> <title> (Eds), </title> <journal> Special issue on digital libraries, CACM, </journal> <month> April </month> <year> 1995. </year>
Reference-contexts: 1 Motivations The number of digital library (DL) projects is increasing rapidly at both the national and the international levels (see, for example, <ref> [2, 11] </ref>) and they are moving rapidly towards supporting on-line retrieval and processing of major collections of digitized documents over the Internet via the WWW. Performance and scalability issues are especially important for DLs.
Reference: [12] <author> E.D. Katz, M. Butler, R. McGrath, </author> <title> A Scalable HTTP Server: the NCSA Prototype, </title> <journal> Computer Networks and ISDN Systems. </journal> <volume> vol. 27, </volume> <year> 1994, </year> <pages> pp. 155-164. </pages>
Reference-contexts: User requests are first evenly routed to processors via DNS rotation <ref> [4, 12] </ref>. Each server node may have its local disk, which is accessible to other nodes via remote file service in the OS. <p> Load balancing with "hot spots". "Hot spots" is a typical problem with DNS rotation, where a single server exhibits a higher load than its peers. Various authors have noted that DNS rotation seems to inevitably lead to load imbalances <ref> [12, 13] </ref>. We examine how our system deals with hot-spots by sending a fixed number of requests to a subset of nodes in our server cluster, giving a wide range of load disparities. Without our scheduler, the selected nodes would have to process all of those requests.
Reference: [13] <author> D. Mosedale, W. Foss, R. McCool, </author> <title> "Administering Very High Volume Internet Services", </title> <booktitle> 1995 LISA IX, </booktitle> <address> Mon-terey, CA, </address> <year> 1995. </year>
Reference-contexts: Load balancing with "hot spots". "Hot spots" is a typical problem with DNS rotation, where a single server exhibits a higher load than its peers. Various authors have noted that DNS rotation seems to inevitably lead to load imbalances <ref> [12, 13] </ref>. We examine how our system deals with hot-spots by sending a fixed number of requests to a subset of nodes in our server cluster, giving a wide range of load disparities. Without our scheduler, the selected nodes would have to process all of those requests.
Reference: [14] <author> A. Poulakidas, A. Srinivasan, O. Egecioglu, O. Ibarra, and T. Yang, </author> <title> "A Compact Storage Scheme for Fast Wavelet-based Subregion Retrieval", </title> <booktitle> in Proc. of 1997 International Computing and Combinatorics Conference (COCOON), </booktitle> <address> Shanghai, China, </address> <month> August, </month> <year> 1997. </year>
Reference-contexts: To support these features, the ADL system is using a wavelet-based hierarchical data representation for multi-resolution images [5, 7]. Figure 3 depicts a task chain for processing a user request to access an image or subimage at a higher resolution, based on the implementation in <ref> [14] </ref>.
Reference: [15] <author> S. Sekiguchi, "Ninf, </author> <title> A network based information library for global world-wide computing infrastructure", </title> <note> http://hpc.etl.go.jp/NinfDemo.html, 1996. 12 </note>
Reference-contexts: TDL is similar to the Durra language proposed in <ref> [15] </ref>, with several extensions to meet our needs. The syntax is straightforward, and many simple tasks can be defined within the language itself. For complex tasks, TDL allows a user to embed a complete C++ function where necessary.
References-found: 15

