URL: http://www.cs.columbia.edu/~mauricio/papers/MPThesis.ps.gz
Refering-URL: http://www.cs.columbia.edu/~mauricio/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: A Generalization of Band Joins and The Merge/Purge Problem  
Author: Mauricio Antonio Hernandez-Sherrington 
Degree: Submitted in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the Graduate School of Arts and Sciences.  
Date: 1996  
Affiliation: Columbia University  
Abstract-found: 0
Intro-found: 1
Reference: [ ACM, 1991 ] <editor> ACM. </editor> <booktitle> SIGMOD record, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: Introduction Merging large databases acquired from different sources with heterogenous representations of information has become an increasingly difficult problem for many organizations. Instances of this problem appearing in the literature have been called the semantic integration problem <ref> [ ACM, 1991 ] </ref> or the instance identification problem [ Wang and Madnick, 1989 ] . A recent NSF Workshop reports this problem as data scrubbing and recognizes it as an open and important problem to be solved [ Silberschatz et al., 1995 ] . <p> A more loosely-coupled collection of databases for which no DBMS provides consistent schemas among them are called Multidatabase Systems (MDBS) [ Elmagarmid and Pu, 1990 ] . Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem <ref> [ ACM, 1991 ] </ref> . In that issue, [ Kent, 1991 ] explains how many assumptions in centralized database systems cannot be taken for granted when using a multidatabase system. Some of his examples are: 1.
Reference: [ Agrawal and Jagadish, 1988 ] <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The more corrupted the data, more runs might be needed to capture the matching records. The transitive closure, however, is executed on pairs of tuple id's, each at most 30 bits, and fast solutions to compute transitive closure exist <ref> [ Agrawal and Jagadish, 1988 ] </ref> . From observing real world scenarios, the size of the data set over which the closure is computed is at least one order of magnitude smaller than the corresponding database of records, and thus does not contribute a large cost.
Reference: [ Anderson and Matessa, 1990 ] <author> J. Anderson and M. Matessa. </author> <title> A Rational Analysis of Categorization. </title> <booktitle> In Machine Learning: Proceedings of the Seventh Int'l Conference, </booktitle> <pages> pages 76-84, </pages> <year> 1990. </year>
Reference-contexts: Some recent examples of this approach include the two Bayesian classifiers in [ Cheeseman et al., 1988 ] and <ref> [ Anderson and Matessa, 1990 ] </ref> . The output of these classifications algorithms can be thought as a set of selection predicates that divide the data into disjoint clusters. We can generalize some of these clustering approaches using the following model.
Reference: [ Batini et al., 1986 ] <author> C. Batini, M. Lenzerini, and S. Navathe. </author> <title> A Comparative Analysis of Methodologies for Database Schema Integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The first issue, where relations have different schema, has been addressed extensively in the literature and is known as the schema integration problem <ref> [ Batini et al., 1986 ] </ref> . This problem is outside the scope of this thesis and is not discussed further. We are primarily interested in the second problem: heterogeneous and errorful representations of data about the same entity and its implication when merging or joining relations.
Reference: [ Bickel, 1987 ] <author> M. A. Bickel. </author> <title> Automatic Correction to Misspelled Names: a Fourth-generation Language Approach. </title> <journal> Communications of the ACM, </journal> <volume> 30(3) </volume> <pages> 224-228, </pages> <year> 1987. </year>
Reference-contexts: Since we only have a corpus for the names of the cities in the U.S.A. (18670 different names), we attempted correcting only the spelling of the city field. We chose the algorithm described by Bickel in <ref> [ Bickel, 1987 ] </ref> for its simplicity and speed. Alternatively, for the address fields, we could have tried the address normalization algorithms described in refWong94.
Reference: [ Bitton and DeWitt, 1983 ] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is <ref> [ Bitton and DeWitt, 1983 ] </ref> . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ Gotlieb, 1975 ] in which the join condition is a user-defined equivalence function. <p> Lower bounds for sorting multisets where studied by [ Munro and Spira, 1976 ] . They showed that the multiplicities (i.e., duplicates) of a set can only be obtained 15 by comparisons if the total order is discovered in the process. Later <ref> [ Bitton and DeWitt, 1983 ] </ref> studied the problem of duplicate elimination in the context of large data files. They first present the "traditional" algorithm for duplicate elimination consisting of a complete sort of the file followed by a scan to remove duplicates. <p> The main differences between Bitton and DeWitt's approach and the approach we propose here are the use of a "window of records" to limit the number of possible duplicates we will consider, and the use of a user-specified, knowledge-based, equality theory to determine if tuples are indeed "duplicates". <ref> [ Bitton and DeWitt, 1983 ] </ref> then modify the traditional approach to allow duplicate elimination during different stages of the sorting procedure. Through some cost-analysis and models, they show the modified duplicate elimination algorithm to be superior than the "traditional" approach. <p> Our experimental results, presented in chapter 4, demonstrate this to be the case. The sorted-neighborhood method resembles a merge-sort in which we are interested in removing duplicates. Two approaches for duplicate elimination using merge-sort were described in <ref> [ Bitton and DeWitt, 1983 ] </ref> . The first approach, called the "traditional" approach (which we will call the "naive" approach), is the following. First, the file is sorted using an external merge-sort algorithm. Then duplicate records are removed in one sequential scan of the sorted database.
Reference: [ Bratbergsengen, 1984 ] <author> K. Bratbergsengen. </author> <title> Hashing Methods and Relational Algebra Operators. </title> <booktitle> In Proceedings of the 1984 VLDB Conference, </booktitle> <month> August </month> <year> 1984. </year> <pages> 127 128 </pages>
Reference-contexts: As a result of this effort, a number of algorithms to perform the costly Join operation have been proposed (see [ Mishra and Eich, 1992 ] ). Currently, three basic algorithms dominate commercial database implementations: nested-loop-joins (the "naive" algorithm), sort-merge-joins, and hash-joins <ref> [ Bratbergsengen, 1984 ] </ref> , with many variants of each. More recently, attention has shifted towards the efficient parallel execution of S-P-J queries (e.g., [ Schneider and DeWitt, 1989 ] ).
Reference: [ Brinkhoff et al., 1994 ] <author> T. Brinkhoff, H. Kriegel, R. Schneider, and S. Bernhard. </author> <title> Multi-Step Processing of Spatial Joins. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 197-208, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: The function F could be a simple arithmetic predicate or a complex inference procedure defined over a mixture of domains of the chosen attributes. There are several application problems in which the generalization of band-joins is used. One example is intersection spatial-joins <ref> [ Brinkhoff et al., 1994 ] </ref> , where a possible set of spatial objects are first identified and then a more complex geometric filter is applied to determine which objects satisfy the spatial-join predicate.
Reference: [ Cheeseman et al., 1988 ] <author> P. Cheeseman, J. Kelly, M. Self, J. Sutz, W. Taylor, and D. Freeman. </author> <title> AUTOCLASS: A Bayesian Classification System. </title> <booktitle> In Proceedings of the Fifth Int'l Conference on Machine Learning, </booktitle> <pages> pages 54-64, </pages> <year> 1988. </year>
Reference-contexts: Some recent examples of this approach include the two Bayesian classifiers in <ref> [ Cheeseman et al., 1988 ] </ref> and [ Anderson and Matessa, 1990 ] . The output of these classifications algorithms can be thought as a set of selection predicates that divide the data into disjoint clusters. We can generalize some of these clustering approaches using the following model.
Reference: [ Church and Gale, 1991 ] <author> K. W. Church and W. A. Gale. </author> <title> Probability Scoring for Spelling Correction. </title> <journal> Statistics and Computing, </journal> <volume> 1 </volume> <pages> 93-103, </pages> <year> 1991. </year>
Reference: [ Clark, 1995 ] <author> T. K. Clark. </author> <title> Analyzing Foster Childrens' Foster Home Payments Database. </title> <note> In KDD Nuggets 95:7 (http://info.gte.com/ ~ kdd/nuggets/95/), </note> <editor> Piatetsky-Shapiro, ed., </editor> <year> 1995. </year>
Reference-contexts: In March of 1995, we started collaborating with DSHS's Office of Children Administrative Research (OCAR) after one of their computer consultant requested information about tools for discovering individuals and patterns in OCAR's databases <ref> [ Clark, 1995 ] </ref> . For the next eight months we worked on the development and deployment of a merge/purge application for OCAR. In return, OCAR allowed us the use of a sample of their data for our studies to develop an approximate equational-theory. <p> In March of 1995 Timothy Clark, Computer Information Consultant for the Office of Children Administrative Research (OCAR) of the Department of Social and Health Services, posted a request on the KDD-nuggets <ref> [ Clark, 1995 ] </ref> asking for assistance to clean one of their databases (i.e., they needed to merge/purge their data). A copy of Clark's request can be found in appendix D. We answered their request and this chapter details our results.
Reference: [ Clark, 1996a ] <author> T. K. Clark. </author> <title> Personal communication with Mauricio A. </title> <type> Hernandez. </type> <month> January 18 </month> <year> 1996. </year>
Reference-contexts: They are confident that the error rate does not exceed what has been reported here and that it may prove to be significantly less as they work with the data further <ref> [ Clark, 1996a ] </ref> . For example, it is now believed that our system is accurate at least 97% of the time when clusters do not contain ghost records [ Clark, 1996b ] .
Reference: [ Clark, 1996b ] <author> T. K. Clark. </author> <title> Personal communication with Mauricio A. </title> <type> Hernandez. </type> <month> January 17 </month> <year> 1996. </year>
Reference-contexts: For example, it is now believed that our system is accurate at least 97% of the time when clusters do not contain ghost records <ref> [ Clark, 1996b ] </ref> . This is a good improvement from the accuracy of their previous system which was estimated at no more than 90%. The increased accuracy is enabling OCAR to perform better analysis of the data.
Reference: [ Codd, 1970 ] <author> E. Codd. </author> <title> A Relational Model for Large Shared Data Banks. </title> <journal> Communications of the ACM, </journal> <volume> 13(6), </volume> <month> June </month> <year> 1970. </year>
Reference-contexts: Take for example the standard definition of a relation under Relational Databases <ref> [ Codd, 1970 ] </ref> . A relation is defined as a set of tuples.
Reference: [ Copeland et al., 1988 ] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data Placement in Bubba. </title> <booktitle> In Proceedings of the 1988 ACM-SIGMOD Conference, </booktitle> <pages> pages 99-108, </pages> <year> 1988. </year>
Reference: [ Davidson, 1993 ] <author> C. Davidson. </author> <title> What your database hides away. </title> <journal> New Scientist, </journal> (1855):28-31, 1993. 
Reference-contexts: The fundamental problem in merge/purge is that the data supplied by various sources typically include identifiers or string data, that are either different among different datasets or simply erroneous due to a variety of reasons (including typographical or transcription errors, or purposeful fraudulent activity (aliases) in the case of names <ref> [ Davidson, 1993 ] </ref> ). Hence, the equality of two values over the domain of the common join attribute is not specified as a "simple" arithmetic predicate, but rather by a set of equational axioms that define equivalence, i.e., by an equational theory. <p> The task of the "transitive-closure" phase is to detect these connected-components from the graph and assign its nodes (the records) a unique cluster identification number. This combination of results have been identified in the popular press as an important step in discovering patterns in the data <ref> [ Davidson, 1993 ] </ref> . Recently, [ Goldberg and Senator, 1995 ] have called Consolidation to the procedure that assigns a unique ID to all records or transactions identified as belonging to the same real-world entity.
Reference: [ Dewan et al., 1994a ] <author> H. M. Dewan, M. A. Hernandez, J. Hwang, and S. Stolfo. </author> <title> Predictive Dynamic Load Balancing of Parallel and Distributed Rule and Query Processing. </title> <booktitle> In Proceedings of the 1994 ACM SIGMOD Conference, </booktitle> <year> 1994. </year> <month> 129 </month>
Reference-contexts: The total completion time of a parallel execution using 18 processors will be, however, determined by that slowest completion time. Techniques for the dynamic load-balance of parallel operations are described in <ref> [ Dewan et al., 1994a ] </ref> and the lessons learned by our own parallel load-balancing of joins [ Dewan et al., 1994b ] could be fruitfully applied to solve this problem.
Reference: [ Dewan et al., 1994b ] <author> H. M. Dewan, M. A. Hernandez, K. Mok, and S. Stolfo. </author> <title> Predictive Load Balancing of Parallel Hash-Joins over Heterogeneous Processors in the Presence of Data Skew. </title> <booktitle> In Proc. 3rd Int'l Conf. on Parallel and Distributed Information Systems, </booktitle> <pages> pages 40-49, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: All previous mentioned works assume a homogeneous set of processing sites. Recent work by ourselves <ref> [ Dewan et al., 1994b ] </ref> and by others [ Lu and Tan, 1994 ] have addressed these problems in the context of load balancing protocols to deal with possible heterogeneity among processing sites. <p> The total completion time of a parallel execution using 18 processors will be, however, determined by that slowest completion time. Techniques for the dynamic load-balance of parallel operations are described in [ Dewan et al., 1994a ] and the lessons learned by our own parallel load-balancing of joins <ref> [ Dewan et al., 1994b ] </ref> could be fruitfully applied to solve this problem. The dynamic load balancing of the window-scan procedure is, however, beyond the scope of this dissertation work. 5.2 Single and Multi-pass clustering method The parallel implementation of the clustering method works as follows. <p> It then redistributes the clusters among processors using a longest processing time first [ Graham, 1969 ] strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained. In <ref> [ Dewan et al., 1994b ] </ref> we detailed the load balancing algorithm in the context of parallel database joins and how it can deal with skewed data distributions in a heterogenous processing environment. The time results for the clustering method are depicted in Figure 5.3 (b).
Reference: [ DeWitt et al., 1991 ] <author> D. J. DeWitt, J. F. Naughton, and D. A. Schneider. </author> <title> An Evaluation of Non-Equijoin Algorithms. </title> <booktitle> In Proc. 17th Int'l. Conf. on Very Large Databases, </booktitle> <pages> pages 443-452, </pages> <address> Barcelona, Spain, </address> <year> 1991. </year>
Reference-contexts: When these "errors" in the data are not severe, we might ideally expect to find the matching instance of a tuple in R within a "band" of tuples in S. This type of non-equijoin joins are called band-joins and have been studied by <ref> [ DeWitt et al., 1991 ] </ref> . <p> Finally, the proposed solution to the merge/purge problem resembles a sort-merge join [ Gotlieb, 1975 ] in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by <ref> [ DeWitt et al., 1991 ] </ref> . <p> Thus, we can define as (S:temp5 R:temp and R:temp S:temp+5). Algorithms for executing this kind of non-equijoin predicate have been presented by <ref> [ DeWitt et al., 1991 ] </ref> . [ DeWitt et al., 1991 ] call joins in which the join-predicate has the form R:A c 1 S:B R:A + c 2 , band-joins. Their paper presents a new algorithm termed a partitioned band join to evaluate these special type of joins. <p> Thus, we can define as (S:temp5 R:temp and R:temp S:temp+5). Algorithms for executing this kind of non-equijoin predicate have been presented by <ref> [ DeWitt et al., 1991 ] </ref> . [ DeWitt et al., 1991 ] call joins in which the join-predicate has the form R:A c 1 S:B R:A + c 2 , band-joins. Their paper presents a new algorithm termed a partitioned band join to evaluate these special type of joins. <p> It is assumed that the range of the "band" will never exceed the range of values in the window of pages of S i . A drawback of the algorithms presented in <ref> [ DeWitt et al., 1991 ] </ref> is the need to sort at least one of the relations before initiating the join. <p> At this point, the only difference from this algorithm to other hash-join algorithms is to determine which two buckets of S will participate in the join with each record in R i . This is easily accomplished using the scrolling window of pages algorithm presented in <ref> [ DeWitt et al., 1991 ] </ref> . Analytical results in [ Soloviev, 1993 ] show that the truncated hash-join will outperform the partitioned version of band-joins under a number of situations (e.g., size of buffers, band size).
Reference: [ DeWitt et al., 1992 ] <author> D. J. DeWitt, J. F. Naughton, D. A. Schneider, and S. Se-shadri. </author> <title> Practical Skew Handling in Parallel Joins. </title> <booktitle> In Proceedings of the 18th VLDB Conference, </booktitle> <pages> pages 27-39, </pages> <year> 1992. </year>
Reference: [ Dietterich and Michalski, 1983 ] <author> T. Dietterich and R. Michalski. </author> <title> A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 41-81. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1983. </year>
Reference-contexts: In this strategy, the N latest elements are selected as "prime-representatives". * Generalization: Generate the "prime-representatives" by generalizing the data collected from several "positive examples" (records) of the concept represented by the cluster. Techniques for generalizing concepts are well known from machine learning <ref> [ Dietterich and Michalski, 1983; Lebowitz, 1986 ] </ref> . * Syntactic: Choose the "largest" or more complete record. * Utility: Choose the record that matched others more frequently.
Reference: [ Dubes and Jain, 1976 ] <author> R. Dubes and A. Jain. </author> <title> Clustering Techniques: The User's Dilema. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 247-260, </pages> <year> 1976. </year>
Reference-contexts: Discovery algorithms can start their identification process 34 by classifying records into classes (or clusters) that reflect patterns inherent in the data 3 . The creation of clusters can involve traditional cluster analysis methods <ref> [ Dubes and Jain, 1976 ] </ref> or more recent conceptual clustering methods which, as the former, use attribute similarity to form clusters, but also take into consideration background knowledge, such as knowledge about likely cluster shapes. <p> From the pattern recognition community, we can think of these "prime-representatives" as analogous to the "cluster cen-troids" <ref> [ Dubes and Jain, 1976 ] </ref> generally used to represent clusters of information, or as the base element of an equivalence class. 114 Definitions: R 0 : The initial relation. i : The i-th increment relation. c i : A relation of only "prime representatives" of the clusters identified by the
Reference: [ Dunn, 1946 ] <author> H. </author> <title> Dunn. </title> <journal> American Journal of Public Health, </journal> <month> December </month> <year> 1946. </year>
Reference-contexts: Although we cannot be sure where and when the first instance of this problem appears, we can probably guess it had something to do with the analysis of census data or medical records <ref> [ Dunn, 1946; Marshall, 1947 ] </ref> . The first instance for which we have evidence appeared in Science in 1959 [ Newcombe et al., 1959 ] . This paper describes the problem of automatically linking records from separate hospital databases.
Reference: [ Elmagarmid and Pu, 1990 ] <author> A. Elmagarmid and C. Pu. </author> <title> Guest Editors' Introduction to the Special Issue on Heterogeneous Databases. </title> <journal> Computing Surveys, </journal> <volume> 22(3) </volume> <pages> 175-178, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: A more loosely-coupled collection of databases for which no DBMS provides consistent schemas among them are called Multidatabase Systems (MDBS) <ref> [ Elmagarmid and Pu, 1990 ] </ref> . Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem [ ACM, 1991 ] .
Reference: [ Fellegi and Sunter, 1969 ] <author> I. Fellegi and A. Sunter. </author> <title> A Theory for Record Linkage. </title> <journal> American Statistical Association Journal, </journal> <pages> pages 1183-1210, </pages> <month> December </month> <year> 1969. </year>
Reference-contexts: A formal theory for record linkage was presented ten years later in <ref> [ Fellegi and Sunter, 1969 ] </ref> . Every subsequent work on record-linkage or merge/purge is affected by their work. Their paper provides a theorical framework for a computer-oriented solution to the problem of recognizing "similar" records. They describe the problem of record linkage as follows. <p> In other words, create matching rules that are acceptably accurate and that minimizes the possible-link category whose links will probably need further processing or human intervention to resolve. Finally <ref> [ Fellegi and Sunter, 1969 ] </ref> realized that comparing every member in A fi B is impractical for any large j A j and j B j and propose the use of a random sample from A and B to find the optimal matching criteria as above. <p> They also hint of the use of some blocking mechanism during later processing to avoid the comparison of all pairs in A fi B. In this thesis, we use a fixed-size window after sorting as our blocking mechanism. Much theoretical and mathematical work has followed <ref> [ Fellegi and Sunter, 1969 ] </ref> 's (see [ Winkler, 1995 ] for a recent survey). In this thesis we will not expand this theoretical framework but instead just note our solutions to the merge/purge problem (in particular, the "merge" portion of the problem) as an instance of record-linkage. <p> First, there is the theoretical and probabilistic analysis necessary for each of the presented algorithms using the work by <ref> [ Fellegi and Sunter, 1969 ] </ref> as a base. Much of the work that has followed [ Fellegi and Sunter, 1969 ] has concentrated on the automatic calculation of weights and thresholds for matching fields of records. <p> First, there is the theoretical and probabilistic analysis necessary for each of the presented algorithms using the work by <ref> [ Fellegi and Sunter, 1969 ] </ref> as a base. Much of the work that has followed [ Fellegi and Sunter, 1969 ] has concentrated on the automatic calculation of weights and thresholds for matching fields of records. These matchings, however, are done by weighted-count of all relevant fields, not by a rule-based approach.
Reference: [ Forgy, 1981 ] <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year>
Reference-contexts: The results displayed in chapter 4 are based upon edit distance computation since the outcome of the program did not vary much among the different distance functions for the particular databases used in our study. For the purpose of experimental study, we wrote an OPS5 <ref> [ Forgy, 1981 ] </ref> rule program consisting of 26 rules for this particular domain of employee records and it was tested repeatedly over relatively small databases of records.
Reference: [ Frawley et al., 1992 ] <author> W. Frawley, G. Piatetsky, and C. Matheus. </author> <title> Knowledge Discovery in Databases: An Overview. </title> <journal> AI Magazine, </journal> <pages> pages 57-70, </pages> <month> Fall </month> <year> 1992. </year> <month> 130 </month>
Reference-contexts: Hash partitioning uses a hash function to divide the input into P partitions or buckets. This partitioning scheme could be useful in cases where the input relation has a key attribute which is known not to be noisy. Discovery algorithms have been described by <ref> [ Frawley et al., 1992 ] </ref> as procedures to extract knowledge from data. Two processes are involved in these procedures: interesting patterns must be identified, and a meaningful description of each pattern must be provided.
Reference: [ Geurts, 1994 ] <author> M. Geurts. </author> <title> Data Problems in Decision Support Systems. </title> <booktitle> In Proceedings of the Twenty-Seventh Annual Hawaii International Conference on Systems Sciences, </booktitle> <year> 1994. </year>
Reference-contexts: Sequential as well as parallel implementations of the techniques have been implemented and comparatively evaluated for efficiency and accuracy. Solutions for this problem had been pointed out as critical for data to be used for Decision Support Systems <ref> [ Geurts, 1994 ] </ref> and Knowledge Discovery [ Piatetsky-Shapiro and Matheus, 1992 ] .
Reference: [ Ghandeharizadeh, 1990 ] <author> S. Ghandeharizadeh. </author> <title> Physical Database Design in Multiprocessor Database Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin - Madison, </institution> <year> 1990. </year>
Reference-contexts: Clustering data as described above raises the issue of how well partitioned the data is after clustering. We use an approach that closely resembles the multidimensional partitioning strategy of <ref> [ Ghandeharizadeh, 1990 ] </ref> . If the data from which the n-attribute key is extracted is distributed uniformly over its domain, then we can expect all clusters to have approximately the same number of records in them. <p> In general, this technique can be applied to several attributes of a relation and is known as multidimensional partition strategy (e.g., <ref> [ Ghandeharizadeh, 1990 ] </ref> ). There are, however, other partitioning strategies we might want to provide in a generic merge/purge facility we plan to implement, namely, constant partitioning, uniform partitioning, hash partitioning, and classification.
Reference: [ Goldberg and Senator, 1995 ] <author> H. G. Goldberg and T. E. Senator. </author> <title> Restructuring Databases for Knowledge Discovery by Consolidation and Link Formation. </title> <booktitle> In Proceedings of the KDD-95 Conference, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: This combination of results have been identified in the popular press as an important step in discovering patterns in the data [ Davidson, 1993 ] . Recently, <ref> [ Goldberg and Senator, 1995 ] </ref> have called Consolidation to the procedure that assigns a unique ID to all records or transactions identified as belonging to the same real-world entity.
Reference: [ Gotlieb, 1975 ] <author> L. Gotlieb. </author> <title> Computing Joins of Relations. </title> <booktitle> In Proceedings of the 1975 ACM SIGMOD Conference, </booktitle> <year> 1975. </year>
Reference-contexts: We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ Bitton and DeWitt, 1983 ] . Finally, the proposed solution to the merge/purge problem resembles a sort-merge join <ref> [ Gotlieb, 1975 ] </ref> in which the join condition is a user-defined equivalence function. Of particular relevance to the merge/purge solution proposed here is the work on "band-joins" by [ DeWitt et al., 1991 ] .
Reference: [ Graham, 1969 ] <author> R. Graham. </author> <title> Bounds on multiprocessing timing anomalies. </title> <journal> SIAM Journal of Computing, </journal> <volume> 17 </volume> <pages> 416-429, </pages> <year> 1969. </year>
Reference-contexts: The coordinator processor keeps track of how many records it sent to each processor (and cluster) and therefore it knows, at the end of the clustering stage, how balanced the partition is. It then redistributes the clusters among processors using a longest processing time first <ref> [ Graham, 1969 ] </ref> strategy. That is, move the largest job in an overloaded processor to the most underloaded processor, and repeat until a "well" balanced load is obtained.
Reference: [ Grudin, 1983 ] <author> J. T. Grudin. </author> <title> Error Patterns in Novice and Skilled Transcription Typing. </title> <editor> In W. E. Cooper, editor, </editor> <booktitle> Cognitive Aspects of Skilled Typewriting, </booktitle> <pages> pages 121-142. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1983. </year>
Reference: [ Harrison and Rubin, 1978 ] <author> M. C. Harrison and N. Rubin. </author> <title> Another generalization of resolution. </title> <journal> Journal of the ACM, </journal> <volume> 25(3), </volume> <month> July </month> <year> 1978. </year>
Reference-contexts: The use of equational theories or knowledge-intensive matching procedures is not a new idea. The first reference to this idea comes from the context of theorem proving by resolution in <ref> [ Harrison and Rubin, 1978 ] </ref> . In that paper, Harrison and Rubin generalize the usual unification procedure allowing the specification of "equality predicates". In a more recent example, [ Tsur, 1991 ] introduced this idea in the context of deductive databases.
Reference: [ Hernandez and Stolfo, 1995a ] <author> M. Hernandez and S. Stolfo. </author> <title> A Generalization of Band-Joins and the Merge/Purge Problem. </title> <journal> Submitted for review to IEEE's Transactions on Knowledge and Data Engineering, </journal> <month> November </month> <year> 1995. </year>
Reference-contexts: Most of the results we present in this thesis have been published in the literature [ Hernandez and Stolfo, 1995b ] and have recently been submitted for archival publication in <ref> [ Hernandez and Stolfo, 1995a ] </ref> . 8 Chapter 2 Previous Work Several lines of work have an impact on efficient solutions for the merge/purge problem.
Reference: [ Hernandez and Stolfo, 1995b ] <author> M. Hernandez and S. Stolfo. </author> <title> The Merge/Purge Problem for Large Databases. </title> <booktitle> In Proceedings of the 1995 ACM-SIGMOD Conference, </booktitle> <month> May </month> <year> 1995. </year> <month> 131 </month>
Reference-contexts: We have also contributed three useful optimizations to the basic merge/purge procedure, including an incremental merge/purge procedure, all fully implemented in a general and widely useful system. Most of the results we present in this thesis have been published in the literature <ref> [ Hernandez and Stolfo, 1995b ] </ref> and have recently been submitted for archival publication in [ Hernandez and Stolfo, 1995a ] . 8 Chapter 2 Previous Work Several lines of work have an impact on efficient solutions for the merge/purge problem.
Reference: [ Hernandez, 1995 ] <author> M. A. Hernandez. </author> <title> A Generalization of Band-Joins and the Merge/Purge Problem. </title> <type> Technical Report CUCS-005-1995, </type> <institution> Department of Computer Science, Columbia University, </institution> <month> February </month> <year> 1995. </year>
Reference-contexts: The generated data simulates a mailing list and the task of the merge/purge application is to remove duplicates from the generated list. Chapter 5 explores parallel processing of these solutions and briefly explores load balancing issues. In our initial presentation of this work <ref> [ Hernandez, 1995 ] </ref> we proposed the implementation of two other merge/purge applications to test our techniques over different domains. One application involved the implementation of intersection spatial-joins while the other involved the re-implementation of the engine for the ALEXSYS expert system.
Reference: [ Hua and Lee, 1990 ] <author> K. A. Hua and C. Lee. </author> <title> An adaptive data placement scheme for parallel database computer systems. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 493-506, </pages> <year> 1990. </year>
Reference-contexts: have addressed various problems related to the parallel execution of S-P-J queries like, data partition among participating processors [ Copeland et al., 1988; Kitsuregawa and Ogawa, 1990; Ghandeharizadeh, 1990 ] , skew handling [ Schneider and DeWitt, 1990; Wolf et al., 1991; DeWitt et al., 1992 ] , and load-balancing <ref> [ Hua and Lee, 1990 ] </ref> . All previous mentioned works assume a homogeneous set of processing sites.
Reference: [ Kent, 1991 ] <author> W. Kent. </author> <title> The Breakdown of the Information Model in Multi-Database Systems. </title> <booktitle> SIGMOD Record, </booktitle> <volume> 20(4) </volume> <pages> 10-15, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The semantic integration problem <ref> [ Kent, 1991 ] </ref> seeks to identify a multiplicity of database objects that represent the same or related real-world entity, even though their database representations are different. This problem has been studied by the heterogenous multi-database community. <p> Semantic Heterogeneity has been recognized as a difficult problem in MDBS. Recently, ACM SIGMOD dedicated a special issue to this problem [ ACM, 1991 ] . In that issue, <ref> [ Kent, 1991 ] </ref> explains how many assumptions in centralized database systems cannot be taken for granted when using a multidatabase system. Some of his examples are: 1. <p> Of particular interest for us in this thesis is the identity and naming problem. This problem has been also called the inter-database identification problem by [ Wang and Madnick, 1989 ] . <ref> [ Kent, 1991 ] </ref> proposes the use of spheres of knowledge to address this problem. Spheres of knowledge create views of the underlying multidatabases to integrate data from diverse sources and attempt to provide a consistent view of that data to the end-user.
Reference: [ Kitsuregawa and Ogawa, 1990 ] <author> M. Kitsuregawa and Y. Ogawa. </author> <title> Bucket Spreading Parallel Hash: A New, Robust, Parallel Hash Join Method for Data Skew in the Super Database Computer (SDC). </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 210-221, </pages> <year> 1990. </year>
Reference: [ Knuth, 1973 ] <author> D. Knuth. </author> <title> The Art of Computer Programming: Sorting and Searching (Volume 3). </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Sorting data is probably the most studied problem in Computer Science and many different algorithms have been presented over the years <ref> [ Knuth, 1973 ] </ref> . We are however only interested in sorting algorithms in which duplicates are removed from the final sorted database. To date, the most relevant work in this area is [ Bitton and DeWitt, 1983 ] . <p> For example, names like Stolfo and Stolpho are spelled differently but are phonetically equivalent. Some use the Soundex coding <ref> [ Knuth, 1973 ] </ref> to avoid this problem by converting every name into a code consisting of it first letter and three digits that encode the phonetics of the name.
Reference: [ Kreinovich, 1995 ] <author> V. Kreinovich. </author> <title> Strongly Transitive Fuzzy Relations: An Alternative Way to Describe Similarity. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 10 </volume> <pages> 1061-1076, </pages> <year> 1995. </year>
Reference-contexts: The use of the transitive closure to merge "similar" pieces of information can also be found in the study of fuzzy sets <ref> [ Kreinovich, 1995 ] </ref> . Let R (a; b) denote "a is identical to b", where a and b are any two objects.
Reference: [ Kukich, 1992 ] <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: Since misspellings are introduced by the database generator, we explored the possibility of improving the results by running a spelling correction program over some fields. Spelling correction algorithms have received a large amount of attention for decades <ref> [ Peterson, 1980; Kukich, 1992 ] </ref> . Most of the spelling correction algorithms we considered use a corpus of correctly spelled words from which the correct spelling is selected.
Reference: [ Lebowitz, 1986 ] <author> M. Lebowitz. </author> <title> Not the Path to Perdition: The Utility of Similarity-Based Learning. </title> <booktitle> In Proceedings of 5th National Conference on Artificial Intelligence, </booktitle> <pages> pages 533-537, </pages> <year> 1986. </year>
Reference-contexts: In this strategy, the N latest elements are selected as "prime-representatives". * Generalization: Generate the "prime-representatives" by generalizing the data collected from several "positive examples" (records) of the concept represented by the cluster. Techniques for generalizing concepts are well known from machine learning <ref> [ Dietterich and Michalski, 1983; Lebowitz, 1986 ] </ref> . * Syntactic: Choose the "largest" or more complete record. * Utility: Choose the record that matched others more frequently.
Reference: [ Lu and Tan, 1994 ] <author> H. Lu and K. Tan. </author> <title> Load-Balanced Join Processing in Shared-Nothing Systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 23(3) </volume> <pages> 382-398, </pages> <month> December </month> <year> 1994. </year> <month> 132 </month>
Reference-contexts: All previous mentioned works assume a homogeneous set of processing sites. Recent work by ourselves [ Dewan et al., 1994b ] and by others <ref> [ Lu and Tan, 1994 ] </ref> have addressed these problems in the context of load balancing protocols to deal with possible heterogeneity among processing sites. However, almost all work in the parallel execution of S-P-J queries has concentrated on hash-join algorithms, and only one type of join query, namely, equijoins.
Reference: [ Major and Riedinger, 1992 ] <author> J. Major and D. Riedinger. EFD: </author> <title> A Hybrid Knowledge/Statistical-Based System for the Detection of Fraud. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7 </volume> <pages> 687-703, </pages> <year> 1992. </year>
Reference: [ Marshall, 1947 ] <author> J. Marshall. </author> <title> Population Studies, </title> <month> 1 </month> <year> 1947. </year>
Reference-contexts: Although we cannot be sure where and when the first instance of this problem appears, we can probably guess it had something to do with the analysis of census data or medical records <ref> [ Dunn, 1946; Marshall, 1947 ] </ref> . The first instance for which we have evidence appeared in Science in 1959 [ Newcombe et al., 1959 ] . This paper describes the problem of automatically linking records from separate hospital databases.
Reference: [ Miranker et al., 1990 ] <author> D. P. Miranker, B. Lofaso, G. Farmer, A. Chandra, and D. </author> <title> Brant. On a TREAT-based Production System Compiler. </title> <booktitle> In Proc. 10th Int'l Conf. on Expert Systems, </booktitle> <pages> pages 617-630, </pages> <year> 1990. </year>
Reference-contexts: Another compiler, the OPS5C compiler <ref> [ Miranker et al., 1990 ] </ref> was not available to us in time for these studies. The OPS5C compiler produces code that is reportedly many times faster than previous compilers.
Reference: [ Mishra and Eich, 1992 ] <author> P. Mishra and M. Eich. </author> <title> Join Processing in Relational Databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(1) </volume> <pages> 63-113, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The fast execution of complex select-project-join (S-P-J) queries has received considerable attention from many researchers over the past 20 years. As a result of this effort, a number of algorithms to perform the costly Join operation have been proposed (see <ref> [ Mishra and Eich, 1992 ] </ref> ). Currently, three basic algorithms dominate commercial database implementations: nested-loop-joins (the "naive" algorithm), sort-merge-joins, and hash-joins [ Bratbergsengen, 1984 ] , with many variants of each.
Reference: [ Monge and Elkan, 1996 ] <author> A. Monge and C. Elkan. </author> <title> Integrating external information sources to guide worldwide web information retrieval. </title> <note> Submitted for review to AI-96, </note> <year> 1996. </year>
Reference-contexts: Finally, we would like to see our Data Cleaner become part of larger commercial database systems. With the growing need for Data Mining and Decision Support Systems over large databases (and distributed information sources like the one found in the WWW <ref> [ Monge and Elkan, 1996 ] </ref> ), the need for having Merge/Purge facilities as part of a commercial database system distribution is also a desirable goal. We believe we have made a substantial step towards achieving this goal.
Reference: [ Munro and Spira, 1976 ] <author> I. Munro and P. Spira. </author> <title> Sorting and Searching in Multi-sets. </title> <journal> SIAM Journal of Computing, </journal> <volume> 5(1) </volume> <pages> 1-8, </pages> <month> March </month> <year> 1976. </year>
Reference-contexts: Moreover, in the classic definition of the project relational operator, duplicates in the projected relation are expected to be removed from the resulting relation. Lower bounds for sorting multisets where studied by <ref> [ Munro and Spira, 1976 ] </ref> . They showed that the multiplicities (i.e., duplicates) of a set can only be obtained 15 by comparisons if the total order is discovered in the process.
Reference: [ Newcombe et al., 1959 ] <author> H. Newcombe, J. Kennedy, S. Axford, and A. James. </author> <title> Automatic Linkage of Vital Records. </title> <journal> Science, </journal> <volume> 130 </volume> <pages> 954-959, </pages> <year> 1959. </year>
Reference-contexts: The first instance for which we have evidence appeared in Science in 1959 <ref> [ Newcombe et al., 1959 ] </ref> . This paper describes the problem of automatically linking records from separate hospital databases. <p> However, most of the techniques explored in the paper were limited by the available technology (they were achieving one comparison every 3 seconds!). <ref> [ Newcombe et al., 1959 ] </ref> concludes that their performance and accuracy will greatly improve as new technology is available and predicts that theory and data-processing techniques for record linkage will be developed before the necessary equipment is available. <p> For this study, the generator selected from 10% to 50% of the generated records for duplication with errors. 47 4.2 Pre-processing the generated database Pre-processing and conditioning the records in the database prior to the merge/purge operation might increase the chance of finding two duplicate records <ref> [ Newcombe et al., 1959; Pu, 1991 ] </ref> . For example, names like Stolfo and Stolpho are spelled differently but are phonetically equivalent.
Reference: [ Nyberg et al., 1994 ] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 233-242, </pages> <year> 1994. </year>
Reference-contexts: functions to deal with the NOT_EQUAL case. - end; of case - i := (i + 1) mod window_size; end; -of while - base := (base + 1) mod window_size; end of for - a second pass, likely more, for a high speed sort like, for example, the Alpha Sort <ref> [ Nyberg et al., 1994 ] </ref> , and a final pass for window processing and application of the rule program for each record entering the sliding window. Depending upon the complexity of the rule program, the last pass may indeed be the dominant cost. <p> Depending upon the complexity of the rule program, the last pass may indeed be the dominant cost. Later we consider the means of improving this phase by processing "parallel windows" in the sorted list. We note with interest that the sorts of optimizations detailed in the AlphaSort paper <ref> [ Nyberg et al., 1994 ] </ref> may of course be fruitfully applied here. In this thesis 27 we are more concerned with alternative process architectures that lead to higher accuracies in the computed results while also reducing time complexity. <p> Thus, in this thesis we consider alternative metrics for the purposes of merge/purge to include how accurately can you merge/purge for a fixed dollar and given time constraint, rather than the specific cost- and time-based metrics proposed in <ref> [ Nyberg et al., 1994 ] </ref> . 3.1.2 The Duplicate Elimination Sorted-Neighborhood Method Similar to the "naive" algorithm, given a collection of two or more databases, we first concatenate them into one sequential list of N records.
Reference: [ Peterson, 1980 ] <author> J. L. Peterson. </author> <title> Computer Programs for Detecting and Correcting Spelling Errors. </title> <journal> Communications of the ACM, </journal> <volume> 23(12) </volume> <pages> 676-687, </pages> <year> 1980. </year>
Reference-contexts: Since misspellings are introduced by the database generator, we explored the possibility of improving the results by running a spelling correction program over some fields. Spelling correction algorithms have received a large amount of attention for decades <ref> [ Peterson, 1980; Kukich, 1992 ] </ref> . Most of the spelling correction algorithms we considered use a corpus of correctly spelled words from which the correct spelling is selected.
Reference: [ Piatetsky-Shapiro and Matheus, 1992 ] <author> G. Piatetsky-Shapiro and C. Matheus. </author> <title> Knowledge Discovery Workbench for Exploring Business Databases. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 7 </volume> <pages> 675-686, </pages> <year> 1992. </year> <month> 133 </month>
Reference-contexts: Sequential as well as parallel implementations of the techniques have been implemented and comparatively evaluated for efficiency and accuracy. Solutions for this problem had been pointed out as critical for data to be used for Decision Support Systems [ Geurts, 1994 ] and Knowledge Discovery <ref> [ Piatetsky-Shapiro and Matheus, 1992 ] </ref> .
Reference: [ Pollock and Zamora, 1987 ] <author> J. J. Pollock and A. Zamora. </author> <title> Automatic spelling correction in scientific and scholarly text. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(4) </volume> <pages> 358-368, </pages> <year> 1987. </year>
Reference: [ Pu, 1991 ] <author> C. Pu. </author> <title> Key Equivalence in Heterogenous Databases. </title> <booktitle> In Proceedings of the First InternationFirts Internatinal Workshop on Interoperability in Multi-database Systems, </booktitle> <pages> pages 314-316, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: For this study, the generator selected from 10% to 50% of the generated records for duplication with errors. 47 4.2 Pre-processing the generated database Pre-processing and conditioning the records in the database prior to the merge/purge operation might increase the chance of finding two duplicate records <ref> [ Newcombe et al., 1959; Pu, 1991 ] </ref> . For example, names like Stolfo and Stolpho are spelled differently but are phonetically equivalent.
Reference: [ Schneider and DeWitt, 1989 ] <author> D. A. Schneider and D. J. DeWitt. </author> <title> A Performance Evaluation of Four Parallel join Algorithm in a Shared-Nothing Multiprocessor Environment. </title> <booktitle> In Proceedings of the 1989 ACM-SIGMOD Conference, </booktitle> <pages> pages 110-121, </pages> <year> 1989. </year>
Reference-contexts: Currently, three basic algorithms dominate commercial database implementations: nested-loop-joins (the "naive" algorithm), sort-merge-joins, and hash-joins [ Bratbergsengen, 1984 ] , with many variants of each. More recently, attention has shifted towards the efficient parallel execution of S-P-J queries (e.g., <ref> [ Schneider and DeWitt, 1989 ] </ref> ).
Reference: [ Schneider and DeWitt, 1990 ] <author> D. Schneider and D. J. DeWitt. </author> <title> Tradeoffs in Processing Complex Join Queries via Hashing in Multiprocessor Database Machines. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 469-480, </pages> <year> 1990. </year>
Reference: [ Senator et al., 1995 ] <author> T. Senator, H. Goldberg, J. Wooton, A. Cottini, A. Umar, C. Klinger, W. Llamas, M. Marrone, and R. Wong. </author> <title> The FinCEN Artificial Intelligence System: Identifying Potential Money Laundering from Reports of Large Cash Transactions. </title> <booktitle> In Proceedings of the 7th Conference on Innovative Applications of AI, </booktitle> <month> August </month> <year> 1995. </year>
Reference: [ Sheth and Larson, 1990 ] <author> A. Sheth and J. Larson. </author> <title> Federated Database Systems for Managing Distributed, Heterogeneous, and Autonomous Databases. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(3), </volume> <month> September </month> <year> 1990. </year>
Reference-contexts: Those systems that attempt to provide an unambiguous schema for a collection of heterogeneous databases are known as Federated database systems <ref> [ Sheth and Larson, 1990 ] </ref> or Heterogeneous database systems. A more loosely-coupled collection of databases for which no DBMS provides consistent schemas among them are called Multidatabase Systems (MDBS) [ Elmagarmid and Pu, 1990 ] . Semantic Heterogeneity has been recognized as a difficult problem in MDBS.
Reference: [ Silberschatz et al., 1995 ] <author> A. Silberschatz, M. Stonebraker, and J. Ullman. </author> <title> Database Research: Achievements and Opportunities into the 21st Century. Report of an NSF Workshop on the Future of Database Systems, </title> <month> May </month> <year> 1995. </year> <note> http://db.stanford.edu/pub/ullman/1995/lagii.ps. 134 </note>
Reference-contexts: A recent NSF Workshop reports this problem as data scrubbing and recognizes it as an open and important problem to be solved <ref> [ Silberschatz et al., 1995 ] </ref> . In this thesis we consider the problem over very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. For instance, one month is a typical business cycle in certain direct marketing operations.
Reference: [ Soloviev, 1993 ] <author> V. Soloviev. </author> <title> A Truncated Hash Algorithm for Processing Band-Join Queries. </title> <booktitle> In Proceedings of the Nineth International Conference on Data Engineering, </booktitle> <year> 1993. </year>
Reference-contexts: A drawback of the algorithms presented in [ DeWitt et al., 1991 ] is the need to sort at least one of the relations before initiating the join. Recently, <ref> [ Soloviev, 1993 ] </ref> presented an algorithm to overcome this problem. [ Soloviev, 1993 ] presents algorithms based on the idea of truncating the join attribute values in order to execute band joins in a way similar to hash join algorithms. <p> A drawback of the algorithms presented in [ DeWitt et al., 1991 ] is the need to sort at least one of the relations before initiating the join. Recently, <ref> [ Soloviev, 1993 ] </ref> presented an algorithm to overcome this problem. [ Soloviev, 1993 ] presents algorithms based on the idea of truncating the join attribute values in order to execute band joins in a way similar to hash join algorithms. <p> This is easily accomplished using the scrolling window of pages algorithm presented in [ DeWitt et al., 1991 ] . Analytical results in <ref> [ Soloviev, 1993 ] </ref> show that the truncated hash-join will outperform the partitioned version of band-joins under a number of situations (e.g., size of buffers, band size). The initial solution we propose for the merge/purge problem uses a similar idea behind the partitioned band join to identify "matching" tuples.
Reference: [ Stolfo et al., 1990 ] <author> S. Stolfo, L. Woodbury, J. Glazier, and P. Chan. </author> <title> The ALEXSYS Mortgage Pool Allocation Expert System: A Case Study of Speeding up Rule-based Systems. </title> <booktitle> In AI and Business Workshop, AAAI-90, </booktitle> <year> 1990. </year>
Reference-contexts: One example is intersection spatial-joins [ Brinkhoff et al., 1994 ] , where a possible set of spatial objects are first identified and then a more complex geometric filter is applied to determine which objects satisfy the spatial-join predicate. Another example is the strategy used in ALEXSYS <ref> [ Stolfo et al., 1990 ] </ref> , an expert system for allocating mortgage pools, where pools that can be successfully allocated can be found "close" to one another if an initial "order" is given to the data.
Reference: [ Thomas et al., 1990 ] <author> G. Thomas, G. Thompson, C. Chung, E. Barkmeyer, F. Carter, M. Templeton, S. Fox, and B. Hartman. </author> <title> Heterogenous Distributed Database Systems for Production Use. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(3) </volume> <pages> 237-266, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: But with the creation of multiple and possibly heterogenous DDBS managing a diversity of information sources, the problem of providing consistent access to data managed by different DBMS has become increasingly difficult <ref> [ Thomas et al., 1990 ] </ref> . Those systems that attempt to provide an unambiguous schema for a collection of heterogeneous databases are known as Federated database systems [ Sheth and Larson, 1990 ] or Heterogeneous database systems.
Reference: [ Tsur, 1991 ] <author> S. Tsur. </author> <title> PODS invited talk: Deductive databases in action. </title> <booktitle> In Proc. of the 1991 ACM-PODS: Symposium on the Principles of Database Systems, </booktitle> <year> 1991. </year>
Reference-contexts: The first reference to this idea comes from the context of theorem proving by resolution in [ Harrison and Rubin, 1978 ] . In that paper, Harrison and Rubin generalize the usual unification procedure allowing the specification of "equality predicates". In a more recent example, <ref> [ Tsur, 1991 ] </ref> introduced this idea in the context of deductive databases. In particular, he points out that in Scientific Databases the data accumulated can contain imprecisions.
Reference: [ Wang and Madnick, 1989 ] <author> Y. R. Wang and S. E. Madnick. </author> <title> The Inter-Database Instance Identification Problem in Integrating Autonomous Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1989. </year>
Reference-contexts: Introduction Merging large databases acquired from different sources with heterogenous representations of information has become an increasingly difficult problem for many organizations. Instances of this problem appearing in the literature have been called the semantic integration problem [ ACM, 1991 ] or the instance identification problem <ref> [ Wang and Madnick, 1989 ] </ref> . A recent NSF Workshop reports this problem as data scrubbing and recognizes it as an open and important problem to be solved [ Silberschatz et al., 1995 ] . <p> On mul-tidatabase systems, because of data entry errors or heterogeneous represen tations, different strings might be returned for the same query. Of particular interest for us in this thesis is the identity and naming problem. This problem has been also called the inter-database identification problem by <ref> [ Wang and Madnick, 1989 ] </ref> . [ Kent, 1991 ] proposes the use of spheres of knowledge to address this problem. Spheres of knowledge create views of the underlying multidatabases to integrate data from diverse sources and attempt to provide a consistent view of that data to the end-user. <p> The database administrator (DBA) supplies the meta-data necessary to integrate conflicting instances into the view, but sometimes the system can only notify the user of a discrepancy when not enough knowledge or information is available for the integration to occur automatically. The work of <ref> [ Wang and Madnick, 1989 ] </ref> also proposes a solution to the problem. Their principal contribution is the idea of letting the user write a set of knowledge-based rules that define when two instances from different databases represent the same entity object. <p> The rules are also used to infer new information about separate instances. In this thesis, we take the same approach: we let the user define an equational theory, which will be represented in a rule-based language, to identify instances from several databases that are deemed equivalent. <ref> [ Wang and Madnick, 1989 ] </ref> apply the user-defined rules to the all the input databases to find the desired instances.
Reference: [ Winkler, 1995 ] <author> W. Winkler. </author> <title> Matching and Record Linkage. </title> <editor> In Brenda G. Cox, editor, </editor> <title> Business survey methods. </title> <publisher> Wiley, </publisher> <year> 1995. </year>
Reference-contexts: In this thesis, we use a fixed-size window after sorting as our blocking mechanism. Much theoretical and mathematical work has followed [ Fellegi and Sunter, 1969 ] 's (see <ref> [ Winkler, 1995 ] </ref> for a recent survey). In this thesis we will not expand this theoretical framework but instead just note our solutions to the merge/purge problem (in particular, the "merge" portion of the problem) as an instance of record-linkage.
Reference: [ Wolf et al., 1991 ] <author> J. L. Wolf, Dias. D. M., P. S. Yu, and J. Turek. </author> <title> Comparative Performance of Parallel Join Algorithms. </title> <booktitle> In Proceedings of the 7th Int'l Conference on Data Engineering, </booktitle> <pages> pages 78-88, </pages> <year> 1991. </year> <month> 135 </month>
References-found: 69

