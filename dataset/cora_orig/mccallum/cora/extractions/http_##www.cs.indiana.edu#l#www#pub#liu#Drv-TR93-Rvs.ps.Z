URL: http://www.cs.indiana.edu/l/www/pub/liu/Drv-TR93-Rvs.ps.Z
Refering-URL: http://www.cs.indiana.edu/l/www/pub/liu/
Root-URL: http://www.cs.indiana.edu
Email: fyanhong, ttg@cs.cornell.edu  
Title: Deriving Incremental Programs  
Author: Yanhong A. Liu Tim Teitelbaum 
Date: October 1993  
Address: Ithaca, NY 14853  
Affiliation: Department of Computer Science, Cornell University,  
Abstract: A systematic stepwise transformational approach is given for deriving incremental programs from non-incremental programs. We exploit partial evaluation, other static analysis and transformation techniques, and domain-specific knowledge in order to provide a degree of incremen-tality not otherwise achievable by a generic incremental evaluator. The generality of our model of incremental computation is discussed and compared with related work. Illustrative examples using the transformational approach are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. Addison-Wesley series in Computer Science. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1986. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers <ref> [1, 7, 8] </ref>, transformational programming [21, 37], interactive editing systems [30, 3], etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs. <p> Hence all or part of the parameter x may have become irrelevant to the final computation. There has been considerable work on such optimizations <ref> [1] </ref>. Standard data-flow and control-flow analysis algorithms can be adapted to reason about the dependencies within the programs and recognize the redundant code to be eliminated.
Reference: [2] <author> B. Alpern, R. Hoover, B. Rosen, P. Sweeney, and K. Zadeck. </author> <title> Incremental evaluation of computational circuits. </title> <booktitle> In Proceedings of the First Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 32-42, </pages> <address> San Francisco, California, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation <ref> [2] </ref>, incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [3] <author> R. A. Ballance, S. L. Graham, and M. L. V. D. Vanter. </author> <title> The Pan language-based editing system. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 95-127, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers [1, 7, 8], transformational programming [21, 37], interactive editing systems <ref> [30, 3] </ref>, etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs. In this paper, we present a technique for formally deriving an incremental program f 0 from a non-incremental program f and input change .
Reference: [4] <editor> B. Bjtrner, A. P. Ershov, and N. D. Jones, editors. </editor> <title> Partial Evaluation and Mixed Computation. </title> <publisher> North-Holland, </publisher> <year> 1988. </year> <booktitle> Proceedings of the IFIP TC2 Workshop on Partial Evaluation and Mixed Computation, </booktitle> <address> Gammel Averns, Denmark, </address> <month> October </month> <year> 1987. </year>
Reference: [5] <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> Journal of the ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: In order to explain the basic ideas further, we break the derivation approach into four steps. Step 1. Expanding the computation of f (x y) to separate out the computations on x from the rest of computation. This is achieved by unfolding <ref> [5] </ref> and simplifying function applications whose arguments involve both x and y. Step 2. Introducing the cached result r of the computation f (x). In this step, we define f fl (x; y; r) to be the expanded computation of f (x y) obtained from Step 1.
Reference: [6] <author> J. Cai and R. Paige. </author> <title> Program derivation by fixed point computation. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 11 </volume> <pages> 197-261, </pages> <month> September </month> <year> 1988/89. </year>
Reference-contexts: In the third step, a subroutine called auxiliary generalized partial evaluation is employed to help discover incrementalities. We also cast our derivation approach in terms of a generalized partial evaluation point of view. Paige's work on finite differencing is the pioneering work on deriving incremental programs <ref> [24, 20, 36, 6] </ref>. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS [37, 38]. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows. <p> They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive incremental programs. Such techniques are indispensable as part of an optimizing compiler for programs written in very high-level languages like SETL or APL <ref> [23, 6] </ref>. The APTS [22] program transformation system has been developed for such purposes. Our technique differs from that work in that it applies to programs written in a standard language like Lisp.
Reference: [7] <author> J. Cocke and K. Kennedy. </author> <title> An algorithm for reduction of operator strength. </title> <journal> Communications of the ACM, </journal> 20(11) 850-856, November 1977. <volume> 37 </volume>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers <ref> [1, 7, 8] </ref>, transformational programming [21, 37], interactive editing systems [30, 3], etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig [20, 24]. Their work generalizes Cocke's method of strength reduction <ref> [7] </ref> and provides a convenient framework in which to implement a host of program transformations including Earley's "iterator inversion"[8]. They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive incremental programs.
Reference: [8] <author> J. Earley. </author> <title> High level iterators and a method for automatically designing data structure repre-sentation. </title> <journal> Journal of Computer Languages, </journal> <volume> 1 </volume> <pages> 321-342, </pages> <year> 1976. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers <ref> [1, 7, 8] </ref>, transformational programming [21, 37], interactive editing systems [30, 3], etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs.
Reference: [9] <author> D. Eppstein, Z. Galil, G. F. Italiano, and A. Nissenzweig. </author> <title> Sparsification a technique for speeding up dynamic graph algorithms. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on FOCS, </booktitle> <address> Pittsburgh, Pennsylvania, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: The degree to which it is possible to generate the auxiliary information automatically is an open question. Corresponding to finding auxiliary information, many dynamic algorithms use specially designed data structures to get speedup. Although there is no universally applicable data structure, some apply to a broad class of problems <ref> [9] </ref>. Accommodating such general data structures in our model for deriving incremental programs might help in deciding whether a data structure is applicable to a certain problem.
Reference: [10] <author> J. Field and T. Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the ACM '90 Conference on LISP and Functional Programming, </booktitle> <pages> pages 307-322, </pages> <year> 1990. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms for particular problems, general evaluation frameworks are pursued for achieving incremental computation automatically, e.g., incremental computation via function caching [27], formal program manipulations like traditional partial evaluation <ref> [41, 40, 10] </ref>, and incremental computation as a program abstraction [16], etc. Note that, when attribute grammars are used as a general description language for applications, the attribute evaluation framework can be viewed as falling into this class too.
Reference: [11] <author> G. N. Frederickson. </author> <title> Data structures for on-line updating of minimum spanning trees, with applications. </title> <journal> SIAM Journal on Computing, </journal> <volume> 14(4) </volume> <pages> 781-798, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., <ref> [46, 29, 11] </ref>, can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [12] <author> B. N. Freeman-Benson, J. Maloney, and A. Borning. </author> <title> An incremental constraint solver. </title> <journal> Communications of the ACM, </journal> <volume> 33(1) </volume> <pages> 54-63, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving <ref> [43, 12] </ref>, etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [13] <author> Y. Futamura and K. Nogi. </author> <title> Generalized partial evaluation. </title> <booktitle> In Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 133-151. </pages> <publisher> North-Holland, </publisher> <year> 1988. </year> <note> In [4]. </note>
Reference-contexts: GPE is a semantic-based program transformation: given an expression e and an information set I that describes partial knowledge about e, GPE produces a residual expression e 0 by specializing e using I. The notion of GPE has appeared in <ref> [13] </ref> [42], where it is called generalized partial computation (GPC). It was originally proposed to overcome the drawbacks of conventional partial evaluation, which only specializes programs given static values for certain arguments. <p> For the example above, the specialization of mtxM ul (C; R) in the context where null (C) is false is an aGPE that yields the right hand side of (8) as an auxiliary residual expression. In <ref> [13] </ref> and [42], GPE transformation methods for a restricted form of language (u-form) and a lazy functional language are discussed, and small examples are given. <p> For any information set I, let e I be a Boolean-valued expression in our language that represents the information of the set I. A logic is called an underlying logic <ref> [13] </ref> for our language if it is compatible with the semantics of our language, where compatibility is defined as follows: Definition 5.1 A logic L is compatible with the semantics E of our language if and only if, for any two information sets I and I 0 such that the equations
Reference: [14] <author> C. Ghezzi and D. Mandrioli. </author> <title> Incremental parsing. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 58-70, </pages> <month> July </month> <year> 1979. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. Examples are incremental parsing <ref> [14, 17] </ref>, incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [15] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <address> Baltimore and London, </address> <note> second edition edition, </note> <year> 1989. </year>
Reference-contexts: Let x be hC; Ri, where C and R are two lists of numbers. The function mtxM ul (C; R) treats C as a column vector, R as a row vector, and returns their outer product <ref> [15] </ref>. The resulting matrix is represented as a list of rows, where each row is a list of 4 numbers. The auxiliary function rowM ul (e; R) multiplies an element e with each element in a row R and returns the row of products.
Reference: [16] <author> R. Hoover. Alphonse: </author> <title> Incremental computation as a programming abstraction. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on PLDI, </booktitle> <pages> pages 261-272, </pages> <address> California, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms for particular problems, general evaluation frameworks are pursued for achieving incremental computation automatically, e.g., incremental computation via function caching [27], formal program manipulations like traditional partial evaluation [41, 40, 10], and incremental computation as a program abstraction <ref> [16] </ref>, etc. Note that, when attribute grammars are used as a general description language for applications, the attribute evaluation framework can be viewed as falling into this class too.
Reference: [17] <author> F. Jalili and J. H. Gallier. </author> <title> Building friendly parsers. </title> <booktitle> In Conference Record of the 9th Annual ACM Symposium on POPL, </booktitle> <pages> pages 196-206, </pages> <address> Albuquerque, New Mexico, </address> <month> January </month> <year> 1982. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. Examples are incremental parsing <ref> [14, 17] </ref>, incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [18] <author> P. Lipps, U. Moncke, M. Olk, and R. Wilhelm. </author> <title> Attribute (re)evaluation in OPTRAN. </title> <journal> Acta Informatica, </journal> <volume> 26 </volume> <pages> 213-239, </pages> <year> 1988. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. Examples are incremental parsing [14, 17], incremental attribute evaluation <ref> [31, 45, 18] </ref>, incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [19] <author> Y. Liu. </author> <title> Theoretical results on deriving incremental programs. </title> <booktitle> Unpublished notes, </booktitle> <year> 1992. </year>
Reference-contexts: In other words, f 0 (x; ffix; r) computes f (x ffix) but uses only the change parameter ffix and the cached result r. In the sort example, sort 0 is a complete incremental version of sort under . For a formal definition of incremental programs, see <ref> [19] </ref>. When no confusion arises, we simply write f 0 for f 0 . For typographical convenience, we shall typically use y for the change parameter rather than ffix. Also, throughout the paper, r always refers to the cached result of a previous computation f (x). In [19], it is proved <p> incremental programs, see <ref> [19] </ref>. When no confusion arises, we simply write f 0 for f 0 . For typographical convenience, we shall typically use y for the change parameter rather than ffix. Also, throughout the paper, r always refers to the cached result of a previous computation f (x). In [19], it is proved that in general, given a program f and an input change operation , whether an incremental version exists is undecidable. Accordingly, we compromise by seeking an approximation to an incremental version, which we call a differentiated version.
Reference: [20] <author> R. Paige. </author> <title> Formal Differentiation: A Program Synthesis Technique, </title> <booktitle> volume 7 of Computer Science. Artificial Intelligence. </booktitle> <publisher> UMI Research Press, </publisher> <address> Ann Arbor, Michigan, </address> <year> 1981. </year> <title> Revision of Ph.D. </title> <address> Thesis New York University, </address> <year> 1979. </year> <month> 38 </month>
Reference-contexts: In the third step, a subroutine called auxiliary generalized partial evaluation is employed to help discover incrementalities. We also cast our derivation approach in terms of a generalized partial evaluation point of view. Paige's work on finite differencing is the pioneering work on deriving incremental programs <ref> [24, 20, 36, 6] </ref>. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS [37, 38]. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig <ref> [20, 24] </ref>. Their work generalizes Cocke's method of strength reduction [7] and provides a convenient framework in which to implement a host of program transformations including Earley's "iterator inversion"[8].
Reference: [21] <author> R. Paige. </author> <title> Transformational programming applications to algorithms and systems. </title> <booktitle> In Con--ference Record of the 10th Annual ACM Symposium on POPL, </booktitle> <pages> pages 73-87, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers [1, 7, 8], transformational programming <ref> [21, 37] </ref>, interactive editing systems [30, 3], etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs.
Reference: [22] <author> R. Paige. </author> <title> Symbolic finite differencing part I. </title> <booktitle> In Proceedings of the 3rd ESOP, </booktitle> <pages> pages 36-56, </pages> <address> Copenhagen, Denmark, </address> <month> May </month> <year> 1990. </year> <note> Springer-Verlag. LNCS 432. </note>
Reference-contexts: They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive incremental programs. Such techniques are indispensable as part of an optimizing compiler for programs written in very high-level languages like SETL or APL [23, 6]. The APTS <ref> [22] </ref> program transformation system has been developed for such purposes. Our technique differs from that work in that it applies to programs written in a standard language like Lisp.
Reference: [23] <author> R. Paige and F. Henglein. </author> <title> Mechanical translation of set theoretic problem specifications into efficient ram code a case study. </title> <journal> Journal of Symbolic Computation, </journal> <volume> 4(2) </volume> <pages> 207-232, </pages> <year> 1987. </year>
Reference-contexts: They develop a set of rules for differentiating set-theoretic expressions and combine these rules using a chain rule to derive inexpensive incremental programs. Such techniques are indispensable as part of an optimizing compiler for programs written in very high-level languages like SETL or APL <ref> [23, 6] </ref>. The APTS [22] program transformation system has been developed for such purposes. Our technique differs from that work in that it applies to programs written in a standard language like Lisp.
Reference: [24] <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: In the third step, a subroutine called auxiliary generalized partial evaluation is employed to help discover incrementalities. We also cast our derivation approach in terms of a generalized partial evaluation point of view. Paige's work on finite differencing is the pioneering work on deriving incremental programs <ref> [24, 20, 36, 6] </ref>. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS [37, 38]. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows. <p> In the third class, explicitly incremental programs are derived from non-incremental programs by formal transformation techniques such as finite differencing <ref> [24, 47] </ref>. Typically, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations. <p> Our work is closest in spirit to the finite differencing techniques of the third class. The name "finite differencing" was originally given by Paige and Koenig <ref> [20, 24] </ref>. Their work generalizes Cocke's method of strength reduction [7] and provides a convenient framework in which to implement a host of program transformations including Earley's "iterator inversion"[8].
Reference: [25] <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs A Formal Approach to Software Development. Texts and Monographs in Computer Science. </title> <publisher> Springer-Verlag, </publisher> <year> 1990. </year>
Reference-contexts: The Munich CIP project <ref> [25] </ref> has a finite differencing strategy similar to that of KIDS. Each of the three classes of work discussed above emphasizes a different, although important, aspect of incremental computation. But none can be regarded as a general model that subsumes the others.
Reference: [26] <author> W. Pugh. </author> <title> Comments on "Incremental computation via partial evaluation". </title> <journal> Private communications, </journal> <month> March </month> <year> 1991. </year>
Reference-contexts: For this reason, these solutions to the incremental computation problem for particular applications are not readily comparable with explicitly derived incremental algorithms such as those in the first class <ref> [41, 26] </ref>. In the third class, explicitly incremental programs are derived from non-incremental programs by formal transformation techniques such as finite differencing [24, 47].
Reference: [27] <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on POPL, </booktitle> <pages> pages 315-328, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms for particular problems, general evaluation frameworks are pursued for achieving incremental computation automatically, e.g., incremental computation via function caching <ref> [27] </ref>, formal program manipulations like traditional partial evaluation [41, 40, 10], and incremental computation as a program abstraction [16], etc. Note that, when attribute grammars are used as a general description language for applications, the attribute evaluation framework can be viewed as falling into this class too.
Reference: [28] <author> G. Ramalingam and T. Reps. </author> <title> A categorized bibliography on incremental computation. </title> <booktitle> In Conference Record of the 20th Annual ACM Symposium on POPL, </booktitle> <pages> pages 502-510, </pages> <address> Charleston, South Carolina, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: How this might be done is another question open for study. 8 Related Work A comprehensive guide to the literature on incremental computation has appeared in <ref> [28] </ref>. Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. <p> How this might be done is another question open for study. 8 Related Work A comprehensive guide to the literature on incremental computation has appeared in <ref> [28] </ref>. Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. <p> By specializing the general techniques to different applications, we will be able to obtain particular incremental algorithms, particular incremental computation techniques, and particular incremental computation languages. Their applications could include most problems discussed in the literature <ref> [28] </ref> on incremental computation.
Reference: [29] <author> M. Rauch. </author> <title> Fully dynamic biconnectivity in graphs. </title> <booktitle> In Proceedings of the 33rd Annual IEEE Symposium on FOCS, </booktitle> <pages> pages 50-59, </pages> <address> Pittsburgh, Pennsylvania, </address> <month> October </month> <year> 1992. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., <ref> [46, 29, 11] </ref>, can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [30] <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers [1, 7, 8], transformational programming [21, 37], interactive editing systems <ref> [30, 3] </ref>, etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs. In this paper, we present a technique for formally deriving an incremental program f 0 from a non-incremental program f and input change .
Reference: [31] <author> T. Reps, T. Teitelbaum, and A. Demers. </author> <title> Incremental context-dependent analysis for language-based editors. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 5(3) </volume> <pages> 449-477, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. Examples are incremental parsing [14, 17], incremental attribute evaluation <ref> [31, 45, 18] </ref>, incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [32] <author> M. Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings of the 4th International Conference on FPCA, </booktitle> <pages> pages 144-156, </pages> <address> London, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: If e he 1 ; :::; e n i, we use T (e) to denote the sum of the T (e i )'s. The function T can be obtained from standard constructions <ref> [44, 32, 35] </ref>. Note that different primitive functions may take different amounts of time to evaluate. Let e (v 1 ; :::; v n ) denote an expression e in which v 1 ; :::; v n are all the possible free variables.
Reference: [33] <author> B. G. Ryder, T. J. Marlowe, and M. C. Paull. </author> <title> Conditions for incremental iteration: examples and counterexamples. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 11(1) </volume> <pages> 1-15, </pages> <year> 1988. </year> <month> 39 </month>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis <ref> [34, 33] </ref>, incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [34] <author> B. G. Ryder and M. C. Paull. </author> <title> Incremental data flow analysis algorithms. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(1) </volume> <pages> 1-50, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis <ref> [34, 33] </ref>, incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [35] <author> D. Sands. </author> <title> Complexity analysis for a lazy higher-order language. </title> <booktitle> In Proceedings of the 3rd ESOP, </booktitle> <pages> pages 361-376, </pages> <address> Copenhagen, Denmark, </address> <month> May </month> <year> 1990. </year> <note> Springer-Verlag. LNCS 432. </note>
Reference-contexts: If e he 1 ; :::; e n i, we use T (e) to denote the sum of the T (e i )'s. The function T can be obtained from standard constructions <ref> [44, 32, 35] </ref>. Note that different primitive functions may take different amounts of time to evaluate. Let e (v 1 ; :::; v n ) denote an expression e in which v 1 ; :::; v n are all the possible free variables.
Reference: [36] <author> M. Sharir. </author> <title> Some observations concerning formal differentiation of set theoretic expressions. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 4(2) </volume> <pages> 196-225, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: In the third step, a subroutine called auxiliary generalized partial evaluation is employed to help discover incrementalities. We also cast our derivation approach in terms of a generalized partial evaluation point of view. Paige's work on finite differencing is the pioneering work on deriving incremental programs <ref> [24, 20, 36, 6] </ref>. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS [37, 38]. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows.
Reference: [37] <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Methods of incremental computation have widespread application throughout software, e.g., optimizing compilers [1, 7, 8], transformational programming <ref> [21, 37] </ref>, interactive editing systems [30, 3], etc. Based on the observation that explicitly incremental programs are hard to write, we aim to provide incremental computation automatically from non-incremental programs. <p> Paige's work on finite differencing is the pioneering work on deriving incremental programs [24, 20, 36, 6]. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS <ref> [37, 38] </ref>. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows. In Section 2, we give the definition of incremental programs. Our basic approach for the derivation of incremental programs is outlined in Section 3. <p> The technique we propose can be 35 regarded as a principle and a systematic approach. Using this approach, incrementalities can be discovered in existing programs written in standard languages. KIDS <ref> [37, 38] </ref> is a semiautomatic program development system that aims to derive programs from high-level specifications, as is APTS. The system performs algorithm design [39], deductive inference, program simplification, partial evaluation, finite differencing, data type refinement, etc. <p> Simplification uses the invariance to simplify F by replacing occurrences of E (x) with c and, in particular, aims to compute E (U (x)) using the invariance c = E (x). As is pointed out in <ref> [37, 38] </ref>, the real benefit of this optimization comes from the potential use of c = E (x) in the computation of E (U (x)).
Reference: [38] <author> D. R. Smith. </author> <title> KIDS a knowledge-based software development system. </title> <editor> In M. R. Lowry and R. D. MacCartney, editors, </editor> <booktitle> Automating Software Design, chapter 19, </booktitle> <pages> pages 483-514. </pages> <publisher> AAAI Press/The MIT Press, </publisher> <year> 1991. </year> <booktitle> Proceedings of the Workshop on Automating Software Design, AAAI '88. </booktitle>
Reference-contexts: Paige's work on finite differencing is the pioneering work on deriving incremental programs [24, 20, 36, 6]. Whereas Paige's methods are for very high-level set-theoretic languages, our techniques apply to general functional programs, as does Smith's approach in KIDS <ref> [37, 38] </ref>. For further discussions and comparisons between these methods, see Section 8. The rest of the paper is organized as follows. In Section 2, we give the definition of incremental programs. Our basic approach for the derivation of incremental programs is outlined in Section 3. <p> The technique we propose can be 35 regarded as a principle and a systematic approach. Using this approach, incrementalities can be discovered in existing programs written in standard languages. KIDS <ref> [37, 38] </ref> is a semiautomatic program development system that aims to derive programs from high-level specifications, as is APTS. The system performs algorithm design [39], deductive inference, program simplification, partial evaluation, finite differencing, data type refinement, etc. <p> Simplification uses the invariance to simplify F by replacing occurrences of E (x) with c and, in particular, aims to compute E (U (x)) using the invariance c = E (x). As is pointed out in <ref> [37, 38] </ref>, the real benefit of this optimization comes from the potential use of c = E (x) in the computation of E (U (x)).
Reference: [39] <author> D. R. Smith and M. R. Lowry. </author> <title> Algorithm theories and design tactics. </title> <booktitle> Science of Computer Programming, </booktitle> <volume> 14 </volume> <pages> 305-321, </pages> <year> 1990. </year>
Reference-contexts: Using this approach, incrementalities can be discovered in existing programs written in standard languages. KIDS [37, 38] is a semiautomatic program development system that aims to derive programs from high-level specifications, as is APTS. The system performs algorithm design <ref> [39] </ref>, deductive inference, program simplification, partial evaluation, finite differencing, data type refinement, etc. Its version of finite differencing was developed for the optimization of its derived functional programs and has two basic operations: abstraction and simplification.
Reference: [40] <author> R. S. Sundaresh. </author> <title> Building incremental programs using partial evaluation. </title> <booktitle> In Proceedings of the Symposium on PEPM, </booktitle> <pages> pages 83-93, </pages> <institution> Yale University, </institution> <month> June </month> <year> 1991. </year> <note> Published as SIGPLAN Notices, 26(9). </note>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms for particular problems, general evaluation frameworks are pursued for achieving incremental computation automatically, e.g., incremental computation via function caching [27], formal program manipulations like traditional partial evaluation <ref> [41, 40, 10] </ref>, and incremental computation as a program abstraction [16], etc. Note that, when attribute grammars are used as a general description language for applications, the attribute evaluation framework can be viewed as falling into this class too.
Reference: [41] <author> R. S. Sundaresh and P. Hudak. </author> <title> Incremental computation via partial evaluation. </title> <booktitle> In Conference Record of the 18th Annual ACM Symposium on POPL, </booktitle> <pages> pages 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In the second class, rather than manually developing particular incremental algorithms for particular problems, general evaluation frameworks are pursued for achieving incremental computation automatically, e.g., incremental computation via function caching [27], formal program manipulations like traditional partial evaluation <ref> [41, 40, 10] </ref>, and incremental computation as a program abstraction [16], etc. Note that, when attribute grammars are used as a general description language for applications, the attribute evaluation framework can be viewed as falling into this class too. <p> For this reason, these solutions to the incremental computation problem for particular applications are not readily comparable with explicitly derived incremental algorithms such as those in the first class <ref> [41, 26] </ref>. In the third class, explicitly incremental programs are derived from non-incremental programs by formal transformation techniques such as finite differencing [24, 47].
Reference: [42] <author> A. Takano. </author> <title> Generalized partial computation for a lazy functional language. </title> <booktitle> In Proceedings of the Symposium on PEPM, </booktitle> <pages> pages 1-11, </pages> <institution> Yale University, </institution> <month> June </month> <year> 1991. </year> <note> Published as SIGPLAN Notices, 26(9). </note>
Reference-contexts: GPE is a semantic-based program transformation: given an expression e and an information set I that describes partial knowledge about e, GPE produces a residual expression e 0 by specializing e using I. The notion of GPE has appeared in [13] <ref> [42] </ref>, where it is called generalized partial computation (GPC). It was originally proposed to overcome the drawbacks of conventional partial evaluation, which only specializes programs given static values for certain arguments. <p> For the example above, the specialization of mtxM ul (C; R) in the context where null (C) is false is an aGPE that yields the right hand side of (8) as an auxiliary residual expression. In [13] and <ref> [42] </ref>, GPE transformation methods for a restricted form of language (u-form) and a lazy functional language are discussed, and small examples are given.
Reference: [43] <author> B. T. Vander Zanden. </author> <title> Incremental constraint satisfaction and its application to graphical interfaces. </title> <type> Ph.D. Thesis TR 88-941, </type> <institution> Department of Computer Science, Cornell University, </institution> <month> October </month> <year> 1988. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving <ref> [43, 12] </ref>, etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [44] <author> B. Wegbreit. </author> <title> Mechanical program analysis. </title> <journal> Communications of the ACM, </journal> <volume> 18(9) </volume> <pages> 528-538, </pages> <month> September </month> <year> 1975. </year>
Reference-contexts: If e he 1 ; :::; e n i, we use T (e) to denote the sum of the T (e i )'s. The function T can be obtained from standard constructions <ref> [44, 32, 35] </ref>. Note that different primitive functions may take different amounts of time to evaluate. Let e (v 1 ; :::; v n ) denote an expression e in which v 1 ; :::; v n are all the possible free variables.
Reference: [45] <author> D. Yeh and U. Kastens. </author> <title> Improvements on an incremental evaluation algorithm for ordered attribute grammars. </title> <journal> SIGPLAN Notices, </journal> <volume> 23(12) </volume> <pages> 45-50, </pages> <year> 1988. </year> <month> 40 </month>
Reference-contexts: Despite the relatively diverse categories discussed in [28], most of the work can be divided into three classes. 34 The first class includes particular incremental algorithms that deal with particular input changes to particular problems. Examples are incremental parsing [14, 17], incremental attribute evaluation <ref> [31, 45, 18] </ref>, incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., [46, 29, 11], can be viewed as falling into this class.
Reference: [46] <author> D. M. Yellin. </author> <title> Speeding up dynamic transitive closure for bounded degree graphs. </title> <journal> Acta Infor--matica, </journal> <volume> 30(4) </volume> <pages> 369-384, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Examples are incremental parsing [14, 17], incremental attribute evaluation [31, 45, 18], incremental data-flow analysis [34, 33], incremental circuit evaluation [2], incremental constraint solving [43, 12], etc. The study of dynamic algorithms, e.g., <ref> [46, 29, 11] </ref>, can be viewed as falling into this class. Although efforts in this class are directed towards particular incremental algorithms, they apply to a broad class of problems, e.g., any attribute grammar, any circuit, etc.
Reference: [47] <author> D. M. Yellin and R. E. Strom. INC: </author> <title> A language for incremental computations. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 211-236, </pages> <month> April </month> <year> 1991. </year> <month> 41 </month>
Reference-contexts: In the third class, explicitly incremental programs are derived from non-incremental programs by formal transformation techniques such as finite differencing <ref> [24, 47] </ref>. Typically, programs are written in very high-level languages with aggregate data structures, e.g., sets and bags, and fixed rules are offered for transforming aggregate operations into more efficient incremental operations.
References-found: 47

