URL: http://www.cs.purdue.edu/research/PaCS/ps/lazy98_tr.ps
Refering-URL: http://www.cs.purdue.edu/research/PaCS/parasol.html
Root-URL: http://www.cs.purdue.edu
Email: pasquini@cs.purdue.edu  rego@cs.purdue.edu  
Title: Lazy Algorithms in Parallel Discrete Event Simulation  
Author: Reuben Pasquini Vernon Rego 
Date: June 24, 1998  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. Aji, A. C. Palaniswamy, and P. A. Wilsey. </author> <title> Interactions of optimizations to a time warp synchronized digital system simulator. </title> <booktitle> Modeling and Simulation, </booktitle> <pages> pages 593-597, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback <ref> [36, 4, 30, 1] </ref>, and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES.
Reference: [2] <author> D. Baezner, G. Lomow, and B. Unger. </author> <title> Sim++: The transition to distributed simulation. </title> <booktitle> In Distributed Simulation, volume 22 of SCS Simulation Series, </booktitle> <pages> pages 211-218, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: SIMKIT exploits object oriented design techniques to support an active-server world view. SIMKIT's object oriented architecture is built upon a simulation kernel which manages the simulation calendar and time warp algorithm. Simulation developers may build domain specific libraries which provide state saved objects for different types of models <ref> [2] </ref>. SIMKIT supports the active server world view and runs on shared memory architectures. The APOSTLE system employs the breathing time-buckets algorithm to enforce the causality constraint [43, 6]. This algorithm constrains its optimism by periodically synchronizing the processors participating in the simulation.
Reference: [3] <author> R. L. Bagrodia and Wen-Toh Liao. Maisie: </author> <title> A language and optimizing environment for distributed simulation. </title> <booktitle> In Proceedings of SCS Multiconference on Distributed Simulation, </booktitle> <pages> pages 205-210, </pages> <year> 1990. </year>
Reference-contexts: Maisie improves upon GTW by making constructs for parallel execution more transparent to the user. Using Maisie we can specify a model for sequential execution without constructs for parallel execution, then later add constructs supporting parallel execution if the sequential model does not yield a satisfactory runtime <ref> [3] </ref>. SIMKIT is a PDES language extending C++ under development at the University of Cal-gary. SIMKIT exploits object oriented design techniques to support an active-server world view. SIMKIT's object oriented architecture is built upon a simulation kernel which manages the simulation calendar and time warp algorithm.
Reference: [4] <author> D. Ball and S. Hoyt. </author> <title> The adaptive time-warp concurrency control algorithm. </title> <booktitle> In Proceedings of the SCS MultiConference on Distributed Simulation, </booktitle> <pages> pages 174-177, </pages> <year> 1990. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback <ref> [36, 4, 30, 1] </ref>, and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES.
Reference: [5] <author> S. Bellenot. </author> <title> Global virtual time algorithms. </title> <booktitle> In Proceedings of the Multiconference on Distributed Simulation, </booktitle> <volume> volume 22, </volume> <pages> pages 122-127, </pages> <year> 1990. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback [36, 4, 30, 1], and global virtual time (gvt) computation <ref> [5, 11, 31] </ref>. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES.
Reference: [6] <author> C. J. M. Booth and D. I. Bruce. </author> <title> Stack-free process-oriented simulation. </title> <booktitle> In Proceedings of the 11th Workshop on Parallel and Distributed Simulation (PADS'97), </booktitle> <pages> pages 182-185, </pages> <year> 1997. </year>
Reference-contexts: Simulation developers may build domain specific libraries which provide state saved objects for different types of models [2]. SIMKIT supports the active server world view and runs on shared memory architectures. The APOSTLE system employs the breathing time-buckets algorithm to enforce the causality constraint <ref> [43, 6] </ref>. This algorithm constrains its optimism by periodically synchronizing the processors participating in the simulation. Periodic synchronization may help decrease the frequency of rollback [41]. We use the ParaSol PDES system in our experiments.
Reference: [7] <author> A. Boukerche and C. Tropper. </author> <title> A static partitioning and mapping algorithm for conservative parallel simulations. </title> <booktitle> In Proceedings of the Eighth Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 164-172, </pages> <year> 1994. </year>
Reference-contexts: Deadlock can arise when each of a cycle of processors blocks, awaiting input that will allow the processor to continue simulating without spoiling causality. Sophisticated synchronization and lookahead algorithms have been proposed to address the deadlock problem <ref> [32, 15, 8, 7] </ref>. The time warp algorithm is an optimistic algorithm for PDES. It is optimistic in the sense that each processor executes events in time-stamp order under the optimistic assumption that causality is not being violated.
Reference: [8] <author> Azzedine Boukerche and Carl Tropper. </author> <title> Semi-global time of the next event algorithm. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 68-77, </pages> <year> 1996. </year>
Reference-contexts: Deadlock can arise when each of a cycle of processors blocks, awaiting input that will allow the processor to continue simulating without spoiling causality. Sophisticated synchronization and lookahead algorithms have been proposed to address the deadlock problem <ref> [32, 15, 8, 7] </ref>. The time warp algorithm is an optimistic algorithm for PDES. It is optimistic in the sense that each processor executes events in time-stamp order under the optimistic assumption that causality is not being violated.
Reference: [9] <author> R. Brown. </author> <title> Calendar queues: A fast O(1) priority queue implementation for the simulation event set problem. </title> <journal> Communications of the ACM, </journal> <volume> 31 </volume> <pages> 1220-1227, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Each event simulated involves one schedule-event and one get-next-event calendar operation. Various efficient calendar implementations have been employed in sequential simulation languages. Most calendar data structures implement some kind of heap based priority queue (a notable exception is the calendar queue <ref> [9] </ref>). Heap based priority queues support the get-next-event and schedule-event operations in O (log n) operations (where n is the size of the calendar) [14, 13, 42]. An efficient calendar implementation is essential for achieving good runtime performance by a simulator.
Reference: [10] <author> Doug Burger and James R. Goodman. </author> <title> Billion-transistor architectures. </title> <booktitle> Computer, </booktitle> <pages> pages 46-50, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: 1 Introduction The problem of rapidly simulating large, complex systems promises to be one of the major challenges of the next decade. For example, the next generation of computer chips will have in excess of one billion transistors making up hundreds of millions of logic gates <ref> [10] </ref>, metropolitan air-traffic control systems must safely manage time and space constraints for hundreds of aircraft every hour [34], and modern military engagement requires the coordination of thousands of entities including aircraft, ships, soldiers, commanders, satellites, computers, and communication systems.
Reference: [11] <author> K. Chandy and L. Lamport. </author> <title> Distributed snapshots: Determining the global states of distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year> <month> 21 </month>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback [36, 4, 30, 1], and global virtual time (gvt) computation <ref> [5, 11, 31] </ref>. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES. <p> A parallel simulation's gvt is the minimum of the local virtual time on each processor and all time stamps on messages in transit between processors. Good distributed algorithms exist for computing gvt <ref> [31, 11] </ref>, but determining an appropriate frequency for gvt computation is still an open problem. To arrive at a good frequency for gvt computation, a parallel simulation must address a bandwidth vs. memory trade off. <p> Once this is done, P 0 continues simulating events. When P 0 receives a report of every processor's local virtual time, and when all outstanding messages have been accounted for (see <ref> [31, 11] </ref>), P 0 then broadcasts a message telling every processor the new gvt. ParaSol uses a lazy algorithm to schedule gvt computation.
Reference: [12] <author> K. M. Chandy and J. Misra. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11) </volume> <pages> 198-206, </pages> <month> April </month> <year> 1981. </year>
Reference-contexts: A PDES must employ an algorithm which ensures that events are executed in a causally consistent way. A simulation is causally consistent if each simulation object is accessed by events in nondecreasing time-stamp order. In seminal works on achieving causal consistency, Chandy and Misra <ref> [12] </ref> and Jefferson [20] came up with very different solutions. The Chandry-Misra algorithm avoids causality errors by ensuring that each processor executes events in time-stamp order. <p> Processor P 0 will roll back when P 0 receives the straggler event. that no event e i with an earlier time-stamp t i &lt; t j will be scheduled on P (by another processor). Because of this, the Chandy-Misra algorithm is called a conservative parallel simulation algorithm <ref> [12] </ref>. Research in conservative parallel simulation has focused on the problem of deadlock avoidance. Deadlock can arise when each of a cycle of processors blocks, awaiting input that will allow the processor to continue simulating without spoiling causality.
Reference: [13] <author> K. Chung, J. Sang, and V. Rego. </author> <title> A performance comparison of event calendar algorithms: An empirical approach. </title> <journal> Software-Practice & Experience, </journal> <volume> 23(10) </volume> <pages> 1107-1138, </pages> <month> Oct. </month> <year> 1993. </year>
Reference-contexts: Most calendar data structures implement some kind of heap based priority queue (a notable exception is the calendar queue [9]). Heap based priority queues support the get-next-event and schedule-event operations in O (log n) operations (where n is the size of the calendar) <ref> [14, 13, 42] </ref>. An efficient calendar implementation is essential for achieving good runtime performance by a simulator. A doubly linked list (dll) based calendar implements the schedule-event operation in O (n) time and the get-next-event operation in O (1) time.
Reference: [14] <author> Thomas Cormen, Charles Leiserson, and Ronald Rivest. </author> <title> Introduction to Algorithms. </title> <institution> Mas-sachusetts Institute of Technology, </institution> <year> 1990. </year>
Reference-contexts: Most calendar data structures implement some kind of heap based priority queue (a notable exception is the calendar queue [9]). Heap based priority queues support the get-next-event and schedule-event operations in O (log n) operations (where n is the size of the calendar) <ref> [14, 13, 42] </ref>. An efficient calendar implementation is essential for achieving good runtime performance by a simulator. A doubly linked list (dll) based calendar implements the schedule-event operation in O (n) time and the get-next-event operation in O (1) time. <p> Table 1 compares the hybrid calendar's performance with the other calendars. ParaSol uses a tree rather than a heap based calendar since the tree supports a search method in O (log n) operations <ref> [14] </ref>. The search operation is useful for finding an entry canceled by an anti-message.
Reference: [15] <author> Phillip Dickens, Paul Reynolds, and Mark Duva. </author> <title> Analysis of bounded time warp and comparison with yawns. </title> <booktitle> ACM Transactions on Modeling and Computer Simulation, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Deadlock can arise when each of a cycle of processors blocks, awaiting input that will allow the processor to continue simulating without spoiling causality. Sophisticated synchronization and lookahead algorithms have been proposed to address the deadlock problem <ref> [32, 15, 8, 7] </ref>. The time warp algorithm is an optimistic algorithm for PDES. It is optimistic in the sense that each processor executes events in time-stamp order under the optimistic assumption that causality is not being violated.
Reference: [16] <author> Robert Felderman and Leonard Kleinrock. </author> <title> Two processor time warp analysis: Some results on a unifying approach. </title> <journal> International Journal of Computer Simulation, </journal> <year> 1990. </year>
Reference-contexts: The use of threads in ParaSol yields the valuable benefit of eliminating time warp thrashing <ref> [16, 25] </ref>. When interdependencies between processes that participate in a parallel simulation produce recurring rollbacks, the simulation's forward progress is significantly slowed. This is called Time warp thrashing. Consider a simple form of thrashing which can occur in a three processor parallel simulation. 1.
Reference: [17] <author> J. Fleischmann and P. A. Wilsey. </author> <title> Comparative analysis of periodic state saving techniques in time warp simulators. </title> <booktitle> In Proceedings of the 9th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 50-58, </pages> <year> 1995. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in
Reference: [18] <author> R. Fujimoto. </author> <title> Time warp on a shared memory multiprocessor. </title> <journal> Transactions of the Society for Computer Simulation, </journal> <volume> 6(3) </volume> <pages> 211-239, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving [39]. The Georgia Tech Time Warp (GTW) is a library supporting the construction of event based parallel discrete event simulations <ref> [18, 33] </ref>. GTW is available for various shared memory parallel machines, and a new version of GTW which runs over distributed memory architectures (including a network of workstations) using PVM is under development. GTW supports the event-scheduling world view.
Reference: [19] <author> F. Gomes, B. Unger, and J. Cleary. </author> <title> Language based state saving extensions for optimistic parallel simulation. </title> <booktitle> In Proceedings of the 1996 Winter Simulation Conference, </booktitle> <year> 1996. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in <p> The incremental state saving algorithm improves upon the basic algorithm by only saving the portion of the processor state which an event changes. At rollback time, a processor undoes each incremental change to the processor state since the rollback v.t. <ref> [19, 39] </ref>. In the object based state saving algorithm, the state of each simulation object and thread changed by that event is saved after each event [29, 28]. Even if only a small portion of an object's state is changed by an event's execution, the object's entire state is saved.
Reference: [20] <author> D. R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3) </volume> <pages> 404-425, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: A PDES must employ an algorithm which ensures that events are executed in a causally consistent way. A simulation is causally consistent if each simulation object is accessed by events in nondecreasing time-stamp order. In seminal works on achieving causal consistency, Chandy and Misra [12] and Jefferson <ref> [20] </ref> came up with very different solutions. The Chandry-Misra algorithm avoids causality errors by ensuring that each processor executes events in time-stamp order. <p> The calendar unexecutes an event e by removing from the calendar every event scheduled during e's execution and sending anti-messages to cancel every event which e scheduled on remote processors <ref> [20] </ref>. Since the calendar must unexecute each event that is rolled back, we know that O (m) is a lower bound on the performance of the rollback-to-vt method.
Reference: [21] <author> F. Knop. </author> <title> Software Architectures for Fault-Tolerant Replications and Multithreaded Decompositions: Experiments with Practical Parallel Simulation. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1996. </year>
Reference-contexts: ParaSol's layered architecture hides state saving and interprocessor communication code from the user who describes his models using ParaSol's programming interface. ParaSol's API is an object oriented derivative of the API for the sequential simulation language, CSIM <ref> [21, 27, 38] </ref>. 3 Lazy Checkpointing Saving state is a large runtime overhead of the time warp simulation algorithm. Each processor participating in a parallel simulation periodically saves a snapshot of its local state.
Reference: [22] <author> A. M. Law and W. D. </author> <title> Kelton. Simulation Modeling and Analysis. </title> <publisher> McGraw-Hill, </publisher> <year> 1982. </year>
Reference-contexts: Each memcpy () operation copies 100 bytes of randomly initialized data between two buffers. Each sample of an exponential random variable is computed via the inverse cdf method using a u16807 random number generator <ref> [22] </ref>. The measurements are reported in clock ticks.
Reference: [23] <author> Y. B. Lin and E. D. Lazowska. </author> <title> Exploiting lookahead in parallel simulation. </title> <type> Technical Report 89-10-06, </type> <institution> Dept. of Computer Science, Univ. of Washington, </institution> <year> 1989. </year>
Reference-contexts: torus = N 2 y-axis: runtime in seconds a. 2 processors b. 4 processors c. 6 processors x-axis: objects in torus = N 2 y-axis: runtime in seconds 5 Laziness and Lookahead The lazy object state saving algorithm (see Section 3.2) and the lazy hybrid calendar both exploit the lookahead <ref> [23, 41] </ref> available in a discrete event simulation to avoid work (state saving and scheduling) which may be rolled back. The larger an application's lookahead, the more successful lazy algorithms are at avoiding wasted work.
Reference: [24] <author> Y. B. Lin, B. R. Priess, W. M. Loucks, and E. D. Lazowska. </author> <title> Selecting the checkpoint interval in time warp simulations. </title> <booktitle> In Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <volume> volume 23, </volume> <pages> pages 3-8, </pages> <year> 1993. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in <p> Rather than saving dirty objects after each event, the ssa algorithm saves dirty objects only after several events execute. At rollback time, the simulation kernel must re-execute events which were rolled back as a result of the state saving period; this re-execution is known as coasting forward <ref> [24, 37] </ref>. Despite the extra cost of coasting at rollback time, selective state saving can reduce state saving overhead if the same object is changed by several sequentially executed events. <p> ParaSol's object based state saving mechanism interacts with the selective state saving algorithm in ways not anticipated by Lin <ref> [24] </ref> or Rongren [37]. Lin and Rongren analyze selective state saving with the assumption that the state of the whole system is saved at checkpoint time (our basic state saving algorithm).
Reference: [25] <author> Boris Lubachevsky and Alan Weiss. </author> <title> An analysis of rollback-based simulation. </title> <booktitle> ACM Transaction on Modeling and Computer Simulation, </booktitle> <volume> 1(2) </volume> <pages> 154-193, </pages> <year> 1991. </year>
Reference-contexts: The use of threads in ParaSol yields the valuable benefit of eliminating time warp thrashing <ref> [16, 25] </ref>. When interdependencies between processes that participate in a parallel simulation produce recurring rollbacks, the simulation's forward progress is significantly slowed. This is called Time warp thrashing. Consider a simple form of thrashing which can occur in a three processor parallel simulation. 1.
Reference: [26] <author> Dale E. Martin and Timothy J. McBrayer. </author> <title> Warped A TimeWarp Parallel Discrete Event Simulator (Documentation for version 0.8). </title> <institution> Dept. of ECECS, University of Cincinnatti, Computer Architecture Design Laboratory, Dept of ECECS, </institution> <address> PO Box 210030 Cincinnati, OH 45221-0030, </address> <month> December </month> <year> 1997. </year>
Reference-contexts: Their calendar is designed for use with a semi-conservative breathing-time-buckets based simulation. Other parallel simulation systems modify sequential calendar data structures for parallel simulation. The WARPED <ref> [26] </ref> PDES system uses a doubly linked list (dll) based calendar. A dll calendar implements an m event rollback in O (m) operations. Each event e in the calendar maintains a list of the events scheduled in the calendar as a result of e's execution.
Reference: [27] <author> E. Mascarenhas. </author> <title> A System for Multithreaded Parallel Simulation and Computation with Migrant Threads and Object. </title> <type> PhD thesis, </type> <institution> Department of Computer Sciences, Purdue University, </institution> <year> 1996. </year>
Reference-contexts: ParaSol's layered architecture hides state saving and interprocessor communication code from the user who describes his models using ParaSol's programming interface. ParaSol's API is an object oriented derivative of the API for the sequential simulation language, CSIM <ref> [21, 27, 38] </ref>. 3 Lazy Checkpointing Saving state is a large runtime overhead of the time warp simulation algorithm. Each processor participating in a parallel simulation periodically saves a snapshot of its local state.
Reference: [28] <author> E. Mascarenhas, F. Knop, R. Pasquini, and V. Rego. </author> <title> Checkpoint and recovery methods in the parasol simulation system. </title> <booktitle> In Proceedings of the 1997 Winter Simulation Conference, </booktitle> <pages> pages 452-459, </pages> <year> 1997. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in <p> At rollback time, a processor undoes each incremental change to the processor state since the rollback v.t. [19, 39]. In the object based state saving algorithm, the state of each simulation object and thread changed by that event is saved after each event <ref> [29, 28] </ref>. Even if only a small portion of an object's state is changed by an event's execution, the object's entire state is saved. This potential increase in state saving overhead is offset by a decrease in state restoration overhead. <p> To balance the cost of coasting with the benefit of reduced state saving, ParaSol employs an adaptive algorithm which dynamically adjusts the size of the state saving interval (the number of events processed between each state saving snapshot) <ref> [28] </ref>. ParaSol's object based state saving mechanism interacts with the selective state saving algorithm in ways not anticipated by Lin [24] or Rongren [37].
Reference: [29] <author> E. Mascarenhas, F. Knop, and V. Rego. ParaSol: </author> <title> A Multi-threaded System for Parallel Simulation Based on Mobile Threads. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 690-697, </pages> <year> 1995. </year>
Reference-contexts: At rollback time, a processor undoes each incremental change to the processor state since the rollback v.t. [19, 39]. In the object based state saving algorithm, the state of each simulation object and thread changed by that event is saved after each event <ref> [29, 28] </ref>. Even if only a small portion of an object's state is changed by an event's execution, the object's entire state is saved. This potential increase in state saving overhead is offset by a decrease in state restoration overhead.
Reference: [30] <author> Edward Mascarenhas, Felipe Knop, and Vernon Rego. </author> <title> Minimum cost adaptive synchronization: Experiments with the parasol system. </title> <booktitle> In Proceedings of the 1997 Winter Simulation Conference, </booktitle> <pages> pages 389-396, </pages> <year> 1997. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback <ref> [36, 4, 30, 1] </ref>, and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES.
Reference: [31] <author> Friedmann Mattern. </author> <title> Efficient algorithms for distributed snapshot and global virtual time approximation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18 </volume> <pages> 423-434, </pages> <year> 1993. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback [36, 4, 30, 1], and global virtual time (gvt) computation <ref> [5, 11, 31] </ref>. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES. <p> A parallel simulation's gvt is the minimum of the local virtual time on each processor and all time stamps on messages in transit between processors. Good distributed algorithms exist for computing gvt <ref> [31, 11] </ref>, but determining an appropriate frequency for gvt computation is still an open problem. To arrive at a good frequency for gvt computation, a parallel simulation must address a bandwidth vs. memory trade off. <p> Once this is done, P 0 continues simulating events. When P 0 receives a report of every processor's local virtual time, and when all outstanding messages have been accounted for (see <ref> [31, 11] </ref>), P 0 then broadcasts a message telling every processor the new gvt. ParaSol uses a lazy algorithm to schedule gvt computation.
Reference: [32] <author> David Nicol. </author> <title> Parallel discrete-event simulation of FCFS stochastic queueing networks. Parallel Programming: Experience with Applications, </title> <booktitle> Languages, and Systems, </booktitle> <month> July </month> <year> 1988. </year> <journal> ACM SIGPLAN. </journal>
Reference-contexts: Deadlock can arise when each of a cycle of processors blocks, awaiting input that will allow the processor to continue simulating without spoiling causality. Sophisticated synchronization and lookahead algorithms have been proposed to address the deadlock problem <ref> [32, 15, 8, 7] </ref>. The time warp algorithm is an optimistic algorithm for PDES. It is optimistic in the sense that each processor executes events in time-stamp order under the optimistic assumption that causality is not being violated.
Reference: [33] <author> Kiran S. Penesar and Richard M. Fujimoto. </author> <title> Adaptive flow control in time warp. </title> <booktitle> In 11th Workshop on Parallel and Distributed Simulation (PADS'97), </booktitle> <pages> pages 108-115, </pages> <month> June 10-13 </month> <year> 1997. </year>
Reference-contexts: Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving [39]. The Georgia Tech Time Warp (GTW) is a library supporting the construction of event based parallel discrete event simulations <ref> [18, 33] </ref>. GTW is available for various shared memory parallel machines, and a new version of GTW which runs over distributed memory architectures (including a network of workstations) using PVM is under development. GTW supports the event-scheduling world view.
Reference: [34] <author> Tekla S. Perry. </author> <title> In search of the future of air traffic control. </title> <journal> IEEE Spectrum, </journal> <pages> pages 18-35, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: For example, the next generation of computer chips will have in excess of one billion transistors making up hundreds of millions of logic gates [10], metropolitan air-traffic control systems must safely manage time and space constraints for hundreds of aircraft every hour <ref> [34] </ref>, and modern military engagement requires the coordination of thousands of entities including aircraft, ships, soldiers, commanders, satellites, computers, and communication systems. Simulators have traditionally aided in the task of understanding, designing, building and operating such systems.
Reference: [35] <author> P. Reiher. </author> <title> Parallel simulation using the time warp operationg system. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 38-45, </pages> <year> 1990. </year>
Reference-contexts: We conclude briefly in Section 7. 2 PDES Systems The Time Warp Operating System (TWOS) was an early implementation of the time warp mechanism <ref> [36, 35] </ref>. TWOS is an event-based system in which logical processes (LPs) exchange time-stamped event messages.
Reference: [36] <author> P. Reiher and D. Jefferson. </author> <title> Limitation of optimism in the time warp operating system. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 765-769, </pages> <year> 1989. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving [28, 39, 19, 17, 37, 24], rollback <ref> [36, 4, 30, 1] </ref>, and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in optimistic PDES. <p> We conclude briefly in Section 7. 2 PDES Systems The Time Warp Operating System (TWOS) was an early implementation of the time warp mechanism <ref> [36, 35] </ref>. TWOS is an event-based system in which logical processes (LPs) exchange time-stamped event messages.
Reference: [37] <author> R. Rongren and R. Ayani. </author> <title> Adaptive checkpointing in time warp. </title> <booktitle> In Proceedings of the 8th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 94-100, </pages> <year> 1994. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in <p> Rather than saving dirty objects after each event, the ssa algorithm saves dirty objects only after several events execute. At rollback time, the simulation kernel must re-execute events which were rolled back as a result of the state saving period; this re-execution is known as coasting forward <ref> [24, 37] </ref>. Despite the extra cost of coasting at rollback time, selective state saving can reduce state saving overhead if the same object is changed by several sequentially executed events. <p> ParaSol's object based state saving mechanism interacts with the selective state saving algorithm in ways not anticipated by Lin [24] or Rongren <ref> [37] </ref>. Lin and Rongren analyze selective state saving with the assumption that the state of the whole system is saved at checkpoint time (our basic state saving algorithm).
Reference: [38] <author> H. D. Schwetman. CSIM: </author> <title> A C-based process-oriented simulation language. </title> <booktitle> In Proceedings of the 1986 Winter Simulation Conference, </booktitle> <pages> pages 387-396, </pages> <year> 1986. </year>
Reference-contexts: ParaSol's layered architecture hides state saving and interprocessor communication code from the user who describes his models using ParaSol's programming interface. ParaSol's API is an object oriented derivative of the API for the sequential simulation language, CSIM <ref> [21, 27, 38] </ref>. 3 Lazy Checkpointing Saving state is a large runtime overhead of the time warp simulation algorithm. Each processor participating in a parallel simulation periodically saves a snapshot of its local state.
Reference: [39] <author> J. Steinman. </author> <title> Incremental state saving in SPEEDES using C++. </title> <booktitle> In Proceedings of the Winter Simulation Conference, </booktitle> <pages> pages 687-696, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: Execution continues from this point, and the straggler is processed in the right time-stamp order (see Figure 1). Research in optimistic parallel simulation has focused on reducing runtime overheads: state-saving <ref> [28, 39, 19, 17, 37, 24] </ref>, rollback [36, 4, 30, 1], and global virtual time (gvt) computation [5, 11, 31]. 1.2 Outline We propose new lazy approaches to state saving, calendar management, and gvt scheduling which try to avoid doing work which is thrown away as the result of rollbacks in <p> TWOS was designed to run on bare hardware, with a bottom layer responsible for low-level tasks (context management, message communication, interrupt handling, etc.) 2 and a top layer responsible for time warp mechanisms (rollback, anti-messages, etc.). SPEEDES <ref> [40, 41, 39] </ref> is a direct descendant of TWOS; it is written in C++, with an object-oriented view. Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving [39]. <p> SPEEDES [40, 41, 39] is a direct descendant of TWOS; it is written in C++, with an object-oriented view. Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving <ref> [39] </ref>. The Georgia Tech Time Warp (GTW) is a library supporting the construction of event based parallel discrete event simulations [18, 33]. <p> The incremental state saving algorithm improves upon the basic algorithm by only saving the portion of the processor state which an event changes. At rollback time, a processor undoes each incremental change to the processor state since the rollback v.t. <ref> [19, 39] </ref>. In the object based state saving algorithm, the state of each simulation object and thread changed by that event is saved after each event [29, 28]. Even if only a small portion of an object's state is changed by an event's execution, the object's entire state is saved.
Reference: [40] <author> J. S. Steinman. SPEEDES: </author> <title> A unified approach to parallel simulation. </title> <booktitle> In Proceedings of the 6th Workshop on Parallel and Distributed Simulation, volume 24 of Simulation Series, </booktitle> <pages> pages 75-84, </pages> <year> 1992. </year>
Reference-contexts: TWOS was designed to run on bare hardware, with a bottom layer responsible for low-level tasks (context management, message communication, interrupt handling, etc.) 2 and a top layer responsible for time warp mechanisms (rollback, anti-messages, etc.). SPEEDES <ref> [40, 41, 39] </ref> is a direct descendant of TWOS; it is written in C++, with an object-oriented view. Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving [39].
Reference: [41] <author> J. S. Steinman. </author> <title> Breathing Time Warp. </title> <booktitle> In Proceedings of the 7th Workshop on Parallel and Distributed Simulation, </booktitle> <pages> pages 109-118, </pages> <year> 1993. </year>
Reference-contexts: TWOS was designed to run on bare hardware, with a bottom layer responsible for low-level tasks (context management, message communication, interrupt handling, etc.) 2 and a top layer responsible for time warp mechanisms (rollback, anti-messages, etc.). SPEEDES <ref> [40, 41, 39] </ref> is a direct descendant of TWOS; it is written in C++, with an object-oriented view. Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism [41] for incremental state-saving [39]. <p> SPEEDES [40, 41, 39] is a direct descendant of TWOS; it is written in C++, with an object-oriented view. Like TWOS, SPEEDES is event-based. It supports multiple synchronization protocols and a delta exchange mechanism <ref> [41] </ref> for incremental state-saving [39]. The Georgia Tech Time Warp (GTW) is a library supporting the construction of event based parallel discrete event simulations [18, 33]. <p> The APOSTLE system employs the breathing time-buckets algorithm to enforce the causality constraint [43, 6]. This algorithm constrains its optimism by periodically synchronizing the processors participating in the simulation. Periodic synchronization may help decrease the frequency of rollback <ref> [41] </ref>. We use the ParaSol PDES system in our experiments. ParaSol provides a simulation developer with libraries of C++ classes with which he can instantiate simulation processes (migratable threads) and objects. <p> torus = N 2 y-axis: runtime in seconds a. 2 processors b. 4 processors c. 6 processors x-axis: objects in torus = N 2 y-axis: runtime in seconds 5 Laziness and Lookahead The lazy object state saving algorithm (see Section 3.2) and the lazy hybrid calendar both exploit the lookahead <ref> [23, 41] </ref> available in a discrete event simulation to avoid work (state saving and scheduling) which may be rolled back. The larger an application's lookahead, the more successful lazy algorithms are at avoiding wasted work.
Reference: [42] <author> J. S. Steinman. </author> <title> Discrete-event simulation and the event horizon part 2: Event list management. </title> <booktitle> In Proceedings of the 10th Workshop on Parallel and Distributed Simulation (PADS'96), </booktitle> <pages> pages 170-178, </pages> <year> 1996. </year>
Reference-contexts: Most calendar data structures implement some kind of heap based priority queue (a notable exception is the calendar queue [9]). Heap based priority queues support the get-next-event and schedule-event operations in O (log n) operations (where n is the size of the calendar) <ref> [14, 13, 42] </ref>. An efficient calendar implementation is essential for achieving good runtime performance by a simulator. A doubly linked list (dll) based calendar implements the schedule-event operation in O (n) time and the get-next-event operation in O (1) time. <p> To the best of our knowledge, SPEEDES and APOSTLE are the only PDES systems using a calendar specially designed for use in parallel simulation <ref> [42] </ref>. Their calendar is designed for use with a semi-conservative breathing-time-buckets based simulation. Other parallel simulation systems modify sequential calendar data structures for parallel simulation. The WARPED [26] PDES system uses a doubly linked list (dll) based calendar. A dll calendar implements an m event rollback in O (m) operations.
Reference: [43] <author> P. Wonnacott and D. Bruce. </author> <title> The apostle simulation language: Granularity control and performance data. </title> <booktitle> In Proceedings of the 10th Workshop on Parallel and Distributed Simulation (PADS'96), </booktitle> <year> 1996. </year> <month> 23 </month>
Reference-contexts: Simulation developers may build domain specific libraries which provide state saved objects for different types of models [2]. SIMKIT supports the active server world view and runs on shared memory architectures. The APOSTLE system employs the breathing time-buckets algorithm to enforce the causality constraint <ref> [43, 6] </ref>. This algorithm constrains its optimism by periodically synchronizing the processors participating in the simulation. Periodic synchronization may help decrease the frequency of rollback [41]. We use the ParaSol PDES system in our experiments.
References-found: 43

