URL: http://www.cs.columbia.edu/pdis_lab/papers/learning/kdd95/k.ps.Z
Refering-URL: http://www.cs.columbia.edu/pdis_lab/
Root-URL: 
Email: pkc@cs.columbia.edu and sal@cs.columbia.edu  
Title: Learning Arbiter and Combiner Trees from Partitioned Data for Scaling Machine Learning  
Author: Philip K. Chan and Salvatore J. Stolfo 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: Knowledge discovery in databases has become an increasingly important research topic with the advent of wide area network computing. One of the crucial problems we study in this paper is how to scale machine learning algorithms, that typically are designed to deal with main memory based datasets, to efficiently learn from large distributed databases. We have explored an approach called meta-learning that is related to the traditional approaches of data reduction commonly employed in distributed query processing systems. Here we seek efficient means to learn how to combine a number of base classifiers, which are learned from subsets of the data, so that we scale efficiently to larger learning problems, and boost the accuracy of the constituent classifiers if possible. In this paper we compare the arbiter tree strategy to a new but related approach called the combiner tree strategy. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: Experiments and Results Two inductive learning algorithms were used in our experiments reported here. ID3 (Quinlan 1986) and CART <ref> (Breiman et al. 1984) </ref> were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Buntine, W., and Caruana, R. </author> <year> 1991. </year> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Experiments and Results Two inductive learning algorithms were used in our experiments reported here. ID3 (Quinlan 1986) and CART (Breiman et al. 1984) were obtained from NASA Ames Research Center in the IND package <ref> (Buntine & Caruana 1991) </ref>. They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993a. </year> <title> Experiments on multistrat-egy learning by meta-learning. </title> <booktitle> In Proc. Second Intl. Conf. Info. Know. Manag., </booktitle> <pages> 314-323. </pages>
Reference-contexts: Once the training set is formed, an arbiter is generated by the same learning algorithm used to train the base classifiers. Together with an arbitration rule, the learned arbiter resolves conflicts among the classifiers when necessary. Combiner In the combiner <ref> (Chan & Stolfo 1993a) </ref> strategy, the predictions of the learned base classifiers on the training set form the basis of the meta-learner's training set. A composition rule, which varies in different schemes, determines the content of training examples for the meta-learner.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1993b. </year> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> In Working Notes AAAI Work. Know. Disc. Databases, </booktitle> <pages> 227-240. </pages>
Reference-contexts: The arbiter/combiner is also a classifier, and hence other arbiters or combiners can be computed from the set of predictions of other arbiters/combiners. Arbiter An arbiter <ref> (Chan & Stolfo 1993b) </ref> is learned by some learning algorithm to arbitrate among predictions generated by different base classifiers. This arbiter, together with an arbitration rule, decides a final classification outcome based upon the base predictions.
Reference: <author> Chan, P., and Stolfo, S. </author> <year> 1995. </year> <title> A comparative evaluation of voting and meta-learning on partitioned data. </title> <booktitle> In Proc. Twelfth Intl. Conf. Machine Learning. To appear. </booktitle>
Reference-contexts: Incremental learning algorithms and windowing techniques aim to solve the scaling problem by piecemeal processing of a large data set. Others have studied approaches based upon direct parallelization of a learning algorithm run on a multiprocessor. A review of such approaches has appeared elsewhere <ref> (Chan & Stolfo 1995) </ref>. An alternative approach we study here is to apply data reduction techniques common in distributed query processing where cluster of computers can be profitably employed to learn from large databases. <p> In this paper we study more sophisticated techniques for combining predictions generated by a set of base classifiers, each of which is computed by a learning algorithm applied to a distinct data subset. In a previous study <ref> (Chan & Stolfo 1995) </ref> we demonstrate that our meta-learning techniques outperform the voting-based and statistical techniques in terms of prediction accuracy. Here we extend our arbiter tree scheme to a related but different scheme called combiner tree. <p> The prediction accuracy on a separate test set is our primary comparison measure. The different strategies were run on the two data sets with the two learning algorithms. The results from the arbiter trees are plotted in Figure 3, which were reported in an earlier study <ref> (Chan & Stolfo 1995) </ref> and are included here for comparison purposes. The results from the combiner trees are in Figure 4. <p> Higher order trees are usually less accurate. This is probably due to the decrease in opportunities for correcting predictions when the height of the tree decreases. The relatively poor performance of one-level (non-tree) meta-learning techniques compared to the multi-level (tree) schemes in our earlier study <ref> (Chan & Stolfo 1995) </ref> also provides support for this observation. Increasing the size of the meta-level training sets improves the accuracy of the learned trees, a likely result from the simple observation that more data are available for training. <p> The experimental data convincingly demonstrate that doubling the training set size of the meta-level partitions is sufficient to maintain the same level of accuracy as the global classifier, and indeed may boost accuracy as well. Concluding Remarks In a previous study <ref> (Chan & Stolfo 1995) </ref> we demonstrated that the meta-learning strategies outperform the voting-based and statistical techniques reported in the literature. We also showed that the arbiter tree approach is viable in sustaining the same level of accuracy as the global classifier learned from the entire data set.
Reference: <author> Craven, M., and Shavlik, J. </author> <year> 1993. </year> <title> Learning to represent codons: A challenge problem for constructive induction. </title> <booktitle> In Proc. IJCAI-93, </booktitle> <pages> 1319-1324. </pages>
Reference: <author> Quinlan, J. R. </author> <year> 1986. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1 </booktitle> <pages> 81-106. </pages>
Reference-contexts: Experiments and Results Two inductive learning algorithms were used in our experiments reported here. ID3 <ref> (Quinlan 1986) </ref> and CART (Breiman et al. 1984) were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana 1991). They are both decision tree learning algorithms that require all training examples to be resident in main memory. Two data sets were used in our studies.
Reference: <author> Towell, G.; Shavlik, J.; and Noordewier, M. </author> <year> 1990. </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proc. AAAI-90, </booktitle> <pages> 861-866. </pages>
Reference: <author> Wolpert, D. </author> <year> 1992. </year> <title> Stacked generalization. </title> <booktitle> Neural Networks 5 </booktitle> <pages> 241-259. </pages>
References-found: 9

