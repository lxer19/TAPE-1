URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/intentions-multiagent-mcc.ps
Refering-URL: http://www.csc.ncsu.edu/faculty/mpsingh/papers/mas/
Root-URL: http://www.csc.ncsu.edu
Email: msingh@mcc.com  
Phone: (512) 338-3431  
Title: Intentions for Multiagent Systems  
Author: Munindar P. Singh 
Note: Copyright c fl1993 Microelectronics and Computer Technology Corporation. All Rights Reserved. Shareholders and Associates of MCC may reproduce and distribute these materials for internal purposes by retaining MCC's copyright notice, proprietary legends, and markings on all complete and partial copies.  
Address: 3500 West Balcones Center Drive Austin, TX 78759, USA  
Affiliation: MCC Nonconfidential Microelectronics and Computer Technology Corporation Information Systems Division  
Abstract: MCC Technical Report Number KBNL-086-93 
Abstract-found: 1
Intro-found: 1
Reference: [ Agre & Chapman, 1987 ] <author> Agre, Philip and Chapman, David; 1987. Pengi: </author> <title> An implementation of a theory of activity. </title> <booktitle> In AAAI. </booktitle> <pages> 268-272. </pages>
Reference-contexts: Traditional theories identify a group with the set of its members. They are also committed to a plan-based view of intelligence, a view that has come under much criticism recently <ref> [ Agre & Chapman, 1987 ] </ref> . The arguments of Agre and Chapman include the following: (1) Symbolic plans are expensive to represent and interpret|many intelligent agents would in fact not be able to use them because of their limitations. (2) Symbolic plans must of necessity be incomplete. <p> Further desiderata for a theory of group intentions are the following. A theory of intentions should not be committed to a plan-based architecture of intelligent agency, since intelligence is not solely a matter of explicitly representing and interpreting symbolic structures, or at least not necessarily so <ref> [ Agre & Chapman, 1987 ] </ref> ; it should, however, be compatible with a plan-based architecture. A useful theory would accommodate the idea of situated action and also consider the interactions among a group's members as they emerge from collective action.
Reference: [ Austin, 1962 ] <author> Austin, John L.; </author> <year> 1962. </year> <title> How to do Things with Words. </title> <publisher> Clarendon, Oxford, </publisher> <address> UK. </address>
Reference-contexts: Strategic Interactions: I define strategic interactions as the abstract interactions among the members of groups that are described at the level of their strategies. An important subclass of such interactions involves illocutionary acts between agents, e.g., assertions, commands and promises <ref> [ Austin, 1962 ] </ref> .
Reference: [ Barwise & Perry, 1983 ] <author> Barwise, Jon and Perry, </author> <title> John; 1983. Situations and Attitudes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: [ Benda et al., 1986 ] <author> Benda, Miroslav; Jaganathan, V.; and Dodhiawala, </author> <title> Rajendra; 1986. On optimal cooperation of knowledge sources. </title> <type> Technical report, </type> <institution> Boeing Advanced Technology Center, Boeing Computer Services, </institution> <address> Seattle, WA. </address> <month> 28 </month>
Reference: [ Bratman, 1987 ] <author> Bratman, Michael E.; </author> <year> 1987. </year> <title> Intention, Plans, and Practical Reason. </title> <publisher> Har--vard University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: However, it is not always appropriate, since we often want to talk of intentions in a sense that gives importance to a psychological interpretation. In such a sense, an intention of an agent may have consequences that are not intended <ref> [ Bratman, 1987, p. 140 ] </ref> . This inference can be prevented by including a direct notion of what agents (and groups) intend. The new predicate, `intends-for,' tells us which strategy an agent has and what condition that strategy is meant to achieve.
Reference: [ Chellas, 1980 ] <author> Chellas, Brian F.; </author> <year> 1980. </year> <title> Modal Logic. </title> <publisher> Cambridge University Press, </publisher> <address> New York, NY. </address>
Reference-contexts: Statements of fact (including statements of what a given agent intends) are evaluated with respect to a formal "model" in which different possible states of the world are involved (e.g., see <ref> [ Chellas, 1980, pp. 34-35 ] </ref> ). Whether the statement "it is raining" is true in the model or not depends only on the state of the world relative to which this statement is evaluated, not the beliefs of any agent. <p> Such models are used in a number of formal theories, e.g., those of [ Fischer & Ladner, 1979 ] , [ Halpern & Moses, 1987 ] and this paper (see <ref> [ Chellas, 1980 ] </ref> for a textbook level introduction). This idea is also given importance in other "naturalist" frameworks, e.g., the one of Barwise and Perry [ 1983 ] .
Reference: [ Cohen & Levesque, 1988 ] <author> Cohen, Philip R. and Levesque, Hector J.; </author> <year> 1988. </year> <title> On acting together: </title> <booktitle> Joint intentions for intelligent agents. In Workshop on Distributed Artificial Intelligence. </booktitle>
Reference: [ Davis & Smith, 1983 ] <author> Davis, Randall and Smith, Reid G.; </author> <year> 1983. </year> <title> Negotiation as a metaphor for distributed problem solving. </title> <booktitle> Artificial Intelligence 20 </booktitle> <pages> 63-109. </pages> <note> Reprinted in Readings in Distributed Artificial Intelligence, </note> <editor> A. H. Bond and L. Gasser, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Examples of this subclass are the following: (1) in a football team, the receivers run the patterns that the quarterback asks them to (i.e., the receivers obey the quarterback's commands); and (2) in the contract net of <ref> [ Davis & Smith, 1983 ] </ref> , one can say the contractors promise to do the given task for a certain price by bidding on it. Another subclass of strategic interactions involves the establishment of various conditions in the world by some members' strategies that other members' strategies rely on.
Reference: [ Demazeau & Muller, 1990 ] <editor> Demazeau, Y. and Muller, J-P., editors. </editor> <booktitle> Decentralized Artificial Intelligence, </booktitle> <address> Amsterdam, </address> <publisher> Holland. Elsevier Science Publishers B.V. / North-Holland. </publisher>
Reference: [ Dennett, 1987 ] <author> Dennett, Daniel C.; </author> <year> 1987. </year> <title> The Intentional Stance. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: I shall take it for granted in the sequel that the importance of folk psychological notions such as belief and intention to AI science and engineering is established <ref> [ Dennett, 1987; McCarthy, 1979 ] </ref> .
Reference: [ Durfee & Montgomery, 1989 ] <author> Durfee, Edmund H. and Montgomery, Thomas A.; </author> <year> 1989. </year> <title> MICE: a flexible testbed for intelligent coordination experiments. </title> <booktitle> In Proc. 9th Workshop on Distributed Artificial Intelligence. </booktitle> <pages> 25-40. </pages>
Reference-contexts: I now apply it to the analysis of a well-known problem in multiagent systems in AI: the pursuit problem. This problem was introduced by Benda et al. [ 1986 ] and has been extensively studied by others <ref> [ Durfee & Montgomery, 1989; Stephens & Merx, 1989 ] </ref> . This problem has been analyzed from several different perspectives. Here my aim is simply to analyze the intentions of the team of Blue agents in terms of the intentions of the individual Blue agents.
Reference: [ Emerson, 1990 ] <author> Emerson, E. A.; </author> <year> 1990. </year> <title> Temporal and modal logic. </title> <editor> In Leeuwen, J.van, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, volume B. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands. </address>
Reference-contexts: Let M x j be the model that results by substituting f x j for f in model M . The satisfaction conditions for the temporal operators too are adapted from those in <ref> [ Emerson, 1990 ] </ref> . Formally, we have the following definitions: 1. M j= w;t (x 1 ; : : : ; x n ) iff hw; ti 2 [[ ]](f w;t (x 1 ); : : : ; f w;t (x n )) 3.
Reference: [ Fagin & Halpern, 1988 ] <author> Fagin, Ronald and Halpern, Joseph Y.; </author> <year> 1988. </year> <title> Belief, awareness, and limited reasoning. </title> <booktitle> Artificial Intelligence 34 </booktitle> <pages> 39-76. </pages>
Reference: [ Fischer & Immerman, 1986 ] <author> Fischer, Michael J. and Immerman, </author> <title> Neil; 1986. Foundations of knowledge for distributed systems. </title> <editor> In Halpern, Joseph Y., editor, </editor> <booktitle> Theoretical Aspects of Reasoning About Knowledge. </booktitle> <pages> 171-185. </pages>
Reference-contexts: This is important because mutual beliefs are impossible to achieve in most realistic scenarios, e.g., where communication delay is not bounded or the communication channels are not reliable <ref> [ Fischer & Immerman, 1986; Halpern & Moses, 1987 ] </ref> . The theory developed in the paper may be used in the design and analysis of multiagent systems in the following way. <p> Not only is the mutual belief requirement computationally demanding (so that agents may reason about others to arbitrary nesting of beliefs), it also requires a lot of communication among the members (for the mutual beliefs to be established). In general, mutual beliefs are impossible to establish using asynchronous communication <ref> [ Fischer & Immerman, 1986; Halpern & Moses, 1987 ] </ref> . In practice, they can be established only if certain conventions are stipulated. Most importantly, however, the mutual belief requirement makes these analyses applicable to a concept different, and more complex, than simple intentions.
Reference: [ Fischer & Ladner, 1979 ] <author> Fischer, Michael J. and Ladner, Richard E.; </author> <year> 1979. </year> <title> The propositional dynamic logic of regular programs. </title> <journal> Journal of Computer and System Sciences 18(2) </journal> <pages> 194-211. </pages>
Reference-contexts: It is a standard claim of pragmatism, the philosophy behind the formal logic and semantics that is based on "possible worlds" models [ Stalnaker, 1984, pp. 15-19 ] . Such models are used in a number of formal theories, e.g., those of <ref> [ Fischer & Ladner, 1979 ] </ref> , [ Halpern & Moses, 1987 ] and this paper (see [ Chellas, 1980 ] for a textbook level introduction). This idea is also given importance in other "naturalist" frameworks, e.g., the one of Barwise and Perry [ 1983 ] . <p> The formal definition of strategies used here is derived from regular programs in Dynamic Logic| a standard notation for describing programs and computations in theoretical Computer Science <ref> [ Fischer & Ladner, 1979 ] </ref> . The idea of using strategies such as these for describing intelligent agents can be traced back to McCarthy and Hayes [ 1969 ] .
Reference: [ Georgeff, 1987 ] <author> Georgeff, Michael P.; </author> <year> 1987. </year> <title> Planning. </title> <editor> In Traub, J. F., editor, </editor> <booktitle> Annual Review of Computer Science, </booktitle> <volume> Vol 2. </volume> <publisher> Annual Reviews Inc., </publisher> <address> Palo Alto, CA. </address>
Reference: [ Grosz & Sidner, 1990 ] <author> Grosz, Barbara and Sidner, </author> <title> Candace; 1990. Plans for discourse. </title> <editor> In Cohen, P.; Morgan, J.; and Pollack, M., editors, </editor> <title> SDF Benchmark Series: Intentions in Communication. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: [ Halpern & Moses, 1987 ] <author> Halpern, Joseph Y. and Moses, Yoram O.; </author> <year> 1987. </year> <title> Knowledge and common knowledge in a distributed environment (revised version). </title> <type> Technical Report RJ 4421, </type> <institution> IBM. </institution> <month> 29 </month>
Reference-contexts: This is important because mutual beliefs are impossible to achieve in most realistic scenarios, e.g., where communication delay is not bounded or the communication channels are not reliable <ref> [ Fischer & Immerman, 1986; Halpern & Moses, 1987 ] </ref> . The theory developed in the paper may be used in the design and analysis of multiagent systems in the following way. <p> Not only is the mutual belief requirement computationally demanding (so that agents may reason about others to arbitrary nesting of beliefs), it also requires a lot of communication among the members (for the mutual beliefs to be established). In general, mutual beliefs are impossible to establish using asynchronous communication <ref> [ Fischer & Immerman, 1986; Halpern & Moses, 1987 ] </ref> . In practice, they can be established only if certain conventions are stipulated. Most importantly, however, the mutual belief requirement makes these analyses applicable to a concept different, and more complex, than simple intentions. <p> It is a standard claim of pragmatism, the philosophy behind the formal logic and semantics that is based on "possible worlds" models [ Stalnaker, 1984, pp. 15-19 ] . Such models are used in a number of formal theories, e.g., those of [ Fischer & Ladner, 1979 ] , <ref> [ Halpern & Moses, 1987 ] </ref> and this paper (see [ Chellas, 1980 ] for a textbook level introduction). This idea is also given importance in other "naturalist" frameworks, e.g., the one of Barwise and Perry [ 1983 ] .
Reference: [ Hamblin, 1987 ] <author> Hamblin, C. L.; </author> <year> 1987. </year> <title> Imperatives. </title> <publisher> Basil Blackwell Ltd., Oxford, </publisher> <address> UK. </address>
Reference: [ Hewitt, 1988 ] <author> Hewitt, </author> <title> Carl; 1988. Organizational knowledge processing. </title> <booktitle> In Workshop on Distributed Artificial Intelligence. </booktitle>
Reference: [ Konolige, 1982 ] <author> Konolige, Kurt G.; </author> <year> 1982. </year> <title> A first-order formalism of knowledge and action for multi-agent planning. </title> <editor> In Hayes, J. E.; Mitchie, D.; and Pao, Y., editors, </editor> <booktitle> Machine Intelligence 10. </booktitle> <publisher> Ellis Horwood Ltd., </publisher> <address> Chichester, UK. </address> <pages> 41-73. </pages>
Reference: [ Kozen & Tiurzyn, 1990 ] <author> Kozen, Dexter and Tiurzyn, </author> <title> Jerzy; 1990. Logics of program. </title> <editor> In Leeuwen, J.van, editor, </editor> <booktitle> Handbook of Theoretical Computer Science. </booktitle> <publisher> North-Holland Publishing Company, </publisher> <address> Amsterdam, The Netherlands. </address>
Reference-contexts: It recursively partitions the task of modeling such systems into the tasks of modeling their internal structure and the strategies of their members. The strategies of members are defined simply using a notation related to regular programs <ref> [ Kozen & Tiurzyn, 1990 ] </ref> and allow us to succinctly describe the relevant aspects of the agents' design (see x5). These strategies yield the intentions of the agents in a simple and direct way. This turns out to be quite important. <p> It can easily be seen that relative to the standard semantics for the constructs introduced above (e.g., see <ref> [ Kozen & Tiurzyn, 1990 ] </ref> ), `Y ' is equivalent to `current (Y ); rest (Y ).' Another obvious consequence of this is that `current (Y )' is always of the form `skip' or `do (A)' or `wait (A).' The syntax of strategies as proposed here is meant to be
Reference: [ McArthur, 1988 ] <author> McArthur, Gregory L.; </author> <year> 1988. </year> <title> Reasoning about knowledge and belief: A survey. </title> <booktitle> Computational Intelligence 4 </booktitle> <pages> 223-243. </pages>
Reference-contexts: It is assumed throughout that operators for quoting and dequoting can be inserted where 12 necessary. This simplifies the notation considerably. Using these operators, we can con-vert between syntactic and semantic objects with ease|this idea is well-known and simple, though it is notationally cumbersome <ref> [ McArthur, 1988, p. 229 ] </ref> . The novel feature of L is that it allows agents to be defined out of predefined agents. Recall that the class of agents in the model, A fl , is closed under group formation.
Reference: [ McCarthy & Hayes, 1969 ] <author> McCarthy, J. and Hayes, P. J.; </author> <year> 1969. </year> <title> Some philosophical problems from the standpoint of artificial intelligence. </title> <booktitle> In Machine Intelligence 4. </booktitle> <publisher> American Elsevier. </publisher>
Reference: [ McCarthy, 1979 ] <author> McCarthy, </author> <title> John; 1979. Ascribing mental qualities to machines. </title> <editor> In Ringle, Martin, editor, </editor> <booktitle> Philosophical Perspectives in Artificial Intelligence. </booktitle> <publisher> Harvester Press. </publisher> <pages> Page nos. </pages> <note> from a revised version, issued as a report in 1987. </note>
Reference-contexts: I shall take it for granted in the sequel that the importance of folk psychological notions such as belief and intention to AI science and engineering is established <ref> [ Dennett, 1987; McCarthy, 1979 ] </ref> .
Reference: [ Rosenschein, 1985 ] <author> Rosenschein, Stanley J.; </author> <year> 1985. </year> <title> Formal theories of knowledge in AI and robotics. New Generation Computing 3(4). </title>
Reference: [ Singh & Asher, 1992 ] <author> Singh, Munindar P. and Asher, Nicholas M.; </author> <year> 1992. </year> <title> A logic of intentions and beliefs. </title> <journal> Journal of Philosophical Logic. </journal> <note> In press. </note>
Reference-contexts: The ideas of this paper can also be combined with the approach of <ref> [ Singh & Asher, 1992 ] </ref> , but since that approach is quite complex and largely orthogonal to the issues addressed in this paper, that is not done here.
Reference: [ Singh, 1990a ] <author> Singh, Munindar P.; </author> <year> 1990a. </year> <title> Group intentions. </title> <booktitle> In 10th Workshop on Distributed Artificial Intelligence. </booktitle>
Reference: [ Singh, 1990b ] <author> Singh, Munindar P.; </author> <year> 1990b. </year> <title> Towards a theory of situated know-how. </title> <booktitle> In 9th European Conference on Artificial Intelligence. </booktitle>
Reference-contexts: While this section is essential for a thorough understanding of the definitions to follow, it may safely be skimmed over on a first reading. 5.1 The Formal Model The formal model here is related to the ones in <ref> [ Singh, 1990b; Singh, 1991a ] </ref> ; an important difference is that the interpretation assigns strategies (that are `had'), conditions (that are `believed') and pairs of strategies and conditions (in which the strategies are `intended-for' the conditions) to agents. <p> These definitions have been borrowed from previous papers <ref> [ Singh, 1990b; Singh, 1991b ] </ref> . I now define a strategy, Y , recursively as follows. 0. skip: the empty strategy 1. do (A): a condition to be achieved 2. wait (A): a condition to be awaited, (for synchronization with other events) 3.
Reference: [ Singh, 1991a ] <author> Singh, Munindar P.; </author> <year> 1991a. </year> <title> Group ability and structure. </title> <editor> In Demazeau, Y. and Muller, J.-P., editors, </editor> <booktitle> Decentralized Artificial Intelligence, </booktitle> <volume> Volume 2. </volume> <publisher> Elsevier Science Publishers B.V. / North-Holland, Amsterdam, Holland. </publisher> <pages> 127-145. </pages>
Reference-contexts: It is convenient to refer to multiagent systems as groups. In recent work, I have developed a formal theory of the ability of a group (as ascribed objectively to it) that accounts for its internal structure <ref> [ Singh, 1991a ] </ref> . Here I plan to extend and modify those ideas and motivate a formal theory of the intentions of a group of agents. <p> The strategy of a group describes what it is doing. This strategy is itself seen as the set of strategies of its members. The structure of a group is captured in terms of the interactions of its members as they follow their respective strategies <ref> [ Singh, 1991a ] </ref> . As a consequence, this theory, unlike those of Grosz and Sidner [ 1990 ] 2 and Cohen and Levesque [ 1988 ] , does not require mutual beliefs among the members of a group. <p> In x4, I present the major methodological motivations behind this work, a clarification of which is useful in placing this work better in the context of multiagent systems in AI. In x5, I review some relevant parts of a previous paper <ref> [ Singh, 1991a ] </ref> extending it as needed for the purposes of this paper. <p> E.g., a free market economy may be ascribed the intention of moving goods efficiently from producers to consumers, while all its participants intend as individuals is to make a profit. Two observations, made in <ref> [ Singh, 1991a ] </ref> , are also in order here. 1. External Monolithicity: A group (e.g., a sports team) may be considered as a single unstructured monolithic agent from without; i.e., groups are "Hobbesian corporate persons," in Hamblin's term [ 1987, pp. 60, 240 ] . <p> While this section is essential for a thorough understanding of the definitions to follow, it may safely be skimmed over on a first reading. 5.1 The Formal Model The formal model here is related to the ones in <ref> [ Singh, 1990b; Singh, 1991a ] </ref> ; an important difference is that the interpretation assigns strategies (that are `had'), conditions (that are `believed') and pairs of strategies and conditions (in which the strategies are `intended-for' the conditions) to agents. <p> These are captured by the predicate `leads-to' and are the primary external criterion for distinguishing among the intentions of different groups. One important difference between the approach of this paper (as it is embodied in the definition of `performs') and that of <ref> [ Singh, 1991a ] </ref> (as it is embodied in the definition of `follows' given there) is that the former considers all sequences of actions that the group can do to succeed with its strategy, while the latter considers only those scenarios over which the group actually forces the success of its
Reference: [ Singh, 1991b ] <author> Singh, Munindar P.; </author> <year> 1991b. </year> <title> A logic of situated know-how. </title> <booktitle> In National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: I assume that [[ ]] respects B; i.e., a 2 B w;t (x). The following coherence conditions on models must be imposed for them to be intuitively reasonable <ref> [ Singh, 1991b ] </ref> : (1) at any time, an action can be done in at most one way on any given scenario; (2) subscenarios are uniquely identified by the times over which they stretch; i.e., the scenario used to refer to them is not important; (3) there is always a <p> These definitions have been borrowed from previous papers <ref> [ Singh, 1990b; Singh, 1991b ] </ref> . I now define a strategy, Y , recursively as follows. 0. skip: the empty strategy 1. do (A): a condition to be achieved 2. wait (A): a condition to be awaited, (for synchronization with other events) 3.
Reference: [ Singh, 1991c ] <author> Singh, Munindar P.; </author> <year> 1991c. </year> <title> Towards a formal theory of communication for multiagent systems. </title> <booktitle> In International Joint Conference on Artificial Intelligence (IJCAI). </booktitle>
Reference-contexts: The intentions of agents are needed not only to predict or explain their behavior abstractly, but also to give a formal semantics for different kinds of communication among them <ref> [ Singh, 1991c ] </ref> . In x2, I present some observations and intuitions about multiagent systems construed abstractly, especially their internal structure, the actions they perform and the intentions they have.
Reference: [ Stalnaker, 1984 ] <author> Stalnaker, Robert C.; </author> <year> 1984. </year> <title> Inquiry. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: The idea that claims of intentions and beliefs ought somehow to be validated by the agent's actions is not novel to this paper. It is a standard claim of pragmatism, the philosophy behind the formal logic and semantics that is based on "possible worlds" models <ref> [ Stalnaker, 1984, pp. 15-19 ] </ref> . Such models are used in a number of formal theories, e.g., those of [ Fischer & Ladner, 1979 ] , [ Halpern & Moses, 1987 ] and this paper (see [ Chellas, 1980 ] for a textbook level introduction). <p> This task is not attempted in this paper. For the single agent case, and in the absence of explicit consideration of intentions, this problem has been addressed by others <ref> [ Stalnaker, 1984 ] </ref> ; further work is needed to make the connection with internal structure explicit and to explore the relativity of beliefs to intention ascriptions.
Reference: [ Stephens & Merx, 1989 ] <author> Stephens, Larry M. and Merx, </author> <month> Matthais; </month> <year> 1989. </year> <title> Agent organization as an effector of DAI system performance. </title> <booktitle> In 9th Workshop on Distributed Artificial Intelligence. </booktitle> <pages> 30 </pages>
Reference-contexts: I now apply it to the analysis of a well-known problem in multiagent systems in AI: the pursuit problem. This problem was introduced by Benda et al. [ 1986 ] and has been extensively studied by others <ref> [ Durfee & Montgomery, 1989; Stephens & Merx, 1989 ] </ref> . This problem has been analyzed from several different perspectives. Here my aim is simply to analyze the intentions of the team of Blue agents in terms of the intentions of the individual Blue agents.
References-found: 34

