URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/94-23.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Title: Linear-Time Algorithms in Memory Hierarchies  
Author: Kenneth W. Regan 
Date: May 1994  
Affiliation: State University of New York at Buffalo  
Abstract: This paper studies linear-time algorithms on a hierarchical memory model called Block Move (BM), which extends the Block Transfer (BT) model of Aggarwal, Chandra, and Snir, and which is more stringent than a pipelining model studied recently by Luccio and Pagli. Upper and lower bounds are shown for various data-processing primitives, and some interesting open problems are given.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggarwal, B. Alpern, A. Chandra, and M. Snir. </author> <title> A model for hierarchical memory. </title> <booktitle> In Proc. 19th Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 305-314, </pages> <year> 1987. </year>
Reference-contexts: Cook [5, 6] proposed replacing the usual unit-cost RAM measure by the log-cost criterion, by which an operation that reads an integer i stored at address a is charged log i + log a time units. Aggarwal, Alpern, Chandra, and Snir <ref> [1] </ref> went further by introducing a parameter : N ! N called a memory access cost function, such that the above operation costs 1 + (a) time units. Note that this still treats register contents i as having unit size.
Reference: [2] <author> A. Aggarwal, A. Chandra, and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In Proc. 28th Annual IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 204-216, </pages> <year> 1987. </year>
Reference-contexts: Note that this still treats register contents i as having unit size. Besides (a) = log a, they studied the parameters d (a) = a 1=d , which model the asymptotic latency of memory laid out on a d-dimensional grid. 1 To allow for pipelining, Aggarwal, Chandra, and Snir <ref> [2] </ref> defined the Block Transfer (BT) model with a special instruction copy [a m : : : a] into [b m : : : b]; which is charged m + maxf (a); (b) g time units. <p> E-mail: regan@cs.buffalo.edu; tel. (716) 645-3189. Part of this work was supported by NSF Grant CCR-9011248 1 The cited authors write f in place of and ff in place of 1=d. 1 (n loglog n) under d (for all d &gt; 1), and (n log n) under 1 <ref> [2] </ref>. Hence virtually no algorithms can be called linear-time on the BT. Luccio and Pagli [8] argued that the benefits of pipelining need not be limited to individual block-transfers. <p> This leaves two interesting problems: (1) Can lists with large elements be merged in linear 1 -time? (2) Can lists with small elements be merged in linear d -time, for any d 1? The results on sorting in <ref> [2] </ref> suggest a negative answer to (2) for all d, but disallow bit operations on data in block transfers. <p> Theorem 5.1 For any fixed set S of block-move operations, e 1 (n) = fi (n log n), and for all d &gt; 1, e d (n) = fi (n loglog n). Proof Sketch. The upper bounds follow as in <ref> [2] </ref>, and need only S 0 , S 1 , and copy. <p> The lower bounds intuitively hold because an operation with max address a can be specified in O (log a) bits, but is charged (a) log a time. (Since there is no disparity for = log , no matching lower bound of (n log fl n), analogous to that in <ref> [2] </ref>, is given for log .) Details of the lower bound for d = 1: Given y, let P be a straight-line program such that P (0 n ) outputs y, and let ng (n) be the 1 -time for this. Note that P itself is a description of y. <p> Since there exist (many) strings y 2 n such that the conditional Kolmogorov complexity K (yj0 n ) is n, it follows that g (n) must be (log n). Corollary 5.2 (compare <ref> [2, 3] </ref>) There exist permutations of small-element lists that cannot be realized in linear d -time, for any d. Proof. Let N = n log n be the bit-length of the list. Since there are n! = 2 fi (n log n) permutations, some have Kolmogorov complexity fi (N ).
Reference: [3] <author> A. Aggarwal and J. Vitter. </author> <title> The input-output complexity of sorting and related problems. </title> <journal> Comm. ACM, </journal> <volume> 31 </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: We give such a model, and investigate fundamental list-processing operations at the bit level, under the memory access cost functions d . Then we give a Kolmogorov-complexity technique that may impact on what Aggarwal and Vitter <ref> [3] </ref> call the "challenging open problem" of extending the lower-bound results of theirs and other papers to models that "allow arbitrary bit-manipulations and dissections of records." 2 The Block Move (BM) Model The BM makes three important changes to the BT. <p> The BM under d is more realistically constrained than the LPM under log-cost; it compromises between fixing the size of linear blocks as in <ref> [3] </ref> and having unrestricted pipelining as in [8]. 2 3 Fundamental List Operations Non-negative integers are encoded over = f 0; 1 g in standard dyadic notation, with the least significant bit first ("leftmost"). Lists whose elements are nonempty 0-1 strings are encoded using boundary markers #. <p> Since there exist (many) strings y 2 n such that the conditional Kolmogorov complexity K (yj0 n ) is n, it follows that g (n) must be (log n). Corollary 5.2 (compare <ref> [2, 3] </ref>) There exist permutations of small-element lists that cannot be realized in linear d -time, for any d. Proof. Let N = n log n be the bit-length of the list. Since there are n! = 2 fi (n log n) permutations, some have Kolmogorov complexity fi (N ).
Reference: [4] <author> G. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: Moreover, there is a straightforward extension to segmented prefix-sum and other "scan" operations. By results of Blelloch <ref> [4] </ref> on expressing many other operations in terms of prefix-sum and prefix-max, this is enough to prove that the BM efficiently simulates his integer-based scan model, except that each scan operation takes O (log n) block moves. 4 Merging Consider normal lists ~x and ~y of size n = rm whose
Reference: [5] <author> S. Cook. </author> <title> Linear time simulation of deterministic two-way pushdown automata. </title> <booktitle> In Proceedings, IFIP '71, </booktitle> <pages> pages 75-80. </pages> <publisher> North-Holland, </publisher> <year> 1971. </year>
Reference-contexts: 1 Introduction Recent years have seen marked dissatisfaction with the computational realism of the classic machine models, such as Turing machines or the standard integer RAM (see [13, 12]). Many algorithms that theoretically run in linear time on the RAM scale non-linearly when it comes time to implement them. Cook <ref> [5, 6] </ref> proposed replacing the usual unit-cost RAM measure by the log-cost criterion, by which an operation that reads an integer i stored at address a is charged log i + log a time units.
Reference: [6] <author> S. Cook and R. Reckhow. </author> <title> Time bounded random access machines. </title> <journal> J. Comp. Sys. Sci., </journal> <volume> 7 </volume> <pages> 354-375, </pages> <year> 1973. </year>
Reference-contexts: 1 Introduction Recent years have seen marked dissatisfaction with the computational realism of the classic machine models, such as Turing machines or the standard integer RAM (see [13, 12]). Many algorithms that theoretically run in linear time on the RAM scale non-linearly when it comes time to implement them. Cook <ref> [5, 6] </ref> proposed replacing the usual unit-cost RAM measure by the log-cost criterion, by which an operation that reads an integer i stored at address a is charged log i + log a time units.
Reference: [7] <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1979. </year>
Reference-contexts: Second, any finite transduction S, not just copy, can be applied to the data stream in a block move S [a 1 : : : b 1 ] into [a 2 : : : b 2 ]: Formally S is a deterministic generalized sequential machine (DGSM), as defined in <ref> [7] </ref>. If z is the string formed by the characters in cells a 1 : : : b 1 , then S (z) is written into the block a 2 : : : b 2 beginning at a 2 .
Reference: [8] <author> F. Luccio and L. Pagli. </author> <title> A model of sequential computation with pipelined access to memory. </title> <journal> Math. Sys. Thy., </journal> <volume> 26 </volume> <pages> 343-356, </pages> <year> 1993. </year>
Reference-contexts: Hence virtually no algorithms can be called linear-time on the BT. Luccio and Pagli <ref> [8] </ref> argued that the benefits of pipelining need not be limited to individual block-transfers. Their LPM model is a RAM with m = n O (1) registers available on inputs of size n, and O (log m) delay for accessing any register, irrespective of its address. <p> The BM under d is more realistically constrained than the LPM under log-cost; it compromises between fixing the size of linear blocks as in [3] and having unrestricted pipelining as in <ref> [8] </ref>. 2 3 Fundamental List Operations Non-negative integers are encoded over = f 0; 1 g in standard dyadic notation, with the least significant bit first ("leftmost"). Lists whose elements are nonempty 0-1 strings are encoded using boundary markers #.
Reference: [9] <author> Y. Mansour, N. Nisan, and P. Tiwari. </author> <title> The computational complexity of universal hashing. </title> <journal> Theor. Comp. Sci., </journal> <volume> 107 </volume> <pages> 121-133, </pages> <year> 1993. </year> <month> 7 </month>
Reference-contexts: We suspect, eyeing the time-space tradeoff arguments of Mansour, Nisan, and Tiwari <ref> [9] </ref>, that this should lead to non-linear lower bounds on d -time for natural functions such as sorting, string convolutions, FFTs, and universal hashing. This may impact on their conjecture that these functions require non-linear time on a Turing machine.
Reference: [10] <author> K. Regan. </author> <title> Machine models and linear time complexity. </title> <journal> SIGACT News, </journal> <volume> 24:5--15, </volume> <month> October </month> <year> 1993. </year> <title> Guest column, </title> <editor> L. Hemachandra ed., </editor> <booktitle> "Compelxity Theory Column". </booktitle>
Reference-contexts: Several particular machine forms of the BM are given in <ref> [10, 11] </ref>. As shown in [11], the BM is very robust : Under any d function (d 1 and rational), these forms all simulate each other up to linear time, not just polynomial time.
Reference: [11] <author> K. Regan. </author> <title> Linear time and memory efficient computation, </title> <note> 1994. Revision of UB-CS-TR 92-28, accepted to SIAM J. Comput. </note>
Reference-contexts: Several particular machine forms of the BM are given in <ref> [10, 11] </ref>. As shown in [11], the BM is very robust : Under any d function (d 1 and rational), these forms all simulate each other up to linear time, not just polynomial time. <p> Several particular machine forms of the BM are given in [10, 11]. As shown in <ref> [11] </ref>, the BM is very robust : Under any d function (d 1 and rational), these forms all simulate each other up to linear time, not just polynomial time. Even machines allowed to violate the validity and strict-boundary conditions are simulated with constant-factor overhead by machines that observe them. <p> Proposition 3.3 Within the same asymptotic time bounds, the padding in Proposition 3.1 can be made leading or trailing in each list element. Proof. Lemma 6.1 (b) of <ref> [11] </ref> shows how two normal lists can be shu*ed in constant-many passes. (The idea is to make spare copies of both lists, overwrite the even elements of one copy and odd elements of the other by @ symbols, triple each @ symbol in one pass, and then overlay the four lists.)
Reference: [12] <author> P. van Emde Boas. </author> <title> Machine models and simulations. </title> <editor> In J. Van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science, </booktitle> <pages> pages 1-66. </pages> <publisher> Elsevier and MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Recent years have seen marked dissatisfaction with the computational realism of the classic machine models, such as Turing machines or the standard integer RAM (see <ref> [13, 12] </ref>). Many algorithms that theoretically run in linear time on the RAM scale non-linearly when it comes time to implement them.
Reference: [13] <author> K. Wagner and G. Wechsung. </author> <title> Computational Complexity. </title> <address> D. </address> <publisher> Reidel, </publisher> <year> 1986. </year> <month> 8 </month>
Reference-contexts: 1 Introduction Recent years have seen marked dissatisfaction with the computational realism of the classic machine models, such as Turing machines or the standard integer RAM (see <ref> [13, 12] </ref>). Many algorithms that theoretically run in linear time on the RAM scale non-linearly when it comes time to implement them.
References-found: 13

