URL: http://www.ai.mit.edu/people/cgdemarc/postscript/acl96.ps
Refering-URL: http://www.ai.mit.edu/people/cgdemarc/cgdemarc.html
Root-URL: 
Email: cgdemarc@ai.mit.edu  
Title: Linguistic Structure as Composition and Perturbation  
Author: Carl de Marcken 
Address: Square Cambridge, MA, 02139, USA  
Affiliation: 545 Technology  
Pubnum: MIT AI Laboratory, NE43-769  
Abstract: This paper discusses the problem of learning language from unprocessed text and speech signals, concentrating on the problem of learning a lexicon. In particular, it argues for a representation of language in which linguistic parameters like words are built by perturbing a composition of existing parameters. The power of this representation is demonstrated by several examples in text segmentation and compression, acquisition of a lexicon from raw speech, and the acquisition of mappings between text and artificial representations of mean ing.
Abstract-found: 1
Intro-found: 1
Reference: <author> J. K. Baker. </author> <year> 1979. </year> <title> Trainable grammars for speech recognition. </title> <booktitle> In Proceedings of the 97th Meeting of the Acoustical Society of America, </booktitle> <pages> pages 547-550. </pages>
Reference-contexts: If the composition operator makes use of context, then this framework extends naturally to a variation of stochastic context-free grammars in which composition corresponds to tree substitution and the inside-outside algorithm <ref> (Baker, 1979) </ref> is used for re-estimation. In particular, if each word is associated with a parent class, and these classes are permissible terminals, then "words" act as production rules.
Reference: <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss. </author> <year> 1970. </year> <title> A maximization technique occuring in the statistical analysis of probabilistic functions in Markov chains. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 41 </volume> <pages> 164-171. </pages>
Reference-contexts: Given a fixed lexicon, the expectation-maximization algorithm (Dempster et al., 1977) can be used to arrive at a (locally) optimal set of probabilities and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure <ref> (Baum et al., 1970) </ref> discussed in (Deligne and Bimbot, 1995). In general, however, the parsing and re-estimation involved in EM can be considerably more complicated.
Reference: <author> P. L. Brown, S. A. Della Pietra, V. J. Della Pietra, J. </author> <note> C. </note>
Reference: <author> Lai, and R. L. Mercer. </author> <year> 1992. </year> <title> An estimate of an upper bound for the entropy of english. </title> <journal> Computational Linguistics, </journal> <volume> 18(1) </volume> <pages> 31-40. </pages>
Reference: <author> G. Carroll and E. Charniak. </author> <year> 1992. </year> <title> Learning probabilistic dependency grammars from labeled text. </title> <booktitle> In Working Notes, Fall Symposium Series, AAAI, </booktitle> <pages> pages 25-31. </pages>
Reference: <author> T. A. Cartwright and M. R. Brent. </author> <year> 1994. </year> <title> Segmenting speech without a lexicon: Evidence for a bootstrapping model of lexical acquisition. </title> <booktitle> In Proc. of the 16th Annual Meeting of the Cognitive Science Society, </booktitle> <address> Hillsdale, New Jersey. </address>
Reference: <author> S. F. Chen. </author> <year> 1995. </year> <title> Bayesian grammar induction for language modeling. </title> <booktitle> In Proc. 32nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 228-235, </pages> <address> Cambridge, Massachusetts. </address>
Reference: <author> N. A. Chomsky. </author> <year> 1955. </year> <title> The Logical Structure of Linguistic Theory. </title> <publisher> Plenum Press, </publisher> <address> New York. </address>
Reference: <author> C. de Marcken. </author> <year> 1995a. </year> <title> Lexical heads, phrase structure and the induction of grammar. </title> <booktitle> In Third Workshop on Very Large Corpora, </booktitle> <address> Cambridge, Massachusetts. </address>
Reference: <author> C. de Marcken. </author> <year> 1995b. </year> <title> The unsupervised acquisition of a lexicon from continuous speech. Memo A.I. </title> <type> Memo 1558, </type> <institution> MIT Artificial Intelligence Lab., Cambridge, Massachusetts. </institution>
Reference-contexts: Similar heuristic approximations can be used to estimate the benefit of deleting words. In that case, a reasonable assumption is that if a word is deleted its representation replaces it everywhere. Again this is not necessarily correct, but serves adequately. 4 See <ref> (de Marcken, 1995b) </ref> for more detailed discussion of approximations. The actual schemes used in the tests discussed in this paper are slightly more complicated than those presented here. <p> The utterances are taken from dictated Wall Street Journal articles. The concatenation operator was used with phonemes as terminals. A second layer was added to the framework to map from phonemes to speech; these extensions are described in more detail in <ref> (de Marcken, 1995b) </ref>. The sound models for the phonemes were estimated independently on a separate corpus of hand-segmented speech. Although the phoneme models are extremely poor, many words are recognizable, and this is the first significant lexicon learned directly from spoken speech without supervision.
Reference: <author> S. Deligne and F. Bimbot. </author> <year> 1995. </year> <title> Language modeling by variable length sequences: Theoretical formulation and evaluation of multigrams. </title> <booktitle> In Proceedings of the International Conference on Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 169-172. </pages>
Reference-contexts: For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in <ref> (Deligne and Bimbot, 1995) </ref>. In general, however, the parsing and re-estimation involved in EM can be considerably more complicated. To update the structure of the lexicon, words can be added or deleted from it if this is predicted to reduce the description length of the input.
Reference: <author> A. P. Dempster, N. M. Liard, and D. B. Rubin. </author> <year> 1977. </year> <title> Maximum liklihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, B(39):1-38. </journal>
Reference-contexts: Given a fixed lexicon, the expectation-maximization algorithm <ref> (Dempster et al., 1977) </ref> can be used to arrive at a (locally) optimal set of probabilities and codelengths for the words in the lexicon. For composition by concatenation, the algorithm reduces to the special case of the Baum-Welch procedure (Baum et al., 1970) discussed in (Deligne and Bimbot, 1995).
Reference: <author> T. M. Ellison. </author> <year> 1992. </year> <title> The Machine Learning of Phonological Structure. </title> <type> Ph.D. thesis, </type> <institution> University of Western Australia. </institution>
Reference: <author> W. N. Francis and H. Kucera. </author> <year> 1982. </year> <title> Frequency analysis of English usage: lexicon and grammar. </title> <address> Houghton-Mi*in, Boston. </address>
Reference-contexts: Such patterns can be indistinguishable from desired ones. For example, in the Brown corpus <ref> (Francis and Kucera, 1982) </ref> scratching her nose occurs 5 times, a corpus-specific idiosyncrasy. This phrase has the same structure as the idiom kicking the bucket.
Reference: <author> Z. Harris. </author> <year> 1968. </year> <title> Mathematical Structure of Language. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> D. C. Olivier. </author> <year> 1968. </year> <title> Stochastic Grammars and Language Acquisition Mechanisms. </title> <type> Ph.D. thesis, </type> <institution> Harvard University, Cambridge, Massachusetts. </institution>
Reference: <author> F. Pereira and Y. Schabes. </author> <year> 1992. </year> <title> Inside-outside rees-timation from partially bracketed corpora. </title> <booktitle> In Proc. 29th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 128-135, </pages> <address> Berkeley, Califor-nia. </address>
Reference: <author> J. Rissanen. </author> <year> 1978. </year> <title> Modeling by shortest data description. </title> <journal> Automatica, </journal> <volume> 14 </volume> <pages> 465-471. </pages>
Reference: <author> J. Rissanen. </author> <year> 1989. </year> <title> Stochastic Complexity in Statistical Inquiry. </title> <publisher> World Scientific, Singapore. </publisher>
Reference: <author> R. J. Solomonoff. </author> <year> 1960. </year> <booktitle> The mechanization of linguistic learning. In Proceedings of the 2nd International Conference on Cybernetics, </booktitle> <pages> pages 180-193. </pages>
Reference: <author> A. Stolcke. </author> <year> 1994. </year> <title> Bayesian Learning of Probabilistic Language Models. </title> <type> Ph.D. thesis, </type> <institution> University of Califor-nia at Berkeley, Berkeley, </institution> <address> CA. </address>
Reference: <author> J. G. Wolff. </author> <year> 1982. </year> <title> Language acquisition, data compression and generalization. </title> <journal> Language and Communication, </journal> <volume> 2(1) </volume> <pages> 57-89. </pages>
Reference: <author> J. Ziv and A. Lempel. </author> <year> 1978. </year> <title> Compression of individual sequences by variable rate coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 24 </volume> <pages> 530-536. </pages>
Reference-contexts: This allows learning algorithms to fit detailed statistical properties of the data. This coding scheme is very similar to that found in popular dictionary-based compression schemes like LZ78 <ref> (Ziv and Lempel, 1978) </ref>. It is capable of compressing a sequence of identical characters of length n to size O (log n).
References-found: 23

