URL: ftp://publications.ai.mit.edu/ai-publications/1500-1999/AIM-1509.ps.Z
Refering-URL: http://www.ai.mit.edu/projects/cbcl/web-pis/jordan/homepage.html
Root-URL: 
Email: zoubin@psyche.mit.edu  
Title: Learning from incomplete data  
Author: Zoubin Ghahramani and Michael I. Jordan 
Note: This publication can be retrieved by anonymous ftp to publications.ai.mit.edu. Copyright c Massachusetts Institute of Technology, 1994  
Date: 1509 December 10,1994  108  
Affiliation: MASSACHUSETTS INSTITUTE OF TECHNOLOGY ARTIFICIAL INTELLIGENCE LABORATORY and CENTER FOR BIOLOGICAL AND COMPUTATIONAL LEARNING DEPARTMENT OF BRAIN AND COGNITIVE SCIENCES  
Pubnum: A.I. Memo No.  C.B.C.L. Paper No.  
Abstract: Real-world learning tasks often involve high-dimensional data sets with complex patterns of missing features. In this paper we review the problem of learning from incomplete data from two statistical perspectives|the likelihood-based and the Bayesian. The goal is two-fold: to place current neural network approaches to missing data within a statistical framework, and to describe a set of algorithms, derived from the likelihood-based framework, that handle clustering, classification, and function approximation from incomplete data in a principled and efficient manner. These algorithms are based on mixture modeling and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977)|both for the estimation of mixture components and for coping with the missing data. This report describes research done at the Center for Biological and Computational Learning and the Artificial Intelligence Laboratory of the Massachusetts Institute of Technology. Support for the Center is provided in part by a grant from the National Science Foundation under contract ASC-9217041. Support for the laboratory's artificial intelligence research is provided in part by the Advanced Research Projects Agency of the Department of Defense. The authors were supported in part by a grant from ATR Auditory and Visual Perception Research Laboratories, by a grant from Siemens Corporation, by grant IRI-9013991 from the National Science Foundation, and by grant N00014-90-J-1942 from the Office of Naval Research. Zoubin Ghahramani was supported by a grant from the McDonnell-Pew Foundation. Michael I. Jordan is a NSF Presidential Young Investigator. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bishop, C. M. </author> <year> (1994). </year> <title> Mixture density networks. </title> <type> Technical Report NCRG/4288, </type> <institution> Aston University, Birm-ingham, U.K. </institution>
Reference: <author> Box, G. E. P. and Tiao, G. C. </author> <year> (1973). </year> <title> Bayesian Inference in Statistical Analysis. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference-contexts: Thus, if we have a prior distribution for input values we can complete the missing data by sampling from this distribution. For complete data problems and simple models the judicious choice of conjugate priors for the parameters often allows analytic computation of their posterior distribution <ref> (Box and Tiao, 1973) </ref>.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., and Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <booktitle> Wadsworth International Group, </booktitle> <address> Belmont, CA. </address>
Reference-contexts: h ij , vary nonlinearly over the input space and can be viewed as corresponding to the output of a classifier that assigns to each point in the input space a probability of belonging to each Gaussian. 6 The least squares estimator has interesting relations to models such as CART <ref> (Breiman et al., 1984) </ref>, MARS (Friedman, 1991), and mixtures of experts (Jacobs et al., 1991; Jordan and Jacobs, 1994), in that the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface on each partition. This similarity has also been noted by Tresp et al. (1994).
Reference: <author> Buntine, W. and Weigend, A. </author> <year> (1991). </year> <title> Bayesian backpropagation. </title> <journal> Complex Systems, </journal> <volume> 5(6) </volume> <pages> 603-643. </pages>
Reference: <author> Cheeseman, P., Kelly, J., Self, M., Stutz, J., Taylor, W., and Freeman, D. </author> <year> (1988). </year> <title> Autoclass: A bayesian classification system. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <institution> The University of Michigan, </institution> <address> Ann Arbor. </address>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> J. Royal Statistical Society Series B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: In fact, the problem of estimating mixture densities can itself be viewed as a missing data problem (the "labels" for the component densities are miss ing) and an Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref> can be developed to handle both kinds of missing data. 4.1 The EM algorithm for mixture models This section outlines the estimation algorithm for finding the maximum likelihood parameters of a mixture model (Dempster et al., 1977). <p> "labels" for the component densities are miss ing) and an Expectation-Maximization (EM) algorithm <ref> (Dempster et al., 1977) </ref> can be developed to handle both kinds of missing data. 4.1 The EM algorithm for mixture models This section outlines the estimation algorithm for finding the maximum likelihood parameters of a mixture model (Dempster et al., 1977). We model the data X = fx i g N i=1 as being generated independently from a mixture density P (x i ) = j=1 where each component of the mixture is denoted ! j and parametrized by j . <p> Since z is unknown l c cannot be utilized directly, so we instead work with its expectation, denoted by Q (j k ). As shown by <ref> (Dempster et al., 1977) </ref>, l (jX ) can be maximized by iterating the following two steps: E-step: Q (j k ) = E [l c (jX ; Z)jX ; k ] M-step: k+1 = arg max The Expectation or E-step computes the expected complete data log likelihood, and the Maximization or <p> This yields a coupled set of nonlinear equations, 7 similar to the EM steps, which can be iterated to find the posterior mode of the parameters <ref> (Dempster et al., 1977) </ref>. To handle missing data the authors state that "for discrete attributes it can be shown that the correct procedure for treating an unknown value is equivalent to adding an `unknown' category to the value set" (p. 62).
Reference: <author> Duda, R. O. and Hart, P. E. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference: <author> Friedman, J. H. </author> <year> (1991). </year> <title> Multivariate adaptive regression splines. </title> <journal> The Annals of Statistics, </journal> <volume> 19 </volume> <pages> 1-141. </pages>
Reference-contexts: over the input space and can be viewed as corresponding to the output of a classifier that assigns to each point in the input space a probability of belonging to each Gaussian. 6 The least squares estimator has interesting relations to models such as CART (Breiman et al., 1984), MARS <ref> (Friedman, 1991) </ref>, and mixtures of experts (Jacobs et al., 1991; Jordan and Jacobs, 1994), in that the mixture of Gaussians competitively partitions the input space, and learns a linear regression surface on each partition. This similarity has also been noted by Tresp et al. (1994).
Reference: <author> Geman, S. and Geman, D. </author> <year> (1984). </year> <title> Stochastic relaxation, Gibbs distributions, and the Bayesian restoration of images. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 6 </volume> <pages> 721-741. </pages>
Reference-contexts: Thus, we obtain samples from the joint distribution of P (X m ; jX o ) by alternately conditioning on one or the other of the unknown variables, a technique known as Gibbs sampling <ref> (Geman and Geman, 1984) </ref>. Under some mild regularity conditions this algorithm can be shown to converge in distribution to the posterior (Tanner and Wong, 1987).
Reference: <author> Ghahramani, Z. </author> <year> (1994). </year> <title> Solving inverse problems using an EM approach to density estimation. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: In the limit, as the covari-ance matrices of the Gaussians approach zero, the approximation becomes a nearest-neighbor map. Not all learning problems lend themselves to least squares estimates|many problems involve learning a one-to-many mapping between the input and target variables <ref> (Ghahramani, 1994) </ref>. The resulting conditional densities are multimodal and no single value of the output given the input will appropriately reflect this fact (Shizawa, 1993; Ghahramani, 1994; Bishop, 1994).
Reference: <author> Hastie, T. and Tibshirani, R. </author> <year> (1994). </year> <title> Discriminant analysis by Gaussian mixtures. </title> <type> Technical report, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, NJ. </address>
Reference: <author> Hastings, W. K. </author> <year> (1970). </year> <title> Monte carlo sampling methods using markov chains and their applications. </title> <journal> Biometrika, </journal> <volume> 57 </volume> <pages> 97-109. </pages>
Reference: <author> Hinton, G. E. and Sejnowski, T. J. </author> <year> (1986). </year> <title> Learning and relearning in Boltzmann machines. </title> <editor> In Rumel-hart, D. E. and McClelland, J. L., editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Mi-crostructure of Cognition. Volume 1: Foundations. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: of inputs, the latter estimates this distribution directly from the data. 7 6 Boltzmann machines and incomplete data Boltzmann machines are networks of binary stochastic units with symmetric connections, in which learning corresponds to minimizing the relative entropy between the probability distribution of the visible states and a target distribution <ref> (Hinton and Sejnowski, 1986) </ref>. The relative entropy cost function can be rewritten to reveal that, if the target distribution is taken to be the empirical distribution of the data, it is equivalent to the model likelihood. Therefore, the Boltzmann learning rule implements maximum likelihood density estimation over binary variables. <p> likelihood conditional density estimation, the Boltzmann learning rule is an instance of the generalized EM algorithm (GEM; Dempster, Laird, and Rubin, 1977): the estimation of the unit correlations given the current weights and the clamped values corresponds to the E-step, and the update of the weights corresponds to the M-step <ref> (Hinton and Sejnowski, 1986) </ref>. It is generalized EM in the sense that the M-step does not actually maximize the likelihood but simply increases it by gradient ascent.
Reference: <author> Jacobs, R., Jordan, M., Nowlan, S., and Hinton, G. </author> <year> (1991). </year> <title> Adaptive mixture of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 79-87. </pages>
Reference: <author> Jordan, M. and Jacobs, R. </author> <year> (1994). </year> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 181-214. </pages>
Reference: <author> Little, R. J. A. and Rubin, D. B. </author> <year> (1987). </year> <title> Statistical Analysis with Missing Data. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: To maintain the breadth of the review we discuss classification, function approximation, and clustering problems. Because missing data can arise in both the input and the target variables, we treat both missing inputs and unlabeled data. The statistical framework that we adopt <ref> (cf. Little and Rubin, 1987) </ref> makes a distinction between the environment, which we assume to generate complete data, and the missing data mechanism which renders some of the output of the environment unobservable to the learner. The supervised learning problem consists of forming a map from inputs to targets. <p> For maximum likelihood methods this implies directly that maximizing L (jX o ) / P (X o j) as a function of is equivalent to maximizing L (; jX o ; R). Therefore the parameters of the missing data mechanism can be ignored for the purposes of estimating <ref> (Little and Rubin, 1987) </ref>. <p> This application has been pursued in the statistics literature mostly for non-mixture density estimation problems. 3 We now show how combining the 3 Some exceptions are the use of mixture densities in the context of contaminated normal models for robust estimation <ref> (Little and Rubin, 1987) </ref>, and in the context of mixed categorical and continuous data with missing values (Little and Schluchter, 1985). missing data application of EM with that of learning mixture parameters results in a set of clustering, classification, and function approximation algorithms for incomplete data.
Reference: <author> Little, R. J. A. and Schluchter, M. D. </author> <year> (1985). </year> <title> Maximum likelihood estimation for mixed continuous and categorical data with missing values. </title> <journal> Biometrika, </journal> <volume> 72 </volume> <pages> 497-512. </pages>
Reference-contexts: literature mostly for non-mixture density estimation problems. 3 We now show how combining the 3 Some exceptions are the use of mixture densities in the context of contaminated normal models for robust estimation (Little and Rubin, 1987), and in the context of mixed categorical and continuous data with missing values <ref> (Little and Schluchter, 1985) </ref>. missing data application of EM with that of learning mixture parameters results in a set of clustering, classification, and function approximation algorithms for incomplete data.
Reference: <author> McLachlan, G. and Basford, K. </author> <year> (1988). </year> <title> Mixture models: Inference and applications to clustering. </title> <publisher> Mar-cel Dekker. </publisher>
Reference-contexts: A possible disadvantage of parametric methods is their lack of flexibility when compared with nonparametric methods. Mixture models, however, largely circumvent this problem as they combine much of the flexibility of non-parametric methods with certain of the analytic advantages of parametric methods <ref> (McLachlan and Basford, 1988) </ref>. Mixture models have been utilized recently for supervised learning problems in the form of the "mixtures of experts" architecture (Jacobs et al., 1991; Jordan and Jacobs, 1994).
Reference: <author> Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and Teller, E. </author> <year> (1953). </year> <title> Equation of state calculations by fast computing machines. </title> <journal> Journal of Chemical Physics, </journal> <volume> 21 </volume> <pages> 1087-1092. </pages>
Reference: <author> Moody, J. and Darken, C. </author> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294. </pages>
Reference: <author> Murphy, P. and Aha, D. </author> <year> (1992). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <institution> University of California, Department of Information and Computer Science, </institution> <address> Irvine, CA. </address>
Reference-contexts: Both of the above "filling-in" techniques, known as mean im putation and regression imputation, yield unsatisfactory 1 See, for example, the UCI Repository of Machine Learning Databases <ref> (Murphy and Aha, 1992) </ref>. results even on this simple parameter estimation problem. The paper is organized as follows. In section 2 we outline the statistical framework that defines the missing data and data generation mechanisms. In section 3 we proceed to describe a likelihood-based approach to learning from incomplete data.
Reference: <author> Nowlan, S. J. </author> <year> (1991). </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <institution> CMU-CS-91-126, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Such mixed models can serve to solve classification problems, as will be discussed in a later section. 4.2 Clustering Gaussian mixture model estimation is a form of soft clustering <ref> (Nowlan, 1991) </ref>. Furthermore, if a full covariance model is used the principal axes of the Gaussians align with the principal components of the data within each soft cluster. For binary or categorical data soft clustering algorithms can also be obtained using the above Bernoulli and multinomial mixture models.
Reference: <author> Poggio, T. and Girosi, F. </author> <year> (1989). </year> <title> A theory of networks for approximation and learning. A.I. Lab Memo 1140, </title> <publisher> MIT, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The author concludes that treating `unknown' as a separate value is not a good solution to the missing value problem, as querying on attributes with unknown values will have higher apparent information gain <ref> (Quinlan, 1986) </ref>. The approach that he advocates instead is to compute the expected information gain, by assuming that the unknown attribute is distributed according to the observed values in the subset of the data at that node of the tree.
Reference: <author> Quinlan, J. R. </author> <year> (1989). </year> <title> Unknown attribute values in induction. </title> <booktitle> In Proceedings of the Sixth International Conference on Machine Learning. </booktitle>
Reference: <author> Rubin, D. B. </author> <year> (1987). </year> <title> Multiple Imputation for Nonresponse in Surveys. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: To maintain the breadth of the review we discuss classification, function approximation, and clustering problems. Because missing data can arise in both the input and the target variables, we treat both missing inputs and unlabeled data. The statistical framework that we adopt <ref> (cf. Little and Rubin, 1987) </ref> makes a distinction between the environment, which we assume to generate complete data, and the missing data mechanism which renders some of the output of the environment unobservable to the learner. The supervised learning problem consists of forming a map from inputs to targets. <p> For maximum likelihood methods this implies directly that maximizing L (jX o ) / P (X o j) as a function of is equivalent to maximizing L (; jX o ; R). Therefore the parameters of the missing data mechanism can be ignored for the purposes of estimating <ref> (Little and Rubin, 1987) </ref>. <p> This application has been pursued in the statistics literature mostly for non-mixture density estimation problems. 3 We now show how combining the 3 Some exceptions are the use of mixture densities in the context of contaminated normal models for robust estimation <ref> (Little and Rubin, 1987) </ref>, and in the context of mixed categorical and continuous data with missing values (Little and Schluchter, 1985). missing data application of EM with that of learning mixture parameters results in a set of clustering, classification, and function approximation algorithms for incomplete data. <p> For such problems one may generate a Markov chain whose stationary distribution is P (jX o ; X m ). 5.2 Multiple imputation and Bayesian backpropagation Multiple imputation <ref> (Rubin, 1987) </ref> is a technique in which each missing value is replaced by m simulated values which reflect uncertainty about the true value of the missing data. After multiple imputation, m completed data sets exist, each of which can be analyzed using complete data methods.
Reference: <author> Schafer, J. </author> <year> (1994). </year> <title> Analysis of Incomplete Multivariate Data by Simulation. </title> <publisher> Chapman & Hall, London. </publisher>
Reference-contexts: However, in incomplete data problems the usual choices of conjugate priors do not generally lead to recognizable posteriors, making iterative simulation and sampling techniques for obtaining the posterior distribution indispensable <ref> (Schafer, 1994) </ref>. 5.1 Data augmentation and Gibbs sampling One such technique, which is closely related in form to the EM algorithm, is data augmentation (Tanner and Wong, 1987). This iterative algorithm consists of two 8 steps. <p> Note that the augmented data can be chosen so as to simplify the P-step in much the same way as indicator variables can be chosen to simplify the M-step in EM. Data augmentation techniques have been recently combined with the Metropolis-Hastings algorithm <ref> (Schafer, 1994) </ref>. In Metropolis-Hastings (Metropolis et al., 1953; Hastings, 1970), one creates a Monte Carlo Markov chain by drawing from a probability distribution meant to approximate the distribution of interest and accepting or rejecting the drawn value based on an acceptance ratio. <p> The results can then be combined to form a single inference. Though multiple imputation requires sampling from P (X m jX o ; ), which may be difficult, iterative simulation methods can also be used in this context <ref> (Schafer, 1994) </ref>. The Bayesian backpropagation technique for missing data presented by Buntine and Weigend (1991) is a special case of multiple imputation.
Reference: <author> Shizawa, M. </author> <year> (1993). </year> <title> Multi-valued standard regularization theory (2): Regularization networks and learning algorithms for approximating multi-valued functions. </title> <type> Technical Report TR-H-045, </type> <institution> ATR, Ky-oto, Japan. </institution> <note> 10 Southcott, </note> <author> M. and Bogner, R. </author> <year> (1993). </year> <title> Classification of incomplete data using neural networks. </title> <booktitle> In Proceedings of the Fourth Australian Conference on Neural Networks (ACNN '93), </booktitle> <address> Sydney, Australia. </address>
Reference: <author> Specht, D. F. </author> <year> (1991). </year> <title> A general regression neural network. </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> 2(6) </volume> <pages> 568-576. </pages>
Reference: <author> Tanner, M. A. and Wong, W. H. </author> <year> (1987). </year> <title> The calculation of posterior distributions by data augmentation. </title> <journal> Journal of American Statistical Association, </journal> <volume> 82(398) </volume> <pages> 528-550. </pages>
Reference-contexts: problems the usual choices of conjugate priors do not generally lead to recognizable posteriors, making iterative simulation and sampling techniques for obtaining the posterior distribution indispensable (Schafer, 1994). 5.1 Data augmentation and Gibbs sampling One such technique, which is closely related in form to the EM algorithm, is data augmentation <ref> (Tanner and Wong, 1987) </ref>. This iterative algorithm consists of two 8 steps. In the Imputation or I-step, instead of comput-ing the expectations of the missing sufficient statistics, we simulate m random draws of the missing data from their conditional distribution P (X m jX o ; ). <p> Under some mild regularity conditions this algorithm can be shown to converge in distribution to the posterior <ref> (Tanner and Wong, 1987) </ref>. Note that the augmented data can be chosen so as to simplify the P-step in much the same way as indicator variables can be chosen to simplify the M-step in EM. Data augmentation techniques have been recently combined with the Metropolis-Hastings algorithm (Schafer, 1994).
Reference: <author> Tresp, V., Ahmad, S., and Neuneier, R. </author> <year> (1994). </year> <title> Training neural networks with deficient data. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> San Francisco, CA. </address> <publisher> Morgan Kaufman Publishers. </publisher>
Reference: <author> White, H. </author> <year> (1989). </year> <title> Learning in artificial neural networks: A statistical perspective. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 425-464. 11 </pages>
Reference-contexts: For feedforward neural networks we know that descent in the error cost function can be interpreted as ascent in the model parameter likelihood <ref> (e.g. White, 1989) </ref>.
References-found: 32

