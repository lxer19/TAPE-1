URL: http://www.cs.kuleuven.ac.be/~lucdr/filp-papers/sebag.ps
Refering-URL: http://www.cs.kuleuven.ac.be/~lucdr/filp.html
Root-URL: 
Email: Michele.Sebag@polytechnique.fr  
Title: Distance Induction in First Order Logic used for classification via a k-nearest-neighbor process. Experiments on
Author: M. Sebag 
Note: This distance can be  
Address: F-91128 Palaiseau  
Affiliation: LMS, Ecole Polytechnique,  
Abstract: This paper tackles the supervised induction of a distance from examples described as Horn clauses or constrained clauses. In opposition to syntax-driven approaches, this approach is discrimination-driven: it proceeds by defining a small set of complex discriminant hypotheses. These hypotheses serve as new concepts, used to redescribe the initial examples. Further, this redescription can be embedded into the space of natural integers, and a distance between examples thus naturally follows. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aamodt and E.Plaza. </author> <title> Case-based reasoning : Foundational issues, methodological 12 variations, and system approaches. </title> <journal> AICOM, </journal> <volume> 7(1), </volume> <year> 1994. </year>
Reference-contexts: Similarity functions are also central to case-based reasoning and conceptual clustering. In case-based reasoning <ref> [1] </ref>, the first stage consists of retrieving the prototypes most "similar" to the case at hand; and if the retrieval is based on an inadequate similarity function, the subsequent stages (e.g. adaptation of the prototypes) are bound to fail.
Reference: [2] <author> G. Bisson. </author> <title> Learning in FOL with a similarity measure. </title> <booktitle> In Proceedings of 10 th AAAI, </booktitle> <year> 1992. </year>
Reference-contexts: Supervised learning can benefit from a distance structure on the problem domain, in several ways. KBG uses a similarity function specifically designed for First-Order Logic (FOL) languages and parameterized by the user, in order to guide generalization <ref> [2] </ref>. RIBL (Relational Instance Based Learner) is a k-Nearest Neighbor (k-NN) classifier also based on a FOL similarity function [4]. <p> Two ways have been explored to incorporate some semantics, or bring a global perspective, into the above calculation. The first one, investigated by KBG <ref> [2] </ref>, consists in computing the distance of two terms by global means (the similarity of two terms depends on the similarity of the terms they are encountered together with in all literals, i.e. the similarity of terms is defined as a fixed point of a system parameterized by the weight of
Reference: [3] <author> P. Domingos. </author> <title> Rule induction and instance-based learning: A unified approach. </title> <booktitle> In Proceedings of IJCAI-95, </booktitle> <pages> pages 1226-1232. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Last, RISE combines a rule classifier with a k-NN: when the instance to classify is not covered by any rule, the distance of the instance to the different rules is evaluated and the instance is classified according to the majority vote of its "neighbor" rules <ref> [3] </ref>. Similarity functions are also central to case-based reasoning and conceptual clustering.
Reference: [4] <author> W. Emde and D. Wettscherek. </author> <title> Relational instance based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13 th International Conference on Machine Learning, </booktitle> <pages> pages 122-130, </pages> <year> 1996. </year>
Reference-contexts: KBG uses a similarity function specifically designed for First-Order Logic (FOL) languages and parameterized by the user, in order to guide generalization [2]. RIBL (Relational Instance Based Learner) is a k-Nearest Neighbor (k-NN) classifier also based on a FOL similarity function <ref> [4] </ref>. Last, RISE combines a rule classifier with a k-NN: when the instance to classify is not covered by any rule, the distance of the instance to the different rules is evaluated and the instance is classified according to the majority vote of its "neighbor" rules [3]. <p> Another approach is that of RIBL <ref> [4] </ref>. The global aspect here comes from the fact that the similarity of any two atoms takes into account all atoms (up to a given depth) connected to the current ones; further, all mappings between these sets of atoms are considered.
Reference: [5] <author> U.M. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From data mining to knowledge discovery: An overview. </title> <booktitle> In Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 1-34. </pages> <publisher> MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Last, clustering appears a main stage of Knowledge 1 Discovery in Databases (KDD) <ref> [5] </ref>: one must somehow divide the enormous amount of available data, in order for knowledge to be conquered. This paper is concerned with the induction of a distance from classified examples expressed as Horn clauses or constrained clauses. <p> Moreover, the distance remains relevant when induced from an unclassified training set. This implies that this approach can be employed in other frameworks, such as KDD, where examples are usually not classified <ref> [5] </ref>. One property of this distance is to be "local", in the sense that similarly modifying similar features in two examples can modify their distance: the distance of any two instances that are red may change if both instances turn blue, everything else being equal.
Reference: [6] <author> D. Fisher. </author> <title> Iterative optimization and simplification of hierarchical clusterings. </title> <type> Technical report, </type> <institution> Vanderbilt University, TR CS-95-01, </institution> <year> 1995. </year>
Reference-contexts: Similarly, conceptual clustering aims at clustering similar examples, in such a way that different clusters can be given symbolic discriminant characterizations <ref> [6] </ref>; this task requires the similarity or difference of any two examples to be somehow measurable. Last, clustering appears a main stage of Knowledge 1 Discovery in Databases (KDD) [5]: one must somehow divide the enormous amount of available data, in order for knowledge to be conquered.
Reference: [7] <author> J. Jaffar and J. L. Lassez. </author> <title> Constraint logic programming. </title> <booktitle> In Proc. of the fourteenth ACM Symposium on the Principles of Programming Languages, </booktitle> <pages> pages 111-119, </pages> <year> 1987. </year>
Reference-contexts: The set of substitutions on C is naturally embedded into the set of constraints over the variables of C, with constraint entailment <ref> [7] </ref> as generality relationship. The discrimination of counter-example F can be based on either predicates (G discriminates F if G contains a predicate absent from F ), or constraints.
Reference: [8] <author> R.D. King, A. Srinivasan, and M.J.E. Sternberg. </author> <title> Relating chemical activity to structure: an examination of ILP successes. </title> <journal> New Gen. Comput., </journal> <volume> 13, </volume> <year> 1995. </year>
Reference-contexts: For instance, let E and F describe "small" chemical molecules involving about 40 atoms <ref> [8] </ref>. A substitution in (F ) maps any of the 40 atoms in E, onto one among the 40 atom in F ; thus the size of is in 40 40 . <p> Nevertheless, as these are parameterized by the user, the complexity of DISTILL remains under control. 4 Experimental validation The relevance of the induced distance is evaluated from the predictive accuracy of the DISTILL classification module, which simply consists of a k-NN classifier. Experiments consider the mutagenesis problem <ref> [8] </ref>, which has been thoroughly investigated [19]. 4.1 The problem domain The dataset is composed of 125 active molecules and 63 inactive molecules.
Reference: [9] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: One shows that C discriminates F iff is inconsistent with all in (F ) [16]. On the other hand, the substitutions on clause C can be given an attribute-value representation, a la LINUS <ref> [9] </ref>: each variable X in C is associated an attribute 2 . The lattice of constraints on C is equivalent to the lattice of attribute-value hypotheses in this attribute-value language.
Reference: [10] <author> R.S. Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In R.S Michalski, J.G. Carbonell, and T.M. Mitchell, editors, </editor> <booktitle> Machine Learning : an artificial intelligence approach, </booktitle> <volume> volume 1, </volume> <pages> pages 83-134. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference-contexts: Another question is: could DISTILL be used in an unsupervised way, and what happens when the training set is not classified ? 4.3 Results The supervised use of DISTILL, as described in section 3.3 is first analyzed with respect to the number D of hypotheses allowed, with D ranging in <ref> [10; 100] </ref>. Table 3 (a) shows the corresponding average predictive accuracy on the test set, with standard deviation. Run times refer to execution on a HP-710 workstation. For D &gt; 20, DISTILL outperforms PROGOL and FOIL. It obtains similar or better results than STILL, with comparable computational cost.
Reference: [11] <author> S. Muggleton. </author> <title> Inverse entailment and PROGOL. </title> <journal> New Gen. Comput., </journal> <volume> 13 </volume> <pages> 245-286, </pages> <year> 1995. </year>
Reference-contexts: More generally, as long as H includes few selected hypotheses, such as produced by most prominent learners <ref> [12, 11, 13] </ref>, d H does not present much interest compared to H itself. But d H gets finer as the number and redundancy of hypotheses increases. <p> This mapping in turn induces a distance d H on the problem domain. Distance d H does not present any further interest when H represents a small set of optimal hypotheses, e.g. induced by prominent ILP systems <ref> [12, 11, 13] </ref>. We therefore use the complex and redundant hypotheses issued from the Disjunctive Version Space approach. In the framework of logic programming, these hypotheses can be learned and used with polynomial complexity via a stochastic bias, implemented in the STILL system.
Reference: [12] <author> J.R. Quinlan. </author> <title> Learning logical definition from relations. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: More generally, as long as H includes few selected hypotheses, such as produced by most prominent learners <ref> [12, 11, 13] </ref>, d H does not present much interest compared to H itself. But d H gets finer as the number and redundancy of hypotheses increases. <p> This mapping in turn induces a distance d H on the problem domain. Distance d H does not present any further interest when H represents a small set of optimal hypotheses, e.g. induced by prominent ILP systems <ref> [12, 11, 13] </ref>. We therefore use the complex and redundant hypotheses issued from the Disjunctive Version Space approach. In the framework of logic programming, these hypotheses can be learned and used with polynomial complexity via a stochastic bias, implemented in the STILL system.
Reference: [13] <author> L. De Raedt and M. Bruynooghe. </author> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of IJCAI-93, </booktitle> <pages> pages 1058-1063. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: More generally, as long as H includes few selected hypotheses, such as produced by most prominent learners <ref> [12, 11, 13] </ref>, d H does not present much interest compared to H itself. But d H gets finer as the number and redundancy of hypotheses increases. <p> This mapping in turn induces a distance d H on the problem domain. Distance d H does not present any further interest when H represents a small set of optimal hypotheses, e.g. induced by prominent ILP systems <ref> [12, 11, 13] </ref>. We therefore use the complex and redundant hypotheses issued from the Disjunctive Version Space approach. In the framework of logic programming, these hypotheses can be learned and used with polynomial complexity via a stochastic bias, implemented in the STILL system.
Reference: [14] <author> M. Sebag. </author> <title> 2 nd order understandability of disjunctive version spaces. </title> <booktitle> In Workshop on Machine Learning and Comprehensibility, IJCAI-95. </booktitle> <institution> Report LRI, Universite Paris-Sud, </institution> <year> 1995. </year>
Reference-contexts: The fact that DiVS involves poorly understandable hypotheses, but is in no way a black box <ref> [14] </ref>, and the validation of STILL on the mutagenesis dataset [17], have been discussed elsewhere. We focus here on the neighborhood relationship defined by: E 0 is neighbor of E if E 0 is subsumed by hypothesis H (E) (section 2.3).
Reference: [15] <author> M. Sebag. </author> <title> Delaying the choice of bias: A disjunctive version space approach. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13 th International Conference on Machine Learning, </booktitle> <pages> pages 444-452. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: Our approach is based on the Disjunctive Version Space, in which the elementary step consists in defining the disjunctive hypothesis discriminating a given pair of examples <ref> [15] </ref>. Such hypothesis can conveniently be thought of as a "hyperplane" dividing the search space into regions. A set of these hypotheses thus defines a new representation of the search space: an example can be described by the set of regions it belongs to. <p> This section is con-cerned with overcoming the well-known limitations of VS (more details are found in <ref> [15, 17] </ref>). 2.1 Hybridizing VS and AQ VS does never fail when it considers a single positive example E [15]. <p> This section is con-cerned with overcoming the well-known limitations of VS (more details are found in [15, 17]). 2.1 Hybridizing VS and AQ VS does never fail when it considers a single positive example E <ref> [15] </ref>. <p> Practically, a further instance E 0 of the problem domain is said to be neighbor of the training example E if E 0 is covered by a hypothesis in (or for short belongs to) H (E); E 0 is then classified in the majority class of its neighbors <ref> [15] </ref>. By construction, E 0 belongs to H (E) iff it belongs to H (E; F ) for all 3 counterexamples F of E. The elementary classification step thus consists in checking whether E 0 belongs to H (E; F ). <p> variables; e.g. (X Y )(t ) = X:t Y:t if X and Y are numerical variables grounded via t ; (X Y )(t ) = 0 if X and Y are linked to the same variable; (X Y )(t ) = don't care otherwise [16]. 3 Or most counter-examples; see <ref> [15] </ref> for heuristics allowing DiVS to cope with the noise and the sparseness of the training data. 4 2.4 Stochastic induction Induction of DiVS gets intractable in first-order languages: the size of (F ) can be exponential in the size of F .
Reference: [16] <author> M. Sebag and C. </author> <title> Rouveirol. Constraint inductive logic programming. </title> <editor> In L. de Raedt, editor, </editor> <booktitle> Advances in ILP. </booktitle> <publisher> IOS Press, </publisher> <year> 1996. </year>
Reference-contexts: Let us concentrate on constraint-based discrimination. Assume that C subsumes F , and let (F ) denote the set of substitutions on C such that C F . One shows that C discriminates F iff is inconsistent with all in (F ) <ref> [16] </ref>. On the other hand, the substitutions on clause C can be given an attribute-value representation, a la LINUS [9]: each variable X in C is associated an attribute 2 . The lattice of constraints on C is equivalent to the lattice of attribute-value hypotheses in this attribute-value language. <p> One shows that E 0 belongs to H (E; F ) iff there exists a substitution t mapping C onto E 0 (t 2 (E 0 )), such that t belongs to H (; ) for all in (F ) <ref> [16] </ref>. 2 This description can be enriched by additional attributes, describing the relationship between the instantiations of variables; e.g. (X Y )(t ) = X:t Y:t if X and Y are numerical variables grounded via t ; (X Y )(t ) = 0 if X and Y are linked to the <p> the relationship between the instantiations of variables; e.g. (X Y )(t ) = X:t Y:t if X and Y are numerical variables grounded via t ; (X Y )(t ) = 0 if X and Y are linked to the same variable; (X Y )(t ) = don't care otherwise <ref> [16] </ref>. 3 Or most counter-examples; see [15] for heuristics allowing DiVS to cope with the noise and the sparseness of the training data. 4 2.4 Stochastic induction Induction of DiVS gets intractable in first-order languages: the size of (F ) can be exponential in the size of F .
Reference: [17] <author> M. Sebag and C. </author> <title> Rouveirol. Tractable induction and classification in FOL. </title> <booktitle> In Proceedings of IJCAI-97. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1997. </year> <month> 13 </month>
Reference-contexts: In order for this paper to be self contained, section 2 briefly recalls the Disjunctive Version Space (DiVS), and the STILL algorithm, which builds a tractable characterization of DiVS in logic programming and constraint logic programming <ref> [17] </ref>. Section 3 discusses how to use a set of hypotheses to redescribe the instances of the problem domain, and map the problem domain onto another domain with "natural" distance, such as boolean or integer spaces. <p> This section is con-cerned with overcoming the well-known limitations of VS (more details are found in <ref> [15, 17] </ref>). 2.1 Hybridizing VS and AQ VS does never fail when it considers a single positive example E [15]. <p> The fact that DiVS involves poorly understandable hypotheses, but is in no way a black box [14], and the validation of STILL on the mutagenesis dataset <ref> [17] </ref>, have been discussed elsewhere. We focus here on the neighborhood relationship defined by: E 0 is neighbor of E if E 0 is subsumed by hypothesis H (E) (section 2.3). <p> The reference results obtained by PROGOL, FOIL and STILL on this problem (reported from <ref> [18, 17] </ref>), are given in Table 2. The results for STILL are averaged over eight parameter settings (the noise parameter is 0 or 2, and the sparseness parameter ranges in [6,9]). Run times (in seconds) are measured on HP-735 workstations. <p> System Accuracy Time FOIL 82 3 .5 PROGOL 88 2 40 950 STILL 90 5 &lt; 120 Table 2: Reference results on the 188-compound problem: Average predictive accuracy on the test set 9 4.2 Experimental Setting DISTILL is evaluated along the same lines as STILL <ref> [17] </ref>, from the predictive accuracy on the test set averaged on 25 independent selections of the training set (including 90% of the data with same repartition as in the whole data set). <p> In opposition, adjusting the parameters of STILL (controlling the noise and sparseness biases) appears more critical <ref> [17] </ref>.
Reference: [18] <author> A. Srinivasan and S. Muggleton. </author> <title> Comparing the use of background knowledge by two ILP systems. </title> <editor> In L. de Raedt, editor, </editor> <booktitle> Proceedings of ILP-95. </booktitle> <institution> Katholieke Universiteit Leuven, </institution> <year> 1995. </year>
Reference-contexts: Experiments consider the mutagenesis problem [8], which has been thoroughly investigated [19]. 4.1 The problem domain The dataset is composed of 125 active molecules and 63 inactive molecules. The background knowledge considered here includes all available information (termed B 4 in <ref> [18] </ref>): the description of atoms and bonds in the molecules, numerical attributes, and simple chemical concepts (e.g. benzenic or methyl group) involved in the molecules. The reference results obtained by PROGOL, FOIL and STILL on this problem (reported from [18, 17]), are given in Table 2. <p> The reference results obtained by PROGOL, FOIL and STILL on this problem (reported from <ref> [18, 17] </ref>), are given in Table 2. The results for STILL are averaged over eight parameter settings (the noise parameter is 0 or 2, and the sparseness parameter ranges in [6,9]). Run times (in seconds) are measured on HP-735 workstations.
Reference: [19] <institution> ESPRIT Project LTR 20237 ILP 2 . PPR-1, </institution> <year> 1997. </year> <month> 14 </month>
Reference-contexts: Experiments consider the mutagenesis problem [8], which has been thoroughly investigated <ref> [19] </ref>. 4.1 The problem domain The dataset is composed of 125 active molecules and 63 inactive molecules.
References-found: 19

