URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR36.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: e-mail: fprakash, singhalg@cis.ohio-state.edu  
Title: Low-Cost Checkpointing and Failure Recovery in Mobile Computing Systems  
Author: Ravi Prakash and Mukesh Singhal 
Keyword: Key words checkpointing, causal dependency, global snapshot, mobile computing systems, portable computers, recovery.  
Address: Columbus, OH 43210.  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: A mobile computing system consists of mobile and stationary nodes, connected to each other by a communication network. The presence of mobile nodes in the system places constraints on the permissible energy consumption and available communication bandwidth. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system (checkpoint) is required. Locating the mobile nodes contributes to the checkpointing and recovery costs. Synchronous snapshot collection algorithms, designed for static networks, either force every node in the system to take a new local snapshot, or block the underlying computation during snapshot collection. Hence, they are not suitable for mobile computing systems. This paper presents a synchronous snapshot collection algorithm for mobile systems that neither forces every node to take a local snapshot, nor blocks the underlying computation during snapshot collection. If a node initiates snapshot collection, local snapshots of only those nodes that have directly or transitively affected the initiator since their last snapshots need to be taken. We also propose a minimal rollback/recovery algorithm in which the computation at a node is rolled back only if it depends on operations that have been undone due to the failure of node(s). Both the algorithms have low communication and storage overheads, and satisfy the low energy consumption and low bandwidth constraints of mobile computing systems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, B. R. Badrinath, and T. Imielinski. </author> <title> Checkpointing Distributed Applications 30 on Mobile Computers. </title> <type> Technical report, </type> <institution> Department of Computer Science, Rutgers University, </institution> <year> 1994. </year>
Reference-contexts: Finally, Section 7 presents conclusions. 2 System Model The system is composed of a set of n nodes, and a network of communication links connecting the nodes. Some of the nodes may change their location with time. They will be referred to as mobile hosts or M H <ref> [1, 3] </ref>. The static nodes (henceforth, referred to as mobile support stations or M SS [1, 3]) are connected to each other by a static network. <p> Some of the nodes may change their location with time. They will be referred to as mobile hosts or M H <ref> [1, 3] </ref>. The static nodes (henceforth, referred to as mobile support stations or M SS [1, 3]) are connected to each other by a static network. An M H can be directly connected to at most one M SS at any given time and can communicate with other M Hs and M SSs only through the M SS to which it is directly connected. <p> The number of system messages required is proportional to the number of channels in the interconnection network along which computation messages have been sent since the last snapshot collection. Therefore, the average message complexity of the proposed algorithm is lower than Chandy-Lamport's algorithm. Acharya et al. <ref> [1] </ref> were the first to present an asynchronous snapshot collection algorithm for distributed applications on mobile computing systems. <p> Also, the local snapshot of a disconnected mobile host M H i is always available, as disconnect snapshot i , at the M SS to which it was last connected. In <ref> [1] </ref>, an M H has to take its snapshot whenever a message reception is preceded by a message transmission at that node. This may lead to as many local snapshots being taken as the number of computation messages (if the transmission and reception of messages are interleaved). <p> Considering that message communication is much more frequent than initiations of synchronous snapshot collection, or movement of M Hs from one M SS to another, the proposed algorithm will require the nodes to take their local snapshots much less frequently than the algorithm in <ref> [1] </ref>. The lazy checkpoint advancement in the proposed algorithm overcomes yet another potential drawback of synchronous checkpointing. During the lazy phase, messages needed for checkpoint advancement are spread over a period of time, rather than being bursty during a short duration.
Reference: [2] <author> B. Awerbuch and D. Peleg. </author> <title> Concurrent Online Tracking of Mobile Users. </title> <booktitle> In Proceedings of the ACM SIGCOMM Symposium on Communication, Architectures and Protocols, </booktitle> <year> 1991. </year>
Reference-contexts: An M H may be disconnected from the network for a finite, but arbitrary period of time while switching from the old M SS to the new M SS. Routing protocols for the network layer, to handle node mobility, have been proposed in <ref> [2, 4, 11, 21, 25] </ref>. At the applications level, the checkpointing algorithm may generate a request for the disconnected M H to take its snapshot. <p> At the end of this phase, the system has been restored to a consistent state. The roll-back requests for M Hs are rerouted to them by the M SS to which they were previously connected, through the M SS to which they are currently connected. The strategies proposed in <ref> [2, 4, 11, 21, 25] </ref> can be employed to locate the new M SS to which the M H is connected. 5.2 Retracing the Lost Computation Once the system has rolled back to a consistent state, the nodes have to retrace their computation that was undone during the roll-back.
Reference: [3] <author> B. R. Badrinath, A. Acharya, and T. Imielinski. </author> <title> Structuring Distributed Algorithms for Mobile Hosts. </title> <booktitle> In Proceedings of the 14 th International Conference on Distributed Computing Systems (to appear), </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: 1 Introduction A mobile computing system is a distributed system where some of nodes are mobile computers <ref> [3] </ref>. The location of mobile computers in the network may change with time. The fixed nodes in the system are connected by a static network. A mobile node communicates with the other nodes in the system through a fixed node to which it is connected. <p> Finally, Section 7 presents conclusions. 2 System Model The system is composed of a set of n nodes, and a network of communication links connecting the nodes. Some of the nodes may change their location with time. They will be referred to as mobile hosts or M H <ref> [1, 3] </ref>. The static nodes (henceforth, referred to as mobile support stations or M SS [1, 3]) are connected to each other by a static network. <p> Some of the nodes may change their location with time. They will be referred to as mobile hosts or M H <ref> [1, 3] </ref>. The static nodes (henceforth, referred to as mobile support stations or M SS [1, 3]) are connected to each other by a static network. An M H can be directly connected to at most one M SS at any given time and can communicate with other M Hs and M SSs only through the M SS to which it is directly connected. <p> The various components like the CPU, display, disk drive, etc. drain the battery. Message transmission and reception also consume energy. Energy consumption can be reduced by powering down individual components during periods of low activity [9]. This strategy is referred to as the doze mode operation <ref> [3] </ref>. Energy can be conserved, during snapshot collection, by forcing a minimal set of nodes to take their local snapshots. Otherwise, some M Hs that have been dozing will be waken up by the snapshot collection.
Reference: [4] <author> P. Bhagwat and C. E. Perkins. </author> <title> A Mobile Networking System Based on Internet Protocol(IP). </title> <booktitle> In Proceedings of the USENIX Symposium on Mobile and Location-Independent Computing, </booktitle> <pages> pages 69-82, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: An M H may be disconnected from the network for a finite, but arbitrary period of time while switching from the old M SS to the new M SS. Routing protocols for the network layer, to handle node mobility, have been proposed in <ref> [2, 4, 11, 21, 25] </ref>. At the applications level, the checkpointing algorithm may generate a request for the disconnected M H to take its snapshot. <p> At the end of this phase, the system has been restored to a consistent state. The roll-back requests for M Hs are rerouted to them by the M SS to which they were previously connected, through the M SS to which they are currently connected. The strategies proposed in <ref> [2, 4, 11, 21, 25] </ref> can be employed to locate the new M SS to which the M H is connected. 5.2 Retracing the Lost Computation Once the system has rolled back to a consistent state, the nodes have to retrace their computation that was undone during the roll-back.
Reference: [5] <author> S. Chandrasekaran and S. Venkatesan. </author> <title> A Message-Optimal Algorithm for Distributed Termination Detection. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 245-252, </pages> <year> 1990. </year>
Reference-contexts: This reduces the amount of computation that has to be undone on node failures. Huang's algorithm [10] is employed to detect the termination of the coordinated snapshot collection. Unlike <ref> [5, 12] </ref>, information about termination is not propagated along a tree rooted at the snapshot initiator. Instead, the nodes send this information directly to the initiator. Hence, termination detection is fast and inexpensive. We assume that at any time, at most one snapshot collection is in progress.
Reference: [6] <author> K. M. Chandy and L. Lamport. </author> <title> Distributed Snapshots : Determining Global States of Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(1) </volume> <pages> 63-75, </pages> <month> February </month> <year> 1985. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in [6, 7, 15, 12, 13, 16, 17]. The snapshot collection algorithm by Chandy and Lamport <ref> [6] </ref> forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in [7, 13, 16, 17] also force every node to take its snapshot. <p> In order to prevent duplicate output messages from being sent, an output message is not sent to the external environment until the checkpoint is advanced to a point that is in the future of the generation of the output message. 6 Comparison With Earlier Work In the Chandy-Lamport algorithm <ref> [6] </ref>, which is one of the earliest snapshot collection algorithms for a system with static nodes, system messages are sent along all the links in the network during snapshot collection. This leads to a message complexity of O (n 2 ).
Reference: [7] <author> E. N. Elnozahy and W. Zwaenepoel. Manetho: </author> <title> Transparent Rollback-Recovery with Low Overhead, Limited Rollback, and Fast Output Commit. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(5) </volume> <pages> 526-531, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in <ref> [7, 13, 16, 17] </ref> also force every node to take its snapshot. In Koo-Toueg's algorithm [12], all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system. <p> Snapshot collection algorithms in [7, 13, 16, 17] also force every node to take its snapshot. In Koo-Toueg's algorithm [12], all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system. Manetho <ref> [7] </ref> employs a snapshot algorithm similar to Koo-Toueg's without suspending the underlying computation. However, all the nodes are forced to take their local snapshots. <p> Moreover, the lazy phase advances the checkpoint slowly, rather than in a burst. This avoids contention for the low bandwidth channels. In previous recovery algorithms for static distributed systems, such as [19], the computation at all the nodes is rolled back to a mutually consistent state during recovery. In <ref> [7, 23] </ref>, no non-faulty node is made to roll-back its computation. However, [23] requires extensive logging of message contents, at the sender as well as receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> In [7, 23], no non-faulty node is made to roll-back its computation. However, [23] requires extensive logging of message contents, at the sender as well as receiver ends. In <ref> [7] </ref> the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. Each computation message has to carry portions of the antecedence graph, significantly increasing the size of the messages. <p> By keeping the number of nodes that need to roll back to a minimum, message traffic (for locating nodes) can be reduced, thus conserving the limited bandwidth. In some recovery algorithms for static distributed systems, no non-faulty node is made to roll-back its computation <ref> [7, 23] </ref>. However, [23] requires extensive logging of message contents, both at the sender and receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> In some recovery algorithms for static distributed systems, no non-faulty node is made to roll-back its computation [7, 23]. However, [23] requires extensive logging of message contents, both at the sender and receiver ends. In <ref> [7] </ref> the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. Each computation message has to carry portions of the antecedence graph, significantly increasing the size of the messages. The M Hs have a limited memory. <p> This is a high price to pay, considering that the low overheads associated with the computation message of our algorithm help accomplish consistent roll-back in one iteration. Moreover, the mobile nodes may change their location between iterations, complicating the roll-back process. The recovery algorithms proposed in <ref> [7, 23] </ref> ensure that only the faulty nodes are rolled back. However, they require extensive logging of messages and high communication overheads. The algorithm proposed in this paper has lower storage and communication overheads than [7, 23] and may require few non-faulty nodes to roll-back. <p> The recovery algorithms proposed in <ref> [7, 23] </ref> ensure that only the faulty nodes are rolled back. However, they require extensive logging of messages and high communication overheads. The algorithm proposed in this paper has lower storage and communication overheads than [7, 23] and may require few non-faulty nodes to roll-back. Our algorithm has slightly higher communication overheads than Venkatesan's algorithm [22], but much smaller delays. <p> As the underlying computation is never suspended during snapshot collection, the run-time overheads are low. Each system message has a small size, and incurs a low overhead as the information about dependencies can be conveyed using just a bit-vector. This compares favorably with existing implementations like Manetho <ref> [7] </ref> where the antecedence graph, incorporating information about the exact ordering of message transmissions and receptions at the nodes, is piggybacked on each message. We used dependency information to develop a minimal recovery algorithm.
Reference: [8] <author> J. Fidge. </author> <title> Timestamps in Message-Passing Systems that Preserve the Partial Ordering. </title> <booktitle> In Proceedings of the 11 th Australian Computer Science Conference, </booktitle> <pages> pages 56-66, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: P 4 is dependent on P 2 after receiving m 3 . Since P 2 was dependent on P 1 before sending m 3 , P 4 becomes (transitively) dependent on P 1 on receiving m 3 . Fidge <ref> [8] </ref> and Mattern [16] proposed vector clocks to maintain causality information. However, the overheads associated with vector clocks are high because a vector of n integers 7 is sent with each message.
Reference: [9] <author> G. H. Forman and J. Zahorjan. </author> <title> The Challenges of Mobile Computing. </title> <journal> IEEE Computer, </journal> <volume> 27(4) </volume> <pages> 38-47, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The mobility of an M H is directly dependent 6 on its energy efficiency. The various components like the CPU, display, disk drive, etc. drain the battery. Message transmission and reception also consume energy. Energy consumption can be reduced by powering down individual components during periods of low activity <ref> [9] </ref>. This strategy is referred to as the doze mode operation [3]. Energy can be conserved, during snapshot collection, by forcing a minimal set of nodes to take their local snapshots. Otherwise, some M Hs that have been dozing will be waken up by the snapshot collection.
Reference: [10] <author> S. T. Huang. </author> <title> Detecting Termination of Distributed Computations by External Agents. </title> <booktitle> In Proceedings of the 9 th International Conference on Distributed Computing Systems, </booktitle> <pages> pages 79-84, </pages> <year> 1989. </year>
Reference-contexts: Moreover, the algorithm is non-intrusive. 11 After the coordinated snapshot collection terminates, the nodes that did not participate in the snapshot collection take their local snapshots in a quasi-asynchronous fashion. This reduces the amount of computation that has to be undone on node failures. Huang's algorithm <ref> [10] </ref> is employed to detect the termination of the coordinated snapshot collection. Unlike [5, 12], information about termination is not propagated along a tree rooted at the snapshot initiator. Instead, the nodes send this information directly to the initiator. Hence, termination detection is fast and inexpensive. <p> A data structure similar to trigger (Section 4.1) can be used to indicate the node that initiated the roll-back. The node that initiates roll-back has an initial weight of one. As in Huang's termination detection algorithm <ref> [10] </ref>, a portion of this weight is sent by the initiator with each roll-back request. Each time a node propagates the roll-back request, it sends a portion of its weight with the corresponding messages. It also sends its residual weight back to the initiator on rolling-back.
Reference: [11] <author> J. Ioannidis, D. Duchamp, and G. Q. Maguire. </author> <title> IP-based Protocols for Mobile Inter-networking. </title> <booktitle> In Proceedings of the ACM SIGCOMM Symposium on Communication, Architectures and Protocols, </booktitle> <pages> pages 235-245, </pages> <year> 1991. </year>
Reference-contexts: An M H may be disconnected from the network for a finite, but arbitrary period of time while switching from the old M SS to the new M SS. Routing protocols for the network layer, to handle node mobility, have been proposed in <ref> [2, 4, 11, 21, 25] </ref>. At the applications level, the checkpointing algorithm may generate a request for the disconnected M H to take its snapshot. <p> At the end of this phase, the system has been restored to a consistent state. The roll-back requests for M Hs are rerouted to them by the M SS to which they were previously connected, through the M SS to which they are currently connected. The strategies proposed in <ref> [2, 4, 11, 21, 25] </ref> can be employed to locate the new M SS to which the M H is connected. 5.2 Retracing the Lost Computation Once the system has rolled back to a consistent state, the nodes have to retrace their computation that was undone during the roll-back.
Reference: [12] <author> R. Koo and S. Toueg. </author> <title> Checkpointing and Rollback-Recovery for Distributed Systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-13(1):23-31, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: In a consistent global snapshot, the reception of a message is recorded by a node only if the corresponding send has been recorded. If a node fails, the system is rolled-back to the latest consistent global snapshot <ref> [12, 19, 20, 23, 24] </ref>, and then the computation proceeds from that point onwards. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system, to advance the checkpoint, is required. <p> This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in [7, 13, 16, 17] also force every node to take its snapshot. In Koo-Toueg's algorithm <ref> [12] </ref>, all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system. Manetho [7] employs a snapshot algorithm similar to Koo-Toueg's without suspending the underlying computation. <p> This reduces the amount of computation that has to be undone on node failures. Huang's algorithm [10] is employed to detect the termination of the coordinated snapshot collection. Unlike <ref> [5, 12] </ref>, information about termination is not propagated along a tree rooted at the snapshot initiator. Instead, the nodes send this information directly to the initiator. Hence, termination detection is fast and inexpensive. We assume that at any time, at most one snapshot collection is in progress. <p> The main advantage of our algorithm over the synchronous Koo-Toueg algorithm <ref> [12] </ref> is that the underlying computation is never suspended during snapshot collection by our algorithm. This significantly reduces the run-time overheads of the algorithm. Moreover, in the snapshot collection algorithm of Koo-Toueg, only direct dependencies are maintained, as opposed to transitive dependencies maintained by our algorithm. <p> Snapshot requests propagate faster along the transitive dependency chain as compared to the direct dependency chains. Knowledge about transitive dependencies also reduces the number of snapshot REQUEST 28 messages required by the algorithm. In <ref> [12] </ref> a node P i sends snapshot requests to all the nodes P j on which it is directly dependent.
Reference: [13] <author> T.-H. Lai and T.-H. Yang. </author> <title> On Distributed Snapshots. </title> <journal> Information Processing Letters, </journal> <volume> 25 </volume> <pages> 153-158, </pages> <year> 1987. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in <ref> [7, 13, 16, 17] </ref> also force every node to take its snapshot. In Koo-Toueg's algorithm [12], all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system. <p> Piggybacking control information, to distinguish between messages sent by a node before and after its snapshot, is a strategy used for consistent snapshot collection in systems where communication channels are non-FIFO and computation messages sent after taking a snapshot may overtake the snapshot collection marker <ref> [13] </ref>. Therefore, the proposed algorithm can be easily modified for consistent snapshot collection of systems where message communication is non-FIFO. 5 Recovery from a Failure To recover from node failures, the system should be restored to a consistent state before the computation can proceed.
Reference: [14] <author> L. Lamport. </author> <title> Time, Clocks and the Ordering of Events in a Distributed System. </title> <journal> Com--munications of the ACM, </journal> <volume> 21(7) </volume> <pages> 558-565, </pages> <month> July </month> <year> 1978. </year>
Reference-contexts: The key to both the algorithms are the inter-node dependencies created by messages. Specifically, there is a dependency from the sender of a message to the receiver. The inter-node dependencies considered in the rest of the paper capture the happened before relationship described in <ref> [14] </ref>. During a snapshot collection, only the nodes from which there is a dependency onto the snapshot initiator, either direct or transitive, since their last checkpoints, are made to take their snapshots.
Reference: [15] <author> P.-J. Leu and B. Bhargava. </author> <title> Concurrent Robust Checkpointing and Recovery in Distributed Systems. </title> <booktitle> In Proceedings of the 4 th International Conference on Data Engineering, </booktitle> <pages> pages 154-163, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected.
Reference: [16] <author> F. Mattern. </author> <title> Virtual Time and Global States of Distributed Systems. </title> <editor> In M.Cosnard et. al., editor, </editor> <booktitle> Proceedings of the Workshop on Parallel and Distributed Algorithm, </booktitle> <pages> pages 215-226. </pages> <publisher> Elsevier Science Publishers B.V.(North-Holland), </publisher> <year> 1989. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in <ref> [7, 13, 16, 17] </ref> also force every node to take its snapshot. In Koo-Toueg's algorithm [12], all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system. <p> P 4 is dependent on P 2 after receiving m 3 . Since P 2 was dependent on P 1 before sending m 3 , P 4 becomes (transitively) dependent on P 1 on receiving m 3 . Fidge [8] and Mattern <ref> [16] </ref> proposed vector clocks to maintain causality information. However, the overheads associated with vector clocks are high because a vector of n integers 7 is sent with each message.
Reference: [17] <author> F. Mattern. </author> <title> Efficient Distributed Snapshots and Global Virtual Time Algorithms for Non-FIFO Systems. </title> <type> Technical Report SFB124-24/90, </type> <institution> University of Kaiserslautern, </institution> <year> 1990. </year>
Reference-contexts: This can be achieved by forcing a minimal subset of nodes to take their local snapshots, and by employing data-structures that impose low memory overheads. Consistent snapshot collection algorithms for static distributed systems have 2 been proposed in <ref> [6, 7, 15, 12, 13, 16, 17] </ref>. The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. <p> The snapshot collection algorithm by Chandy and Lamport [6] forces every node to take its local snapshot. The underlying computation is allowed to proceed while the global snapshot is being collected. Snapshot collection algorithms in <ref> [7, 13, 16, 17] </ref> also force every node to take its snapshot. In Koo-Toueg's algorithm [12], all the nodes are not forced to take their local snapshots. However, the underlying computation is suspended during snapshot collection. This imposes high run-time overheads on the system.
Reference: [18] <author> R. Prakash and M. Singhal. </author> <title> Maximal Global Snapshot with Concurrent Initiators. </title> <type> Technical Report OSU-CISRC-3/94-TR12, </type> <institution> The Ohio State University, </institution> <year> 1994. </year>
Reference-contexts: Instead, the nodes send this information directly to the initiator. Hence, termination detection is fast and inexpensive. We assume that at any time, at most one snapshot collection is in progress. Techniques to handle concurrent initiations of snapshot collection by multiple nodes can be found in <ref> [18] </ref>. 4.1 Data Structures Each node maintains the following data structures: interval number : an integer value maintained at each node that is incremented each time the node takes its local snapshot. interval vector : an array of n integers at each node, where interval vector [j] indicates the interval number
Reference: [19] <author> A. P. Sistla and J. L. Welch. </author> <title> Efficient Distributed Recovery Using Message Logging. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pages 223-238, </pages> <year> 1989. </year>
Reference-contexts: In a consistent global snapshot, the reception of a message is recorded by a node only if the corresponding send has been recorded. If a node fails, the system is rolled-back to the latest consistent global snapshot <ref> [12, 19, 20, 23, 24] </ref>, and then the computation proceeds from that point onwards. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system, to advance the checkpoint, is required. <p> This further reduces the amount of computation that is rolled back during recovery from node failures. Moreover, the lazy phase advances the checkpoint slowly, rather than in a burst. This avoids contention for the low bandwidth channels. In previous recovery algorithms for static distributed systems, such as <ref> [19] </ref>, the computation at all the nodes is rolled back to a mutually consistent state during recovery. In [7, 23], no non-faulty node is made to roll-back its computation. However, [23] requires extensive logging of message contents, at the sender as well as receiver ends.
Reference: [20] <author> R. E. Strom and S. Yemini. </author> <title> Optimistic Recovery in Distributed Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 3(3) </volume> <pages> 204-226, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: In a consistent global snapshot, the reception of a message is recorded by a node only if the corresponding send has been recorded. If a node fails, the system is rolled-back to the latest consistent global snapshot <ref> [12, 19, 20, 23, 24] </ref>, and then the computation proceeds from that point onwards. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system, to advance the checkpoint, is required.
Reference: [21] <author> F. Teraoka, Y. Yokote, and M. Tokoro. </author> <title> A Network Architecture Providing Host Migration Transparency. </title> <booktitle> In Proceedings of the ACM SIGCOMM Symposium on Communication, Architectures and Protocols, </booktitle> <year> 1991. </year>
Reference-contexts: An M H may be disconnected from the network for a finite, but arbitrary period of time while switching from the old M SS to the new M SS. Routing protocols for the network layer, to handle node mobility, have been proposed in <ref> [2, 4, 11, 21, 25] </ref>. At the applications level, the checkpointing algorithm may generate a request for the disconnected M H to take its snapshot. <p> At the end of this phase, the system has been restored to a consistent state. The roll-back requests for M Hs are rerouted to them by the M SS to which they were previously connected, through the M SS to which they are currently connected. The strategies proposed in <ref> [2, 4, 11, 21, 25] </ref> can be employed to locate the new M SS to which the M H is connected. 5.2 Retracing the Lost Computation Once the system has rolled back to a consistent state, the nodes have to retrace their computation that was undone during the roll-back.
Reference: [22] <author> S. Venkatesan. </author> <title> Message-Optimal Incremental Snapshots. </title> <journal> Journal of Computer and Software Engineering, </journal> <volume> 1(3) </volume> <pages> 211-231. </pages>
Reference-contexts: During the lazy phase, messages needed for checkpoint advancement are spread over a period of time, rather than being bursty during a short duration. Such low density traffic is suitable for the low bandwidth communication networks of the mobile computing systems. In Venkatesan's algorithm <ref> [22] </ref>, a node sends out markers (corresponding to REQUESTs in the proposed algorithm) on all the outgoing edges along which computation messages have been sent since the last checkpoint. <p> However, as already explained in Section 3, in order to efficiently collect a consistent snapshot, checkpointing REQUESTs need only be propagated from the receiver of messages to the sender, not the other way round as in <ref> [22] </ref>. Therefore, the proposed algorithm, because it propagates checkpointing decision in the receiver to sender direction, makes a minimal set of nodes to take its snapshot and is more suited for mobile computing systems than the algorithm given in [22]. <p> messages to the sender, not the other way round as in <ref> [22] </ref>. Therefore, the proposed algorithm, because it propagates checkpointing decision in the receiver to sender direction, makes a minimal set of nodes to take its snapshot and is more suited for mobile computing systems than the algorithm given in [22]. The main advantage of our algorithm over the synchronous Koo-Toueg algorithm [12] is that the underlying computation is never suspended during snapshot collection by our algorithm. This significantly reduces the run-time overheads of the algorithm. <p> However, they require extensive logging of messages and high communication overheads. The algorithm proposed in this paper has lower storage and communication overheads than [7, 23] and may require few non-faulty nodes to roll-back. Our algorithm has slightly higher communication overheads than Venkatesan's algorithm <ref> [22] </ref>, but much smaller delays. Thus our algorithm has slightly higher overheads than the most economical recovery algorithm, and a slightly greater delay than the fastest recovery algorithm, however it does not suffer from the drawbacks, i.e., extensive logging and high communication overheads, of either.
Reference: [23] <author> S. Venkatesan. </author> <title> Optimistic Crash Recovery Without Rolling Back Non-Faulty Processors. </title> <note> accepted for Information Sciences | An International Journal, </note> <year> 1993. </year>
Reference-contexts: In a consistent global snapshot, the reception of a message is recorded by a node only if the corresponding send has been recorded. If a node fails, the system is rolled-back to the latest consistent global snapshot <ref> [12, 19, 20, 23, 24] </ref>, and then the computation proceeds from that point onwards. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system, to advance the checkpoint, is required. <p> Moreover, the lazy phase advances the checkpoint slowly, rather than in a burst. This avoids contention for the low bandwidth channels. In previous recovery algorithms for static distributed systems, such as [19], the computation at all the nodes is rolled back to a mutually consistent state during recovery. In <ref> [7, 23] </ref>, no non-faulty node is made to roll-back its computation. However, [23] requires extensive logging of message contents, at the sender as well as receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> This avoids contention for the low bandwidth channels. In previous recovery algorithms for static distributed systems, such as [19], the computation at all the nodes is rolled back to a mutually consistent state during recovery. In [7, 23], no non-faulty node is made to roll-back its computation. However, <ref> [23] </ref> requires extensive logging of message contents, at the sender as well as receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> By keeping the number of nodes that need to roll back to a minimum, message traffic (for locating nodes) can be reduced, thus conserving the limited bandwidth. In some recovery algorithms for static distributed systems, no non-faulty node is made to roll-back its computation <ref> [7, 23] </ref>. However, [23] requires extensive logging of message contents, both at the sender and receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> By keeping the number of nodes that need to roll back to a minimum, message traffic (for locating nodes) can be reduced, thus conserving the limited bandwidth. In some recovery algorithms for static distributed systems, no non-faulty node is made to roll-back its computation [7, 23]. However, <ref> [23] </ref> requires extensive logging of message contents, both at the sender and receiver ends. In [7] the antecedence graph, containing the entire causal relationship, is kept in volatile storage and periodically copied to stable storage. <p> The first copy has been processed. So, the subsequent copies should be discarded. A transparent recovery algorithm should never generate duplicate output messages to the environment. The proposed recovery algorithm maintains data structures similar to those in <ref> [23] </ref>; how ever, it logs messages in volatile storage only at the sender. 1. <p> This is a high price to pay, considering that the low overheads associated with the computation message of our algorithm help accomplish consistent roll-back in one iteration. Moreover, the mobile nodes may change their location between iterations, complicating the roll-back process. The recovery algorithms proposed in <ref> [7, 23] </ref> ensure that only the faulty nodes are rolled back. However, they require extensive logging of messages and high communication overheads. The algorithm proposed in this paper has lower storage and communication overheads than [7, 23] and may require few non-faulty nodes to roll-back. <p> The recovery algorithms proposed in <ref> [7, 23] </ref> ensure that only the faulty nodes are rolled back. However, they require extensive logging of messages and high communication overheads. The algorithm proposed in this paper has lower storage and communication overheads than [7, 23] and may require few non-faulty nodes to roll-back. Our algorithm has slightly higher communication overheads than Venkatesan's algorithm [22], but much smaller delays.
Reference: [24] <author> S. Venkatesan and Tony T.-Y. Juang. </author> <title> Low Overhead Optimistic Crash Recovery. </title> <booktitle> Preliminary version appears in Proceedings of 11 th International Conference on Distributed Computing Systems as Crash Recovery with Little Overhead, </booktitle> <pages> pp 454-461, </pages> <year> 1991. </year>
Reference-contexts: In a consistent global snapshot, the reception of a message is recorded by a node only if the corresponding send has been recorded. If a node fails, the system is rolled-back to the latest consistent global snapshot <ref> [12, 19, 20, 23, 24] </ref>, and then the computation proceeds from that point onwards. To minimize the lost computation during recovery from node failures, periodic collection of a consistent snapshot of the system, to advance the checkpoint, is required. <p> In our algorithm, if P i knows that a REQUEST has already been sent to P j by some other node, then it does not send a REQUEST to P j . This information is carried by the bit-vector of the REQUEST messages. In Venkatesan-Juang's optimistic failure recovery algorithm <ref> [24] </ref>, no dependency information is sent with the computation messages. However, several iterations may be needed for all the nodes to roll-back to mutually consistent states, at the time of recovery.
Reference: [25] <author> H. Wada, T. Yozawa, T. Ohnishi, and Y. Tanaka. </author> <title> Mobile Computing Environment Based on Internet Packet Forwarding. </title> <booktitle> In 1991 Winter USENIX, </booktitle> <year> 1993. </year> <month> 32 </month>
Reference-contexts: An M H may be disconnected from the network for a finite, but arbitrary period of time while switching from the old M SS to the new M SS. Routing protocols for the network layer, to handle node mobility, have been proposed in <ref> [2, 4, 11, 21, 25] </ref>. At the applications level, the checkpointing algorithm may generate a request for the disconnected M H to take its snapshot. <p> At the end of this phase, the system has been restored to a consistent state. The roll-back requests for M Hs are rerouted to them by the M SS to which they were previously connected, through the M SS to which they are currently connected. The strategies proposed in <ref> [2, 4, 11, 21, 25] </ref> can be employed to locate the new M SS to which the M H is connected. 5.2 Retracing the Lost Computation Once the system has rolled back to a consistent state, the nodes have to retrace their computation that was undone during the roll-back.
References-found: 25

