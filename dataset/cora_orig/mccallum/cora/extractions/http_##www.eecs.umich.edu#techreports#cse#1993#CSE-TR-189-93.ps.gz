URL: http://www.eecs.umich.edu/techreports/cse/1993/CSE-TR-189-93.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse93.html
Root-URL: http://www.eecs.umich.edu
Email: E-mail: farag@umich.edu  
Phone: Phone: (313) 995 1157  
Title: S E I F: A System for Learning Adaptable Actions  
Author: Wafik M. Farag 
Date: June, 1989  
Address: Ann Arbor, MI 48109-2110  
Affiliation: Artificial Intelligence Laboratory Department of Electrical Engineering and Computer Science The University of Michigan  
Abstract: Current Explanation-Based Learning (EBL) systems take a training example in a non-operational form and transform it into a general operational concept which describes the goal to be achieved. However, the form in which knowledge is embedded in such learned concepts may restrict them from being usefully applied to later situations. This restriction arises from three possible causes: the learned concept's knowledge is too specific, the learned concept lacks domain knowledge lost in the generalization process, the learned concept is unable to adapt to differences found in similar situations encountered later. This results in the learned concept often not being useful in complex domains. Case-Based Learning (CBL) systems retain all the specific knowledge in the form of training examples. This restrains CBL's deduction capabilities, as the mapping of knowledge from a pre-stored example to a current situation limits the flexibility to explain slightly different cases. Pre-stored training examples restrict the flexibility of the system to adapt to a new change. This lack of flexibility will result in the learned concepts not being useful in complex domains. This is a proposal for a system (SEIF) that learns generalized concepts from single/multiple examples and which stores specific knowledge separately in a categorized fashion. This method of knowledge acquisition will allow the system to adapt to similar examples encountered later without having to change the generalized concept. Saving specific knowledge in categories will leave the generalized concept as a skeleton that is used to recognize a wide class of situations. The learned concept will represent the concept provided in the example, but in a form more general according to the environment the example is deduced from to suit later situations. Initially the system will be limited to learning actions instead of concepts. Thus, SEIF will be learning "generalized/adaptable actions." A generalized action is learned from a single training example (action), while compiling some of the situation specific knowledge given in a categorized fashion. SEIF will later integrate its internal knowledge from previous examples with the generalized learned action to carry out a situation specific action (adapted action). An adapted action means that the system is able to perform an action even if it hasn't encountered the situation before, provided the necessary knowledge required for that action was given to the system by earlier examples. This frees SEIF from the limitations of other knowledge intensive concept formation type systems (e.g. EBL, CBL). It achieves a higher level of generality while keeping the learned concept or action adaptable (i.e. "operational"). 
Abstract-found: 1
Intro-found: 1
Reference: [Anderson87] <author> Anderson, John R., </author> <year> (1987). </year> <title> Causal analysis and inductive learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 288-299. </pages>
Reference: [Bareiss87] <author> Bareiss, E. Ray, Porter, Bruce W., & Wier Craig C., </author> <year> (1987). </year> <title> Protos: An examplar-based learning Apprentice. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 12-23. </pages>
Reference-contexts: This means that concept descriptions have sharp boundaries and all its members are equal representatives of a concept. Although this simplification is useful for research, it misses some important aspects of the human notion of a concept. For example, <ref> [Bareiss87] </ref> considers inductive learning of the concepts "science" or "friend" to be difficult to learn. If general descriptions of such concepts can be formed, they will be so vague as to be useless for classification of new cases.
Reference: [Braver88] <author> Braverman, M. S., & Russell, S. J. </author> <year> (1988). </year> <title> Boundaries of operationality. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <address> Ann Arbor, Michigan: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address>
Reference: [Braver88a] <author> Braverman, M. S., & Russell, S. J. </author> <year> (1988). </year> <title> Explanation-based learning in complex domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 90-94. </pages>
Reference: [Braver88b] <author> Braverman, M. S., & Russell, S. J. </author> <year> (1988). </year> <title> "IMEX: Overcoming Intractability in Explanation-Based Learning." </title> <booktitle> In Proceedings of the AAAI-88, </booktitle> <address> St. Paul, Minnesota, </address> <month> August 21-26, </month> <year> 1988, </year> <pages> pp. 575-579. </pages>
Reference: [Carbonell86] <author> Carbonell, J. G., </author> <year> (1986). </year> <title> Derivational Analogy: A theory of reconstructive Problem Solving and Expertise acquisition. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach. </booktitle> <volume> Vol. </volume> <editor> II R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA., </address> <pages> pp. 371-392. </pages>
Reference-contexts: Having the original example as is enables the system to try different alternatives to achieve the goal and not just applying the rule. This requires a lot of analogy transformations which have been found to be very difficult tasks <ref> [Carbonell86] </ref>. The problems of knowledge and adaptation appear in the above two categories. Both types of systems adapt to new knowledge either by forming a concept (Generalization systems) or by adding an example to the system (CBL systems). <p> SWALE may be classified as a CBL system. In SWALE's explanation construction algorithm, two obstacles might limit the system's performance. Modifying an old XP to suit a new event is not easy with the XP format of representation <ref> [Carbonell86] </ref>, as it requires what Kass notes as XP tweaking. The second obstacle is picking (indexing) the right XP for the event under consideration, or else an iterative process of trying different XPs will take place, which slows the system's performance.
Reference: [Cockton88] <author> Cockton, </author> <title> Gilbert (1988).Bringing AI back home. </title> <note> In AIList@ai.ai.mit.edu, AIList digest volume.8-issue.117 Article 4. </note>
Reference: [Danyluk87] <author> Danyluk, A. P., </author> <year> (1987). </year> <title> The use of explanations for similarity-based learning. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 274-276. </pages>
Reference: [Dejong86] <author> Dejong, G. F., & Mooney, R. J. </author> <year> (1986). </year> <title> Explanation-Based Learning: An alternative view. </title> <journal> Machine Learning Journal, </journal> <volume> 1(2), 1:2, </volume> <pages> 145-167. </pages>
Reference-contexts: In Explanation-Based systems, adaptation was recognized to be an important feature for the concepts learned. <ref> [Dejong86] </ref> proposed the term Explanation-based learning (EBL) as a broader term and more accurate for the concepts to be learned than Explanation-based generalization (EBG). The term EBL was proposed for systems which could perform generalization and refinement on their learned concepts and not only generalizations as in EBG. <p> Improvement should not be limited to a single phase or a single feature within a phase. Some systems learn compiled forms during the aquisition phase seeking faster response for future exact events. For example forming schemas in EBL <ref> [Dejong86] </ref> learning only in the acquisition phase and having speed as its main performance criteria. Other systems (e.g. <p> The accuracy of the learned concept and the generality of the different predicates in the expression of the generalized concept should be considered. It is the accuracy of the concept that led <ref> [DeJong86] </ref> to propose specialization (a method for concept refinement) as a solution to over-generalized concepts for achieving better operationality. <p> If SWALE is exposed to a number of new situations leading to generalizations of its XPs, then SWALE will suffer from the generalization problems (like [Keller88] and <ref> [Dejong86] </ref>), especially if XPs are the only vehicle of knowledge available in the system. 3.2.5 EBL and EBG: Are These Enough? In the past five years there have been a number of variations, extensions and applications to EBL systems. This shows that EB-type systems are widely accepted. <p> The reason for our speculation rose from the different systems that have concept formation as their goal but did not provide what a concept is. Those systems that are categorized as concept formation systems form concepts that may not seem comparable. For example, the kidnapping concept <ref> [Dejong86] </ref> is different from the arch concept [Winston75]. The arch concept is usually referred to as learning a structural concept. The kidnapping concept learned is more of a behavioral type concept. These behavioral concepts are harder to formalize than structural concepts. <p> These differences if not stated (or even better, defined), will make it hard to compare and measure the efficiencies of concept learning systems. <ref> [Dejong86] </ref> noted, concerning his EBL system, that: "The domains they have chosen have little in common with the domains of Mitchell's (in EBG), so it is hardly surprising that we have reached different conclusions about explanation-based learning." It was not stated in what way were the domains different, and what exactly <p> The premise underlying the schema notion is that information about the likely properties of the environment is stored in memory in clusters that can be accessed as large units which can serve to generate plausible inferences and problem solutions. <ref> [Dejong86] </ref> uses a similar definition for a schema in EBL and uses it interchangeably with the term concept. The difference is that Dejong uses the definition of a schema as is, from [Schank77 & Abeson77, Charniak76] and does not clarify if there is any distinction between it and a concept. <p> Unfortunately, neither in the EBL paradigm description, nor in the kidnapping example given to explain the system, or in [Mooney88]'s thesis, is there any other kind of learning mentioned other than generalization. In DeJong's' solution to the dilemnas Mitchell's system suffered from, as discussed in <ref> [DeJong86] </ref>, he introduced two things: the learning of schemata, and an alternative method to goal regression. Neither of the two things Dejong introduced have the ability to do any kind of learning other than generalization. <p> The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators [Korf83], chunks [Laird86], or concepts <ref> [DeJong86] </ref> in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Dietterich86] <author> Dietterich, Thomas G. </author> <year> (1986). </year> <title> "Learning at the Knowledge Level." </title> <booktitle> In Machine Learning 1, </booktitle> <volume> No. </volume> <pages> 3. </pages> <address> Boston: </address> <publisher> Kluwer Academic Publishers. </publisher> <pages> pp. 287-315. </pages>
Reference-contexts: Thus, we can see why knowledge acquisition is an important type of learning. Knowledge level learning was first introduced by [Newell81] to be a type of learning for acquiring knowledge. <ref> [Dietterich86] </ref> criticized Newells' definition and defined knowledge level learning systems to be: A system is said to exhibit knowledge level learning if it exhibits a positive change in its knowledge level description over time.
Reference: [Dietterich88] <author> Dietterich, Thomas G. & Bennett, James S., </author> <year> (1988). </year> <institution> "Varieties of Operationality." In Ore-gon State University Technical Report 88-30-6, Department of Computer Science, Oregon State University, Corvallis, Oregon, </institution> <month> 97331. </month>
Reference: [Ellman85] <author> Ellman, T., </author> <year> (1985). </year> <title> "Generalizing Logic Circuit Design by Analyzing Proofs of Correctness." </title> <booktitle> In Proceedings of the Ninth International Joint Conference on Artificial Intelligence. </booktitle> <address> Los Angeles, CA. </address>
Reference-contexts: EBL claims to have a better algorithm for achieving a more desirable operational generalized concept then previous systems <ref> [Ellman85, Mitchell86, Winston83] </ref>. Dejong discussed several aspects about what an explanation based system should take into account. In discussing these points Dejong drew several comparisons among other systems. In particular, DeJong reviewed [Mitchell86]'s EBG system showing the various limitations it faced.
Reference: [Fikes72] <author> Fikes, Richard E., Hart, Peter E., & Nilsson, Nils J., </author> <year> (1972). </year> <title> "Learning and Executing Generalized Robot Plans." </title> <journal> In Artificial Intelligence, </journal> <volume> vol. 3, no. </volume> <pages> 1-4, </pages> <month> Winter </month> <year> 1972. </year> <pages> pp. 251-288. </pages>
Reference-contexts: The issue we need to discuss now is the goal's second point: acquiring knowledge to improve system's performance over time. It has rarely been explicitly mentioned that speed is the only criteria for the performance achieved. However, it became clearly understood as [Mooney88] stated when comparing the STRIPS algorithm <ref> [Fikes72] </ref>, the EBG algorithm [Mitchell86], and his own EGGS algorithm (used in EBL), that: It is reasonably clear that all of the above algorithms compute the same desired generalized explanation.
Reference: [Hirsh88] <author> Hirsh, H. </author> <year> (1988). </year> <title> Reasoning about Operationality for explanation-based learning. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning. </booktitle> <address> Ann Arbor, Michigan: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <month> 32 </month>
Reference: [Hirsh88a] <author> Hirsh, H. </author> <year> (1988). </year> <title> Empirical techniques for Repairing imperfect theories. </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 57-61. </pages>
Reference-contexts: This leaves Kellers' solution for achieving a better operationality for a general concept very weak. Kellers' solution reduces the problem of knowledge to a simple matter of speed-up, and not to the understanding and evaluation of the semantics of the learned concept. 3.2.3 Reasoning About Operationality In <ref> [Hirsh88a] </ref>, limits of operationality are introduced to restrict a concept to the situation it is intended to be used in. The restriction is enforced by including the conditions from the situation a concept is learned from in the generalization (i.e., general concept).
Reference: [Holland86] <author> Holland, J. H., </author> <year> (1986). </year> <title> Escaping Brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach. </booktitle> <volume> Vol. </volume> <editor> II R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA., </address> <pages> pp. 593-623. </pages>
Reference: [Holland87] <author> Holland, John H., Holyoak, Keith J., Nisbett, Richard E. & Thagard, Paul R., </author> <year> (1987). </year> <title> Induction., </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Unfortunately, this is how a concept is implicitly defined by most AI concept formation systems. Another slightly different opinion is how <ref> [Holland87] </ref> views a concept to be, as just a different label for a schema. <p> Neither of the two things Dejong introduced have the ability to do any kind of learning other than generalization. To clarify this more, we will discuss how schemata as a knowledge representation tool is not suitable for refinement. A strong argument is provided by <ref> [Holland87] </ref> regarding the schema representation and its flexibility. This leaves the second solution DeJong introduced which is an alternative method to goal regression. Goal regression is an algorithm used in EGGS for forming the generalized concept (i.e. schema in EBL) which has nothing to do with refinement. <p> This can be more clear when we see what a schema looks like from the Capture-Bargain example (see figure 2)[Mooney88]. <ref> [Holland87] </ref> has made clear the limitations schema-type systems face. He saw how the utility and attractiveness of the schema notion have blinded investigators to some of its very real limitations. Although schemas are valuable for chunking information together, they exact a toll in inflexibility.
Reference: [Kass86] <author> Kass, Alex, </author> <year> (1986). </year> <title> "Modifying Explanations to Understand Stories." </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Amherst, Massachusetts, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: The terms knowledge and adaptation as introduced in section 2 help in understanding such issues better. 3.2.4 Learning Explanations by Incremental Adaptation [Kass88] introduces a system that incrementally learns new explanations (explanation patterns). SWALE <ref> [Kass86, Leake86] </ref> is a system that has a memory formed of eXplanation Patterns (XPs) as components. Each XP represents an annotated causal view of an episode or a generalized class of episodes (like MOP [Schank82]). The explanation construction algorithm is activated only at performance.
Reference: [Kass88] <author> Kass, Alex & Owens, Christopher C., </author> <year> (1988). </year> <title> "Learning New Explanations by Incremental Adaptation." </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 180-184. </pages>
Reference-contexts: The terms operationality and generality are used in different contexts; this blurs there meaning and unclarifies the goals EB-systems are really seeking. The terms knowledge and adaptation as introduced in section 2 help in understanding such issues better. 3.2.4 Learning Explanations by Incremental Adaptation <ref> [Kass88] </ref> introduces a system that incrementally learns new explanations (explanation patterns). SWALE [Kass86, Leake86] is a system that has a memory formed of eXplanation Patterns (XPs) as components. Each XP represents an annotated causal view of an episode or a generalized class of episodes (like MOP [Schank82]).
Reference: [Kedar-Cabelli87] <author> Kedar-Cabelli, Smadar T., </author> <year> (1987). </year> <title> "Explanation-Based Generalization as Resolution Theorem Proving." </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 383-389. </pages>
Reference-contexts: The prover will regress over the rules and facts to generalize or specialize depending on the phase the system is in. The process of generalizing or specializing a low-level action is done using a modified method of regression as proposed by <ref> [Kedar-Cabelli87] </ref>. We will employ this method to conduct our resolution theorem proving. This method was presented in EGGS [Mooney88] to correct the error found in the EBG algorithm [Mitchell86].
Reference: [Keller87] <author> Keller, Richard M., </author> <year> (1987). </year> <title> "Concept Learning in Context." </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 91-102. </pages>
Reference: [Keller88] <author> Keller, Richard M., </author> <year> (1988). </year> <title> "Operationality and Generality in Explanation-Based Learning: </title> <booktitle> Separate dimensions or opposite endpoints?" In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 153-157. </pages>
Reference-contexts: In short, all that IMEX has achieved is a modification of the explanation and formation of a new concept, which does not bring it ahead of EBG in forming the same new concept given the same domain knowledge in the required format. 3.2.2 Operationality and Generality in EBL In <ref> [Keller88] </ref>, the controversy of generality vs. operationality is discussed. This evolved due to the popular misconception (as claimed by Keller) that less general descriptions are necessarily more operational, and in turn, that more general descriptions are less operational. Unfortunately, [Keller88]'s method of evaluating operationality does not depend on the description level <p> Unfortunately, <ref> [Keller88] </ref>'s method of evaluating operationality does not depend on the description level (whether general or specific), but depends on some hardware speed criteria. Since the performance objective in EBL systems is execution time [Mitchell86,Dejong86,Keller88], operationality was then based on the speed of recognizing a concept [Keller88]. An example was introduced to show how a more general concept can still be more operational. A concept of a CONTAINER would be less operational than a concept of a CUP (a specific kind of CONTAINER). <p> If SWALE is exposed to a number of new situations leading to generalizations of its XPs, then SWALE will suffer from the generalization problems (like <ref> [Keller88] </ref> and [Dejong86]), especially if XPs are the only vehicle of knowledge available in the system. 3.2.5 EBL and EBG: Are These Enough? In the past five years there have been a number of variations, extensions and applications to EBL systems. This shows that EB-type systems are widely accepted.
Reference: [Kodratoff86] <author> Kodratoff, Yves & Ganascia, Jean-Gabriel, </author> <year> (1986). </year> <title> "Improving the Generalization Step in Learning." </title> <booktitle> In Machine Learning: An Artificial Intelligence Approach. </booktitle> <volume> Vol. </volume> <editor> II R. S. Michalski, J. G. Carbonell, & T. M. Mitchell (Eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA., </address> <pages> pp. 215-244. </pages>
Reference-contexts: Although, this is what most humans have in mind (and maybe AI scientists as well), it is not accurate enough to use for defining the goals concept formation learning systems should use in AI. <ref> [Kodratoff86] </ref> gave one of the early definitions of a concept as a one way implication. The concept he gave is defined as: 8x [P (x) ) Concept (x)] where Concept is the name of the concerned concept and P (x) is its recognition function.
Reference: [Korf83] <author> Korf, Richard E., </author> <year> (1983). </year> <title> "Learning to Solve Problems by Searching for Macro Operators." </title> <type> Doctoral Dissertation, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: Three limitations can be currently stated: * In small domains with well defined parameters (e.g. 8-puzzle game), the system will perform less efficiently over time than other learning systems. The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators <ref> [Korf83] </ref>, chunks [Laird86], or concepts [DeJong86] in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Laird86] <author> Laird, John E., Rosenbloom, P. S., Newell, Alan, </author> <year> (1986). </year> <title> "Universal Subgoaling and Chunking: The automatic generation and learning of goal hierarchies." In Machine Learning Journal, </title> <address> Hingham, MA, </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators [Korf83], chunks <ref> [Laird86] </ref>, or concepts [DeJong86] in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Laird86] <author> Laird, John E., Rosenbloom, P. S., Newell, Alan, </author> <year> (1986). </year> <title> "Chunking in SOAR: The anatomy of a general learning mechanism." </title> <journal> In Machine Learning Journal, </journal> <volume> 1, </volume> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 11-46. </pages>
Reference-contexts: The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators [Korf83], chunks <ref> [Laird86] </ref>, or concepts [DeJong86] in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Laird86] <author> Laird, John E., Rosenbloom, P. S., Newell, Alan, </author> <year> (1986). </year> <title> "Universal Subgoaling and Chunking: The automatic generation and learning of goal hierarchies." In Machine Learning Journal, </title> <address> Hingham, MA, </address> <publisher> Kluwer Academic Publishers. </publisher>
Reference-contexts: The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators [Korf83], chunks <ref> [Laird86] </ref>, or concepts [DeJong86] in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Laird86] <author> Laird, John E., Rosenbloom, P. S., Newell, Alan, </author> <year> (1986). </year> <title> "Chunking in SOAR: The anatomy of a general learning mechanism." </title> <journal> In Machine Learning Journal, </journal> <volume> 1, </volume> <publisher> Kluwer Academic Publishers, </publisher> <pages> pp. 11-46. </pages>
Reference-contexts: The decrease in efficiency will occur due to the presence of compiled formats learned from previous experience like macro-operators [Korf83], chunks <ref> [Laird86] </ref>, or concepts [DeJong86] in the other systems. The number of complied formats tend not to increase due to the simplicity of the problem and its static nature. These systems will not get cluttered with compiled formats, hence their performance will not degrade.
Reference: [Leake86] <author> Leake, D. & Owens, Christopher C., </author> <year> (1986). </year> <title> "Organizing Memory for Explanation." </title> <booktitle> In Proceedings of the Eighth Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Amherst, Massachusetts, </address> <month> August </month> <year> 1986. </year> <month> 33 </month>
Reference-contexts: The terms knowledge and adaptation as introduced in section 2 help in understanding such issues better. 3.2.4 Learning Explanations by Incremental Adaptation [Kass88] introduces a system that incrementally learns new explanations (explanation patterns). SWALE <ref> [Kass86, Leake86] </ref> is a system that has a memory formed of eXplanation Patterns (XPs) as components. Each XP represents an annotated causal view of an episode or a generalized class of episodes (like MOP [Schank82]). The explanation construction algorithm is activated only at performance.
Reference: [Lebowitz86] <author> Lebowitz, Micheal, </author> <year> (1986). </year> <title> Not the path to perdition: The utility of similarity-based learning. </title> <booktitle> In Proceedings of the AAAI-86, </booktitle> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986, </year> <pages> pp. 533-537. </pages>
Reference: [Mahadevan88] <author> Mahadevan, S., Natarajan, B. K., & Tadepalli P., </author> <year> (1988). </year> <title> "A Framework for Learning as Improving Problem-Solving Performance." </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 215-219. </pages>
Reference-contexts: This means that if EBL systems lose speed in their performance, then they would have failed one of their two goals. <ref> [Mahadevan88] </ref> gives rise to the issue of learning in EBL systems whether it is of any value regarding speed efficiency. Mahadevan states: Why Learn?: It is assumed that a domain theory that can be used to explain the training examples is available to the learning system.
Reference: [Michalski87] <author> Michalski, Ryszard S.,(1987). </author> <title> How to learn imprecise concepts: A method for employing a two-tiered knowledge representation in learning. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 50-58. </pages>
Reference-contexts: If general descriptions of such concepts can be formed, they will be so vague as to be useless for classification of new cases. Human concepts, except for some special cases, allow a varying degree of match between them and observed instances which have context-dependent meaning. <ref> [Michalski87] </ref> refers to concepts, to be in general, as symbolic structures denoting classes of objects. The boundaries of these classes are flexible and context-dependent. 14 Michalski implements the flexibility by a Base Concept Representation (BCR) and an Inferential Concept Interpretation (ICI). <p> The inferential concept interpretation applies an interpretation method associated with the concept. This method dynamically determines the actual meaning of a concept instance in a given context by conducting inference on the BCR <ref> [Michalski87] </ref>. Thus an adequate concept representation should include not only a description that permits one to recognize the given concept among other concepts or to evaluate the typicality of its members, but also a number of other components.
Reference: [Michalski87] <author> Michalski, R. S., </author> <year> (1987). </year> <title> "How to Learn Imprecise Concepts: A method for employing a two-tiered knowledged representation in learning." </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning., </booktitle> <address> Irvine, CA.: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA. </address> <pages> pp. 50-58. </pages>
Reference-contexts: If general descriptions of such concepts can be formed, they will be so vague as to be useless for classification of new cases. Human concepts, except for some special cases, allow a varying degree of match between them and observed instances which have context-dependent meaning. <ref> [Michalski87] </ref> refers to concepts, to be in general, as symbolic structures denoting classes of objects. The boundaries of these classes are flexible and context-dependent. 14 Michalski implements the flexibility by a Base Concept Representation (BCR) and an Inferential Concept Interpretation (ICI). <p> The inferential concept interpretation applies an interpretation method associated with the concept. This method dynamically determines the actual meaning of a concept instance in a given context by conducting inference on the BCR <ref> [Michalski87] </ref>. Thus an adequate concept representation should include not only a description that permits one to recognize the given concept among other concepts or to evaluate the typicality of its members, but also a number of other components.
Reference: [Minton88] <author> Minton, S., </author> <year> (1988). </year> <title> "Quantitative Results Concerning the Utility of Explanation-Based Learning." </title> <booktitle> In Proceedings of the AAAI-88, </booktitle> <address> St. Paul, Minnesota, </address> <month> August 21-26, </month> <year> 1988, </year> <pages> pp. 564-569. </pages>
Reference-contexts: That is, if these systems try to operate in complex domains with a large number of concepts, they will lose their performance objective regarding speed. <ref> [Minton88] </ref> described this as the utility problem, where the behavior of the system may be very poor as the learned descriptions grow in number and size. [Minton88] showed how this problem cannot be solved by the development of highly parallel hardware or powerful indexing schemes. <p> is, if these systems try to operate in complex domains with a large number of concepts, they will lose their performance objective regarding speed. <ref> [Minton88] </ref> described this as the utility problem, where the behavior of the system may be very poor as the learned descriptions grow in number and size. [Minton88] showed how this problem cannot be solved by the development of highly parallel hardware or powerful indexing schemes. It is definitely the problem of generating schemas or chunks (i.e. any type of packaged format) in an unconstrained fashion which clutters the system thus leaving it very slow. <p> The increase in the amount of chunks, macro-operators, concepts, examples, etc. (from here on called "compiled forms") to cover every small drift in the environment will cause the system to slow down, due to the utility problem <ref> [Minton88] </ref>. In most cases these compiled forms are replicated for just a small difference in one of its units (i.e. variables, predicates, elements, etc.). <p> This will increase the number of concepts in the system, and thus slowing it down (i.e. back to the utility problem as described by <ref> [Minton88] </ref>). It might be better to study the controversy between accuracy, speed, and the number of learned concepts versus generality rather than operationality versus generality. The terms operationality and generality are used in different contexts; this blurs there meaning and unclarifies the goals EB-systems are really seeking. <p> The difference between them lies in the number of unifications and substitutions required and the order in which they are performed In other words, the three algorithms appear to compute the same thing, but have different efficiencies <ref> [Minton88] </ref>. This means that if EBL systems lose speed in their performance, then they would have failed one of their two goals. [Mahadevan88] gives rise to the issue of learning in EBL systems whether it is of any value regarding speed efficiency. <p> Hence it is unclear why any learning is necessary. In the worst case, storing generalized examples might actually slow down the computation rather than speed it up. The argument given by Mahadevan agrees closely with what <ref> [Minton88] </ref> pointed out regarding the utility problem facing EB-systems. The first part of the goal, forming a general operational concept, is not quite achieved either. This gives rise to seek a solution to overcome some of the issues discussed above that views EBL systems to be less appealing.
Reference: [Mitchell86] <author> Mitchell, T. M., Keller, R. M., & Kedar-Cabelli, S. T. </author> <year> (1986). </year> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning Journal, </journal> <volume> 1(1), </volume> <pages> pp. 47-80. </pages>
Reference-contexts: In [Braverman88], the example of the safe-to-stack <ref> [Mitchell86] </ref> is presented to show how it can be more operational using the proposed IMEX algorithm. <p> It is the accuracy of the concept that led [DeJong86] to propose specialization (a method for concept refinement) as a solution to over-generalized concepts for achieving better operationality. The example of the CUP <ref> [Mitchell86] </ref> was not operational in DeJong's opinion due to the over-general description of the handle position (If on top, it would be as a pail, thus not useful for drinking). <p> It has rarely been explicitly mentioned that speed is the only criteria for the performance achieved. However, it became clearly understood as [Mooney88] stated when comparing the STRIPS algorithm [Fikes72], the EBG algorithm <ref> [Mitchell86] </ref>, and his own EGGS algorithm (used in EBL), that: It is reasonably clear that all of the above algorithms compute the same desired generalized explanation. <p> The process of generalizing or specializing a low-level action is done using a modified method of regression as proposed by [Kedar-Cabelli87]. We will employ this method to conduct our resolution theorem proving. This method was presented in EGGS [Mooney88] to correct the error found in the EBG algorithm <ref> [Mitchell86] </ref>. The arguments used by predicates in each of the ten action statements mentioned above have a special syntax denoting four different types. The four types of arguments are used by the predicates in the action statement category rules and/or facts. <p> EBL claims to have a better algorithm for achieving a more desirable operational generalized concept then previous systems <ref> [Ellman85, Mitchell86, Winston83] </ref>. Dejong discussed several aspects about what an explanation based system should take into account. In discussing these points Dejong drew several comparisons among other systems. In particular, DeJong reviewed [Mitchell86]'s EBG system showing the various limitations it faced.
Reference: [Mooney88] <author> Mooney, Raymond Joseph, </author> <year> (1988). </year> <title> "A General Explanation-Based Learning Mechanism and Its Application to Narrative Understanding." </title> <type> PhD. Thesis, </type> <institution> University of Illinois, Urbana-Champaign, Illinois, </institution> <month> January </month> <year> 1988. </year>
Reference-contexts: The issue we need to discuss now is the goal's second point: acquiring knowledge to improve system's performance over time. It has rarely been explicitly mentioned that speed is the only criteria for the performance achieved. However, it became clearly understood as <ref> [Mooney88] </ref> stated when comparing the STRIPS algorithm [Fikes72], the EBG algorithm [Mitchell86], and his own EGGS algorithm (used in EBL), that: It is reasonably clear that all of the above algorithms compute the same desired generalized explanation. <p> The process of generalizing or specializing a low-level action is done using a modified method of regression as proposed by [Kedar-Cabelli87]. We will employ this method to conduct our resolution theorem proving. This method was presented in EGGS <ref> [Mooney88] </ref> to correct the error found in the EBG algorithm [Mitchell86]. The arguments used by predicates in each of the ten action statements mentioned above have a special syntax denoting four different types. <p> EBL may tweak a given example distorting some of its information to satisfy a set of criteria used in one of its modules during the concept formation process. In <ref> [Mooney88] </ref> section 11.3 for generalizing explanations, the EBL faces difficulties when forming a general explanation. For example, the system tries to form a concept of murdering for inheritance.
Reference: [Morganstern87] <author> Morganstern, Leora, </author> <year> (1987). </year> <title> "Knowledge Preconditions for Actions and Plans." </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 867-874. </pages>
Reference: [Newell81] <author> Newell, A. </author> <year> (1981). </year> <title> "The Knowledge Level." </title> <journal> In AI Magazine, </journal> <volume> 2. </volume> <pages> pp. 1-20. </pages>
Reference-contexts: This is why, for instance, it is important in a lot of cases for students to know the theme (skeleton) of the final exam to facilitate their study efforts. Thus, we can see why knowledge acquisition is an important type of learning. Knowledge level learning was first introduced by <ref> [Newell81] </ref> to be a type of learning for acquiring knowledge. [Dietterich86] criticized Newells' definition and defined knowledge level learning systems to be: A system is said to exhibit knowledge level learning if it exhibits a positive change in its knowledge level description over time.
Reference: [Pazzani85] <author> Pazzani, M. J., </author> <year> (1985). </year> <title> Explanation and generalization based memory. </title> <booktitle> In Proceedings of the Seventh Annual Conference of the Cognitive Science Society. </booktitle> <publisher> Lawrence Erlbaum, </publisher> <address> Irvine, CA., </address> <year> 1985. </year>
Reference: [Pazzani87] <author> Pazzani, M. J., Dyer, M., & Flowers, M., </author> <title> (1987) Using prior learning to facilitate the learning of new causal theories. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence. </booktitle> <address> Milan, Italy, </address> <month> August </month> <year> 1987, </year> <pages> pp. 277-279. </pages>
Reference: [Quinlan83] <author> Quinlan, J.R. </author> <year> (1983). </year> <title> "Learning efficient classification procedures and their application to chess endgames." </title> <editor> In Michalski, R.S., Carbonell, J.G., and Mitchell, T.M. </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <pages> pp. 463-482. </pages> <address> Palo Alto, CA: </address> <publisher> Tioga Publishing Company. </publisher>
Reference-contexts: That is why we tend to view an intelligent learner in a complex domain to be using a knowledge-intensive approach rather than an empirical approach (e.g. ID3 <ref> [Quinlan83] </ref>). The intelligent learner will use its knowledge to direct its behavior in the environment. Knowledge is thus needed as a vehicle for an AI learning system to interact with complex domains.
Reference: [Rosenbloom87] <author> Rosenbloom, Paul S., Laird, John E., Newell, Alan, </author> <year> (1987). </year> <title> "Knowledge Level Learning in SOAR." </title> <type> Personal communication. </type>
Reference: [Schank77] <author> Schank, R. C. & Abelson, R. P., </author> <year> (1977). </year> <title> Scripts, plans, goals, and understanding: An inquiry into human knowledge structures. </title> <address> Hillsdale, New Jersey: </address> <publisher> Lawrence Erlbaum. </publisher>
Reference-contexts: In any case human intervention is required. It would be easier if the system interface is configured closer to the human language understanding used by machines. Conceptual dependency <ref> [Schank77] </ref> and thematic-roles from natural language understanding use a type of categories to constrain how verbs and thematic role instances form sentences. It is found that through such categories it is easier to analyze some human written text. <p> It is found that through such categories it is easier to analyze some human written text. That is why we choose categories similar to the categories used in conceptual dependency's (CD's) <ref> [Schank77] </ref> to facilitate the human communication and understanding with the system. 15 Requirements: According to [Morgenstern87]'s theory of action, for every theory of action there is an explicit or implicit ontology of action.
Reference: [Schank82] <author> Schank, R. C., </author> <year> (1982). </year> <title> Dynamic Memory. </title> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference-contexts: SWALE [Kass86, Leake86] is a system that has a memory formed of eXplanation Patterns (XPs) as components. Each XP represents an annotated causal view of an episode or a generalized class of episodes (like MOP <ref> [Schank82] </ref>). The explanation construction algorithm is activated only at performance. When SWALE is called upon to explain an event, the system can retrieve an entire XP and apply it as a whole. <p> What is to be done with situations or things that are not neatly matched by any existing schema? Schemata are useful to some extent if combined with other knowledge structures (e.g. rules). Holland has referred to Schank's schema <ref> [Schank82] </ref> (which Dejong uses), and to how the "restaurant script" is very inflexible. Even [Schank82] acknowledged the inflexibility of schemas. This leaves the EBL system unable to achieve its goal of concept refinement, thus restricting it to an EBG system. <p> Holland has referred to Schank's schema <ref> [Schank82] </ref> (which Dejong uses), and to how the "restaurant script" is very inflexible. Even [Schank82] acknowledged the inflexibility of schemas. This leaves the EBL system unable to achieve its goal of concept refinement, thus restricting it to an EBG system.
Reference: [Scott83] <author> Scott, P.D. </author> <year> (1983). </year> <title> "Learning: The construction of a posteriori knowledge structures." </title> <booktitle> Proceedings of The National Conference on Artificial Intelligence AAAI-83. </booktitle> <pages> pp. 359-363. </pages>
Reference-contexts: Of course in such an example the system will perform well, as the pyramid's location are of a quiet static nature. But what if the population changes (which is an inherented feature in real life domains)? <ref> [Scott83] </ref> pointed out that learning, in Simon's view is a functional definition. That is, it defines learning in terms of what it achieves rather then how it achieves it. This might mislead us in cases where performance is enhanced without an act of learning taking place.
Reference: [Segre88] <author> Segre, A. M., </author> <year> (1988). </year> <title> Operationality and real-world plans. </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 158-163. 34 </pages>
Reference: [Selbig86] <author> Selbig, </author> <title> Joachim (1986). Knowledge acquisition by inductive learning from examples. In Analogical and Inductive Inference Klaus P. </title> <editor> Jantke (Ed.), </editor> <publisher> Springer-Verlag, Berlin, W. </publisher> <address> Germany (1987), </address> <pages> pp. 145-163. </pages>
Reference: [Shavlik88] <author> Shavlik, J. W., </author> <year> (1988). </year> <title> Issues in generalizing to N in explanation-based learning. </title> <booktitle> In Proceedings of the AAAI Spring Symposium Series on Explanation-Based Learning. </booktitle> <address> Stanford, California: </address> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA., </address> <pages> pp. 78-83. </pages>
Reference: [Simon83] <author> Simon, H.A. </author> <year> (1983). </year> <title> "Why should machines learn?" In Michalski, R.S., </title> <editor> Carbonell, J.G., and Mitchell, T.M. (Eds.) </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <pages> pp. 25-33. </pages> <address> Los Altos, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: First other opinions of learning are reviewed followed by a discussion by what is meant by learning in SEIF. Many suggestions have appeared in literature about the definition of learning. <ref> [Simon83] </ref> proposed the most widely accepted definition: "Learning is any change in a system that allows it to perform better the second time on repetition of the same task or another task drawn from the same population" We have to watch for the limitations of same population as noted above. <p> For example the motor of a car gives better performance after an oil change, although the system (car motor) has not learned that (maybe the driver is the one who learned such an act)! <ref> [Simon83] </ref> excluded some other types of learning. Viewing learning as an event, it should influence the system's potential behavior subsequent to that event. System behavior can be influenced in a manner other than faster performance.
Reference: [Webster] <institution> Merriam Webster Unabridged Dictionary. </institution>
Reference-contexts: That is why we feel it is important to view the different opinions of what people in AI define a concept to be. We first start with what the definition of a concept is, as given by <ref> [Webster] </ref>: An idea, especially a generalized idea of a class of objects; a thought; general notion. <p> We deal with actions like making a phone call, riding a bus, kidnapping someone or simple actions of the same sort. It can be viewed that actions are a subclass of concepts. To clarify what we mean by an action, we refer to <ref> [Webster] </ref> in defining an action to be: * The doing of something; hence, the state of acting or moving; exertion of power or force, as when one body acts on another. * The effect or influence of something (on something else); as the action of a drug; motion produced. * An
Reference: [Winston75] <author> Winston, Patrick Henry, </author> <year> (1975). </year> <title> "Learning structural descriptions from examples." </title> <editor> In Winston, P. H. (Ed.) </editor> <booktitle> The Psychology of Computer Vision. </booktitle> <publisher> McGraw-Hill Book Company, </publisher> <address> New York, </address> <year> 1975. </year> <title> Based on a PhD thesis, </title> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1970. </year>
Reference-contexts: Those systems that are categorized as concept formation systems form concepts that may not seem comparable. For example, the kidnapping concept [Dejong86] is different from the arch concept <ref> [Winston75] </ref>. The arch concept is usually referred to as learning a structural concept. The kidnapping concept learned is more of a behavioral type concept. These behavioral concepts are harder to formalize than structural concepts.
Reference: [Winston83] <author> Winston, Patrick Henry, Binford, Thomas O., & Lowry, Michael R., </author> <year> (1983). </year> <title> "Learning Physical Descriptions from Functional Definitions, Examples and Precedents." </title> <booktitle> In Proceedings of The National Conference on Artificial Intelligence, </booktitle> <address> Washington, D.C., </address> <year> 1983. </year> <month> 35 </month>
Reference-contexts: EBL claims to have a better algorithm for achieving a more desirable operational generalized concept then previous systems <ref> [Ellman85, Mitchell86, Winston83] </ref>. Dejong discussed several aspects about what an explanation based system should take into account. In discussing these points Dejong drew several comparisons among other systems. In particular, DeJong reviewed [Mitchell86]'s EBG system showing the various limitations it faced.
References-found: 52

