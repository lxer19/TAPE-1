URL: ftp://ftp.cs.toronto.edu/pub/parallel/Okrieg_PhD.ps.Z
Refering-URL: http://www.eecg.toronto.edu/~okrieg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: HFS: A flexible file system for shared-memory multiprocessors  
Author: by Orran Krieger 
Degree: A thesis submitted in conformity with the requirements for the degree of Doctor of Philosophy  
Address: Toronto  
Affiliation: Department of Electrical and Computer Engineering, University of Toronto.  Graduate Department of Electrical and Computer Engineering Computer Engineering Group University of  
Note: PhD Dissertation,  c flOrran Krieger 1994  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Vadim Abrossimov, Marc Rozier, and Marc Shapiro. </author> <title> Generic virtual memory management for operating system kernels. </title> <booktitle> In Proc. 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 123-136, </pages> <address> Litchfield Park, Arizona, </address> <month> December </month> <year> 1989. </year>
Reference-contexts: The principle of bounded overhead also applies to the space costs of the internal data structures of the system. While the data structures must grow at a rate proportional to the physical resources of the hardware <ref> [1, 130] </ref>, the principle of bounded space cost restricts growth to be no more than linear.
Reference: [2] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera computer system. </title> <booktitle> In 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <year> 1990. </year>
Reference-contexts: ASF has been implemented for HURRICANE [129, 128], SunOS [63], IRIX [118], AIX [91], and HP-UX [133]. Also ASF has been adopted by TERA computer <ref> [2] </ref> as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> In addition to the Alloc Stream Interface (ASI), our implementation exports the Unix I/O and stdio I/O interfaces. We have had requests for ASF from many researchers and from dozens of companies, and ASF has been adopted by TERA computer <ref> [2] </ref> as the I/O library for their parallel supercomputer. Our implementation of HFS uses mapped-file I/O to minimize copying costs, and hence the HURRICANE memory manager is involved in most requests to read and write file data.
Reference: [3] <author> James W. Arendt. </author> <title> Parallel genome sequence comparison using a concurrent file system. </title> <type> Technical Report UIUCDCS-R-91-1674, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <year> 1991. </year>
Reference-contexts: These problems have similar I/O requirements to the applications described above. A particular example of a Grand Challenges problem is the genome-sequence pattern matching problem <ref> [3] </ref>. The sequence comparison is embarrassingly parallel since a single input sequence has to be compared against a large number of independent sequences in the data base.
Reference: [4] <author> Ramesh Balan and Kurt Gollhardt. </author> <title> A scalable implementation of virtual memory HAT layer for shared memory multiprocessor machines. </title> <booktitle> In Summer '92 USENIX, </booktitle> <pages> pages 107-115, </pages> <address> San Antonio, TX, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [4, 5, 15, 18, 60] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [5] <author> David Barach, Robert Wells, and Thomas Uban. </author> <title> Design of parallel virtual memory management on the TC2000. </title> <type> Technical Report 7296, </type> <institution> BBN Advanced Computers Inc., </institution> <address> 10 Moulton Street, Cambridge, Massachusetts, 02138, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [4, 5, 15, 18, 60] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [6] <author> Amnon Barak and Yoram Kornatzky. </author> <title> Design principles of operating systems for large scale multicomputers. </title> <institution> Computer Science RC 13220 (#59114), IBM Research Division, T.J. Watson Research Center, </institution> <address> Yorktown Heights, NY 10598, </address> <month> October </month> <year> 1987. </year>
Reference-contexts: In this section we discuss related work in file system architectures and implementations. In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations <ref> [6, 22, 53, 98, 132] </ref>. A good overview of research in this area is given by Levy and Sibershatz [80]. <p> This also means that the file system should enact policies that balance the I/O demand across the system disks. Bounded overhead: The overhead for each independent system service request must be bounded by a constant <ref> [6] </ref>. If the overhead of each service call increases with the size of the system, then the system will ultimately saturate. Hence, the demand on any single resource cannot increase with the size of the system.
Reference: [7] <author> Azer Bestavros. </author> <title> IDA-based redundant arrays of inexpensive disks. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 2-9, </pages> <year> 1991. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [8] <author> D. Bitton and J. Gray. </author> <title> Disk shadowing. </title> <booktitle> In 14th International Conference on Very Large Data Bases, </booktitle> <pages> pages 331-338, </pages> <year> 1988. </year>
Reference-contexts: If the load on disks varies, then it may be a good idea to direct write operations to the least loaded disks, especially in a multiprogrammed environment. Replicating a file can be used both to get better performance <ref> [8, 120] </ref> and to manage locality (since the more local copy of a disk block can be used to satisfy a read request).
Reference: [9] <author> R. Bordawekar, J. del Rosario, and Alok Choudhary. </author> <title> Design and evaluation of primitives for parallel I/O. </title> <booktitle> In Proceedings Supercomputing, </booktitle> <pages> pages 452-461. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned.
Reference: [10] <author> F. P. Brooks. </author> <title> The Mythical Man Month Essays on Software Engineering. </title> <publisher> Addison Wesley, </publisher> <address> Reading, MA, </address> <year> 1975. </year>
Reference-contexts: CHAPTER 10. CONCLUDING REMARKS 99 We recognized them by designing and implementing everything incorrectly at least once. This experience mirrors the advice given by Butler Lampson in his often cited paper Hints for Computer System Design [76]: Plan to throw one away: you will anyhow <ref> [10] </ref>. If there is anything new about the function of a system, the first implementation will have to be redone completely to achieve a satisfactory (i.e., acceptably small, fast, and maintainable) result. It costs a lot less if you plan to have a prototype.
Reference: [11] <author> Edmund Burke. </author> <title> An overview of system software for the KSR1. </title> <booktitle> In Proceedings of Compcom, </booktitle> <pages> pages 295-299, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: The nCUBE file system [31, 32] provides a mapping interface that is similar (but more constrained) than that provided by Vesta. According to Kotz [67], most file systems of commercial multiprocessors provide the standard Unix I/O interface, typically with extensions for asynchronous I/O. The KSR provides for mapped-file I/O <ref> [11] </ref>. Some file systems (e.g., the CFS file system for the iPSC/2 and iPSC/860 [105]) implement the Unix I/O interface in a user-level library on top of a lower-level internal system-call interface. <p> be read into a remote memory [78], and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks <ref> [11, 31, 36, 66, 105] </ref>, possibly with the stripe size and number of disks to stripe across determined by the application.
Reference: [12] <author> Henry Burkhardt III, Steven Frank, Bruce Knobe, and James Rothnie. </author> <title> Overview of the KSR1 computer system. </title> <type> Technical Report KSR-TR-9202001, </type> <institution> Kendell Square Research, </institution> <address> Boston, </address> <month> February </month> <year> 1992. </year>
Reference-contexts: CHAPTER 2. BACKGROUND AND MOTIVATION 14 2.4.1 The target architecture The class of machine architecture we considered is the class of large-scale NUMA multiprocessors with disks distributed across the multiprocessor (Figure 2.4). NUMA multiprocessors achieve their scalability by using segmented architectures <ref> [12, 79, 131] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
Reference: [13] <author> M. Burrows, C. Jerian, B. Lampson, and T. Mann. </author> <title> On-line data compression in a log-structured file system. </title> <booktitle> In Proc. Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2-9, </pages> <month> Oct </month> <year> 1992. </year> <journal> ACM SIGPLAN Notices. </journal>
Reference-contexts: A log-structured file system requires that a segment cleaner run periodically to compresses the live information from fragmented segments. The log-structured file system has been extended in a number of ways. For example, Burrows et al. describe a method for adding automatic compression that benefits from the large writes <ref> [13] </ref>. The sequential nature and large size of the writes also makes log-structured file systems attractive for automatic migration of data to tertiary storage [64]. <p> ASO classes of this type might use the Lempel-Ziv compression algorithm, designed for whole file access [134], the algorithms described by Burrows et al, that allow random file access <ref> [13] </ref>, or application-specific algorithms, such as that used to compress the data base for the Chinook checkers program [114]. encryption/decryption ASO classes: These classes provide for additional security by encrypting and decrypting files as they are accessed.
Reference: [14] <author> Luis-Felipe Cabrera and Darrell D. E. Long. Swift: </author> <title> Using distributed disk striping to provide high I/O data rates. </title> <journal> Computing Systems, </journal> <volume> 4(4), </volume> <month> Fall </month> <year> 1991. </year>
Reference: [15] <author> Roy H. Campbell, Nayeem Islam, Ralph Johnson, Panos Kougiouris, and Peter Madany. </author> <title> Choices, frameworks and refinement. </title> <booktitle> In 1991 Workshop on Object Orientation in Operating Systems, </booktitle> <pages> pages 9-15, </pages> <year> 1991. </year> <note> 107 BIBLIOGRAPHY 108 </note>
Reference-contexts: Two examples that use an object-oriented approach are the Spring file system [61] and the Choices file system <ref> [15] </ref>. <p> Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [4, 5, 15, 18, 60] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [16] <author> Pei Cao, Swee Boon Lim, Shivakumar Venkataraman, and John Wilkes. </author> <title> The TickerTAIP parallel RAID architecture. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 52-63, </pages> <year> 1993. </year>
Reference-contexts: However, these architectures still use simple disk striping for distributing data across the disks <ref> [16, 127] </ref>.) We assume in our file system that the disks, processors, and memories are all connected using the same network.
Reference: [17] <author> Russel Carter, Bob Ciotti, Sam Fineberg, and Bill Nitzberg. </author> <title> NHT-1 I/O benchmarks. </title> <type> Technical Report RND-92-061, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA 94035-1000, </address> <month> November </month> <year> 1992. </year>
Reference-contexts: NHT-1 is an I/O benchmark developed at NASA to evaluate the I/O systems of parallel machines <ref> [17, 40] </ref>.
Reference: [18] <author> H.H.Y. Chang and B. Rosenburg. </author> <title> Experience porting Mach to the RP3 large-scale shared-memory multiprocessor. </title> <booktitle> Future Generation Computer Systems, </booktitle> <address> 7(2-3):259-267, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [4, 5, 15, 18, 60] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [19] <author> Peter Chen, Garth Gibson, Randy Katz, and David Patterson. </author> <title> An evaluation of redundant arrays of disks using an Amdahl 5890. </title> <booktitle> In Proceedings of the 1990 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 74-85, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [20] <author> Peter M. Chen, Edward K. Lee, Ann L. Drapeau, Ken Lutz, Ethan L. Miller, Srinivasan Seshan, Ken Shirriff, David A. Patterson, and Randy H. Katz. </author> <title> Performance and design evaluation of the RAID-II storage server. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 110-120, </pages> <year> 1993. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [21] <author> D. Cheriton. UIO: </author> <title> A Uniform I/O system interface for distributed systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 5(1) </volume> <pages> 12-46, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: We refer to an interface that can be used in this fashion as a uniform I/O interface. Uniform I/O interfaces allow programs to be independent of the type of data sources and sinks with which they communicate <ref> [21] </ref>. open ( filename, mode ) : opens a stream read ( stream, buffer, nbytes ) : reads nbytes from the current stream offset into buffer and increments the stream offset by nbytes write ( stream, buffer, nbytes ) : writes nbytes from buffer to the current stream offset and increments <p> Other read/write interfaces Other read/write interfaces overcome some of the limitations of Unix I/O. For example, Cheriton's UIO interface <ref> [21] </ref> (i) allows different threads to make random I/O requests without interference, (ii) takes into account the block size of the requested service, and (iii) returns thread-specific errors. <p> If a page is accessed that has not yet been read from disk then a page fault will occur and the faulting process will be blocked until the data becomes available. Other mapped-file interfaces Other interfaces for mapped-file I/O include the HURRICANE BindRegion/UnbindRegion interface (inspired by V <ref> [21] </ref>) and the AIX shmget interface [91]. The BindRegion interface is similar to the mmap interface, except that the management of bound regions is based on regions of an application's address space rather than individual pages. The AIX shmget interface maps entire files into regions of the application address space.
Reference: [22] <author> D. R. Cheriton. </author> <title> The V distributed system. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 314-333, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: In this section we discuss related work in file system architectures and implementations. In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations <ref> [6, 22, 53, 98, 132] </ref>. A good overview of research in this area is given by Levy and Sibershatz [80].
Reference: [23] <author> Ann L. Chervenak and Randy H. Katz. </author> <title> Performance of a disk array prototype. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 188-197, </pages> <year> 1991. </year>
Reference-contexts: The copying between buffers (described in the previous section) increases contention on the memory. For example, researchers who have dramatically improved file I/O bandwidth (by introducing disk arrays [102]) have found that Unix I/O copying overhead makes it difficult to exploit this bandwidth <ref> [23] </ref>. Since the application can specify an arbitrary number of bytes for I/O (and use a buffer with an arbitrary alignment), and since most devices have some fixed block size, the operating system must buffer data, and hence it is difficult to avoid copying when Unix I/O is used. <p> An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [24] <editor> Conner Peripherals, </editor> <publisher> Inc, </publisher> <address> San Jose, CA. </address> <note> CP3200 product manual, v.2 edition, </note> <month> June </month> <year> 1990. </year>
Reference-contexts: I/O hardware Seven Conner CP3200 disks are connected to the HECTOR prototype. The specifications for these disks are given in Table 9.3 <ref> [24] </ref>. The on-disk controller sequentially prefetches disk blocks into the on-disk cache. If we assume data is transferred from the disk at the maximum rate for 8 tracks (given 8 heads), and take the rotational latency and the CHAPTER 9.
Reference: [25] <author> P. F. Corbett, D. G. Feitelson, S. J. Baylor, and J. Prost. </author> <title> Parallel access to files in the Vesta file system. </title> <booktitle> In Proceeding of Supercomputing. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: The tool interface allows applications to take into account the disk/memory locality by executing code on the I/O nodes that store the applications' data. The Vesta file system for the SP2 multicomputer <ref> [25, 26] </ref> provides a record oriented interface optimized for matrix operations. On creating a file, the application specifies the number of physical partitions (normally per-disk sub-files) that the data will be distributed across and the record size. <p> Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix. <p> On read requests, the file system permutes the data between the two, collecting the corresponding data from the different disks. In terms of distribution, the most flexible among existing parallel file systems is the Vesta file system for the SP2 multicomputer <ref> [25, 26] </ref>. It has separate concepts for physical partitions (effectively the number of disks used) and logical partitions. Logical partitions allow applications to specify a mapping between file data and an application's view of the data. <p> There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta <ref> [25, 26] </ref> for the SP2, the OSF/1 file system [136], the file system for the nCUBE [31], and the Rama file system [90]. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. <p> As described in Chapter 2, the ELFS file system [49, 50] provides for similar functionality. Special ASO classes can also be defined to provide for functionality like that of the Vesta file system <ref> [25, 26] </ref>, where an application can request a particular view of a file. The view determines a mapping between the bytes in the file and the order that the application will see these bytes. <p> CONCLUDING REMARKS 98 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta <ref> [25, 26] </ref> for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system [35], and the Rama file system [90]. <p> The fact that the OSF/1 file system for the Paragon multicomputer uses mapped-file I/O [30] suggests that it would not be a problem. However, it is not clear how the mapping facility provided by the Vesta file system <ref> [25] </ref> (where an application can impose a mapping between the bytes in the file and the order that the file system will provide these bytes to the application) can be efficiently supported for the case where the mapping requires a small amount of data to be transferred from each of a
Reference: [26] <author> Peter F. Corbett, Sandra Johnson Baylor, and Dror G. Feitelson. </author> <title> Overview of the Vesta parallel file system. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 1-16, </pages> <year> 1993. </year>
Reference-contexts: Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements. <p> The tool interface allows applications to take into account the disk/memory locality by executing code on the I/O nodes that store the applications' data. The Vesta file system for the SP2 multicomputer <ref> [25, 26] </ref> provides a record oriented interface optimized for matrix operations. On creating a file, the application specifies the number of physical partitions (normally per-disk sub-files) that the data will be distributed across and the record size. <p> However, we believe that for many applications memory/disk locality can be effectively exploited by file system software, and hence the additional cost of a parallel network is wasted for these applications. The majority of the current large-scale systems (including the Vulcan, SP2, iPSC/2, iPSC/860, Paragon, CM-2 and CM-5 <ref> [26, 39, 83] </ref>) have separate I/O and compute nodes. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned. <p> On read requests, the file system permutes the data between the two, collecting the corresponding data from the different disks. In terms of distribution, the most flexible among existing parallel file systems is the Vesta file system for the SP2 multicomputer <ref> [25, 26] </ref>. It has separate concepts for physical partitions (effectively the number of disks used) and logical partitions. Logical partitions allow applications to specify a mapping between file data and an application's view of the data. <p> There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta <ref> [25, 26] </ref> for the SP2, the OSF/1 file system [136], the file system for the nCUBE [31], and the Rama file system [90]. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. <p> As described in Chapter 2, the ELFS file system [49, 50] provides for similar functionality. Special ASO classes can also be defined to provide for functionality like that of the Vesta file system <ref> [25, 26] </ref>, where an application can request a particular view of a file. The view determines a mapping between the bytes in the file and the order that the application will see these bytes. <p> CHAPTER 6. APPLICATION-LEVEL LIBRARY IMPLEMENTATION: THE ASF 62 mapping ASO classes: These classes allow applications to specify a mapping function between the bytes in the file and the order in which the application will obtain these bytes. Such mapping functions can simplify applications, like blocked matrix multiply <ref> [26] </ref>. In some cases, it may be important that data be aligned on particular memory boundaries. For example, some languages always align records on word boundaries. <p> CONCLUDING REMARKS 98 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta <ref> [25, 26] </ref> for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system [35], and the Rama file system [90].
Reference: [27] <author> Thomas H. Cormen and David Kotz. </author> <title> Integrating theory and practice in parallel file systems. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 64-74, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution> <note> Revised from Dartmouth PCS-TR93-188. </note>
Reference-contexts: The data distribution policy should match the application access pattern to avoid concurrent requests for different data being directed to a single disk <ref> [27, 56, 135] </ref>. Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O [49, 69, 71, 103, 121] and to avoid unnecessary disk requests. <p> The disk striping techniques used by the controllers of such disk arrays, where data is distributed in a simple round-robin fashion across the array, are very restrictive. For parallel applications it is crucial that the file data distribution across the disks matches the access patterns of the application <ref> [27, 56, 135] </ref>. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned.
Reference: [28] <author> Thomas W. Crockett. </author> <title> File concepts for parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference-contexts: The local access patterns are those where each process independently accesses portions of the file sequentially. The global access patterns are those where the access pattern described is the merged access pattern of the different processes in the application. Crockett <ref> [28] </ref> describes six different access patterns: sequential, sequential partitioned, sequential interleaved, sequential self-scheduled, global random and partitioned random. Reddy and Banerjee [108] studied the implicit and explicit file I/O activity of five applications from the Perfect benchmark suite running on a simulator. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern [9, 26, 27, 28, 56, 135]. Crockett <ref> [28] </ref> describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned.
Reference: [29] <author> Wiebren de Jonge, M. Frans Kaashoek, and Wilson C. Hsieh. </author> <title> The logical disk: A new approach to improving file systems. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 15-28, </pages> <year> 1993. </year>
Reference: [30] <author> R. Dean and F. </author> <title> Armand. Data movement in kernelized systems. </title> <booktitle> In Usenix Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: For example, the OSF/1 implementation of Unix I/O uses an emulation library in the application address space that maps (Section 2.2.2) large file regions into the application address space and copies data to or from the mapped regions, hence reducing the number of system calls <ref> [30] </ref>. Accessing uncached file data Unix I/O also entails large overhead when data is being transferred to or from disks. The copying between buffers (described in the previous section) increases contention on the memory. <p> One open question in porting HFS to a multicomputer would be whether mapped-file I/O should be used. The fact that the OSF/1 file system for the Paragon multicomputer uses mapped-file I/O <ref> [30] </ref> suggests that it would not be a problem.
Reference: [31] <author> Erik P. DeBenedictis and Juan Miguel del Rosario. </author> <title> Modular scalable I/O. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):122-128, January and February 1993. </note>
Reference-contexts: More importantly, it allows common matrix access patterns to be specified directly. The Vesta file system also provides asynchronous requests and RECKLESS requests that have relaxed the Unix I/O atomicity constraints. The nCUBE file system <ref> [31, 32] </ref> provides a mapping interface that is similar (but more constrained) than that provided by Vesta. According to Kotz [67], most file systems of commercial multiprocessors provide the standard Unix I/O interface, typically with extensions for asynchronous I/O. The KSR provides for mapped-file I/O [11]. <p> be read into a remote memory [78], and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks <ref> [11, 31, 36, 66, 105] </ref>, possibly with the stripe size and number of disks to stripe across determined by the application. <p> The file system for the nCUBE distributes file data across the disks in a simple striped fashion with the stripe size determined by the system <ref> [31, 32] </ref>. However, when an application opens a file it specifies a mapping between the data in the files and on the processors. On read requests, the file system permutes the data between the two, collecting the corresponding data from the different disks. <p> There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the file system for the nCUBE <ref> [31] </ref>, and the Rama file system [90]. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. For example, the Vesta file system hashes file names across all I/O nodes in order to balance the load on directory operations. <p> 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the nCUBE file system <ref> [31] </ref>, the Bridge file system [35], and the Rama file system [90]. Our current implementation supports or can easily be extended to support all of the policies used by these file systems to distribute file data across the disks.
Reference: [32] <author> Erik P. DeBenedictis and Stephen C. Johnson. </author> <title> Extending Unix for scalable computing. </title> <journal> IEEE Computer, </journal> <volume> 26(11) </volume> <pages> 43-54, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: More importantly, it allows common matrix access patterns to be specified directly. The Vesta file system also provides asynchronous requests and RECKLESS requests that have relaxed the Unix I/O atomicity constraints. The nCUBE file system <ref> [31, 32] </ref> provides a mapping interface that is similar (but more constrained) than that provided by Vesta. According to Kotz [67], most file systems of commercial multiprocessors provide the standard Unix I/O interface, typically with extensions for asynchronous I/O. The KSR provides for mapped-file I/O [11]. <p> The file system for the nCUBE distributes file data across the disks in a simple striped fashion with the stripe size determined by the system <ref> [31, 32] </ref>. However, when an application opens a file it specifies a mapping between the data in the files and on the processors. On read requests, the file system permutes the data between the two, collecting the corresponding data from the different disks.
Reference: [33] <author> J. M. del Rosario and A. Choudhary. </author> <title> High performance I/O for massively parallel computers: Problems and prospects. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Third, an increasing number of I/O bound applications are being developed for parallel supercomputers. For example, many of the Grand Challenges problems have extremely high I/O requirements <ref> [33] </ref>. As another example, parallel supercomputers are starting to be used extensively for business applications such as data bases. <p> Subsequent sections describe file system policies for optimizing performance. We conclude with a brief review of the salient characteristics of other file system architectures. 2.5.1 Workload studies and models Most current parallel machines have poor support for high-performance I/O <ref> [33, 56] </ref>. As a result, the parallel processing community has mainly studied applications that have small I/O requirements. In this section, we provide motivation for our research by describing studies that suggest that many important applications have high I/O requirements. <p> The example provided is that of an airplane modeled with 150,000 degrees of freedom, requiring 90 Gigabytes of storage to hold the matrix that is accessed in full on every iteration. del Rosario and Choudhary give a summary of the overall I/O requirements for a number of Grand Challenges applications <ref> [33] </ref>. These problems have similar I/O requirements to the applications described above. A particular example of a Grand Challenges problem is the genome-sequence pattern matching problem [3]. <p> We plan to extend on our current research by studying the I/O demands of parallel applications and the policies that can best handle these demands. Most current file systems constrain how applications request I/O if they are to obtain good performance <ref> [33] </ref>. Hence, it is difficult to use results obtained from these systems to characterize the real requirements of the applications.
Reference: [34] <author> Juan Miguel del Rosario, Rajesh Bordawekar, and Alok Choudhary. </author> <title> Improved parallel I/O via a two-phase run-time access strategy. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 56-70, </pages> <year> 1993. </year> <note> Also published in Computer Architecture News 21(5), </note> <month> December </month> <year> 1993, </year> <pages> pages 31-38. BIBLIOGRAPHY 109 </pages>
Reference-contexts: Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements.
Reference: [35] <author> Peter Dibble, Michael Scott, and Carla Ellis. </author> <title> Bridge: A high-performance file system for parallel processors. </title> <booktitle> In Proceedings of the Eighth International Conference on Distributed Computer Systems, </booktitle> <pages> pages 154-161, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: In the Bridge (also called PIFS) file system <ref> [35, 36] </ref>, Dibble defines three interfaces: 1) a standard interface that is similar to Unix I/O except that it is record oriented, 2) a parallel open interface, where a process issues read and write requests that, in a SIMD fashion, transfer data concurrently to a pre-specified set of workers, and 3) <p> systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system <ref> [35] </ref>, and the Rama file system [90]. Our current implementation supports or can easily be extended to support all of the policies used by these file systems to distribute file data across the disks.
Reference: [36] <author> Peter C. Dibble. </author> <title> A Parallel Interleaved File System. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: In the Bridge (also called PIFS) file system <ref> [35, 36] </ref>, Dibble defines three interfaces: 1) a standard interface that is similar to Unix I/O except that it is record oriented, 2) a parallel open interface, where a process issues read and write requests that, in a SIMD fashion, transfer data concurrently to a pre-specified set of workers, and 3) <p> Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix. <p> be read into a remote memory [78], and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks <ref> [11, 31, 36, 66, 105] </ref>, possibly with the stripe size and number of disks to stripe across determined by the application.
Reference: [37] <author> Fred Douglis, John K. Ousterhout, M. Frans Kaashoek, and Andrew S. Tanenbaum. </author> <title> A comparison of two distributed systems: Amoeba and Sprite. </title> <journal> Computing Systems, </journal> <volume> 4(4) </volume> <pages> 353-384, </pages> <year> 1991. </year>
Reference: [38] <author> P. Druschel and L.L. Peterson. Fbufs: </author> <title> A high-bandwidth cross-domain transfer facility. </title> <booktitle> In Proc. 14th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 189-202, </pages> <year> 1993. </year>
Reference-contexts: Also, we could easily develop new stream modules that exploit specialized facilities for transferring data between address spaces, such as Govidan and Anderson's memory-mapped stream facility [48] and Druschel and Peterson's Fbufs facility <ref> [38] </ref>.
Reference: [39] <author> T. H. Dunigan. </author> <title> Performance of the Intel iPSC/860 and Ncube 6400 hypercubes. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1285-1302, </pages> <year> 1991. </year>
Reference-contexts: However, we believe that for many applications memory/disk locality can be effectively exploited by file system software, and hence the additional cost of a parallel network is wasted for these applications. The majority of the current large-scale systems (including the Vulcan, SP2, iPSC/2, iPSC/860, Paragon, CM-2 and CM-5 <ref> [26, 39, 83] </ref>) have separate I/O and compute nodes.
Reference: [40] <author> Samuel A. Fineberg. </author> <title> Implementing the NHT-1 application I/O benchmark. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <pages> pages 37-55, </pages> <year> 1993. </year>
Reference-contexts: NHT-1 is an I/O benchmark developed at NASA to evaluate the I/O systems of parallel machines <ref> [17, 40] </ref>.
Reference: [41] <author> High Performance Fortran Forum. </author> <title> DRAFT High Performance Fortran language specification. </title> <type> Technical report, </type> <institution> Rice University, </institution> <year> 1992. </year>
Reference-contexts: We believe that this extra level of control is important, given the current lack of understanding of the requirements of parallel applications. 1 Even the most recent draft language specification for High Performance Fortran (HPF) does not contain a set of parallel I/O extensions to Fortran <ref> [41] </ref>. CHAPTER 1. INTRODUCTION 3 1.3 The HFS file system HFS uses an object-oriented building-block approach, where files are implemented by combining together a (possibly large) number of simple building blocks called storage objects. <p> If all system-specific information can be specified in the first phase, then (through the use of virtual functions) the majority of the application code can be written to be independent of the system. This is similar in philosophy to the pragmas used by HPF <ref> [41] </ref> that specify data distribution across memory banks, where only the pragmas and not the code that accesses memory has to be changed to optimize performance when a HPF program is ported to a new platform. 2.3 The Unix Fast File System Most Unix file systems, including parallel ones, are based
Reference: [42] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of a parallel input/output system for the Intel iPSC/2 hypercube. </title> <booktitle> In Proceedings of the 1991 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 178-187, </pages> <year> 1991. </year>
Reference: [43] <author> James C. French, Terrence W. Pratt, and Mriganka Das. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):115-121, January and February 1993. </note>
Reference: [44] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-driven parallel I/O. </title> <booktitle> In Proceedings Supercomputing, </booktitle> <pages> pages 388-395. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix. <p> CHAPTER 2. BACKGROUND AND MOTIVATION 17 was essentially identical for each cycle. Another study of I/O intensive supercomputing applications by Pasquale and Polyzos [100] supports Miller and Katz's results. Galbreath, Gropp and Levine <ref> [44] </ref> investigated the I/O pattern of various applications running at Argonne National Laboratory. Their characterization of the application demands is similar to Miller and Katz's. Kotz and Ellis [70, 71] used a synthetic workload to analyze different cache management and prefetching policies.
Reference: [45] <author> Benjamin Gamsa, Orran Krieger, and Michael Stumm. </author> <title> Optimizing IPC performance for shared-memory multiprocessors. </title> <type> Technical Report 294, </type> <institution> CSRI, University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: implemented, a device driver architecture for the HURRICANE operating system was designed, an NFS service was implemented, application-level I/O libraries were developed, a naming facility that directs requests to system services was developed, a scalable reader-writer locking algorithm was developed [72], and the basic IPC mechanism of HURRICANE was re-designed <ref> [45] </ref>. CHAPTER 1. INTRODUCTION 4 mapped files, or an operating system designed for scalability. Our file system architecture was designed as a part of a new experimental operating system (HURRICANE) on a new experimental hardware platform (HECTOR). <p> Third, HURRICANE supports a new interprocess communication facility, called Protected Procedure Calls (PPC) that 1) allows fast, local, cross-address space invocation of server code and data, and 2) results in new workers being created in the server address space as they are required to handle requests <ref> [45] </ref>. Since PPC requests are fast 5 , the HURRICANE file system can be partitioned into separate address spaces without a major impact on performance. The PPC facility also affects the file system architecture in that it simplifies deadlock prevention. <p> CHAPTER 4. HFS SERVERS 39 address spaces are PPC operations. Dotted lines are used to indicate remote accesses to shared data structures. Solid lines indicate page faults and page fault completions. All of the explicit cross-address-space communication is performed using HURRICANE PPC operations <ref> [45] </ref>. We chose to use PPC operations because they are very fast (on the order of 30 sec), they have semantics similar to that of procedure calls, and they require no locks in the micro-kernel.
Reference: [46] <author> Joydeep Ghosh, Kelvin D. Goveas, and Jeffrey T. Draper. </author> <title> Performance evaluation of a parallel I/O subsystem for hypercube multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):90-106, January and February 1993. </note>
Reference-contexts: An alternative would be to use a separate network for the disks, which can have the advantage that data can be distributed to the disks without regard for locality to the memory modules <ref> [46] </ref>. However, we believe that for many applications memory/disk locality can be effectively exploited by file system software, and hence the additional cost of a parallel network is wasted for these applications.
Reference: [47] <author> Garth A. Gibson. </author> <title> Designing disk arrays for high data reliability. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 17(1-2):4-27, </volume> <month> January/February </month> <year> 1993. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [48] <author> R. Govidan and D.P. Anderson. </author> <title> Scheduling and IPC mechanisms for continuous media. </title> <booktitle> In Proc. 13th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 68-109, </pages> <year> 1991. </year>
Reference-contexts: Also, we could easily develop new stream modules that exploit specialized facilities for transferring data between address spaces, such as Govidan and Anderson's memory-mapped stream facility <ref> [48] </ref> and Druschel and Peterson's Fbufs facility [38].
Reference: [49] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: Object-oriented extensible file systems. </title> <type> Technical Report TR-91-14, </type> <institution> Univ. of Virginia Computer Science Department, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O <ref> [49, 69, 71, 103, 121] </ref> and to avoid unnecessary disk requests. Second, the file system must manage locality so that a processor's I/O requests are primarily directed to nearby devices. In practice (especially when an application accesses persistent data), locality may be difficult to achieve in an application-independent manner. <p> Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements. <p> Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix. <p> The application-level interface we have developed for HFS is an object-oriented interface similar to that developed by Grimshaw and Loyot <ref> [49, 50] </ref> for the ELFS file system. An object-oriented approach has the advantage that the interface can be easily extended by adding new object classes (through inheritance, re-using the code of other classes with common characteristics). <p> and Satyanarayanan propose having the compiler provide the file system with high level hints that will allow intelligent prefetching [103]. 9 The ELFS file system prefetches file data by taking advantage of application-specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern <ref> [49, 50] </ref>. (An ELFS application specifies its access pattern by its choice of object.) While it is typically not under software control, it is important to keep in mind that most current disks support disk caches into which data is prefetched sequentially according to the position of the data on the <p> As described in Chapter 2, the ELFS file system <ref> [49, 50] </ref> provides for similar functionality. Special ASO classes can also be defined to provide for functionality like that of the Vesta file system [25, 26], where an application can request a particular view of a file.
Reference: [50] <author> Andrew S. Grimshaw and Edmond C. Loyot, Jr. </author> <title> ELFS: Object-oriented extensible file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 177-179, </pages> <year> 1991. </year>
Reference-contexts: The application-level interface we have developed for HFS is an object-oriented interface similar to that developed by Grimshaw and Loyot <ref> [49, 50] </ref> for the ELFS file system. An object-oriented approach has the advantage that the interface can be easily extended by adding new object classes (through inheritance, re-using the code of other classes with common characteristics). <p> and Satyanarayanan propose having the compiler provide the file system with high level hints that will allow intelligent prefetching [103]. 9 The ELFS file system prefetches file data by taking advantage of application-specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern <ref> [49, 50] </ref>. (An ELFS application specifies its access pattern by its choice of object.) While it is typically not under software control, it is important to keep in mind that most current disks support disk caches into which data is prefetched sequentially according to the position of the data on the <p> As described in Chapter 2, the ELFS file system <ref> [49, 50] </ref> provides for similar functionality. Special ASO classes can also be defined to provide for functionality like that of the Vesta file system [25, 26], where an application can request a particular view of a file.
Reference: [51] <author> John H. Hartman and John K. Ousterhout. </author> <title> Zebra: A striped network file system. </title> <booktitle> In Proceedings of the Usenix File Systems Workshop, </booktitle> <pages> pages 71-78, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Finally, the Zebra file system combines ideas from log-structured file system and RAID-5 to maintain the file data for a distributed system in a fault tolerant fashion <ref> [51] </ref>. 11 There has been a great deal of work in recent years to improve the flexibility of file systems in order to make it easier to add new features such as compression and encryption. <p> This file will be striped with distributed parity across the disks connected to the cluster. A similar approach is used by the Zebra file system <ref> [51] </ref>, which combines ideas from log-structured file systems and RAID-5 to maintain file data and meta-data for a distributed system in a fault tolerant fashion. 1 Because the data of small files is stored directly in the member data of a small-data PSO, and the small-data PSO is in turn stored
Reference: [52] <author> John S. Heidemann and Gerald J. Popek. </author> <title> File-system development with stackable layers. </title> <journal> ACM transactions on Computer Systems, </journal> <volume> 12(1) </volume> <pages> 58-89, </pages> <month> Feb. </month> <year> 1994. </year>
Reference-contexts: Another reason why the policies that can be invoked on a FFS file are limited is that they are typically implemented internal to the Unix kernel <ref> [52] </ref>. Since it is difficult to modify the Unix kernel, it is hard to develop new file system policies, especially if the new policies require input from the application (or application-level library). <p> Stackable file systems are another approach for achieving the goal of flexibility, where the file system is composed of layer building blocks <ref> [52] </ref>. The layers can be developed by independent parties, each building on the functionality of the layers below it. <p> that does not have to be implemented at the physical server layer and, for security or performance reasons, should not be implemented in the application-level 7 The UCLA stackable layers interface adopts a very similar solution to the same problem, where their default function is equivalent to our special function <ref> [52] </ref>. Also, special is similar to ioctl provided by Unix, where it is used for exceptional requests, that are unique to particular types of storage objects, and under conditions where the cost to marshal the arguments into the buffer is not important. CHAPTER 3. ARCHITECTURAL OVERVIEW 26 library. <p> Conceptually, HFS has more in common with flexible file systems designed for uniprocessors than it does with other parallel file systems. For example, stackable file systems <ref> [52] </ref> implement a file using layer building blocks in the same way that HFS uses storage objects. However, the goals of stackable file systems are very different from those of HFS, and hence their architectures have little in common with that of HFS.
Reference: [53] <author> John Howard, Michael Kazar, Sherri Menees, David Nichols, Mahadev Satyanarayanan, Robert Sidebotham, and Michael West. </author> <title> Scale and performance in a distributed file system. </title> <journal> ACM Transactions of Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <month> February </month> <year> 1988. </year> <note> BIBLIOGRAPHY 110 </note>
Reference-contexts: In this section we discuss related work in file system architectures and implementations. In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations <ref> [6, 22, 53, 98, 132] </ref>. A good overview of research in this area is given by Levy and Sibershatz [80].
Reference: [54] <institution> Concurrent I/O application examples. Intel Corporation Background Information, </institution> <year> 1989. </year>
Reference-contexts: Also, we describe work by others that attempt to characterize the types of I/O requirements supercomputer applications have. Several examples of scientific applications requiring high performance I/O are described by Intel <ref> [54] </ref>.
Reference: [55] <institution> Paragon XP/S product overview. Intel Corporation, </institution> <year> 1991. </year>
Reference-contexts: Our choice of target machine architecture was motivated largely by the availability of such a machine (HECTOR). However, much of our work is also relevant to multiprocessors with disk arrays, multiprocessors with separate I/O networks, multicomputers (such as the Paragon <ref> [55] </ref> or CM5 [127]), and even vector or SIMD computers with a large number of disks. 2.4.2 The target operating system The HURRICANE File System is being developed as part of the HURRICANE operating system [129, 125, 128].
Reference: [56] <author> David Wayne Jensen. </author> <title> Disk I/O In High-Performance Computing Systems. </title> <type> PhD thesis, </type> <institution> Univ. Illinois, Urbana-Champagne, </institution> <year> 1993. </year>
Reference-contexts: The data distribution policy should match the application access pattern to avoid concurrent requests for different data being directed to a single disk <ref> [27, 56, 135] </ref>. Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O [49, 69, 71, 103, 121] and to avoid unnecessary disk requests. <p> However, the interconnection network will become congested if a large number of disks are concurrently transferring data over the network <ref> [56] </ref>. 4 Hence, it is important that most of a processor's I/O requests be directed to nearby devices. <p> The disk striping techniques used by the controllers of such disk arrays, where data is distributed in a simple round-robin fashion across the array, are very restrictive. For parallel applications it is crucial that the file data distribution across the disks matches the access patterns of the application <ref> [27, 56, 135] </ref>. <p> Subsequent sections describe file system policies for optimizing performance. We conclude with a brief review of the salient characteristics of other file system architectures. 2.5.1 Workload studies and models Most current parallel machines have poor support for high-performance I/O <ref> [33, 56] </ref>. As a result, the parallel processing community has mainly studied applications that have small I/O requirements. In this section, we provide motivation for our research by describing studies that suggest that many important applications have high I/O requirements. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned. <p> Jensen analyzed a series of distribution functions and found that the performance of these functions depends on the total I/O load, the access pattern of the applications, the concurrency in the applications, and the number of files being accessed <ref> [56] </ref>. An important consideration in the results is the effect of the on-disk cache, where mapping functions 9 The authors argue that the application should provide the file system with hints disclosing the application access pattern rather than specific advice on what resources should be reserved for the application.
Reference: [57] <author> M. Jones. </author> <title> Bringing the C libraries with us into a multi-threaded future. </title> <booktitle> In USENIX-Winter 91, </booktitle> <pages> pages 81-91, </pages> <year> 1991. </year>
Reference-contexts: Unix I/O is grossly inadequate for multithreaded programs. An obvious inadequacy is the way in which errors are reported to applications: a single global variable errno is set by the I/O system whenever an I/O error is encountered <ref> [57] </ref>. If multiple concurrent I/O operations incur errors, then having a single errno variable makes it impossible to properly identify the errors. A second inadequacy of Unix I/O for multithreaded applications is the way random-access I/O is supported. <p> An important example of where the comparison to the IRIX system was not fair is the compress and uncompress programs, where the overhead of locking in the installed version of the putc and getc macros is probably quite high <ref> [57] </ref>. We have generally made every effort to ensure that the comparisons to installed facilities are fair, but it is difficult to be certain they are. For example, we do not know what size buffer was used by the installed versions of stdio that we compare against.
Reference: [58] <author> W. Joy, E. Cooper, R. Fabry, S. Leffler, K. McKusick, and D. </author> <title> Mosher. 4.2BSD system manual. </title> <year> 1983. </year>
Reference-contexts: Once a mapping is established, accesses to the memory region behave as if they were accesses to the corresponding file region. The mmap/munmap <ref> [58] </ref> interface is the standard Unix mapped-file interface. Mmap takes as parameters the file number, protection and control flags, the length of the region to be mapped, an offset into a file, and optionally a virtual address indicating where the file should be mapped.
Reference: [59] <author> K. Kennedy. </author> <title> Keynote address. </title> <booktitle> International Conference on Parallel Processing, </booktitle> <year> 1993. </year>
Reference-contexts: Eventually it was recognized that application programmers must modify their programs in order to be able to fully exploit the power of these machines <ref> [59] </ref>. Similarly, all parallel supercomputer vendors have recognized that they must support new programming languages (e.g., HPF) that allow applications to provide extra information to simplify the task of exploiting the processing power of their machines.
Reference: [60] <author> J. Kent Peacock, S. Saxena, D. Thomas, F. Yang, and W. Yu. </author> <title> Experiences from multithreading system V release 4. </title> <booktitle> In SEDMS III. Symposium on Experiences with Distributed and Multiprocessor Systems, </booktitle> <pages> pages 77-91. </pages> <publisher> Usenix Assoc, </publisher> <month> March </month> <year> 1992. </year>
Reference-contexts: The majority of file system work on shared-memory multiprocessors targets small scale systems. In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency <ref> [60, 77, 82, 104] </ref>. The most innovative file system development in recent years is that of log-structured file systems [97, 110]. A log-structured file system treats the disk as a segmented append-only log. <p> Existing operating systems have typically been scaled to accommodate a large number of processors in an ad hoc manner, by repeatedly identifying and then removing the most contended bottlenecks <ref> [4, 5, 15, 18, 60] </ref>. This is done either by splitting existing locks, or by replacing existing data structures with more elaborate, but concurrent ones.
Reference: [61] <author> Yousef A. Khalidi and Michael N. Nelson. </author> <title> Extensible file systems in Spring. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-14, </pages> <year> 1993. </year>
Reference-contexts: Two examples that use an object-oriented approach are the Spring file system <ref> [61] </ref> and the Choices file system [15].
Reference: [62] <author> Michelle Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(11):978-988, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [63] <author> S. Kleiman and D. Williams. </author> <title> SunOS on SPARC. </title> <institution> Sun Technology, </institution> <month> Summer </month> <year> 1988. </year>
Reference-contexts: ASF can support a variety of I/O interfaces, including stdio, emulated Unix I/O, ASI, and C++ streams, in a way that allows applications to freely intermix calls to the different interfaces, resulting in improved code re-usability. ASF has been implemented for HURRICANE [129, 128], SunOS <ref> [63] </ref>, IRIX [118], AIX [91], and HP-UX [133]. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> The HFS application-level library design has advantages that extend beyond HFS and our operating systems. ASF can easily be ported to new platforms; only the set of service-specific ASO classes must be changed. We have ported ASF to SunOS <ref> [63] </ref>, IRIX [118], AIX [91], and HP-UX [133], where it runs independent of any server or special kernel support. Also, ASF can easily be extended to support new types of I/O services that a particular platform might provide. <p> The flexibility of our approach makes ASF readily portable to other platforms. In addition to running on HURRICANE as a part of HFS, ASF runs independent of any server or kernel support on SunOS <ref> [63] </ref>, IRIX [118], AIX [91], and HP-UX [133]. Although each of these systems support some variant of Unix, we have found that large improvements in performance can be obtained by adapting the facility to the particular characteristics of each system. <p> We have implemented many of stream-specific ASO classes, a sequential-locking ASO class, and an ASO class for line buffered streams. Our implementation of the application-level library of HFS, called the Alloc Stream Facility (ASF), has been ported to a number of Unix systems, including SunOS <ref> [63] </ref>, IRIX [118], AIX [91], and HP-UX [133]. In addition to the Alloc Stream Interface (ASI), our implementation exports the Unix I/O and stdio I/O interfaces.
Reference: [64] <author> J. Kohl, C. Staelin, and M. Stonebraker. Highlight: </author> <title> Using a log-structured file system for tertiary storage management. </title> <booktitle> In USENIX Winter Conference, </booktitle> <pages> pages 435-447. </pages> <publisher> USENIX Association, </publisher> <month> Jan </month> <year> 1993. </year>
Reference-contexts: For example, Burrows et al. describe a method for adding automatic compression that benefits from the large writes [13]. The sequential nature and large size of the writes also makes log-structured file systems attractive for automatic migration of data to tertiary storage <ref> [64] </ref>.
Reference: [65] <author> D. Korn and K.-Phong Vo. Sfio: </author> <note> Safe/fast string/file I/O. In USENIX-Summer'91, </note> <year> 1991. </year>
Reference-contexts: For example, the basic I/O operations, fread and fwrite, require that data be copied to or from application-specified buffers. The stdio interface is also inappropriate for multi-threaded applications. Some of the problems with stdio have been addressed by sfio, an application-level library developed by Korn and Vo <ref> [65] </ref>. Sfio is a replacement library for stdio that, in addition to the stdio interface, provides a more consistent, powerful and natural interface than stdio and uses algorithms that are more efficient than those used by typical stdio implementations. The sfio implementation exploits mapped-file I/O whenever possible. <p> For record I/O, Cobol and PL/I read both provide a current record, the location of which is determined by the language implementation [95]. Also, the sfpeek operation provided by the sfio library <ref> [65] </ref> allows applications access to the libraries internal buffers. However, the data of a Cobol or PL/I read or a sfio sfpeek is only available until the next I/O operation, which is not suitable for multi-threaded applications. In contrast, ASI data is available until it is explicitly freed.
Reference: [66] <author> David Kotz. </author> <title> Prefetching and Caching Techniques in File Systems for MIMD Multiprocessors. </title> <type> PhD thesis, </type> <institution> Duke University, </institution> <month> April </month> <year> 1991. </year> <note> Available as technical report CS-1991-016. </note>
Reference-contexts: From the preceding section, it is clear that, while sequential access may be the norm for many parallel applications, the remainder of the conditions are less likely to hold. A good overview of different latency-hiding techniques is given by Kotz <ref> [66] </ref>. Kotz and Ellis have studied automatic cache management and prefetching techniques for shared-memory multiprocessors [69, 70, 71], where automatic means that the system invokes policies dependent on detected application access patterns (rather than in response to application directives). <p> This reference count is used to avoid the block being ejected while it is the most recently used one for some process. CHAPTER 2. BACKGROUND AND MOTIVATION 18 access until it detects random accesses. Kotz and Ellis compare their policies against simple and optimal policies on the RAPID-Transit testbed <ref> [66] </ref>, which is a file system implemented on a shared-memory multiprocessor that uses simulated disks, and found that for the synthetic workload used (described earlier) their relatively simple automatic prefetching techniques perform close to optimal in many cases. <p> be read into a remote memory [78], and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks <ref> [11, 31, 36, 66, 105] </ref>, possibly with the stripe size and number of disks to stripe across determined by the application.
Reference: [67] <author> David Kotz. </author> <title> Multiprocessor file system interfaces. </title> <booktitle> In Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 194-201, </pages> <year> 1993. </year>
Reference-contexts: Requests to the different interfaces provided by ASF can be interleaved with predictable results. 2.2.4 Parallel I/O interfaces As with sequential I/O interfaces, parallel I/O can be supported with either system-call interfaces or application-level interfaces. Parallel system-call interfaces Kotz provides an overview of different parallel I/O interfaces in <ref> [67] </ref>, and proposes a set of extensions to the Unix I/O interface to handle parallel I/O. These extensions include (i) a choice of per-application or per-process file stream pointers, (ii) readp/writep operations that return the file offset used to handle the request, (iii) record oriented I/O, CHAPTER 2. <p> The Vesta file system also provides asynchronous requests and RECKLESS requests that have relaxed the Unix I/O atomicity constraints. The nCUBE file system [31, 32] provides a mapping interface that is similar (but more constrained) than that provided by Vesta. According to Kotz <ref> [67] </ref>, most file systems of commercial multiprocessors provide the standard Unix I/O interface, typically with extensions for asynchronous I/O. The KSR provides for mapped-file I/O [11].
Reference: [68] <author> David Kotz. </author> <title> Disk-directed I/O for MIMD multiprocessors. </title> <type> Technical Report PCS-TR94-226, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements.
Reference: [69] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Prefetching in file systems for MIMD multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(2) </volume> <pages> 218-230, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O <ref> [49, 69, 71, 103, 121] </ref> and to avoid unnecessary disk requests. Second, the file system must manage locality so that a processor's I/O requests are primarily directed to nearby devices. In practice (especially when an application accesses persistent data), locality may be difficult to achieve in an application-independent manner. <p> A good overview of different latency-hiding techniques is given by Kotz [66]. Kotz and Ellis have studied automatic cache management and prefetching techniques for shared-memory multiprocessors <ref> [69, 70, 71] </ref>, where automatic means that the system invokes policies dependent on detected application access patterns (rather than in response to application directives). <p> Other researchers have proposed techniques for the file system to automatically recognize how data is being accessed so that prefetching requests can be issued by the file system on the applications' behalf <ref> [69] </ref>. In general, techniques that prefetch file data can only be effective for applications that access file data in a very regular fashion (e.g., sequentially). <p> ASF allows the threads of an application to use a single prefetching ASO, a prefetching ASO per-thread, or some solution in-between. Hence, it was only necessary to define three ASO classes to handle seven of the eight representative parallel file access patterns described by Kotz and Ellis <ref> [69] </ref>. 6.2.5 Matching application semantics Many files have some underlying semantic structure. For example, the data in a file may be a two or three dimensional matrix of fixed-sized elements.
Reference: [70] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Caching and writeback policies in parallel file systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 17(1-2):140-145, January and February 1993. </note>
Reference-contexts: Another study of I/O intensive supercomputing applications by Pasquale and Polyzos [100] supports Miller and Katz's results. Galbreath, Gropp and Levine [44] investigated the I/O pattern of various applications running at Argonne National Laboratory. Their characterization of the application demands is similar to Miller and Katz's. Kotz and Ellis <ref> [70, 71] </ref> used a synthetic workload to analyze different cache management and prefetching policies. They characterize parallel application access patterns as: local whole file, local fixed-length portions, local random portions, segmented, global whole file, or global fixed portions. <p> A good overview of different latency-hiding techniques is given by Kotz [66]. Kotz and Ellis have studied automatic cache management and prefetching techniques for shared-memory multiprocessors <ref> [69, 70, 71] </ref>, where automatic means that the system invokes policies dependent on detected application access patterns (rather than in response to application directives). <p> A final advantage of ASI over other interfaces is that it is optimized for uni-directional accesses, that is, a read-only or write-only stream. The majority of streams are used uni-directionally <ref> [70, 99] </ref>. Also, even for streams open for read/write access, most applications do not frequently interleave read and write accesses.
Reference: [71] <author> David Kotz and Carla Schlatter Ellis. </author> <title> Practical prefetching techniques for multiprocessor file systems. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 33-51, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O <ref> [49, 69, 71, 103, 121] </ref> and to avoid unnecessary disk requests. Second, the file system must manage locality so that a processor's I/O requests are primarily directed to nearby devices. In practice (especially when an application accesses persistent data), locality may be difficult to achieve in an application-independent manner. <p> Another study of I/O intensive supercomputing applications by Pasquale and Polyzos [100] supports Miller and Katz's results. Galbreath, Gropp and Levine [44] investigated the I/O pattern of various applications running at Argonne National Laboratory. Their characterization of the application demands is similar to Miller and Katz's. Kotz and Ellis <ref> [70, 71] </ref> used a synthetic workload to analyze different cache management and prefetching policies. They characterize parallel application access patterns as: local whole file, local fixed-length portions, local random portions, segmented, global whole file, or global fixed portions. <p> A good overview of different latency-hiding techniques is given by Kotz [66]. Kotz and Ellis have studied automatic cache management and prefetching techniques for shared-memory multiprocessors <ref> [69, 70, 71] </ref>, where automatic means that the system invokes policies dependent on detected application access patterns (rather than in response to application directives).
Reference: [72] <author> Orran Krieger, Michael Stumm, Ron Unrau, and Jonathan Hanna. </author> <title> A fair fast scalable reader-writer lock. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, volume II Software, pages II-201-II-204, </booktitle> <address> Boca Raton, FL, August 1993. </address> <publisher> CRC Press. </publisher>
Reference-contexts: this research, new hardware for connecting disks to HECTOR was implemented, a device driver architecture for the HURRICANE operating system was designed, an NFS service was implemented, application-level I/O libraries were developed, a naming facility that directs requests to system services was developed, a scalable reader-writer locking algorithm was developed <ref> [72] </ref>, and the basic IPC mechanism of HURRICANE was re-designed [45]. CHAPTER 1. INTRODUCTION 4 mapped files, or an operating system designed for scalability. Our file system architecture was designed as a part of a new experimental operating system (HURRICANE) on a new experimental hardware platform (HECTOR).
Reference: [73] <author> Orran Krieger, Michael Stumm, and Ronald Unrau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <type> Technical Report CSRI-275, </type> <institution> Computer Systems Research Institute, University of Toronto, Toronto, Canada, M5S 1A1, </institution> <month> October </month> <year> 1992. </year>
Reference-contexts: Our file system makes extensive use of mapped files to reduce this overhead and in doing so improves I/O performance. Second, a large portion of HFS is implemented within the address space of the applications that use it. This application-level facility, called the Alloc Stream Facility (ASF) <ref> [73, 74, 75] </ref>, has several key advantages: 1. The structure of ASF makes it easy to adapt the facility to any operating system and hardware platform; taking advantage of features specific to each platform to improve performance. 2. <p> In addition to being the application-level library for the HFS file system, it is the library for all other I/O services provided on HURRICANE. Also, ASF has been ported (independent of the rest of the file system) to other operating systems, including SunOS, IRIX, AIX, and HP-UX <ref> [73, 74, 75] </ref>. The common interface provided by all Application-level Storage Objects (ASOs) is called the Alloc Stream Interface (ASI). While having the characteristics described in Section 3.2, ASI differs from the other storage-object interfaces (provided by PSOs and LSOs) in that it is a uniform interface.
Reference: [74] <author> Orran Krieger, Michael Stumm, and Ronald Unrau. </author> <title> Exploiting the advantages of mapped files for stream I/O. </title> <booktitle> In 1992 Winter USENIX, </booktitle> <year> 1992. </year> <note> BIBLIOGRAPHY 111 </note>
Reference-contexts: Our file system makes extensive use of mapped files to reduce this overhead and in doing so improves I/O performance. Second, a large portion of HFS is implemented within the address space of the applications that use it. This application-level facility, called the Alloc Stream Facility (ASF) <ref> [73, 74, 75] </ref>, has several key advantages: 1. The structure of ASF makes it easy to adapt the facility to any operating system and hardware platform; taking advantage of features specific to each platform to improve performance. 2. <p> In addition to being the application-level library for the HFS file system, it is the library for all other I/O services provided on HURRICANE. Also, ASF has been ported (independent of the rest of the file system) to other operating systems, including SunOS, IRIX, AIX, and HP-UX <ref> [73, 74, 75] </ref>. The common interface provided by all Application-level Storage Objects (ASOs) is called the Alloc Stream Interface (ASI). While having the characteristics described in Section 3.2, ASI differs from the other storage-object interfaces (provided by PSOs and LSOs) in that it is a uniform interface.
Reference: [75] <author> Orran Krieger, Michael Stumm, and Ronald Unrau. </author> <title> The Alloc Stream Facility: A redesign of application-level stream I/O. </title> <journal> IEEE Computer, </journal> <volume> 27(3) </volume> <pages> 75-83, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Our file system makes extensive use of mapped files to reduce this overhead and in doing so improves I/O performance. Second, a large portion of HFS is implemented within the address space of the applications that use it. This application-level facility, called the Alloc Stream Facility (ASF) <ref> [73, 74, 75] </ref>, has several key advantages: 1. The structure of ASF makes it easy to adapt the facility to any operating system and hardware platform; taking advantage of features specific to each platform to improve performance. 2. <p> In addition to being the application-level library for the HFS file system, it is the library for all other I/O services provided on HURRICANE. Also, ASF has been ported (independent of the rest of the file system) to other operating systems, including SunOS, IRIX, AIX, and HP-UX <ref> [73, 74, 75] </ref>. The common interface provided by all Application-level Storage Objects (ASOs) is called the Alloc Stream Interface (ASI). While having the characteristics described in Section 3.2, ASI differs from the other storage-object interfaces (provided by PSOs and LSOs) in that it is a uniform interface.
Reference: [76] <author> Butler W. Lampson. </author> <title> Hints for computer system design. </title> <journal> IEEE Software, </journal> <volume> 1(1) </volume> <pages> 11-31, </pages> <month> January </month> <year> 1984. </year>
Reference-contexts: CHAPTER 10. CONCLUDING REMARKS 99 We recognized them by designing and implementing everything incorrectly at least once. This experience mirrors the advice given by Butler Lampson in his often cited paper Hints for Computer System Design <ref> [76] </ref>: Plan to throw one away: you will anyhow [10]. If there is anything new about the function of a system, the first implementation will have to be redone completely to achieve a satisfactory (i.e., acceptably small, fast, and maintainable) result.
Reference: [77] <author> A. Langerman, J. Boykin, S. LoVerso, and S. Mangalat. </author> <title> A highly-parallelized Mach-based vnode filesystem. </title> <booktitle> In Proceedings of the Winter USENIX, </booktitle> <pages> pages 297-312. </pages> <publisher> USENIX Association, </publisher> <year> 1990. </year>
Reference-contexts: The majority of file system work on shared-memory multiprocessors targets small scale systems. In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency <ref> [60, 77, 82, 104] </ref>. The most innovative file system development in recent years is that of log-structured file systems [97, 110]. A log-structured file system treats the disk as a segmented append-only log.
Reference: [78] <author> Richard P. JR. LaRowe and Carla Schlatter Ellis. </author> <title> Page placement policies for NUMA multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 11(2) </volume> <pages> 112-129, </pages> <month> Feb </month> <year> 1991. </year>
Reference-contexts: On the other hand, the Intel Paragon prefetches data into the memory of the compute nodes [111]. With a shared-memory multiprocessor there is a greater range of alternatives. For example, data can be read into a remote memory <ref> [78] </ref>, and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks [11, 31, 36, 66, 105], possibly with
Reference: [79] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: CHAPTER 2. BACKGROUND AND MOTIVATION 14 2.4.1 The target architecture The class of machine architecture we considered is the class of large-scale NUMA multiprocessors with disks distributed across the multiprocessor (Figure 2.4). NUMA multiprocessors achieve their scalability by using segmented architectures <ref> [12, 79, 131] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
Reference: [80] <author> Eliezer Levy and Abraham Silberschatz. </author> <title> Distributed file systems: Concepts and examples. </title> <journal> ACM Computing Surveys, </journal> <volume> 22(4) </volume> <pages> 323-374, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations [6, 22, 53, 98, 132]. A good overview of research in this area is given by Levy and Sibershatz <ref> [80] </ref>. Key issues on these systems are scalability, fault tolerance, preserving traditional uniprocessor semantics in a distributed environment, and achieving good performance through caching, replication, and migration of file data and meta-data. <p> All file systems cache naming information in memory in order to minimize the number of disk I/O operations required for name lookup. On large systems, file systems may replicate naming information to improve performance <ref> [80] </ref>. In HFS, the particular naming LSO class determines the policy for caching and replicating the naming information of each directory. Allowing these policies to be determined on a per-directory basis is important, since different naming objects have different types of access patterns.
Reference: [81] <author> Zheng Lin and Songnian Zhou. </author> <title> Parallelizing I/O intensive applications on a workstation cluster: a case study. </title> <booktitle> In IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <year> 1993. </year>
Reference-contexts: very large and are read sequentially. (While currently the data base can fit in the memory of large supercomputers, this problem is expected to eventually become I/O bound.) An application with I/O characteristics similar to genome sequence pattern matching is a full text retrieval program described by Lin and Zhou <ref> [81] </ref>. NHT-1 is an I/O benchmark developed at NASA to evaluate the I/O systems of parallel machines [17, 40].
Reference: [82] <author> S. LoVerso, N. Paciorek, A. Langerman, and G. Feinberg. </author> <title> The OSF/1 Unix filesystem (UFS). </title> <booktitle> In Proceedings of the Winter 1991 USENIX, </booktitle> <pages> pages 207-218. </pages> <publisher> USENIX Association, </publisher> <month> Jan </month> <year> 1991. </year>
Reference-contexts: The majority of file system work on shared-memory multiprocessors targets small scale systems. In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency <ref> [60, 77, 82, 104] </ref>. The most innovative file system development in recent years is that of log-structured file systems [97, 110]. A log-structured file system treats the disk as a segmented append-only log.
Reference: [83] <author> Susan J. LoVerso, Marshall Isman, Andy Nanopoulos, William Nesheim, Ewan D. Milne, and Richard Wheeler. sfs: </author> <title> A parallel file system for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference, </booktitle> <pages> pages 291-305, </pages> <year> 1993. </year>
Reference-contexts: Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements. <p> However, we believe that for many applications memory/disk locality can be effectively exploited by file system software, and hence the additional cost of a parallel network is wasted for these applications. The majority of the current large-scale systems (including the Vulcan, SP2, iPSC/2, iPSC/860, Paragon, CM-2 and CM-5 <ref> [26, 39, 83] </ref>) have separate I/O and compute nodes. <p> There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs <ref> [83] </ref> for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the file system for the nCUBE [31], and the Rama file system [90]. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. <p> CONCLUDING REMARKS 98 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs <ref> [83] </ref> for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system [35], and the Rama file system [90].
Reference: [84] <author> P. Lu. </author> <title> Parallel search of narrow game trees. </title> <type> Master's thesis, </type> <institution> Department of Computing Science, University of Alberta, Edmonton, </institution> <year> 1993. </year>
Reference-contexts: Cache misses result in random-access I/O. Even with this large cache, the sequential version of the program is I/O bound. A parallel searching version of Chinook further increases the I/O rate <ref> [84] </ref>. (Computing the database is even more I/O intensive than running a match.) 2.5.2 Latency hiding Even if a system has a large disk bandwidth (i.e., if there are a large number of disks) it may be difficult for I/O bound parallel applications to exploit this bandwidth, because of the high
Reference: [85] <author> C. Maeda and B.N. Bershad. </author> <title> Protocol service decomposition for high-performance networking. </title> <booktitle> In Proc. 14th ACM Symp. on Operating System Principles, </booktitle> <pages> pages 244-255, </pages> <year> 1993. </year>
Reference-contexts: We have concentrated in this chapter (and in this dissertation in general) on-disk files. We believe that with the development of new service-specific ASO classes, ASF can deliver improved performance for other I/O services such as pipes or network facilities. Related work by Maeda and Bershad <ref> [85] </ref> demonstrates that substantial performance advantages result from moving critical portions of network protocol processing into the application address space and by modifying the networking interface to avoid unnecessary copying.
Reference: [86] <author> M. Malkawi and J. Patel. </author> <title> Compiler directed memory management policy for numerical programs. </title> <booktitle> In Proc. of the Tenth ACM Symp. on Oper. Syst. Princ, </booktitle> <pages> pages 97-106, </pages> <year> 1985. </year>
Reference-contexts: Some steps have been taken in this direction. For example, Malkawi and Patel describe how memory directives can be inserted by the compiler to manage the memory being used for an application in a multiprogrammed environment <ref> [86] </ref>.
Reference: [87] <author> Marshall K. McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry. </author> <title> A fast file system for Unix. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-193, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: only the pragmas and not the code that accesses memory has to be changed to optimize performance when a HPF program is ported to a new platform. 2.3 The Unix Fast File System Most Unix file systems, including parallel ones, are based on the the Unix Fast File System (FFS) <ref> [87] </ref>. In this section we describe how the FFS (together with the Unix operating system) implements files, and why the approach taken by this file system to implement files is inherently inflexible.
Reference: [88] <author> L. W. McVoy and S. R. Kleiman. </author> <title> Extent-like performance from a UNIX file system. </title> <booktitle> In USENIX-Winter 91, </booktitle> <pages> pages 1-11, </pages> <year> 1991. </year>
Reference-contexts: Before Sun's UFS file system was modified to adopt such a strategy, it took about half of a 12 MIPS CPU to get just half the disk bandwidth of a 1.5 MB/second disk <ref> [88] </ref>. 10 These large write operations are especially important if a RAID-5 disk array is being used, since these disk arrays perform poorly for small write operations. 11 Multiple small files are combined together into a log that is distributed across a large number of disks.
Reference: [89] <author> E.L. Miller and R.H. Katz. </author> <booktitle> Input/output behavior of supercomputing applications. In Proceedings Supercomputing, </booktitle> <pages> pages 567-76. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <month> Nov </month> <year> 1991. </year>
Reference-contexts: Miller and Katz traced and analyzed the I/O behaviour of a number of supercomputer applications on a Cray Y-MP <ref> [89] </ref> (mostly computational fluid dynamics problems).
Reference: [90] <author> Ethan L. Miller and Randy H. Katz. </author> <title> RAMA: A file system for massively-parallel computers. </title> <booktitle> In Proceedings of the Twelfth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 163-168, </pages> <year> 1993. </year>
Reference-contexts: However, in systems where there is a strict division between compute nodes and I/O nodes, the processing power of the I/O nodes is wasted during periods of heavy computation and the compute nodes are idle during high I/O periods <ref> [90] </ref>. Also, we believe that if I/O resources are provided to all processors, the locality between the processor and its local disks can be exploited by the file system. Our choice of target machine architecture was motivated largely by the availability of such a machine (HECTOR). <p> They assume that the file system is being used primarily as a staging area for actively used data stored in tertiary storage. The Rama file system acts as a cache for tertiary storage <ref> [90] </ref>. Disk space is broken into lines of a few Megabytes, where each line has a descriptor identifying the file blocks it holds and the status of these blocks. <p> There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the file system for the nCUBE [31], and the Rama file system <ref> [90] </ref>. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. For example, the Vesta file system hashes file names across all I/O nodes in order to balance the load on directory operations. <p> other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system [35], and the Rama file system <ref> [90] </ref>. Our current implementation supports or can easily be extended to support all of the policies used by these file systems to distribute file data across the disks.
Reference: [91] <author> M. Misra, </author> <title> editor. </title> <institution> IBM RISC System/6000 Technology, volume SA23-2619. IBM, </institution> <year> 1990. </year>
Reference-contexts: ASF can support a variety of I/O interfaces, including stdio, emulated Unix I/O, ASI, and C++ streams, in a way that allows applications to freely intermix calls to the different interfaces, resulting in improved code re-usability. ASF has been implemented for HURRICANE [129, 128], SunOS [63], IRIX [118], AIX <ref> [91] </ref>, and HP-UX [133]. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> Other mapped-file interfaces Other interfaces for mapped-file I/O include the HURRICANE BindRegion/UnbindRegion interface (inspired by V [21]) and the AIX shmget interface <ref> [91] </ref>. The BindRegion interface is similar to the mmap interface, except that the management of bound regions is based on regions of an application's address space rather than individual pages. The AIX shmget interface maps entire files into regions of the application address space. <p> The HFS application-level library design has advantages that extend beyond HFS and our operating systems. ASF can easily be ported to new platforms; only the set of service-specific ASO classes must be changed. We have ported ASF to SunOS [63], IRIX [118], AIX <ref> [91] </ref>, and HP-UX [133], where it runs independent of any server or special kernel support. Also, ASF can easily be extended to support new types of I/O services that a particular platform might provide. <p> The flexibility of our approach makes ASF readily portable to other platforms. In addition to running on HURRICANE as a part of HFS, ASF runs independent of any server or kernel support on SunOS [63], IRIX [118], AIX <ref> [91] </ref>, and HP-UX [133]. Although each of these systems support some variant of Unix, we have found that large improvements in performance can be obtained by adapting the facility to the particular characteristics of each system. <p> We have implemented many of stream-specific ASO classes, a sequential-locking ASO class, and an ASO class for line buffered streams. Our implementation of the application-level library of HFS, called the Alloc Stream Facility (ASF), has been ported to a number of Unix systems, including SunOS [63], IRIX [118], AIX <ref> [91] </ref>, and HP-UX [133]. In addition to the Alloc Stream Interface (ASI), our implementation exports the Unix I/O and stdio I/O interfaces.
Reference: [92] <author> James Morris, Mahadev Satyanarayanan, Michael Conner, John Howard, David Rosenthal, and F. Donelson Smith. Andrew: </author> <title> A distributed personal computing environment. </title> <journal> Communications of the ACM, </journal> <volume> 29(3) </volume> <pages> 184-201, </pages> <month> March </month> <year> 1986. </year>
Reference: [93] <author> Todd C. Mowry. </author> <title> Tolerating Latency Through Software-Controlled Data Prefetching. </title> <type> PhD thesis, </type> <institution> Stanford University, Computer Systems Laboratory, Stanford, </institution> <address> CA 94305, </address> <month> March </month> <year> 1994. </year>
Reference-contexts: Recent research has shown that regular memory access patterns can be detected by compilers, and the compiler can use this information to insert instructions into the code that prefetch data into the processor cache <ref> [93, 94] </ref>. It seems feasible that the compiler could provide similar information to the file system to manage the file cache. Some steps have been taken in this direction. <p> Recent research has shown that regular memory access patterns can be detected by compilers, and this information can be used to prefetch cache lines into the processor cache <ref> [93, 94] </ref>. We believe that compilers could also detect file access patterns, and hence, rather than having the file system attempt to recognize the file access 9 A per-thread ASO can make a single large-grain salloc to its sub-object and service multiple fine-grain sallocs from the buffer returned. <p> If the application accesses data without copying it, than it may be possible to overlap the time to process the data with the time to transfer it to or from the memory (e.g., if prefetching is used for input data <ref> [93, 94] </ref>). It is interesting to note that the SPARCserver 400 series machines implement hardware accelerators to reduce the kernel cost of copying data. This accelerator performs memory to memory copies at the full memory bandwidth, bypassing the cache.
Reference: [94] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta. </author> <title> Design and evaluation of a compiler algorithm for prefetching. </title> <booktitle> In Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 62-73, </pages> <month> October </month> <year> 1992. </year> <note> BIBLIOGRAPHY 112 </note>
Reference-contexts: Recent research has shown that regular memory access patterns can be detected by compilers, and the compiler can use this information to insert instructions into the code that prefetch data into the processor cache <ref> [93, 94] </ref>. It seems feasible that the compiler could provide similar information to the file system to manage the file cache. Some steps have been taken in this direction. <p> Recent research has shown that regular memory access patterns can be detected by compilers, and this information can be used to prefetch cache lines into the processor cache <ref> [93, 94] </ref>. We believe that compilers could also detect file access patterns, and hence, rather than having the file system attempt to recognize the file access 9 A per-thread ASO can make a single large-grain salloc to its sub-object and service multiple fine-grain sallocs from the buffer returned. <p> If the application accesses data without copying it, than it may be possible to overlap the time to process the data with the time to transfer it to or from the memory (e.g., if prefetching is used for input data <ref> [93, 94] </ref>). It is interesting to note that the SPARCserver 400 series machines implement hardware accelerators to reduce the kernel cost of copying data. This accelerator performs memory to memory copies at the full memory bandwidth, bypassing the cache.
Reference: [95] <author> J. </author> <title> Nicholls. </title> <booktitle> The Structure and Design of Programming Languages, chapter 11, </booktitle> <pages> pages 443-446. </pages> <publisher> Addison-Wesley, </publisher> <year> 1975. </year>
Reference-contexts: The idea of having the I/O facility choose the location of the I/O data is in itself not new. For record I/O, Cobol and PL/I read both provide a current record, the location of which is determined by the language implementation <ref> [95] </ref>. Also, the sfpeek operation provided by the sfio library [65] allows applications access to the libraries internal buffers. However, the data of a Cobol or PL/I read or a sfio sfpeek is only available until the next I/O operation, which is not suitable for multi-threaded applications.
Reference: [96] <author> J. Ousterhout. </author> <title> Why aren't operating systems getting faster as fast as hardware? In Proc. </title> <booktitle> of the Summer USENIX Conference, </booktitle> <pages> pages 247-256, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Moreover, in many systems (e.g., multiprocessors), memory bandwidth may be a contended resource. CHAPTER 2. BACKGROUND AND MOTIVATION 7 The system-call overhead of Unix I/O has also been increasing relative to processor speeds <ref> [96] </ref>. This is again partially due to the effects of slower relative memory speeds and due to the increased number of registers some modern processors need to save and restore on context switches.
Reference: [97] <author> J. Ousterhout and F. Douglis. </author> <title> Beating the I/O bottleneck: A case for log-structured file systems. </title> <journal> ACM press, Operating Systems Review, </journal> <volume> 23(1) </volume> <pages> 11-28, </pages> <year> 1989. </year>
Reference-contexts: In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency [60, 77, 82, 104]. The most innovative file system development in recent years is that of log-structured file systems <ref> [97, 110] </ref>. A log-structured file system treats the disk as a segmented append-only log. <p> Also, from a pragmatic perspective, our file system is part of an experimental operating system running on experimental hardware, so fast reboots are important. The log structured file system <ref> [97, 110] </ref> can recover from crashes quickly. It appends all file system state to the end of a log. Whenever data is flushed to disk, the log structured file system first appends file data, then file meta-data, and finally the superblock to the log.
Reference: [98] <author> J.K. Ousterhout, A.R. Cherenson, F. Douglis, M.N. Nelson, and B.B. Welch. </author> <title> The Sprite network operating system. </title> <journal> IEEE Computer, </journal> <volume> 21(2) </volume> <pages> 23-36, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: In this section we discuss related work in file system architectures and implementations. In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations <ref> [6, 22, 53, 98, 132] </ref>. A good overview of research in this area is given by Levy and Sibershatz [80].
Reference: [99] <author> John K. Ousterhout, Herve Da Costa, David Harrison, John A. Kunze, Mike Kupfer, and James G. Thompson. </author> <title> A trace-driven analysis of the UNIX 4.2 BSD file system. </title> <booktitle> In Proceedings 10th ACM Symposium on Operating System Principles, </booktitle> <year> 1985. </year>
Reference-contexts: Once accessed, files will often remain cached in memory, so that the operating system I/O calls no longer involve accesses to I/O devices. Many files in fact are created and deleted without ever being written to secondary storage <ref> [99] </ref>. For this reason, an important component of the cost of Unix I/O is the overhead that stems from copying data from one memory buffer to another and from the cost of making calls to the operating system. <p> A well-known trace study of uniprocessor Unix systems by Ousterhout et al. found that files are typically read and written sequentially in their entirety, files are repeatedly accessed, many files are temporary and, if they are effectively cached, will be deleted before ever being written to disk <ref> [99] </ref>. From the preceding section, it is clear that, while sequential access may be the norm for many parallel applications, the remainder of the conditions are less likely to hold. A good overview of different latency-hiding techniques is given by Kotz [66]. <p> A final advantage of ASI over other interfaces is that it is optimized for uni-directional accesses, that is, a read-only or write-only stream. The majority of streams are used uni-directionally <ref> [70, 99] </ref>. Also, even for streams open for read/write access, most applications do not frequently interleave read and write accesses.
Reference: [100] <author> B. K. Pasquale and G. C. Polyzos. </author> <title> A static analysis of I/O characteristics of scientific applications in a production workload. </title> <booktitle> In Proceedings Supercomputing. </booktitle> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: CHAPTER 2. BACKGROUND AND MOTIVATION 17 was essentially identical for each cycle. Another study of I/O intensive supercomputing applications by Pasquale and Polyzos <ref> [100] </ref> supports Miller and Katz's results. Galbreath, Gropp and Levine [44] investigated the I/O pattern of various applications running at Argonne National Laboratory. Their characterization of the application demands is similar to Miller and Katz's.
Reference: [101] <author> David Patterson, Peter Chen, Garth Gibson, and Randy H. Katz. </author> <title> Introduction to redundant arrays of inexpensive disks (RAID). </title> <booktitle> In Proceedings of IEEE Compcon, </booktitle> <pages> pages 112-117, </pages> <month> Spring </month> <year> 1989. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [102] <author> David Patterson, Garth Gibson, and Randy Katz. </author> <title> A case for redundant arrays of inexpensive disks (RAID). </title> <booktitle> In ACM SIGMOD Conference, </booktitle> <pages> pages 109-116, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Accessing uncached file data Unix I/O also entails large overhead when data is being transferred to or from disks. The copying between buffers (described in the previous section) increases contention on the memory. For example, researchers who have dramatically improved file I/O bandwidth (by introducing disk arrays <ref> [102] </ref>) have found that Unix I/O copying overhead makes it difficult to exploit this bandwidth [23]. <p> An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [103] <author> R. Hugo Patterson, Garth A. Gibson, and M. Satyanarayanan. </author> <title> Informed prefetching: Converting high throughput to low latency. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 41-55, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O <ref> [49, 69, 71, 103, 121] </ref> and to avoid unnecessary disk requests. Second, the file system must manage locality so that a processor's I/O requests are primarily directed to nearby devices. In practice (especially when an application accesses persistent data), locality may be difficult to achieve in an application-independent manner. <p> Hence, we believe the choice of policies used by the file system can be greatly simplified if the application can identify its expected demands in advance. Most researchers studying file system issues for parallel supercomputers have recognized the need for cooperation between the application and the file system <ref> [26, 34, 49, 68, 83, 103] </ref>. Our approach differs from others in that HFS gives the application the ability to explicitly customize the implementation of a file to conform to its specific requirements. <p> Also, many Unix systems (e.g., IRIX version 5.1 and SunOS) provide asynchronous-I/O facilities that allow applications to request that data be read or written without blocking. Asynchronous-I/O facilities based on a read/write interface are, however, difficult to use <ref> [103] </ref>, since on reading data, the application must check to see if the request has completed before it can use the data. Extensions to Unix I/O allow some systems to deliver the full disk bandwidth to the applications. <p> Patterson, Gibson and Satyanarayanan propose having the compiler provide the file system with high level hints that will allow intelligent prefetching <ref> [103] </ref>. 9 The ELFS file system prefetches file data by taking advantage of application-specific knowledge of both the logical file structure (e.g., a two dimensional matrix) and the application access pattern [49, 50]. (An ELFS application specifies its access pattern by its choice of object.) While it is typically not under <p> However, other researchers have shown that prefetching that only takes into account the application and not the state of the system (e.g., the file cache and disk utilization) can result in poor performance <ref> [103] </ref>. CHAPTER 6. APPLICATION-LEVEL LIBRARY IMPLEMENTATION: THE ASF 62 mapping ASO classes: These classes allow applications to specify a mapping function between the bytes in the file and the order in which the application will obtain these bytes. Such mapping functions can simplify applications, like blocked matrix multiply [26].
Reference: [104] <author> J.K. Peacock. </author> <title> File system multithreading in system V release 4 MP. </title> <booktitle> In Proceedings of the Summer 1992 USENIX, </booktitle> <pages> pages 19-29. </pages> <publisher> USENIX Association, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: The majority of file system work on shared-memory multiprocessors targets small scale systems. In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency <ref> [60, 77, 82, 104] </ref>. The most innovative file system development in recent years is that of log-structured file systems [97, 110]. A log-structured file system treats the disk as a segmented append-only log.
Reference: [105] <author> Paul Pierce. </author> <title> A concurrent file system for a highly parallel mass storage system. </title> <booktitle> In Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 155-160, </pages> <year> 1989. </year>
Reference-contexts: According to Kotz [67], most file systems of commercial multiprocessors provide the standard Unix I/O interface, typically with extensions for asynchronous I/O. The KSR provides for mapped-file I/O [11]. Some file systems (e.g., the CFS file system for the iPSC/2 and iPSC/860 <ref> [105] </ref>) implement the Unix I/O interface in a user-level library on top of a lower-level internal system-call interface. The OSF/1 file system for the Paragon implements the Unix I/O interface in an emulation library on top of a mapped-file interface [111]. <p> be read into a remote memory [78], and, rather than ejecting data from the file cache, it can be migrated to remote memories. 2.5.3 Disk block distribution The majority of file systems for parallel machines use policies that distribute data in a simple round-robin fashion across a number of disks <ref> [11, 31, 36, 66, 105] </ref>, possibly with the stripe size and number of disks to stripe across determined by the application. <p> Key issues on these systems are scalability, fault tolerance, preserving traditional uniprocessor semantics in a distributed environment, and achieving good performance through caching, replication, and migration of file data and meta-data. There have been a number of file systems designed for large-scale multicomputers, including CFS <ref> [105] </ref> for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the file system for the nCUBE [31], and the Rama file system [90]. <p> CONCLUDING REMARKS 98 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS <ref> [105] </ref> for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system [136], the nCUBE file system [31], the Bridge file system [35], and the Rama file system [90].
Reference: [106] <author> P.J. Plauger. </author> <title> The Standard C Library. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey 07632, </address> <year> 1992. </year>
Reference-contexts: The standard I/O library for the C programming language, stdio, is an example of a well known application-level I/O facility that is available on numerous operating systems running on almost all current hardware bases <ref> [106] </ref>. The stdio interface (Fig. 2.2) provides functions that correspond to the Unix I/O interface (fopen, fread, fwrite, fseek, fclose), functions for character-based I/O (getc, putc), and functions for formatted I/O (fprintf, fscanf).
Reference: [107] <author> A. Reddy and P. Banerjee. </author> <title> Evaluation of multiple-disk I/O systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38 </volume> <pages> 1680-1690, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: An alternative to distributing the disks across the multiprocessor is to put all disks in a single large disk array <ref> [7, 19, 20, 23, 47, 62, 101, 102, 107] </ref>, where the distribution of data to the disks is handled by a hardware controller associated 3 In the context of HFS, when we refer to the file system software we are referring to all components of the file system including the servers,
Reference: [108] <author> A. L. Narasimha Reddy and Prithviraj Banerjee. </author> <title> A study of I/O behavior of Perfect benchmarks on a multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 312-321, </pages> <year> 1990. </year>
Reference-contexts: The global access patterns are those where the access pattern described is the merged access pattern of the different processes in the application. Crockett [28] describes six different access patterns: sequential, sequential partitioned, sequential interleaved, sequential self-scheduled, global random and partitioned random. Reddy and Banerjee <ref> [108] </ref> studied the implicit and explicit file I/O activity of five applications from the Perfect benchmark suite running on a simulator. These programs were originally written: 1) without trying to avoid false page-level sharing, 2) to run on machines with small amounts of memory, and 3) to do little I/O.
Reference: [109] <author> Mendel Rosenblum and John Ousterhout. </author> <title> The LFS storage manager. </title> <booktitle> In Summer 1990 USENIX Conference, </booktitle> <pages> pages 315-324, </pages> <address> Anaheim, CA, </address> <month> June </month> <year> 1990. </year> <institution> USENIX Association. </institution>
Reference: [110] <author> Mendel Rosenblum and John K. Ousterhout. </author> <title> The design and implementation of a log-structured file system. </title> <booktitle> In Proceedings of 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 1-15. </pages> <institution> Association for Computing Machinery SIGOPS, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: In particular, there have been a number of projects that introduced fine grain locks into existing file system code to allow for greater CHAPTER 2. BACKGROUND AND MOTIVATION 20 concurrency [60, 77, 82, 104]. The most innovative file system development in recent years is that of log-structured file systems <ref> [97, 110] </ref>. A log-structured file system treats the disk as a segmented append-only log. <p> Also, from a pragmatic perspective, our file system is part of an experimental operating system running on experimental hardware, so fast reboots are important. The log structured file system <ref> [97, 110] </ref> can recover from crashes quickly. It appends all file system state to the end of a log. Whenever data is flushed to disk, the log structured file system first appends file data, then file meta-data, and finally the superblock to the log.
Reference: [111] <author> P. J. Roy. </author> <title> UNIX file access and caching in a multicomputer environment. </title> <booktitle> In USENIX Mach III symposium proceedings, </booktitle> <pages> pages 21-37, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: The OSF/1 file system for the Paragon implements the Unix I/O interface in an emulation library on top of a mapped-file interface <ref> [111] </ref>. Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries [9, 25, 36, 44, 49, 119]. <p> It is also important to consider where in memory data should be cached. The Intel CFS prefetches data sequentially into memory on the I/O nodes. On the other hand, the Intel Paragon prefetches data into the memory of the compute nodes <ref> [111] </ref>. With a shared-memory multiprocessor there is a greater range of alternatives.
Reference: [112] <author> Kenneth Salem and Hector Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In IEEE 1986 Conference on Data Engineering, </booktitle> <pages> pages 336-342, </pages> <year> 1986. </year>
Reference: [113] <author> M. Satyanarayanan. </author> <title> The influence of scale on distributed file system design. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 18(1) </volume> <pages> 1-8, </pages> <month> Jan </month> <year> 1992. </year> <note> BIBLIOGRAPHY 113 </note>
Reference: [114] <author> J. Schaeffer, J. Culberson, N. Treloar, B. Knight, P. Lu, and D. Szafron. </author> <title> A world championship caliber checkers program. </title> <journal> Artificial Intelligence, </journal> <volume> 53(2-3):273-289, </volume> <year> 1992. </year>
Reference-contexts: They found that there was substantial variance in the size of the explicit I/O requests of the different applications, and that the explicit I/O requests were entirely sequential. The success of the world championship calibre Chinook checkers program is largely due to its endgame databases <ref> [114] </ref>. The current databases represent over 150 billion positions, requiring 30 Gigabytes of storage when uncompressed. Using application-specific techniques, the databases are compressed into 5.22 Gigabytes of disk storage. <p> ASO classes of this type might use the Lempel-Ziv compression algorithm, designed for whole file access [134], the algorithms described by Burrows et al, that allow random file access [13], or application-specific algorithms, such as that used to compress the data base for the Chinook checkers program <ref> [114] </ref>. encryption/decryption ASO classes: These classes provide for additional security by encrypting and decrypting files as they are accessed.
Reference: [115] <author> David S. Scott. </author> <title> Parallel I/O and solving out of core systems of linear equations. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 123-130, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: Note, while application-level facilities could accept such information to optimize I/O performance, most current application-level facilities are written to be independent of the underlying system. For example, Scott describes how it is necessary to bypass the Fortran run time library to get good I/O performance on an IPSC/860 <ref> [115] </ref>. CHAPTER 2. BACKGROUND AND MOTIVATION 12 management) in the application-level facility, simplifying the lower levels of the file system and avoiding contradictory policies (e.g., different prefetching policies) being concurrently invoked at different levels. However, such an approach has disadvantages in a multi-programmed environment. <p> Many of the applications require that the persistent data be in tertiary storage; the file system is used as a staging area for data actively being accessed. Scott describes one of these applications, out of memory solving of large systems of linear equations, in more detail <ref> [115] </ref>.
Reference: [116] <author> M. Seltzer, K. Bostic, M. K. McKusick, and C. Staelin. </author> <title> An implementation of a Log-structured file system for UNIX. </title> <booktitle> In USENIX Winter Conference. USENIX Association, </booktitle> <month> Jan </month> <year> 1993. </year>
Reference: [117] <author> Bill Shannon. </author> <title> Implementation information for Unix I/O on SunOS. </title> <type> personal communication, </type> <year> 1992. </year>
Reference-contexts: If a page in this subset is accessed, then the kernel can service the read request quickly; otherwise, the kernel incurs the cost of a page fault to service the request <ref> [117] </ref>. We have compared the cost of a page read to the cost of a page fault (i.e., touching a mapped page) because APPENDIX A.
Reference: [118] <institution> Silicon Graphics, Inc., Mountain View, California. </institution> <note> IRIX Programmer's Reference Manual. </note>
Reference-contexts: ASF can support a variety of I/O interfaces, including stdio, emulated Unix I/O, ASI, and C++ streams, in a way that allows applications to freely intermix calls to the different interfaces, resulting in improved code re-usability. ASF has been implemented for HURRICANE [129, 128], SunOS [63], IRIX <ref> [118] </ref>, AIX [91], and HP-UX [133]. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> The HFS application-level library design has advantages that extend beyond HFS and our operating systems. ASF can easily be ported to new platforms; only the set of service-specific ASO classes must be changed. We have ported ASF to SunOS [63], IRIX <ref> [118] </ref>, AIX [91], and HP-UX [133], where it runs independent of any server or special kernel support. Also, ASF can easily be extended to support new types of I/O services that a particular platform might provide. <p> The flexibility of our approach makes ASF readily portable to other platforms. In addition to running on HURRICANE as a part of HFS, ASF runs independent of any server or kernel support on SunOS [63], IRIX <ref> [118] </ref>, AIX [91], and HP-UX [133]. Although each of these systems support some variant of Unix, we have found that large improvements in performance can be obtained by adapting the facility to the particular characteristics of each system. <p> We have implemented many of stream-specific ASO classes, a sequential-locking ASO class, and an ASO class for line buffered streams. Our implementation of the application-level library of HFS, called the Alloc Stream Facility (ASF), has been ported to a number of Unix systems, including SunOS [63], IRIX <ref> [118] </ref>, AIX [91], and HP-UX [133]. In addition to the Alloc Stream Interface (ASI), our implementation exports the Unix I/O and stdio I/O interfaces.
Reference: [119] <author> Tarvinder Pal Singh and Alok Choudhary. ADOPT: </author> <title> A dynamic scheme for optimal prefetching in parallel file systems. </title> <type> Technical report, </type> <institution> NPAC, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: Both mapped-file I/O and Unix I/O interfaces are exported to the application on the Paragon. Parallel application-level I/O interfaces Recently, there have been several proposals for developing application-level parallel I/O libraries <ref> [9, 25, 36, 44, 49, 119] </ref>. By providing an interface that matches common application requirements, these libraries can simplify the job of the application programmer. For example, they can provide operations specific to accessing a 2-D matrix.
Reference: [120] <author> John A. Solworth and Cyril U. Orji. </author> <title> Distorted mirrors. </title> <journal> Journal of Distributed and Parallel Databases, </journal> <volume> 1(1) </volume> <pages> 81-102, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: If the load on disks varies, then it may be a good idea to direct write operations to the least loaded disks, especially in a multiprogrammed environment. Replicating a file can be used both to get better performance <ref> [8, 120] </ref> and to manage locality (since the more local copy of a disk block can be used to satisfy a read request).
Reference: [121] <author> I. Song and Y. Cho. </author> <title> Page prefetching based on fault history. </title> <booktitle> In USENIX Mach III symposium proceedings, </booktitle> <pages> pages 203-213, </pages> <month> April </month> <year> 1993. </year>
Reference-contexts: Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O <ref> [49, 69, 71, 103, 121] </ref> and to avoid unnecessary disk requests. Second, the file system must manage locality so that a processor's I/O requests are primarily directed to nearby devices. In practice (especially when an application accesses persistent data), locality may be difficult to achieve in an application-independent manner. <p> An important observation in their work is that, even given optimistic assumptions about the workload, automatic prefetching can in some cases result in substantially worse performance than if no prefetching is done. An automatic policy for page prefetching due to implicit I/O was proposed by Song and Cho <ref> [121] </ref>. This approach uses a history of previous page faults to predict subsequent page faults when an application repeatedly accesses pages in the same order.
Reference: [122] <author> C. Staelin and H. Garcia-Molina. </author> <title> Smart filesystems. </title> <booktitle> In Winter USENIX, </booktitle> <pages> pages 45-51, </pages> <year> 1991. </year>
Reference-contexts: Two examples that use an object-oriented approach are the Spring file system [61] and the Choices file system [15]. The iPcress file system by Staelin and Garcia-Molina <ref> [122] </ref> also uses (in a more limited fashion) an object-oriented approach, where each file is a file object that maintains statistics on the usage of the file to determine how data should be cached and how the file should be stored on disk.
Reference: [123] <author> W. Richard Stevens. </author> <title> Advanced Programming in the UNIX Environment. Professional Computing Series. </title> <publisher> Addison Wesley, </publisher> <address> One Jacob Way, Reading, Massachusetts 01867, </address> <year> 1992. </year>
Reference-contexts: As is the case with most application-level I/O facilities, the interface and implementation of stdio have remained largely unchanged since the mid-70s <ref> [123] </ref>. For this reason, the stdio interface has many of the same problems as the Unix I/O interface. For example, the basic I/O operations, fread and fwrite, require that data be copied to or from application-specified buffers. The stdio interface is also inappropriate for multi-threaded applications.
Reference: [124] <author> Bjarne Stroustrup. </author> <title> The C++ Programming Language. </title> <publisher> Addison-Wesley, </publisher> <address> second edition edition, </address> <year> 1991. </year>
Reference-contexts: In this section we informally introduce some of the concepts and terminology associated with object-oriented programming. We use C++ terminology throughout this dissertation <ref> [124] </ref>. An object encapsulates both data and functions that operate on that data. The data and functions are referred to as member data and member functions. Public members define the object's external interface (both functions and data).
Reference: [125] <author> Michael Stumm, Ron Unrau, and Orran Krieger. </author> <title> Designing a scalable operating system for shared memory multiprocessors. </title> <booktitle> In USENIX Workshop on Micro-kernels and Other Kernel Architectures, </booktitle> <pages> pages 285-303, </pages> <address> Seattle, Wa., </address> <month> April </month> <year> 1992. </year>
Reference-contexts: to multiprocessors with disk arrays, multiprocessors with separate I/O networks, multicomputers (such as the Paragon [55] or CM5 [127]), and even vector or SIMD computers with a large number of disks. 2.4.2 The target operating system The HURRICANE File System is being developed as part of the HURRICANE operating system <ref> [129, 125, 128] </ref>. There are a number of characteristics of HURRICANE that have had an impact on the design of the file system. First, HURRICANE is a micro-kernel operating system, where most of the operating system services are provided by user-level servers.
Reference: [126] <author> Andrew Tanenbaum, Robbert van Renesse, Hans van Staveren, Gregory Sharp, Sape Mullender, Jack Jansen, and Guido van Rossum. </author> <title> Experiences with the Amoeba Distributed Operating System. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference: [127] <institution> The Connection Machine CM-5 Technical Summary. Thinking Machines Corporation, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: However, these architectures still use simple disk striping for distributing data across the disks <ref> [16, 127] </ref>.) We assume in our file system that the disks, processors, and memories are all connected using the same network. <p> Our choice of target machine architecture was motivated largely by the availability of such a machine (HECTOR). However, much of our work is also relevant to multiprocessors with disk arrays, multiprocessors with separate I/O networks, multicomputers (such as the Paragon [55] or CM5 <ref> [127] </ref>), and even vector or SIMD computers with a large number of disks. 2.4.2 The target operating system The HURRICANE File System is being developed as part of the HURRICANE operating system [129, 125, 128].
Reference: [128] <author> R. Unrau, O. Krieger, B. Gamsa, and M. Stumm. </author> <title> Experiences with locking in a numa multiprocessor operating system kernel. </title> <booktitle> In OSDI Symposium, </booktitle> <month> Nov </month> <year> 1994. </year> <note> accepted for publication. </note>
Reference-contexts: ASF can support a variety of I/O interfaces, including stdio, emulated Unix I/O, ASI, and C++ streams, in a way that allows applications to freely intermix calls to the different interfaces, resulting in improved code re-usability. ASF has been implemented for HURRICANE <ref> [129, 128] </ref>, SunOS [63], IRIX [118], AIX [91], and HP-UX [133]. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> to multiprocessors with disk arrays, multiprocessors with separate I/O networks, multicomputers (such as the Paragon [55] or CM5 [127]), and even vector or SIMD computers with a large number of disks. 2.4.2 The target operating system The HURRICANE File System is being developed as part of the HURRICANE operating system <ref> [129, 125, 128] </ref>. There are a number of characteristics of HURRICANE that have had an impact on the design of the file system. First, HURRICANE is a micro-kernel operating system, where most of the operating system services are provided by user-level servers.
Reference: [129] <author> R. Unrau, M. Stumm, O. Krieger, and B. Gamsa. </author> <title> Hierarchical clustering: A structure for scalable multiprocessor operating system design. </title> <journal> Journal of Supercomputing. </journal> <note> To appear. Also available as technical report CSRI-268 from ftp.csri.toronto.edu. </note>
Reference-contexts: ASF can support a variety of I/O interfaces, including stdio, emulated Unix I/O, ASI, and C++ streams, in a way that allows applications to freely intermix calls to the different interfaces, resulting in improved code re-usability. ASF has been implemented for HURRICANE <ref> [129, 128] </ref>, SunOS [63], IRIX [118], AIX [91], and HP-UX [133]. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> to multiprocessors with disk arrays, multiprocessors with separate I/O networks, multicomputers (such as the Paragon [55] or CM5 [127]), and even vector or SIMD computers with a large number of disks. 2.4.2 The target operating system The HURRICANE File System is being developed as part of the HURRICANE operating system <ref> [129, 125, 128] </ref>. There are a number of characteristics of HURRICANE that have had an impact on the design of the file system. First, HURRICANE is a micro-kernel operating system, where most of the operating system services are provided by user-level servers. <p> CHAPTER 2. BACKGROUND AND MOTIVATION 16 Finally, HURRICANE is structured for scalability using a technique called Hierarchical Clustering <ref> [129, 130] </ref>. With this structuring technique, the operating system is constructed from a hierarchy of clusters, where each cluster manages a unique group of neighboring processors. All system services are replicated to the different clusters. <p> The HURRICANE File System is part of a larger effort to investigate operating system design for large-scale multiprocessors. Hierarchical Clustering is an operating system structuring technique developed for scalability <ref> [129, 130] </ref>. Section 7.1 describes Hierarchical Clustering. Section 7.2 describes how we apply Hierarchical Clustering to the HFS. 7.1 Hierarchical Clustering It is difficult to design software that is scalable.
Reference: [130] <author> Ronald C. Unrau. </author> <title> Scalable Memory Management through Hierarchical Symmetric Multiprocessing. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, Toronto, Canada, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: CHAPTER 2. BACKGROUND AND MOTIVATION 16 Finally, HURRICANE is structured for scalability using a technique called Hierarchical Clustering <ref> [129, 130] </ref>. With this structuring technique, the operating system is constructed from a hierarchy of clusters, where each cluster manages a unique group of neighboring processors. All system services are replicated to the different clusters. <p> The HURRICANE File System is part of a larger effort to investigate operating system design for large-scale multiprocessors. Hierarchical Clustering is an operating system structuring technique developed for scalability <ref> [129, 130] </ref>. Section 7.1 describes Hierarchical Clustering. Section 7.2 describes how we apply Hierarchical Clustering to the HFS. 7.1 Hierarchical Clustering It is difficult to design software that is scalable. <p> We have developed a more structured way of designing scalable operating systems called Hierarchical Clustering. This structuring technique was developed from a set of guidelines for designing scalable demand-driven systems, of which operating systems are an example <ref> [130] </ref>. 7.1.1 Scalability guidelines Unrau used fundamental equations of quantitative performance evaluation to derive a set of necessary and sufficient conditions for the scalability of demand-driven systems [130]. <p> This structuring technique was developed from a set of guidelines for designing scalable demand-driven systems, of which operating systems are an example <ref> [130] </ref>. 7.1.1 Scalability guidelines Unrau used fundamental equations of quantitative performance evaluation to derive a set of necessary and sufficient conditions for the scalability of demand-driven systems [130]. These conditions were then used to derive the following set of guidelines for designing scalable demand-driven systems 1 : Preserving parallelism: A demand-driven system must preserve the parallelism afforded by the applications. <p> The principle of bounded overhead also applies to the space costs of the internal data structures of the system. While the data structures must grow at a rate proportional to the physical resources of the hardware <ref> [1, 130] </ref>, the principle of bounded space cost restricts growth to be no more than linear.
Reference: [131] <author> Zvonko G. Vranesic, Michael Stumm, Ron White, and David Lewis. </author> <title> The Hector multiprocessor. </title> <journal> Computer, </journal> <volume> 24(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: CHAPTER 2. BACKGROUND AND MOTIVATION 14 2.4.1 The target architecture The class of machine architecture we considered is the class of large-scale NUMA multiprocessors with disks distributed across the multiprocessor (Figure 2.4). NUMA multiprocessors achieve their scalability by using segmented architectures <ref> [12, 79, 131] </ref>. As new segments are added, there is an increase in: 1) the number of processors, 2) the network bandwidth, and 3) the number of memory banks and hence the memory bandwidth.
Reference: [132] <author> B. Walker, G. Popek, R. English, C. Kline, and G. Theil. </author> <title> The LOCUS distributed operating system. </title> <booktitle> In Proceedings of the 9th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 49-70. </pages> <booktitle> Operating Systems Review, </booktitle> <month> October </month> <year> 1983. </year>
Reference-contexts: In this section we discuss related work in file system architectures and implementations. In the last decade, most file system research has been directed towards developing file systems for loosely coupled networks of workstations <ref> [6, 22, 53, 98, 132] </ref>. A good overview of research in this area is given by Levy and Sibershatz [80].
Reference: [133] <author> S. Wang and J. Lindberg. HP-UX: </author> <title> Implementation of Unix on the HP 9000 series 500 computer systems. </title> <journal> Hewlett-Packard Journal, </journal> <volume> 35(3) </volume> <pages> 7-15, </pages> <month> March </month> <year> 1984. </year>
Reference-contexts: ASF has been implemented for HURRICANE [129, 128], SunOS [63], IRIX [118], AIX [91], and HP-UX <ref> [133] </ref>. Also ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer. On most systems, improved performance can be achieved when applications use ASF rather than the facilities provided by the system. <p> The HFS application-level library design has advantages that extend beyond HFS and our operating systems. ASF can easily be ported to new platforms; only the set of service-specific ASO classes must be changed. We have ported ASF to SunOS [63], IRIX [118], AIX [91], and HP-UX <ref> [133] </ref>, where it runs independent of any server or special kernel support. Also, ASF can easily be extended to support new types of I/O services that a particular platform might provide. <p> The flexibility of our approach makes ASF readily portable to other platforms. In addition to running on HURRICANE as a part of HFS, ASF runs independent of any server or kernel support on SunOS [63], IRIX [118], AIX [91], and HP-UX <ref> [133] </ref>. Although each of these systems support some variant of Unix, we have found that large improvements in performance can be obtained by adapting the facility to the particular characteristics of each system. <p> Our implementation of the application-level library of HFS, called the Alloc Stream Facility (ASF), has been ported to a number of Unix systems, including SunOS [63], IRIX [118], AIX [91], and HP-UX <ref> [133] </ref>. In addition to the Alloc Stream Interface (ASI), our implementation exports the Unix I/O and stdio I/O interfaces. We have had requests for ASF from many researchers and from dozens of companies, and ASF has been adopted by TERA computer [2] as the I/O library for their parallel supercomputer.
Reference: [134] <author> Terry A. Welch. </author> <title> A technique for high performance data compression. </title> <journal> Computer, </journal> <volume> 17(6) </volume> <pages> 8-19, </pages> <month> June </month> <year> 1984. </year> <note> BIBLIOGRAPHY 114 </note>
Reference-contexts: ASO classes of this type might use the Lempel-Ziv compression algorithm, designed for whole file access <ref> [134] </ref>, the algorithms described by Burrows et al, that allow random file access [13], or application-specific algorithms, such as that used to compress the data base for the Chinook checkers program [114]. encryption/decryption ASO classes: These classes provide for additional security by encrypting and decrypting files as they are accessed.
Reference: [135] <author> David Womble, David Greenberg, Stephen Wheat, and Rolf Riesen. </author> <title> Beyond core: Making parallel computer I/O practical. </title> <booktitle> In Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <pages> pages 56-63, </pages> <address> Hanover, NH, </address> <month> June </month> <year> 1993. </year> <institution> Dartmouth Institute for Advanced Graduate Studies. </institution>
Reference-contexts: The data distribution policy should match the application access pattern to avoid concurrent requests for different data being directed to a single disk <ref> [27, 56, 135] </ref>. Similarly, the latency-hiding policy (e.g., prefetching and poststoring) should match the application access pattern to prevent the application from seeing the large latency of disk I/O [49, 69, 71, 103, 121] and to avoid unnecessary disk requests. <p> The disk striping techniques used by the controllers of such disk arrays, where data is distributed in a simple round-robin fashion across the array, are very restrictive. For parallel applications it is crucial that the file data distribution across the disks matches the access patterns of the application <ref> [27, 56, 135] </ref>. <p> However, a number of studies have indicated that it is important that data be distributed to the disks in a mapping that matches the application access pattern <ref> [9, 26, 27, 28, 56, 135] </ref>. Crockett [28] describes six application access patterns and suggests different distributions that match these patterns. He suggests that the best distribution patterns (given the access patterns) are striped or partitioned. <p> This same study also suggests the possibility of custom mappings for application-specific access patterns. Womble et al. observe that programmers using multicomputers have to explicitly arrange for the data they access to be in local memory <ref> [135] </ref>. Given this, they suggest that it is natural for the I/O system to maintain the same view of local secondary storage, where each processor has its own logical disk.
Reference: [136] <author> R. Zajcew, P. Roy, D. Black, C. Peak, P. Guedes, B. Kemp, J. LoVerso, M. Leibensperger, M. Barnett, F. Rabii, and D. Netterwala. </author> <title> An OSF/1 UNIX for massively parallel multicomputers. </title> <booktitle> In USENIX Winter Conference, </booktitle> <pages> pages 449-468. </pages> <publisher> Usenix, </publisher> <month> January </month> <year> 1993. </year>
Reference-contexts: There have been a number of file systems designed for large-scale multicomputers, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system <ref> [136] </ref>, the file system for the nCUBE [31], and the Rama file system [90]. To achieve scalability, most of the file systems incorporate to some extent the strategies of distributed file systems. <p> CONCLUDING REMARKS 98 10.1 Comparison to other file systems HFS is more flexible than other existing and proposed parallel file system of which we are aware, including CFS [105] for the Intel iPSC, sfs [83] for the CM-5, Vesta [25, 26] for the SP2, the OSF/1 file system <ref> [136] </ref>, the nCUBE file system [31], the Bridge file system [35], and the Rama file system [90]. Our current implementation supports or can easily be extended to support all of the policies used by these file systems to distribute file data across the disks.
References-found: 136

