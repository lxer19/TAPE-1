URL: http://lcs.www.media.mit.edu/groups/gn/discourse/bodychat.ps
Refering-URL: http://lcs.www.media.mit.edu/groups/gn/discourse/
Root-URL: http://www.media.mit.edu
Email: hannes, justine-@media.mit.edu  
Title: BodyChat: Autonomous Communicative Behaviors in Avatars  
Author: Hannes Hgni Vilhjlmsson and Justine Cassell 
Address: 20 Ames Street, Cambridge, MA 02139-4307 USA  
Affiliation: MIT Media Laboratory Gesture and Narrative Language Group  
Abstract: Although avatars may resemble animated communicating interface agents, they have for the most part not profited from recent research into autonomous systems. In particular, even though avatars function within conversational environments (for example, chat or games), and even though they often resemble humans (with a head, hands, and a body) they are incapable of representing the kinds of knowledge that humans have about how to use the body during communication. Their appearance does not translate into increased communicative bandwidth. Faceto-face conversation among humans, however, does make extensive use of the visual channel for interaction management where many subtle and even involuntary cues are read from stance, gaze and gesture. We argue that the modeling and animation of such fundamental behavior is crucial for the credibility and effectiveness of the virtual interaction in chat. By treating the avatar as a communicative agent, we propose a method to automate the animation of important communicative behavior, deriving from work in context analysis and discourse theory. BodyChat is a system that allows users to communicate via text while their avatars automatically animate attention, salutations, turn taking, back-channel feedback and facial expression, as well as simple body functions such as the blinking of the eyes.
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, D.B., Barrus, J.W., Brogan, D., Casey M., McKeown, S., Sterns, I., Waters, R., Yerazunis, W. </author> <year> (1996). </year> <title> Diamond Park and Spline: A Social Virtual Reality System with 3D Animation, Spoken Interaction, and Runtime Modifiability. </title> <type> Technical Report at MERL, </type> <address> Cambridge. </address>
Reference: <author> Argyle, M., Cook, M. </author> <year> (1976). </year> <title> Gaze and Mutual Gaze. </title> <publisher> Cambridge University Press. </publisher>
Reference-contexts: For example, when giving feedback one can avoid overlapping a partner by giving it over a secondary channel, such as by facial expression, while receiving information over the speech channel <ref> (Argyle and Cook 1976) </ref>. The channels can also work together, supplementing or complementing each other by emphasizing salient points (Chovil 1992, Prevost 1996), directing the listeners attention (Goodwin 1986) or providing additional information or elaboration (McNeill 1992, Cassell forthcoming).
Reference: <author> Argyle, M., Ingham, R., Alkema, F., McCallin, M. </author> <year> (1973). </year> <title> The Different Functions of Gaze. </title> <publisher> Semiotica. </publisher>
Reference: <author> Bates, J., Loyall, </author> <title> A.B., Reilley, W.S., (1991). Broad Agents. </title> <journal> SIGART Bulletin, </journal> <volume> 4 (2). </volume>
Reference-contexts: Real-time external control of animated autonomous actors has called for methods to direct animated behavior on a number of different levels such as in ALIVE (Blumberg and Galyean 1995) and in the OZ Project <ref> (Bates et al. 1991) </ref>. In this sense, the goals of BodyChat are similar, but the set of behaviors is different. Here we focus on those behaviors that accompany language. We have also introduced, for the first time, a distinction between conversational Phenomena and Communicative Behaviors. 5.
Reference: <author> Benford, S., Bowers, J., Fahlen, L.E., Greenhalgh, C., Snowdon, D. </author> <year> (1995). </year> <title> User Embodiment in Collaborative Virtual Environments. </title> <booktitle> In Proceedings of CHI95, </booktitle> <pages> 242-249. </pages>
Reference-contexts: RELATED WORK Embodiment in Distributed Virtual Environments has been a research issue in systems such as MASSIVE at CRG Nottingham University, UK where various techniques and design issues have been proposed <ref> (Benford et al. 1995) </ref>. There it is made clear that involuntary facial expression and gesture are important but hard to capture. Avatar autonomy however is not suggested.
Reference: <author> Blumberg, B. M., Galyean, T. A. </author> <year> (1995). </year> <title> MultiLevel Direction of Autonomous Creatures for Real-Time Virtual Environments. </title> <booktitle> Proceedings of SIGGRAPH 95. </booktitle>
Reference-contexts: However, automatically generating the appropriate communicative behaviors and synchronizing them with an actual conversation between users has not been addressed yet in these systems. Real-time external control of animated autonomous actors has called for methods to direct animated behavior on a number of different levels such as in ALIVE <ref> (Blumberg and Galyean 1995) </ref> and in the OZ Project (Bates et al. 1991). In this sense, the goals of BodyChat are similar, but the set of behaviors is different. Here we focus on those behaviors that accompany language.
Reference: <author> Cary, M. S. </author> <year> (1978). </year> <title> The Role of Gaze in the Initiation of Conversation. Social Psychology, </title> <type> 41(3). </type>
Reference: <author> Cassell, J. </author> <title> (forthcoming). A Framework For Gesture Generation And Interpretation. </title> <editor> In R. Cipolla and A. Pentland (eds.), </editor> <title> Computer Vision in Human-Machine Interaction. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Cassell, J., Pelachaud, C., Badler, N., Steedman, M., Achorn, B., Becket, T., Douville, B., Prevost, S., Stone, M. </author> <year> (1994b). </year> <title> Animated Conversation: Rule-based Generation of Facial Expression, Gesture & Spoken Intonation for Multiple Conversational Agents. </title> <booktitle> Proceedings of SIGGRAPH 94. </booktitle>
Reference-contexts: Creating fully autonomous agents capable of natural multimodal interaction entails integrating speech, gesture and facial expression. By applying knowledge from discourse analysis and studies of social cognition, systems like Animated Conversation <ref> (Cassell et al. 1994b) </ref> and Gandalf (Thrisson 1997) have been developed. Animated Conversation renders a graphical representation of two autonomous agents engaged in conversation. The systems dialogue planner generates the conversation and its accompanying communicative signals, based on the agents initial goals and knowledge.
Reference: <author> Cassell, J., Stone, M., Douville, B., Prevost, S., Achorn, B., Steedman, M., Badler, N., Pelachaud, C. </author> <year> (1994a). </year> <title> Modeling the Interaction between Speech and Gesture. </title> <booktitle> Proceedings of the Cognitive Science Society Annual Conference Chovil, </booktitle> <address> N. </address> <year> (1992). </year> <title> Discourse-Oriented Facial Displays in Conversation. Research on Language and Social Interaction, </title> <booktitle> 25, </booktitle> <pages> 163-194. </pages>
Reference: <author> Donath, J. </author> <year> (1995). </year> <title> The Illustrated Conversation. </title> <booktitle> Multimedia Tools and Applications, </booktitle> <volume> 1, </volume> <pages> 79-88. </pages>
Reference-contexts: However these systems have not been able to naturally integrate the graphics with the communication that is taking place. Studies of human communicative behavior have seldom been considered in the design of believable avatars. Significant work includes Judith Donaths Collaboration-at-a-Glance <ref> (Donath 1995) </ref>, where on-screen participants gaze direction changes to display their attention, and Microsofts Comic Chat (Kurlander et al. 1996), where illustrative comic-style images are automatically generated from the interaction. In Collaboration-at-a-Glance the users lack a body and the system only implements a few functions of the head.
Reference: <author> Fiedman, B., Nissenbaum, H. </author> <year> (1997). </year> <title> Software Agents and User Autonomy. </title> <booktitle> Proceedings of Agents'97, </booktitle> <pages> 466-469. </pages>
Reference: <author> Goodwin, C. </author> <year> (1986). </year> <title> Gestures as a Resource for the Organization of Mutual Orientation. </title> <journal> Semiotica, </journal> <volume> 62(1/2). </volume>
Reference-contexts: The channels can also work together, supplementing or complementing each other by emphasizing salient points (Chovil 1992, Prevost 1996), directing the listeners attention <ref> (Goodwin 1986) </ref> or providing additional information or elaboration (McNeill 1992, Cassell forthcoming). When multiple channels are employed in a conversation, we refer to it as being multimodal.
Reference: <author> Kendon, A. </author> <year> (1990). </year> <title> Conducting Interaction: Patterns of behavior in focused encounters. </title> <publisher> Cambridge University Press. </publisher> <address> New York. </address>
Reference-contexts: Susan (unaquainted with Paul) walks by, mutual glances are exchanged, Paul nods smiling, Susan looks at Paul and smiles [distance salutation] (Kendon 1990, 173; Cary 1978, 269) Susan touches the hem of her shirt [grooming] as she dips her head, ceases to smile and approaches Paul <ref> (Kendon 1990, 186, 177) </ref>. She looks back up at Paul when she is within 10 [for initiating a close salutation], meeting his gaze, smiling again (Kendon 1990, 188; Argyle 1976, 113). <p> Paul tilts his head to the side slightly and says Paul, as he offers Susan his hand, which she shakes lightly while facing him and replying Susan [close salutation] <ref> (Kendon 1990, 188, 193) </ref>. Then she steps a little to the side to face Paul at an angle (Kendon 1990, 193; Argyle 1976, 101). A conversation starts. <p> If the partner replies with a Break Away sentence, the conversation is broken with a mutual farewell. If the partner replies with a normal sentence, the Break Away is cancelled and the conversation continues. Only when both partners produce subsequent Break Away sentences, is the conversation broken <ref> (Kendon 1990, Schegloff and Sacks 1973) </ref>. 5.2. Generated behaviors When discussing the communicative signals, it is essential to make clear the distinction between the Conversational Phenomena on one hand and the Communicative Behaviors on the other.
Reference: <author> Kendon, A. </author> <year> (1992). </year> <title> The negotiation of context in faceto-face interaction. </title> <booktitle> In A. </booktitle>
Reference: <author> Duranti and C. Goodwin (eds.), </author> <title> Rethinking context: language as interactive phenomenon. </title> <publisher> Cambridge University Press. </publisher> <address> New York. </address>
Reference: <author> Kurlander, D., Skelly, T., Salesin, D. </author> <year> (1996). </year> <note> Comic Chat. Proceedings of SIGGRAPH 96. </note>
Reference-contexts: Studies of human communicative behavior have seldom been considered in the design of believable avatars. Significant work includes Judith Donaths Collaboration-at-a-Glance (Donath 1995), where on-screen participants gaze direction changes to display their attention, and Microsofts Comic Chat <ref> (Kurlander et al. 1996) </ref>, where illustrative comic-style images are automatically generated from the interaction. In Collaboration-at-a-Glance the users lack a body and the system only implements a few functions of the head.
Reference: <author> McNeill, D. </author> <year> (1992). </year> <title> Hand and Mind: What Gestures Reveal about Thought. </title> <institution> University of Chicago. </institution>
Reference-contexts: The channels can also work together, supplementing or complementing each other by emphasizing salient points (Chovil 1992, Prevost 1996), directing the listeners attention (Goodwin 1986) or providing additional information or elaboration <ref> (McNeill 1992, Cassell forthcoming) </ref>. When multiple channels are employed in a conversation, we refer to it as being multimodal. The current work focuses on gaze and communicative facial expression mainly because these are fundamental in establishing and maintaining a live link between participants in a conversation.
Reference: <author> Perlin, K., Goldberg, A. </author> <year> (1996). </year> <title> Improv: A System for Scripting Interactive Actors in Virtual Worlds. </title> <note> SIGGRAPH 1996 Course Notes #25. </note>
Reference-contexts: The real-time animation of lifelike 3D humanoid figures has been greatly improved in recent years. The Improv system <ref> (Perlin and Goldberg 1996) </ref> demonstrates a visually appealing humanoid animation and provides tools for scripting complex behaviors, ideal for agents as well as avatars. Similarly the Humanoid 2 project deals with virtual actors performing scripts as well as improvising role-related behavior (Wavish and Connah 1997).
Reference: <author> Prevost, S. </author> <year> (1996). </year> <title> Modeling Contrast in the Generation and Synthesis of Spoken Language. </title> <booktitle> In Proceedings of ICSLP 96. </booktitle> <volume> 16 Schegloff, </volume> <editor> E. </editor> <year> (1968). </year> <title> Sequencing in Conversational Openings. </title> <journal> American Anthropologist, </journal> <volume> 70, </volume> <pages> 1075-1095. </pages>
Reference: <author> Schegloff, E., Sacks, H. </author> <year> (1973). </year> <title> Opening up closings. </title> <journal> Semiotica, </journal> <volume> 8, </volume> <pages> 289-327. </pages>
Reference: <author> Thrisson, K. R. </author> <year> (1997). </year> . <title> Gandalf: An Embodied Humanoid Capable of Real-Time Multimodal Dialogue with People. </title> <booktitle> Proceedings of Agents'97, </booktitle> <pages> 536-537. </pages>
Reference-contexts: Creating fully autonomous agents capable of natural multimodal interaction entails integrating speech, gesture and facial expression. By applying knowledge from discourse analysis and studies of social cognition, systems like Animated Conversation (Cassell et al. 1994b) and Gandalf <ref> (Thrisson 1997) </ref> have been developed. Animated Conversation renders a graphical representation of two autonomous agents engaged in conversation. The systems dialogue planner generates the conversation and its accompanying communicative signals, based on the agents initial goals and knowledge.
Reference: <author> Thrisson, K.R., Cassell, J. </author> <title> (1996) Why Put an Agent in a Human Body: The Importance of Communicative Feedback in Human-Humanoid Dialogue. </title> <booktitle> (abstract) In Proceedings of Lifelike Computer Characters '96, Snowbird, Utah, </booktitle> <pages> 44-45. </pages>
Reference-contexts: However, lively emotional expression in interaction is in vain if mechanisms for establishing and maintaining mutual focus and attention are not in place <ref> (Thrisson and Cassell 1996) </ref>. We tend to take communicative behaviors such as gaze and head movements for granted, as their spontaneous nature and nonvoluntary fluid execution makes them easy to overlook when recalling a previous encounter (Cassell, forthcoming).
Reference: <author> Torres, O., Cassell, J., Prevost, S. </author> <year> (1997). </year> <title> Modeling Gaze Behavior as a Function of Discourse Structure. </title> <booktitle> In Proceedings of the First International Workshop on Human-Computer Conversations 1997. </booktitle> <address> Bellagio, Italy. </address>
Reference: <author> Wavish, P., Connah, D. </author> <year> (1997). </year> <title> Virtual actors that can perform scripts and improvise roles. </title> <booktitle> Proceedings of Agents'97, </booktitle> <pages> 317-322. </pages>
Reference-contexts: The Improv system (Perlin and Goldberg 1996) demonstrates a visually appealing humanoid animation and provides tools for scripting complex behaviors, ideal for agents as well as avatars. Similarly the Humanoid 2 project deals with virtual actors performing scripts as well as improvising role-related behavior <ref> (Wavish and Connah 1997) </ref>. However, automatically generating the appropriate communicative behaviors and synchronizing them with an actual conversation between users has not been addressed yet in these systems.
References-found: 25

