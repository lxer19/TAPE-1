URL: http://www.eecs.umich.edu/PPP/HPL-97-24.ps
Refering-URL: http://www.eecs.umich.edu/PPP/publist.html
Root-URL: http://www.cs.umich.edu
Email: abandah@hpl.hp.com  
Title: Characterizing Shared-Memory Applications: A Case Study of the NAS Parallel Benchmarks Keywords: Application Analysis, Shared-Memory
Author: Gheith A. Abandah 
Date: January 14, 1997  
Affiliation: Computer Systems Laboratory HP Laboratories  
Abstract: The objective of this report is to present our characterization of a shared-memory implementation of the NAS Parallel Benchmarks (NPB). This characterization is needed to support the design decisions of future shared-memory multiprocessors. This report presents two sets of characterization data; the first set is the application characteristics that do not change from one hardware configuration to another, and the second set is the traffic characteristics of the application when run on a possible future hardware configuration. The data presented in this report include characterization of NPB communication, sharing, and cache behavior. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Abandah, </author> <title> "Tools for characterizing distributed shared memory applications," </title> <type> Tech. Rep. </type> <institution> HPL-96-157, HP Laboratories, </institution> <month> Dec. </month> <year> 1996. </year>
Reference-contexts: This study makes use of a collection of analysis tools that we have developed to analyze and characterize shared-memory applications. These tools are described in detail in our HP Laboratories technical report <ref> [1] </ref>. We have developed two main tools for analyzing shared-memory applications. The first tool is intended to generate abstractions that expose the inherent application characteristics. The second tool is intended to predict the application performance on a specific hardware configuration. <p> It shows that a shared-memory multiprocessor is used to collect traces by executing instrumented application codes. Nevertheless, other methods can be used for trace collection. The Shared-Memory Application Instrumentation Tool (SMAIT) is used to instrument applications <ref> [1] </ref>. Instead of generating trace files, SMAIT can also pipe the traces to the analysis tools for on-the-fly analysis. On-the-fly analysis enables analyzing longer execution periods by solving the problem of huge trace files.
Reference: [2] <author> D. Bailey et al., </author> <title> "The NAS parallel benchmarks," </title> <type> Technical Report RNR-94-07, </type> <institution> NASA Ames Research Center, </institution> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Section 3 presents NPB configuration independent characteristics. Section 4 presents NPB traffic characteristics on a possible future distributed shared memory (DSM) multiprocessor configuration. Section 5 concludes the paper by stating some of the key NPB characteristics. 2 NAS Parallel Benchmarks The NAS Parallel Benchmarks 1.0 <ref> [2] </ref> are 5 kernels and 3 pseudo-applications that mimic the computation and data movement characteristics of large-scale computational fluid dynamic applications. These benchmarks are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines.
Reference: [3] <author> T. Brewer, </author> <title> "A highly scalable system utilizing up to 128 PA-RISC processors," </title> <booktitle> in Digest of papers, COMPCON'95, </booktitle> <pages> pp. 133-140, </pages> <month> Mar. </month> <year> 1995. </year>
Reference-contexts: These benchmarks are specified algorithmically so that computer vendors can implement them on a wide range of parallel machines. In this report we present our analysis of the HP Convex implementation of NPB on the Convex Exemplar multiprocessor <ref> [3] </ref>. This implementation was mainly developed by Herb Rothmund of the HP Convex Technology Center. The performance of an earlier version of this implementation is reported in a NPB results report [4].
Reference: [4] <author> S. Saini and D. H. Bailey, </author> <title> "NAS parallel benchmark results 12-95," </title> <type> Tech. Rep. </type> <institution> NAS-95-021, NASA Ames Research Center, </institution> <month> Dec. </month> <year> 1995. </year>
Reference-contexts: This implementation was mainly developed by Herb Rothmund of the HP Convex Technology Center. The performance of an earlier version of this implementation is reported in a NPB results report <ref> [4] </ref>. We have analyzed two of the NPB kernels (CG and MG) and the three pseudo-applications (SP, LU, and BT). Table 1 shows the two NPB problem sizes analyzed in this study. The MG problem sizes are not standard and are selected to get a reasonable trace length.
Reference: [5] <author> D. Lenoski et al., </author> <title> "The Stanford DASH Multiprocessor," </title> <journal> Computer, </journal> <volume> vol. 25, </volume> <pages> pp. 63-79, </pages> <month> Mar. </month> <year> 1992. </year> <month> 21 </month>
Reference-contexts: Each node has 4 processors connected by a cache coherent bus and has a 4-bank memory with full directory. The system uses a cache coherence protocol similar to the one used in the DASH <ref> [5] </ref> project. Each processor has a combined level 2 cache that is 4 MB in size and 4 way set-associative. We used a number of cache line sizes ranging from 32 to 256 bytes. Unless otherwise specified, the cache line size is 64 bytes.
References-found: 5

