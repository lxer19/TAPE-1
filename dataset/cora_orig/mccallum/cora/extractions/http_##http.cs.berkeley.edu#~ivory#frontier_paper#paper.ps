URL: http://http.cs.berkeley.edu/~ivory/frontier_paper/paper.ps
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/postgres/oldlunch.html
Root-URL: 
Title: Modeling and Identifying Bottlenecks in EOSDIS  
Author: James Demmel Melody Y. Ivory Sharon L. Smith 
Address: UC Berkeley Berkeley, CA 94720 Berkeley, CA 94720  
Affiliation: Computer Science Division Computer Science Division Computer Science Division Department of Mathematics UC Berkeley UC Berkeley  
Note: Appears in the Proceedings of the Frontiers '96 Conference, IEEE, 1996. 1  
Abstract: Many parallel application areas that exploit massive parallelism, such as climate modeling, require massive storage systems for the archival and retrieval of data sets. As such, advances in massively parallel computation must be coupled with advances in mass storage technology in order to satisfy I/O constraints of these applications. We demonstrate the effects of such I/O-computation disparity for a representative distributed information system, NASA's Earth Observing System Distributed Information System (EOSDIS). We use performance modeling to identify bottlenecks in EOS-DIS for two representative user scenarios from climate change research. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Arakawa and V. R. Lamb. </author> <title> Computational design of the basic dynamical processes of the UCLA general circulation model. Methods in Comput. </title> <journal> Phys., </journal> <volume> 17 </volume> <pages> 173-265, </pages> <year> 1977. </year>
Reference-contexts: For the particular scenario described here, we assume that atmospheric variables, such as wind speeds, temperatures, pressures and humidities, have been archived from observational data or from model data created by an atmospheric general circulation model (AGCM), such as the UCLA AGCM <ref> [1, 11, 10] </ref>. Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) [7, 8, 9].
Reference: [2] <author> P. Brown and M. Stonebraker. </author> <title> Big Sur: a system for the management of earth science data. </title> <booktitle> In Proceedings of the 21st International Conference on Very Large Data Bases, </booktitle> <pages> pages 720-728, </pages> <address> Zurich, Switzerland, </address> <month> September </month> <year> 1995. </year>
Reference-contexts: A prototype for the EOS Data and Information System (EOSDIS) is currently being developed by NASA to perform the data management activities of the EOS project [5]. In addition, several research activities, such as Big Sur <ref> [2] </ref>, are being funded to investigate database management issues in EOSDIS. In this paper we scrutinize the performance of the proposed tertiary storage system for EOSDIS.
Reference: [3] <author> L. T. Chen, R. Drach, M. Keating, S. Louis, D. Rotem, and A. Shoshani. </author> <title> Efficient organization and access of multi-dimensional datasets on tertiary storage systems. </title> <journal> Information Systems Journal, </journal> <year> 1995. </year>
Reference-contexts: The organization of multi-dimensional arrays, other than a purely linear allocation in which arrays are stored to disk in some contiguous ordering of the dimensions such as column-major or row-major order, has been the subject of several recent studies <ref> [3, 14] </ref>. Research shows that efficient array organization is imperative to minimize the time to process queries of the type that the EOSDIS DBMS will support. Consequently, we experiment with different array organizations to assess the performance tradeoffs of each.
Reference: [4] <author> A. L. Chervenak. </author> <title> Tertiary storage: an evaluation of new applications. </title> <type> PhD thesis, </type> <institution> University of California, Berkeley, </institution> <year> 1994. </year>
Reference-contexts: We characterize parallelism by the parameter, n tape drives, which is equal to the total number of cabinets available to the tertiary storage system. In general, there are possibly multiple drives to a cabinet allowing for parallelism within a cabinet as well as across cabinets. As shown in <ref> [4] </ref>, contention for drives severely limits the ability to access drives simultaneously within a cabinet, so we conservatively assume that only one tape drive per cabinet can be accessed at any given time.
Reference: [5] <author> F. Davis, W. Farrell, J. Gray, C. R. Mechoso, R. Moore, S. Sides, and M. Stonebraker. </author> <title> EOSDIS alternative architecture. </title> <type> Sequoia 2000 Technical Report 94/1, </type> <institution> University of California, Berkeley, </institution> <address> CA, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: A prototype for the EOS Data and Information System (EOSDIS) is currently being developed by NASA to perform the data management activities of the EOS project <ref> [5] </ref>. In addition, several research activities, such as Big Sur [2], are being funded to investigate database management issues in EOSDIS. In this paper we scrutinize the performance of the proposed tertiary storage system for EOSDIS. <p> from the tertiary storage system to a disk cache, and the time to communicate the data over a network from the disk cache. 3.1.1 Tertiary storage systems We model two high-density linear tape systems representative of the Super DAAC and Peer DAAC systems described in the EOSDIS alternative architecture report <ref> [5] </ref>. An IBM NTP 3495 DLT robot represents the high speed tertiary storage system for a Super DAAC while a DEC TL820 DLT robot represents the lower cost tertiary storage system for a Peer DAAC. Table 2 outlines characteristics of these systems. <p> This strategy has to be supportive of the range of queries that the EOS-DIS DBMS will handle. EOSDIS researchers estimate that 80% to 90% of these queries will be ad hoc <ref> [5] </ref> which means that optimal chunk strategies, such as the ones used for each scenario, may not exhibit good performance in general. This was demonstrated by using the scenario 1 archive and optimal shape to process the query for scenario 2.
Reference: [6] <author> J. Demmel and S. L. Smith. </author> <title> Parallelizing a global atmospheric chemical tracer model. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conf., </booktitle> <pages> pages 718-725, </pages> <address> Knoxville, TN., </address> <month> May </month> <year> 1994. </year>
Reference-contexts: Such an approach has been an effective tool in the design, performance tuning, and scalability assessment of a parallel implementation for the GATOR code <ref> [6, 7] </ref>.
Reference: [7] <author> J. Demmel and S. L. Smith. </author> <title> Performance of a parallel global atmospheric chemical tracer mode l. </title> <booktitle> In Supercomputing 95, </booktitle> <address> San Diego, CA., </address> <month> Decem-ber </month> <year> 1995. </year>
Reference-contexts: Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) <ref> [7, 8, 9] </ref>. The second user scenario describes the use of data mining as a means for extracting large-scale patterns from climate data. <p> Such an approach has been an effective tool in the design, performance tuning, and scalability assessment of a parallel implementation for the GATOR code <ref> [6, 7] </ref>.
Reference: [8] <author> M. Z. Jacobson. </author> <title> Developing, coupling and applying a gas, aerosol, transport, and radiation model to study urban and regional air pollution. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> Los Angeles, </address> <year> 1994. </year>
Reference-contexts: Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) <ref> [7, 8, 9] </ref>. The second user scenario describes the use of data mining as a means for extracting large-scale patterns from climate data.
Reference: [9] <author> M. Z. Jacobson, R. P. Turco, R. Lu, and O. B. Toon. </author> <title> Development and application of a new air pollution modeling system. part i: </title> <institution> Gas-phase simulation. Atmos. Environ. B, </institution> <year> 1995. </year>
Reference-contexts: Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) <ref> [7, 8, 9] </ref>. The second user scenario describes the use of data mining as a means for extracting large-scale patterns from climate data.
Reference: [10] <author> C. R. Mechoso, C. C. Ma, J. D. Farrara, and J. A. Spahr. </author> <title> Simulations of interannual variablil-ity with a coupled atmosphere-ocean general circulation model. </title> <booktitle> In Fifth Conference on Climate Variations, </booktitle> <address> Boston, MA, 1991. </address> <publisher> American Meteorology Society. </publisher>
Reference-contexts: For the particular scenario described here, we assume that atmospheric variables, such as wind speeds, temperatures, pressures and humidities, have been archived from observational data or from model data created by an atmospheric general circulation model (AGCM), such as the UCLA AGCM <ref> [1, 11, 10] </ref>. Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) [7, 8, 9].
Reference: [11] <author> C. R. Mechoso, C. C. Ma, J. D. Farrara, J. A. Spahr, and R. W. Moore. </author> <title> Parallelization and distribution of a coupled atmosphere-ocean general circulation model. </title> <journal> Monthly Weather Review, </journal> <volume> 121(7) </volume> <pages> 2062-2076, </pages> <year> 1993. </year>
Reference-contexts: For the particular scenario described here, we assume that atmospheric variables, such as wind speeds, temperatures, pressures and humidities, have been archived from observational data or from model data created by an atmospheric general circulation model (AGCM), such as the UCLA AGCM <ref> [1, 11, 10] </ref>. Table 1 shows an example of the type of variables that might be archived. These variables are then used as forced input to an atmospheric chemical tracer model, such as the gas, aerosol, transport and radiation model (GATOR) [7, 8, 9].
Reference: [12] <author> E. Mesrobian, R. R. Muntz, E. C. Shek, C. R. Me-choso, J. D. Farrara, J. A. Spahr, and P. Stolorz. </author> <title> Real time data mining, management, and visualization of GCM output. </title> <type> Technical report, </type> <institution> University of California at Los Angelos, </institution> <year> 1994. </year>
Reference-contexts: This type of analysis, known as feature extraction, is an area of current research and promises to be a valuable tool in the analysis of geophysical science datasets. One system that employs feature extraction is CONQUEST <ref> [12, 13] </ref>, which has been used to study cyclones in model and observational data at UCLA. A high level description of the software elements that comprise both scenarios is shown in Figure 1.
Reference: [13] <author> E. Mesrobian, R. R. Muntz, E. C. Shek, C. R. Me-choso, J. D. Farrara, J. A. Spahr, and P. Stolorz. </author> <title> QUEST: Content-based access to geophysical databases. </title> <booktitle> In AAAI Workshop on AI Technologies in Environmental Applications, </booktitle> <address> Seattle, WA., </address> <month> July </month> <year> 1994. </year>
Reference-contexts: This type of analysis, known as feature extraction, is an area of current research and promises to be a valuable tool in the analysis of geophysical science datasets. One system that employs feature extraction is CONQUEST <ref> [12, 13] </ref>, which has been used to study cyclones in model and observational data at UCLA. A high level description of the software elements that comprise both scenarios is shown in Figure 1.
Reference: [14] <author> S. Sarawagi and M. Stonebraker. </author> <title> Efficient organization of large multidimensional arrays. </title> <type> Technical Report SEQUOIA 2000 Report 93/32, </type> <institution> University of California, Berkeley, </institution> <year> 1993. </year>
Reference-contexts: The organization of multi-dimensional arrays, other than a purely linear allocation in which arrays are stored to disk in some contiguous ordering of the dimensions such as column-major or row-major order, has been the subject of several recent studies <ref> [3, 14] </ref>. Research shows that efficient array organization is imperative to minimize the time to process queries of the type that the EOSDIS DBMS will support. Consequently, we experiment with different array organizations to assess the performance tradeoffs of each.
Reference: [15] <author> S. Smith, M. Y. Ivory, and J. Demmel. </author> <title> Modeling and identifying bottlenecks in the EOSDIS architecture. </title> <type> Technical Report SEQUOIA 2000 Report 96/67, </type> <institution> University of California, Berkeley, </institution> <year> 1996. </year>
Reference-contexts: Tape allocation determines how much of a tape can be used for data relevant to our archive. In this paper we assume that 25% of each tape accessed is dedicated to our archive. We discuss other tape allocation policies in <ref> [15] </ref>. Tape layout policies determine how tapes are distributed among cabinets in the tertiary storage system. We use optimal tape layout policies for both scenarios in this paper and explore other policies in [15]. <p> We discuss other tape allocation policies in <ref> [15] </ref>. Tape layout policies determine how tapes are distributed among cabinets in the tertiary storage system. We use optimal tape layout policies for both scenarios in this paper and explore other policies in [15]. In scenario one, all of the chunks that we need to satisfy the query are laid out contiguously in the allocated space on each tape.
References-found: 15

