URL: http://www.cs.ucsb.edu/TRs/techreports/TRCS94-23.ps
Refering-URL: http://www.cs.ucsb.edu/TRs/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Static Global Scheduling for Optimal Computer Vision and Image Processing Operations on Distributed-Memory Multiprocessors  
Author: Cheolwhan Lee Yuan-Fang Wang Tao Yang 
Keyword: parallel processing, image processing, computer vision, schedule, mapping  
Address: Santa Barbara, CA 93106  
Affiliation: Department of Computer Science University of California, Santa Barbara  
Pubnum: TRC94-23  
Email: fchlee, yfwang, tyangg@cs.ucsb.edu  
Date: December 1, 1994  
Abstract: In this paper, we develop a static global scheduling scheme for mapping computer vision and image processing (CVIP) operations on distributed-memory multiprocessors. Unlike most current parallel image processing research which focuses on parallelizing individual processing algorithms on a particular parallel architecture, our scheduler is for optimizing processor assignment and data partition for an entire image processing pipeline. The scheduler operates on task graphs specified by conventional visual languages such as Khoros and Explorer. A task graph is assumed to be a linear chain of operations with any number of nested loops. The task chain is first decomposed into simpler subchains; each a linear sequence of tasks without loops. The communication and computation costs of the component tasks in the subchains are determined by a taxonomy of CVIP operations. Data redistribution overheads in between tasks can also be tabulated in advance for many popular data partitioning schemes. The scheduler then employs a shortest path algorithm to optimize the parallel time, taking into consideration possible variation in the task and resource parameters (such as the image size and number of processors used), and both the intra-operation and the inter-operation computation and communication times. In this paper, we present the scheduling scheme, and provide analyses and experimental results to verify our approach. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Annaratone et al., </author> <title> The Warp Computer: Architecture, Implementation and Performance, </title> <journal> IEEE Trans. Computers, </journal> <volume> Vol. C-36, No. 12, </volume> <month> Dec. </month> <year> 1987, </year> <pages> pp. 1523-1538. </pages>
Reference-contexts: In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) [24] makes image processing a major limiting factor in digesting and cataloging image data. So far, parallel image processing research has focused on implementing individual algorithms on a particular parallel architecture <ref> [1, 3, 11, 19] </ref>. However, optimizing individual tasks in a processing pipeline comprising many such tasks does not guarantee the optimal performance of the whole operation.
Reference: [2] <author> R. F. Browne and R. M. Hodgson, </author> <title> Mapping Image Processing Operations onto Transputer Networks, </title> <journal> Microprocessors and Microsystems, </journal> <volume> Vol. 13 No. 3, </volume> <month> April </month> <year> 1989, </year> <month> pp.203-211 </month>
Reference-contexts: If an output pixel value of an operator generally depends upon the values of the whole input image, the operator is classified as a global operator <ref> [2] </ref>. The parallel execution time of a task, P T i , is the sum of its computation time, T cp (i), and communication time, T cm (i).
Reference: [3] <author> P. K. Biswas, J. Mukherjee, and B. N. Chatterji, </author> <title> Component Labeling in Pyramid Architecture, </title> <journal> Pattern Recognition, </journal> <volume> Vol. 26, No. 7, </volume> <year> 1993, </year> <pages> pp. 1099-1115. </pages>
Reference-contexts: In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) [24] makes image processing a major limiting factor in digesting and cataloging image data. So far, parallel image processing research has focused on implementing individual algorithms on a particular parallel architecture <ref> [1, 3, 11, 19] </ref>. However, optimizing individual tasks in a processing pipeline comprising many such tasks does not guarantee the optimal performance of the whole operation.
Reference: [4] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: Shortest Path Searching The computation and communication cost of each node and the data redistribution cost of each edge in an SG are resolved using the given set of parameter values. Then a shortest path algorithm <ref> [4] </ref> is employed to find the shortest path from the dummy starting node to the dummy end node. Because a scheduling graph is a dag, the shortest path can be found efficiently in O (jV j + jEj) time.
Reference: [5] <institution> IRIX Explorer User's Guide, Silicon Graphics, Inc., Mountain View, </institution> <address> CA. </address> <year> 1992. </year>
Reference-contexts: The scheduler operates on the task graph representations of CVIP algorithms, which can be specified using commercial visual languages such as Explorer <ref> [5] </ref> and Khoros [17]. A task graph is assumed to be a linear chain of operations with any number of nested loops. The task chain is first decomposed into a number of subsequences based on the loop structures and the types of component operations in the graph.
Reference: [6] <author> M. Gupta and P. Banerjee, </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> Vol. 3, No. 2, </volume> <month> Mar. </month> <year> 1992, </year> <pages> pp. 179-193. </pages>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations.
Reference: [7] <author> R C. Gonzalez and R. E. Woods, </author> <title> Digital Image Processing, </title> <publisher> Addison-Wesley Pub. Company, </publisher> <year> 1992. </year>
Reference-contexts: The next node was a masking operation; hence, the node was expanded into six nodes in the scheduling graph using six different data partitioning schemes. The Fourier transform operation assumed two different implementations. Both of them used the transpose algorithm for the parallel 1D FFT operation. For 2D FFT <ref> [7] </ref>, one library routine used row partitioning for 1D FFT, followed by a transpose operation, and then by another row-wise 1D FFT. The other routine was similar but assumed a column partition for 1D FFT instead.
Reference: [8] <author> A. Gerasoulis and T. Yang, </author> <title> On the Granularity and Clustering of Directed Acylic Task Graphs, </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> Vol. 4, No. 6, </volume> <month> June </month> <year> 1993, </year> <pages> pp. 686-701. </pages>
Reference-contexts: Furthermore, we assume that the blocking factor b in the cyclic patterns is such that the computation time is greater than the communication time for an efficient parallel implementation <ref> [8] </ref>. A node representing a global operation can be similarly expanded; however, legal data partition schemes may be limited to those supported by the library of existing parallel codes. <p> fi = 2:4sec=word, ! = 1:6sec=f lop; and an INTEL Paragon architecture with ff = 250sec, fi = 0:1sec=word, ! = 0:2sec=f lop. b, the blocking factor, was assumed to be 8 to assure an efficient parallel implementation where the local communication cost did not dominate the local computation cost <ref> [8] </ref>.
Reference: [9] <author> L. H. Jamieson, et al., </author> <title> Parallel Scalable Libraries and Algorithms for Computer Vision, </title> <booktitle> Proceedings of the 12th IAPR Int. Conf. on Pattern Recognition, </booktitle> <address> Jerusalem, Israel, </address> <month> Oct. </month> <year> 1994, </year> <pages> pp. 223-228. </pages>
Reference-contexts: A static scheduler also enjoys the following advantages: a static scheduler does not incur run-time scheduling overhead, is simpler to design, and is applicable to a large class of image operations. Library-based parallel system development tools were discussed in <ref> [9, 10, 18] </ref>. [18] presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. [9, 10] illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. <p> Library-based parallel system development tools were discussed in [9, 10, 18]. [18] presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. <ref> [9, 10] </ref> illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. This system was able to select the most efficient implementation of an algorithm by considering various system parameters. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [10] <author> L. H. Jamieson, E. J. Delp, J. N. Patel, C.-C. Wang, and A. A. Khokhar, </author> <title> A Library-Based Program Development Environment for Parallel Image Processing, </title> <booktitle> Proceedings of Scalable Parallel Libraries Conference, </booktitle> <institution> Mississippi State Univ., Mississippi, </institution> <month> Oct. </month> <year> 1993, </year> <pages> pp. 187-194. </pages>
Reference-contexts: A static scheduler also enjoys the following advantages: a static scheduler does not incur run-time scheduling overhead, is simpler to design, and is applicable to a large class of image operations. Library-based parallel system development tools were discussed in <ref> [9, 10, 18] </ref>. [18] presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. [9, 10] illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. <p> Library-based parallel system development tools were discussed in [9, 10, 18]. [18] presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. <ref> [9, 10] </ref> illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. This system was able to select the most efficient implementation of an algorithm by considering various system parameters. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [11] <author> J. F. Jeng and S. Sahni, </author> <title> Reconfigurable Mesh Algorithms for the Hough Transform, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 20, </volume> <year> 1994, </year> <pages> pp. 69-77. </pages>
Reference-contexts: In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) [24] makes image processing a major limiting factor in digesting and cataloging image data. So far, parallel image processing research has focused on implementing individual algorithms on a particular parallel architecture <ref> [1, 3, 11, 19] </ref>. However, optimizing individual tasks in a processing pipeline comprising many such tasks does not guarantee the optimal performance of the whole operation.
Reference: [12] <author> C. H. Koelbel, D. B. Loveman, R. S. Schreiber, G. L. Steele Jr., and M. E. Zosel, </author> <title> "The High Performance Fortran Handbook", </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: In our simulation, we used the system parameters (processor speed, communication bandwidth, etc.) from Intel iPSC/860 and Paragon, and from nCube which support mesh connection. A task may employ one of the following six data partitioning schemes, which are also provided by the High-Performance Fortran <ref> [12] </ref>: row, column, row-cyclic, column-cyclic, block, and block-cyclic partitions.
Reference: [13] <author> V. K. P. Kumar (Ed.), </author> <title> Parallel Architectures and Algorithms for Image Understanding, </title> <publisher> Academic Press Inc., </publisher> <address> Boston, Mass., </address> <year> 1991. </year> <month> 23 </month>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [14] <author> V. K. P. Kumar, et al., </author> <title> Introduction to Parallel Computing: Design and Analysis of Algorithms, </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> Redwood City, CA., </address> <year> 1994. </year>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule. <p> Column-to-block and block-to-column redistributions have a communication pattern (Figure 5 (g)-(i)) where data in the ith processor row are exchanged with data in the ith processor column. The above all-to-all personalized communication pattern for exchanging data among processors has been studied before <ref> [14] </ref>. However, no specific data distribution was associated with each communication scheme. Our research here presents the detailed cost functions from one data partition to another based on previously developed all-to-all personalized communication for the six popular data allocation schemes. <p> To realize the all-to-all personalized communication on a 2-D mesh of processors, messages from a processor anywhere in the mesh must be delivered to a processor anywhere else. One well-known communication pattern accomplishes the feast in two separate phases <ref> [14] </ref>. In the first phase, processors located in the same row communicate with one another by circulating messages (i.e., data) to the left (or right) neighbors. <p> This is a pattern in which each processor exchanges data only with processors in the same row, so that the pattern becomes an all-to-all personalized communication on a ring <ref> [14] </ref>. At the first step, every processor sends a message of size ( p 1)= p fi n 2 =p to the next processor. Each processor then retains 1= p p fi n 2 =p image data which are addressed to itself, and transmits the remaining to the next processor.
Reference: [15] <author> S.-Y. Lee and J. K. Aggarwal, </author> <title> A System Design/Scheduling Strategy for Parallel Image Processing, </title> <journal> IEEE Trans. PAMI, </journal> <volume> Vol. 12, No. 2, </volume> <month> Feb. </month> <year> 1990, </year> <pages> pp. 194-204. </pages>
Reference-contexts: However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations. For example, <ref> [15] </ref> discussed static and dynamic design/scheduling strategies for image processing tasks comprising a linear sequence of tasks. Although this research provided a useful design framework for parallel image processing, it was not applicable to message-passing architectures and did not consider the communication overheads for redistributing image data. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [16] <author> C. Lee, Y. F. Wang, D. Uecker, and Y. Wang, </author> <title> Image Analysis for Automated Tracking in Robot-Assisted Endoscopic Surgery, </title> <booktitle> Proceedings of 12th Int. Conf. on Pattern Recognition, </booktitle> <address> Jerusalem, Israel, </address> <month> Sep. </month> <year> 1994, </year> <pages> pp. 88-92. </pages>
Reference-contexts: Program structures, by a large extent, comprise nested loops over image data, and a number of loop parallelization schemes are potentially applicable [25]. Furthermore, CVIP problems can be computationally intensive, and hence, can benefit from the parallel computation support. For example, many robotic servoing tasks using visual feedback <ref> [16] </ref> require image sequences processed at the video frame rate (30 frames per second). In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) [24] makes image processing a major limiting factor in digesting and cataloging image data.
Reference: [17] <author> J. Rasure and S. Kubica, </author> <title> The Khoros Application Development Environment, </title> <publisher> Khoral Research Inc., </publisher> <address> Albuquerque, New Mexico, </address> <year> 1992. </year>
Reference-contexts: The scheduler operates on the task graph representations of CVIP algorithms, which can be specified using commercial visual languages such as Explorer [5] and Khoros <ref> [17] </ref>. A task graph is assumed to be a linear chain of operations with any number of nested loops. The task chain is first decomposed into a number of subsequences based on the loop structures and the types of component operations in the graph.
Reference: [18] <author> A. P. Reeves, </author> <title> Parallel Programming for Computer Vision, </title> <journal> IEEE Software, </journal> <volume> Vol. 8, No. 6, </volume> <month> Nov. </month> <year> 1991, </year> <pages> pp. 51-59. </pages>
Reference-contexts: A static scheduler also enjoys the following advantages: a static scheduler does not incur run-time scheduling overhead, is simpler to design, and is applicable to a large class of image operations. Library-based parallel system development tools were discussed in <ref> [9, 10, 18] </ref>. [18] presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. [9, 10] illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. <p> A static scheduler also enjoys the following advantages: a static scheduler does not incur run-time scheduling overhead, is simpler to design, and is applicable to a large class of image operations. Library-based parallel system development tools were discussed in [9, 10, 18]. <ref> [18] </ref> presented a parallel library of CVIP routines, but it did not provide scheduling support for 2 parallel tasks. [9, 10] illustrated a user-friendly interface of a parallel library, which stored several implementations of a single algorithm, each optimized for a different parallel architecture. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [19] <author> H. J. Siegel, J. B. Armstrong, and D. W. Watson, </author> <title> Mapping Computer-Vision-Related Tasks onto Reconfigurable Parallel-Processing Systems, </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 2, </volume> <month> Feb. </month> <year> 1992, </year> <month> pp.54-63. </month>
Reference-contexts: In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) [24] makes image processing a major limiting factor in digesting and cataloging image data. So far, parallel image processing research has focused on implementing individual algorithms on a particular parallel architecture <ref> [1, 3, 11, 19] </ref>. However, optimizing individual tasks in a processing pipeline comprising many such tasks does not guarantee the optimal performance of the whole operation.
Reference: [20] <author> J. A. Webb, </author> <title> Steps Toward Architecture-independent Image Processing, </title> <journal> IEEE Computer, </journal> <volume> Vol. 25, No. 2, </volume> <month> Feb. </month> <year> 1992, </year> <pages> pp. 21-31. </pages>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures <ref> [20, 22] </ref>. Many CVIP problems are inherently parallel because, often times, similar or identical operations are applied to all pixels in an image without conditional branching. Program structures, by a large extent, comprise nested loops over image data, and a number of loop parallelization schemes are potentially applicable [25].
Reference: [21] <author> J. A. Webb, </author> <title> High Performance Computing in Image Processing and Computer Vision, </title> <booktitle> Proceedings of 12th IAPR Int. Conf. on Pattern Recognition, </booktitle> <address> Jerusalem, Israel, </address> <month> Oct. </month> <year> 1994, </year> <pages> pp. 218-222. </pages>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [22] <author> C. C. Weems, et al., </author> <title> Parallel Processing in the DARPA Strategic Computing Vision Program, </title> <journal> IEEE Expert, </journal> <volume> Vol. 6, No. 5, </volume> <month> Oct. </month> <year> 1991, </year> <pages> pp. 23-38. </pages>
Reference-contexts: 1 Introduction Computer Vision and Image Processing (CVIP) algorithms possess characteristics which are ideally suited for implementation on a variety of parallel architectures <ref> [20, 22] </ref>. Many CVIP problems are inherently parallel because, often times, similar or identical operations are applied to all pixels in an image without conditional branching. Program structures, by a large extent, comprise nested loops over image data, and a number of loop parallelization schemes are potentially applicable [25]. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule.
Reference: [23] <author> F. Weil, L. H. Jamieson, and E. J. Delp, </author> <title> Dynamic Intelligent Scheduling and Control of Reconfigurable Parallel Architectures for Computer Vision/Image Processing, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 13, </volume> <year> 1991, </year> <pages> pp. 273-285. </pages>
Reference-contexts: Although this research provided a useful design framework for parallel image processing, it was not applicable to message-passing architectures and did not consider the communication overheads for redistributing image data. DISC <ref> [23] </ref> developed at Purdue University is a dynamic scheduler aimed at generating data partitioning and processor assignment schemes at the run time for operations involving many data-dependent tasks and conditional branches. <p> However, it requires runtime task migration and/or processor reconfiguration as it is done in DISC <ref> [23] </ref>. Runtime scheduling thus suffers a high overhead in a message-passing architecture. It is still an open problem how to use runtime scheduling for better load balancing without undue overhead. <p> This is to ensure that interprocessor communication can be easily established for this partitioning pattern. Many researchers have proposed parallel versions of various popular CVIP algorithms, assuming different data partitioning schemes and processor architectures (e.g., <ref> [9, 10, 13, 14, 15, 18, 21, 22, 23] </ref>). The difficulty here, as mentioned before, is that optimizing individual local operations does not guarantee a globally optimal schedule. <p> Our scheduler, to be used at the compilation time, is well suited for tasks involving data-independent operations. Data-dependent operations pose more serious problems, as their behaviors cannot be predicted at the compilation time. The difficulty of mapping data-dependent operations was first recognized in the DISC project <ref> [23] </ref>. And some worst-case, best-case, or average-case analysis can be employed, but the optimality of such analyses cannot be guaranteed. Under some special conditions, CVIP algorithms involving data dependent operations can still be scheduled at the compilation time, we discuss these cases in Section 5.
Reference: [24] <author> S. Wharton and J. Newcomer, </author> <title> Land Image Data Processing Requirements for the EOS Era, </title> <journal> IEEE Trans. Geoscience and Remote Sensing, </journal> <volume> Vol. 27, No. 2, </volume> <month> Mar. </month> <year> 1989, </year> <pages> pp. 236-242. </pages>
Reference-contexts: For example, many robotic servoing tasks using visual feedback [16] require image sequences processed at the video frame rate (30 frames per second). In remote-sensing applications, the enormous size of the image databases (typically in the gigabyte or terabyte range) <ref> [24] </ref> makes image processing a major limiting factor in digesting and cataloging image data. So far, parallel image processing research has focused on implementing individual algorithms on a particular parallel architecture [1, 3, 11, 19].
Reference: [25] <author> M. J. Wolfe, </author> <title> Optimizing Supercompilers for Supercomputers, </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1989. </year>
Reference-contexts: Many CVIP problems are inherently parallel because, often times, similar or identical operations are applied to all pixels in an image without conditional branching. Program structures, by a large extent, comprise nested loops over image data, and a number of loop parallelization schemes are potentially applicable <ref> [25] </ref>. Furthermore, CVIP problems can be computationally intensive, and hence, can benefit from the parallel computation support. For example, many robotic servoing tasks using visual feedback [16] require image sequences processed at the video frame rate (30 frames per second).
Reference: [26] <author> R. Wolski and J. Feo, </author> <title> Program Partitioning for NUMA Multiprocessor Computer Systems, </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 19, No. 3, </volume> <month> Nov. </month> <year> 1993, </year> <pages> pp. 203-218. </pages>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations.
Reference: [27] <author> T. Yang and A. Gerasoulis, </author> <title> PYRROS: Static Task Scheduling and Code Generation for Message Passing Multiprocessors, </title> <booktitle> Proceedings of 6th ACM Int. Conf. on Supercomputing, </booktitle> <address> Wash-ington D.C., </address> <month> Jul. </month> <year> 1992, </year> <pages> pp. 428-437. 24 </pages>
Reference-contexts: Our ultimate goal is to use the scheduling results for generating parallel codes for a processing algorithm automatically by employing a library of existing parallel routines. Related Research Work There have been a number of projects aimed at developing software environments, including compilers and schedulers <ref> [6, 13, 14, 21, 26, 27] </ref>, for parallel processing. However, automated compilation without taking advantage of specific domain knowledge is still quite difficult. Several researchers presented preliminary designs of sched-ulers for parallel CVIP operations.
References-found: 27

