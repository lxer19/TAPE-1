URL: http://vlsicad.cs.ucla.edu/~abk/papers/conference/c42.ps
Refering-URL: http://vlsicad.cs.ucla.edu/~abk/publications.html
Root-URL: http://www.cs.ucla.edu
Title: Training minimal artificial neural network architectures for subsoil object detection  
Author: Kenneth D. Boese, Donald E. Franklin and Andrew B. Kahng 
Keyword: neural networks, simulated annealing, subsoil detection, separated-aperture radar  
Note: U. S. Army Night Vision and Electronic Sensors  
Address: Los Angeles, CA 90024-1596  Ft. Belvoir, VA 22060  
Affiliation: UCLA Dept. of Computer Science,  Directorate,  
Abstract: We cast the training of minimal artificial neural network architectures as a problem of global optimization, and study the simulated annealing (SA) global optimization heuristic under a "best-so-far" model. Our testbed consists of separated-aperture radar data for subsoil mine detection. In previous analyses, we have found that the traditional SA "cooling" paradigm can be suboptimal for small instances of combinatorial global optimizations. Here, we demonstrate that traditional cooling is also suboptimal for training minimal neural networks for mine detection. Related issues include (i) how to find minimal network architectures; (ii) considering tradeoffs between minimality and trainability; (iii) the question of whether multi-start/parallel implementations of SA can be superior to a single long SA run; and (iv) adaptive annealing strategies based on the best-so-far objective. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. H. L. Aarts and J. Korst, </author> <title> Simulated Annealing and Boltzmann Machines: a Stochastic Approach to Combinatorial Optimization and Neural Computing, </title> <publisher> Wiley, </publisher> <year> 1989. </year>
Reference-contexts: In this paper, we study the simulated annealing (SA) heuristic [11][20]. For neural network learning, SA has been used in "Boltzmann learning" for Boltzmann machines <ref> [1] </ref> and in the mean-field sense to optimize other connectionist architectures [7][28]. While simulated annealing is slower than other training algorithms, it outperforms greedy methods and remains popular when minimum error in the trained network is critical.
Reference: [2] <author> E. H. L. Aarts and P. J. M. van Laarhoven, </author> <title> "A New Polynomial-Time Cooling Schedule", </title> <booktitle> in Proc. IEEE Intl. Conf. on Computer-Aided Design, </booktitle> <month> Nov. </month> <year> 1985, </year> <pages> pp. 206-8. </pages>
Reference: [3] <author> B. C. Arnold, N. Balakrishnan and H. N. Nagaraja, </author> <title> A First Course in Order Statistics, </title> <publisher> Wiley, </publisher> <year> 1992, </year> <note> p. 12. </note>
Reference-contexts: We also assume that the f (s i ) are normally distributed with mean and variance 2 (f (s i )). An elementary result of order statistics <ref> [3] </ref> can then be used to compute the median of the distribution of min M i=k f (s i ) as follows. Let F be the cumulative distribution of f (s i ) and let G be the cumulative distribution of min M i=k f (s i ).
Reference: [4] <author> M. R. Azimi-Sadjadi, D. E. Poole, S. Sheedvash, K. D. Sherbondy and S. A. Stricker, </author> <title> "Detection and Classification of Buried Dielectric Anomalies Using a Separated Aperture Sensor and a Neural Network Discriminator", </title> <booktitle> IEEE Trans. on Instrumentation and Measurement 41(1) (1992), </booktitle> <pages> pp. 137-143. </pages>
Reference-contexts: Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in <ref> [4] </ref> [5] [17] [30].
Reference: [5] <author> M. R. Azimi-Sadjadi and S. A. Stricker, </author> <title> "Detection and Classification of Buried Dielectric Anomalies Using Neural Networks-Further Results", </title> <booktitle> IEEE Trans. on Instrumentation and Measurement 43(1) (1994), </booktitle> <pages> pp. 34-39. </pages>
Reference-contexts: Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in [4] <ref> [5] </ref> [17] [30].
Reference: [6] <author> E. Barnard, </author> <title> "Optimization for Training Neural Nets", </title> <booktitle> IEEE Trans. on Neural Networks 3(2) (1992), </booktitle> <pages> pp. 232-240. </pages>
Reference-contexts: Network training heuristics reflect the global optimization literature (see [15] for a survey); common approaches include gradient-based methods (back-propagation [31], or conjugate gradient with restarts [29]), stochastic hill-climbing, and other methods such as variable-metric [13] and Kalman filter-based <ref> [6] </ref> techniques. In this paper, we study the simulated annealing (SA) heuristic [11][20]. For neural network learning, SA has been used in "Boltzmann learning" for Boltzmann machines [1] and in the mean-field sense to optimize other connectionist architectures [7][28].
Reference: [7] <author> G. L. Bilbro, W. E. Snyder, S. J. Garnier and J. W. Gault, </author> <title> "Mean Field Annealing: A Formalism for Constructing GNC-Like Algorithms", </title> <booktitle> IEEE Trans. on Neural Networks 3(1) (1992), </booktitle> <pages> pp. 131-138. </pages>
Reference: [8] <author> K. D. Boese and A. B. Kahng, </author> <title> "Simulated Annealing of Neural Networks: the `Cooling' Strategy Reconsidered", </title> <booktitle> in Proc. IEEE Int. Symp. on Circuits and Systems, </booktitle> <month> May </month> <year> 1993, </year> <pages> pp. 2572-2575. </pages>
Reference-contexts: Searching over only two parameters to find a locally optimal exponential schedule is practical for our small object-detection networks. 3 This section extends our previous work <ref> [8] </ref> using linear annealing schedules on the same data. Tables 1 and 2 report the BSF SSE of training runs with 16,384 steps and 32,768 steps, respectively.
Reference: [9] <author> K. D. Boese, A. B. Kahng and C.-W. A. Tsao, </author> <title> "Best-So-Far vs. Where-You-Are: New Perspectives on Simulated Annealing for CAD", </title> <booktitle> in Proc. European Design Automation Conf., </booktitle> <month> Sept. </month> <year> 1993, </year> <pages> pp. 78-83. </pages>
Reference: [10] <author> K. D. Boese and A. B. Kahng, </author> <title> "Best-So-Far vs. Where-You-Are: Implications for Optimal Finite-Time Annealing", </title> <journal> Systems and Control Letters, </journal> <volume> 22(1), </volume> <month> Jan. </month> <year> 1994, </year> <pages> pp. 71-8. </pages>
Reference: [11] <author> V. Cerny, </author> <title> "Thermodynamical Approach to the Traveling Salesman Problem: an Efficient Simulation Algorithm", </title> <editor> J. </editor> <booktitle> Optimization Theory and Applications 45(1) (1985), </booktitle> <pages> pp. 41-51. </pages>
Reference: [12] <author> P. Courrieu, </author> <title> "A Convergent Generator of Neural Networks," </title> <booktitle> Neural Networks 6 (6) (1993), </booktitle> <pages> pp. 835-44. </pages>
Reference: [13] <author> J. E. Dennis, Jr. and R. B. Schnabel, </author> <title> Numerical Methods for Unconstrained Optimization and Nonlinear Equations, </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall, </publisher> <year> 1983. </year>
Reference-contexts: Network training heuristics reflect the global optimization literature (see [15] for a survey); common approaches include gradient-based methods (back-propagation [31], or conjugate gradient with restarts [29]), stochastic hill-climbing, and other methods such as variable-metric <ref> [13] </ref> and Kalman filter-based [6] techniques. In this paper, we study the simulated annealing (SA) heuristic [11][20]. For neural network learning, SA has been used in "Boltzmann learning" for Boltzmann machines [1] and in the mean-field sense to optimize other connectionist architectures [7][28].
Reference: [14] <author> A. G. Ferreira and J. Zerovnik, </author> <title> "Bounding the Probability of Success of Stochastic Methods for Global Optimization", </title> <booktitle> Computers and Mathematics with Applications 25(10/11) (1993), </booktitle> <pages> pp. 1-8. </pages>
Reference-contexts: For instance, Ferreira and Zerovnik <ref> [14] </ref> showed that as M approaches infinity, an equivalent number of greedy descents will be preferred to a single annealing run. Lasserre et al. [24] showed that for several small problem instances, SA is frequently outperformed by multi-start greed, random search, and "steepest ascent descent".
Reference: [15] <author> R. Hecht-Neilsen, </author> <title> Neurocomputing, </title> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1990. </year>
Reference-contexts: This process is equivalent to global optimization of a multimodal error function of the connection weights, and is both theoretically intractable [32] and difficult in practice. Network training heuristics reflect the global optimization literature (see <ref> [15] </ref> for a survey); common approaches include gradient-based methods (back-propagation [31], or conjugate gradient with restarts [29]), stochastic hill-climbing, and other methods such as variable-metric [13] and Kalman filter-based [6] techniques. In this paper, we study the simulated annealing (SA) heuristic [11][20].
Reference: [16] <author> A. B. Kahng, </author> <title> "Exploiting Fractalness in Error Surfaces: New Methods for Neural Network Learning", </title> <booktitle> Proc. IEEE Intl. Symp. on Circuits and Systems, </booktitle> <address> San Diego, </address> <year> 1992, </year> <pages> pp. 41-44. </pages>
Reference: [17] <author> A. B. </author> <title> Kahng,"Random Structure of Error Surfaces: New Stochastic Learning Methods", invited paper, </title> <booktitle> Proc. SPIE Conf. on Neural Networks and Optimization, </booktitle> <address> Orlando, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in [4] [5] <ref> [17] </ref> [30]. The 29 training cases are printed in [17]. 3 Our intuition suggests that the expected BSF SSE is a convex function of T 1 and T M , which would imply that any local minimum should also be a global minimum. 4 Each run of 16,384 steps requires approximately <p> Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in [4] [5] <ref> [17] </ref> [30]. The 29 training cases are printed in [17]. 3 Our intuition suggests that the expected BSF SSE is a convex function of T 1 and T M , which would imply that any local minimum should also be a global minimum. 4 Each run of 16,384 steps requires approximately ten seconds of CPU on an HP Apollo 735. <p> Sample size = 400 runs for each architecture. 7 If i is the number of inputs and h is the number of hidden nodes, then the number of weights equals (i + 2) fl h + 1. (Recall that we include threshold weights.) 8 Kahng in <ref> [17] </ref> ran comparisons on even larger architectures that confirm that very large architectures do train more slowly. 4 CONTINUING RESEARCH 4.1 Ideas I: Multi-Start Annealing A number of works on simulated annealing have asked whether a single long annealing run is preferred to a number of shorter annealing runs or greedy
Reference: [18] <author> D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Schevon, </author> <title> "Optimization by Simulated Annealing: an Experimental Evaluation; Part I, Graph Partitioning", </title> <booktitle> Operations Research 27 (6) (1989), </booktitle> <pages> pp. 865-892. </pages>
Reference: [19] <author> D. S. Johnson, C. R. Aragon, L. A. McGeoch, and C. Schevon, </author> <title> "Optimization by Simulated Annealing: an Experimental Evaluation; Part II, Graph Coloring and Number Partitioning", </title> <booktitle> Operations Research 39 (3) (1991), </booktitle> <pages> pp. 378-406. </pages>
Reference: [20] <author> S. Kirkpatrick, Jr. C. D. Gelatt, and M. Vecchi, </author> <title> "Optimization by Simulated Annealing", </title> <booktitle> Science 220(4598) (1983), </booktitle> <pages> pp. 671-680. </pages>
Reference-contexts: An exponential schedule has the form T i+1 = ff fl T i (2) for all i and a constant ff, ff &gt; 0. Exponential schedules were used by Kirkpatrick et al. in their original SA paper <ref> [20] </ref> and have been studied extensively by Johnson et al. [18,19]. For any fixed run length M , the space of exponential schedules is defined by exactly two parameters (e.g., T 1 and T M ).
Reference: [21] <author> P. J. M. van Laarhoven, </author> <title> Theoretical and Computational Aspects of Simulated Annealing, </title> <publisher> Amsterdam: Stichting Mathematisch Centrum, </publisher> <year> 1988. </year>
Reference-contexts: In other words, SA is "optimal" in the limit of infinite time <ref> [21] </ref>. 1 Note that S can be assumed finite even for neural network training if we impose limits on the size and precision of each connection weight in the network. 2.2 Annealing: Theory Vs. <p> The BSF-optimal schedule is derived from our experimental results in Table 2, which estimate that a constant schedule with T i = :009 is nearly optimal within the class of exponential schedules. We have designed a "traditional" exponential schedule based on the traditional wisdom (see e.g., <ref> [21] </ref>) that T 1 should be very high, so that initially between 90 and 95 percent of all candidate moves s 0 are accepted.
Reference: [22] <author> J. K.-C. Lam, </author> <title> "An Efficient Simulated Annealing Schedule", </title> <type> Ph.D. Thesis, </type> <institution> Yale University, </institution> <month> Dec. </month> <year> 1988. </year>
Reference: [23] <author> J. Lam and J.-M. Delosme, </author> <title> "Performance of a New Annealing Schedule", </title> <booktitle> in Proc. ACM/IEEE Design Automation Conf., </booktitle> <month> June </month> <year> 1988, </year> <pages> pp. 306-11. </pages>
Reference: [24] <author> J. B. Lasserre, P. P. Varaiya, and J. Walrand, </author> <title> "Simulated Annealing, Random Search, Multistart or SAD?", </title> <journal> Systems and Control Letters 8, </journal> <month> 297-301 </month> <year> (1987). </year>
Reference-contexts: For instance, Ferreira and Zerovnik [14] showed that as M approaches infinity, an equivalent number of greedy descents will be preferred to a single annealing run. Lasserre et al. <ref> [24] </ref> showed that for several small problem instances, SA is frequently outperformed by multi-start greed, random search, and "steepest ascent descent". Our preliminary experiments with multi-start greed for network training had very little success.
Reference: [25] <author> M. A. Lehr and B. Widrow, </author> <title> "Adaptive Multisource Decision-Making: Detecting Land Mines with Neural Networks Using Separated Aperture Sensor Data Collected at Fort Belvoir", </title> <type> report BRDE-ISL/TR-1/1, </type> <month> April </month> <year> 1992, </year> <institution> Stanford University Department of Electrical Engineering. </institution>
Reference: [26] <author> R. M. Loynes, </author> <title> "Extreme Values in Uniformly Mixing Stationary Stochastic Processes", </title> <journal> Annals Math. Stats. </journal> <volume> 36 (1965), </volume> <pages> pp. 993-999. </pages>
Reference-contexts: A result of Loynes <ref> [26] </ref> shows that for computing min M i=k f (s i ), we can assume that each of the f (s i ) are independent if M k is sufficiently large.
Reference: [27] <author> M. Lundy and A. Mees, </author> <title> "Convergence of an Annealing Algorithm", Math. </title> <booktitle> Programming 34 (1986), </booktitle> <pages> pp. 111-124. </pages>
Reference-contexts: SA enjoys certain theoretical attractions: using Markov chain arguments and basic aspects of Gibbs-Boltzmann statistics, one can show that for any finite S, SA will converge to a globally optimal solution given infinitely large M and a temperature schedule that converges to 0 sufficiently slowly <ref> [27] </ref>, i.e., P r (s M 2 R) ! 1 as M ! 1 (1) where R S denotes the set of all globally optimal solutions.
Reference: [28] <author> C. Peterson and J. R. Anderson, </author> <title> "A Mean Field Theory Learning Algorithm for Neural Networks", </title> <booktitle> Complex Systems 1 (1987), </booktitle> <pages> pp. 995-1019. </pages>
Reference: [29] <author> M. J. D. Powell, </author> <title> "Restart Procedures for the Conjugate Gradient Method", </title> <booktitle> Mathematical Programming 12 (1977), </booktitle> <pages> pp. 241-254. </pages>
Reference-contexts: Network training heuristics reflect the global optimization literature (see [15] for a survey); common approaches include gradient-based methods (back-propagation [31], or conjugate gradient with restarts <ref> [29] </ref>), stochastic hill-climbing, and other methods such as variable-metric [13] and Kalman filter-based [6] techniques. In this paper, we study the simulated annealing (SA) heuristic [11][20].
Reference: [30] <author> L. S. Riggs and C. A. Amazeen, </author> <title> "Research Efforts with the Waveguide Beyond Cutoff or Separated Aperture Dielectric Anomaly Detection Scheme", </title> <type> report, </type> <institution> U. S. Army Belvoir RD&E Center, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Schedules outside the range [0:005; 0:012] for a majority of steps are significantly worse than schedules with temperatures exclusively within this range. 2 The ground sensor data are obtained via the wave guide beyond cutoff (WGBCO), or separated aperture, microwave technique <ref> [30] </ref> and were provided by the U.S. Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in [4] [5] [17] [30]. <p> temperatures exclusively within this range. 2 The ground sensor data are obtained via the wave guide beyond cutoff (WGBCO), or separated aperture, microwave technique <ref> [30] </ref> and were provided by the U.S. Army Belvoir RD&E Center. The sensor apparatus and provenance of the data set are described in [4] [5] [17] [30].
Reference: [31] <editor> D. E. Rumelhart, J. L. McClelland et al., </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition , Cambridge, </booktitle> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: This process is equivalent to global optimization of a multimodal error function of the connection weights, and is both theoretically intractable [32] and difficult in practice. Network training heuristics reflect the global optimization literature (see [15] for a survey); common approaches include gradient-based methods (back-propagation <ref> [31] </ref>, or conjugate gradient with restarts [29]), stochastic hill-climbing, and other methods such as variable-metric [13] and Kalman filter-based [6] techniques. In this paper, we study the simulated annealing (SA) heuristic [11][20].
Reference: [32] <author> A. Torn and A. Zilinskas, </author> <booktitle> Global Optimization , Lecture Notes in Computer Science 350, </booktitle> <editor> G. Goos and J. Hartmanis, eds., </editor> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: This process is equivalent to global optimization of a multimodal error function of the connection weights, and is both theoretically intractable <ref> [32] </ref> and difficult in practice. Network training heuristics reflect the global optimization literature (see [15] for a survey); common approaches include gradient-based methods (back-propagation [31], or conjugate gradient with restarts [29]), stochastic hill-climbing, and other methods such as variable-metric [13] and Kalman filter-based [6] techniques.
Reference: [33] <author> B.-T. Zhang and H. Muhlenbein, </author> <title> "Genetic programming of minimal neural nets using Occam's razor", </title> <booktitle> in Proc. ICGA-93: Fifth Intl. Conf. on Genetic Algorithms, </booktitle> <month> July </month> <year> 1993, </year> <pages> pp. 342-9. </pages>
References-found: 33

