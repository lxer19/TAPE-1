URL: http://american.cs.ucdavis.edu/publications/Micro23.90b.ps
Refering-URL: http://american.cs.ucdavis.edu/ArchLabPersonnel/Farrens/PubList.html
Root-URL: http://www.cs.ucdavis.edu
Email: (arp@tosca.colorado.edu)  
Title: d d AN EVALUATION OF FUNCTIONAL UNIT LENGTHS FOR SINGLE-CHIP PROCESSORS  
Author: Matthew K. Farrens Andrew R. Pleszkun 
Address: Davis, CA 95616  (farrens@american.ucdavis.edu) Boulder, CO 80309-0425  
Affiliation: Computer Science Division Department of Electrical and University of California, Davis Computer Engineering  University of Colorado-Boulder  
Abstract: When designing a pipelined single-chip processor (SCP) with pipelined functional units of varying length, the processor issue logic must deal with scheduling of the result bus. In order to prevent serious performance degradation due to result bus conflicts, some pipeline scheduling techniques developed in the 1970's may need to be incorporated into the issue logic. Since this is a nontrivial complication of the issue logic, a set of simulations were performed in order to evaluate the effectiveness of the combination of multiple length functional units and scheduling techniques. Analysis of the simulation results indicates that providing relatively short multiple length functional units is not worthwhile. Multiple length functional unit configurations employing result bus scheduling do perform slightly better than uniform length configurations, but the difference is often less than 1%. Thus, the SCP designer should not waste valuable time improving the performance of each functional unit, but rather should produce a good design for the most complicated unit and design all other units to match it. 
Abstract-found: 1
Intro-found: 1
Reference: [Farr89] <author> M. K. Farrens, </author> <title> The Design and Analysis of a High Performance Single Chip Processor, </title> <type> Ph.D. Thesis, </type> <institution> Department of Electrical and Computer Engineering,, Madison, Wisconsin, </institution> <month> (August </month> <year> 1989). </year>
Reference-contexts: The Simulator The simulator used in this study was a modified version of the PIPE simulator, which was written to facilitate the study of the PIPE processor <ref> [Farr89] </ref>.
Reference: [FaPl89] <author> M. K. Farrens and A. R. Pleszkun, </author> <title> ``Improving the Performance of Small On-Chip Instruction Caches'', </title> <booktitle> Proceedings of the Sixteenth Annual International Symposium on Computer Architecture, </booktitle> <volume> vol. 17, no. </volume> <month> 3 (June </month> <year> 1989), </year> <pages> pp. 234-241. </pages>
Reference-contexts: [GHLP85] and employs a load/store register-register architecture similar to the CRAY and CDC architectures [Russ78, Thor70] with an elemental (or reduced) instruction set (designed both for ease of decode and to simplify the hardware issue logic), a 5-stage pipeline, a small on-chip instruction cache and sophisticated instruction fetch support logic <ref> [FaPl89] </ref>, limited subroutine call support, a branch mechanism that allows the compiler to specify the number of delay slots after a branch, and architectural input and output queues at the processor/memory interface.
Reference: [GHLP85] <author> J. R. Goodman, J. T. Hsieh, K. Liou, A. R. Pleszkun, P. B. Schechter and H. C. Young, </author> <title> ``PIPE: a VLSI Decoupled Architecture'', </title> <booktitle> Proceedings of the Twelveth Annual International Symposium on Computer Architecture(June 1985), </booktitle> <pages> pp. 20-27. </pages>
Reference-contexts: The PIPE processor, a single chip processor designed and built at the University of Wisconsin, is an outgrowth of the PIPE project <ref> [GHLP85] </ref> and employs a load/store register-register architecture similar to the CRAY and CDC architectures [Russ78, Thor70] with an elemental (or reduced) instruction set (designed both for ease of decode and to simplify the hardware issue logic), a 5-stage pipeline, a small on-chip instruction cache and sophisticated instruction fetch support logic [FaPl89],
Reference: [HsPG84] <author> J. T. Hsieh, A. R. Pleszkun and J. R. Goodman, </author> <title> ``Performance Evaluation of the PIPE Computer Architecture'', </title> <institution> Computer Science Department Technical Report #566, University of Wisconsin-Madison , Madison, Wisconsin (November 1984). d d </institution>
Reference-contexts: As can be seen by comparing Figure 1 to Figure 2, the use of pipeline scheduling will improve the perfor mance of this code sequence by over 16%. Based on the results of the small section of code presented above and the results presented in <ref> [HsPG84] </ref>, it would appear that performing some sort of result bus scheduling in an SCP could be a valuable performance enhancing technique.
Reference: [McMa84] <author> F. H. McMahon, </author> <title> LLNL FORTRAN KERNELS: </title> <type> MFLOPS, </type> <institution> Lawrence Livermore Laboratories, Livermore, California, </institution> <month> (March </month> <year> 1984). </year>
Reference-contexts: This is an example of the less obvious advantages of using I/O queues. 3.3. The Benchmark Program The benchmark programs selected were the first 14 Lawrence Livermore loops as defined in <ref> [McMa84] </ref>. The loops were first compiled by the SUN4 optimizing compiler, in order to get a feel for the kinds of optimizations a compiler could perform. The loops were then completely hand-written using the output of the SUN compiler as a guide.
Reference: [PaDa76] <author> J. H. Patel and E. S. Davidson, </author> <title> ``Improving the Throughput of a Pipeline by Insertion of Delays'', </title> <journal> Computer Architecture News (ACM-SIGARCH), </journal> <volume> vol. 4, no. </volume> <month> 4 (January </month> <year> 1976), </year> <pages> pp. 159-164. </pages>
Reference-contexts: By preventing Instruction A from issuing, the instructions behind it are also prevented from beginning. Overall throughput may be reduced if some of these instructions do not have the same result bus conflict d d as Instruction A. Special pipeline scheduling techniques <ref> [PaDa76] </ref> exist that can be used to reduce the impact of these result bus conflicts and which are much less compli cated than supporting out of order issue and execution [Toma67].
Reference: [Patt85] <author> D. A. Patterson, </author> <title> ``Reduced Instruction Set Computers'', </title> <journal> Communications of the ACM, </journal> <volume> vol. 28, no. </volume> <month> 1 (January </month> <year> 1985), </year> <pages> pp. 8-21. </pages>
Reference-contexts: It is important to remember that a solution that takes fewer clock cycles but requires significantly more complicated hardware may actually take more real time to execute than a simpler scheme with less complicated hardware. (This is, in fact, the basic argument behind reduced instruction set processors <ref> [Patt85] </ref>.) 3.1. The Simulator The simulator used in this study was a modified version of the PIPE simulator, which was written to facilitate the study of the PIPE processor [Farr89].
Reference: [Russ78] <author> R. M. Russell, </author> <title> ``The CRAY-1 Computer System'', </title> <journal> Communications of the ACM, </journal> <volume> vol. 21, no. </volume> <month> 1 (January </month> <year> 1978), </year> <pages> pp. 63-72. </pages>
Reference-contexts: While on the surface this approach seems logical, closer inspection reveals certain problems with supporting multiple length functional units. When different length functional units are implemented with a CRAY-like issue strategy <ref> [Russ78] </ref>, the issue logic must ensure that an instruction waiting to issue will not require the use of the result bus during a clock cycle when the result bus will be used by an already issued instruction. <p> The PIPE processor, a single chip processor designed and built at the University of Wisconsin, is an outgrowth of the PIPE project [GHLP85] and employs a load/store register-register architecture similar to the CRAY and CDC architectures <ref> [Russ78, Thor70] </ref> with an elemental (or reduced) instruction set (designed both for ease of decode and to simplify the hardware issue logic), a 5-stage pipeline, a small on-chip instruction cache and sophisticated instruction fetch support logic [FaPl89], limited subroutine call support, a branch mechanism that allows the compiler to specify the
Reference: [Smit88] <author> J. E. Smith, </author> <title> ``Characterizing Computer Performance with a Single Number'', </title> <journal> Communications of the ACM, </journal> <volume> vol. 31, no. </volume> <month> 10 (October </month> <year> 1988), </year> <pages> pp. 1202-1206. </pages>
Reference-contexts: This metric was chosen because the best way to evaluate the performance of various internal processor configurations is to monitor the amount of time the processor takes to complete a test program with a given configuration <ref> [Smit88] </ref>. It is important to remember that a solution that takes fewer clock cycles but requires significantly more complicated hardware may actually take more real time to execute than a simpler scheme with less complicated hardware. (This is, in fact, the basic argument behind reduced instruction set processors [Patt85].) 3.1.
Reference: [Thor70] <author> J. E. Thorton, </author> <title> Design of a Computer The Control Data 6600, Scott, </title> <publisher> Foreman and Co, </publisher> <address> Glenview, Illinois, </address> <year> (1970). </year>
Reference-contexts: The PIPE processor, a single chip processor designed and built at the University of Wisconsin, is an outgrowth of the PIPE project [GHLP85] and employs a load/store register-register architecture similar to the CRAY and CDC architectures <ref> [Russ78, Thor70] </ref> with an elemental (or reduced) instruction set (designed both for ease of decode and to simplify the hardware issue logic), a 5-stage pipeline, a small on-chip instruction cache and sophisticated instruction fetch support logic [FaPl89], limited subroutine call support, a branch mechanism that allows the compiler to specify the
Reference: [Toma67] <author> R. M. Tomasulo, </author> <title> ``An Efficient Algorithm for Exploiting Multiple Arithmetic Units'', </title> <journal> IBM Journal, </journal> <volume> vol. </volume> <month> 11 (January </month> <year> 1967), </year> <pages> pp. 25-33. </pages>
Reference-contexts: Special pipeline scheduling techniques [PaDa76] exist that can be used to reduce the impact of these result bus conflicts and which are much less compli cated than supporting out of order issue and execution <ref> [Toma67] </ref>.
Reference: [YoGo84] <author> H. C. Young and J. R. Goodman, </author> <title> ``A Simulation Study of Architectural Data Queues and Prepare-to-Branch Instruction'', </title> <booktitle> Proceedings of the IEEE INT Conference on Computer Design: VLSI in Computers, Port Chester, </booktitle> <address> New York (October 1984), </address> <pages> pp. 544-549. </pages>
Reference-contexts: Making the queues visible to the programmer also makes it possible to schedule memory requests such that the impact of a slow external memory on processor performance is significantly reduced <ref> [YoGo84] </ref>. The use of I/O queues also impact the design of a processor in ways that are more subtle and less obvious. Throughout the rest of this paper we will attempt to point out some of these unexpected advantages.
References-found: 12

