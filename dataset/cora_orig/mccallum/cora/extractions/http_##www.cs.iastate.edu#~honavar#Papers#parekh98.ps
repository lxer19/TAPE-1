URL: http://www.cs.iastate.edu/~honavar/Papers/parekh98.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/homepage.html
Root-URL: http://www.cs.iastate.edu
Email: rpare@allstate.com  VASANT HONAVAR honavar@cs.iastate.edu  
Title: Learning DFA from Simple Examples  
Author: RAJESH PAREKH 
Address: 321 Middlefield Road, Menlo Park CA 94025, USA  Ames IA 50011, USA  
Affiliation: Allstate Research and Planning Center,  Department of Computer Science, Iowa State University,  
Abstract: Efficient learning of DFA is a challenging research problem in grammatical inference. It is known that both exact and approximate (in the PAC sense) identifiability of DFA is hard. Pitt, in his seminal paper posed the following open research problem: Are DFA PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? [25]. We demonstrate that the class of simple DFA (i.e., DFA whose canonical representations have logarithmic Kolmogorov complexity) is efficiently PAC learnable under the Solomonoff Levin universal distribution. We prove that if the examples are sampled at random according to the universal distribution by a teacher that is knowledgeable about the target concept, the entire class of DFA is efficiently PAC learnable under the universal distribution. Thus, we show that DFA are efficiently learnable under the PACS model [6]. Further, we prove that any concept that is learnable under Gold's model for learning from characteristic samples, Goldman and Mathias' polynomial teachability model, and the model for learning from example based queries is also learnable under the PACS model. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Angluin. </author> <title> A note on the number of queries needed to identify regular languages. </title> <journal> Information and Control, </journal> <volume> 51:7687, </volume> <year> 1981. </year>
Reference-contexts: Angluin showed that given a live-complete set of examples (that contains a representative string for each live state of the target DFA) and a knowledgeable teacher to answer membership queries it is possible to exactly learn the target DFA <ref> [1] </ref>. In a later paper, Angluin relaxed the requirement of a live-complete set and has designed a polynomial time inference algorithm using both membership and equivalence queries [2].
Reference: 2. <author> D. Angluin. </author> <title> Learning regular sets from queries and counterexamples. Information and Computation, </title> <address> 75:87106, </address> <year> 1987. </year>
Reference-contexts: In a later paper, Angluin relaxed the requirement of a live-complete set and has designed a polynomial time inference algorithm using both membership and equivalence queries <ref> [2] </ref>. The regular positive and negative inference (RPNI) algorithm is a framework for identifying in polynomial time, a DFA consistent with a given sample S [21]. <p> We define PAC learning of DFA more formally in section 2. Angluin's L fl algorithm <ref> [2] </ref> that learns DFA in polynomial time using membership and equivalence queries can be recast under the PAC framework to learn by posing membership queries alone. Even approximate learnability of DFA was proven to be a hard problem.
Reference: 3. <author> D. Angluin. </author> <title> Queries and concept learning. </title> <booktitle> Machine Learning, </booktitle> <address> 2(4):319342, </address> <year> 1988. </year>
Reference-contexts: Further, we demonstrate how the model of learning from simple examples naturally extends the model of learning concepts from representative examples [11], the polynomial teachability model [12], and the model of learning from example based queries <ref> [3] </ref> to a probabilistic framework. This paper is organized as follows: Section 2 briefly introduces some concepts used in the results described in this paper. This includes a discussion of the PAC learning model, Kolmogorov complexity, and the universal distribution. Section 3 reviews the RPNI algorithm for learning DFA. <p> the PACS Model to other Learning Models In this section we study the relationship of the PACS model to other prominent learning models such as Gold's model of polynomial identifiability from characteristic samples [11], Goldman and Mathias' polynomial teachability model [12], and the model of learning from example based queries <ref> [3] </ref>. We explain how the PACS learning model naturally extends these other models to a probabilistic framework. In the discussion that follows we will let X be the instance space, C be the concept class and R be the set of representations of the concepts in C. 16 6.1. <p> Learning from Example Based Queries A variety of concept classes are known to be learnable in deterministic polynomial time when the learner is allowed access to a teacher (or an oracle) that answers example based queries <ref> [3] </ref>.
Reference: 4. <author> J. Castro and D. Guijarro. </author> <title> Query, pacs and simple-pac learning. </title> <type> Technical Report LSI-98-2-R, </type> <institution> Universitat Polyt ectica de Catalunya, Spain, </institution> <year> 1998. </year>
Reference-contexts: Recently Castro and Guijarro have independently shown that any concept class that is learnable using membership and equivalence queries is also learnable under the PACS model <ref> [4] </ref>. Further, they have intuitively demonstrated how this result can be extended to all example based queries. Theorem 7 above is an alternate proof of the relationship between query learning and PACS learning. <p> Recently it has been shown that if a concept class is learnable under the PACS model using an algorithm that satisfies certain properties then simple concepts of that concept class are learnable under the simple PAC learning model <ref> [4] </ref>
Reference: 5. <author> N. Chomsky. </author> <title> Three models for the description of language. </title> <address> PGIT, 2(3):113124, </address> <year> 1956. </year>
Reference-contexts: 1. Introduction The problem of learning the minimum state DFA that is consistent with a given sample has been actively studied for over two decades. DFAs are recognizers for regular languages which constitute the simplest class in the Chomsky hierarchy of formal languages <ref> [5, 15] </ref>. An understanding of the issues and problems encountered in learning regular languages (or equivalently, identification of the corresponding DFA) are therefore likely to provide insights into the problem of learning more general classes of languages.
Reference: 6. <author> F. Denis, C. D'Halluin, and R. Gilleron. </author> <title> Pac learning with simple examples. </title> <booktitle> STACS'96 Proceedings of the 13 th Annual Symposium on the Theoretical Aspects of Computer Science, </booktitle> <pages> pages 231242, </pages> <year> 1996. </year>
Reference-contexts: Following up on this, Denis et al proposed a model of learning where examples are drawn at random according to the universal distribution by a teacher that is knowledgeable about the target concept <ref> [6] </ref>. This model is known as the PACS learning model. In this paper, we present a method for efficient PAC learning of DFA from simple examples. <p> Given a string r 2 fl , the universal distribution based on the knowledge of r, m r , is defined as is defined as m r (ff) = r 2 K (ffjr) where r ff2 fl 2 K (ffjr) = 1 (i.e., r 1) <ref> [6] </ref>. Further, m r is such that 2 K (ffjr) m r (ff) 2 K (ffjr) where is a constant. The interested reader is referred to [20] for a thorough treatment of Kolmogorov complexity, universal distribution, and related topics. 7 3. <p> Denis et al proposed a model of learning where examples are drawn at random according to the universal distribution by a teacher that is knowledgeable about the target concept <ref> [6] </ref>. Under this model, examples with low conditional Kolmogorov complexity given a representation r of the target concept are called simple examples. <p> The Occam's Razor theorem proved by Denis et al states that if there exists a representative set of simple examples for each concept in a concept class then the concept class is PACS learnable <ref> [6] </ref>. We now demonstrate that the class of DFA is efficiently learnable under the PACS model. <p> This proves the lemma. 13 Lemma 4 (Due to Denis et al <ref> [6] </ref>) Suppose that a sample S is drawn according to m r . For an integer l jrj, and 0 &lt; ffi 1, if jSj = O (l lg ( 1 ffi )) then with probability greater than 1 ffi, S r sim S.
Reference: 7. <author> F. Denis and R. Gilleron. </author> <title> Pac learning under helpful distributions. </title> <booktitle> In Proceedings of the Eighth International Workshop on Algorithmic Learning Theory (ALT'97), Lecture Notes in Artificial Intelligence 1316, </booktitle> <pages> pages 132145, </pages> <address> Sendai, Japan, </address> <year> 1997. </year>
Reference-contexts: This opens up a number of interesting possibilities for learning under simple distributions. In a recent paper Denis and Gilleron have proposed a new model of learning under helpful distributions <ref> [7] </ref>. A helpful distribution is one in which examples belonging to the characteristic set for the concept (if there exists one) are assigned non-zero probability. A systematic characterization of the class of helpful distributions would perhaps give us a more practical framework for learning from simple examples.
Reference: 8. <author> P. Dupont. </author> <title> Incremental regular inference. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> pages 222237, </pages> <address> Montpellier, France, </address> <year> 1996. </year>
Reference-contexts: If the sample is a characteristic set for the target DFA then the algorithm is guaranteed to return a canonical representation of the target DFA. Our description of the RPNI algorithm is based on the explanation given in <ref> [8] </ref>. A labeled sample S = S + [ S is provided as input to the algorithm. It constructs a prefix tree automaton P T A (S + ) and numbers its states in the standard order. <p> This recursive merging of states can go on for at most N 1 steps and the resulting automaton M ^ is guaranteed to be a DFA <ref> [8] </ref>. Note that since ~ t ^ we know by the grammar covers relation that if M ~ accepts a negative example in S then so would M ^ . <p> These results have an interesting implication on the framework for incremental learning of the target DFA. In the RPNI2 incremetal algorithm for learning DFA, the learner maintains a hypothesis that is consistent with all labeled examples seen thus far and modifies it whenever a new inconsistent example is observed <ref> [8] </ref>. The convergence of this algorithm relies on the fact that sooner or later, the set of labeled examples seen by the learner will include a characteristic set.
Reference: 9. <author> P. </author> <type> Dupont. </type> <institution> Utilisation et Apprentissage de Mod eles de Language pour la Reconnaissance de la Parole Continue. </institution> <type> PhD thesis, </type> <institution> Ecole Normale Sup erieure des T el ecommunications, Paris, France, </institution> <year> 1996. </year>
Reference-contexts: Further, if S is a superset of a characteristic set (see section 2.1) for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to the target <ref> [21, 9] </ref>. Pitt surveyed several approaches for approximate identification of DFA [25].
Reference: 10. <author> P. Dupont, L. Miclet, and E. Vidal. </author> <title> What is the search space of the regular inference? In Proceedings of the Second International Colloquium on Grammatical Inference (ICGI'94), </title> <booktitle> pages 2537, </booktitle> <address> Alicante, Spain, </address> <year> 1994. </year>
Reference-contexts: set S + is said to be structurally complete with respect to a DFA A if S + covers each transition of A (except the transitions associated with the dead state d 0 ) and uses every element of the set of final states of A as an accepting state <ref> [22, 23, 10] </ref>. It can be verified that the set S + = fb; aa; aaaag is structurally complete with respect to the DFA in Fig. 1. Given a set S + , let P T A (S + ) denote the prefix tree acceptor for S + . <p> Given a canonical DFA A and a set S + that is structurally complete with respect to A, the lattice (S + ) derived from P T A (S + ) is guaranteed to contain A <ref> [22, 23, 10] </ref>.
Reference: 11. <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and Control, </journal> <volume> 37(3):302320, </volume> <year> 1978. </year>
Reference-contexts: Exact learning of the target DFA from an arbitrary presentation of labeled examples is a hard problem <ref> [11] </ref>. Gold showed that the problem of identifying the minimum state DFA consistent with a presentation S comprising of a finite non-empty set of positive examples S + and possibly a finite non-empty set of negative examples S is N P -hard. <p> Further, we demonstrate how the model of learning from simple examples naturally extends the model of learning concepts from representative examples <ref> [11] </ref>, the polynomial teachability model [12], and the model of learning from example based queries [3] to a probabilistic framework. This paper is organized as follows: Section 2 briefly introduces some concepts used in the results described in this paper. <p> Thus, the class of DFA is efficiently PAC learnable under the PACS model. 6. Relationship of the PACS Model to other Learning Models In this section we study the relationship of the PACS model to other prominent learning models such as Gold's model of polynomial identifiability from characteristic samples <ref> [11] </ref>, Goldman and Mathias' polynomial teachability model [12], and the model of learning from example based queries [3]. We explain how the PACS learning model naturally extends these other models to a probabilistic framework. <p> Using the above definition Gold's result can be restated as follows. Theorem 4 (due to Gold <ref> [11] </ref>) The class of DFA is polynomially identifiable from characteristic samples. The problem of identifying a minimum state DFA that is consistent with an arbitrary labeled sample S = S + [ S is known to be NP-complete [11]. <p> Theorem 4 (due to Gold <ref> [11] </ref>) The class of DFA is polynomially identifiable from characteristic samples. The problem of identifying a minimum state DFA that is consistent with an arbitrary labeled sample S = S + [ S is known to be NP-complete [11]. This result does not contradict the one in theorem 4 because a characteristic set is not any arbitrary set of examples but a special set that enables the learning algorithm to correctly infer the target concept in polynomial time (see the RPNI algorithm in section 3). 6.2.
Reference: 12. <author> S. Goldman and H. Mathias. </author> <title> Teaching a smarter learner. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory (COLT'93), </booktitle> <pages> pages 6776. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: Further, we demonstrate how the model of learning from simple examples naturally extends the model of learning concepts from representative examples [11], the polynomial teachability model <ref> [12] </ref>, and the model of learning from example based queries [3] to a probabilistic framework. This paper is organized as follows: Section 2 briefly introduces some concepts used in the results described in this paper. This includes a discussion of the PAC learning model, Kolmogorov complexity, and the universal distribution. <p> Relationship of the PACS Model to other Learning Models In this section we study the relationship of the PACS model to other prominent learning models such as Gold's model of polynomial identifiability from characteristic samples [11], Goldman and Mathias' polynomial teachability model <ref> [12] </ref>, and the model of learning from example based queries [3]. We explain how the PACS learning model naturally extends these other models to a probabilistic framework. <p> Polynomial Teachability of Concept Classes Goldman and Mathias developed a teaching model for efficient learning of target concepts <ref> [12] </ref>. Their model takes into account the quantity of information that a good teacher must provide to the learner. An additional player called the adversary is introduced in this model to ensure that there is no collusion whereby the teacher gives the learner an encoding of the target concept. <p> Example based queries include equivalence, membership, subset, superset, disjointedness, exhaustive, justifying assignment, and partial equivalence queries. 18 Definition 4. (due to Goldman and Mathias <ref> [12] </ref>) An example based query is any query of the form 8 (x 1 ; x 2 ; : : : ; x k ) 2 X k does r (x 1 ; x 2 ; : : : ; x k ) = 1? where r is the target concept <p> Theorem 6 (due to Goldman and Mathias <ref> [12] </ref>) Any concept class that is learnable in deterministic polynomial time using example based queries is semi-polynomially T/L teachable. The above theorem enables us to connect learning from example based queries to PACS learning as follows.
Reference: 13. <author> S. Goldman and H. Mathias. </author> <title> Teaching a smarter learner. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52:255267, </volume> <year> 1996. </year>
Reference-contexts: Collusion and PACS Learning Learning models that involve interaction between a knowledgeable teacher and a learner are vulnerable to unnatural collusion wherein a the teacher passes information about the representation of the target function as part of the training set <ref> [16, 13] </ref>. Consider for simplicity that the instance space is f0; 1g n (i.e., the training examples are n bits long). The teacher and learner can a-priori agree on some suitable binary encoding of concepts. <p> Goldman and Mathias' work on polynomial teachability <ref> [13] </ref> shows that an adversary whose task it is to embed the training set provided by the teacher (called teaching set) into a larger set of correctly labeled examples is sufficient to prevent the type of collusion discussed above. <p> Goldman and Mathias claim that this is not collusion according to their definition wherein a colluding teacher learner pair is one where the teacher can potentially influence the distribution on the concept class C when presenting the learner with teaching sets for logically equivalent concepts <ref> [13] </ref>. Specifically, say the teacher encodes the representation r of the target DFA A as part of the teaching set. The adversary can simply add a correctly labeled example r 0 which is a representation of a DFA A 0 that is logically equivalent to A to the teaching set.
Reference: 14. <author> Colin de la Higuera. </author> <title> Characteristic sets for polynomial grammatical inference. </title> <editor> In L. Miclet and C. Higuera, editors, </editor> <booktitle> Proceedings of the Third ICGI-96, Lecture Notes in Artificial Intelligence 1147, </booktitle> <pages> pages 5971, </pages> <address> Montpellier, France, </address> <year> 1996. </year>
Reference-contexts: Definition 2. (due to Colin de la Higuera <ref> [14] </ref>) C is polynomially identifiable from characteristic samples iff there exist two polynomials p 1 () and p 2 () and an algorithm A such that 1. <p> A scenario in which the teacher constructs 17 a very small teaching set whose examples are unreasonably long is clearly undesirable and must be avoided. This is explained more formally in the following definition. Definition 3. (due to Colin de la Higuera <ref> [14] </ref>) A concept class C is semi-polynomially T/L teachable iff there exist polynomials p 1 (), p 2 (), and p 3 (), a teacher T , and a learner L, such that for any adversary ADV and any concept c with representation r that is selected by ADV , after <p> proved that the model for polynomial identification from characteristic samples and the model for polynomial teachability are equivalent to each other (i.e., by identifying the characteristic set with the teaching sample it was shown that a concept class is polynomially identifiable from characteristic samples iff it is semi-polynomially T/L teachable) <ref> [14] </ref>. Lemma 5 Let c 2 C be a concept with corresponding representation r 2 R.
Reference: 15. <author> J. Hopcroft and J. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison Wesley, </publisher> <year> 1979. </year>
Reference-contexts: 1. Introduction The problem of learning the minimum state DFA that is consistent with a given sample has been actively studied for over two decades. DFAs are recognizers for regular languages which constitute the simplest class in the Chomsky hierarchy of formal languages <ref> [5, 15] </ref>. An understanding of the issues and problems encountered in learning regular languages (or equivalently, identification of the corresponding DFA) are therefore likely to provide insights into the problem of learning more general classes of languages. <p> Without loss of generality, we will assume that the target DFA being learned is a canonical DFA. Let N denote the number of states of A. It can be shown that any canonical DFA has at most one dead state <ref> [15] </ref>. One can define a standard encoding of DFA as binary strings such that any DFA with N states is encoded as a binary string of length O (N lg N ).
Reference: 16. <author> J. Jackson and A. Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proceedings of the Workshop on Computational Learning Theory (COLT'92), </booktitle> <publisher> ACM Press, </publisher> <pages> pages 319326, </pages> <year> 1992. </year>
Reference-contexts: Collusion and PACS Learning Learning models that involve interaction between a knowledgeable teacher and a learner are vulnerable to unnatural collusion wherein a the teacher passes information about the representation of the target function as part of the training set <ref> [16, 13] </ref>. Consider for simplicity that the instance space is f0; 1g n (i.e., the training examples are n bits long). The teacher and learner can a-priori agree on some suitable binary encoding of concepts. <p> The learner could quickly discover the target concept without even considering the labels of the training examples. The teaching model due to Jackson and Tomkins <ref> [16] </ref> prevents this blatant coding of the target concept by requiring that the learner must still succeed if the teacher is replaced by an adversarial substitute (who does not code the target concept as the teacher above).
Reference: 17. <author> M. Kearns and L. G. Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <booktitle> In Proceedings of the 21 st Annual ACM Symposium on Theory of Computing, </booktitle> <address> New York, </address> <pages> pages 433444, </pages> <year> 1989. </year>
Reference-contexts: Further, Kearns and Valiant showed that an efficient algorithm for learning DFA would entail efficient algorithms for solving problems such as breaking the RSA cryptosystem, factoring Blum integers, and detecting quadratic residues <ref> [17] </ref>. Under the standard cryptographic assumptions these problems are known to be hard to solve. Thus, they argued that DFA learning is a hard problem. The PAC model's requirement of learnability under all conceivable distributions is often considered too stringent for practical learning scenarios. <p> Some of the negative results in approximate identification of DFA are derived by showing that an efficient algorithm for learning DFA would entail algorithms for solving known hard problems such as learning boolean formulae [26] and breaking the RSA cryptosystem <ref> [17] </ref>. It would be interesting to explore the implications of our results on efficient learning of DFA from simple examples on these problems.
Reference: 18. <author> K. J. Lang. </author> <title> Random dfa's can be approximately learned from sparse uniform sample. </title> <booktitle> In Proceedings of the 5th ACM workshop on Computational Learning Theory, </booktitle> <pages> pages 4552, </pages> <year> 1992. </year>
Reference-contexts: Pitt's paper identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? [25]. Using a variant of Trakhtenbrot and Barzdin's algorithm, Lang empirically demonstrated that random DFAs are approximately learnable from a sparse uniform sample <ref> [18] </ref>. However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. Several efforts have been made to study the learnability of concept classes under restricted classes of distributions.
Reference: 19. <author> M. Li and P. Vit anyi. </author> <title> Learning simple concepts under simple distributions. </title> <journal> SIAM Journal of Computing, </journal> <volume> 20:911935, </volume> <year> 1991. </year>
Reference-contexts: Learning Simple DFA under the Simple PAC model Li and Vit anyi have proposed the simple PAC learning model where the class of probability distributions is restricted to simple distributions <ref> [19] </ref>. A distribution is simple if it is multiplicatively dominated by some enumerable distribution. Simple distributions include a variety of distributions including all computable distributions. <p> Further, the simple distribution independent learning theorem due to Li and Vitany i says that a concept class is learnable under universal distribution m iff it is learnable under the entire class of simple distributions provided the examples are drawn according to the universal distribution <ref> [19] </ref>. Thus, the simple PAC learning model is sufficiently general. Concept classes such as log n-term DNF and simple k-reversible DFA are learnable under the simple PAC model whereas their PAC learnability in the standard sense is unknown [19]. <p> simple distributions provided the examples are drawn according to the universal distribution <ref> [19] </ref>. Thus, the simple PAC learning model is sufficiently general. Concept classes such as log n-term DNF and simple k-reversible DFA are learnable under the simple PAC model whereas their PAC learnability in the standard sense is unknown [19]. We show that the class of simple DFA is polynomially learnable under the simple PAC learning model. 10 A simple DFA is one with low Kolmogorov complexity. <p> It has been shown that a concept class is efficiently learnable under the universal distribution if and only if it is efficiently learnable under each simple distribution provided that sampling is done according to the universal distribution <ref> [19] </ref>. This raises the possibility of using sampling under the universal distribution to learn under all computable distributions. However, the universal distribution is not computable. Whether one can instead get by with a polynomially computable approximation of the universal distribution remains an open question. <p> However, the universal distribution is not computable. Whether one can instead get by with a polynomially computable approximation of the universal distribution remains an open question. It is known that the universal distribution for the class of polynomially-time bounded simple distributions is computable in exponential time <ref> [19] </ref>. This opens up a number of interesting possibilities for learning under simple distributions. In a recent paper Denis and Gilleron have proposed a new model of learning under helpful distributions [7].
Reference: 20. <author> M. Li and P. Vit anyi. </author> <title> An Introduction to Kolmogorov Complexity and its Applications, 2 nd edition. </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: Thus, for any enumerable probability distribution P there is a constant c 2 N such that for all strings ff; c m (ff) P (ff). The Coding Theorem due independently to Schnorr, Levin, and Chaitin <ref> [20] </ref> states that 9 2 N such that 8ff m M (ff) 2 K (ff) : Intuitively this means that if there are several programs for a string ff on some machine M then there is a short program for ff on the universal Turning machine (i.e., ff has a low <p> Further, m r is such that 2 K (ffjr) m r (ff) 2 K (ffjr) where is a constant. The interested reader is referred to <ref> [20] </ref> for a thorough treatment of Kolmogorov complexity, universal distribution, and related topics. 7 3. The RPNI Algorithm The regular positive and negative inference (RPNI) algorithm [21] is a polynomial time algorithm for identification of a DFA consistent with a given set S = S + [ S .
Reference: 21. <author> J. Oncina and P. Garc a. </author> <title> Inferring regular languages in polynomial update time. </title> <editor> In N. et al P erez, editor, </editor> <title> Pattern Recognition and Image Analysis, </title> <publisher> World Scientific, </publisher> <pages> pages 4961, </pages> <year> 1992. </year>
Reference-contexts: The regular positive and negative inference (RPNI) algorithm is a framework for identifying in polynomial time, a DFA consistent with a given sample S <ref> [21] </ref>. Further, if S is a superset of a characteristic set (see section 2.1) for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to the target [21, 9]. Pitt surveyed several approaches for approximate identification of DFA [25]. <p> Further, if S is a superset of a characteristic set (see section 2.1) for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to the target <ref> [21, 9] </ref>. Pitt surveyed several approaches for approximate identification of DFA [25]. <p> A sample S = S + [ S is said to be characteristic with respect to a regular language L (with a canonical acceptor A) if it satisfies the following two conditions <ref> [21] </ref>: * 8ff 2 N (L); if ff 2 L then ff 2 S + else 9fi 2 fl such that fffi 2 S + . * 8ff 2 S p (L); 8fi 2 N (L); if L ff 6= L fi then 9fl 2 fl such that (fffl 2 S <p> The interested reader is referred to [20] for a thorough treatment of Kolmogorov complexity, universal distribution, and related topics. 7 3. The RPNI Algorithm The regular positive and negative inference (RPNI) algorithm <ref> [21] </ref> is a polynomial time algorithm for identification of a DFA consistent with a given set S = S + [ S . If the sample is a characteristic set for the target DFA then the algorithm is guaranteed to return a canonical representation of the target DFA.
Reference: 22. <author> T. Pao and J. Carr. </author> <title> A solution of the syntactic induction-inference problem for regular languages. </title> <booktitle> Computer Languages, </booktitle> <address> 3:5364, </address> <year> 1978. </year>
Reference-contexts: set S + is said to be structurally complete with respect to a DFA A if S + covers each transition of A (except the transitions associated with the dead state d 0 ) and uses every element of the set of final states of A as an accepting state <ref> [22, 23, 10] </ref>. It can be verified that the set S + = fb; aa; aaaag is structurally complete with respect to the DFA in Fig. 1. Given a set S + , let P T A (S + ) denote the prefix tree acceptor for S + . <p> The set of all quotient automata obtained by systematically merging the states of a DFA A represents a lattice of FSA <ref> [22] </ref>. This lattice is ordered by the grammar cover relation . <p> Given a canonical DFA A and a set S + that is structurally complete with respect to A, the lattice (S + ) derived from P T A (S + ) is guaranteed to contain A <ref> [22, 23, 10] </ref>.
Reference: 23. <author> R. G. Parekh and V. G. Honavar. </author> <title> Efficient learning of regular languages using teacher supplied positive examples and learner generated queries. </title> <booktitle> In Proceedings of the Fifth UNB Conference on AI, </booktitle> <address> Fredricton, Canada, pages 195203, </address> <year> 1993. </year>
Reference-contexts: set S + is said to be structurally complete with respect to a DFA A if S + covers each transition of A (except the transitions associated with the dead state d 0 ) and uses every element of the set of final states of A as an accepting state <ref> [22, 23, 10] </ref>. It can be verified that the set S + = fb; aa; aaaag is structurally complete with respect to the DFA in Fig. 1. Given a set S + , let P T A (S + ) denote the prefix tree acceptor for S + . <p> Given a canonical DFA A and a set S + that is structurally complete with respect to A, the lattice (S + ) derived from P T A (S + ) is guaranteed to contain A <ref> [22, 23, 10] </ref>.
Reference: 24. <author> R. G. Parekh and V. G. Honavar. </author> <title> Learning dfa from simple examples. </title> <booktitle> In Proceedings of the Eighth International Workshop on Algorithmic Learning Theory (ALT'97), Lecture Notes in Artificial Intelligence 1316, </booktitle> <pages> pages 116131, </pages> <address> Sendai, Japan, </address> <year> 1997. </year> <title> Springer. Also presented at the Workshop on Grammar Inference, Automata Induction, and Language Acquisition (ICML'97), </title> <address> Nashville, TN, </address> <month> July 12, </month> <year> 1997. </year>
Reference-contexts: Pitt surveyed several approaches for approximate identification of DFA [25]. Valiant's distribution-independent model of learning, also called the probably approximately correct (PAC) learning model [29], * The results in section 5 were presented earlier in <ref> [24] </ref>. ** This research was partially supported by the National Science Foundation grants IRI-9409580 and IRI-9643299 to Vasant Honavar. 2 is a widely used framework for approximate learning of concept classes.
Reference: 25. <author> L. Pitt. </author> <title> Inductive inference, DFAs and computational complexity. In Analogical and Inductive Inference, </title> <booktitle> Lecture Notes in Artificial Intelligence 397, </booktitle> <publisher> Springer-Verlag, </publisher> <pages> pages 1844, </pages> <year> 1989. </year>
Reference-contexts: Further, if S is a superset of a characteristic set (see section 2.1) for the target DFA then the DFA output by the RPNI algorithm is guaranteed to be equivalent to the target [21, 9]. Pitt surveyed several approaches for approximate identification of DFA <ref> [25] </ref>. <p> The PAC model's requirement of learnability under all conceivable distributions is often considered too stringent for practical learning scenarios. Pitt's paper identified the following open research problem: Are DFA's PAC-identifiable if examples are drawn from the uniform distribution, or some other known simple distribution? <ref> [25] </ref>. Using a variant of Trakhtenbrot and Barzdin's algorithm, Lang empirically demonstrated that random DFAs are approximately learnable from a sparse uniform sample [18]. However, exact identification of the target DFA was not possible even in the average case with a randomly drawn training sample. <p> In the context of learning DFA, D is restricted to a probability distribution on strings of fl of length at most m. Definition 1. (due to Pitt <ref> [25] </ref>) DFAs are PAC-identifiable iff there exists a (possibly randomized) algorithm A such that on input of any parameters * and ffi, for any DFA M of size N , for any number m, and for any probability distribution D on strings of fl of length at most m, if A
Reference: 26. <author> L. Pitt and M. K. Warmuth. </author> <title> Reductions among prediction problems: on the difficulty of predicting automata. </title> <booktitle> In Proceedings of the 3 rd IEEE Conference on Structure in Complexity Theory, </booktitle> <pages> pages 6069, </pages> <year> 1988. </year>
Reference-contexts: Under the standard complexity theoretic assumption P 6= N P , Pitt and Warmuth showed that no polynomial time algorithm can be guaranteed to produce a DFA with at most N (1*)loglog (N) states from a set of labeled examples corresponding to a DFA with N states <ref> [26] </ref>. Efficient learning algorithms for identification of DFA assume that additional information is provided to the learner. <p> Some of the negative results in approximate identification of DFA are derived by showing that an efficient algorithm for learning DFA would entail algorithms for solving known hard problems such as learning boolean formulae <ref> [26] </ref> and breaking the RSA cryptosystem [17]. It would be interesting to explore the implications of our results on efficient learning of DFA from simple examples on these problems.
Reference: 27. <author> L. Pitt and M. K. Warmuth. </author> <title> The minimum consistency dfa problem cannot be approximated within any polynomial. </title> <booktitle> In Proceedings of the 21 st ACM Symposium on the Theory of Computing, ACM, </booktitle> <pages> pages 421432, </pages> <year> 1989. </year>
Reference-contexts: Even approximate learnability of DFA was proven to be a hard problem. Pitt and Warmuth showed that the problem of polynomially approximate predictability of the class of DFA is hard <ref> [27] </ref>. They used prediction preserving reductions to show that if DFAs are polynomially approximately predictable then so are other known hard to predict concept classes such as boolean formulas.
Reference: 28. <author> B. Trakhtenbrot and Ya. Barzdin. </author> <title> Finite Automata: Behavior and Synthesis. </title> <publisher> North Holland Publishing Company, </publisher> <address> Amsterdam, </address> <year> 1973. </year>
Reference-contexts: Trakhtenbrot and Barzdin described a polynomial time algorithm for constructing the smallest DFA consistent with a complete labeled sample i.e., a sample that includes all strings up to a particular length and the corresponding label that states whether the string is accepted by the target DFA or not <ref> [28] </ref>. Angluin showed that given a live-complete set of examples (that contains a representative string for each live state of the target DFA) and a knowledgeable teacher to answer membership queries it is possible to exactly learn the target DFA [1].
Reference: 29. <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27:11341142, </volume> <year> 1984. </year>
Reference-contexts: Pitt surveyed several approaches for approximate identification of DFA [25]. Valiant's distribution-independent model of learning, also called the probably approximately correct (PAC) learning model <ref> [29] </ref>, * The results in section 5 were presented earlier in [24]. ** This research was partially supported by the National Science Foundation grants IRI-9409580 and IRI-9643299 to Vasant Honavar. 2 is a widely used framework for approximate learning of concept classes.
References-found: 29

