URL: http://www.cs.yale.edu/users/rasmussen/lib/papers/icra97.ps.gz
Refering-URL: http://www.cs.yale.edu/users/rasmussen/research.html
Root-URL: http://www.cs.yale.edu
Email: rasmuss@powered.cs.yale.edu  
Title: Visual Learning for Collision Avoidance in a Simulated Environment  
Author: Christopher Rasmussen 
Address: 51 Prospect Street New Haven, CT 06520-8285  
Affiliation: Department of Computer Science Yale University  
Abstract: We demonstrate a simulated robot in a three-dimensional, texture-mapped graphical environment that learns from optical flow calculations to avoid collisions with walls. The robot has no preprogrammed notion of divergence or left-right asymmetry, but by associating the values of visual variables leading up to the moment of each collision with the fact and particulars of that collision, the robot gradually approximates these functions and improves its performance. The credit assignment problem associated with reinforcement learning is greatly reduced in this continuous domain, permitting a simple nearest-neighbor learning algorithm. It is hoped that this technique may be extended to other mobile robot problems for which appropriate functions are not as easily discerned from first principles. Further, the use of artificial visual environments is advanced as a tool for judging the feasibility of vision-based learning algorithms before committing them to real robot platforms. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Ancona and T. Poggio. </author> <title> Optical Flow from 1-D Correlation: Application to a simple Time-To-Crash Detector. </title> <journal> Int. J. Computer Vision, </journal> <volume> Vol. 14, No. 2, </volume> <year> 1995. </year>
Reference-contexts: Though an expensive algorithm could be used for the simulation, in anticipation of its use on a real robot a fast, approximate optical flow algorithm <ref> [1] </ref> was chosen to obtain an estimate of the flow field quickly. This algorithm computes correlations of a 1-D window about each pixel over a range constrained by the maximum expected image pixel velocity.
Reference: [2] <author> M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. </author> <title> Vision-based reinforcement learning for purposive behavior acquisition. </title> <booktitle> In Int. Conf. Robotics & Automation, </booktitle> <pages> pp. 146-153, </pages> <year> 1995. </year>
Reference-contexts: An algorithm that tuned the TTC estimate based on experience would as a byproduct adapt to the size of the robot it was running on. There has been some related work on mobile robot learning <ref> [2, 4, 8] </ref> to use as a guide, but the question of which particular learning algorithms are best suited to specific problem domains is open.
Reference: [3] <author> D. Coombs, M. Herman, T. Hong, and M. Nashman. </author> <title> Real-time obstacle avoidance using central flow divergence and peripheral flow. </title> <booktitle> In Int. Conf. on Computer Vision, </booktitle> <year> 1995. </year>
Reference-contexts: Our work here is intended to demonstrate the viability of this approach for a simple problem|collision avoidance| in a highly constrained environment. We are currently extending it to more complicated problems. Time-to-collision (TTC) estimates and other functions of the optical flow field have been a subject of recent study <ref> [9, 3] </ref> as methods for controlling vision-equipped robots navigating among obstacles. <p> The time-to-collision (TTC), or time remaining until the robot runs into what it is looking at, can be computed from the divergence of the optical flow field <ref> [3] </ref>. For our framing of the problem, the TTC is approximated by TTC = s 1 (x r x l ) for some scaling factor s 1 . <p> One might imagine various control schemes based on this acquired information. Something to be careful of, though, is that camera rotation, which normally accompanies turning, induces an optical flow of its own that is difficult to separate from that due to the environment. This has been dealt with <ref> [3] </ref> by keeping the camera pointing in the same direction while the base turns, then realigning them quickly and resetting all filters when the angle gets too large. <p> This is probably due to a lack of smoothing on TTC estimates rather than any error in the estimates themselves. As was noted in <ref> [3] </ref>, TTC estimates decrease nonmonotonically as a wall is approached. They used Kalman filtering on the raw estimates to get a more continuous sequence of estimates, and also use a few more criteria for ultimately deciding when to turn.
Reference: [4] <author> D. Floreano and F. Mondada. </author> <title> Automatic Creation of an Autonomous Agent: Genetic Evolution of a Neural-Network Driven Robot. </title> <booktitle> In Int. Conf. on Simulation of Adaptive Behavior, </booktitle> <year> 1994. </year>
Reference-contexts: An algorithm that tuned the TTC estimate based on experience would as a byproduct adapt to the size of the robot it was running on. There has been some related work on mobile robot learning <ref> [2, 4, 8] </ref> to use as a guide, but the question of which particular learning algorithms are best suited to specific problem domains is open.
Reference: [5] <author> I. Horswill. </author> <title> Specialization of Perceptual Processes. </title> <type> Ph.D. </type> <institution> diss., Dept. of Electrical Engineering & Computer Science, MIT, </institution> <year> 1993. </year>
Reference-contexts: Segmentation methods for robot navigation have also been explored <ref> [5, 7] </ref> which exhibit strengths and shortcomings somewhat different from those of motion-based techniques. It seems clear that some combination of these two approaches would yield a more robust mobile robot controller, but by doing so the problem of which to favor in which visual circumstances is introduced.
Reference: [6] <author> L. Kaelbling, M. Littman, and A. Moore. </author> <title> Reinforcment Learning: A Survey. </title> <journal> J. AI Research, </journal> <volume> No. 4, </volume> <pages> pp. 237-285, </pages> <year> 1996. </year>
Reference-contexts: This is not a new contention; rather, it has been the foundation of those working on applying reinforcement learning <ref> [6] </ref> and other discrete techniques to robotics (albeit often with non-visual inputs). However, the task of vision-based collision avoidance, as well as many other basic motor skills, permits an assumption of the approximate continuity of visual input with respect to feedback. In other words, similar input heralds similar outcomes.
Reference: [7] <author> D. Pomerleau. RALPH: </author> <title> Rapidly Adapting Lateral Position Handler. </title> <booktitle> In IEEE Symposium on Intelligent Vehicles, </booktitle> <year> 1995. </year>
Reference-contexts: Segmentation methods for robot navigation have also been explored <ref> [5, 7] </ref> which exhibit strengths and shortcomings somewhat different from those of motion-based techniques. It seems clear that some combination of these two approaches would yield a more robust mobile robot controller, but by doing so the problem of which to favor in which visual circumstances is introduced.
Reference: [8] <author> S. Mahadevan and J. Connell. </author> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning. </title> <booktitle> In AAAI, </booktitle> <pages> pp. 8-14, </pages> <year> 1991. </year>
Reference-contexts: An algorithm that tuned the TTC estimate based on experience would as a byproduct adapt to the size of the robot it was running on. There has been some related work on mobile robot learning <ref> [2, 4, 8] </ref> to use as a guide, but the question of which particular learning algorithms are best suited to specific problem domains is open.
Reference: [9] <author> R. Nelson and Y. Aloimonos. </author> <title> Obstacle Avoidance Using Flow Field Divergence. </title> <journal> IEEE Trans. Pattern Analysis & Machine Intelligence, </journal> <volume> Vol. 11, No. 10, </volume> <pages> pp. 1102-1106, </pages> <year> 1989. </year>
Reference-contexts: Our work here is intended to demonstrate the viability of this approach for a simple problem|collision avoidance| in a highly constrained environment. We are currently extending it to more complicated problems. Time-to-collision (TTC) estimates and other functions of the optical flow field have been a subject of recent study <ref> [9, 3] </ref> as methods for controlling vision-equipped robots navigating among obstacles.
Reference: [10] <author> D. Terzopoulos and T. Rabie. </author> <title> Animat Vision: </title> <booktitle> Active Vision in Artificial Animals. In Int. Conf. on Computer Vision, </booktitle> <pages> pp. 801-808, </pages> <year> 1995. </year>
Reference-contexts: There has been some related work on mobile robot learning [2, 4, 8] to use as a guide, but the question of which particular learning algorithms are best suited to specific problem domains is open. We have chosen simulated vision (see <ref> [10] </ref> for a defense of this route) primarily as a prototyping tool: at this early stage we are still configuring tasks and learning methods before attempting them on a real robot.
References-found: 10

