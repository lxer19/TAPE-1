URL: http://www.cs.iastate.edu/~honavar/Papers/icnn96.ps
Refering-URL: http://www.cs.iastate.edu/~honavar/honavar.html
Root-URL: 
Email: balakris@cs.iastate.edu, honavar@cs.iastate.edu  
Title: Analysis of Neurocontrollers Designed by Simulated Evolution  
Author: Karthik Balakrishnan and Vasant Honavar 
Address: Ames, IA 50011.  
Affiliation: Artificial Intelligence Research Group Iowa State University  
Abstract: Randomized, adaptive, greedy search using evolutionary algorithms offers a powerful and versatile approach to the automated design of neural network architectures for a variety of tasks in artificial intelligence and robotics. In this paper we present results from the evolutionary design of a neuro-controller for a robotic bulldozer. This robot is given the task of clearing an arena littered with boxes by pushing boxes to the sides. Through a careful analysis of the evolved networks we show how evolution exploits the design constraints and properties of the environment to produce network structures of high fitness. We conclude with a brief summary of related ongoing research examining the intricate interplay between environment and evolutionary processes in determining the structure and function of the resulting neural architectures.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Balakrishnan and V. Honavar. </author> <title> Analysis of neurocontrollers designed by simulated evolution. </title> <type> Technical Report CS TR 95-25, </type> <institution> Department of Computer Science, Iowa State University, Ames, </institution> <address> IA - 50011, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: We used a genetic representation that codes for the input connectivity of each of the units of the network as is shown in Figure 2 (ref <ref> [1, 3] </ref> for details). The genetic operator crossover was defined to swap the input connectivities of units, while mutation was defined to either exchange two input connections of a unit, or randomly change the value of one input connection.
Reference: [2] <author> K. Balakrishnan and V. Honavar. </author> <title> Evolutionary design of neural architectures | a preliminary taxonomy and guide to literature. </title> <type> Technical Report CS TR 95-01, </type> <institution> Department of Computer Science, Iowa State University, Ames, </institution> <address> IA - 50011, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: 1 Introduction Artificial neural networks offer an attractive paradigm for the design of behavior and control systems in robots and autonomous agents for a variety of reasons <ref> [7, 5, 2] </ref> including: ability to adapt and learn, potential for resistance to noise, faults and component failures, potential for real-time performance in dynamic environments (through massive parallelism and suitable hardware realization). <p> Evolutionary Algorithms (EA) that are loosely inspired models of natural evolution, offer a particularly attractive tool for the design of neuro-controllers for a number of reasons <ref> [2] </ref>: They have been shown to be effective in searching several vast, complex, multi-modal and deceptive search spaces [8, 6]; EA constitute a form of population-based reinforcement learning and hence can deal with problems where precise knowledge of the desired actions in response to particular sensory inputs is lacking (and hence
Reference: [3] <author> K. Balakrishnan and V. Honavar. </author> <title> Properties of genetic representations of neural architectures. </title> <booktitle> In Proceedings of the World Congress on Neural Networks, </booktitle> <pages> pages 807-813, </pages> <address> Washington D.C, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: We used a genetic representation that codes for the input connectivity of each of the units of the network as is shown in Figure 2 (ref <ref> [1, 3] </ref> for details). The genetic operator crossover was defined to swap the input connectivities of units, while mutation was defined to either exchange two input connections of a unit, or randomly change the value of one input connection.
Reference: [4] <author> V. </author> <title> Braitenberg. Vehicles: Experiments in Synthetic Psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year>
Reference-contexts: Table 2 shows a different mapping of the neural network outputs to actions. inspired by Braitenberg's thought experiments with vehicles <ref> [4] </ref>.
Reference: [5] <author> D. Floreano and F. Mondada. </author> <title> Automatic creation of an autonomous agent: Genetic evolution of a neural-network driven robot. </title> <editor> In D. Cliff, P. Husbands, J-A Meyer, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 421-430, </pages> <address> Cambridge, MA, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction Artificial neural networks offer an attractive paradigm for the design of behavior and control systems in robots and autonomous agents for a variety of reasons <ref> [7, 5, 2] </ref> including: ability to adapt and learn, potential for resistance to noise, faults and component failures, potential for real-time performance in dynamic environments (through massive parallelism and suitable hardware realization).
Reference: [6] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: Evolutionary Algorithms (EA) that are loosely inspired models of natural evolution, offer a particularly attractive tool for the design of neuro-controllers for a number of reasons [2]: They have been shown to be effective in searching several vast, complex, multi-modal and deceptive search spaces <ref> [8, 6] </ref>; EA constitute a form of population-based reinforcement learning and hence can deal with problems where precise knowledge of the desired actions in response to particular sensory inputs is lacking (and hence it is difficult to use conventional supervised neural network learning algorithms); More importantly, unlike most reinforcement learning techniques, <p> For reasons mentioned in section 1, we use to EA to search for such a neuro-controller in a suitably chosen space of neural architectures. 3 Implementation Details In our simulations we use Genetic Algorithms (GA) <ref> [8, 6] </ref> to evolve neuro-controller designs. We used a genetic representation that codes for the input connectivity of each of the units of the network as is shown in Figure 2 (ref [1, 3] for details).
Reference: [7] <author> I. Harvey, P. Husbands, and D. Cliff. </author> <title> Issues in evolutionary robotics. </title> <editor> In J. Meyer, H. Roitblat, and S. Wilson, editors, </editor> <booktitle> From Animals to Animats II: Proceedings of the Second International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 364-373, </pages> <address> Cambridge, MA, 1992. </address> <publisher> MIT Press-Bradford Books. </publisher>
Reference-contexts: 1 Introduction Artificial neural networks offer an attractive paradigm for the design of behavior and control systems in robots and autonomous agents for a variety of reasons <ref> [7, 5, 2] </ref> including: ability to adapt and learn, potential for resistance to noise, faults and component failures, potential for real-time performance in dynamic environments (through massive parallelism and suitable hardware realization).
Reference: [8] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> The University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: Evolutionary Algorithms (EA) that are loosely inspired models of natural evolution, offer a particularly attractive tool for the design of neuro-controllers for a number of reasons [2]: They have been shown to be effective in searching several vast, complex, multi-modal and deceptive search spaces <ref> [8, 6] </ref>; EA constitute a form of population-based reinforcement learning and hence can deal with problems where precise knowledge of the desired actions in response to particular sensory inputs is lacking (and hence it is difficult to use conventional supervised neural network learning algorithms); More importantly, unlike most reinforcement learning techniques, <p> For reasons mentioned in section 1, we use to EA to search for such a neuro-controller in a suitably chosen space of neural architectures. 3 Implementation Details In our simulations we use Genetic Algorithms (GA) <ref> [8, 6] </ref> to evolve neuro-controller designs. We used a genetic representation that codes for the input connectivity of each of the units of the network as is shown in Figure 2 (ref [1, 3] for details).
Reference: [9] <author> A. Teller. </author> <title> The evolution of mental models. </title> <editor> In Kim Kinnear, editor, </editor> <booktitle> Advances in Genetic Programming, </booktitle> <pages> pages 199-219. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year> <month> 6 </month>
Reference-contexts: This research was partially supported by the National Science Foundation through grant NSF IRI-9409580 to Vasant Honavar. 1 2 The Task Domain The task environment, proposed by Teller <ref> [9] </ref>, consists of an arena of N fi N squares strewn with M boxes in the inner (N 2) fi (N 2) grid. The arena is enclosed by impenetrable walls. <p> The mapping of network output to actions is shown in Table 1. Thus, output Unit 1 codes for the action (move vs turn), while Unit 2 determines the direction of the turn. The general framework for the simulation experiments was borrowed from <ref> [9] </ref>. Thus we used 6 fi 6 arenas, with 6 boxes randomly distributed in the inner 4 fi 4 grid. <p> This effect of increased memory is demonstrated in Figure 7. In our simulations the robot is allowed 80 time steps within which it is to clear the arena. This choice was based on an O (N 2 ) simulation time proposed by Teller <ref> [9] </ref>. However, a little deliberation shows that even under idealistic circumstances the robot needs O (N ) time for each of the M = O (N 2 ) boxes, leading to an O (N 3 ) time requirement.
References-found: 9

