URL: http://www.math.rutgers.edu/~sontag/FTP_DIR/net-ident.ps.gz
Refering-URL: http://www.math.rutgers.edu/~sontag/papers.html
Root-URL: 
Email: E-mail: albertin@pdmat1.unipd.it, sontag@hilbert.rutgers.edu  
Title: FOR NEURAL NETWORKS, FUNCTION DETERMINES FORM  
Author: Francesca Albertini(*) Eduardo D. Sontag 
Note: Research supported in part by US Air Force Grant AFOSR-91-0343, and in part by an INDAM  F. Severi) fellowship. Running head: FUNCTION DETERMINES FORM  
Address: New Brunswick, NJ 08903  Via Belzoni 7, 35100 Padova, Italy.  
Affiliation: Department of Mathematics Rutgers University,  (*)Also: Universita' degli Studi di Padova, Dipartimento di Matematica Pura ed Applicata,  (Istituto Nazionale di Alta Matematica  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Albertini, F., and E.D. Sontag. </author> <year> (1992). </year> <title> Identifiability of discrete-time neural networks. </title> <type> Preprint. </type>
Reference-contexts: Here one may expect a connection to optimal Hankel approxi mation as well as other linear control theory issues. * Design algorithms for parameter identification using the techniques introduced here. An analogous result for discrete-time networks has been recently obtained; see <ref> [1] </ref>. 29
Reference: [2] <author> Cleeremans, A., D. Servan-Schreiber, and J.L. McClelland. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 372-381. </pages>
Reference-contexts: called, are used as models whose parameters are fit to input/output data. (The purpose may be to use these models instead of a real plant, for purposes of control, or for predictive purposes.) This is done, for instance, in certain approaches to grammatical inference and speech processing; see for instance <ref> [2] </ref>, [11]. Typically, gradient descent algorithms are used in order to fit parameters thorugh the minimization of an error functional that penalizes mismatches between the desired outputs and those that a candidate net produces (the term "continuous backpropagation" is sometimes used for the gradient descent procedure).
Reference: [3] <author> Cohen, M.A., and S. Grossberg. </author> <year> (1983). </year> <title> Absolute stability of global pattern formation and parallel memory storage by competitive neural networks. </title> <journal> IEEE Trans. Systems, Man, and Cybernetics, </journal> <volume> 13, </volume> <pages> 815-826. </pages>
Reference-contexts: 1 Introduction Many recent papers have explored the computational and dynamical properties of systems of interconnected "neurons." For instance, Hopfield ([7]), Cowan ([4]), and Grossberg and his school (see e.g. <ref> [3] </ref>), have all studied devices that can be modelled by sets of nonlinear differential equations such as _x i (t) = x i (t) + @ j=1 m X b ij u j (t) A ; i = 1; : : :; n ; (1) _x i (t) = x i
Reference: [4] <author> Cowan, J.D.. </author> <year> (1968). </year> <title> Statistical mechanics of neural nets. In E.R. </title> <editor> Caianiello (ed.), </editor> <booktitle> Neural Networks. </booktitle> <address> Berlin: </address> <publisher> Springer, </publisher> <pages> pp. 181-188. </pages>
Reference: [5] <author> Hecht-Nielsen, R.. </author> <title> (1989) Theory of the backpropagation neural network. </title> <booktitle> In Proceedings of the Int. Joint Conf. on Neural Networks, </booktitle> <address> Washington, </address> <publisher> IEEE Publications, NY, </publisher> <pages> 593-605. </pages>
Reference-contexts: For precisely the above reasons, but restricted to the particular case of feedforward (that is, nondynamic) nets, the question of deciding if the only possible symmetries are indeed the ones that we find was asked by Hecht-Nielsen in <ref> [5] </ref>.
Reference: [6] <author> Hirsch, M.W.. </author> <year> (1989). </year> <title> Convergent activation dynamics in continuous-time networks. </title> <booktitle> Neural Networks, </booktitle> <volume> 2, </volume> <pages> 331-349. </pages>
Reference-contexts: [14]) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([15]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example <ref> [6] </ref>, [9], and references there.
Reference: [7] <author> Hopfield, J.J.. </author> <year> (1984). </year> <title> Neurons with graded responses have collective computational properties like those of two-state neurons. </title> <journal> Proc. of the Natl. Acad. of Sciences, USA, </journal> <volume> 81, </volume> <pages> 3088-3092. </pages>
Reference-contexts: Electrical circuit implementations of these equations, employing resistively connected networks of n identical nonlinear amplifiers, and adjusting the resistor characteristics to obtain the desired weights, have been proposed as models of analog computers, in particular in the context of constraint satisfaction problems and in content-addressable memory applications (see e.g. <ref> [7] </ref>). We also assume given a certain number p of probes, or measurement devices, whose outputs signal to the environment the collective response of the net to the stimuli presented in the channels u i . Each such device averages the activation values of many neurons.
Reference: [8] <author> Isidori, A.. </author> <year> (1985). </year> <title> Nonlinear Control Systems: An Introduction. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in <ref> [8] </ref>, [10], [19], can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [9] <author> Michel, A.N., J.A. Farrell, and W. Porod. </author> <title> (1989) Qualitative analysis of neural networks. </title> <journal> IEEE Trans. Circuits and Sys., </journal> <volume> 36, </volume> <pages> 229-243. </pages>
Reference-contexts: when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([15]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example [6], <ref> [9] </ref>, and references there.
Reference: [10] <author> Nijmeijer, H., and A.V. Van der Schaft. </author> <title> (1990) Nonlinear Dynamical Control Systems. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in [8], <ref> [10] </ref>, [19], can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [11] <author> Robinson, A.J., and F. Fallside. </author> <year> (1988). </year> <title> Static and dynamic error propagation networks with application to speech coding. </title> <editor> In D.Z. Anderson (ed.), </editor> <booktitle> Neural Information Processing Systems. </booktitle> <address> New York: </address> <publisher> American Institute of Physics, </publisher> <pages> pp. 632-641. </pages>
Reference-contexts: are used as models whose parameters are fit to input/output data. (The purpose may be to use these models instead of a real plant, for purposes of control, or for predictive purposes.) This is done, for instance, in certain approaches to grammatical inference and speech processing; see for instance [2], <ref> [11] </ref>. Typically, gradient descent algorithms are used in order to fit parameters thorugh the minimization of an error functional that penalizes mismatches between the desired outputs and those that a candidate net produces (the term "continuous backpropagation" is sometimes used for the gradient descent procedure).
Reference: [12] <editor> Schwarzschild, R., and E.D. Sontag. </editor> <booktitle> (1991) Algebraic theory of sign-linear systems. Proceedings of the Automatic Control Conference, </booktitle> <address> Boston, MA, </address> <pages> 799-804. </pages>
Reference-contexts: Moreover, for suitably sharp nonlinearities , they are approximate models of discontinuous equations such as _x = sign (Ax+ Bu). (See <ref> [12] </ref> for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see [13], [14]) when all weights are rational numbers, and a general model of analog
Reference: [13] <author> Siegelmann, H.T., and E.D. Sontag. </author> <year> (1991). </year> <title> Turing computability with neural nets. </title> <journal> Appl. Math. Lett., </journal> <volume> 4(6), </volume> <pages> 77-80. </pages>
Reference-contexts: approximate models of discontinuous equations such as _x = sign (Ax+ Bu). (See [12] for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see <ref> [13] </ref>, [14]) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([15]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example
Reference: [14] <editor> Siegelmann, H.T., and E.D. Sontag. </editor> <booktitle> (1992). On the computational power of neural nets. In Proc. Fifth ACM Workshop on Computational Learning Theory, </booktitle> <address> Pittsburgh, </address> <month> July </month> <year> 1992, </year> <pages> 440-449. </pages>
Reference-contexts: models of discontinuous equations such as _x = sign (Ax+ Bu). (See [12] for related work on systems that mix linear dynamics and sign functions.) In discrete-time, systems of the type (7) have been recently shown to be at least as powerful as any possible digital computational device (see [13], <ref> [14] </ref>) when all weights are rational numbers, and a general model of analog computers when the weights are allowed to be real ([15]). 1.1 Uniqueness of Weights Stability properties, memory capacity, and other characteristics of the above types of systems have been thoroughly investigated by many authors; see for example [6],
Reference: [15] <author> Siegelmann, H.T., and E.D. Sontag. </author> <title> Analog computation, neural networks, and circuits. </title> <note> Submitted. </note>
Reference: [16] <author> Sontag, E.D.. </author> <year> (1979). </year> <title> On the observability of polynomial systems. </title> <journal> SIAM J.Control and Opt., </journal> <volume> 17, </volume> <pages> 139-151. </pages>
Reference-contexts: so it is fair to say the response of the net to a "random" input will suffice, at least theoretically, for determination of the number of units and unique identification of all weights.) The proof of the above Theorem is immediate from the general results for control systems given in <ref> [16] </ref> and [18], which imply that identifiability is equivalent to "single experiment" identifiability, for systems defined by analytic differential equations and depending analytically on parameters (here, the weights).
Reference: [17] <author> Sontag, E.D.. </author> <year> (1990). </year> <title> Mathematical Control Theory: Deterministic Finite Dimensional Systems. </title> <address> New York: </address> <publisher> Springer. </publisher>
Reference-contexts: In the very special case when is the identity, classical linear realization theory |see for instance <ref> [17] </ref>, Chapter 5| implies that, generically, the triple (A; B; C) is determined only up to an invertible change of variables in the state space. <p> For the systems of interest in neural network theory, f (x; u) is always uniformly Lipschitz with respect to x, so " = T . (All results that we use on existence of solutions and continous dependence are included in standard texts such as <ref> [17] </ref>.) For each control, we let (u) = (u) be the output function corresponding to the initial state x (0) = 0, that is, (u)(t) := h ((t; 0; u)) ; defined at least on some interval [0; "). <p> f (x; u) = Dx + ~(Ax + Bu) + Gu (19) for some matrices A 2 R nfin ; D 2 R nfin ; B 2 R nfim ; G 2 R nfim ; and C 2 R pfin : These are continuous time systems in the sense of <ref> [17] </ref>. We will call such a system a system, and denote it by = (D; A; B; G; C) . Observe that in the special case in which is the identity, or more generally is linear, we have a linear system in the usual sense. <p> The following is a well-known formula (see e.g. <ref> [17] </ref> page 210): @ k fi fi fi y i L X i u 1 j (x i 13 for all x 0 2 IR n , where L X h denotes the Lie-derivative of the function h along the vector field X. <p> n;m;p consisting of those triples (A; B; C) which are canonical, i.e. observable: rank [C T ; A T C T ; : : : ; (A T ) n1 C T ] = n and controllable: rank [B; AB; : : : ; A n1 B] = n; see <ref> [17] </ref>, section 5.5. This is a generic, in the sense of the introduction, subset of S n;m;p , for each n; m, and p. Proposition 5.5 Assume 1 and 2 are i/o equivalent. <p> in this case (D = diagonal, and G = 0), the assumption rank [A; B] = n is redundant, as it follows from controllability of the pair (A + ffI; B), or equivalently, of the pair (A; B); this is just the case = ff of the Hautus condition (c.f. <ref> [17] </ref>, Lemma 3.3.7). 5.4.2 When D is diagonal and B = 0 Assume that we fix again an ff 2 IR, and this time we restrict our attention to the subclass of systems of the form: = (ffI; A; 0; G; C) where ff is this fixed real number (the same <p> Proof. Given an admissible control u (), we can find a sequence u n () 2 A such that the controls u n () are equibounded and converge to u () almost everywhere. Now we need only apply the approximation results in Theorem 1 of <ref> [17] </ref> to conclude the desired result. Let 1 and 2 be two systems of type (56), and ~ i for i = 1; 2 their corresponding systems of type (57). Proposition 5.12 If 1 and 2 are i/o equivalent, then ~ 1 and ~ 2 are also i/o equivalent. Proof.
Reference: [18] <author> Sussmann, H.J.. </author> <year> (1972). </year> <title> Single-input observability of continuous-time systems. </title> <journal> Math. Systems Theory, </journal> <volume> 12, </volume> <pages> 371-393. </pages>
Reference-contexts: is fair to say the response of the net to a "random" input will suffice, at least theoretically, for determination of the number of units and unique identification of all weights.) The proof of the above Theorem is immediate from the general results for control systems given in [16] and <ref> [18] </ref>, which imply that identifiability is equivalent to "single experiment" identifiability, for systems defined by analytic differential equations and depending analytically on parameters (here, the weights).
Reference: [19] <author> Sussmann, H.J.. </author> <year> (1977). </year> <title> Existence and uniqueness of minimal realizations of nonlinear systems. </title> <journal> Math. Sys. Theory, </journal> <volume> 10, </volume> <pages> 263-284. 30 </pages>
Reference-contexts: In this sense, structure (weights) is uniquely determined by function (desired i/o behavior). 1.4 Remarks Note that nonlinear realization theory, as described for instance in [8], [10], <ref> [19] </ref>, can be also applied to the problem considered here. This theory would allow us to conclude that, under suitable assumptions of controllability and observability, there is some abstract diffeomorphism which relates two networks having the same i/o behavior.
Reference: [20] <author> Sussmann, H.J.. </author> <title> Uniqueness of the weights for minimal feedforward nets with a given input-output map. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 589-593. 31 </pages>
Reference-contexts: The question was partially answered (for so-called "single-hidden layer" nets, and using a particular activation function) by Sussmann in <ref> [20] </ref>, who established a uniqueness result which, in our setting, would apply to systems of the special type _x = ~(Bu); y = Cx, with = tanh (x). (That is, there is no "A" matrix; the result does allow for a constant bias vector inside the sigmoid, however.
References-found: 20

