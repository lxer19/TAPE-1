URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers/topicTR.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/user/kseymore/html/papers.html
Root-URL: 
Title: Large-scale Topic Detection And Language Model Adaptation  
Author: Kristie Seymore Ronald Rosenfeld 
Note: The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the U.S. Government or the National Science Foundation.  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: June 1997  
Pubnum: CMU-CS-97-152  
Abstract: This research was sponsored by the Department of the Navy, Naval Research Laboratory under Grant No. N00014-93-1-2005, the National Security Agency under Grant numbers MDA904-96- 1-0113 and MDA904-97-1-0006, and under a National Science Foundation Graduate Research Fellowship. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Carlson. </author> <title> Unsupervised topic clustering of switchboard speech messages. </title> <booktitle> In Proceedings of ICASSP-96, </booktitle> <pages> pages 315318, </pages> <year> 1996. </year>
Reference-contexts: Language models built at various nodes along the active paths can be combined to best model the current document. The construction of topic trees has been explored in the Switchboard domain by Carlson <ref> [1] </ref>. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework [2, 3].
Reference: [2] <author> R. Iyer and M. Ostendorf. </author> <title> Modeling long distance dependence in language: Topic mixtures vs. dynamic cache models. </title> <booktitle> In Proceedings of ICSLP, </booktitle> <pages> pages 236239, </pages> <year> 1996. </year>
Reference-contexts: Language models built at various nodes along the active paths can be combined to best model the current document. The construction of topic trees has been explored in the Switchboard domain by Carlson [1]. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [2, 3] </ref>.
Reference: [3] <author> P. Clarkson and A. Robinson. </author> <title> Language model adaptation using mixtures and an exponentially decaying cache. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <pages> pages 799802, </pages> <year> 1997. </year>
Reference-contexts: Language models built at various nodes along the active paths can be combined to best model the current document. The construction of topic trees has been explored in the Switchboard domain by Carlson [1]. Agglomerative clustering has been used successfully for topic adaptation in a mixture modeling framework <ref> [2, 3] </ref>.
Reference: [4] <author> G. Salton. </author> <title> Developments in automatic text retrieval. </title> <booktitle> Science, </booktitle> <address> 253:974980, </address> <year> 1991. </year>
Reference-contexts: As long as the word errors in the hypothesis are not significantly topic-correlated, the correct content words in the hypothesis will provide enough evidence for the selection of appropriate clusters. 2.2.1 The TFIDF Classifier The TFIDF measure <ref> [4] </ref> assigns a weight to each unique word in a document representing how topic-specific that word is to its document or cluster.
Reference: [5] <author> T. Imai, R. Schwartz, F. Kubala, and L. Nguyen. </author> <title> Improved topic discrimi-nation of broadcast news using a model of multiple simultaneous topics. </title> <booktitle> In Proceedings of ICASSP-97, </booktitle> <pages> pages 727730, </pages> <year> 1997. </year>
Reference-contexts: Imai et al. have developed a Hidden Markov Model system for topic detection which identifies multiple topics per story and considers that each word in the story need not be related to all of the story's topics <ref> [5] </ref>. Joachims analyzes several topic detection algorithms, including TFIDF and the naive Bayes classifier, in [6]. 2.3 Language Models In the speech recognition paradigm, each time a new story is decoded an initial hypothesis transcription is produced.
Reference: [6] <author> T. Joachims. </author> <title> A probabilistic analysis of the rocchio algorithm with TFIDF for text categorization. </title> <type> Technical Report CMU-CS-96-118, </type> <institution> Carnegie Mellon University, </institution> <month> March </month> <year> 1996. </year> <month> 15 </month>
Reference-contexts: Joachims analyzes several topic detection algorithms, including TFIDF and the naive Bayes classifier, in <ref> [6] </ref>. 2.3 Language Models In the speech recognition paradigm, each time a new story is decoded an initial hypothesis transcription is produced. We then feed the hypothesis transcription to the classifier, which chooses the most similar topic clusters.
Reference: [7] <author> Slava M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> ASSP-35(3):400401, </volume> <month> March </month> <year> 1987. </year>
Reference-contexts: We then feed the hypothesis transcription to the classifier, which chooses the most similar topic clusters. Language models are built from the text in each of the selected clusters. Here, Good-Turing discounted trigram backoff models <ref> [7] </ref> using all bigrams and trigrams (no cutoffs) were built with the CMU Statistical Language Modeling toolkit [8]. 2.4 Model Interpolation The individual language models built from the chosen clusters (or from nodes farther up in the tree when a topic tree is being used) are interpolated together at the word
Reference: [8] <author> Ronald Rosenfeld. </author> <title> The CMU statistical language modeling toolkit and its use in the 1994 ARPA CSR evaluation. </title> <booktitle> In Proceedings of the Spoken Language Systems Technology Workshop, </booktitle> <pages> pages 4750, </pages> <address> Austin, Texas, </address> <month> January </month> <year> 1995. </year> <note> [9] http://www.thomson.com/psmedia/bnews.html. </note>
Reference-contexts: Language models are built from the text in each of the selected clusters. Here, Good-Turing discounted trigram backoff models [7] using all bigrams and trigrams (no cutoffs) were built with the CMU Statistical Language Modeling toolkit <ref> [8] </ref>. 2.4 Model Interpolation The individual language models built from the chosen clusters (or from nodes farther up in the tree when a topic tree is being used) are interpolated together at the word level to produce a new language score, as in Equation 5. p new (w i j w
Reference: [10] <author> I. Witten, A. Moffat, and T. Bell. </author> <title> Managing Gigabytes: Compressing and Indexing Documents and Images. </title> <publisher> Van Nostrand Reinhold, </publisher> <year> 1994. </year>
Reference-contexts: The "correct" topics for each test story were the manually assigned keywords that accompanied each story that were also found among the 5883 leaf clusters. Precision and recall results at 5, 10 and 20 were calculated as in <ref> [10] </ref> and are shown in Table 2. For this task, the naive Bayes classifier outperforms the TFIDF classifier across all three levels of precision and recall.
Reference: [11] <editor> P. Placeway et al. </editor> <booktitle> The 1996 Hub-4 Sphinx-3 system. In Proceedings of the 1997 ARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year>
Reference-contexts: The largest story from the Hub4 development set consists of 2131 words and discusses suspicions of drug use by Chinese swimmers during the 1996 Olympics. The correct story transcript and the errorful first-pass Sphinx III <ref> [11] </ref> recognition hypotheses for this story (45% WER) were classified using both the TFIDF measure and the naive Bayes classifier. The 10 most similar clusters chosen by the TFIDF measure for the correct and errorful transcripts are shown in Table 3.
Reference: [12] <author> K. Seymore, S. Chen, M. Eskenazi, and R. Rosenfeld. </author> <booktitle> Language and pro-nunciation modeling in the CMU 1996 Hub 4 Evaluation. In Proceedings of the 1997 ARPA Speech Recognition Workshop, </booktitle> <year> 1997. </year> <month> 16 </month>
Reference-contexts: Rescoring consisted of using the original acoustic score, the new language model score, and a word insertion penalty. For the development set, N = 500, and the for the evaluation set, N = 200. Filled pauses were predicted from manually set unigram probabilities <ref> [12] </ref>. For the development set, the first-pass WER with no rescoring was 40.2%. The lowest N-best WER, found by using the reference transcripts to choose the N-best hypotheses with the lowest error, was 34.6%. The lowest N-best WER represents an upper bound on the performance of N-best rescoring. <p> Adaptation on the evaluation set with Bayes-chosen leaves results in only a 0.1% decrease in WER. For both the development and evaluation sets, rescoring with a Kneser-Ney smoothed general trigram model (as opposed to our Good-Turing smoothed general model) results in a lower WER than the topic models <ref> [12] </ref>. The Kneser-Ney model results in a WER of 39.4% on the development set and 34.9% on the evaluation set.
References-found: 11

