URL: http://wwwipd.ira.uka.de/~prechelt/Biblio/cupit2_tr97.ps.gz
Refering-URL: 
Root-URL: 
Email: (hopp@ira.uka.de)  (prechelt@ira.uka.de)  ++49/721/608-f3972,4068g  
Title: CuPit-2 A Parallel Language for Neural Algorithms: Language Reference and Tutorial  
Author: Holger Hopp Lutz Prechelt 
Date: March 3, 1997  4/97  
Address: 76128 Karlsruhe, Germany  
Affiliation: Institut fur Programmstrukturen und Datenorganisation Universitat Karlsruhe  
Pubnum: Technical Report  
Abstract: and load balancing even for irregular neural networks. The idea to achieve these goals lies in the programming model: CuPit-2 programs are object-centered, with connections and nodes of a graph (which is the neural network) being the objects. Algorithms are based on parallel local computations in the nodes and connections and communication along the connections (plus broadcast and reduction operations). This report describes the design considerations and the resulting language definition and discusses in detail a tutorial example program. This CuPit-2 language manual and tutorial is an updated version of the original CuPit language manual [Pre94]. The new language CuPit-2 differs from the original CuPit in several ways. All language changes from CuPit to CuPit-2 are listed in the appendix. 
Abstract-found: 1
Intro-found: 1
Reference: [AR88] <editor> J.A. Anderson and E. Rosenfeld, editors. Neurocomputing: </editor> <booktitle> Foundations of Research. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1988. </year>
Reference: [GHL + 92] <author> Robert W. Gray, Vincent P. Heuring, Steven P. Levi, Anthony M. Sloane, and William M. Waite. Eli: </author> <title> A complete, flexible compiler construction system. </title> <journal> Communications of the ACM, </journal> <volume> 35(2) </volume> <pages> 121-131, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: In order to fulfill purpose (c), the report contains a complete syntax specification of CuPit-2 in the notation of the Eli <ref> [GHL + 92] </ref> compiler construction system. This specification is typeset with FunnelWeb [Wil92] and can automatically be extracted and used to generate the front-end of the CuPit-2 compiler. 1 Purpose and scope of the language Before anything else, it is necessary to describe what the scope of CuPit-2 is. <p> CupitProgram: CupitParts. CupitParts: /* nothing */ / CupitParts CupitPart ';'. CupitPart: TypeDef / DataObjectDef / 79 ProcedureDef / FunctionDef / IOStyleDef / IOSpecDef / ReductionFunctionDef / WtaFunctionDef. g This macro is invoked in definition 114. All these definitions are now put into the Eli <ref> [GHL + 92] </ref> grammar specification file grammar.con: grammar.con [114] 114 f Cupit Program [113] Type Definition [55] Subroutine Definition [67] IO Definition [73] Data Object Definition [66] Statement [78] Expression [93] g This macro is attached to an output file. 16 Basic syntactic elements All keywords and operators in a CuPit-2 <p> This compiler gererates MPL, which is MasPar's data parallel C variant. The compilers are fully functional and is implemented using the Eli compiler construction system <ref> [GHL + 92] </ref>. The compiler source code is written as a FunnelWeb literate programming document. This means the source code is available as a well-structured 300 page document with table of contents, global keyword index, and interspersed documentation text [Pre95].
Reference: [Hoa83] <author> C. A. R. Hoare. </author> <title> Hints on programming language design. </title> <editor> In Ellis Horowitz, editor, </editor> <booktitle> Programming Languages: A Grand Tour, </booktitle> <pages> pages 31-40. </pages> <publisher> Springer Verlag, </publisher> <year> 1983. </year>
Reference-contexts: hierarchies occur in most neural network learning algorithms. process we use the concept taxonomy, which is shown in figure 2, plus the following general idea: A programming language should allow to model a problem and its solution in natural terms in order to ease understanding of the program (see e.g. <ref> [Hoa83, Wir83] </ref>). Thus, a program that defines a problem-adequate solution to the backprop learning problem must explicitly map the above concepts onto program entities and should provide appropriate abstractions that allow to use these concepts in a problem-oriented way.
Reference: [HP97] <author> H. Hopp and L. Prechelt. CuPit-2: </author> <title> A portable parallel programming language for artificial neural networks. </title> <booktitle> In 15th IMACS World Congress, Organized Session on Simulation of Artificial Neural Networks, </booktitle> <address> Berlin, Germany, </address> <month> August </month> <year> 1997. </year> <note> to appear. </note>
Reference-contexts: For simple feed-forward algorithms (backprop, rprop) the performance of sequential code is comparable to the performance of a simulator like SNNS [ZMV + 95]. The parallel performance of the SMP version is good for example parallelism, for node parallelism large networks are needed <ref> [HP97] </ref>. The CuPit-2 compilers can be used like other UNIX compilers and have most of the standard compiler options. The distribution contains a small library with I/O procedures, activation functions, timer functions, procedures for reading patterns in two formats (internal and SNNS), reading/writing of networks in SNNS format, etc.
Reference: [KR77] <author> Brian W. Kernighan and Dennis M. Ritchie. </author> <title> The C Programming Language. </title> <publisher> Prentice Hall, </publisher> <year> 1977. </year>
Reference-contexts: to informally describe the semantics of language constructs on different levels of detail, preciseness, and "intuitiveness"; these techniques are not orthogonal: 1. natural language description 2. example application 3. case discrimination 4. equivalent CuPit-2 program fragments 5. reference to well-known other programming languages, especially Modula-2 [Wir85], Ada [Uni81], and C <ref> [KR77] </ref> Given these techniques of description, it is impossible to guarantee that the language definition is precise enough and complete enough to be unambiguous for practical purposes. If you find any aspects of the language that are not made sufficiently clear in this report, please contact the author.
Reference: [MP43] <author> Warren McCulloch and Walter Pitts. </author> <title> A logical calculus of ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year> <note> Reprinted in [AR88]. </note>
Reference-contexts: These special categories of data types also motivate the name of the language: CuPit-2 is named after Warren McCulloch and Walter Pitts, who published the first description of a formal neuron in 1943 <ref> [MP43] </ref>. 1 Or at least: that explains most constructs and is not broken by the others 4.3 Program structure and parallelism 9 4.3 Program structure and parallelism The overall structure of a CuPit-2 program is as follows: The main program consists of a procedure that is executed sequentially.
Reference: [Pre94] <author> Lutz Prechelt. </author> <title> CuPit | a parallel language for neural algorithms: Language reference and tutorial. </title> <type> Technical Report 4/94, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <note> Jan-uary 1994. Anonymous FTP: /pub/papers/techreports/1994/1994-4.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: Element Enum Enumeration Expr Expression Func Function Id, Ident Identifier, Identification iff if and only if Init Initialization, Initializer Obj Object Op Operator, Operation Opt Optional Param Parameter Proc Procedure Stmt Statement Val Value B Language changes from CuPit to CuPit-2 The new language CuPit-2 differs from the original CuPit <ref> [Pre94] </ref> in several ways, especially: * The connection type definition now must contain the connected node interfaces (i. e. node type and interface name).
Reference: [Pre95] <author> Lutz Prechelt. </author> <title> The CuPit compiler for the MasPar | a literate programming document. </title> <type> Technical Report 1/95, </type> <institution> Fakultat fur Informatik, Universitat Karlsruhe, Germany, </institution> <month> January </month> <year> 1995. </year> <note> Anonymous FTP: /pub/papers/techreports/1995/1995-1.ps.gz on ftp.ira.uka.de. </note>
Reference-contexts: IO Style Definition [74] 74 f IOStyleDef: 5 Templates are pieces of source code that are directly inserted into the generated C code by the CuPit-2 compiler. For a detailed description of the template method used in the current CuPit-2 compiler refer <ref> [Pre95] </ref>. 10.2 The I/O style definition 57 'IOSTYLE' NewIOStyleId 'IS' IOStyleDefBody 'END' OptIOSTYLE. IOStyleDefBody: IOStyleDescription IOKinds. NewIOStyleId: LowercaseIdent. IOStyleDescription: 'CASE' IOStyleDefType / IOStyleDescription 'CASE' IOStyleDefType. IOStyleDefType: TypeCase ':' IOStyleList ';' OptIOParams OptSEMICOLON. TypeCase: TypeCaseId. TypeCaseId: 'CONNECTION' / 'NODE' / 'NODE GROUP' / 'NETWORK'. <p> The compiler source code is written as a FunnelWeb literate programming document. This means the source code is available as a well-structured 300 page document with table of contents, global keyword index, and interspersed documentation text <ref> [Pre95] </ref>. F Availability of compilers and literature All CuPit-2 compilers, all literature about CuPit-2 can be fetched from the CuPit-2 web page http://wwwipd.ira.uka.de/~hopp/cupit.html.
Reference: [RB93] <author> Martin Riedmiller and Heinrich Braun. </author> <title> A direct adaptive method for faster backpropagation learning: The RPROP algorithm. </title> <booktitle> In Proc. of the IEEE Int. Conf. on Neural Networks, </booktitle> <address> San Francisco, CA, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: The original reference to backpropagation is probably [Wer74], the most often-cited one is chapter 8 of [RM86], which also contains a quite nice mathematical derivation of the algorithm. We will extend the original algorithm by the RPROP rule for the computation of learning step sizes at each weight <ref> [RB93] </ref> and by a simple connection elimination scheme. Backprop is a so-called supervised learning algorithm. This means that it learns from examples. Each example consists of n input values and m output values, all in the range 1 : : : 1. <p> In order to exploit example parallelism (which is sensible on parallel machines only) we introduce the "replication and merge"-method in the next section. In fact, we do not derive a program for "vanilla" backpropagation, but for the RPROP Backpropagation variant <ref> [RB93] </ref>.
Reference: [RM86] <editor> David Rumelhart and John McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The original reference to backpropagation is probably [Wer74], the most often-cited one is chapter 8 of <ref> [RM86] </ref>, which also contains a quite nice mathematical derivation of the algorithm. We will extend the original algorithm by the RPROP rule for the computation of learning step sizes at each weight [RB93] and by a simple connection elimination scheme. Backprop is a so-called supervised learning algorithm.
Reference: [Uni81] <institution> United States Department of Defense, </institution> <address> Springer Verlag, </address> <booktitle> Lecture Notes in Computer Science 106. The Programming Language Ada, </booktitle> <year> 1981. </year>
Reference-contexts: A program that contains this kind of error is called erroneous . The behavior of an erroneous program is completely undefined. This definition of error categories is similar to that used in Ada <ref> [Uni81] </ref>. 5 About this report This report does not contain any kind of complete formal description of the CuPit-2 semantics. Instead, it partly relies on the intuitive understanding of the CuPit-2 semantics, based on the reader's (assumed) familiarity with other programming languages. <p> used in order to informally describe the semantics of language constructs on different levels of detail, preciseness, and "intuitiveness"; these techniques are not orthogonal: 1. natural language description 2. example application 3. case discrimination 4. equivalent CuPit-2 program fragments 5. reference to well-known other programming languages, especially Modula-2 [Wir85], Ada <ref> [Uni81] </ref>, and C [KR77] Given these techniques of description, it is impossible to guarantee that the language definition is precise enough and complete enough to be unambiguous for practical purposes.
Reference: [Wer74] <author> Paul Werbos. </author> <title> Beyond Regression: New Tools for Prediction and Analysis in the Behavioral Sciences. </title> <type> PhD thesis, </type> <institution> Harvard University, </institution> <year> 1974. </year>
Reference-contexts: The original reference to backpropagation is probably <ref> [Wer74] </ref>, the most often-cited one is chapter 8 of [RM86], which also contains a quite nice mathematical derivation of the algorithm. We will extend the original algorithm by the RPROP rule for the computation of learning step sizes at each weight [RB93] and by a simple connection elimination scheme.
Reference: [Wil92] <author> Ross N. Williams. </author> <note> FunnelWeb User's Manual, version 1.0 for funnelweb 3.0 edition, </note> <month> May </month> <year> 1992. </year>
Reference-contexts: In order to fulfill purpose (c), the report contains a complete syntax specification of CuPit-2 in the notation of the Eli [GHL + 92] compiler construction system. This specification is typeset with FunnelWeb <ref> [Wil92] </ref> and can automatically be extracted and used to generate the front-end of the CuPit-2 compiler. 1 Purpose and scope of the language Before anything else, it is necessary to describe what the scope of CuPit-2 is.
Reference: [Wir83] <author> Niklaus Wirth. </author> <title> On the design of programming languages. </title> <editor> In Ellis Horowitz, editor, </editor> <booktitle> Programming Languages: A Grand Tour, </booktitle> <pages> pages 23-30. </pages> <publisher> Springer Verlag, </publisher> <year> 1983. </year>
Reference-contexts: hierarchies occur in most neural network learning algorithms. process we use the concept taxonomy, which is shown in figure 2, plus the following general idea: A programming language should allow to model a problem and its solution in natural terms in order to ease understanding of the program (see e.g. <ref> [Hoa83, Wir83] </ref>). Thus, a program that defines a problem-adequate solution to the backprop learning problem must explicitly map the above concepts onto program entities and should provide appropriate abstractions that allow to use these concepts in a problem-oriented way.
Reference: [Wir85] <author> Niklaus Wirth. </author> <title> Programming in Modula-2. </title> <publisher> Springer Verlag, </publisher> <year> 1985. </year>
Reference-contexts: techniques are used in order to informally describe the semantics of language constructs on different levels of detail, preciseness, and "intuitiveness"; these techniques are not orthogonal: 1. natural language description 2. example application 3. case discrimination 4. equivalent CuPit-2 program fragments 5. reference to well-known other programming languages, especially Modula-2 <ref> [Wir85] </ref>, Ada [Uni81], and C [KR77] Given these techniques of description, it is impossible to guarantee that the language definition is precise enough and complete enough to be unambiguous for practical purposes.

References-found: 15

