URL: ftp://ftp.cs.wisc.edu/machine-learning/shavlik-group/shavlik.tr92.ps
Refering-URL: http://www.cs.wisc.edu/~shavlik/abstracts/shavlik.tr92.ps.abstract.html
Root-URL: 
Email: shavlik@cs.wisc.edu  
Title: Framework for Combining Symbolic and Neural Learning rule extraction from neural networks the KBANN algorithm
Author: Jude W. Shavlik 
Keyword: knowledge-based neural networks theory refinement  
Address: Wisconsin Madison  
Affiliation: Computer Sciences Department University of  
Note: A  use of prior knowledge  
Abstract: Technical Report 1123, Computer Sciences Department, University of Wisconsin - Madison, Nov. 1992 ABSTRACT This article describes an approach to combining symbolic and connectionist approaches to machine learning. A three-stage framework is presented and the research of several groups is reviewed with respect to this framework. The first stage involves the insertion of symbolic knowledge into neural networks, the second addresses the refinement of this prior knowledge in its neural representation, while the third concerns the extraction of the refined symbolic knowledge. Experimental results and open research issues are discussed. A shorter version of this paper will appear in Machine Learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atlas, L., Cole, R., Connor, J., El-Sharkawi, M., Marks II, R. , Muthusamy, Y., & Barnard, E. </author> <year> (1990). </year> <title> Performance comparisons between backpropagation networks and classification trees on three real-world applications. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 2), </volume> <editor> D. Touretzky (ed.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Abu-Mostafa, Y. S. </author> <year> (1990). </year> <title> Learning from hints in neural networks. </title> <journal> Journal of Complexity, </journal> <volume> 6, </volume> <pages> 192-198. </pages>
Reference: <author> Berenji, H. R. </author> <year> (1991). </year> <title> Refinement of approximate reasoning-based controllers by reinforcement learning. </title> <booktitle> Proceedings of the Eighth International Machine Learning Workshop (pp. </booktitle> <pages> 475-479), </pages> <address> Evanston, IL: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Cleermans, A., Servan-Schreiber, D., & McClelland, J. L. </author> <year> (1989). </year> <title> Finite state automata and simple recurrent networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <month> 372-381. </month> <title> Page 11 Combining Symbolic and Neural Learning Das, </title> <editor> S., Giles, C. L., & G. Z. </editor> <title> Sun (1993). Using hints to successfully learn context-free grammars with a neural network pushdown automaton. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 5), </volume> <editor> S. Hanson, </editor> <publisher> J. </publisher>
Reference: <editor> Cowans, & L. Giles (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Dietterich, T. G., Hild, H., & Bakiri, G. </author> <year> (1990). </year> <title> A comparative study of ID3 and backpropagation for English text-to-speech mapping. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 24-31), </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14, </volume> <pages> 179-211. </pages>
Reference: <author> Fisher, D. H. & McKusick, K. B. </author> <year> (1989). </year> <title> An empirical comparison of ID3 and back-propagation. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial CAIntelligence (pp. </booktitle> <pages> 788-793). </pages> <address> Detroit: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Frasconi, P., Gori, M., Maggini, M., & Soda, G. </author> <year> (1991). </year> <title> A unified approach for integrating explicit knowledge and learning by example in recurrent networks. </title> <booktitle> Proceedings of the International Joint Conference on Neural Networks, </booktitle> <pages> (pp. 811-816). </pages> <address> Seattle: </address> <publisher> IEEE Press. </publisher>
Reference: <author> Fu, L. M. </author> <year> (1989). </year> <title> Integration of neural heuristics into knowledge-based inference. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 325-340. </pages>
Reference: <author> Fu, L. M. </author> <year> (1991). </year> <title> Rule learning by searching on adapted nets. </title> <booktitle> Proceedings of the Ninth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 590-595). </pages> <address> Anaheim, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Gallant, S. I. </author> <year> (1988). </year> <title> Connectionist expert systems. </title> <journal> Communications of the ACM, </journal> <volume> 31, </volume> <pages> 152-169. </pages>
Reference: <author> Gemen, S., Bienenstock, E. & Doursat, R. </author> <year> (1992). </year> <title> Neural networks and the bias/variance dilemma. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 1-58. </pages>
Reference: <author> Giles, C., Sun, G., Chen, H., Lee, Y., & Chen, D. </author> <year> (1990). </year> <title> Higher order recurrent networks and grammatical inference. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 2), </volume> <editor> D. Touretzky (ed.), </editor> <address> San Mateo, </address> : <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Giles, C., Miller, C., Chen, D., Chen, H., Sun, G., & Lee, Y. </author> <year> (1992). </year> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 393-405. </pages>
Reference: <author> Hartigan, J. A. </author> <year> (1975). </year> <title> Clustering Algorithms, </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference-contexts: The first step - see the upper, middle panel in Figure 6 clusters the incoming weights of a unit, using a standard clustering algorithm <ref> (Hartigan, 1975) </ref>. The weights on the links in each cluster are then replaced by the average of the cluster's weights, which regularizes the network. Next, the algorithm analyzes each cluster to determine if any clusters are irrelevant.
Reference: <author> Hawley, D. K. & McClure, W. R. </author> <year> (1983). </year> <title> Compilation and analysis of Escherichia Coli promoter DNA sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 11, </volume> <pages> 2237-2255. </pages>
Reference: <author> Hayashi, Y. </author> <year> (1991). </year> <title> A neural expert system with automated extraction of fuzzy if-then rules and its application to medical diagnosis. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 3), </volume> <editor> R. Lippmann, J. Moody, </editor> & <address> D. </address>
Reference: <editor> Touretzky (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <editor> Hendler, J. A. (ed.) </editor> <year> (1989). </year> <journal> Special issue on hybrid systems (symbolic/connectionist). Connection Science, </journal> <volume> 1. </volume>
Reference: <author> Hinton, G. E. </author> <year> (1986). </year> <title> Learning distributed representations of concepts. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 1-12). </pages> <address> Amherst, MA: </address> <publisher> Erlbaum. </publisher>
Reference-contexts: One may wish to constrain weight changes to maintain the symbolic interpretation of the network (McMillan et al., 1992). Finally, networks often decay their weights toward zero during training <ref> (Hinton, 1986) </ref>. Weights in knowledge-based networks should decay toward their initial values (Hinton, personal communication; Tresp et al., 1993), thereby encouraging the network to preserve the knowledge in the initial domain theory. There are several open questions regarding the use of symbolic information to aid the refinement step. <p> Can one use symbolic information to focus the back-propagated error signal, especially in deep networks? Deep networks often occur when basing the network topology on the dependency structure of a rule base, so this problem is exacerbated in knowledge-based networks. Finally, do we need to prevent distributed representations <ref> (Hinton, 1986) </ref> from evolving during training? Since hidden units in knowledge-based networks initially have a symbolic meaning, it seems that Page 8 Combining Symbolic and Neural Learning distributed representations are undesirable; however, there could be some way to take advantage of distributed representations.
Reference: <author> Hinton, G. E. </author> <year> (1989). </year> <journal> Connectionist learning procedures Artificial Intelligence, </journal> <volume> 40, </volume> <pages> 185-234. </pages>
Reference-contexts: Opitz and Shavlik (1992) developed an algorithm that interprets networks symbolically to decide where to add new nodes. There have been several changes to standard connectionist learning motivated by symbolic problems. Rather than minimizing mean-squared error, the cross-entropy error function <ref> (Hinton, 1989) </ref> is a better choice for knowledge-based networks (see Towell (1992) for an explanation). Refining rules with certainty factors requires the use of a different activation function for nodes (Fu, 1989; Mahoney & Mooney, 1993).
Reference: <author> Hinton, G. E. (ed.) </author> <year> (1990). </year> <title> Special issue on connectionist symbol processing. </title> <journal> Artificial Intelligence, </journal> <volume> 46. </volume>
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3, </volume> <pages> 79-87. </pages>
Reference-contexts: Fu (1989) and Mahoney and Mooney (1993) map rules containing certainty factors. Berenji (1991) and Masuoka et al. (1990) map fuzzy-logic rules, while McMillan, Mozer, and Smolensky (1992) use gating networks <ref> (Jacobs et al., 1991) </ref> to map production rules. Scott et al. (1992) and Roscheisen et al. (1992) map mathematical equations, demonstrating that the KBANN approach does not require logic-oriented domain theories.
Reference: <author> Jordan, M. I. </author> <year> (1986). </year> <title> Attractor dynamics and parallelism in a connectionist sequential machine. </title> <booktitle> Proceedings of the Eighth Annual Conference of the Cognitive Science Society, </booktitle> <pages> (pp. 531-546), </pages> <address> Amherst, MA: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Jordan, M. I. & Rumelhart, D. E. </author> <year> (1992). </year> <title> Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference-contexts: Hence, it appears worthwhile to investigate using neural learning methods to produce and refine symbolic information. In addition, neural network approaches have proven successful on a wide range of "real world" tasks, such as speech understanding (Lippmann, 1989), handwritten-character recognition (Le Cun et al., 1989), control of dynamic systems <ref> (Jordan & Rumelhart, 1992) </ref>, gene finding (Uberbacher & Mural, 1991), and language learning (Touretzky, 1991). These experiments strongly suggest that connectionist learning is a powerful approach, and the use of neural networks with symbolic knowledge merits exploration.
Reference: <author> Le Cun, Y., Boser, B., Denker, J., Henderson, D., Howard, R., Hubbard, W., & Jackel, L. </author> <year> (1989). </year> <title> Backpropagation applied to handwritten zip code recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 541-551. </pages>
Reference-contexts: Hence, it appears worthwhile to investigate using neural learning methods to produce and refine symbolic information. In addition, neural network approaches have proven successful on a wide range of "real world" tasks, such as speech understanding (Lippmann, 1989), handwritten-character recognition <ref> (Le Cun et al., 1989) </ref>, control of dynamic systems (Jordan & Rumelhart, 1992), gene finding (Uberbacher & Mural, 1991), and language learning (Touretzky, 1991). These experiments strongly suggest that connectionist learning is a powerful approach, and the use of neural networks with symbolic knowledge merits exploration.
Reference: <author> Lippmann, R. P. </author> <year> (1989). </year> <title> Review of neural networks for speech recognition. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 1-38. </pages>
Reference-contexts: Hence, it appears worthwhile to investigate using neural learning methods to produce and refine symbolic information. In addition, neural network approaches have proven successful on a wide range of "real world" tasks, such as speech understanding <ref> (Lippmann, 1989) </ref>, handwritten-character recognition (Le Cun et al., 1989), control of dynamic systems (Jordan & Rumelhart, 1992), gene finding (Uberbacher & Mural, 1991), and language learning (Touretzky, 1991). These experiments strongly suggest that connectionist learning is a powerful approach, and the use of neural networks with symbolic knowledge merits exploration.
Reference: <author> Maclin, R. & Shavlik, J. W. </author> <title> (in press). Using knowledge-based neural networks to improve algorithms: Refining the Chou-Fasman algorithm for protein folding. </title> <booktitle> Machine Learning. </booktitle>
Reference: <author> Mahoney, J. J. & Mooney, R. J. </author> <year> (1993). </year> <title> Combining neural and symbolic learning to revise probabilistic rule bases. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 5), </volume> <editor> S. Hanson, J. Cowans, & L. Giles (eds.), </editor> <address> San Mateo, CA: </address> <note> Morgan Kaufmann. Page 12 Combining Symbolic and Neural Learning Masuoka, </note> <author> R., Watanabe, N., Kawamura, A., Owada, Y., & Asakawa, K. </author> <year> (1990). </year> <title> Neurofuzzy system fuzzy inference using a structured neural network. </title> <booktitle> Proceedings of the International Conference on Fuzzy Logic & Neural Networks (pp. </booktitle> <pages> 173-177), </pages> <address> Iizuka, Japan. </address>
Reference: <author> McMillan, C., Mozer, M. C., & Smolensky, P. </author> <year> (1992). </year> <title> Rule induction through integrated symbolic and subsymbolic processing. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 4), </volume> <editor> J. Moody, S. Hanson, & R. Lippmann (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Refining rules with certainty factors requires the use of a different activation function for nodes (Fu, 1989; Mahoney & Mooney, 1993). One may wish to constrain weight changes to maintain the symbolic interpretation of the network <ref> (McMillan et al., 1992) </ref>. Finally, networks often decay their weights toward zero during training (Hinton, 1986). Weights in knowledge-based networks should decay toward their initial values (Hinton, personal communication; Tresp et al., 1993), thereby encouraging the network to preserve the knowledge in the initial domain theory.
Reference: <author> Mitchell, T. M. & Thrun, S. </author> <year> (1993). </year> <title> Explanation-based neural network learning for robot control. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 5), </volume> <editor> S. Hanson, J. Cowans, & L. Giles (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mooney, R., Shavlik, J., Towell, G., & Gove, A. </author> <year> (1989). </year> <title> An experimental comparison of symbolic and connectionist learning algorithms. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 775-780). </pages> <address> Detroit: </address> <note> Morgan Kaufmann. (An extended version appeared in Machine Learning, 6, 111-143, 1991.) </note> <author> Mozer, M. C. & Das, S. </author> <year> (1993). </year> <title> A connectionist chunker that induces the structure of context-free languages. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 5), </volume> <editor> S. Hanson, J. Cowans, & L. Giles (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Muggleton, S. </author> <year> (1992). </year> <title> Inductive Logic Programming, </title> <publisher> London: Academic Press. </publisher>
Reference: <author> Noordewier, M. O., Towell, G. G., & Shavlik, J. W. </author> <year> (1991). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 3), </volume> <editor> R. Lippmann, J. Moody, & D. Touretzky (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Nowlan, S. J. & Hinton, G. E. </author> <year> (1992). </year> <title> Simplifying neural networks by soft weight-sharing. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 4), </volume> <editor> J. Moody, S. Hanson, & R. Lippmann (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Oliver, W. L. & Schneider, W. </author> <year> (1988). </year> <title> Using rules and task division to augment connectionist learning. </title> <booktitle> Proceedings of the Tenth Annual Conference of the Cognitive Science Society (pp. </booktitle> <pages> 55-61), </pages> <address> Montreal: </address> <publisher> Erlbaum. </publisher>
Reference: <author> Omlin, C. W. & Giles, C. L. </author> <year> (1992). </year> <title> Training second-order recurrent neural networks using hints. </title> <booktitle> Proceedings of the Ninth International Conference on Machine Learning (pp. </booktitle> <pages> 361-366), </pages> <address> Aberdeen, Scotland: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> O'Neill, M. C. </author> <year> (1989). </year> <title> Escherichia Coli promoters. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264, </volume> <pages> 5522-5530. </pages>
Reference: <author> Opitz, D. & Shavlik, J.W. </author> <year> (1992). </year> <title> Heuristically expanding knowledge-based neural networks. </title> <type> Technical Report, </type> <institution> Madison, WI: University of Wisconsin, Computer Sciences Department. </institution>
Reference: <author> Pollack, J. </author> <year> (1990). </year> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 77-105. </pages>
Reference: <author> Pomerleau, D.A., Gowdy, J., & Thorpe, C.E. </author> <year> (1991). </year> <title> Combining artificial neural networks and symbolic processing for autonomous robot guidance. </title> <journal> In Engineering Applications of Artificial Intelligence, </journal> <volume> 4, </volume> <pages> 279-285. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations, </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference: <author> Roscheisen, M., Hofmann, R. & Tresp, V. </author> <year> (1992). </year> <title> Neural control for rolling mills: Incorporating domain theories to overcome data deficiency. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 4), </volume> <editor> J. Moody, S. Hanson, & R. Lippmann (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <booktitle> In Parallel Distributed Processing (Vol. </booktitle> <volume> 1), </volume> <editor> D. E. Rumelhart & J. L. McClelland (eds.). </editor> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Saito, K. & Nakano, R. </author> <year> (1988). </year> <title> Medical diagnostic expert system based on the PDP model. </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks (pp. </booktitle> <pages> 255-262). </pages> <publisher> IEEE Press. </publisher>
Reference: <author> Scott, G., Shavlik, J., & Ray, W. </author> <year> (1992). </year> <title> Refining PID controllers using neural networks. </title> <journal> Neural Computation, </journal> <volume> 4, </volume> <pages> 746-757. </pages>
Reference-contexts: KBANN has been applied to successfully refining domain theories for real-world problems such as gene finding (Towell et al., 1990), protein folding (Maclin & Shavlik, in press), and the control of a simple chemical plant <ref> (Scott, Shavlik, & Ray, 1992) </ref>. The appendix to this paper presents the application of KBANN to a problem from the Human Genome Project.
Reference: <author> Sejnowski, T. J. & Rosenberg, C. </author> <year> (1987). </year> <title> Parallel networks that learn to pronounce English text. </title> <journal> Complex Systems, </journal> <volume> 1, </volume> <pages> 145-168. </pages>
Reference: <author> Shavlik, J. W. & Towell, G. G. </author> <year> (1989). </year> <title> An approach to combining explanation-based and neural learning algorithms. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <month> 233-255. </month> <title> Page 13 Combining Symbolic and Neural Learning Touretzky, </title> <editor> D. S. (ed.) </editor> <year> (1991). </year> <title> Special issue on connectionist approaches to language learning. </title> <journal> Machine Learning, </journal> <volume> 7. </volume>
Reference-contexts: One might ask, where is the symbolic learning in the approaches presented so far? One answer is that domain-theory refinement, which is what knowledge-based networks do, addresses the incorrect-theory problem of explanation-based learning. In fact, this perspective was the initial motivation for the KBANN research <ref> (Shavlik & Towell, 1989) </ref>. (Recently, Mitchell and Thrun (1993) proposed an explanation-based, though non-symbolic, method for training neural networks for reinforcement learning tasks.) But what about performing symbolic inductive learning in conjunction with neural learning? As mentioned above, Utgoff's (1988) perceptron trees are one method for doing so, but his algorithm
Reference: <author> Towell, G. G. </author> <year> (1992). </year> <title> Symbolic Knowledge and Neural Networks: Insertion, Refinement, and Extraction. </title> <type> Doctoral dissertation, </type> <institution> Madison, WI: University of Wisconsin, Computer Sciences Department. </institution>
Reference: <author> Towell, G. G., Shavlik, J. W. & Noordewier, M. O. </author> <year> (1990). </year> <title> Refinement of approximately correct domain theories by knowledge-based neural networks. </title> <booktitle> Proceedings of the Eighth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 861-866), </pages> <address> Boston: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Table 1 shows the correspondences between a domain theory and a neural network, and Figure 2 alcontains a simple example of the K approach to mapping a domain theory into a neural networks. KBANN has been applied to successfully refining domain theories for real-world problems such as gene finding <ref> (Towell et al., 1990) </ref>, protein folding (Maclin & Shavlik, in press), and the control of a simple chemical plant (Scott, Shavlik, & Ray, 1992). The appendix to this paper presents the application of KBANN to a problem from the Human Genome Project.
Reference: <author> Towell G. G. & Shavlik, J. W. </author> <year> (1992a). </year> <title> Using symbolic inductive learning to improve knowledge-based neural networks. </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence (pp. </booktitle> <pages> 177-182), </pages> <address> San Jose, CA: </address> <publisher> AAAI Press. </publisher>
Reference: <author> Towell, G. G. & Shavlik, J. W. </author> <year> (1992b). </year> <title> Interpretation of artificial neural networks: Mapping knowledge-based neural networks into rules. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 4), </volume> <editor> J. Moody, S. Hanson, & R. Lippmann (eds.), </editor> <address> San Mateo, CA: </address> <note> Morgan Kaufmann. (A longer version of this paper will appear in Machine Learning under the title "Extracting refined rules from knowledge-based neural networks.") </note> <author> Tresp, V., Hollatz, J., & Ahmad, S. </author> <year> (1993). </year> <title> Network structuring and training using rule-based knowledge. </title> <booktitle> In Advances in Neural Information Processing Systems (Vol. </booktitle> <volume> 5), </volume> <editor> S. Hanson, J. Cowans, & L. Giles (eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Uberbacher, E. C. & Mural, R. J. </author> <year> (1991). </year> <title> Locating protein coding regions in human DNA sequences by a multiple sensor neural network approach. </title> <booktitle> Proceedings of the National Academy of Sciences, </booktitle> <volume> 88, </volume> <month> 11,261-11,265. </month>
Reference-contexts: In addition, neural network approaches have proven successful on a wide range of "real world" tasks, such as speech understanding (Lippmann, 1989), handwritten-character recognition (Le Cun et al., 1989), control of dynamic systems (Jordan & Rumelhart, 1992), gene finding <ref> (Uberbacher & Mural, 1991) </ref>, and language learning (Touretzky, 1991). These experiments strongly suggest that connectionist learning is a powerful approach, and the use of neural networks with symbolic knowledge merits exploration. Finally, it is important to note that there are connectionist architectures beyond the simple, feed-forward, single-hidden-layer neural networks.
Reference: <author> Utgoff, P. E. </author> <year> (1988). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <booktitle> Proceedings of the Seventh National Conference on Artificial Intelligence (pp. </booktitle> <pages> 601-606). </pages> <address> St. Paul, MN: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Weiss, S. & Kapouleas, I. </author> <year> (1989). </year> <title> An empirical comparison of pattern recognition, neural nets, and machine learning classification methods. </title> <booktitle> Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (pp. </booktitle> <pages> 781-786). </pages> <address> Detroit: </address> <publisher> Morgan Kaufmann. </publisher>
References-found: 57

