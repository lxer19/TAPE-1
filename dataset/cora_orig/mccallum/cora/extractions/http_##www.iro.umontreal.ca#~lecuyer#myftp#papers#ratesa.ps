URL: http://www.iro.umontreal.ca/~lecuyer/myftp/papers/ratesa.ps
Refering-URL: http://www.iro.umontreal.ca/~lecuyer/papers.html
Root-URL: http://www.iro.umontreal.ca
Title: BUDGET-DEPENDENT CONVERGENCE RATE OF STOCHASTIC APPROXIMATION  
Author: PIERRE L'ECUYER AND GEORGE YIN 
Keyword: Key words. stochastic optimization, discrete-event systems, stochastic approximation, gradient estimate, rate of convergence, limit theorems  
Date: February 1, 1997  
Note: Version:  AMS subject classifications. 60F05, 60F17, 62L20, 93E23, 93E25, 93E30  
Abstract: Convergence rate results are derived for a stochastic optimization problem where a performance measure is minimized with respect to a vector parameter . Assuming that a gradient estimator is available and that both the bias and the variance of the estimator are (known) functions of the budget devoted to its computation, the gradient estimator is employed in conjunction with a stochastic approximation (SA) algorithm. Our interest is to figure out how to allocate the total available computational budget to the successive SA iterations. The effort is devoted to solving the asymptotic version of this problem by finding the convergence rate of SA towards the optimizer, first as a function of the number of iterations, and then as a function of the total computational effort. As a result the optimal rate of increase of the computational budget per iteration can be found. Explicit expressions for the case where the computational budget devoted to an iteration is a polynomial in the iteration number, and where the bias and variance of the gradient estimator are polynomials of the computational budget, are derived. Applications include the optimization of steady-state simulation models with likelihood ratio, perturbation analysis, or finite-difference gradient estimators, optimization of infinite-horizon models with discounting, optimization of functions of several expectations, and so on. Several examples are discussed. Our results readily generalize to general root-finding problems through stochastic approximation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Andrad ottir, </author> <title> A stochastic approximation algorithm with varying bounds, </title> <journal> Operations Research, </journal> <volume> 43 (1995), </volume> <pages> pp. 1037-1048. </pages>
Reference-contexts: That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu [4], Yin and Zhu [45], and Andradottir <ref> [1] </ref>. Before studying convergence rates, we first state the convergence of the algorithm in the sense of w.p.1. By virtue of the ODE approach developed in Kushner and Clark [20] (see in particular the argument of their Theorem 2.3.1), the following proposition holds. Proposition 2.1. <p> This gives a stronger result than convergence in distribution alone. To begin with, define W n (t) = bntc 1+ffi=2 p bntc+1 for t 2 <ref> [0; 1] </ref>, where bzc denotes the integral part of z, for z 2 IR. <p> The average of those N n replications is the gradient estimator n . We suppose that n p N n ! N and t n = ln n ! t as n ! 1, where p 0, N &gt; 0, and t 2 <ref> [0; 1] </ref> are fixed constants (Here, t = 1 is a loose notation to indicate the case where t n increases faster than fi (ln n).) Table 5.3 gives the values of and ~j such that Ek n k 2 = fi (n ) = fi (((E [C n ]) 1 <p> We first prove a lemma which gives an asymptotic equivalence. Lemma A.1. Under the conditions of Theorem 4.1, W n (t) = M n (t) + o (1); for all t 2 <ref> [0; 1] </ref> where M n (t) = bntc 1+ffi=2 p bntc X i=1 i 1 D bntci (E i i i ); D ni is defined by D ni = j=i+1 (I H=j); n &gt; i; (A.16) p ! 0 as n ! 1, uniformly in t. <p> In addition, A n () is also non-negative definite. It is now clear that to derive the limit covariance matrix, we need only look at A ij n (). For a fixed t 2 <ref> [0; 1] </ref>, A n (t) = bntc 2+ffi n bntc X i=1 i 2ffi exp (H ln (i=bntc)) R exp (H 0 ln (i=bntc)) + bntc 2+ffi n bntc X i=1 i D bntc;i RD 0 bntc;i exp (H ln (i=bntc)) R exp (H 0 ln (i=bntc)) j The last term <p> From that, the optimal values given in Table 5.2 are easily obtained. BUDGET-DEPENDENT STOCHASTIC APPROXIMATION 29 Infinite-horizon discounted model (Example 5.6). Let t n = ln n ! t as n ! 1, where t 2 <ref> [0; 1] </ref>. We want to show that t must be finite and large enough (as specified in Table 5.3) to maximize ~j, and that the convergence rate actually depends on the product ae t .
Reference: [2] <author> F. Azadivar and J. Talavage, </author> <title> Optimization of stochastic simulation models, </title> <booktitle> Mathematics and Computers in Simulation, XXII (1980), </booktitle> <pages> pp. 231-241. </pages>
Reference-contexts: If the growth rate of the objective function is not linearly bounded, then one possible solution is to project n over a compact set G at each step of (1.1), as in Example 1.1 (see also Section 6, or Komlos and Revesz [18], Kushner and Clark [20], Azadivar and Talavage <ref> [2] </ref>, and the references therein). That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu [4], Yin and Zhu [45], and Andradottir [1].
Reference: [3] <author> J. A. Bather, </author> <title> Stochastic approximation: A generalization of the Robbins-Monro procedure, </title> <booktitle> in Proceedings of the Fourth Prague Symposium Asymptotic Statist., </booktitle> <editor> P. Mandl and M. Huskova, eds., </editor> <year> 1989, </year> <pages> pp. 13-27. </pages>
Reference-contexts: In this section, we briefly discuss other possible variants of our setup, such as SA with averaging and projection methods, then give a conclusion. 6.1. Algorithms with averaging. Very recently, some new methods were proposed and suggested for stochastic approximation, by Polyak [33], Ruppert [37], and Bather <ref> [3] </ref>. See also Yin [42], Polyak and Juditsky [34] and Kushner and Yang [23]. It has been a long time effort (dated back to Chung [5]) to improve the performance of SA algorithms. <p> problem we may consider the algorithm n+1 = n n fl n ; n = n j=1 Notice that the averaging here creates no additional burden since the average can be recursively updated as n+1 = n + ( n+1 n )=(n + 1): Motivated by the work of Bather <ref> [3] </ref> (see also Schwabe [38] and Yin and Yin [44]), we may consider another algorithm, which uses averaging in both trajectories and observations (measurements). In addition to the advantages mentioned above, that algorithm appears to be more stable in the initial period (see [44]).
Reference: [4] <author> H. F. Chen and Y. M. Zhu, </author> <title> Stochastic approximation procedure with randomly varying truncations, </title> <journal> Scientic Sinica, </journal> <volume> 29 (1986), </volume> <pages> pp. 914-926. </pages>
Reference-contexts: That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu <ref> [4] </ref>, Yin and Zhu [45], and Andradottir [1]. Before studying convergence rates, we first state the convergence of the algorithm in the sense of w.p.1. By virtue of the ODE approach developed in Kushner and Clark [20] (see in particular the argument of their Theorem 2.3.1), the following proposition holds. <p> A variant of such a projection procedure is an algorithm with random truncation bounds or random projection regions, along the lines of, for example, Chen and Zhu <ref> [4] </ref>. Let q i (), i , be continuously differentiable functions and G = f : q i () 0 for i = 1; 2; : : : ; g: Suppose that G is bounded, convex, and is the closure of its interior.
Reference: [5] <author> K. L. Chung, </author> <title> On a stochastic approximation method, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 25 (1954), </volume> <pages> pp. 463-483. </pages>
Reference-contexts: Algorithms with averaging. Very recently, some new methods were proposed and suggested for stochastic approximation, by Polyak [33], Ruppert [37], and Bather [3]. See also Yin [42], Polyak and Juditsky [34] and Kushner and Yang [23]. It has been a long time effort (dated back to Chung <ref> [5] </ref>) to improve the performance of SA algorithms. Among the choices of step size a n = 1=n fl for 1=2 &lt; fl &lt; 1, a n = 1=n gives the highest order of convergence.
Reference: [6] <author> S. N. Ethier and T. G. Kurtz, </author> <title> Markov Processes: Characterization and Convergence, </title> <editor> J. </editor> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: Proof of Theorem 4.1. The proof is divided into three steps. In the first step, we establish an asymptotic equivalence, while in the second and third steps, we obtain the weak convergence by virtue of a result of Ethier and Kurtz <ref> [6] </ref>, and compute the asymptotic covariance matrix. We first prove a lemma which gives an asymptotic equivalence. Lemma A.1. <p> To complete the proof of Theorem 4.1, we can thus apply Theorem 7.1.4 of Ethier and Kurtz <ref> [6] </ref>. Notice that ~ A n () is non-negative definite. <p> 0 ln (i=bntc)) n!1 Z 1 u 2ffi exp (H ln u)R exp (H 0 ln u)du Z 1 exp ((1 + ffi)v) exp (Hv)R exp (H 0 v)dv (with v = ln u) = t 0 Up to now, all the conditions in Theorem 7.1.4 of Ethier and Kurtz <ref> [6] </ref> are satisfied. The desired result then follows from that theorem. Proof of Theorem 4.3.
Reference: [7] <author> V. Fabian, </author> <title> On asymptotic normality in stochastic approximation, </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 39 (1968), </volume> <pages> pp. 1327-1332. </pages>
Reference-contexts: Such results are more precise than the bounds obtained in the previous section and may provide further opportunities to improve the performance of the algorithms. Limit theorems of that sort already appeared in Fabian <ref> [7] </ref> and earlier papers cited there, under assumptions different than ours. <p> To simplify the notation here, let be a scalar. A generalization to the multidimensional case is straightforward and the rates (e.g., in Table 5.1) are the same (see also Fabian <ref> [7] </ref> for multidimensional results). <p> In this case, the optimal values are (; ; j) = (1=4; 1=2; 1=2) for FDf and (; ; j) = (1=6; 2=3; 2=3) for FDc. These values are well-known (see, e.g., Fabian <ref> [7] </ref>, Kushner and Clark [20], and Kushner and Huang [21]) and the SA algorithm is then called the Kiefer-Wolfowitz algorithm. For p &gt; 0, N n increases with n, so we spend more and more time per iteration as n goes up.
Reference: [8] <author> B. L. Fox and P. W. Glynn, </author> <title> Simulating discounted costs, </title> <booktitle> Management Science, 35 (1989), </booktitle> <pages> pp. 1297-1315. </pages>
Reference-contexts: An unbiased estimator of ff might not be available, but a biased estimator can generally be obtained by simulating the process over a truncated (finite) horizon t, using either finite differences, or BUDGET-DEPENDENT STOCHASTIC APPROXIMATION 5 perturbation analysis, or perhaps other methods. Fox and Glynn <ref> [8] </ref> analyze different estimators of ff () in terms of their convergence rates as a function of the computational budget. They show in particular that if the cost rate is non-negative, then the truncated-horizon estimator of ff () has bias O (e aet ) and bounded variance. <p> Let Var [X (; t)] ! oe 2 and e aet (ff () ff (; t)) ! fi when ! 0 and t ! 1, where oe and fi are positive constants. This assumption is reasonable; see Fox and Glynn <ref> [8] </ref> for justifications. Suppose also that the time required for computing X (; t) is approximately T t, for some constant T . Fox and Glynn [8] have shown that to optimize the convergence rate of the MSE when i.i.d. replicates of X (; t) are used to estimate ff () <p> This assumption is reasonable; see Fox and Glynn <ref> [8] </ref> for justifications. Suppose also that the time required for computing X (; t) is approximately T t, for some constant T . Fox and Glynn [8] have shown that to optimize the convergence rate of the MSE when i.i.d. replicates of X (; t) are used to estimate ff () under the constraint of a limited budget of size C, the horizon length t should increase in such a way that t= ln C ! 1=(2ae)
Reference: [9] <author> P. Glasserman, </author> <title> Gradient Estimation Via Perturbation Analysis, </title> <publisher> Kluwer Academic, Norwell, </publisher> <address> MA, </address> <year> 1991. </year>
Reference-contexts: Under (A11), this sample derivative turns out to be an unbiased derivative estimator, with variance uniformly bounded over G. It is called the infinitesimal perturbation analysis (IPA) estimator (Glasserman <ref> [9] </ref>). For CRN2 and IPA, n converges at rate 14 PIERRE L'ECUYER AND GEORGE YIN O (C n ), which is the canonical rate in terms of C n . The convergence in terms of n is supercanonical ( &gt; 1) when p &gt; 0.
Reference: [10] <author> P. Glasserman and D. D. Yao, </author> <title> Some guidelines and guarantees for common random numbers, </title> <booktitle> Management Science, 38 (1992), </booktitle> <pages> pp. 884-908. </pages> <institution> 30 PIERRE L'ECUYER AND GEORGE YIN </institution>
Reference-contexts: Finite difference estimators can be improved by using common random numbers (CRN), as explained by, e.g., Glynn [11], Glasserman and Yao <ref> [10] </ref>, and L'Ecuyer and Perron [27]. The basic idea is to view X as a function of and !, say X (; !), where ! represents an underlying sample point whose distribution does not depend on . For example, in a simulation model, ! may represent a sequence of i.i.d.
Reference: [11] <author> P. W. Glynn, </author> <title> A GSMP formalism for discrete event systems, </title> <booktitle> Proceedings of the IEEE, 77 (1989), </booktitle> <pages> pp. </pages> <month> 14-23. </month> <title> [12] , Likelihood ratio gradient estimation for stochastic systems, </title> <journal> Communications of the ACM, </journal> <volume> 33 (1990), </volume> <pages> pp. 75-84. </pages>
Reference-contexts: In Appendix 2, we also examine the asymptotic mean and variance of C j=2 n n in terms of the asymptotic constants c ; N , and so on. Finite difference estimators can be improved by using common random numbers (CRN), as explained by, e.g., Glynn <ref> [11] </ref>, Glasserman and Yao [10], and L'Ecuyer and Perron [27]. The basic idea is to view X as a function of and !, say X (; !), where ! represents an underlying sample point whose distribution does not depend on . <p> We may further assume (as in Glynn <ref> [11] </ref>) that there is a constant oe D &gt; 0 such that cVar [ ] ! oe 2 D as c ! 0 and ! 0, and call this the "CRN1" setup.
Reference: [13] <author> P. W. Glynn and P. Heidelberger, </author> <title> Bias properties of budget constrained simulations, </title> <journal> Operations Research, </journal> <volume> 38 (1990), </volume> <pages> pp. 801-814. </pages>
Reference-contexts: This estimator is generally biased because of the non-linearity of g. Under appropriate conditions on g and on the estimators X j and Y j , it can be proved that both the bias and variance of are of fi (N 1 ) (see Glynn and Heidelberger <ref> [13] </ref>). Let T (N ) be the computing time for performing the required simulations and computing . <p> By applying the results of Glynn and Heidelberger <ref> [13] </ref> and Glynn and Whitt [16, Example 2] to ~g, we find that (A6) and (A7) hold with fi = ffi = p, B = (1=2) P d P d and R = i=1 k=1 ~g i ~g k ik .
Reference: [14] <author> P. W. Glynn and P. L'Ecuyer, </author> <title> On the existence and estimation of performance measure derivatives for stochastic recursions, </title> <booktitle> in Lectures Notes in Control and Information Sciences, </booktitle> <volume> vol. 199, </volume> <month> June </month> <year> 1994, </year> <pages> pp. 429-435. </pages> <booktitle> Proceedings of the 11th International Conference on Analysis and Optimization of Systems. </booktitle>
Reference-contexts: There is also a control-variate variant of LR (L'Ecuyer [26]) for which (under some conditions) the variance V n is fi (1). It is denoted by CLR in the table. Example 5.4. Let us return to the GI=GI=1 queue of Example 1.1. For that problem, L'Ecuyer and Glynn <ref> [14] </ref> have proven the convergence w.p.1 of SA (using BUDGET-DEPENDENT STOCHASTIC APPROXIMATION 17 projection over a finite interval) combined with each of the gradient estimation methods discussed so far in this section, under a given set of assumptions. They did not study the convergence rates.
Reference: [15] <author> P. W. Glynn, P. L'Ecuyer, and M. </author> <title> Ad es, Gradient estimation for ratios, </title> <booktitle> in Proceedings of the 1991 Winter Simulation Conference, </booktitle> <month> Dec </month> <year> 1991, </year> <pages> pp. 986-993. </pages>
Reference-contexts: The gradient estimator is then = ( X 2 (N )) 2 : Gradient estimators for ratios of expectations are studied in Glynn, L'Ecuyer, and Ades <ref> [15] </ref>. Example 1.3. Consider a continuous-time stochastic process fZ (t); t 0g, whose probability law depends on , and which represents a time-varying cost-rate.
Reference: [16] <author> P. W. Glynn and W. Whitt, </author> <title> The asymptotic efficiency of simulation estimators, </title> <journal> Operations Research, </journal> <volume> 40 (1992), </volume> <pages> pp. 505-520. </pages>
Reference-contexts: An interesting question is then: What is the optimal rate of increase of E [T n ]? A first step toward answering this question is to figure out how the mean square error depends on B n and V n . Glynn and Whitt <ref> [16] </ref> have developed a framework for studying the asymptotic efficiency of simulation estimators as a function of the available computational budget. Their goal was to capture the interplay between the variability of an estimator and the computational effort required. <p> Remark 4.2. If n p T n ! 1 in probability as n ! 1, then (A8) holds with b = 1 =(p+1). In accordance with Theorem 2 of Glynn and Whitt <ref> [16] </ref>, if n p E n [T n ] ! 1 w.p.1 and n 2p1+" E [T n E n [T n ]] 2 ! 2 w.p.1 for some positive constants 1 , 2 , and ", then n p1 C n ! b = 1 =(p+1) w.p.1. <p> By applying the results of Glynn and Heidelberger [13] and Glynn and Whitt <ref> [16, Example 2] </ref> to ~g, we find that (A6) and (A7) hold with fi = ffi = p, B = (1=2) P d P d and R = i=1 k=1 ~g i ~g k ik .
Reference: [17] <author> A. Haurie, P. L'Ecuyer, and Ch. Van Delft, </author> <title> Monte-Carlo optimization of parametrized policies in a class of piecewise deterministic control systems arising in manufacturing flow control, Discrete Event Dynamic Systems: </title> <journal> Theory and Applications, </journal> <volume> 4 (1994), </volume> <pages> pp. 87-111. </pages>
Reference-contexts: A similar situation occurs for other objective functions, more general queueing networks, and many other Markov chains or discrete-event models. For other specific examples, see Yin, Yan, and Lou [43], or Haurie, L'Ecuyer, and van Delft <ref> [17] </ref>, where IPA gradient estimators for the performance of an unreliable manufacturing system, with respect to threshold (hedging point) values, are used for optimization; these estimators have bias and variance both of O (t 1 ). <p> This gives j = 1 provided that p 1. Example 5.5. Haurie, L'Ecuyer, and Van Delft <ref> [17] </ref> proved the convergence w.p.1 of SA combined with IPA for a class of piecewise-deterministic control systems encountered in manufacturing. They did not find the convergence rates.
Reference: [18] <author> J. Koml os and P. R ev ez, </author> <title> A modification of the Robbins-Monro process, </title> <journal> Studia Sci. Math. Hungar, </journal> <volume> 8 (1973), </volume> <pages> pp. 329-340. </pages>
Reference-contexts: If the growth rate of the objective function is not linearly bounded, then one possible solution is to project n over a compact set G at each step of (1.1), as in Example 1.1 (see also Section 6, or Komlos and Revesz <ref> [18] </ref>, Kushner and Clark [20], Azadivar and Talavage [2], and the references therein). That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu [4], Yin and Zhu [45], and Andradottir [1].
Reference: [19] <author> H. J. Kushner, </author> <title> Approximation and Weak Convergence Methods for Random Processes, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1984. </year>
Reference-contexts: If f (n)=g (n) ! 1, we say that "f (n) is similar to g (n)", denoted by f (n) g (n). SA algorithms have been studied extensively and employed in a wide range of applications; see for example Wasan [40], Nevel'son and Khasminskii [32], Kushner <ref> [19] </ref>, Kushner and Clark [20], L'Ecuyer and Glynn [29], among others. For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad [22] and the references therein. <p> In absence of bias, the order is purely determined by how the "noise" behaves. In particular, for ffi = 0 (bounded variance), our result agrees with the classical result (cf., Kushner <ref> [19] </ref>) which says that Ek n k 2 = O (n fl ). If fl + ffi &gt; 2fi, then the error bound becomes Ek n k 2 = O (n 2fi ), i.e., the convergence speed depends on how fast the bias diminishes.
Reference: [20] <author> H. J. Kushner and D. S. Clark, </author> <title> Stochastic Approximation Methods for Constrained and Unconstrained Systems, </title> <booktitle> vol. 26 of Applied Mathematical Sciences, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1978. </year>
Reference-contexts: SA algorithms have been studied extensively and employed in a wide range of applications; see for example Wasan [40], Nevel'son and Khasminskii [32], Kushner [19], Kushner and Clark <ref> [20] </ref>, L'Ecuyer and Glynn [29], among others. For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad [22] and the references therein. <p> If the growth rate of the objective function is not linearly bounded, then one possible solution is to project n over a compact set G at each step of (1.1), as in Example 1.1 (see also Section 6, or Komlos and Revesz [18], Kushner and Clark <ref> [20] </ref>, Azadivar and Talavage [2], and the references therein). That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu [4], Yin and Zhu [45], and Andradottir [1]. <p> Before studying convergence rates, we first state the convergence of the algorithm in the sense of w.p.1. By virtue of the ODE approach developed in Kushner and Clark <ref> [20] </ref> (see in particular the argument of their Theorem 2.3.1), the following proposition holds. Proposition 2.1. Under assumptions (A0)-(A5), n ! fl w.p.1 as n ! 1. 3. An Asymptotic Bound on the MSE Ek n fl k 2 . <p> The most relevant question in the present context is to derive a nontrivial limit result for n =2 n and calculate explicitly the asymptotic covariance matrix. The exponent =2 represents the rate of convergence (see, e.g., Kushner and Clark <ref> [20] </ref>, p.233). The covariance matrix of the limiting normal distribution is another important characterization of the convergence speed. Such results are more precise than the bounds obtained in the previous section and may provide further opportunities to improve the performance of the algorithms. <p> At each iteration of SA, we need a gradient estimator n at = n . A classical way of estimating the gradient ff () is through the use of finite differences (FD) (see, e.g., Kushner and Clark <ref> [20] </ref>, L'Ecuyer and Perron [27], or Zazanis and Suri [46] for more details on FD methods). To simplify the notation here, let be a scalar. A generalization to the multidimensional case is straightforward and the rates (e.g., in Table 5.1) are the same (see also Fabian [7] for multidimensional results). <p> In this case, the optimal values are (; ; j) = (1=4; 1=2; 1=2) for FDf and (; ; j) = (1=6; 2=3; 2=3) for FDc. These values are well-known (see, e.g., Fabian [7], Kushner and Clark <ref> [20] </ref>, and Kushner and Huang [21]) and the SA algorithm is then called the Kiefer-Wolfowitz algorithm. For p &gt; 0, N n increases with n, so we spend more and more time per iteration as n goes up. <p> If we use the projection algorithm in this case, the limit of f n g will be related to the set of Fritz-John points or Kuhn-Tucker points (see p. 191 of Kushner and Clark <ref> [20] </ref> for more details; see also and Kushner and Yin [24] for a comprehensive and updated development of constrained and unconstrained algorithms). Again, our convergence rate results apply if fl lies in the interior of G, but not if it is on the boundary.
Reference: [21] <author> H. J. Kushner and H. Huang, </author> <title> Rates of convergence for stochastic approximation type algo-righms, </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 17 (1979), </volume> <pages> pp. 607-617. </pages>
Reference-contexts: For simplicity, the discussion is focused on the case of fl = 1, which is the most common case in practice. Similar results hold for the case 1=2 &lt; fl &lt; 1, although one then needs to work with an interpolated sequence as in Kushner and Huang <ref> [21] </ref> to obtain the limit theorems. In order not to disrupt the flow of discussion, the proofs of the technical results are placed in Appendix 1. <p> In this case, the optimal values are (; ; j) = (1=4; 1=2; 1=2) for FDf and (; ; j) = (1=6; 2=3; 2=3) for FDc. These values are well-known (see, e.g., Fabian [7], Kushner and Clark [20], and Kushner and Huang <ref> [21] </ref>) and the SA algorithm is then called the Kiefer-Wolfowitz algorithm. For p &gt; 0, N n increases with n, so we spend more and more time per iteration as n goes up. This increases the cost per iteration, but reduces the variance of n .
Reference: [22] <author> H. J. Kushner and F. J. V azquez-Abad, </author> <title> Stochastic approximation algorithms for systems over an infinite horizon, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 34 (1996), </volume> <pages> pp. 712-756. </pages>
Reference-contexts: For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad <ref> [22] </ref> and the references therein. Those analyses generally assume that the computational effort T n is (roughly) the same at all iterations, and the convergence rates are obtained in terms of the number of iterations only.
Reference: [23] <author> H. J. Kushner and J. Yang, </author> <title> Stochastic approximation with averaging of the iterates: Optimal asymptotic rate of convergence for general processes, </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 31 (1993), </volume> <pages> pp. 1045-1062. </pages>
Reference-contexts: Algorithms with averaging. Very recently, some new methods were proposed and suggested for stochastic approximation, by Polyak [33], Ruppert [37], and Bather [3]. See also Yin [42], Polyak and Juditsky [34] and Kushner and Yang <ref> [23] </ref>. It has been a long time effort (dated back to Chung [5]) to improve the performance of SA algorithms. Among the choices of step size a n = 1=n fl for 1=2 &lt; fl &lt; 1, a n = 1=n gives the highest order of convergence.
Reference: [24] <author> H. J. Kushner and G. Yin, </author> <title> Stochastic Approximation Algorithms and Applications, </title> <publisher> Springer Verlag, </publisher> <address> New York, </address> <year> 1997. </year>
Reference-contexts: If we use the projection algorithm in this case, the limit of f n g will be related to the set of Fritz-John points or Kuhn-Tucker points (see p. 191 of Kushner and Clark [20] for more details; see also and Kushner and Yin <ref> [24] </ref> for a comprehensive and updated development of constrained and unconstrained algorithms). Again, our convergence rate results apply if fl lies in the interior of G, but not if it is on the boundary. <p> Again, our convergence rate results apply if fl lies in the interior of G, but not if it is on the boundary. For more general constrained optimization problems, other methods, such as large deviations techniques (see the discussion in Chapter 10 of <ref> [24] </ref> and the references therein) may be needed to obtain the rate of convergence. 20 PIERRE L'ECUYER AND GEORGE YIN 6.3. Conclusion. We have studied budget- and moment-dependent stochastic optimization algorithms, and ascertained their rates of convergence for different situations arising in discrete-event simulation.
Reference: [25] <author> P. L'Ecuyer, </author> <title> An overview of derivative estimation, </title> <booktitle> in Proceedings of the 1991 Winter Simulation Conference, </booktitle> <publisher> IEEE Press, </publisher> <month> Dec </month> <year> 1991, </year> <pages> pp. </pages> <month> 207-217. </month> <title> [26] , Convergence rates for steady-state derivative estimators, </title> <journal> Annals of Operations Research, </journal> <volume> 39 (1992), </volume> <pages> pp. </pages> <month> 121-136. </month> <title> [27] , On the interchange of derivative and expectation for likelihood ratio derivative estimators, </title> <booktitle> Management Science, 41 (1995), </booktitle> <pages> pp. 738-748. </pages>
Reference-contexts: These rates are obtained under the conditions given in Table 5.1: = (p + 1)=3 for FDf and = (p + 1)=5 for FDc. Besides IPA, there is another approach for obtaining an unbiased derivative estimator, called the likelihood ratio (LR) method (Glynn [12], L'Ecuyer <ref> [25] </ref>, Rubinstein and Shapiro [35]). Its variance decreases at the same rate as that of IPA as a function of the computing budget, but it is typically larger by a constant factor.
Reference: [28] <author> P. L'Ecuyer, N. Giroux, and P. W. Glynn, </author> <title> Stochastic optimization by simulation: Numerical experiments with the M=M=1 queue in steady-state, </title> <booktitle> Management Science, 40 (1994), </booktitle> <pages> pp. 1245-1261. </pages>
Reference-contexts: Then, one adds dC ()=d to obtain a derivative estimator for ff. Several specific estimators for the derivative of w (), based on a finite-horizon simulation, are discussed and experimented in L'Ecuyer and Glynn [29] and L'Ecuyer, Giroux, and Glynn <ref> [28] </ref>. Generally, those estimators have a bias of O (t 1 ) due to the truncation of the horizon, and sometimes additional bias such as that due to the use of finite differences to estimate the gradient. Their variances have different orders of magnitudes. <p> In particular, Table 5.2 gives the appropriate convergence rates for several of the variants which were experimented by L'Ecuyer, Giroux, and Glynn <ref> [28] </ref> for the M=M=1 queue, and therefore explains much of the numerical results obtained by these authors.
Reference: [29] <author> P. L'Ecuyer and P. W. Glynn, </author> <title> Stochastic optimization by simulation: Convergence proofs for the GI=G=1 queue in steady-state, </title> <booktitle> Management Science, 40 (1994), </booktitle> <pages> pp. </pages> <month> 1562-1578. </month> <title> [30] , Uniform convergence of truncated-horizon estimators for parametrized regenerative systems. </title> <note> In preparation, </note> <year> 1996. </year>
Reference-contexts: SA algorithms have been studied extensively and employed in a wide range of applications; see for example Wasan [40], Nevel'son and Khasminskii [32], Kushner [19], Kushner and Clark [20], L'Ecuyer and Glynn <ref> [29] </ref>, among others. For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad [22] and the references therein. <p> Then, one adds dC ()=d to obtain a derivative estimator for ff. Several specific estimators for the derivative of w (), based on a finite-horizon simulation, are discussed and experimented in L'Ecuyer and Glynn <ref> [29] </ref> and L'Ecuyer, Giroux, and Glynn [28]. Generally, those estimators have a bias of O (t 1 ) due to the truncation of the horizon, and sometimes additional bias such as that due to the use of finite differences to estimate the gradient. Their variances have different orders of magnitudes. <p> To check that condition, we should normally look for a bound on the conditional bias as a function of the computational effort (e.g., number of replications) uniform over (s n ; n ). Condition (A2) would be verified in a similar way. See <ref> [29, 30] </ref> for examples of how that can be achieved. In (A2), ffi can be either positive, zero or negative, which corresponds to a variance that (asymptotically) decreases, remains bounded, or increases with n, respectively.
Reference: [31] <author> S. P. Meyn and R. L. Tweedie, </author> <title> Markov Chains and Stochastic Stability, </title> <publisher> Springer Verlag, </publisher> <year> 1993. </year>
Reference-contexts: These epochs may be rare, or their identification may require non-negligible effort, especially if the system is modeled as a Harris-recurrent Markov chain (see, e.g., Sigman and Wolff [39] or Meyn and Tweedie <ref> [31] </ref>). Table 5.2 tells us how the values of p, q, and can be chosen in order to maximize j, for the different types of gradient estimators. See Appendix 2 for the technical details.
Reference: [32] <author> M. B. Nevelson and R. Z. Khasminskii, </author> <title> Stochastic Approximation and Recursive Estimation, </title> <journal> vol. 47 of Translation of Math. Monographs, American Mathematical Society, </journal> <year> 1976. </year>
Reference-contexts: If f (n)=g (n) ! 1, we say that "f (n) is similar to g (n)", denoted by f (n) g (n). SA algorithms have been studied extensively and employed in a wide range of applications; see for example Wasan [40], Nevel'son and Khasminskii <ref> [32] </ref>, Kushner [19], Kushner and Clark [20], L'Ecuyer and Glynn [29], among others. For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad [22] and the references therein.
Reference: [33] <author> B. T. Polyak, </author> <title> New method of stochastic approximation type, </title> <journal> Avtomatika i telemekhanika, </journal> <volume> 7 (1990), </volume> <pages> pp. 937-946. </pages> <booktitle> Translation in Automation and Remote Control (1991), </booktitle> <pages> 937-946. </pages>
Reference-contexts: Further remarks and conclusion. In this section, we briefly discuss other possible variants of our setup, such as SA with averaging and projection methods, then give a conclusion. 6.1. Algorithms with averaging. Very recently, some new methods were proposed and suggested for stochastic approximation, by Polyak <ref> [33] </ref>, Ruppert [37], and Bather [3]. See also Yin [42], Polyak and Juditsky [34] and Kushner and Yang [23]. It has been a long time effort (dated back to Chung [5]) to improve the performance of SA algorithms. <p> Meanwhile, by means of averaging, they keep the convergence at the optimal rate with the smallest possible asymptotic BUDGET-DEPENDENT STOCHASTIC APPROXIMATION 19 covariance. Inspired by the averaging approach suggested by Polyak <ref> [33] </ref> and Ruppert [37], for our problem we may consider the algorithm n+1 = n n fl n ; n = n j=1 Notice that the averaging here creates no additional burden since the average can be recursively updated as n+1 = n + ( n+1 n )=(n + 1): Motivated
Reference: [34] <author> B. T. Polyak and A. B. Juditsky, </author> <title> Acceleration of stochastic approximation by averaging, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 30 (1992), </volume> <pages> pp. 838-855. </pages>
Reference-contexts: Algorithms with averaging. Very recently, some new methods were proposed and suggested for stochastic approximation, by Polyak [33], Ruppert [37], and Bather [3]. See also Yin [42], Polyak and Juditsky <ref> [34] </ref> and Kushner and Yang [23]. It has been a long time effort (dated back to Chung [5]) to improve the performance of SA algorithms.
Reference: [35] <author> R. Y. Rubinstein and A. Shapiro, </author> <title> Discrete Event Systems: Sensitivity Analysis and Stochastic Optimization by the Score Function Method, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1993. </year>
Reference-contexts: These rates are obtained under the conditions given in Table 5.1: = (p + 1)=3 for FDf and = (p + 1)=5 for FDc. Besides IPA, there is another approach for obtaining an unbiased derivative estimator, called the likelihood ratio (LR) method (Glynn [12], L'Ecuyer [25], Rubinstein and Shapiro <ref> [35] </ref>). Its variance decreases at the same rate as that of IPA as a function of the computing budget, but it is typically larger by a constant factor. <p> In the infinite-horizon case, the likelihood ratio (LR) estimator typically behaves much differently than IPA, since its variance typically increases linearly w.r.t. the horizon length t n (Glynn [12], Rubinstein and Shapiro <ref> [35] </ref>). One must then select a small value of q to control the variance, and as a result, the overall convergence turns out to be quite slow (j = 1=2).
Reference: [36] <author> R. Ruppert, </author> <title> Almost-sure approximation to the Robbins-Monro and Kiefer-Wolfowitz processes with dependent noise, </title> <journal> Annals of Probability, </journal> <volume> 10 (1982), </volume> <pages> pp. </pages> <month> 178-187. </month> <title> [37] , Handbook in sequential analysis, in Stochastic Approximation, </title> <editor> B. K. Ghosh and P. K. Sen, eds., </editor> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1991, </year> <pages> pp. 503-529. </pages>
Reference-contexts: They obtained limit theorems for several examples including two SA settings: classical Robbins-Monro and Kiefer-Wolfowitz based on central finite differences. Their analysis for these two examples is based on convergence rate results (in terms of n) by Ruppert <ref> [36] </ref>. Our results generalize those studies. To further motivate our development, we now introduce some examples where our framework is appropriate. Example 1.1. Consider a GI=GI=1 queue with mean arrival rate = 1 and mean service time 0, such that is a scale parameter of the service time distribution.
Reference: [38] <author> R. Schwabe, </author> <title> Stability results for smoothed stochastic approximation procedures, </title> <type> Tech. Rep. Preprint Nr. </type> <institution> A-92-14, Fachbereich Mathematik, Freie Universitat Berlin, </institution> <year> 1992. </year> <title> BUDGET-DEPENDENT STOCHASTIC APPROXIMATION 31 </title>
Reference-contexts: the algorithm n+1 = n n fl n ; n = n j=1 Notice that the averaging here creates no additional burden since the average can be recursively updated as n+1 = n + ( n+1 n )=(n + 1): Motivated by the work of Bather [3] (see also Schwabe <ref> [38] </ref> and Yin and Yin [44]), we may consider another algorithm, which uses averaging in both trajectories and observations (measurements). In addition to the advantages mentioned above, that algorithm appears to be more stable in the initial period (see [44]).
Reference: [39] <author> K. Sigman and R. W. Wolff, </author> <title> A review of regenerative processes, </title> <journal> SIAM Review, </journal> <volume> 35 (1993), </volume> <pages> pp. 269-288. </pages>
Reference-contexts: These epochs may be rare, or their identification may require non-negligible effort, especially if the system is modeled as a Harris-recurrent Markov chain (see, e.g., Sigman and Wolff <ref> [39] </ref> or Meyn and Tweedie [31]). Table 5.2 tells us how the values of p, q, and can be chosen in order to maximize j, for the different types of gradient estimators. See Appendix 2 for the technical details.
Reference: [40] <author> M. T. Wasan, </author> <title> Stochastic Approximation, </title> <publisher> Cambridge Press, </publisher> <year> 1969. </year>
Reference-contexts: If f (n)=g (n) ! 1, we say that "f (n) is similar to g (n)", denoted by f (n) g (n). SA algorithms have been studied extensively and employed in a wide range of applications; see for example Wasan <ref> [40] </ref>, Nevel'son and Khasminskii [32], Kushner [19], Kushner and Clark [20], L'Ecuyer and Glynn [29], among others. For an extensive survey on general stochastic approximation for parameterized models of either continuous or discrete event systems, see Kushner and Vazquez-Abad [22] and the references therein.
Reference: [41] <author> G. Yin, </author> <title> A stopping rule for the Robbins-Monro method, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 67 (1990), </volume> <pages> pp. </pages> <month> 151-173. </month> <title> [42] , On extensions of Polyak's averaging approach to stochastic approximation, </title> <journal> Stochastics, </journal> <volume> 36 (1992), </volume> <pages> pp. 245-264. </pages>
Reference-contexts: In addition, bntc 1+ffi=2 p n 0 1 X i 1 D bntci (E i i i ) p ! 0 as n ! 1, uniformly in t: Thus the lemma follows. First, following from Theorem 3.1, M n () is a square integrable martingale. Sim ilar to Yin <ref> [41] </ref>, it can be shown that E sup kM n (t) M n (t )k 2 n!1 E sup k ~ A ij n (t )k n!1 where ~ A n () (with ~ A ij n () denoting its ijth entry) is defined by ~ A ij bntc 2+ffi n
Reference: [43] <author> G. Yin, H. Yan, and X. Lou, </author> <title> On a class of stochastic optimization algorithms with applications to manufacturing models, </title> <booktitle> in Proceedings of the Third Model-Oriented Data Analysis, </booktitle> <editor> H. P. W. W. G. Muller and A. A. Zhigljavsky, eds., </editor> <address> Heidelberg, 1993, </address> <publisher> Physica-Verlag, </publisher> <pages> pp. 213-226. </pages>
Reference-contexts: A similar situation occurs for other objective functions, more general queueing networks, and many other Markov chains or discrete-event models. For other specific examples, see Yin, Yan, and Lou <ref> [43] </ref>, or Haurie, L'Ecuyer, and van Delft [17], where IPA gradient estimators for the performance of an unreliable manufacturing system, with respect to threshold (hedging point) values, are used for optimization; these estimators have bias and variance both of O (t 1 ).
Reference: [44] <author> G. Yin and K. Yin, </author> <title> Asymptotically optimal rate of convergence of smoothed stochastic recursive algorithms, </title> <journal> Stochastics, </journal> <volume> 47 (1994), </volume> <pages> pp. 21-46. </pages>
Reference-contexts: n fl n ; n = n j=1 Notice that the averaging here creates no additional burden since the average can be recursively updated as n+1 = n + ( n+1 n )=(n + 1): Motivated by the work of Bather [3] (see also Schwabe [38] and Yin and Yin <ref> [44] </ref>), we may consider another algorithm, which uses averaging in both trajectories and observations (measurements). In addition to the advantages mentioned above, that algorithm appears to be more stable in the initial period (see [44]). <p> 1): Motivated by the work of Bather [3] (see also Schwabe [38] and Yin and Yin <ref> [44] </ref>), we may consider another algorithm, which uses averaging in both trajectories and observations (measurements). In addition to the advantages mentioned above, that algorithm appears to be more stable in the initial period (see [44]). Consider n+1 = n n fl i=1 1 n X j ;(6.2) where 1=2 &lt; fl &lt; 1. The study of asymptotic properties of the averaging algorithms in conjunction with the setup of this paper could be carried out. <p> The study of asymptotic properties of the averaging algorithms in conjunction with the setup of this paper could be carried out. The idea would be to combine the approach in Yin [42] and Yin and Yin <ref> [44] </ref> with the results of this work. This is a topic for further investigation. 6.2. Projection algorithms. The discussion in this paper is based on the basic recursive SA algorithm (1.1). Many variants of the algorithm can also be considered.
Reference: [45] <author> G. Yin and Y. M. Zhu, </author> <title> Almost sure convergence of stochastic approximation algorithms with non-additive noise, </title> <journal> International J. Control, </journal> <volume> 47 (1989), </volume> <pages> pp. 1361-1376. </pages>
Reference-contexts: That compact set could vary adaptively between iterations, yielding a SA algorithm with varying bounds, as suggested in, for example, Chen and Zhu [4], Yin and Zhu <ref> [45] </ref>, and Andradottir [1]. Before studying convergence rates, we first state the convergence of the algorithm in the sense of w.p.1. By virtue of the ODE approach developed in Kushner and Clark [20] (see in particular the argument of their Theorem 2.3.1), the following proposition holds. Proposition 2.1.
Reference: [46] <author> M. A. Zazanis and R. Suri, </author> <title> Convergence rates of finite-difference sensitivity estimates stochastic systems, </title> <journal> Operations Research, </journal> <volume> 41 (1993), </volume> <pages> pp. 694-703. </pages>
Reference-contexts: At each iteration of SA, we need a gradient estimator n at = n . A classical way of estimating the gradient ff () is through the use of finite differences (FD) (see, e.g., Kushner and Clark [20], L'Ecuyer and Perron [27], or Zazanis and Suri <ref> [46] </ref> for more details on FD methods). To simplify the notation here, let be a scalar. A generalization to the multidimensional case is straightforward and the rates (e.g., in Table 5.1) are the same (see also Fabian [7] for multidimensional results). <p> Finite-horizon models (Example 5.1). Let the assumptions made in Example 5.1 hold and assume that Var [X] ! oe 2 as ! 0. In the case of FD with independent random numbers, we have the following (see L'Ecuyer and Perron [27] or Zazanis and Suri <ref> [46] </ref>): For FDf, if ff is twice continuously differentiable at fl = 0 with second derivative H = min &gt; 0, then n B n ! H c =2 and n p2 V n ! 2 1 N 2 if ff is three times continuously differentiable at 0 and H 3
References-found: 40

