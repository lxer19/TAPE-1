URL: http://www.cs.ucsd.edu/users/vdonalds/jpdc.ps
Refering-URL: http://www.cs.ucsd.edu/users/vdonalds/
Root-URL: http://www.cs.ucsd.edu
Title: Program Speedup in a Heterogeneous Computing Network  
Author: Val Donaldson Francine Berman Ramamohan Paturi 
Address: 9500 Gilman Drive La Jolla, CA 92093-0114  
Affiliation: Department of Computer Science and Engineering University of California, San Diego  
Date: 21:3 (June 1994), 316-322  
Note: Appears in Journal of Parallel and Distributed Computing  
Abstract: Program speedup is an important measure of the performance of an algorithm on a parallel machine. Of particular importance is the near linear or superlinear speedup exhibited by the most performance-efficient algorithms for a given system. We describe network and program models for heterogeneous networks, define notions of speedup and superlinear speedup, and observe that speedup consists of both heterogeneous and parallel components. We also consider the case of linear tasks, give a lower bound for the speedup, and show that there is no theoretical upper limit on heterogeneous speedup.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Beguelin, A., Dongarra, J.J., Geist, G.A., Manchek, R., and Sunderam, </author> <title> V.S. Graphical development tools for network-based concurrent supercomputing. </title> <booktitle> Proc. Supercomputing '91. </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 435-444. </pages>
Reference-contexts: Our heterogeneous network and program models are similar to the models defined in <ref> [1, 4, 8, 14, 18] </ref> etc. with some modifications. 2.1 Heterogeneous Network Model Our heterogeneous network model is a set H = fM 1 ; M 2 ; : : : ; M m g of m distinct machines. Each machine can communicate with any other machine. <p> Due to the difficulties of programming in a heterogeneous environment there are as yet few applications which have exploited heterogeneity at the network level for single programs, although recently developed systems such as HeNCE <ref> [1] </ref> may change this. Kung et al [9], describe three different heterogeneous applications, but only report execution results for running the applications on a homogeneous network of Sun 4 workstations.
Reference: [2] <author> Bokhari, S.H. </author> <title> A shortest tree algorithm for optimal assignments across space and time in a distributed processor system. </title> <journal> IEEE Trans. Software Engrg. </journal> <volume> SE-7, </volume> <month> 6 (Nov. </month> <year> 1981), </year> <pages> 583-589. </pages>
Reference-contexts: The problem of choosing an optimal mapping is NP-complete [6], even with a number of simplifying restrictions. Stone [16] solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari <ref> [2] </ref> solves the problem for arbitrary m and n when the task graph is a tree. Related results are derived in [3, 13, 17]. A (nonpreemptive) mapping of tasks to machines is a function : V ! H. <p> Communication times between successive tasks are a function of the machines assigned to the tasks as well as the tasks themselves. Bokhari <ref> [2] </ref> describes an O (m 2 n) shortest path algorithm which can be used to derive the optimal assignment.
Reference: [3] <author> Bokhari, S.H. </author> <title> Partitioning problems in parallel, pipelined, </title> <journal> and distributed computing. IEEE Trans. Comput. </journal> <volume> 37, </volume> <month> 1 (Jan. </month> <year> 1988), </year> <pages> 48-57. </pages>
Reference-contexts: Stone [16] solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari [2] solves the problem for arbitrary m and n when the task graph is a tree. Related results are derived in <ref> [3, 13, 17] </ref>. A (nonpreemptive) mapping of tasks to machines is a function : V ! H. If maps all n tasks to the same machine, the mapping is sequential. If the codomain of contains more than one machine, the mapping may permit concurrent execution of tasks on distinct machines.
Reference: [4] <author> Eshagian, M.M., and Freund, </author> <title> R.F. Cluster-M paradigms for high-order heterogeneous procedural specification computing. </title> <booktitle> Proc. Workshop on Heterogeneous Processing. </booktitle> <address> Bev-erly Hills, CA, </address> <month> Mar. </month> <year> 1992, </year> <pages> pp. 47-49. </pages>
Reference-contexts: Our heterogeneous network and program models are similar to the models defined in <ref> [1, 4, 8, 14, 18] </ref> etc. with some modifications. 2.1 Heterogeneous Network Model Our heterogeneous network model is a set H = fM 1 ; M 2 ; : : : ; M m g of m distinct machines. Each machine can communicate with any other machine.
Reference: [5] <author> Freund, </author> <title> R.F. Optimal selection theory for superconcurrency. </title> <booktitle> Proc. Supercomputing '89. </booktitle> <address> Reno, NV, </address> <month> Nov. </month> <year> 1989, </year> <pages> pp. 699-703. </pages>
Reference-contexts: We then have S P = T M 1 P =T H P . If S P &gt; 1 then the heterogeneous network performs better than the best individual machine. Program speedup for heterogeneous networks is discussed in several papers. Freund et al. <ref> [5] </ref> discuss how speedups for code segments on individual machines may vary. Wang et al. [18] define a notion of "actual speedup" as the product of optimal speedup, available parallelism and a measure of how well a code segment matches its machine. <p> Since there is only one path from t 1 to t n , we drop the subscript of the previous subsection, and let O = P n1 i=1 N t i ;t i+1 . Linear task graphs have been considered by <ref> [5, 11, 18] </ref>. Heterogeneous applications like the MaxSegs algorithm [12] are modeled by linear task graphs.
Reference: [6] <author> Garey, M.R., and Johnson, </author> <title> D.S. Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1979. </year>
Reference-contexts: Figure 1a is an example of a task graph. For a program consisting of n tasks and a network of m machines, there are m n possible mappings of tasks to machines. The problem of choosing an optimal mapping is NP-complete <ref> [6] </ref>, even with a number of simplifying restrictions. Stone [16] solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari [2] solves the problem for arbitrary m and n when the task graph is a tree.
Reference: [7] <author> Gustafson, J.L. </author> <title> The consequences of fixed time performance measurement. </title> <booktitle> Proc. 25th Hawaii International Conference on System Sciences. </booktitle> <month> Jan. </month> <year> 1992, </year> <pages> pp. 113-124. </pages>
Reference-contexts: 1 Introduction Program speedup is a widely used measure of the performance of an algorithm on a multiprocessor or multicomputer. Although there are differing notions of the definition of speedup <ref> [7] </ref>, speedup measurements are still almost universally quoted as proof of system efficiency. <p> Superlinear speedup, where S P &gt; m, is especially interesting and somewhat controversial. Quinn [15] and Gustafson <ref> [7] </ref> discuss arguments for and against the possibility of superlinear speedup. A heterogeneous network is one in which multiple, possibly dissimilar, machines cooperate in solving a problem.
Reference: [8] <author> Khokhar, A., Prasanna, V., Shaaban, M., and Wang, C.-L. </author> <title> Heterogeneous supercomputing: problems and issues. </title> <booktitle> Proc. Workshop on Heterogeneous Processing. </booktitle> <address> Beverly Hills, CA, </address> <month> Mar. </month> <year> 1992, </year> <pages> pp. 3-12. </pages>
Reference-contexts: Our heterogeneous network and program models are similar to the models defined in <ref> [1, 4, 8, 14, 18] </ref> etc. with some modifications. 2.1 Heterogeneous Network Model Our heterogeneous network model is a set H = fM 1 ; M 2 ; : : : ; M m g of m distinct machines. Each machine can communicate with any other machine.
Reference: [9] <author> Kung, H.T., Steenkiste, P., Gubitoso, M., and Khaira, M. </author> <title> Parallelizing a new class of large applications over high-speed networks. </title> <booktitle> Symposium on Principles and Practice of Parallel Programming. </booktitle> <address> Williamsburg, VA, </address> <month> Apr. </month> <year> 1991, </year> <pages> pp. 167-177. </pages>
Reference-contexts: Due to the difficulties of programming in a heterogeneous environment there are as yet few applications which have exploited heterogeneity at the network level for single programs, although recently developed systems such as HeNCE [1] may change this. Kung et al <ref> [9] </ref>, describe three different heterogeneous applications, but only report execution results for running the applications on a homogeneous network of Sun 4 workstations.
Reference: [10] <author> Mechoso, C.R., Ma, C.-C., Farrara, J.D., Spahr, J.A., and Moore, R.W. </author> <title> Distribution of a climate model across high-speed networks. </title> <booktitle> Proc. Supercomputing '91. </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 253-260. 11 </pages>
Reference-contexts: Kung et al [9], describe three different heterogeneous applications, but only report execution results for running the applications on a homogeneous network of Sun 4 workstations. Mechoso et al. <ref> [10] </ref> have designed a climate modeling application to run on several machines, but only report execution times for executing the different tasks on separate processors of a multiprocessor Cray Y-MP.
Reference: [11] <author> Moore, R.W. </author> <title> Distributing applications across wide area networks. </title> <booktitle> Proc. Los Alamos Gigabit Testbed Workshop. </booktitle> <address> Santa Fe, NM, </address> <month> Jan. </month> <year> 1990. </year>
Reference-contexts: Freund et al. [5] discuss how speedups for code segments on individual machines may vary. Wang et al. [18] define a notion of "actual speedup" as the product of optimal speedup, available parallelism and a measure of how well a code segment matches its machine. Moore <ref> [11] </ref> defines a similar notion of speedup for the particular instance of a heterogeneous network of a SIMD and a MIMD machine. <p> Since there is only one path from t 1 to t n , we drop the subscript of the previous subsection, and let O = P n1 i=1 N t i ;t i+1 . Linear task graphs have been considered by <ref> [5, 11, 18] </ref>. Heterogeneous applications like the MaxSegs algorithm [12] are modeled by linear task graphs.
Reference: [12] <author> Nicholas, H., Giras, G., Hartonas-Garmhausen, V., Kopko M., Maher, C., and Ro-pelewski, A. </author> <title> Distributing the comparison of DNA and protein sequences across heterogeneous supercomputers. </title> <booktitle> Proc. Supercomputing '91. </booktitle> <address> Albuquerque, NM, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 139-146. </pages>
Reference-contexts: Linear task graphs have been considered by [5, 11, 18]. Heterogeneous applications like the MaxSegs algorithm <ref> [12] </ref> are modeled by linear task graphs. The case of linear tasks is especially interesting since the best assignment of tasks to machines can be easily determined for an arbitrary set of heterogeneous machines and arbitrary communication times between machines executing adjacent tasks. <p> Mechoso et al. [10] have designed a climate modeling application to run on several machines, but only report execution times for executing the different tasks on separate processors of a multiprocessor Cray Y-MP. Nicholas et al. <ref> [12] </ref> report on a heterogeneous application which runs as a sequence of two tasks on a Thinking Machines CM-2 and Cray Y-MP. Each machine implements a version of the MaxSegs dynamic programming algorithm, which compares a DNA or protein query sequence against a large set of library sequences. <p> fall below a user-specified similarity threshold 9 CM-2 Y-MP Heterogeneous Network Filter Task Time 6.0 74.3 6.0 Communication Time | | 0.5 Backtrack Task Time ? 0.9 0.9 Total Time ? 75.2 7.4 Table II: Task execution times (CPU times in minutes) for the heterogeneous MaxSegs algorithm as reported in <ref> [12] </ref>. value. Similarity scores for the remaining library sequences are then recalculated on the Y-MP, as the first step in calculating actual sequence alignments using a backtracking scheme. Additionally, the Y-MP version is capable of finding multiple alignments for the same library sequence.
Reference: [13] <author> Nicol, </author> <title> D.M., and O'Hallaron, D.R. Improved algorithms for mapping pipelined and parallel computations. </title> <journal> IEEE Trans. Comput. </journal> <volume> 40, </volume> <month> 3 (Mar. </month> <year> 1991), </year> <pages> 295-306. </pages>
Reference-contexts: Stone [16] solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari [2] solves the problem for arbitrary m and n when the task graph is a tree. Related results are derived in <ref> [3, 13, 17] </ref>. A (nonpreemptive) mapping of tasks to machines is a function : V ! H. If maps all n tasks to the same machine, the mapping is sequential. If the codomain of contains more than one machine, the mapping may permit concurrent execution of tasks on distinct machines.
Reference: [14] <author> Prakash, S., and Parker, </author> <title> A.C. A design method for optimal synthesis of application-specific heterogeneous multiprocessor systems. </title> <booktitle> Proc. Workshop on Heterogeneous Processing. </booktitle> <address> Beverly Hills, CA, </address> <month> Mar. </month> <year> 1992, </year> <pages> pp. 75-80. </pages>
Reference-contexts: Our heterogeneous network and program models are similar to the models defined in <ref> [1, 4, 8, 14, 18] </ref> etc. with some modifications. 2.1 Heterogeneous Network Model Our heterogeneous network model is a set H = fM 1 ; M 2 ; : : : ; M m g of m distinct machines. Each machine can communicate with any other machine.
Reference: [15] <author> Quinn, M.J. </author> <title> Designing Efficient Algorithms for Parallel Computers. </title> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: Superlinear speedup, where S P &gt; m, is especially interesting and somewhat controversial. Quinn <ref> [15] </ref> and Gustafson [7] discuss arguments for and against the possibility of superlinear speedup. A heterogeneous network is one in which multiple, possibly dissimilar, machines cooperate in solving a problem.
Reference: [16] <author> Stone, H.S. </author> <title> Multiprocessor scheduling with the aid of network flow algorithms. </title> <journal> IEEE Trans. Software Engrg. </journal> <volume> SE-3, </volume> <month> 1 (Jan. </month> <year> 1977), </year> <pages> 85-93. </pages>
Reference-contexts: For a program consisting of n tasks and a network of m machines, there are m n possible mappings of tasks to machines. The problem of choosing an optimal mapping is NP-complete [6], even with a number of simplifying restrictions. Stone <ref> [16] </ref> solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari [2] solves the problem for arbitrary m and n when the task graph is a tree. Related results are derived in [3, 13, 17].
Reference: [17] <author> Towsley, D. </author> <title> Allocating programs containing branches and loops within a multiple processor system. </title> <journal> IEEE Trans. Software Engrg. </journal> <volume> SE-12, </volume> <month> 10 (Oct. </month> <year> 1986), </year> <pages> 1018-1024. </pages>
Reference-contexts: Stone [16] solves the heterogeneous network scheduling problem for the case m = 2 machines using a network flow algorithm. Bokhari [2] solves the problem for arbitrary m and n when the task graph is a tree. Related results are derived in <ref> [3, 13, 17] </ref>. A (nonpreemptive) mapping of tasks to machines is a function : V ! H. If maps all n tasks to the same machine, the mapping is sequential. If the codomain of contains more than one machine, the mapping may permit concurrent execution of tasks on distinct machines.
Reference: [18] <author> Wang, M.-C., Kim, S.-D., Nichols, M.A., Freund, R.F., Siegel, H.J., and Nation, </author> <title> W.G. Augmenting the optimal selection theory for superconcurrency. </title> <booktitle> Proc. Workshop on Heterogeneous Processing. </booktitle> <address> Beverly Hills, CA, </address> <month> Mar. </month> <year> 1992, </year> <pages> pp. 13-22. 12 </pages>
Reference-contexts: Our heterogeneous network and program models are similar to the models defined in <ref> [1, 4, 8, 14, 18] </ref> etc. with some modifications. 2.1 Heterogeneous Network Model Our heterogeneous network model is a set H = fM 1 ; M 2 ; : : : ; M m g of m distinct machines. Each machine can communicate with any other machine. <p> If S P &gt; 1 then the heterogeneous network performs better than the best individual machine. Program speedup for heterogeneous networks is discussed in several papers. Freund et al. [5] discuss how speedups for code segments on individual machines may vary. Wang et al. <ref> [18] </ref> define a notion of "actual speedup" as the product of optimal speedup, available parallelism and a measure of how well a code segment matches its machine. Moore [11] defines a similar notion of speedup for the particular instance of a heterogeneous network of a SIMD and a MIMD machine. <p> Since there is only one path from t 1 to t n , we drop the subscript of the previous subsection, and let O = P n1 i=1 N t i ;t i+1 . Linear task graphs have been considered by <ref> [5, 11, 18] </ref>. Heterogeneous applications like the MaxSegs algorithm [12] are modeled by linear task graphs.
References-found: 18

