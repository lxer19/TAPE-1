URL: ftp://ftp.cs.utexas.edu/pub/mooney/papers/ngc.ps.Z
Refering-URL: http://www.cs.utexas.edu/users/ml/abstracts.html
Root-URL: 
Email: fmecaliff,mooneyg@cs.utexas.edu  
Title: Advantages of Decision Lists and Implicit Negatives in Inductive Logic Programming  
Author: Mary Elaine Califf and Raymond J. Mooney 
Date: January 20, 1996  
Address: Austin, TX 78712  
Affiliation: Department of Computer Sciences University of Texas at Austin  
Abstract: This paper demonstrates the capabilities of Foidl, an inductive logic programming (ILP) system whose distinguishing characteristics are the ability to produce first-order decision lists, the use of an output completeness assumption as a substitute for negative examples, and the use of intensional background knowledge. The development of Foidl was originally motivated by the problem of learning to generate the past tense of English verbs; however, this paper demonstrates its superior performance on two different sets of benchmark ILP problems. Tests on the finite element mesh design problem show that Foidl's decision lists enable it to produce generally more accurate results than a range of methods previously applied to this problem. Tests with a selection of list-processing problems from Bratko's introductory Prolog text demonstrate that the combination of implicit negatives and intensionality allow Foidl to learn correct programs from far fewer examples than Foil.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W., Lapointe, S., Ling, C. X., & Matwin, S. </author> <year> (1994). </year> <title> Learning recursive relations with randomly selected small training sets. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 12-18 New Brunswick, NJ. </address>
Reference-contexts: Also, although Filp assumes that its target relation is a function, the definitions learned consist of unordered sets of clauses. Other systems, most notably Lopster (Lapointe & Matwin, 1992) and Crustacean <ref> (Aha et al., 1994) </ref>, have addressed learning recursive relations from very small sets of examples.
Reference: <author> Bergadano, F., & Gunetti, D. </author> <year> (1993). </year> <title> An interactive system to learn functional logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1044-1049 Chambery, France. </address>
Reference-contexts: can exploit its ability to use implicit negative examples to learn these relations from far fewer positive examples than Foil requires (75 for member, 81 for del, and 81 for insert) and no explicit negatives. 5 Related Work The first ILP system to focus specifically on learning functions was Filp <ref> (Bergadano & Gunetti, 1993) </ref>. It assumes that the target and all background relations must be functional and uses this knowledge to limit its search for literals. However, these assumptions prevent its application to many of the tasks considered here because they typically involve nonfunctional background relations.
Reference: <author> Bergadano, F., Gunetti, D., & Trinchero, U. </author> <year> (1993). </year> <title> The difficulties of learning logic programs with cut. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1, </volume> <pages> 91-107. </pages>
Reference-contexts: can exploit its ability to use implicit negative examples to learn these relations from far fewer positive examples than Foil requires (75 for member, 81 for del, and 81 for insert) and no explicit negatives. 5 Related Work The first ILP system to focus specifically on learning functions was Filp <ref> (Bergadano & Gunetti, 1993) </ref>. It assumes that the target and all background relations must be functional and uses this knowledge to limit its search for literals. However, these assumptions prevent its application to many of the tasks considered here because they typically involve nonfunctional background relations.
Reference: <author> Bloom, P. </author> <year> (1994). </year> <title> Overview: Controversies in language acquisition. </title> <editor> In Bloom, P. (Ed.), </editor> <booktitle> Language Acquisition: Core Readings, </booktitle> <pages> pp. 5-48. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Bratko, I. </author> <year> (1990). </year> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Addison Wesley, Reading:MA. </publisher>
Reference: <author> Califf, M. E., & Mooney, R. </author> <year> (1997). </year> <title> Applying ilp-based techniques to natural language information extraction: An experiment in relational learning. </title> <booktitle> In Working Notes of the IJCAI-97 Workshop on Frontiers in Inductive Logic Programming. </booktitle> <volume> 17 Cameron-Jones, </volume> <editor> R. M., & Quinlan, J. R. </editor> <year> (1993). </year> <title> Avoiding pitfalls when learning recursive theories. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1050-1055 Chambery, France. </address>
Reference-contexts: The SCOPE system builds on the ideas in Dolphin. It uses EBG and ILP to improve planning systems by constructing domain-specific control rules (Estlin & Mooney, 1997). The Rapier system, while it does not use a logic representation, uses ILP techniques to learn patterns for information extraction <ref> (Califf & Mooney, 1997) </ref>. 1 2.2 FOIL Since Foidl is based on Foil, we present a brief review of this important ILP system; see articles on Foil for a more complete description (Quinlan, 1990; Quinlan & Cameron-Jones, 1993; Cameron-Jones & Quinlan, 1994). 2 Foil learns a function-free, first-order, Horn-clause definition of
Reference: <author> Cameron-Jones, R. M., & Quinlan, J. R. </author> <year> (1994). </year> <title> Efficient top-down induction of logic programs. </title> <journal> SIGART Bulletin, </journal> <volume> 5 (1), </volume> <pages> 33-42. </pages>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-284. </pages>
Reference-contexts: In the original algorithm of Rivest (1987) and in CN2 <ref> (Clark & Niblett, 1989) </ref>, rules are learned in the order they appear in the final decision list (i.e. new rules are appended to the end of the list as they are learned).
Reference: <author> Cohen, W. W. </author> <year> (1993). </year> <title> Pac-learning a resticted class of recursive logic programs. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 86-92 Wash-ington, D.C. </address>
Reference-contexts: Therefore, it is difficult for extensional systems to learn recursive definitions from random examples because the coverage of recursive clauses cannot be properly determined since an incomplete extensional definition provides only a "noisy oracle" for the target predicate <ref> (Cohen, 1993) </ref>. In an intensional system such as Foidl or Chillin (Zelle & Mooney, 1994; Zelle, 1995), a recursive call is evaluated intensionally using the current partially-learned definition.
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1993). </year> <title> A theory of clausal discovery. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1058-1063 Chambery, France. </address>
Reference-contexts: The non-monotonic semantics used to eliminate the need for negative examples in Claudien <ref> (De Raedt & Bruynooghe, 1993) </ref> has the same effect as an output completeness assumption in the case where all arguments of the target relation are outputs.
Reference: <author> Dolsak, B., & Muggleton, S. </author> <year> (1992). </year> <title> The application of inductive logic programming to finite-element mesh design. </title> <editor> In Muggleton, S. (Ed.), </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pp. 453-472. </pages> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference: <author> Estlin, T. A., & Mooney, R. </author> <year> (1997). </year> <title> Learning to improve both efficiency and quality of planning. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. 1227-1232. </pages>
Reference-contexts: The Dolphin system combines ILP with Explanation-Base Generalization (EBG) to learn search control rules which 2 eliminate backtracking in Prolog programs (Zelle & Mooney, 1993). The SCOPE system builds on the ideas in Dolphin. It uses EBG and ILP to improve planning systems by constructing domain-specific control rules <ref> (Estlin & Mooney, 1997) </ref>.
Reference: <author> Inuzaka, N., Kamo, M., Ishii, N., Seki, H., & Itoh, H. </author> <year> (1996). </year> <title> Top-down induction of logic programs from incomplete samples. </title> <booktitle> In Proceedings of the Sixth International Workshop on Inductive Logic Programming, </booktitle> <pages> pp. 119-136. </pages>
Reference-contexts: The experiments on learning definitions for simple list manipulation predicates in section 4.2 demonstrate that the approach works better than previous extensional methods. The same basic approach was initially used by Chillin and versions have been subsequently employed by SKILit (Jorge & Brazdil, 1996) and Foil-I <ref> (Inuzaka, Kamo, Ishii, Seki, & Itoh, 1996) </ref>. 3.3 Adding Determinate Literals Determinate literals are literals that may have little or no gain, but add at least one new variable that can only take on exactly one value for each positive tuple and and no more than one value for each negative
Reference: <author> Jorge, A., & Brazdil, P. </author> <year> (1996). </year> <title> Architecture for iterative learning of recursive definitions. </title> <editor> In De Raedt, L. (Ed.), </editor> <booktitle> Advances in Inductive Logic Programming. </booktitle> <publisher> IOS Press, Amsterdam. </publisher>
Reference-contexts: The experiments on learning definitions for simple list manipulation predicates in section 4.2 demonstrate that the approach works better than previous extensional methods. The same basic approach was initially used by Chillin and versions have been subsequently employed by SKILit <ref> (Jorge & Brazdil, 1996) </ref> and Foil-I (Inuzaka, Kamo, Ishii, Seki, & Itoh, 1996). 3.3 Adding Determinate Literals Determinate literals are literals that may have little or no gain, but add at least one new variable that can only take on exactly one value for each positive tuple and and no more
Reference: <author> Karalic, A. </author> <year> (1995). </year> <title> First Order Regression. </title> <type> Ph.D. thesis, </type> <institution> University of Ljubljana,Slovenia. </institution>
Reference-contexts: Table 1 shows our results for Foidl along with those reported for several other systems. The numbers for Foil and FFoil are taken from (Quinlan, 1996). Those for mFoil and Golem are from (Lavrac & Dzeroski, 1994). Fors <ref> (Karalic, 1995) </ref>, like FFoil and Foidl is a system specialized for learning functions. It is a first order regression system, which is particularly appropriate for this task, which is essentially a regression task.
Reference: <author> Lapointe, S., & Matwin, S. </author> <year> (1992). </year> <title> Sub-unification: A tool for efficient induction of recursive programs. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 273-281 Aberdeen, Scotland. </address>
Reference-contexts: However, these assumptions prevent its application to many of the tasks considered here because they typically involve nonfunctional background relations. Also, although Filp assumes that its target relation is a function, the definitions learned consist of unordered sets of clauses. Other systems, most notably Lopster <ref> (Lapointe & Matwin, 1992) </ref> and Crustacean (Aha et al., 1994), have addressed learning recursive relations from very small sets of examples.
Reference: <author> Lavrac, N., & Dzeroski, S. </author> <year> (1994). </year> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood. </publisher>
Reference-contexts: Table 1 shows our results for Foidl along with those reported for several other systems. The numbers for Foil and FFoil are taken from (Quinlan, 1996). Those for mFoil and Golem are from <ref> (Lavrac & Dzeroski, 1994) </ref>. Fors (Karalic, 1995), like FFoil and Foidl is a system specialized for learning functions. It is a first order regression system, which is particularly appropriate for this task, which is essentially a regression task.
Reference: <author> Mooney, R. J. </author> <year> (1996). </year> <title> Inductive logic programming for natural-language processing. </title> <booktitle> In Proceedings of the Sixth International Workshop on Inductive Logic Programming, </booktitle> <pages> pp. 205-224. </pages>
Reference-contexts: The Chill system applies ILP to the problem of learning parsers for natural language <ref> (Zelle & Mooney, 1996) </ref>. ILP is used to learn control rules for an initial overly general parser. The Dolphin system combines ILP with Explanation-Base Generalization (EBG) to learn search control rules which 2 eliminate backtracking in Prolog programs (Zelle & Mooney, 1993).
Reference: <author> Mooney, R. J., & Califf, M. E. </author> <year> (1995). </year> <title> Induction of first-order decision lists: Results on learning the past tense of English verbs. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <pages> 1-24. </pages>
Reference-contexts: These characteristics allow Foidl to out-perform all previous learning methods applied to this problem <ref> (Mooney & Califf, 1995) </ref>. However, although these novel characteristics of the algorithm were motivated by the past-tense problem, they are general properties that promised to be useful for other problems as well. <p> The Forte system applies theory refinement techniques to first-order Horn clause theories <ref> (Richards & Mooney, 1995) </ref>. It identifies possible errors in the initial theory using a set of training examples and revises the theory using operators developed using a variety of methods such as propositional theory refinement, first-order induction, and inversion resolution.
Reference: <author> Muggleton, S., & Feng, C. </author> <year> (1990). </year> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the First Conference on Algorithmic Learning Theory Tokyo, </booktitle> <address> Japan. </address> <publisher> Ohmsha. </publisher>
Reference: <author> Muggleton, S. H. (Ed.). </author> <year> (1992). </year> <title> Inductive Logic Programming. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: <author> Muggleton, S. </author> <year> (1995). </year> <title> Inverse entailment and Progol. </title> <journal> New Generation Computing Journal, </journal> <volume> 13, </volume> <pages> 245-286. </pages>
Reference-contexts: For each problem, Foil was provided with all positive examples of the relation in the specified universe and generates negatives using the closed world assumption. Because Foil (and other systems like Golem and Progol <ref> (Muggleton, 1995) </ref>) must either be provided with explicit negatives or be able to generate negatives using a closed world assumption, it cannot learn programs from smaller sets of examples if only positives are provided.
Reference: <author> Quinlan, J. R. </author> <year> (1991). </year> <title> Determinate literals in inductive logic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning Evanston, </booktitle> <address> IL. </address>
Reference-contexts: Literals Determinate literals are literals that may have little or no gain, but add at least one new variable that can only take on exactly one value for each positive tuple and and no more than one value for each negative tuple when extensional tuples are maintained as in Foil <ref> (Quinlan, 1991) </ref>. Determinate literals are added when no literal has positive gain, in the hope that the new variables will allow for a literal that does have gain. For example, in learning list processing relations such as append, it is often necessary to look at the structure of a list.
Reference: <author> Quinlan, J. R. </author> <year> (1996). </year> <title> Learning first-order definitions of functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 5, </volume> <pages> 139-161. </pages>
Reference-contexts: We then tested the system on two ILP benchmarks: the finite element mesh design problem introduced by Dolsak and Muggleton (1992) and a selection of the list-processing programs from Bratko (1990) previously used by Quinlan and Cameron-Jones (1993). We compare Foidl's performance to Foil and to FFoil <ref> (Quinlan, 1996) </ref>, a recent specialized version of Foil for learning single-output functions without explicit negative examples. First-order decision lists enable Foidl to achieve accuracy on the finite element mesh design problem that is generally superior to a range of previous ILP systems. <p> The gain metric evaluates literals based on the number of positive and negative tuples covered, preferring literals that cover many positives and few negatives. The papers referenced above provide details and information on additional features. 2.3 FFOIL FFoil <ref> (Quinlan, 1996) </ref> is a descendant of Foil with modifications, somewhat similar to Foidl's, that specialize it for learning functional relations. 3 First, FFoil assumes that the final argument of the relation is an output argument and that the other arguments of 1 For further information on these systems and other work <p> Table 1 shows our results for Foidl along with those reported for several other systems. The numbers for Foil and FFoil are taken from <ref> (Quinlan, 1996) </ref>. Those for mFoil and Golem are from (Lavrac & Dzeroski, 1994). Fors (Karalic, 1995), like FFoil and Foidl is a system specialized for learning functions. It is a first order regression system, which is particularly appropriate for this task, which is essentially a regression task.
Reference: <author> Quinlan, J. R., & Cameron-Jones, R. M. </author> <year> (1993). </year> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pp. 3-20 Vienna. </pages>
Reference-contexts: Foidl avoids infinitely recursive clauses by using a pre-specified depth bound when proving examples. Alternatively, techniques for proving termination could be used to prevent infinite recursion <ref> (Cameron-Jones & Quinlan, 1993) </ref>. Once a definition is learned that covers all of the positive examples, redundant clauses are greedily deleted in order to simplify the definition and eliminate any unnecessary positive examples initially added to the definition.
Reference: <author> Quinlan, J. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5 (3), </volume> <pages> 239-266. </pages>
Reference: <author> Richards, B. L., & Mooney, R. J. </author> <year> (1995). </year> <title> Automated refinement of first-order Horn-clause domain theories. </title> <journal> Machine Learning, </journal> <volume> 19 (2), </volume> <pages> 95-131. </pages>
Reference-contexts: The Forte system applies theory refinement techniques to first-order Horn clause theories <ref> (Richards & Mooney, 1995) </ref>. It identifies possible errors in the initial theory using a set of training examples and revises the theory using operators developed using a variety of methods such as propositional theory refinement, first-order induction, and inversion resolution.
Reference: <author> Rivest, R. L. </author> . <year> (1987). </year> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 (3), </volume> <pages> 229-246. </pages>
Reference-contexts: As described above, these are ordered sets of clauses each ending in a cut. When answering an output query, the cuts simply eliminate all but the first answer produced when trying the clauses in order. Therefore, this representation is similar to propositional decision lists <ref> (Rivest, 1987) </ref>, which are ordered lists of pairs (rules) of the form (t i ; c i ) where the test t i is a conjunction of features and c i is a category label and an example is assigned to the category of the first pair whose test it satisfies
Reference: <author> Rumelhart, D. E., & McClelland, J. </author> <year> (1986). </year> <title> On learning the past tense of English verbs. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> Vol. II, </volume> <pages> pp. 216-271. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Webb, G. I., & Brkic, N. </author> <year> (1993). </year> <title> Learning decision lists by prepending inferred rules. </title> <booktitle> In Proceedings of the Australian Workshop on Machine Learning and Hybrid Systems, </booktitle> <pages> pp. </pages> <address> 6-10 Melbourne, Australia. </address>
Reference: <author> Zelle, J. M. </author> <year> (1995). </year> <title> Using Inductive Logic Programming to Automate the Construction of Natural Language Parsers. </title> <type> Ph.D. thesis, </type> <institution> University of Texas, Austin, TX. </institution> <note> Also appears as Artificial Intelligence Laboratory Technical Report AI 96-249. </note>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1993). </year> <title> Combining FOIL and EBG to speed-up logic programs. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pp. </pages> <address> 1106-1111 Chambery, France. </address>
Reference-contexts: ILP is used to learn control rules for an initial overly general parser. The Dolphin system combines ILP with Explanation-Base Generalization (EBG) to learn search control rules which 2 eliminate backtracking in Prolog programs <ref> (Zelle & Mooney, 1993) </ref>. The SCOPE system builds on the ideas in Dolphin. It uses EBG and ILP to improve planning systems by constructing domain-specific control rules (Estlin & Mooney, 1997).
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1994). </year> <title> Combining top-down and bottom-up methods in inductive logic programming. </title> <booktitle> In Proceedings of the Eleventh International Conference on Machine Learning, </booktitle> <pages> pp. </pages> <address> 343-351 New Brunswick, NJ. </address>
Reference: <author> Zelle, J. M., & Mooney, R. J. </author> <year> (1996). </year> <title> Learning to parse database queries using inductive logic programming. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence Portland, </booktitle> <address> OR. </address> <month> 19 </month>
Reference-contexts: The Chill system applies ILP to the problem of learning parsers for natural language <ref> (Zelle & Mooney, 1996) </ref>. ILP is used to learn control rules for an initial overly general parser. The Dolphin system combines ILP with Explanation-Base Generalization (EBG) to learn search control rules which 2 eliminate backtracking in Prolog programs (Zelle & Mooney, 1993).
References-found: 34

