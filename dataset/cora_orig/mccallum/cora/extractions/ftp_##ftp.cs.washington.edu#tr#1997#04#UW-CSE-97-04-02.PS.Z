URL: ftp://ftp.cs.washington.edu/tr/1997/04/UW-CSE-97-04-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Email: -eggers,levy,jlo-@cs.washington.edu  -emer,stamm-@vssad.enet.dec.com  tullsen@cs.ucsd.edu  
Title: Simultaneous Multithreading: A Platform for Next-generation Processors  
Author: Susan J. Eggers, Joel Emer^, Henry M. Levy, Jack L. Lo, Rebecca Stamm^ and Dean M. Tullsen 
Address: Box 352350  Seattle, WA 98195-2350  HLO2-3/J3 77 Reed Road Hudson, MA 01749  9500 Gilman Drive  LaJolla, CA 92093-0114  
Affiliation: Dept. of Computer Science and Engineering  University of Washington  ^Digital Equipment Corporation  Dept. of Computer Science and Engineering  University of California, San Diego  
Abstract-found: 0
Intro-found: 1
Reference: [1] <institution> Microprocessor Report, </institution> <note> April 18, 1994 (PowerPC 604), May 30, 1994 (UltraSPARC 1), October 3, 1994 (HP PA-8000), October 24, 1994 (R10000), </note> <month> Feb. </month> <title> 16, 1995 (Intel Pentium Pro), </title> <month> Oct. </month> <note> 28, 1996 (DEC Alpha 21264). </note>
Reference-contexts: This can be caused by a long latency instruction (such as a memory read) that is holding back further instruction issue. A standard superscalar, such as the DEC Alpha 21164 or 21264, HP PA-8000, Intel Pentium Pro, MIPS R10000, PowerPC 604 or UltraSPARC 1 <ref> [1] </ref> appears in Figure 1a. As in all superscalars, it is executing a single program, or thread, from which it attempts to find multiple instructions per cycle to issue. When it cannot, the issue slots go unused, and it incurs both horizontal and vertical waste. <p> Our current SMT model is derived from a high-performance, out-of-order, superscalar architecture, whose dynamic scheduling core is similar to the MIPS R10000 <ref> [1] </ref>. (The organization of the superscalar core is illustrated in Figure 2; a glossary of terms appears in Table 1.) On each cycle, the superscalar fetches eight instructions from the instruction cache. <p> For example, the functional units could be partitioned across a duplicated register file, as is done in the Alpha 21264 <ref> [1] </ref>. The technique halves the number of read ports (for each copy of the register file), at no cost in intra-partition register accesses and only a slight cost when passing values between partitions. <p> McFarling-style branch prediction hardware <ref> [1] </ref>: a 256-entry, 4-way set-associative branch target buffer with an additional thread identifier field, and a hybrid branch predictor that selects between global and local predictors. (The global predictor has 13 history bits; the local predictor has a 2048-entry local history table that indexes into a 4096-entry prediction table.) 3 Experimental
Reference: [2] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In International Conference on Supercomputing, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: As in all superscalars, it is executing a single program, or thread, from which it attempts to find multiple instructions per cycle to issue. When it cannot, the issue slots go unused, and it incurs both horizontal and vertical waste. Multithreaded architectures, for example, the Tera <ref> [2] </ref>, contain hardware state -- a program counter and registers -- for several threads. On any given cycle the processor executes instructions from one of the threads. On the next cycle, it switches to a different thread context, so that it can execute instructions from the new thread. <p> Rather than building wider superscalar processors, they advocate the use of multiple, simpler superscalars on the same chip. Multithreaded architectures have also been widely investigated; the Tera <ref> [2] </ref> is a fine-grain multithreaded processor, which issues up to three operations each cycle. The M-Machine [3] utilizes multithreading in a different manner, relying on two levels of parallelism, called H-threads and V-threads.
Reference: [3] <author> M. Fillo, S. Keckler, W. Dally, N. Carter, A. Chang, Y. Gurevich, and W. Lee. </author> <title> The M-Machine multicomputer. </title> <booktitle> In 28th Annual International Symposium on Microarchitecture, </booktitle> <month> November </month> <year> 1995. </year>
Reference-contexts: Rather than building wider superscalar processors, they advocate the use of multiple, simpler superscalars on the same chip. Multithreaded architectures have also been widely investigated; the Tera [2] is a fine-grain multithreaded processor, which issues up to three operations each cycle. The M-Machine <ref> [3] </ref> utilizes multithreading in a different manner, relying on two levels of parallelism, called H-threads and V-threads.
Reference: [4] <author> M. Gulati and N. Bagherzadeh. </author> <title> Performance study of a multithreaded superscalar microprocessor. </title> <booktitle> In 2nd International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1996. </year>
Reference-contexts: We then presented a realizable architecture for SMT and investigated the fetch bottlenecks in such a system [10]. In addition to our previous work on SMT, Gulati and Bagherzadeh <ref> [4] </ref> also proposed extensions to superscalar processors to implement simultaneous multithreading. In contrast to our processor model, their base processor was a 4-issue machine with fewer functional units, which limited the speedups they obtained when using additional threads.
Reference: [5] <author> P.G. Lowney, S.M. Freudenberger, T.J. Karzes, W.D. Lichtenstein, R.P. Nix, J.S. ODonnell and J.C. Ruttenberg. </author> <title> The Multiow trace 15 scheduling compiler. </title> <booktitle> In Journal of Supercomputing, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: The SPEC programs were parallelized with the SUIF compiler [13], using parallel-ization policies that were developed for shared memory machines. The multiprogramming workload was compiled with cc; the parallel benchmarks were compiled with the Multiow compiler <ref> [5] </ref>, which has been modified to produce Alpha executables. 3 For all programs most compiler optimizations were set to maximize each programs performance on the superscalar, for example, by taking advantage of deep loop unrolling, and, for the Multiow compiles, instruction scheduling for an eight-wide machine.
Reference: [6] <author> K. Olukotun, B. A. Nayfeh, L. Hammond, K. Wilson, and K. Chang. </author> <title> The case for a single-chip multiprocessor. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: Yamamoto and Nemirovsky [14] evaluated an SMT architecture with separate instruction queues for up to 4 threads. Thread or task-level parallelism is also essential to other next-generation architectures. As discussed in this paper, single-chip multiprocessing exploits this type of parallelism with additional processors. Olukotun, et al., <ref> [6] </ref> investigated design tradeoffs for a single-chip multiprocessor and compared the performance and estimated area of this architecture with superscalars. Rather than building wider superscalar processors, they advocate the use of multiple, simpler superscalars on the same chip.
Reference: [7] <author> G. S. Sohi and M. Franklin. </author> <title> High-bandwidth data memory systems for superscalar processors. </title> <booktitle> In 7th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: The technique halves the number of read ports (for each copy of the register file), at no cost in intra-partition register accesses and only a slight cost when passing values between partitions. For caches, the age-old technique of interleaving, augmented with multiple independently-addressed banks <ref> [7] </ref>, can increase the number of simultaneous accesses without also increasing the number of cache ports. Wilson, et al. suggest placing recently accessed data in a small associative buffer, which is accessed when the main cache ports are busy [12].
Reference: [8] <author> G. S. Sohi, S. E. Breach, and T. Vijaykumar. </author> <title> Multiscalar processors. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: One could think of the M-Machine as a coarser-grain, compiler-driven SMT. Threads can also be used in a speculative manner to exploit both task-level and instruction-level parallelism, as in the Multiscalar <ref> [8] </ref> and superthreaded [9] architectures. In the Multiscalar, tasks (which can be as big as a collection of basic blocks) can be speculatively executed by hardware using dynamic branch prediction techniques. A circular queue of processing units execute different tasks.
Reference: [9] <author> J.-Y. Tsai and P.-C. Yew. </author> <title> The superthreaded architecture: Thread pipelining with run-time data dependence checking and control speculation. </title> <booktitle> In 1996 International Conference on Parallel Architectures and Compilation Techniques, </booktitle> <month> October </month> <year> 1996. </year>
Reference-contexts: One could think of the M-Machine as a coarser-grain, compiler-driven SMT. Threads can also be used in a speculative manner to exploit both task-level and instruction-level parallelism, as in the Multiscalar [8] and superthreaded <ref> [9] </ref> architectures. In the Multiscalar, tasks (which can be as big as a collection of basic blocks) can be speculatively executed by hardware using dynamic branch prediction techniques. A circular queue of processing units execute different tasks.
Reference: [10] <author> D. M. Tullsen, S. J. Eggers, J. S. Emer, H. M. Levy, J. L. Lo, and R. L. Stamm. </author> <title> Exploiting choice: Instruction fetch and issue on an implementable simultaneous multithreading processor. </title> <booktitle> In 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: hardware, a subset of these instructions is then chosen for decoding: instructions are taken from the first thread until a branch instruction or the end of a cache line is encountered; the remainder come from the second thread. (We call this scheme 2.8; it is described in more detail in <ref> [10] </ref>.) Because the instructions come from two different threads, there is a greater likelihood of fetching useful instructions -- 2.8s performance was 10% better than fetching from only one thread at a time. <p> Our thread selection hardware, called the Icount feedback technique (also described in <ref> [10] </ref>), gives the highest priority to the threads that have the least number of instructions in the decode, renaming, and queue pipeline stages (pictured in Figure 3). The technique benefits performance in several ways. <p> In Tullsen, et al., [11] we evaluated the potential of SMT, comparing performance of SMT with superscalars, multithreaded processors, and multiprocessors, using a more abstract processor model and an older (SPEC92) workload. We then presented a realizable architecture for SMT and investigated the fetch bottlenecks in such a system <ref> [10] </ref>. In addition to our previous work on SMT, Gulati and Bagherzadeh [4] also proposed extensions to superscalar processors to implement simultaneous multithreading. In contrast to our processor model, their base processor was a 4-issue machine with fewer functional units, which limited the speedups they obtained when using additional threads.
Reference: [11] <author> D. M. Tullsen, S. J. Eggers, and H. M. Levy. </author> <title> Simultaneous multithreading: Maximizing on-chip parallelism. </title> <booktitle> In 22nd Annual International Symposium on Computer Architecture, </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: We will limit our discussion of related work to the more recent SMT studies and several architectures that represent alternative approaches to exploiting parallelism. In Tullsen, et al., <ref> [11] </ref> we evaluated the potential of SMT, comparing performance of SMT with superscalars, multithreaded processors, and multiprocessors, using a more abstract processor model and an older (SPEC92) workload. We then presented a realizable architecture for SMT and investigated the fetch bottlenecks in such a system [10].
Reference: [12] <author> K.M. Wilson, K. Olukotun and M. Rosenblum. </author> <title> Increasing cache port efficiency for dynamic superscalar microprocessors. </title> <booktitle> In 23rd Annual International Symposium on Computer Architecture, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: Wilson, et al. suggest placing recently accessed data in a small associative buffer, which is accessed when the main cache ports are busy <ref> [12] </ref>. Additional cache ports can also be provided by using wave-pipelining techniques, as in the 21264, which provides dual ports by starting a new access on each half-clock cycle.
Reference: [13] <author> R. Wilson, R. French, C. Wilson, S. Amarasinghe, J. Anderson, S. Tjiang, S.-W. Liao, C.-W. Tseng, M. Hall, M. Lam, and J. Hennessy. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <booktitle> In SIGPLAN 94 Conference on Programming Language Design and Implementation, </booktitle> <month> December </month> <year> 1994. </year>
Reference-contexts: Where feasible, we executed the entire parallel portions of the programs; for the long-running SPEC95 programs, we simulated several iterations of the main loops, using their larger data sets. The SPEC programs were parallelized with the SUIF compiler <ref> [13] </ref>, using parallel-ization policies that were developed for shared memory machines.
Reference: [14] <author> W. Yamamoto and M. Nemirovsky. </author> <title> Increasing superscalar performance through multistreaming. </title> <booktitle> In IFIP WG10.3 Working Conference on Parallel Architectures and Compilation Techniques (PACT 95), </booktitle> <month> June </month> <year> 1995. </year>
Reference-contexts: In contrast to our processor model, their base processor was a 4-issue machine with fewer functional units, which limited the speedups they obtained when using additional threads. Yamamoto and Nemirovsky <ref> [14] </ref> evaluated an SMT architecture with separate instruction queues for up to 4 threads. Thread or task-level parallelism is also essential to other next-generation architectures. As discussed in this paper, single-chip multiprocessing exploits this type of parallelism with additional processors.
References-found: 14

