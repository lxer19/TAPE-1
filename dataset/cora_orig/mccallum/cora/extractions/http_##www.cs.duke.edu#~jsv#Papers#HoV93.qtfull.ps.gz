URL: http://www.cs.duke.edu/~jsv/Papers/HoV93.qtfull.ps.gz
Refering-URL: http://www.cs.duke.edu/~jsv/Papers/catalog/node34.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Design and Analysis of Fast Text Compression Based on Quasi-Arithmetic Coding  
Author: Paul G. Howard and Jeffrey Scott Vitter 
Note: To appear in Information Processing and Management. A shorter version appears in the proceedings of the IEEE Computer Society/NASA/CESDIS Data Compression Conference, Snowbird, Utah, March 30-April 1, 1993, pages 98-107.  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R. B. Arps, G. G. Langdon & J. J. Rissanen, </author> <title> "Method for Adaptively Initializing a Source Model for Symbol Encoding," </title> <journal> IBM Technical Disclosure Bulletin 26 (May 1984), </journal> <pages> 6292-6294. </pages>
Reference: [2] <author> T. C. Bell, J. G. Cleary & I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1990. </year>
Reference-contexts: Then we discuss the coding phase, particularly quasi-arithmetic coding with precomputed tables. We give an extended example that includes complete coding tables for a small coder. Data structure for high order models. We use a multiply-linked list structure similar to the vine pointers of Bell et al. <ref> [2] </ref>; the structure is illustrated in Figure 1. In the versions of the Fast PPM system that use Rice coding, we keep the context lists sorted according to frequency count, while in the version that uses only quasi-arithmetic coding we do not reorganize the lists at all. <p> create at most one new node for any symbol instance, just one order higher than the one at which the symbol was found. (If it was found at the highest order, we do not create any new nodes.) This procedure runs somewhat counter to a recommendation of Bell et al. <ref> [2, pages 149-150] </ref>, but compression does not appear to suffer greatly. We also use a lazy update rule as in [2], updating statistics only for contexts actually searched. In our implementation we allow the model to grow without bound, never deleting nodes or restarting the model. <p> We also use a lazy update rule as in <ref> [2] </ref>, updating statistics only for contexts actually searched. In our implementation we allow the model to grow without bound, never deleting nodes or restarting the model. This is a reasonable approach considering the increasing availability of large amounts of inexpensive memory.
Reference: [3] <author> J. G. Cleary & I. H. Witten, </author> <title> "Data Compression Using Adaptive Coding and Partial String Matching," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-32 (Apr. </month> <year> 1984), </year> <pages> 396-402. </pages>
Reference-contexts: 1 Introduction For compression of text files, the best compression results from the use of high-order models in conjunction with statistical coding techniques. The best compression reported in the literature comes from the PPM (prediction by partial matching) method of Cleary and Witten <ref> [3] </ref>; the most widely used implementation is Moffat's PPMC. The PPM methods use adaptive context models with a fixed maximum order, and arithmetic coding for the coder.
Reference: [4] <author> S. W. Golomb, </author> <title> "Run-Length Encodings," </title> <journal> IEEE Trans. Inform. Theory IT-12 (July 1966), </journal> <pages> 399-401. </pages>
Reference-contexts: Rice codes are a subset of Golomb codes <ref> [4] </ref>; in Golomb codes we encode n by outputting bn=mc in unary and n mod m in binary (adjusted to avoid wasting code space if m is not a power of 2).
Reference: [5] <author> D. S. Hirschberg & D. A. Lelewer, </author> <title> "Context Modeling for Text Compression," in Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> 113-144. </pages>
Reference-contexts: Using quasi-arithmetic coding for just the first symbol in the longest context is a good practical choice, as is using quasi-arithmetic coding until the FOUND probability falls below a specified threshold. Lelewer and Hirschberg <ref> [5] </ref> also use the idea of coding a symbol's position within a PPM context list. Quasi-arithmetic coding. <p> In our implementation we allow the model to grow without bound, never deleting nodes or restarting the model. This is a reasonable approach considering the increasing availability of large amounts of inexpensive memory. Hirschberg and Lelewer <ref> [5] </ref> use a hashing approach to save space in PPM-like models. Exclusion mechanism. The standard approach for exclusions is to maintain a bit map of alphabet symbols, together with a list of currently excluded symbols to quickly reset the bit map after every symbol.
Reference: [6] <author> P. G. Howard & J. S. Vitter, </author> <title> "Practical Implementations of Arithmetic Coding," in Image and Text Compression, </title> <editor> J. A. Storer, ed., </editor> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1992, </year> <pages> 85-112. </pages>
Reference-contexts: For the coder, we replace the time-consuming arithmetic coding step with various combinations of quasi-arithmetic coding and simple prefix codes from the Rice family. Quasi-arithmetic coding, introduced and explained in <ref> [6] </ref>, is a variation of arithmetic coding [11] that uses lookup tables after performing all the arithmetic ahead of time. The computations are done to low precision to keep the table sizes manageable. In Section 2 we briefly describe the PPM method and our speed-oriented enhancements. <p> The probabilities passed to the coder are based on symbol frequency counts, periodically scaled down to exploit locality of reference. At least seven different methods have been used to estimate the escape probability [1,3,6,8,10]; Moffat's PPMC [8] is the most widely used, although our PPMD method <ref> [6] </ref> consistently gives about one percent better compression on text files. Fast PPM. We observe that the use of arithmetic coding guarantees good compression but runs slowly: the multi-symbol version used in PPMC requires two multiplications and two divisions for each symbol coded, including escapes. <p> Witten, Neal, and Cleary [11] present a very clear implementation of arithmetic coding; they use a large N for the interval, namely N = 65;536. In <ref> [6] </ref> we introduce quasi-arithmetic coding, a reduced-precision version of the Witten-Neal-Cleary implementation of arithmetic coding. Our idea is to do all the arithmetic ahead of time and to store the results in lookup tables.
Reference: [7] <author> P. G. Howard & J. S. Vitter, </author> <title> "Fast and Efficient Lossless Image Compression," </title> <booktitle> in Proc. Data Compression Conference, </booktitle> <editor> J. A. Storer & M. Cohn, eds., </editor> <address> Snowbird, Utah, </address> <month> Mar. </month> <journal> 30-Apr. </journal> <volume> 1, </volume> <year> 1993, </year> <pages> 351-360. </pages>
Reference-contexts: This parameter estimation method is presented in detail in <ref> [7] </ref>, where we prove that under reasonable assumptions it produces a code length only O ( p t) bits in excess of that of the optimal Rice code for a context that occurs t times.
Reference: [8] <author> A. M. Moffat, </author> <title> "Implementing the PPM Data Compression Scheme," </title> <journal> IEEE Trans. Comm. </journal> <month> COM-38 (Nov. </month> <year> 1990), </year> <pages> 1917-1921. </pages>
Reference-contexts: The symbols are coded using a multi-symbol arithmetic coder. The probabilities passed to the coder are based on symbol frequency counts, periodically scaled down to exploit locality of reference. At least seven different methods have been used to estimate the escape probability [1,3,6,8,10]; Moffat's PPMC <ref> [8] </ref> is the most widely used, although our PPMD method [6] consistently gives about one percent better compression on text files. Fast PPM.
Reference: [9] <author> R. F. Rice, </author> <title> "Some Practical Universal Noiseless Coding Techniques," </title> <institution> Jet Propulsion Laboratory, JPL Publication 79-22, Pasadena, California, </institution> <month> Mar. </month> <year> 1979. </year>
Reference-contexts: Rice codes. Because a quasi-arithmetic coder must encode a number of binary decisions, a text coder that uses quasi-arithmetic coding alone can take almost as long as PPMC. By encoding a number of decisions at once, however, we can speed up the coder. Rice codes <ref> [9] </ref> are eminently suitable for encoding a number of NOT-FOUND decisions followed by a single FOUND decision. Each Rice code has a non-negative integer parameter k. We encode a non-negative integer n by outputting bn=2 k c in unary, then outputting n mod 2 k in binary.
Reference: [10] <author> I. H. Witten & T. C. Bell, </author> <title> "The Zero Frequency Problem: Estimating the Probabilities of Novel Events in Adaptive Text Compression," </title> <journal> IEEE Trans. Inform. Theory IT-37 (July 1991), </journal> <pages> 1085-1094. </pages>
Reference: [11] <author> I. H. Witten, R. M. Neal & J. G. Cleary, </author> <title> "Arithmetic Coding for Data Compression," </title> <journal> Comm. </journal> <note> ACM 30 (June 1987), 520-540. </note>
Reference-contexts: For the coder, we replace the time-consuming arithmetic coding step with various combinations of quasi-arithmetic coding and simple prefix codes from the Rice family. Quasi-arithmetic coding, introduced and explained in [6], is a variation of arithmetic coding <ref> [11] </ref> that uses lookup tables after performing all the arithmetic ahead of time. The computations are done to low precision to keep the table sizes manageable. In Section 2 we briefly describe the PPM method and our speed-oriented enhancements. <p> In practice we use integer arithmetic and subintervals of an integer interval [0; N ). We output bits as soon as we know them and expand the interval, allowing us to limit the coding delay and to use finite precision arithmetic. Witten, Neal, and Cleary <ref> [11] </ref> present a very clear implementation of arithmetic coding; they use a large N for the interval, namely N = 65;536. In [6] we introduce quasi-arithmetic coding, a reduced-precision version of the Witten-Neal-Cleary implementation of arithmetic coding. <p> Continuing the example, 15 fl suppose that the output buffer contains 6 useful bits, so there is room for 2 more, and that the pending count is 2, meaning that the next output bit will be followed by two opposite bits, as in the bits-to-follow mechanism of Witten et al. <ref> [11] </ref> 5 16 fl The leading output bit L is 1, so 17 fl we put 10000000 into the low byte of the buffer (if L had been 0, we would have put 01111111 into the low byte of the buffer).
References-found: 11

