URL: http://drl.cs.uiuc.edu/pubs/siam97-short.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/siam97.html
Root-URL: http://www.cs.uiuc.edu
Title: Application Experience with Parallel Input/Output: Panda and the H3expresso Black Hole Simulation on the SP2  
Author: S. Kuo M. Winslett Y. Chen Y. Cho M. Subramaniam K. Seamons 
Abstract: This paper summarizes our experiences using the Panda parallel I/O library with the H3expresso numerical relativity code on the Cornell SP2. Two performance issues are described: providing efficient off-loading of output data, and satisfying users' desire to dedicate fewer nodes to I/O. We explore the tradeoffs between potential solutions, and present performance results for our approaches. We also show that Panda's high-level interface, which allows the user to request input or output of a set of arrays with a single command, is a good match for H3expresso's needs.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Acharya, M. Uysal, R. Bennett et al., </author> <title> Tuning the Performance of I/O-Intensive Parallel Applications, </title> <booktitle> Fourth Annual Workshop on I/O in Parallel and Distributed Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Parallel i/o researchers have emphasized the design of parallel file systems or parallel i/o libraries suitable for large scale scientific applications. Most of these works applied the collective i/o approach to out-of-core applications [3, 6]. Instead of optimizing the performance of the i/o system, <ref> [1] </ref> took a different direction. They tuned the applications, e.g., reorganizing the order of the loops, to achieve high performance. [1] experimented with part-time i/o nodes and found that part-time nodes slow down read-intensive applications. [7] showed that when compute nodes double as file servers, applications must accept occasional interruptions for <p> Most of these works applied the collective i/o approach to out-of-core applications [3, 6]. Instead of optimizing the performance of the i/o system, <ref> [1] </ref> took a different direction. They tuned the applications, e.g., reorganizing the order of the loops, to achieve high performance. [1] experimented with part-time i/o nodes and found that part-time nodes slow down read-intensive applications. [7] showed that when compute nodes double as file servers, applications must accept occasional interruptions for providing file-system service, resulting in slower execution.
Reference: [2] <author> J. L. Bell, </author> <title> A Specialized Data Management System for Parallel Execution of Particle Physics Codes, </title> <booktitle> ACM SIGMOD International Conference on Management of Data, </booktitle> <year> 1988. </year>
Reference-contexts: Database researchers have emphasized providing efficient data organization on disk. <ref> [2] </ref> designed a specialized database system for particle physics codes; [5] used a PLOP file structure for the array storage of radio astronomy applications. Parallel i/o researchers have emphasized the design of parallel file systems or parallel i/o libraries suitable for large scale scientific applications.
Reference: [3] <author> R. Bordawekar, A. Choudhary, K. Kennedy, C. Koelbel and M. Paleczny, </author> <title> A Model and Compilation Strategy for Out-of-Core Data Parallel Programs, </title> <booktitle> ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <year> 1995. </year>
Reference-contexts: Parallel i/o researchers have emphasized the design of parallel file systems or parallel i/o libraries suitable for large scale scientific applications. Most of these works applied the collective i/o approach to out-of-core applications <ref> [3, 6] </ref>. Instead of optimizing the performance of the i/o system, [1] took a different direction.
Reference: [4] <author> S. A. Fineberg, </author> <title> Implementing the NHT-1 Application I/O Benchmark, </title> <booktitle> IPPS '93 Workshop on Input/Output in Parallel Computer Systems, </booktitle> <year> 1993. </year>
Reference-contexts: For each run, we report the total elapsed time T t and the i/o overhead ff. T t is measured from the beginning to the end of the execution of H3expresso, including the completion of all i/o activity. The i/o overhead ff, defined in <ref> [4] </ref>, is used to measure system balance. It is computed as follows: ff = T t =T c 1; where T c is the computation time of the application, exclusive of i/o. A value for ff of 10% or less is considered reasonable by [4] and also by the H3expresso users. <p> The i/o overhead ff, defined in <ref> [4] </ref>, is used to measure system balance. It is computed as follows: ff = T t =T c 1; where T c is the computation time of the application, exclusive of i/o. A value for ff of 10% or less is considered reasonable by [4] and also by the H3expresso users. Since computation and i/o are interleaved and sometimes overlapped, it is hard to extract the pure computation time directly from T t . So, the value we use is the run time of H3expresso with no i/o. local disk experiments.
Reference: [5] <author> J. F. Karpovich, J. C. French and A. S. Grimshaw, </author> <title> High Performance Access to Radio Astronomy Data: A Case Study, </title> <booktitle> Conf. on Scientific and Stat. Database Management, </booktitle> <year> 1994. </year>
Reference-contexts: Database researchers have emphasized providing efficient data organization on disk. [2] designed a specialized database system for particle physics codes; <ref> [5] </ref> used a PLOP file structure for the array storage of radio astronomy applications. Parallel i/o researchers have emphasized the design of parallel file systems or parallel i/o libraries suitable for large scale scientific applications. Most of these works applied the collective i/o approach to out-of-core applications [3, 6].
Reference: [6] <author> D. Kotz, </author> <title> Disk-directed I/O for an Out-of-Core Computation, </title> <booktitle> Symposium on High Performance Distributed Computing, </booktitle> <year> 1995. </year>
Reference-contexts: Parallel i/o researchers have emphasized the design of parallel file systems or parallel i/o libraries suitable for large scale scientific applications. Most of these works applied the collective i/o approach to out-of-core applications <ref> [3, 6] </ref>. Instead of optimizing the performance of the i/o system, [1] took a different direction.
Reference: [7] <author> D. Kotz and T. Cai, </author> <title> Exploring the Use of I/O Nodes for Computation in a MIMD Multiprocessor, </title> <booktitle> IPPS '95 Workshop on Input/Output in Parallel and Distributed Systems. </booktitle>
Reference-contexts: Instead of optimizing the performance of the i/o system, [1] took a different direction. They tuned the applications, e.g., reorganizing the order of the loops, to achieve high performance. [1] experimented with part-time i/o nodes and found that part-time nodes slow down read-intensive applications. <ref> [7] </ref> showed that when compute nodes double as file servers, applications must accept occasional interruptions for providing file-system service, resulting in slower execution.
Reference: [8] <author> J. S. Ryan and S. K. Weeratunga, </author> <title> Parallel Computation of 3-D Navier-Stokes Flowfields for Supersonic Vehicles, </title> <booktitle> Proceedings of the 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <year> 1993. </year>
Reference-contexts: However, we know of no previous work that has examined the problem of data migration. <ref> [8] </ref> developed a CFD module with the same i/o requirements as H3expresso.
Reference: [9] <author> K. E. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett, </author> <title> Server-Directed Collective I/O in Panda, </title> <booktitle> Proceedings of Supercomputing '95. </booktitle>
Reference-contexts: In performance tests on the NASA Ames Research Center's IBM SP2, Panda provided excellent i/o bandwidth under a wide variety of conditions, offering 85-95% of the peak bandwidth of the AIX file system on each i/o node <ref> [9] </ref>. Thus we felt ready to try fl This research was supported by an ARPA Fellowship in High Performance Computing administered by the Institute for Advanced Computer Studies, University of Maryland, by NSF under PYI grant IRI 89 58552, and by NASA under NAGW 4244 and NCC5 106. <p> Other information is available on-line at http://www.tc.cornell.edu/UserDoc/Hardware/SP/. The JFS and AFS performance numbers were determined empirically following the methodology of <ref> [9] </ref>. In the case of AFS, performance is extremely dependent on the load, so we report average throughput rather than the peak throughput used for JFS and Unitree.
Reference: [10] <institution> Application Working Group of the Scalable I/O Initiative, </institution> <note> Preliminary Survey of I/O Intensive Applications, Scalable I/O Initiative Working Paper No. 1. </note>
Reference-contexts: 1 Introduction Grand Challenge applications running on massively parallel computers often deal with large multidimensional arrays and are i/o intensive <ref> [10] </ref>, with the need to periodically output the current state of computation, read array data that cannot fit into memory, or do checkpoint/restart operations. Because of the slow rate of improvement in speed of access to disk storage, these applications are often i/o bottlenecked.
References-found: 10

