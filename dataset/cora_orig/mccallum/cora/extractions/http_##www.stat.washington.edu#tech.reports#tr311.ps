URL: http://www.stat.washington.edu/tech.reports/tr311.ps
Refering-URL: http://www.stat.washington.edu/fraley/resources.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Algorithms for Model-Based Gaussian Hierarchical Clustering 1  
Author: C. Fraley 
Note: 1 Funded by the Office of Naval Research under contracts N00014-96-1-0192 and N00014-96-1-0330. This work could not have been accomplished without the expertise and enthusiastic support of principal investigator Adrian Raftery.  
Date: October 29, 1996  
Address: Box 354322 Seattle, WA 98195-4322 USA  
Affiliation: Department of Statistics University of Washington  
Pubnum: Technical Report No. 311  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J. D. Banfield and A. E. Raftery. </author> <title> Model-based Gaussian and non-Gaussian clustering. </title> <journal> Biometrics, </journal> <volume> 49 </volume> <pages> 803-821, </pages> <year> 1993. </year>
Reference-contexts: Image-processing applications include unsupervised texture image segmentation, tissue classification in biomedical images, identification of objects in astronomy, analysis of images from molecular spectroscopy, and recognition and classification of surface defects in manufactured products. Agglomerative hierarchical clustering (Murtagh and Raftery [8], Banfield and Raftery <ref> [1] </ref>), the EM algorithm and related iterative techniques (Celeux and Govaert [3]) or some combination of these (Dasgupta and Raftery [4]) are effective computational techniques for obtaining partitions from these models. The subject of efficient computation in this context has however received little attention. <p> The overall approach is much more general and is not restricted to multivariate normal distributions <ref> [1] </ref>. However, experience to date suggests that clustering based on the multivariate normal distribution is useful in a great many situations of interest ([8], [1], [9], [3], [4]). <p> The overall approach is much more general and is not restricted to multivariate normal distributions <ref> [1] </ref>. However, experience to date suggests that clustering based on the multivariate normal distribution is useful in a great many situations of interest ([8], [1], [9], [3], [4]). <p> An alternative that allows a different variance for each group is k = 2 k I; fl is chosen so as to minimize P G n k <ref> [1] </ref>. If k is the same for all groups but otherwise has no structural constraints, then values of fl that minimize fi fi k=1 W k fi maximize the log-likelihood [5]. <p> For model-based clustering, another advantage of hierarchical agglomeration is that there is an associated Bayesian criterion for choosing the best partition (hence the optimal number of clusters) from among those defined by the hierarchy <ref> [1] </ref>. Hierarchical clustering can be accomplished by splitting rather than agglomeration, but the complexity of such algorithms is combinatorial unless severe restrictions on the allowed subdivisions are applicable. <p> For large data sets, one possible strategy is to apply hierarchical agglomeration to a subset of the data and partition the remaining observations via supervised classification or discriminant analysis. Banfield and Raftery <ref> [1] </ref> used only 522 out of 26,000 pixels in an initial hierarchical phase to successfully classify tissue from an MRI brain-scan image via Gaussian model-based techniques. For each classical method, there is a simple recurrence relation for updating the cost of merging pairs. <p> Note that the time scale for the constant-variance method differs from that of the other methods, which use more memory in exchange for improved time efficiency. 3 Extension to more Complex Gaussian Models Banfield and Raftery <ref> [1] </ref> developed a model-based framework that subsumes all of the parameterizations in Table 1. The resulting clustering methods include some criteria that are more general than k = 2 I or constant k , while still constraining the structure of k . <p> This paradigm is particularly useful for two and three-dimensional data, where geometric features can often be identified visually. It may also be applicable for higher-dimensional data when multivariate visualization analysis reveals some structure. For example, Banfield and Raftery <ref> [1] </ref> were able to closely match the clinical classification of a biomedical data set using Gaussian hierarchical clustering after analyzing its geometric features. <p> In one example, they successfully apply the method to an astronomical image in which one tightly clustered galaxy is contained within another more dispersed one. Table 2 shows relationships between orientation, volume and shape discussed in <ref> [1] </ref>. Criteria based on other combinations of these factors are also possible [3]. Software for hierarchical clustering based on these models is available in the public domain (see [1]); it has been used in a variety of applications with some success [9]. <p> Table 2 shows relationships between orientation, volume and shape discussed in <ref> [1] </ref>. Criteria based on other combinations of these factors are also possible [3]. Software for hierarchical clustering based on these models is available in the public domain (see [1]); it has been used in a variety of applications with some success [9]. A revision based on the techniques described in this paper is currently in progress. <p> A revision based on the techniques described in this paper is currently in progress. Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], <ref> [1] </ref>, [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], <ref> [1] </ref>, [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], <ref> [1] </ref>, [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and <p> were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], <ref> [1] </ref>, [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. <p> I Spherical fixed fixed NA [11],[5], [10], [8], <ref> [1] </ref>, [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2. <p> I Spherical variable fixed NA <ref> [1] </ref>, [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2. <p> [3] D k AD k Elliptical fixed fixed variable [8], <ref> [1] </ref>, [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2. We conclude this section by showing how those techniques can be applied to the remaining models : k = D k AD k and k = k D k AD k .
Reference: [2] <author> G. Celeux and G. Govaert. </author> <title> Comparison of the mixture and the classification maximum likelihood in cluster analysis. </title> <journal> Journal of Statistical Computation and Simulation, </journal> <volume> 47 </volume> <pages> 127-146, </pages> <year> 1993. </year>
Reference-contexts: Metrics appropriate for various intermediate situations can also be formulated. For example, assuming that k = k I or 2 k I implies that the underlying densities are spherical, while variation in k between groups allows their volumes to differ. Celeux and Govaert <ref> [2] </ref> analyzed this criterion and showed that it can give classification performance that is much better than traditional methods. In one example, they successfully apply the method to an astronomical image in which one tightly clustered galaxy is contained within another more dispersed one.
Reference: [3] <author> G. Celeux and G. Govaert. </author> <title> Gaussian parsimonius clustering models. </title> <journal> Pattern Recognition, </journal> <volume> 28 </volume> <pages> 781-793, </pages> <year> 1995. </year>
Reference-contexts: Agglomerative hierarchical clustering (Murtagh and Raftery [8], Banfield and Raftery [1]), the EM algorithm and related iterative techniques (Celeux and Govaert <ref> [3] </ref>) or some combination of these (Dasgupta and Raftery [4]) are effective computational techniques for obtaining partitions from these models. The subject of efficient computation in this context has however received little attention. We aim to fill this gap in the case of agglomerative hierarchical clustering. <p> The overall approach is much more general and is not restricted to multivariate normal distributions [1]. However, experience to date suggests that clustering based on the multivariate normal distribution is useful in a great many situations of interest ([8], [1], [9], <ref> [3] </ref>, [4]). <p> In one example, they successfully apply the method to an astronomical image in which one tightly clustered galaxy is contained within another more dispersed one. Table 2 shows relationships between orientation, volume and shape discussed in [1]. Criteria based on other combinations of these factors are also possible <ref> [3] </ref>. Software for hierarchical clustering based on these models is available in the public domain (see [1]); it has been used in a variety of applications with some success [9]. A revision based on the techniques described in this paper is currently in progress. <p> A revision based on the techniques described in this paper is currently in progress. Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], [1], <ref> [3] </ref> k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], [1], <ref> [3] </ref> k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], [1], <ref> [3] </ref> k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their <p> given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], [8], [1], <ref> [3] </ref> k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. <p> Spherical fixed fixed NA [11],[5], [10], [8], [1], <ref> [3] </ref> k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2. <p> Spherical variable fixed NA [1], <ref> [3] </ref> DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2.
Reference: [4] <author> A. Dasgupta and A. E. Raftery. </author> <title> Detecting features in spatial point processes with clutter via model-based clustering. </title> <type> Technical Report 295, </type> <institution> University of Washington, Department of Statistics, </institution> <month> October </month> <year> 1995. </year> <note> See http://www.stat.washington.edu/tech.reports. </note>
Reference-contexts: Agglomerative hierarchical clustering (Murtagh and Raftery [8], Banfield and Raftery [1]), the EM algorithm and related iterative techniques (Celeux and Govaert [3]) or some combination of these (Dasgupta and Raftery <ref> [4] </ref>) are effective computational techniques for obtaining partitions from these models. The subject of efficient computation in this context has however received little attention. We aim to fill this gap in the case of agglomerative hierarchical clustering. <p> The overall approach is much more general and is not restricted to multivariate normal distributions [1]. However, experience to date suggests that clustering based on the multivariate normal distribution is useful in a great many situations of interest ([8], [1], [9], [3], <ref> [4] </ref>).
Reference: [5] <author> H. P. Friedman and J. Rubin. </author> <title> On some invariant criteria for grouping data. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 62 </volume> <pages> 1159-1178, </pages> <year> 1967. </year>
Reference-contexts: If k is the same for all groups but otherwise has no structural constraints, then values of fl that minimize fi fi k=1 W k fi maximize the log-likelihood <ref> [5] </ref>. When k is allowed to vary completely between groups, the log-likelihood is maximized whenever fl minimizes P G fi fi W k fi fi [10]. <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],<ref> [5] </ref>, [10], [8], [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian
Reference: [6] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference-contexts: G k=1 W k when groups i and j are merged can be represented as W W + W hi;ji (W i + W j ) = W + w ij w T Instead of W itself, we maintain a lower triangular Cholesky factor L for W (see e. g. <ref> [6] </ref>), since then the determinant can be easily computed as the square of the product of the diagonals of L : jW j = fi fiLL T fi fi = jLj = diag (L) o 2 Noting that since W = 0 and hence L = 0 when each observation is <p> The time efficiency for the Cholesky update is O (p 2 ), in contrast to O (p 3 ) for forming a new Cholesky factor from the updated p fi p matrix W . For details of the Cholesky update via Givens rotations, see e. g. <ref> [6] </ref>. Although the criterion is defined for all possible partitions, there remains a problem with initialization: jW j = 0 whenever W has rank less than p.
Reference: [7] <author> L. Kaufman and P. J. Rousseeuw. </author> <title> Finding Groups in Data. </title> <publisher> Wiley, </publisher> <year> 1990. </year>
Reference-contexts: The value returned consists of a `classification tree' (a list of pairs of clusters merged), and possibly the optimal value of the change in criterion at each stage. In classical agglomerative methods (e. g. sum of squares, nearest and farthest neighbor (single and complete link) <ref> [7] </ref>), there is a metric or `cost' based on geometric considerations associated with merging a pair of clusters.
Reference: [8] <author> F. Murtagh and A. E. Raftery. </author> <title> Fitting straight lines to point patterns. </title> <journal> Pattern Recognition, </journal> <volume> 17 </volume> <pages> 479-483, </pages> <year> 1984. </year>
Reference-contexts: Image-processing applications include unsupervised texture image segmentation, tissue classification in biomedical images, identification of objects in astronomy, analysis of images from molecular spectroscopy, and recognition and classification of surface defects in manufactured products. Agglomerative hierarchical clustering (Murtagh and Raftery <ref> [8] </ref>, Banfield and Raftery [1]), the EM algorithm and related iterative techniques (Celeux and Govaert [3]) or some combination of these (Dasgupta and Raftery [4]) are effective computational techniques for obtaining partitions from these models. The subject of efficient computation in this context has however received little attention. <p> A revision based on the techniques described in this paper is currently in progress. Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], [10], <ref> [8] </ref>, [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical <p> Reference I Spherical fixed fixed NA [11],[5], [10], <ref> [8] </ref>, [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation. The models shown here are those discussed in Banfield and Raftery [1]. 2.
Reference: [9] <author> A. E. Raftery. </author> <title> Transitions from ONR Contract N00014-91-J-1074 `Time Series and Image Analysis'. </title> <type> manuscript, </type> <institution> Department of Statistics, University of Washington, </institution> <month> De-cember </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Multivariate Gaussian models have been proposed for quite some time as a basis for clustering algorithms. Recently, methods of this type have shown promise in a number of practical applications <ref> [9] </ref>. Examples in the geophysical sciences include seismic data processing, in the biological sciences classification of cell types based on chemical responses, and in the social sciences classification based on attachment theory in psychology. They have also been used for clustering various types of industrial and financial data. <p> The overall approach is much more general and is not restricted to multivariate normal distributions [1]. However, experience to date suggests that clustering based on the multivariate normal distribution is useful in a great many situations of interest ([8], [1], <ref> [9] </ref>, [3], [4]). <p> Criteria based on other combinations of these factors are also possible [3]. Software for hierarchical clustering based on these models is available in the public domain (see [1]); it has been used in a variety of applications with some success <ref> [9] </ref>. A revision based on the techniques described in this paper is currently in progress.
Reference: [10] <author> A. J. Scott and M. J. Symons. </author> <title> Clustering methods based on likelihood ratio criteria. </title> <journal> Biometrics, </journal> <volume> 27 </volume> <pages> 387-397, </pages> <year> 1971. </year>
Reference-contexts: When k is allowed to vary completely between groups, the log-likelihood is maximized whenever fl minimizes P G fi fi W k fi fi <ref> [10] </ref>. <p> A revision based on the techniques described in this paper is currently in progress. Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], <ref> [10] </ref>, [8], [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k <p> Efficient computational methods for the first four models in Table 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], <ref> [10] </ref>, [8], [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model <p> 2 were given in Section 10 k Distribution Volume Shape Orientation Reference I Spherical fixed fixed NA [11],[5], <ref> [10] </ref>, [8], [1], [3] k I Spherical variable fixed NA [1], [3] DAD Elliptical fixed fixed fixed [5], [10], [1], [3] k D k A k D k Elliptical variable variable variable [10], [1], [3] D k AD k Elliptical fixed fixed variable [8], [1], [3] k D k AD k Elliptical variable fixed variable [1], [3] Table 2: Parameterizations of the covariance matrix k in the Gaussian model and their geometric interpretation.
Reference: [11] <author> J. H. Ward. </author> <title> Hierarchical groupings to optimize an objective function. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 234-244, </pages> <year> 1963. </year> <month> 13 </month>
Reference-contexts: If k = 2 I, then the log-likelihood (3) is maximized by classifications fl that minimize tr k=1 W k . This is the well-known sum of squares criterion which, for example, was suggested by Ward <ref> [11] </ref> as a possible metric when he proposed the agglomerative hierarchical method for clustering. An alternative that allows a different variance for each group is k = 2 k I; fl is chosen so as to minimize P G n k [1]. <p> k I k=1 n k k criterion fi fi G X W k fi fi k k=1 fi fi W k fi fi Table 1: Four parameterizations of the covariance matrix k in the Gaussian model with the corresponding criteria to be minimized. 1.2 Hierarchical Agglomeration Agglomerative hierarchical clustering (Ward <ref> [11] </ref>) is a stagewise procedure in which `optimal' pairs of clusters are successively merged. Each stage of merging corresponds to a unique 2 number of clusters, and a unique partition of the data.
References-found: 11

