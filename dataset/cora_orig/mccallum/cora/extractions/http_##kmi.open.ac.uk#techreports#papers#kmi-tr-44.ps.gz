URL: http://kmi.open.ac.uk/techreports/papers/kmi-tr-44.ps.gz
Refering-URL: http://kmi.open.ac.uk/techreports/kmi-tr-list.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> W. L. Buntine. </author> <title> Operations for learning with graphical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 159-225, </pages> <year> 1994. </year> <title> 9 Learn Bayesian Networks from Incomplete Databases </title>
Reference-contexts: In the following we will denote X i = x ij as x ij , and i = ij as ij . Several efforts have been addressed to develop Bayesian methods to learn bbns from databases <ref> [2, 1, 5] </ref>.
Reference: [2] <author> G.F. Cooper and E. Herskovitz. </author> <title> A bayesian method for the induction of probabilistic networks from data. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 309-347, </pages> <year> 1992. </year>
Reference-contexts: In the following we will denote X i = x ij as x ij , and i = ij as ij . Several efforts have been addressed to develop Bayesian methods to learn bbns from databases <ref> [2, 1, 5] </ref>. <p> The last term takes into account the completions that would lead to the updating of different conditional distributions. As the number of completions increase, the exact local updating is apparently infeasible: its complexity is in fact exponential in the number of missing data in the database <ref> [2] </ref>, and approximate methods therefore are required. Deterministic methods, such as fractional updating or matched moments [9, 3] are a possible solution.
Reference: [3] <author> R G Cowell, A.P. Dawid, and P. Sebastiani. </author> <title> A comparison of sequential learning methods for incomplete data. </title> <booktitle> In Bayesian Statistics 5, </booktitle> <pages> pages 533-542. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1996. </year>
Reference-contexts: As the number of completions increase, the exact local updating is apparently infeasible: its complexity is in fact exponential in the number of missing data in the database [2], and approximate methods therefore are required. Deterministic methods, such as fractional updating or matched moments <ref> [9, 3] </ref> are a possible solution. However, the need to process data sequentially, and the fact that the results of the learning process are sensitive to the order of cases in the database are serious limitations that can impair their applicability.
Reference: [4] <author> D. Heckerman. </author> <title> A tutorial on learning Bayesian networks. </title> <type> Technical Report MSR-TR-95-06, </type> <institution> Microsoft Research, Microsoft Corporation, </institution> <year> 1995. </year>
Reference-contexts: An alternative approach, very popular nowadays, is the use of a stochastic approximation of the posterior distribution using Markov Chain Monte Carlo (MCMC) methods, such as the Gibbs Sampling, see <ref> [4] </ref> for a recent review.
Reference: [5] <author> D. Heckerman, D. Geiger, </author> <title> and D.M. Chickering. Learning bayesian networks: The combinations of knowledge and statistical data. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 197-243, </pages> <year> 1995. </year>
Reference-contexts: In the following we will denote X i = x ij as x ij , and i = ij as ij . Several efforts have been addressed to develop Bayesian methods to learn bbns from databases <ref> [2, 1, 5] </ref>.
Reference: [6] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust learning with missing data. </title> <type> Technical Report KMi-TR-28, </type> <institution> Knowledge Media Institute, The Open University, </institution> <year> 1996. </year> <note> Available at http://kmi.open.ac.uk/techreports/KMi-TR-28. </note>
Reference-contexts: These bounds depend only on the frequencies of complete entries in the database and the "artificial" frequencies of the completed entries, so that the whole data can be processed at once. This method is implemented using an algorithm based on discrimination trees to efficiently store information about the parameters <ref> [6] </ref>.
Reference: [7] <author> M. Ramoni and P. Sebastiani. </author> <title> Robust parameter learning in Bayesian networks with missing data. </title> <booktitle> In Proceedings of the Sixth Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 339-406, </pages> <address> Fort Lauderdale, FL, </address> <year> 1997. </year>
Reference-contexts: The underlying assumption is that the unreported data are Missing at Random (MAR) so that D obs is a representative sample of D. Experimental evidences <ref> [8, 7] </ref> show that, when this assumption is violated, these methods are not robust with respect to the pattern of missing data. Ramoni and Sebastiani [7] introduced an efficient method to learn conditional probabilities from incomplete databases which does not rely on this assumption. <p> Experimental evidences [8, 7] show that, when this assumption is violated, these methods are not robust with respect to the pattern of missing data. Ramoni and Sebastiani <ref> [7] </ref> introduced an efficient method to learn conditional probabilities from incomplete databases which does not rely on this assumption. <p> Method In <ref> [7] </ref> it was shown that the missing entries in the database lead to bounds on the possible estimates that could be obtained from the complete database.
Reference: [8] <author> D J Spiegelhalter and R G Cowell. </author> <title> Learning in probabilistic expert systems. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 447-466. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1992. </year>
Reference-contexts: The underlying assumption is that the unreported data are Missing at Random (MAR) so that D obs is a representative sample of D. Experimental evidences <ref> [8, 7] </ref> show that, when this assumption is violated, these methods are not robust with respect to the pattern of missing data. Ramoni and Sebastiani [7] introduced an efficient method to learn conditional probabilities from incomplete databases which does not rely on this assumption.
Reference: [9] <author> D.J. Spiegelhalter and S.L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> Networks, </journal> <volume> 20 </volume> <pages> 157-224, </pages> <year> 1990. </year>
Reference-contexts: The Bayesian estimate of is then the posterior expectation E (jD) of . We shall assume that the parameters are mutually independent and have a Dirichlet distribution. Thus, by taking advantage of local computation and conjugate analysis <ref> [9] </ref>, if the prior distribution of the parameter vector i = f i1 ; : : : ; iM g associated to the conditional probabilities p (x im j ij ), m = 1; : : : ; M , is a Dirichlet D (ff i1 ; : : : ; <p> As the number of completions increase, the exact local updating is apparently infeasible: its complexity is in fact exponential in the number of missing data in the database [2], and approximate methods therefore are required. Deterministic methods, such as fractional updating or matched moments <ref> [9, 3] </ref> are a possible solution. However, the need to process data sequentially, and the fact that the results of the learning process are sensitive to the order of cases in the database are serious limitations that can impair their applicability.
Reference: [10] <author> A Thomas, D J Spiegelhalter, and W R Gilks. </author> <title> Bugs: A program to perform bayesian inference using gibbs sampling. </title> <booktitle> In Bayesian Statistics 4, </booktitle> <pages> pages 837-42. </pages> <publisher> Clarendon Press, Oxford, </publisher> <year> 1992. </year> <month> 10 </month>
Reference-contexts: The algorithm is iterated to reach stability, and then a sample from the joint posterior distribution is taken which can be used to provide empirical estimates of the posterior means <ref> [10] </ref>. Convergence rate and convergence detection are open issues of this stochastic method. These methods share a common strategy: they complete the database by inferring somehow the missing data from the available information and then learn from the completed database. <p> comparison with Gibbs Sampling meaningful, bc was provided with the MAR assumption. 5 Learn Bayesian Networks from Incomplete Databases 3.1 Materials and Methods We compared an implementation of bc written in Common Lisp under CLISP version 1996/10/10 to the implementation of the Gibbs Sampling provided by the BUGS version 0.5 <ref> [10] </ref> on a Sun Sparc 5 under SunOS 5.5. Figure 1 shows the graphical structure of the bbn | defined by three binary variables A, B, and C | used for this comparison.
References-found: 10

