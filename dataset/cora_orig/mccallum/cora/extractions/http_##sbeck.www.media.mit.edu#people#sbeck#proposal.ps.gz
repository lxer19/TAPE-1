URL: http://sbeck.www.media.mit.edu/people/sbeck/proposal.ps.gz
Refering-URL: http://sbeck.www.media.mit.edu/people/sbeck/research.html
Root-URL: http://www.media.mit.edu
Title: Vision-assisted modeling for model-based video coding  
Author: by Shawn C. Becker V. Michael Bove Jr. Alex P. Pentland Seth Teller Stephen A. Benton 
Degree: Thesis Proposal for the degree of Doctor of Philosophy at the  Thesis Advisor:  Associate Professor Program in Media Arts and Sciences Thesis Readers:  Assistant Professor  Chairman, Departmental Committee on Graduate Students  
Note: Associate Professor of Computers, Communication and Design Technology Program in  Program in  
Affiliation: Massachusetts Institute of Technology  Media Arts and Sciences  Department of Electrical Engineering and Computer Science  Media Arts and Sciences  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> R.Y. Tsai, </author> <title> An Efficient and Accurate Camera Calibration Technique for 3-D Machine Vision, </title> <booktitle> Proceedings IEEE Computer Society Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Miami Beach, Florida, </address> <pages> pp. 364-374, </pages> <month> June 22-26, </month> <year> 1986. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [2] <author> R.K. Lenz and R.Y. Tsai, </author> <title> Techniques for Calibration of the Scale Factor and Image Center for High Accuracy 3-D Machine Vision Metrology, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 19, No. 5, </volume> <month> September </month> <year> 1988. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [3] <author> D. Zhang, Y. Nomura, and S. Fujii, </author> <title> A simple and accurate camera calibration method, </title> <booktitle> Proceedings of the SPIE, </booktitle> <volume> Vol. 1822, </volume> <pages> pp. 139-148, </pages> <year> 1992. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [4] <author> Z.C. Lai, </author> <title> On the sensitivity of camera calibration, </title> <booktitle> Proceedings of the SPIE, </booktitle> <volume> Vol. 1822, </volume> <year> 1992. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [5] <author> Y. Nomura, M. Sagara, Hiroshi Naruse, and Atsushi Ide, </author> <title> Simple Calibration Algorithm for High-Distortion-Lens Camera, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 14, No. 11, </volume> <pages> pp. 1095-1099, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [6] <author> S. Shah and J.K. Aggarwal, </author> <title> A Simple Calibration Procedure for Fish-Eye (High Distortion) Lens Camera, </title> <booktitle> Proceedings 1994 IEEE International Conference on Robotics and Automation San Diego, California, </booktitle> <pages> pp. 3422-3427, </pages> <month> May 8-13, </month> <year> 1994. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [7] <author> S.W. Shih, Y.P. Hung, and W.S. Lin, </author> <title> When should we consider lens distortion in camera calibration, </title> <journal> Pattern Recognition, </journal> <volume> Vol. 28. No. 3, </volume> <pages> pp. 447-461, </pages> <year> 1995. </year>
Reference-contexts: Typical approaches <ref> [1, 2, 3, 4, 5, 6, 7] </ref> employ specially constructed calibration templates (usually a planar grid) with a large number of precisely positioned fiducial points or lines.
Reference: [8] <author> G.P. Stein, </author> <title> Internal Camera Calibration using Rotation and Geometric Shapes, </title> <institution> Thesis for Master of Science in EECS at MIT, </institution> <month> February </month> <year> 1993. </year>
Reference-contexts: Using this formulation, lines which are parallel in the scene are constrained to be straight as well as share a common vanishing point in the image. 2.1.5 Projections of spheres Assuming negligible radial lens distortion and a scene with two or more spheres, Stein <ref> [8] </ref> solves principal point, effective focal length and horizontal pixel scale by analyzing each sphere's 9 elliptical projection on the image plane.
Reference: [9] <author> O.D. Faugeras, </author> <title> What can be seen in three dimensions with an uncalibrated stereo rig? Proceedings Computer Vision EECV'92 Second European Conference on Computer Vision, </title> <editor> p. </editor> <booktitle> xv+909, </booktitle> <pages> 563-78, </pages> <address> Santa Margherita Ligure, Italy, </address> <month> May 18-23, </month> <year> 1992. </year>
Reference-contexts: As described by Faugeras <ref> [9, 10] </ref> and others [11] given 4 nonplanar point correspondences, point-wise scene structure can be recovered up to an arbitrary affine transformation relative to the positions of those four points in three-dimensional space.
Reference: [10] <author> O.D. Faugeras, </author> <title> Stratification of three-dimensional vision: projective, affine, and metric representations, </title> <journal> Journal Optical Society of America, </journal> <volume> Vol 12, No. 3, </volume> <pages> pp. 465-484, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: As described by Faugeras <ref> [9, 10] </ref> and others [11] given 4 nonplanar point correspondences, point-wise scene structure can be recovered up to an arbitrary affine transformation relative to the positions of those four points in three-dimensional space.
Reference: [11] <author> A. Shashua and N. Navab, </author> <title> Relative affine structure: theory and application to 3D reconstruction from perspective views, </title> <booktitle> In Proceedings IEEE Conference on CVPR, </booktitle> <address> Seattle, Washington, </address> <year> (1994). </year>
Reference-contexts: As described by Faugeras [9, 10] and others <ref> [11] </ref> given 4 nonplanar point correspondences, point-wise scene structure can be recovered up to an arbitrary affine transformation relative to the positions of those four points in three-dimensional space. This formulation completely evades the effects of camera calibration by recovering only relative information at the expense of losing metric.
Reference: [12] <author> B. Caprile and V. Torre, </author> <title> Using vanishing points for camera calibration, </title> <journal> IJCV, </journal> <volume> Vol. 4, </volume> <pages> pp. 127-140, </pages> <year> 1990. </year>
Reference-contexts: Caprile and Torre <ref> [12] </ref> and Wang and Tsai [14] exploit these properties to determine principal point and focal length from the vanishing points in a view of a cube. These algorithms assume known or negligable lens distortion. Since they use so few features, these calibration approaches are quite sensitive to detection errors.
Reference: [13] <author> P. Parodi and G. Piccioli, </author> <title> "3D Shape Reconstruction by Using Vanishing Points", </title> <journal> IEEE Trans. on Pattern Anal. and Mach. Intel., </journal> <volume> Vol. 18, No. 2, </volume> <pages> pp. 211-217, </pages> <month> February </month> <year> 1996. </year> <month> 30 </month>
Reference-contexts: More recently algorithms have been presented [52, 53] which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well. In the special case of trihedral scenes with known vanishing points <ref> [13] </ref> show that line detection can be constrained and in some cases labeled automatically. <p> If a line is assigned two surfaces then that line marks the hinge or intersection between two planes. In order for the structure-and-motion-recovery stage to recover surface positions, all surfaces must share some hinge-line with another surface. Scenes with this property are sometimes called "origami" scenes <ref> [13] </ref>. 3.2.6 Optional specification of additional line correspondences If the two surfaces mentioned above have visible lines in a second image, then the hinge-line in the second image implicitly matches the hinge-line of the first.
Reference: [14] <author> L.L. Wang and W.H. Tsai, </author> <title> Computing Camera Parameters using Vanishing-Line Infor--mation from a Rectangular Parallelipiped, </title> <journal> Machine Vision and Applications, </journal> <volume> 3 </volume> <pages> 129-141, </pages> <year> 1990. </year>
Reference-contexts: Caprile and Torre [12] and Wang and Tsai <ref> [14] </ref> exploit these properties to determine principal point and focal length from the vanishing points in a view of a cube. These algorithms assume known or negligable lens distortion. Since they use so few features, these calibration approaches are quite sensitive to detection errors.
Reference: [15] <author> Y. Lui and T.S. Huang, </author> <title> Motion estimation from corner correspondences, </title> <booktitle> Proceedings International Conference on Image Processing, </booktitle> <address> Singapore, </address> <month> September 5-8, </month> <pages> pp. 785-790, </pages> <year> 1989.NEED </year>
Reference-contexts: Several approaches [43, 42, 41] show how 4 or more matching points known to be coplanar provide solutions for incremental camera rotation, plane orientation and translation to an unknown scale factor. Others <ref> [15] </ref> use orthogonal corners as features, that is points that mark the intersection of 3 mutually orthogonal edges. Given a calibrated camera, one orthogonal corner and two point or line correspondences are all that are needed to determine motion and structure uniquely.
Reference: [16] <editor> S.C. Becker and V.M. Bove Jr., </editor> <title> Semiautomatic 3-D model extraction from uncalibrated 2-D views, </title> <booktitle> Proceedings SPIE Visual Data Exploration and Analysis II, </booktitle> <volume> Vol. 2410, </volume> <pages> pp. 447-461, </pages> <address> San Jose, California, </address> <month> February 8-10, </month> <year> 1995. </year>
Reference-contexts: These algorithms assume known or negligable lens distortion. Since they use so few features, these calibration approaches are quite sensitive to detection errors. While previous examples use 3-point perspective to calibrate cameras using rectangular solids of known dimensions, Becker and Bove <ref> [16] </ref> show how one or more perspective projections of a right parallelipiped with 3-point perspective can be used to recover rectangular object dimensions as well as intrinsic and extrinsic parameters. <p> This approach to solving lens distortion differs from template-based calibration techniques in that no knowledge of world points is required. Rather, linear relationships between points (i.e. straightness) is all that is known. Becker and Bove <ref> [16] </ref> improve upon the plumb-line technique by exploiting the vanishing point property of parallel lines as well as Brown's straightness constraint. They present an iterative technique for finding the distortion parameters which minimize the vanishing point dispersion derived using Kanatani's [20] statistical foundations. <p> Requirements for camera calibration: * two or more lines in each of three non-coplanar directions with known 3-D heading * exactly one line of known 3-D position, 3-D direction and length Lens distortion correction In each image, a batch estimation technique, described in a previous article <ref> [16] </ref>, uses the downhill simplex method 9 to find the distortion parameters which globally minimize the vanishing point dispersion for all lines of the same direction. As described in [16] these optimum parameters are used to undistort image textures via stochastic resampling and to undistort associated point and lines. <p> 3-D position, 3-D direction and length Lens distortion correction In each image, a batch estimation technique, described in a previous article <ref> [16] </ref>, uses the downhill simplex method 9 to find the distortion parameters which globally minimize the vanishing point dispersion for all lines of the same direction. As described in [16] these optimum parameters are used to undistort image textures via stochastic resampling and to undistort associated point and lines. Independent view calibration Image relative center of projection, or COP, is found directly from the positions of vanishing points from lines which are associated with 3 known directions.
Reference: [17] <author> D.H. Ballard, </author> <title> Generalizing the Hough Transform to Detect Arbitrary Shapes, </title> <year> 1980.CHECK </year>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in <ref> [17, 19, 18] </ref>. Error analysis of vanishing point estimation is discussed in [20, 21, 22]. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene. <p> Pose search methods [37, 74] attempt to test the space of all poses to find the one that best fits the model to the observed image. Pose clustering methods [85] use techniques like the generalized Hough transform <ref> [17] </ref> to accumulate evidence for possible coordinate transformations from all possible matches. And alignment methods [84, 55, 78] compute pose from all possible matches of critical sets and then see if any of the remaining model features are supported by image features.
Reference: [18] <author> R.T. Collins and R.S. Weiss, </author> <title> Vanishing Point Calculation as a Statistical Inference on the Unit Sphere, </title> <journal> Machine Vision and Applications, </journal> <volume> 3 </volume> <pages> 400-403, </pages> <year> 1990. </year>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in <ref> [17, 19, 18] </ref>. Error analysis of vanishing point estimation is discussed in [20, 21, 22]. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene.
Reference: [19] <author> E. Lutton, H. Maitre, and J. Lopez-Krahe, </author> <title> Contribution to the Determination of Vanishing Points Using Hough Transform, </title> <journal> IEEE PAMI, </journal> <volume> Vol. 16, No. 4. </volume> <month> April, </month> <year> 1994, </year> <pages> pp. 430-438. </pages>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in <ref> [17, 19, 18] </ref>. Error analysis of vanishing point estimation is discussed in [20, 21, 22]. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene.
Reference: [20] <author> K. Kanatani, </author> <title> Statistical foundation for hypothesis testing of image data, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 60, No. 3, </volume> <month> November, </month> <pages> pp. 382-391, </pages> <year> 1994. </year>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in [17, 19, 18]. Error analysis of vanishing point estimation is discussed in <ref> [20, 21, 22] </ref>. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene. <p> Becker and Bove [16] improve upon the plumb-line technique by exploiting the vanishing point property of parallel lines as well as Brown's straightness constraint. They present an iterative technique for finding the distortion parameters which minimize the vanishing point dispersion derived using Kanatani's <ref> [20] </ref> statistical foundations. <p> In some cases, lines which share the same vanishing point can be automatically grouped into the same direction. We can represent 2-D lines in the image as N-vectors <ref> [20] </ref> or epipolar normals on the Gaussian sphere of unit directions whose center is some approximate center of projection 5 .
Reference: [21] <author> K. Kanatani, </author> <title> Statistical analysis of focal-length calibration using vanishing points, </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> Vol. 8, No. 6, </volume> <month> December, </month> <year> 1992, </year> <pages> pp. 767-775. </pages>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in [17, 19, 18]. Error analysis of vanishing point estimation is discussed in <ref> [20, 21, 22] </ref>. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene.
Reference: [22] <author> K. Kanatani, </author> <title> Computational Cross Ratio for Computer Vision, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 60, No. 3, </volume> <month> November, </month> <pages> pp. 371-381, </pages> <year> 1994. </year>
Reference-contexts: Robust algorithms for finding vanishing points from a sets of image lines are presented in [17, 19, 18]. Error analysis of vanishing point estimation is discussed in <ref> [20, 21, 22] </ref>. The disadvantage of 3-point perspective approaches to camera calibration is the requirement that sufficient rectangular structure is visible in the scene.
Reference: [23] <author> D. C. Brown, </author> <title> Close-range camera calibration, </title> <booktitle> Presented at the Symposium on close-range photogrammetry, </booktitle> <address> Urbana, Illinois, </address> <month> January, </month> <year> 1971. </year>
Reference-contexts: intrinsic and extrinsic parameters can be accurately recovered without requiring explicit measurements, but simply by taking advantage of geometric properties of objects visible in a single image. 2.1.4 Plumb-line techniques Another example of an approach that takes advantage of geometric properties in scenes to solve intrinsic camera parameters is Brown's <ref> [23] </ref> plumb-line technique for solving lens distortion. Given a single image of a scene of parallel edges, the effects of lens distortion can be corrected by iteratively finding the distortion parameters that minimize curvature among points in each line in the distorted image.
Reference: [24] <author> A. Azarbayejani and A. Pentland, </author> <title> Recursive estimation of motion, structure, and focal length, </title> <journal> IEEE Pattern Analysis and Machine Intelligence, </journal> <month> April, </month> <year> 1994. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth [46, 48, 49, 50]. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. Structure-from-motion approaches <ref> [24, 25] </ref> use large numbers of automatically detected and 4 tracked features over almost-calibrated views 1 to recover camera-relative motion and depth. 1.1.2 Modeling At the other end of the scale, CAD approaches to scene modeling use critically small sets of features carefully specified with manual intervention. <p> If these equations are sufficiently overconstrained, intrinsic parameters (like focal length) can be solved along with extrinsic camera parameters and scene structure simultaneously as shown by Azarbayejani and Pentland <ref> [24, 26] </ref>. 2.2 Structure-from-motion algorithms The recovery of scene structure from 2-D images alone is a convenient and powerful means of quickly acquiring knowledge about a scene. <p> To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem. Several approaches have been presented <ref> [39, 40, 24] </ref> which use incremental estimation frameworks based on the Extended Kalman Filter or EKF [38].
Reference: [25] <author> B. Horowitz, A. Azarbyejani, T. Galyean, and A. Pentland, </author> <title> "Models from Video", MIT Media Laboratory Vision and Modeling Technical Report #207 </title>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth [46, 48, 49, 50]. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. Structure-from-motion approaches <ref> [24, 25] </ref> use large numbers of automatically detected and 4 tracked features over almost-calibrated views 1 to recover camera-relative motion and depth. 1.1.2 Modeling At the other end of the scale, CAD approaches to scene modeling use critically small sets of features carefully specified with manual intervention.
Reference: [26] <author> A. Azarbayejani and A. Pentland, </author> <title> Camera self-calibration from one point correspondence, </title> <institution> MIT Media Laboratory, </institution> <note> Perceptual Computing Technical Report #341 Submitted to the IEEE Symposium on Computer Vision, </note> <month> April, </month> <year> 1995. </year>
Reference-contexts: If these equations are sufficiently overconstrained, intrinsic parameters (like focal length) can be solved along with extrinsic camera parameters and scene structure simultaneously as shown by Azarbayejani and Pentland <ref> [24, 26] </ref>. 2.2 Structure-from-motion algorithms The recovery of scene structure from 2-D images alone is a convenient and powerful means of quickly acquiring knowledge about a scene.
Reference: [27] <author> R. Szeliski and S.B. Kang, </author> <title> "Recovering 3d shape and motion from image streams using non-linear least squares", </title> <type> CRL 93/3, </type> <institution> Digital Equipment Corporation, Cambridge Research Labs, </institution> <address> Cambridge, MA, </address> <month> March </month> <year> 1993. </year>
Reference-contexts: Several approaches have been presented [39, 40, 24] which use incremental estimation frameworks based on the Extended Kalman Filter or EKF [38]. Others <ref> [32, 27] </ref> use batch estimation frameworks, having higher computational overhead but potentially better accuracy since they avoid the linearizing assumptions inherent in the EKF. 10 2.2.2 2-D to 2-D planar points and orthogonal corners If these matched 2-D points in multiple images have some known structure, then this structure can be
Reference: [28] <author> T.S. Huang and A.N. Netravali, </author> <title> Motion and Structure from Feature Correspondences: A Review, </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> Vol. 82, No. 2, </volume> <month> February, </month> <pages> pp. 252-268, </pages> <year> 1994. </year> <month> 31 </month>
Reference-contexts: Having determined relative pose (or relative orientation), the 3-D position of features relative to one camera can be found by triangulation. 2.2.1 2-D to 2-D point correspondences Examples of structure-from-motion techniques which use critical sets of matching 2-D points among two or more images include <ref> [31, 28, 29, 30] </ref>. To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem. <p> Straight line features provide useful structural information (as shown in section 2.1) and since they have more image support they can be localized more accurately. While these properties make lines advantageous for use in structure-from-motion algorithms they require more views and more correspondences than points. As described in <ref> [28, 34, 37] </ref> at least 3 views of 6 or more line correspondences, are required as long as the lines are not arranged in certain degenerate configurations. 2.2.4 Optical flow and spatiotemporal derivatives Optical flow can be interpreted as the field of 2-D image motion caused by 3-D motion of a <p> Haralick [56] gives a good review of these techniques and their relative numerical stability. If 4 or 5 point correspondences are available, either a linear least-squares solution can be found <ref> [28] </ref> or Fischler and Bolle's RANSAC method [55] can be used to find the solution that best describes all observations. Ganapathy [94] presents a method that in general produce a unique solution given 6 or more point correspondences as long as the points are not colinear on the image plane.
Reference: [29] <author> H.C. Longuet-Higgens, </author> <title> A computer algorithm for reconstructing a scene from two projec-tions, </title> <journal> Nature, </journal> <volume> 293(10), </volume> <pages> pp. 133-135, </pages> <year> 1981. </year>
Reference-contexts: Having determined relative pose (or relative orientation), the 3-D position of features relative to one camera can be found by triangulation. 2.2.1 2-D to 2-D point correspondences Examples of structure-from-motion techniques which use critical sets of matching 2-D points among two or more images include <ref> [31, 28, 29, 30] </ref>. To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem.
Reference: [30] <author> A. Kara, K.M. Wilkes and K. Kawamura, </author> <title> 3D Structure Reconstruction from Point Correspondences between Two Perspective Projections, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 60, No. 3, </volume> <month> November, </month> <pages> pp. 392-397, </pages> <year> 1994. </year>
Reference-contexts: Having determined relative pose (or relative orientation), the 3-D position of features relative to one camera can be found by triangulation. 2.2.1 2-D to 2-D point correspondences Examples of structure-from-motion techniques which use critical sets of matching 2-D points among two or more images include <ref> [31, 28, 29, 30] </ref>. To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem.
Reference: [31] <author> S. Ullman, </author> <title> The Interpretation of Visual Motion, </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Ma, </address> <year> 1979. </year>
Reference-contexts: Having determined relative pose (or relative orientation), the 3-D position of features relative to one camera can be found by triangulation. 2.2.1 2-D to 2-D point correspondences Examples of structure-from-motion techniques which use critical sets of matching 2-D points among two or more images include <ref> [31, 28, 29, 30] </ref>. To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem.
Reference: [32] <author> Carlo Tomasi and Takeo Kanade, </author> <title> Shape and motion from image streams: a factorization method; full report on the orthographic case, </title> <journal> International Journal of Computer Vision, </journal> <volume> 9(2) </volume> <pages> 127-154, </pages> <month> November, </month> <year> 1992. </year>
Reference-contexts: Several approaches have been presented [39, 40, 24] which use incremental estimation frameworks based on the Extended Kalman Filter or EKF [38]. Others <ref> [32, 27] </ref> use batch estimation frameworks, having higher computational overhead but potentially better accuracy since they avoid the linearizing assumptions inherent in the EKF. 10 2.2.2 2-D to 2-D planar points and orthogonal corners If these matched 2-D points in multiple images have some known structure, then this structure can be
Reference: [33] <author> Larry Matthies and Takeo Kanade, </author> <title> "Kalman Filter-based Algorithms for Estimating Depth from Image Sequences", </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 3, </volume> <pages> pp. 209-236, </pages> <year> 1989. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth [46, 48, 49, 50]. Multi-baseline-stereo approaches <ref> [33] </ref> use optical flow or matched features over views with known motion to recover scene structure and texture. <p> If there are local deviations from planarity and if sufficient texture detail exists we can actually correct for it. As described in <ref> [90, 33] </ref> given known or pre-calculated cameras, image texture from one subject view can be planar perspective transformed to match the surface as seen in some reference view 12 .
Reference: [34] <author> N. Navab, O.D. Faugeras and T. Vieville, </author> <title> The Critical Sets of Lines for Camera Displacement Estimation: A Mixed Euclidean-Projective and Constructive Approach, </title> <address> ICCV, </address> <year> 1993. </year>
Reference-contexts: Straight line features provide useful structural information (as shown in section 2.1) and since they have more image support they can be localized more accurately. While these properties make lines advantageous for use in structure-from-motion algorithms they require more views and more correspondences than points. As described in <ref> [28, 34, 37] </ref> at least 3 views of 6 or more line correspondences, are required as long as the lines are not arranged in certain degenerate configurations. 2.2.4 Optical flow and spatiotemporal derivatives Optical flow can be interpreted as the field of 2-D image motion caused by 3-D motion of a <p> To explain why even non-critical sets (i.e. more than the theoretic minimum) of line correspondences do not always give robust results, Navab and Faugeras <ref> [34] </ref> show that for certain degenerate configurations no unique solution for pose can be found. Kumar and Hanson [58] present a RANSAC-based iterative technique which minimizes error due to rotation and translation simultaneously, and claim its superiority to techniques that solve for rotation and translation separately.
Reference: [35] <author> A. Pentland, A. and S. Sclaroff, </author> <title> Closed-Form Solutions For Physically Based Shape Modeling and Recognition, </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 13, No. 7, </volume> <pages> pp. 715-730, </pages> <year> 1991. </year>
Reference: [36] <editor> D.J. Heeger and J.R. Bergen, Pyramid-Based Texture Analysis/Synthesis, </editor> <booktitle> Computer Graphics Proceedings Siggraph, </booktitle> <address> Los Angeles, </address> <month> April, </month> <pages> pp. 229-238, </pages> <year> 1995. </year>
Reference-contexts: This brings the testing cost down only quadratic order in the number of regions. Approaches for color histogram-based region segmentation and matching are found in [86, 77]. Texture analysis and synthesis results can also be used for texture-based region segmentation as shown in <ref> [88, 36, 89] </ref>. 12 Another way to prune the number of hypothesis tests needed for pose determination is consider only those 2-D and 3-D features that have similar identifying characteristics. Properties which are invariant (i.e. consistent) even under large changes in viewing or lighting conditions are particularly useful.
Reference: [37] <author> C.J. Taylor and D.J. Kriegman, </author> <title> Structure and Motion from Line Segments in Multiple Images, </title> <institution> Yale University Technical Report No. 0402b, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Straight line features provide useful structural information (as shown in section 2.1) and since they have more image support they can be localized more accurately. While these properties make lines advantageous for use in structure-from-motion algorithms they require more views and more correspondences than points. As described in <ref> [28, 34, 37] </ref> at least 3 views of 6 or more line correspondences, are required as long as the lines are not arranged in certain degenerate configurations. 2.2.4 Optical flow and spatiotemporal derivatives Optical flow can be interpreted as the field of 2-D image motion caused by 3-D motion of a <p> Nagao and Horn [81] present a similar technique that uses direct moments of image intensities and intensity gradients. 2.3.2 Strategies for feature-based pose-determination Approaches for determining arbitrary pose given a hypothesized model can be categorized into 3 groups. Pose search methods <ref> [37, 74] </ref> attempt to test the space of all poses to find the one that best fits the model to the observed image. Pose clustering methods [85] use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. <p> Self-calibrating structure-from-motion algorithms can be used to get a scene model directly from an image sequence, but this approach requires many feature correspondences over a long sequence of images, and may still not provide the required level of detail and structural accuracy. Some in the vision community <ref> [37] </ref> have suggested that the accuracy and robustness of structure-from-motion algorithms can be improved if we go beyond simple feature correspondence and attempt to exploit other geometric relationships among features. This thesis work attempts to satisfy this need.
Reference: [38] <author> A. Gelb, </author> <title> Applied Optimal Estimation, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1989. </year>
Reference-contexts: Several approaches have been presented [39, 40, 24] which use incremental estimation frameworks based on the Extended Kalman Filter or EKF <ref> [38] </ref>. <p> Use of line correspondences may be preferred over points because of their relative ease of detection and tracking. As shown by Liu, Huang and Faugeras [57], EKF <ref> [38] </ref> can be used to first find camera rotation given 3 line correspondences, and then 3 additional line correspondences are sufficient to determine camera translation.
Reference: [39] <author> T.J. Broida, S. Chandrashekhar, and R. Chellappa, </author> <title> Recursive 3-D motion estimation from a monocular image sequence, </title> <journal> IEEE Transactions on Aerospace and Electronic Systems, </journal> <volume> Vol. 16, No. 4, </volume> <month> July, </month> <year> 1990. </year>
Reference-contexts: To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem. Several approaches have been presented <ref> [39, 40, 24] </ref> which use incremental estimation frameworks based on the Extended Kalman Filter or EKF [38].
Reference: [40] <author> N.Cui, J.J. Weng, and P. Cohen, </author> <title> Recursive-Batch Estimation of Motion and Structure from Monocular Image Sequences, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 59, No. 2, </volume> <month> March, </month> <pages> pp. 154-170, </pages> <year> 1994. </year>
Reference-contexts: To combat difficulties presented with trying to solve the non-linear projection equation with noisy or erroneous feature matches, some researchers are using long image sequences to overconstrain the problem. Several approaches have been presented <ref> [39, 40, 24] </ref> which use incremental estimation frameworks based on the Extended Kalman Filter or EKF [38].
Reference: [41] <author> S. Mann and S. C. Becker, </author> <title> "Computation of some projective-chirplet-transform and metaplectic-chirplet-transform subspaces, with applications in image processing", </title> <booktitle> Proceedings DSP World Symposium, </booktitle> <address> Boston, Massachusetts, </address> <month> November, </month> <year> 1992. </year>
Reference-contexts: Several approaches <ref> [43, 42, 41] </ref> show how 4 or more matching points known to be coplanar provide solutions for incremental camera rotation, plane orientation and translation to an unknown scale factor. Others [15] use orthogonal corners as features, that is points that mark the intersection of 3 mutually orthogonal edges.
Reference: [42] <author> T. Melen, </author> <title> "Extracting physical camera parameters from the 3 by 3 direction linear transformation matrix", Optical 3-D Measurement Techniques II: Applications in inspection, quality control and robotics, </title> <publisher> Wichman Pub., </publisher> <editor> edited by A. Gruen and H. </editor> <booktitle> Kahmen, </booktitle> <pages> pp. 355-365, </pages> <year> 1993. </year>
Reference-contexts: Several approaches <ref> [43, 42, 41] </ref> show how 4 or more matching points known to be coplanar provide solutions for incremental camera rotation, plane orientation and translation to an unknown scale factor. Others [15] use orthogonal corners as features, that is points that mark the intersection of 3 mutually orthogonal edges.
Reference: [43] <author> R. Y. Tsai and T. S. Huang, </author> <title> Estimating Three-Dimensional Motion Parameters of a Rigid Planar Patch, </title> <journal> IEEE Transactions on Acoustics, Speech and Signal Processing, </journal> <volume> Vol. ASSP-29, No. 6, </volume> <month> December, </month> <pages> pp. 1147-1152, </pages> <year> 1981. </year>
Reference-contexts: Several approaches <ref> [43, 42, 41] </ref> show how 4 or more matching points known to be coplanar provide solutions for incremental camera rotation, plane orientation and translation to an unknown scale factor. Others [15] use orthogonal corners as features, that is points that mark the intersection of 3 mutually orthogonal edges. <p> Adiv [46] attempts to fit optical flow to locally planar patches and then does local hypothesis testing to aggregate patches that have compatible motion parameters (i.e. the 8-parameter planar perspective transformation used by <ref> [43] </ref>). Adiv's result can be considered to similar to [68] except that a 3-D perspective camera model was assumed. Here we see a blurring between efforts of video coding and total 3-D scene reconstruction.
Reference: [44] <author> K. Prazdny, </author> <title> On the Information in Optical Flows, Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 22, </volume> <pages> pp. 239-259, </pages> <year> 1983. </year>
Reference-contexts: Theoretically, it can be used to recover relative motion and depth <ref> [44] </ref> but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by [45].
Reference: [45] <author> Barron, Fleet and Beauchemin, </author> <title> Systems and Experiment: Performance of Optical Flow Techniques, </title> <journal> International Journal of Computer Vision, </journal> <volume> 12:1, </volume> <pages> pp. 43-77, </pages> <year> 1994. </year>
Reference-contexts: Theoretically, it can be used to recover relative motion and depth [44] but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by <ref> [45] </ref>. <p> Adiv's result can be considered to similar to [68] except that a 3-D perspective camera model was assumed. Here we see a blurring between efforts of video coding and total 3-D scene reconstruction. As pointed out by <ref> [45] </ref> structure-from-motion algorithms are ill-suited for general video coding because of the difficulties of feature detection and tracking due to the aperture problem (which is that image flow can only be resolved in the direction of available texture detail).
Reference: [46] <author> G. Adiv, </author> <title> Determining Three-Dimensional Motion and Structure from Optical Flow Generated by Several Moving Objects, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. PAMI-7, No. 4, </volume> <pages> pp. 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth <ref> [46, 48, 49, 50] </ref>. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. <p> Theoretically, it can be used to recover relative motion and depth [44] but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by [45]. Structure-from-motion algorithms using optical flow and spatiotemporal brightness gradients directly have been presented by <ref> [46] </ref> and [48, 49, 50]. 2.3 Pose-determination from known structure Suppose we use camera calibration and structure-from-motion techniques to recover an accurate photorealistic model of a scene from a few stills or key frames of video. <p> Kermode [70] developed a hybrid video coder that uses similar principles but adds capabilities for image mosaicing and resolution enhancement and works within the current 14 MPEG digital video coding framework. Adiv <ref> [46] </ref> attempts to fit optical flow to locally planar patches and then does local hypothesis testing to aggregate patches that have compatible motion parameters (i.e. the 8-parameter planar perspective transformation used by [43]). <p> for the modelMaker application may be found at [113] courtesy Eugene Lin. 3.4 A proposed application : Model-based video coding Recent results for affine optical flow segmentation approaches [68, 69, 70] for video coding are promising, but limited success appears to have been made in handling the true perspective case <ref> [46, 91] </ref>.
Reference: [47] <author> B. K. P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Local intensity gradient magnitude can be used as a measure of certainty for surface disparity and relaxation techniques <ref> [47] </ref> can be used to smooth and fill in areas with low certainty and to force hinge edges at surface boundaries to remain fixed so they properly join with other connected surfaces. This approach can be repeated for all subject views where portions of the surface are visible.
Reference: [48] <author> S. Negahdaripour, and B. K. P. Horn, </author> <title> Direct Passive Navigation, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-9:1, </volume> <month> January </month> <year> 1987. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth <ref> [46, 48, 49, 50] </ref>. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. <p> Theoretically, it can be used to recover relative motion and depth [44] but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by [45]. Structure-from-motion algorithms using optical flow and spatiotemporal brightness gradients directly have been presented by [46] and <ref> [48, 49, 50] </ref>. 2.3 Pose-determination from known structure Suppose we use camera calibration and structure-from-motion techniques to recover an accurate photorealistic model of a scene from a few stills or key frames of video.
Reference: [49] <author> Berthold K. P. Horn and E.J. Weldon Jr., </author> <title> Direct methods for recovering motion, </title> <journal> IJCV, </journal> <volume> 2, </volume> <month> 51-76 </month> <year> 1988. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth <ref> [46, 48, 49, 50] </ref>. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. <p> Theoretically, it can be used to recover relative motion and depth [44] but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by [45]. Structure-from-motion algorithms using optical flow and spatiotemporal brightness gradients directly have been presented by [46] and <ref> [48, 49, 50] </ref>. 2.3 Pose-determination from known structure Suppose we use camera calibration and structure-from-motion techniques to recover an accurate photorealistic model of a scene from a few stills or key frames of video.
Reference: [50] <author> David Heeger and Allan Jepson, </author> <title> Simple method for computing 3D motion and depth, </title> <booktitle> Proceedings 3rd IEEE International Conference on Computer Vision, </booktitle> <pages> pp. 96-100, </pages> <year> 1990. </year>
Reference-contexts: For example, optical flow over many views can be used used to recover qualitative measures of camera-relative motion and depth <ref> [46, 48, 49, 50] </ref>. Multi-baseline-stereo approaches [33] use optical flow or matched features over views with known motion to recover scene structure and texture. <p> Theoretically, it can be used to recover relative motion and depth [44] but its use for anything but a qualitative measure of structure and motion in real image sequences is questioned by [45]. Structure-from-motion algorithms using optical flow and spatiotemporal brightness gradients directly have been presented by [46] and <ref> [48, 49, 50] </ref>. 2.3 Pose-determination from known structure Suppose we use camera calibration and structure-from-motion techniques to recover an accurate photorealistic model of a scene from a few stills or key frames of video.
Reference: [51] <author> A.F. Bobick, </author> <title> Using Stability of Interpretation as Verification for Low Level Processing: An example from ego-motion and optic flow, </title> <booktitle> IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> New York, </address> <month> June </month> <year> 1993. </year>
Reference: [52] <author> T. Marill, </author> <title> Emulating the Human Interpretation of Line-Drawings as Three-Dimensional Objects, </title> <journal> IJCV, </journal> <volume> 6:2, </volume> <pages> pp. 147-161, </pages> <year> 1991. </year>
Reference-contexts: Initial work [95, 97, 96] focussed on the problem of partitioning wireframe scenes into separate trihedral polyhedral objects made up of collections of labeled vertices and edges. More recently algorithms have been presented <ref> [52, 53] </ref> which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well. In the special case of trihedral scenes with known vanishing points [13] show that line detection can be constrained and in some cases labeled automatically.
Reference: [53] <author> Yvan G. Leclerc and Martin A. Fischler, </author> <title> An Optimization-Based Approach to the Interpretation of Single Line Drawings as 3D Wire Frames, </title> <address> IJCV, 9:2, </address> <month> 113-136 </month> <year> (1992) </year>
Reference-contexts: Initial work [95, 97, 96] focussed on the problem of partitioning wireframe scenes into separate trihedral polyhedral objects made up of collections of labeled vertices and edges. More recently algorithms have been presented <ref> [52, 53] </ref> which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well. In the special case of trihedral scenes with known vanishing points [13] show that line detection can be constrained and in some cases labeled automatically.
Reference: [54] <author> R. Nevatia, M. Zerroug, and F. Ulupinar, </author> <title> Recovery of Three-Dimensional Shape of Curved Objects from a Single Image, /em Handbook of Pattern Recognition and Image Processing: Computer Vision, </title> <publisher> Academy Press, </publisher> <pages> pp. 101-129, </pages> <year> 1994. </year>
Reference-contexts: Object contours (i.e. the locus of visible surface points either tangent to the view or discontinuous in surface curvature) are another type of image cue which convey rich geometric information and are stable with respect to illumination changes. Algorithms are presented <ref> [54] </ref> to recover a description of generalized cylinders by fitting an observed or hypothesized cross section to an observed set of contours.
Reference: [55] <author> M.A. Fischler and R.C. Bolles, </author> <title> Random Sample Consensus: A Paradigm for Model Fitting with Applications to Image Analysis and Automated Cartography, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 24, No. 6, </volume> <pages> pp. 381-395, </pages> <month> June </month> <year> 1981. </year>
Reference-contexts: Haralick [56] gives a good review of these techniques and their relative numerical stability. If 4 or 5 point correspondences are available, either a linear least-squares solution can be found [28] or Fischler and Bolle's RANSAC method <ref> [55] </ref> can be used to find the solution that best describes all observations. Ganapathy [94] presents a method that in general produce a unique solution given 6 or more point correspondences as long as the points are not colinear on the image plane. <p> Pose clustering methods [85] use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. And alignment methods <ref> [84, 55, 78] </ref> compute pose from all possible matches of critical sets and then see if any of the remaining model features are supported by image features.
Reference: [56] <author> R.M. Haralick, C.N. Lee, K. Ottenberg, M. Nolle, </author> <title> Review and Analysis of Solutions of the Three Point Perspective Pose Estimation Problem, </title> <journal> IJCV, </journal> <volume> Vol. 13, No. 3, </volume> <pages> pp. 33-356, </pages> <year> 1994. </year>
Reference-contexts: For normal perspective, 3 point pairs can be used to form a fourth-degree polynomial in one of the unknowns, thus resulting in as many as 4 possible solutions for pose. Haralick <ref> [56] </ref> gives a good review of these techniques and their relative numerical stability. If 4 or 5 point correspondences are available, either a linear least-squares solution can be found [28] or Fischler and Bolle's RANSAC method [55] can be used to find the solution that best describes all observations.
Reference: [57] <author> Y. Liu, T.S. Huang, and O.D. Faugeras, </author> <title> Determination of camera location from 2D to 3D line and point correspondences, </title> <booktitle> Proceedings CVPR, </booktitle> <address> Ann Arbor, MI, </address> <month> June, </month> <year> 1988.NEED </year>
Reference-contexts: Use of line correspondences may be preferred over points because of their relative ease of detection and tracking. As shown by Liu, Huang and Faugeras <ref> [57] </ref>, EKF [38] can be used to first find camera rotation given 3 line correspondences, and then 3 additional line correspondences are sufficient to determine camera translation.
Reference: [58] <author> R. Kumar and A. R. Hanson, </author> <title> Robust Methods for Estimating Pose and a Sensitivity Analysis, CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 60, No. 3, </volume> <month> November, </month> <pages> pp 313-342, </pages> <year> 1994. </year> <month> 33 </month>
Reference-contexts: To explain why even non-critical sets (i.e. more than the theoretic minimum) of line correspondences do not always give robust results, Navab and Faugeras [34] show that for certain degenerate configurations no unique solution for pose can be found. Kumar and Hanson <ref> [58] </ref> present a RANSAC-based iterative technique which minimizes error due to rotation and translation simultaneously, and claim its superiority to techniques that solve for rotation and translation separately.
Reference: [59] <author> J.I. Par, N. Yagi, K. Enami, K. Aizawa, and M. Hatori, </author> <title> Estimation of Camera Parameters from Image Sequence for Model-Based Video Coding, </title> <journal> IEEE Transactions on Circuits and Systems for Video Technology, </journal> <volume> Vol. 4, No. 3. </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: Aizawa <ref> [59] </ref> presents an iterative technique for finding pure rotation, focal length and zoom given point correspondences in a pair of views.
Reference: [60] <author> J.D. Foley, A. Van Dam, S.K. Feiner and J.F. Hughes, </author> <title> Computer Graphics Principles and Practice, 2nd Ed., </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1990. </year>
Reference: [61] <author> R. Forchheimer and O. Fahlander, </author> <title> Low bit-rate coding through animation, </title> <booktitle> in Proceedings Picture Coding Symposium (PCS-83), Davis, </booktitle> <pages> pp. 113-114, </pages> <month> March </month> <year> 1983.NEED </year>
Reference-contexts: What could be the first application of model-based video coding of humans was presented 13 in 1983 by Forchheimer and Fahlander <ref> [61] </ref> using animation to describe facial image sequences. They introduced the notion of giving the sender and receiver a shared set of models of objects seen in common image sequences (i.e. a human face and shoulders on a static background).
Reference: [62] <author> H. Li, P. Roivainen and R. Forchheimer, </author> <title> 3-D Motion Estimation in Model-Based Facial Image Coding, </title> <journal> IEEE Transactions on PAMI, </journal> <volume> Vol. 15, No. 6, </volume> <pages> pp. 545-555, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: They introduced the notion of giving the sender and receiver a shared set of models of objects seen in common image sequences (i.e. a human face and shoulders on a static background). For the special case of head-and-shoulder scenes, Li, Roivainen and Forchheimer <ref> [62] </ref> suggest that motion parameters for head-and-shoulder scenes be separated into global rigid motion and local deformations due to facial expressions. Essa's work [63] represents the state of the art in compact model-based representations for facial expression.
Reference: [63] <author> I.A. Essa, </author> <title> Analysis, Interpretation and Synthesis of Facial Expression, </title> <type> MIT Media Laboratory Ph.D. Thesis, </type> <month> February </month> <year> 1995. </year>
Reference-contexts: For the special case of head-and-shoulder scenes, Li, Roivainen and Forchheimer [62] suggest that motion parameters for head-and-shoulder scenes be separated into global rigid motion and local deformations due to facial expressions. Essa's work <ref> [63] </ref> represents the state of the art in compact model-based representations for facial expression.
Reference: [64] <author> E.H. Adelson, and J.R. Bergen, </author> <title> "The Penoptic Function and the Elements of Early Vision", Computational Models of Visual Processing, Chapter 1, Edited by M. </title> <editor> Landy and J. A. Mavson, </editor> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass, </address> <year> 1991. </year>
Reference-contexts: Contributing to the goal of determining static visual content from a set of monocular views with a moving camera, Adelson and Bergen <ref> [64] </ref> formalized the concept of 2-D visibility over the entire sphere of possible directions as the plenoptic function.
Reference: [65] <author> L. McMillan and G. Bishop, </author> <title> Plenoptic Modeling: An Image-Based Rendering System, </title> <booktitle> Proceedings ACM Siggraph '95, </booktitle> <pages> pp. 39-46, </pages> <year> 1995. </year>
Reference-contexts: To acheive this type of visual quality, some researchers are investigating the use photographic images of real scenes directly rather than attempting to synthesize geometry and texture <ref> [65, 90, 107] </ref>. Besides attacking the ill-posed problem of structure recovery, this requires that we also face problems of proper registration and merging of texture samples from multiple views. <p> Aizawa [59] presents an iterative technique for finding pure rotation, focal length and zoom given point correspondences in a pair of views. McMillan and Bishop <ref> [65] </ref> describe transformations needed to project images onto a common cylinder, claiming to be able to solve 5 intrinsic parameters as well as 3 parameters of pure rotation from 12 or more feature correspondences.
Reference: [66] <author> S. Mann and R. </author> <title> Picard, "Video Orbits of the Projective Group: A New Perspective on Image Mosaicing", MIT Media Laboratory Perceptual Computing Section Technical Report No. </title> <type> 338. </type>
Reference-contexts: McMillan and Bishop [65] describe transformations needed to project images onto a common cylinder, claiming to be able to solve 5 intrinsic parameters as well as 3 parameters of pure rotation from 12 or more feature correspondences. Mann and Picard <ref> [66] </ref> present a feature-less iconic approach for estimating the 8-parameter perspective transformation used to reproject and register multiple images onto a common plane via "video-orbits" for image mosaicing.
Reference: [67] <author> L. Teodosio, </author> <title> "Salient Stills", </title> <type> MIT Media Laboratory Masters Thesis, </type> <year> 1992. </year>
Reference-contexts: If the focal length is considered infinite (i.e. having viewing conditions of pure orthography), a 6-parameter affine transformation describing image flow over an entire region can be found as shown. This affine transformation is used by several model-based video coding algorithms for image mosaicing and resolution enhancement <ref> [67, 69, 70] </ref>.
Reference: [68] <author> J. Wang, T. Adelson and U. Desai, </author> <title> Applying Mid-level Vision Techniques for Video and Data Compression and Manipulation, </title> <booktitle> Proceedings of the SPIE: Digital Video and Data Compression on Personal Computers: Algorithms and Technologies, </booktitle> <volume> Vol. </volume> <pages> 2187, </pages> <address> San Jose, </address> <month> February </month> <year> 1994. </year>
Reference-contexts: Attempts have been made <ref> [68, 69] </ref> to iteratively classify optical flow into regions which fit a common set of affine motion parameters. In this way, they effectively model an image sequence as a set of planar textured "layers" being viewed under orthography. <p> Adiv [46] attempts to fit optical flow to locally planar patches and then does local hypothesis testing to aggregate patches that have compatible motion parameters (i.e. the 8-parameter planar perspective transformation used by [43]). Adiv's result can be considered to similar to <ref> [68] </ref> except that a 3-D perspective camera model was assumed. Here we see a blurring between efforts of video coding and total 3-D scene reconstruction. <p> On-line documentation for the modelMaker application may be found at [113] courtesy Eugene Lin. 3.4 A proposed application : Model-based video coding Recent results for affine optical flow segmentation approaches <ref> [68, 69, 70] </ref> for video coding are promising, but limited success appears to have been made in handling the true perspective case [46, 91].
Reference: [69] <author> M. Irani, S. Hsu, P. Anandan, </author> <title> Mosaic-based video compression, </title> <booktitle> Proceedings SPIE: Digital Video Compression, </booktitle> <volume> Vol. 2419, </volume> <pages> pp. 242-253, </pages> <address> San Jose, California, </address> <month> February 8-10, </month> <year> 1995. </year>
Reference-contexts: If the focal length is considered infinite (i.e. having viewing conditions of pure orthography), a 6-parameter affine transformation describing image flow over an entire region can be found as shown. This affine transformation is used by several model-based video coding algorithms for image mosaicing and resolution enhancement <ref> [67, 69, 70] </ref>. <p> Attempts have been made <ref> [68, 69] </ref> to iteratively classify optical flow into regions which fit a common set of affine motion parameters. In this way, they effectively model an image sequence as a set of planar textured "layers" being viewed under orthography. <p> On-line documentation for the modelMaker application may be found at [113] courtesy Eugene Lin. 3.4 A proposed application : Model-based video coding Recent results for affine optical flow segmentation approaches <ref> [68, 69, 70] </ref> for video coding are promising, but limited success appears to have been made in handling the true perspective case [46, 91].
Reference: [70] <author> R.G. Kermode, </author> <title> Building the BIG Picture: Enhanced Resolution from Coding, </title> <type> MIT Media Laboratory Masters Thesis, </type> <month> June </month> <year> 1994. </year>
Reference-contexts: If the focal length is considered infinite (i.e. having viewing conditions of pure orthography), a 6-parameter affine transformation describing image flow over an entire region can be found as shown. This affine transformation is used by several model-based video coding algorithms for image mosaicing and resolution enhancement <ref> [67, 69, 70] </ref>. <p> Attempts have been made [68, 69] to iteratively classify optical flow into regions which fit a common set of affine motion parameters. In this way, they effectively model an image sequence as a set of planar textured "layers" being viewed under orthography. Kermode <ref> [70] </ref> developed a hybrid video coder that uses similar principles but adds capabilities for image mosaicing and resolution enhancement and works within the current 14 MPEG digital video coding framework. <p> On-line documentation for the modelMaker application may be found at [113] courtesy Eugene Lin. 3.4 A proposed application : Model-based video coding Recent results for affine optical flow segmentation approaches <ref> [68, 69, 70] </ref> for video coding are promising, but limited success appears to have been made in handling the true perspective case [46, 91].
Reference: [71] <author> J. Canny, </author> <title> A Computational Approach to Edge Detection, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. PAMI-8, No. 6. </volume> <month> Nov. </month> <year> (1986), </year> <pages> pp. 679-698. </pages>
Reference-contexts: Still images can be digitally scanned to arbitrary resolution using a flatbed image scanner or individual frames of video can be digitized using standard video capture hardware. 3.2.2 Line detection The Canny <ref> [71] </ref> edge detection algorithm is applied to each image after which a line detection algorithm [72] is used to find and traverse continuous chains of edge pixels which are broken into piece-wise linear segments. Each line in an image is given its own edge attribute.
Reference: [72] <author> Stefan Agamanolis, </author> <booktitle> "Line-finder man pages", </booktitle> <institution> MIT Media Lab TVOT man pages, Decem-ber, </institution> <year> 1995. </year>
Reference-contexts: Still images can be digitally scanned to arbitrary resolution using a flatbed image scanner or individual frames of video can be digitized using standard video capture hardware. 3.2.2 Line detection The Canny [71] edge detection algorithm is applied to each image after which a line detection algorithm <ref> [72] </ref> is used to find and traverse continuous chains of edge pixels which are broken into piece-wise linear segments. Each line in an image is given its own edge attribute.
Reference: [73] <author> Stefan Panayiotis Agamanolis, </author> <title> "High-level scripting environments for interactive multimedia systems", MIT Media Lab, </title> <type> Masters Thesis, </type> <month> February </month> <year> 1996. </year>
Reference-contexts: practical advantages over other methods of video coding in that it made 3-D model-based video coding practical if allowed the initial startup cost of manually specifying the 3-D scene. 2.5 ISIS : a custom delivery mechanism for model-based video Isis is a scheme-like interpretive programming language written by Stefan Agamanolis <ref> [73] </ref> which can be used as a stand-alone programming environment as well as interface language to packages of C functions that do specialized operations from networking and file access to synchronized processing of graphics and sound.
Reference: [74] <author> H. Holtzman, </author> <title> Three-Dimensional Representations of Video using Knowledge-Based Estimation, </title> <type> MIT Media Laboratory Masters Thesis, </type> <year> 1991. </year> <month> 34 </month>
Reference-contexts: Nagao and Horn [81] present a similar technique that uses direct moments of image intensities and intensity gradients. 2.3.2 Strategies for feature-based pose-determination Approaches for determining arbitrary pose given a hypothesized model can be categorized into 3 groups. Pose search methods <ref> [37, 74] </ref> attempt to test the space of all poses to find the one that best fits the model to the observed image. Pose clustering methods [85] use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. <p> Holtzman's <ref> [74] </ref> uses a straight forward pose search strategy using textures. <p> Although the affine model is only an approximation to true perspective flow, it is in practice more tractable because it has a linear solution which uses only 1st order spatiotemporal brightness derivatives. 2.4.5 Dimensionalization Holtzman <ref> [74] </ref> sidesteps the optical flow perspective segmentation problem for video-coding by allowing partially known scene geometry. In an initial frame, 3-D planar polygons describing key surfaces in the scene are manually placed via a CAD system relative to the initial camera's origin with known focal length. <p> However, inspiration from Holtzman's ground breaking work in dimensionalization <ref> [74] </ref>, and consideration of recent advancements in self-calibration, structure-from-motion and pose-determination in known environments have prompted a reevaluation of the model-based video coding problem. 3.4.1 Pose-determination in a known scene We can envision a model-based video coder that is given a previously analyzed set of textured polygons describing a scene.
Reference: [75] <author> W.H. Press, B.P. Flannery, S.A. Teukolsky, and W.T. Vetterling, </author> <title> Numerical recipes in C, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK, </address> <year> 1988. </year>
Reference-contexts: Consequently, we attempt to solve all 3 terms of center of projection using information contained in the digitized image itself. 9 a gradient descent method that needs only function evaluations at each iteration not Jacobian or Hessian evaluation <ref> [75] </ref> 20 For most computer vision setups, the COP is considered to be the origin of the camera's coordinate frame and a virtual image plane is positioned one focal length in front of the COP toward the scene. <p> This is formed by using the current parameterization of surfaces to get new infinite hinge-edges (edges which mark the intersection between pairs of non-parallel surfaces). Infinite hinge-edges are projected via parameterized cameras to their respective image plane and compared to observed lines. * Use downhill simplex method <ref> [75] </ref> to find the scene parameter that minimizes this cost function. * Use final scene parameterization to update cameras and surfaces. * Using final cameras and surfaces to project observed lines into scene to get final weighted least-squares infinite edges.
Reference: [76] <author> W. T. Freeman, </author> <title> "Steerable Filters and Local Analysis of Image Structure", </title> <type> Ph.D. Thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1992. </year> <note> Also available as Vision and Modeling Technical Report 190, </note> <institution> MIT Media Laboratory. </institution>
Reference-contexts: Properties which are invariant (i.e. consistent) even under large changes in viewing or lighting conditions are particularly useful. Some examples include junction labeling of points <ref> [76] </ref>, color opponency of points and lines [80], as well as color and texture metrics for region matching. 2.3.3 Strategies for iconic-based pose-determination While feature-based approaches to pose-determination use a difference measure which is based on mis-registration of 2-D point or line features, iconic-based approaches can be used instead which compare
Reference: [77] <author> C. Wren, A. Azarbayejani, T. Darrell, A. Pentland, Pfinder: </author> <title> Real-Time Tracking of the Human Body, MIT Media Laboratory Perceptual Computing Section, </title> <type> Technical Report No. 353, </type> <year> 1995. </year>
Reference-contexts: Region segmentation and region matching effectively groups features into smaller sets for comparisons. This brings the testing cost down only quadratic order in the number of regions. Approaches for color histogram-based region segmentation and matching are found in <ref> [86, 77] </ref>. Texture analysis and synthesis results can also be used for texture-based region segmentation as shown in [88, 36, 89]. 12 Another way to prune the number of hypothesis tests needed for pose determination is consider only those 2-D and 3-D features that have similar identifying characteristics. <p> Proper pose estimation allows us to synthesize the static scene as viewed without unmodeled objects and lighting effects. Once an uncluttered background image is reconstructed, color histogram approaches like those presented in <ref> [77, 86] </ref> might allow segmentation of actors and unmodeled objects from each frame. Failing this, I plan to extend the case of segmenting moving figures in front of a fixed camera in the presence of pixel noise.
Reference: [78] <author> W. E. L. Grimson, D. P. Huttenlocher and T. D. </author> <title> Alter, "Recognizing 3D Objects from 2D Images: An Error Analysis", MIT AI Laboratory, </title> <journal> A.I. </journal> <volume> Memo No. 1362, </volume> <month> July 10, </month> <year> 1992. </year>
Reference-contexts: Pose clustering methods [85] use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. And alignment methods <ref> [84, 55, 78] </ref> compute pose from all possible matches of critical sets and then see if any of the remaining model features are supported by image features.
Reference: [79] <author> K. Nagao and W. E. </author> <title> Grimson, Object Recognition by Alignment using Invariant Projections of Planar Surfaces, MIT AI Laboratory, </title> <journal> A.I. </journal> <volume> Memo No. 1463, </volume> <month> February, </month> <year> 1994. </year>
Reference-contexts: If individual matches between 2-D and 3-D point features can't be made, but sets of points can, Nagao and Grimson <ref> [79, 80] </ref> show how 2nd order statistics of spatial distributions of matched clusters of point features can be used to solve camera pose under weak perspective projection up to an unknown image plane rotation.
Reference: [80] <author> K. Nagao and W. E. </author> <title> Grimson, "Recognizing 3D Objects Using Photometric Invarient", MIT AI Laboratory, </title> <journal> A.I. </journal> <volume> Memo No. 1523, </volume> <month> February, </month> <year> 1995. </year>
Reference-contexts: If individual matches between 2-D and 3-D point features can't be made, but sets of points can, Nagao and Grimson <ref> [79, 80] </ref> show how 2nd order statistics of spatial distributions of matched clusters of point features can be used to solve camera pose under weak perspective projection up to an unknown image plane rotation. <p> Properties which are invariant (i.e. consistent) even under large changes in viewing or lighting conditions are particularly useful. Some examples include junction labeling of points [76], color opponency of points and lines <ref> [80] </ref>, as well as color and texture metrics for region matching. 2.3.3 Strategies for iconic-based pose-determination While feature-based approaches to pose-determination use a difference measure which is based on mis-registration of 2-D point or line features, iconic-based approaches can be used instead which compare image depths or image intensities directly.
Reference: [81] <author> K. Nagao and B. K. P. Horn, </author> <title> "Direct object recognition using no higher than second or third order statistics of the image", MIT AI Laboratory, </title> <journal> A.I. </journal> <volume> Memo No. 1526, </volume> <month> February, </month> <year> 1995. </year>
Reference-contexts: Nagao and Horn <ref> [81] </ref> present a similar technique that uses direct moments of image intensities and intensity gradients. 2.3.2 Strategies for feature-based pose-determination Approaches for determining arbitrary pose given a hypothesized model can be categorized into 3 groups.
Reference: [82] <author> P. A. Viola, </author> <title> Alignment by Maximization of Mutual Information, MIT AI Laboratory, A.I. </title> <type> Technical Report No. 1548, </type> <month> June, </month> <year> 1995. </year>
Reference-contexts: Holtzman's [74] uses a straight forward pose search strategy using textures. Viola <ref> [82] </ref> iteratively finds the pose parameters which maximize the mutual information (i.e. functional consistency) between pixel depths of the rendered model and observed image intensities. 2.3.4 Pose-determination : Overview Feature-based methods for pose-determination are combinatorically expensive but robust in the presence of clutter, noise and occlusion.
Reference: [83] <author> J. A. Provine and L. T. Bruton, </author> <title> "3-D Model Based Coding A Very Low Bit Rate Coding Scheme for Video-conferencing", </title> <booktitle> to appear in Proc. IEEE Intl. Symp. on Circuits and Systems, </booktitle> <year> 1996. </year>
Reference-contexts: Essa's work [63] represents the state of the art in compact model-based representations for facial expression. To make facial expression appear more life-like, Provine and Bruton <ref> [83] </ref> explore psychovisual models that "trick" the eye just as psychoacoustic models are used to "trick" the ear in audio coding. 2.4.2 Detecting static content in a video sequence Other research efforts aim to recover information about the entire scene, not just a moving human figure, from video sequences.
Reference: [84] <author> S. Ullman, </author> <title> An Approach to Object Recognition: Aligning Pictorial Descriptions, </title> <journal> AI Memo, </journal> <volume> No. 931, </volume> <month> December </month> <year> 1986. </year>
Reference-contexts: For the case of point features, Ullman <ref> [84] </ref> shows how 3 pairs of 2-D to 3-D point matches can be used to find a unique transformation describing weak perspective (i.e. orthography and scale factor). <p> Pose clustering methods [85] use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. And alignment methods <ref> [84, 55, 78] </ref> compute pose from all possible matches of critical sets and then see if any of the remaining model features are supported by image features.
Reference: [85] <author> W. Eric L. Grimson and David Huttenlocher, </author> <title> On the Sensitivity of the Hough Transform for Object Recognition, </title> <journal> MIT A.I. </journal> <volume> Memo No. 1044, </volume> <month> May </month> <year> 1988. </year>
Reference-contexts: Pose search methods [37, 74] attempt to test the space of all poses to find the one that best fits the model to the observed image. Pose clustering methods <ref> [85] </ref> use techniques like the generalized Hough transform [17] to accumulate evidence for possible coordinate transformations from all possible matches. And alignment methods [84, 55, 78] compute pose from all possible matches of critical sets and then see if any of the remaining model features are supported by image features.
Reference: [86] <author> Tanveer Fathima Syeda-Mahmood, </author> <title> "Data and Model-driven Selection using Color Regions", </title> <booktitle> CVVV'92, </booktitle> <pages> pp. 115-123, </pages> <year> 1992. </year> <month> May </month> <year> 1993. </year>
Reference-contexts: Region segmentation and region matching effectively groups features into smaller sets for comparisons. This brings the testing cost down only quadratic order in the number of regions. Approaches for color histogram-based region segmentation and matching are found in <ref> [86, 77] </ref>. Texture analysis and synthesis results can also be used for texture-based region segmentation as shown in [88, 36, 89]. 12 Another way to prune the number of hypothesis tests needed for pose determination is consider only those 2-D and 3-D features that have similar identifying characteristics. <p> Proper pose estimation allows us to synthesize the static scene as viewed without unmodeled objects and lighting effects. Once an uncluttered background image is reconstructed, color histogram approaches like those presented in <ref> [77, 86] </ref> might allow segmentation of actors and unmodeled objects from each frame. Failing this, I plan to extend the case of segmenting moving figures in front of a fixed camera in the presence of pixel noise.
Reference: [87] <author> S. Tanveer F. Mahmood, </author> <title> "Data and Model-driven Selection using Parallel-Line Groups", MIT A.I. </title> <journal> Memo, </journal> <volume> No. 1399, </volume> <month> May </month> <year> 1993. </year>
Reference: [88] <author> W.K. Pratt, O.D. Faugeras and A. Gagalowicz, </author> <title> "Applications of Stochastic Texture Field Models to Image Processing", </title> <journal> Proc. of the IEEE, </journal> <volume> Vol. 69, No. 5, </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: This brings the testing cost down only quadratic order in the number of regions. Approaches for color histogram-based region segmentation and matching are found in [86, 77]. Texture analysis and synthesis results can also be used for texture-based region segmentation as shown in <ref> [88, 36, 89] </ref>. 12 Another way to prune the number of hypothesis tests needed for pose determination is consider only those 2-D and 3-D features that have similar identifying characteristics. Properties which are invariant (i.e. consistent) even under large changes in viewing or lighting conditions are particularly useful.
Reference: [89] <author> K. Popat and R.W. </author> <title> Picard, "Novel cluster-based probability model for texture synthesis, classification, and compression", MIT Media Lab Perceptual Computing Group Technical Report #234, </title> <month> November </month> <year> 1993. </year> <month> 35 </month>
Reference-contexts: This brings the testing cost down only quadratic order in the number of regions. Approaches for color histogram-based region segmentation and matching are found in [86, 77]. Texture analysis and synthesis results can also be used for texture-based region segmentation as shown in <ref> [88, 36, 89] </ref>. 12 Another way to prune the number of hypothesis tests needed for pose determination is consider only those 2-D and 3-D features that have similar identifying characteristics. Properties which are invariant (i.e. consistent) even under large changes in viewing or lighting conditions are particularly useful.
Reference: [90] <author> P.E. Debevec, C. J. Taylor and J. Malik, </author> <title> "Modeling and Rendering Architecture from Photographs", </title> <type> Technical Report UCB/CSD-96-893, </type> <month> January </month> <year> 1996. </year>
Reference-contexts: For example, the scene modeling approaches used to create environements for visual simulation [100, 104] use known cameras and scene geometry found from architectural plans or physical measurements. In <ref> [90] </ref> the user manually corresponds user defined image lines to 3-D edges of manually created sets of parametric volumes which are then automatically adjusted to minimize observed deviation from pre-calibrated cameras. <p> To acheive this type of visual quality, some researchers are investigating the use photographic images of real scenes directly rather than attempting to synthesize geometry and texture <ref> [65, 90, 107] </ref>. Besides attacking the ill-posed problem of structure recovery, this requires that we also face problems of proper registration and merging of texture samples from multiple views. <p> One way to sidestep all the problems associated with view-dependent texture differences is to blend textures from different views in the rendering stage as suggested in <ref> [90] </ref>. In an effort to keep the model simple and compact, we choose instead to attempt to improve texture registration from multiple views and build a static description for texture. <p> If there are local deviations from planarity and if sufficient texture detail exists we can actually correct for it. As described in <ref> [90, 33] </ref> given known or pre-calculated cameras, image texture from one subject view can be planar perspective transformed to match the surface as seen in some reference view 12 .
Reference: [91] <author> S. Ayer, P. Schroeter and J. Bigun, </author> <title> "Segmentation of moving objects by robust motion parameter estimation over multiple frames", </title> <journal> CVVP-94, </journal> <volume> Vol. 801, </volume> <pages> pp. 316-327. </pages>
Reference-contexts: for the modelMaker application may be found at [113] courtesy Eugene Lin. 3.4 A proposed application : Model-based video coding Recent results for affine optical flow segmentation approaches [68, 69, 70] for video coding are promising, but limited success appears to have been made in handling the true perspective case <ref> [46, 91] </ref>.
Reference: [92] <author> R. E. Grant, A. B. Mahmoodi, O. L. Nelson, </author> <title> "Chapter 11: Image Compression and Transmission", </title> <booktitle> Imaging Processes and Materials, </booktitle> <pages> pp. 323-349. </pages>
Reference-contexts: Other less computationally demanding analytical error measures like normalized mean square error are also used; however, all analytical image quality measures are only approximations to and are not successful against real subjective evalutions <ref> [92] </ref>. Indeed, researchers [98, 99] have explored the limits of the human visual system's with regard to detectability of changes in images over time.
Reference: [93] <institution> Roberts. </institution>
Reference-contexts: However, by using large numbers of precisely known template features very accurately calibration parameters can be found. If distortion can safely be considered negligible, Roberts <ref> [93] </ref> shows that the remaining parameters can be combined to form a single homogeneous 3 by 4 matrix which can be solved given 6 or more known world points and their 2-D projections in two images.
Reference: [94] <institution> Ganapathy. </institution>
Reference-contexts: To determine of the position of the camera itself relative to known object points, Ganapathy <ref> [94] </ref> shows how this 3 by 4 matrix can be inverted into its constituent intrinsic and extrinsic parameters. 2.1.2 Affine calibration This matrix can also be used with corresponding pairs of 2-D points to recover a more qualitative description of scene structure. <p> If 4 or 5 point correspondences are available, either a linear least-squares solution can be found [28] or Fischler and Bolle's RANSAC method [55] can be used to find the solution that best describes all observations. Ganapathy <ref> [94] </ref> presents a method that in general produce a unique solution given 6 or more point correspondences as long as the points are not colinear on the image plane. Use of line correspondences may be preferred over points because of their relative ease of detection and tracking.
Reference: [95] <institution> Guzman and Falk. </institution>
Reference-contexts: Shape-from-wireframe algorithms aim to recover 3-D geometry of a polyhedral scene (e.g. "blocks world" or "LEGO") from a single line drawing. Initial work <ref> [95, 97, 96] </ref> focussed on the problem of partitioning wireframe scenes into separate trihedral polyhedral objects made up of collections of labeled vertices and edges. More recently algorithms have been presented [52, 53] which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well.
Reference: [96] <institution> Clowes. </institution>
Reference-contexts: Shape-from-wireframe algorithms aim to recover 3-D geometry of a polyhedral scene (e.g. "blocks world" or "LEGO") from a single line drawing. Initial work <ref> [95, 97, 96] </ref> focussed on the problem of partitioning wireframe scenes into separate trihedral polyhedral objects made up of collections of labeled vertices and edges. More recently algorithms have been presented [52, 53] which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well.
Reference: [97] <author> Huffman. </author> <note> [98] . [100] http://www.cs.Berkeley.EDU/ sequin/soda/soda.html [101] http://www.portola.com:80/TR/background.html [102] http://vrml.wired.com/concepts/raggett.html [103] http://quasar.poly.edu/ llin/GISS/sessions/SecII-A.html [104] http://vision.ucsd.edu/ [105] http://sdsc.edu/SDSC/Partners/vrml/doc.html [106] http://vrml.wired.com/future/scale.html [107] http://www.photomodeler.com [108] http://garden-docs.www.media.mit.edu/garden docs/gnu/readline/rlman toc.html [109] http://cuiwww.unige.ch/eao/www/TclTk.html [110] http://www.osf.org/motif/Motif20/index.html [111] http://www.sgi.com/Technology/Inventor/ [112] file:///mas/garden/grad/bill/images/modeling/HowTo.html [113] file:///mas/garden/urop/elin/Projects/ModelMaker/Documentation/ModelMaker docs.html [114] http://cheops.www.media.mit.edu/projects/cheops/ 36 [115] http://www.microsoft.com/intdev/avr/avwhite.htm [116] http://webspace.sgi.com/moving-worlds 37 </note>
Reference-contexts: Shape-from-wireframe algorithms aim to recover 3-D geometry of a polyhedral scene (e.g. "blocks world" or "LEGO") from a single line drawing. Initial work <ref> [95, 97, 96] </ref> focussed on the problem of partitioning wireframe scenes into separate trihedral polyhedral objects made up of collections of labeled vertices and edges. More recently algorithms have been presented [52, 53] which find 3-D polyhedral interpretations of wireframe drawings that match human observations amazingly well.
References-found: 97

