URL: ftp://ftp.speech.sri.com/pub/papers/aaai98-discourse.ps
Refering-URL: http://www.speech.sri.com/people/stolcke/publications.html
Root-URL: 
Title: Dialog Act Modeling for Conversational Speech  
Author: Andreas Stolcke and Elizabeth Shriberg, Rebecca Bates, Boston University Noah Coccaro and Daniel Jurafsky, Paul Taylor, 
Note: Carol Van Ess-Dykema, Department of Defense  
Affiliation: SRI International  University of Colorado at Boulder Rachel Martin, Johns Hopkins University Marie Meteer, BBN Technologies Klaus Ries, Carnegie Mellow University  University of Edinburgh  
Abstract: We describe an integrated approach for statistical modeling of discourse structure for natural conversational speech. Our model is based on 42 `dialog acts' (e.g., Statement, Question, Backchannel, Agreement, Disagreement, Apology), which were hand-labeled in 1155 conversations from the Switchboard corpus of spontaneous human-to-human telephone speech. We developed several models and algorithms to automatically detect dialog acts from transcribed or automatically recognized words and from prosodic properties of the speech signal, and by using a statistical discourse grammar. All of these components were probabilistic in nature and estimated from data, employing a variety of techniques (hidden Markov models, N-gram language models, maximum entropy estimation, decision tree classifiers, and neural networks). In preliminary studies, we achieved a dialog act labeling accuracy of 65% based on recognized words and prosody, and an accuracy of 72% based on word transcripts. Since humans achieve 84% on this task (with chance performance at 35%) we find these results encouraging. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bahl, L. R.; Jelinek, F.; and Mercer, R. L. </author> <year> 1983. </year> <title> A maximum likelihood approach to continuous speech recognition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 5(2) </journal> <pages> 179-190. </pages>
Reference-contexts: model with locally decomposable likelihoods and Markovian discourse grammar, it will therefore find precisely the DA sequence with the highest posterior probability: U fl = argmax U The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition <ref> (Bahl, Jelinek, & Mercer 1983) </ref> and tagging (Church 1988). It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Stolcke, Konig, & Weintraub 1997).
Reference: <author> Breiman, L.; Friedman, J. H.; Olshen, R. A.; and Stone, C. J. </author> <year> 1983. </year> <title> Classification and Regression Trees. </title> <address> Pacific Grove, California: </address> <publisher> Wadsworth & Brooks. </publisher>
Reference-contexts: A complete discussion of the features used can be found in Shriberg et al. (1997). Prosodic decision trees For our prosodic classifiers, we used CART-style decision trees <ref> (Breiman et al. 1983) </ref>. Decision trees allow combination of discrete and continuous features, and can be inspected to gain an understanding of the role of different features and feature combinations.
Reference: <author> Bridle, J. </author> <year> 1990. </year> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition. </title> <editor> In Soulie, F., and J.Herault., eds., Neurocomputing: </editor> <booktitle> Algorithms, Architectures and Applications. </booktitle> <address> Berlin: </address> <publisher> Springer. </publisher> <pages> 227-236. </pages>
Reference-contexts: Most important, neural networks with hidden units can learn new features that combine multiple input features. Results from preliminary experiments on the Top-5 classification task showed that a softmax network <ref> (Bridle 1990) </ref> without hidden units resulted in a slight improvement over a decision tree on the same task.
Reference: <author> Carletta, J. </author> <year> 1996. </year> <title> Assessing agreement on classification tasks: The Kappa statistic. </title> <booktitle> Computational Linguistics 22(2) </booktitle> <pages> 249-254. </pages>
Reference-contexts: Labeling was carried out in a three-month period by eight linguistics graduate students at CU Boulder. Inter-labeler agreement was 84%, resulting in a Kappa statistic of 0.80. The Kappa statistic measures agreement normalized for chance; values of 0.8 or higher are considered considered high reliability <ref> (Carletta 1996) </ref>. A total of 1155 Switchboard conversations were labeled, comprising 205,000 utterances and 1.4 million words.
Reference: <author> Church, K. W. </author> <year> 1988. </year> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Second Conference on Applied Natural Language Processing, </booktitle> <pages> 136-143. </pages>
Reference-contexts: discourse grammar, it will therefore find precisely the DA sequence with the highest posterior probability: U fl = argmax U The combination of likelihood and prior modeling, HMMs, and Viterbi decoding is fundamentally the same as the standard probabilistic approaches to speech recognition (Bahl, Jelinek, & Mercer 1983) and tagging <ref> (Church 1988) </ref>. It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct (Stolcke, Konig, & Weintraub 1997).
Reference: <author> Core, M., and Allen, J. </author> <year> 1997. </year> <title> Coding dialogs with the DAMSL annotation scheme. </title> <booktitle> In AAAI Fall Symposium on Communicative Action in Humans and Machines. </booktitle>
Reference-contexts: We chose to follow a recent standard for shallow discourse structure annotation, the Dialog Act Markup in Several Layers (DAMSL) tag set, which was recently designed by the natural-language processing community <ref> (Core & Allen 1997) </ref>. We began with this markup system and modified it in several ways to make it more useful for our corpus.
Reference: <author> Godfrey, J.; Holliman, E.; and McDaniel, J. </author> <year> 1992. </year> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> 517-520. </pages>
Reference: <author> Grosz, B., and Sidner, C. </author> <year> 1986. </year> <title> Attention, intention, and the structure of discourse. </title> <booktitle> Computational Linguistics 12(3) </booktitle> <pages> 175-204. </pages>
Reference: <author> Jurafsky, D.; Bates, R.; Coccaro, N.; Martin, R.; Meteer, M.; Ries, K.; Shriberg, E.; Stolcke, A.; Taylor, P.; and Van Ess-Dykema, C. </author> <year> 1997. </year> <title> Switchboard discourse language modeling project report. </title> <type> Technical report, </type> <institution> Center for Speech and Language Processing, Johns Hopkins University, Baltimore. </institution>
Reference: <author> Jurafsky, D.; Shriberg, E.; and Biasca, D. </author> <year> 1997. </year> <title> Switchboard-DAMSL Labeling Project Coder's Manual. </title> <address> http://stripe.colorado.edu/~jurafsky/manual.august1.html. </address>
Reference: <author> Katz, S. M. </author> <year> 1987. </year> <title> Estimation of probabilities from sparse data for the language model component of a speech recog-niser. </title> <journal> IEEE Trans. Acoustics, Speech and Signal Processing 35(3) </journal> <pages> 400-401. </pages>
Reference-contexts: N-gram discourse models A computationally convenient type of discourse grammar is an N-gram model based on DA tags, as it allows efficient decoding in the HMM framework. We trained standard backoff N-gram models <ref> (Katz 1987) </ref>, using the frequency smoothing approach of Witten & Bell (1991). Models of various orders were compared by their perplexities, i.e., the average number of choices the model predicts for each tag, conditioned on the preceding tags.
Reference: <author> Kita, K.; Fukui, Y.; Nagata, M.; and Morimoto, T. </author> <year> 1996. </year> <title> Automatic acquisition of probabilistic dialogue models. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> 196-199. </pages>
Reference: <author> Kuhn, R., and de Mori, R. </author> <year> 1990. </year> <title> A cache-based natural language model for speech recognition. </title> <journal> IEEE Trans. Pattern Analysis and Machine Intelligence 12(6) </journal> <pages> 570-583. </pages>
Reference-contexts: Another motivation is that N-grams fail to model long-distance dependencies, such as the fact that speakers may tend to repeat certain DAs or patterns throughout the conversation. The first alternative approach was a standard cache model <ref> (Kuhn & de Mori 1990) </ref>, which boosts the probabilities of previously observed unigrams and bigrams, on the theory that tokens tend to repeat themselves over longer distances.
Reference: <author> Mast, M.; Kompe, R.; Harbeck, S.; Kieling, A.; Nie-mann, H.; ; and Noth, E. </author> <year> 1996. </year> <title> Dialog act classification with the help of prosody. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> 1728-1731. </pages>
Reference: <author> Meteer, M., et al. </author> <year> 1995. </year> <title> Dysfluency Annotation Stylebook for the Switchboard Corpus. Linguistic Data Consortium. </title> <note> Revised June 1995 by Ann Taylor. ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-book.ps.gz. </note>
Reference-contexts: The raw Switchboard data is not segmented in a linguistically consistent way; we therefore made use of a version that had been hand-segmented at the utterance level <ref> (Meteer & others 1995) </ref>. Automatic segmentation of spontaneous speech is an open research problem in its own right (Mast et al. 1996; Stolcke & Shriberg 1996), but we decided not to confound the DA detection task with the additional problems introduced by automatic segmentation.
Reference: <author> Morgan, N.; Fosler, E.; and Mirghafori, N. </author> <year> 1997. </year> <title> Speech recognition using on-line estimation of speaking rate. </title> <booktitle> In Proc. EUROSPEECH. </booktitle>
Reference: <author> Nagata, M., and Morimoto, T. </author> <year> 1994. </year> <title> First steps toward statistical modeling of dialogue to predict the speech act type of the next utterance. </title> <booktitle> Speech Communication 15 </booktitle> <pages> 193-203. </pages>
Reference: <author> Rabiner, L. R., and Juang, B. H. </author> <year> 1986. </year> <title> An introduction to hidden Markov models. </title> <journal> IEEE ASSP Magazine 3(1) </journal> <pages> 4-16. </pages>
Reference-contexts: The importance of the Markov assumption for the discourse grammar is that we can now view the whole system of discourse grammar and local utterance-based likelihoods as a kth-order hidden Markov model (HMM) <ref> (Rabiner & Juang 1986) </ref>. The HMM states correspond to DAs, observations correspond to utterances, transition probabilities are given by the discourse grammar, and observation probabilities are given by the local likelihoods P (E i jU i ).
Reference: <author> Reithinger, N.; Engel, R.; Kipp, M.; and Klesen, M. </author> <year> 1996. </year> <title> Predicting dialogue acts for a speech-to-speech translation system. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> 654-657. </pages>
Reference: <author> Rose, R. C.; Chang, E. I.; and Lippmann, R. P. </author> <year> 1991. </year> <title> Techniques for information retrieval from voice messages. </title> <booktitle> In Proc. ICASSP, </booktitle> <pages> 317-320. </pages>
Reference-contexts: For example, a context-free discourse grammar could potentially account for the nested structures proposed in Grosz & Sidner (1986). Word-based DA discrimination has obvious parallels to topic spotting and message classification, and we should explore techniques developed in that paradigm, such as keyword-based detectors <ref> (Rose, Chang, & Lippmann 1991) </ref>. For prosodic DA detection, we are studying the use of multiple trees, both to cascade classifiers trained on subtasks, and to combine parallel classifiers using a disjoint subset of features, which we believe will increase robustness.
Reference: <author> Rosenfeld, R. </author> <year> 1996. </year> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer Speech and Language 10 </booktitle> <pages> 187-228. </pages>
Reference-contexts: However, this does not seem to be true for DA sequences in our corpus, as the cache model showed no improvement over the standard N-gram. Second, we built a discourse grammar that incorporated constraints on DA sequences in a nonhierarchical way, using maximum entropy (ME) estimation <ref> (Rosenfeld 1996) </ref>. The model was designed so that the current DA label was constrained by features such as unigram statistics, the previous DA and the DA once removed, DAs occurring within a window in the past, and whether the previous utterance was by the same speaker.
Reference: <author> Shriberg, E.; Bates, R.; Taylor, P.; Stolcke, A.; Juraf-sky, D.; Ries, K.; Coccaro, N.; Martin, R.; Meteer, M.; and Van Ess-Dykema, C. </author> <year> 1997. </year> <title> Can prosody aid the automatic classification of dialog acts in conversational speech? Submitted. </title>
Reference: <author> Stolcke, A., and Shriberg, E. </author> <year> 1996. </year> <title> Automatic linguistic segmentation of conversational speech. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> 1005-1008. </pages>
Reference: <author> Stolcke, A.; Konig, Y.; and Weintraub, M. </author> <year> 1997. </year> <title> Explicit word error minimization in N-best list rescoring. </title> <booktitle> In Proc. EUROSPEECH, </booktitle> <volume> volume 1, </volume> <pages> 163-166. </pages>
Reference-contexts: It maximizes the probability of getting the entire DA sequence correct, but it does not necessarily find the DA sequence that has the most DA labels correct <ref> (Stolcke, Konig, & Weintraub 1997) </ref>. To minimize the overall utterance labeling error, we need to maximize the probability of getting each DA label correct individually, i.e., we need to maximize P (U i jE) for each i = 1; : : : ; n.
Reference: <author> Suhm, B., and Waibel, A. </author> <year> 1994. </year> <title> Toward better language models for spontaneous speech. </title> <booktitle> In Proc. ICLSP, </booktitle> <pages> 831-834. </pages>
Reference: <author> Taylor, P.; King, S.; Isard, S.; Wright, H.; and Kowtko, J. </author> <year> 1997. </year> <title> Using intonation to constrain language models in speech recognition. </title> <booktitle> In Proc. EUROSPEECH, </booktitle> <pages> 2763-2766. </pages>
Reference: <author> Witten, I. H., and Bell, T. C. </author> <year> 1991. </year> <title> The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Trans. Information Theory 37(4) </journal> <pages> 1085-1094. </pages>
Reference: <author> Woszczyna, M., and Waibel, A. </author> <year> 1994. </year> <title> Inferring linguistic structure in spoken language. </title> <booktitle> In Proc. ICSLP, </booktitle> <pages> 847-850. </pages>
Reference: <author> Yamaoka, T., and Iida, H. </author> <year> 1991. </year> <title> Dialogue interpretation model and its application to next utterance prediction for spoken language processing. </title> <booktitle> In Proc. EUROSPEECH, </booktitle> <pages> 849-852. </pages>
References-found: 29

