URL: http://www.cs.cmu.edu/afs/cs/user/lindaq/mosaic/wpaper2.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/user/lindaq/mosaic/my-home-page.html
Root-URL: 
Email: chase@cs.cmu.edu  
Title: CONFIDENCE ANNOTATION FOR AUTOMATIC RECOGNITION OF CONVERSATIONAL TELEPHONE SPEECH  
Author: Lin Chase 
Address: 5000 Forbes Avenue Pittsburgh, Pennsylvania 15213 USA  
Affiliation: The Robotics Institute Carnegie Mellon University  
Abstract: The construction and performance of a confidence annotator for a telephone channel speech recognizer is discussed. The confidence annotator labels each word output by the speech recognizer with a probability that indicates how certain it is that the output is correct. An application of this functionality is demonstrated in which the recognition system identifies automatically those conversations for which it believes it is having difficulties. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Finke, P., M. G., Hild, H., Kemp, T., Ries, K., and Westphal, M. </author> <title> The Karlsruhe-Verbmobil Speech Recognition Engine. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <year> 1997. </year>
Reference-contexts: This additional layer of meta-information can be used to filter the output of the recognizer into regions of high and low reliability. In this paper we describe the construction and performance of a confidence annotator for telephone speech that works with the Janus <ref> [1] </ref> large vocabulary continuous speech recognition system. The confidence annotations make it possible to filter the recognizer output, both at a local level (Is there a problem here for a few seconds?) and at a longer-term level (Am I having a problem with this channel?).
Reference: 2. <author> Chase, L. L. </author> <title> Error-Responsive Feedback Mechanisms for Speech Recognizers. </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh PA USA, </institution> <note> also available as Tech Report CMU-RI-TR-97-18, </note> <month> April </month> <year> 1997. </year>
Reference-contexts: The predictor variables are constructed in reference to these important sources of information: 1 See <ref> [2] </ref> for complete details of this experiment. * Word and phone segmentations and acoustic scores from the best-scoring hypothesis produced by the recognizer, * Language model score information from the same best-scoring hypothesis, which describes how likely the hypothesized word is given the words that precede it in the hypothe sis,
Reference: 3. <author> Neti, C., Roukos, S., and Eide, E. </author> <title> Confidence Measures Based Search Strategy for Continuous Speech Recognition. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <year> 1996. </year>
Reference-contexts: This number varies between 0 and 1. 3 The predictor variables used in this experiment, some of which were inspired by or previously implemented in <ref> [3] </ref> [4] [5] [6], are: * percBestCor: A match is computed between the hypothesized word and the best-scoring basephone at each frame. * phAvgpercBestCor: The percBestCor figure is calculated for each phone in the word, then averaged within word over the number of phones in the word. * phMinpercBestCor: The percBestCor
Reference: 4. <author> Richardson, F., Siu, M., and Gish, H. </author> <title> Confidence Scoring for the 1996 Spanish CallHome Evaluation. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <year> 1996. </year>
Reference-contexts: This number varies between 0 and 1. 3 The predictor variables used in this experiment, some of which were inspired by or previously implemented in [3] <ref> [4] </ref> [5] [6], are: * percBestCor: A match is computed between the hypothesized word and the best-scoring basephone at each frame. * phAvgpercBestCor: The percBestCor figure is calculated for each phone in the word, then averaged within word over the number of phones in the word. * phMinpercBestCor: The percBestCor figure
Reference: 5. <author> Gillick, L. and Ito, Y. </author> <title> Confidence Estimation and Evaluation. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <year> 1996. </year>
Reference-contexts: This number varies between 0 and 1. 3 The predictor variables used in this experiment, some of which were inspired by or previously implemented in [3] [4] <ref> [5] </ref> [6], are: * percBestCor: A match is computed between the hypothesized word and the best-scoring basephone at each frame. * phAvgpercBestCor: The percBestCor figure is calculated for each phone in the word, then averaged within word over the number of phones in the word. * phMinpercBestCor: The percBestCor figure is
Reference: 6. <author> Beaufays, F. and Weintraub, M. </author> <title> Neural-network Based Measures of Confidence. </title> <booktitle> LVCSR Hub5 Workshop Presentation, </booktitle> <year> 1996. </year>
Reference-contexts: This number varies between 0 and 1. 3 The predictor variables used in this experiment, some of which were inspired by or previously implemented in [3] [4] [5] <ref> [6] </ref>, are: * percBestCor: A match is computed between the hypothesized word and the best-scoring basephone at each frame. * phAvgpercBestCor: The percBestCor figure is calculated for each phone in the word, then averaged within word over the number of phones in the word. * phMinpercBestCor: The percBestCor figure is calculated
Reference: 7. <author> Cox, S. and Rose, R. </author> <title> Confidence Measures for the Switchboard Database. </title> <booktitle> in: IEEE International Conference on Acoustics, Speech, and Signal Processing. </booktitle> <year> 1996, </year> <pages> pp. 511 514. </pages>
Reference-contexts: The maximum phone-level value found within the word is reported. * acNormS <ref> [7] </ref> [8]: The ratio between the best possible acoustic score within the segmentation of the word and the actual acoustic score of the word. * AvgPhAcNormS: acNormS calculated for each phone in the word, then averaged over the number of phones in the word. * DelNormS: The difference between acNormS and
Reference: 8. <author> Young, S. R. </author> <title> Recognition Confidence Measures: Detection of Misrecognitions and Out-of-Vocabulary Words. no. No. </title> <institution> CMU-CS-94-157, Carnegie Mellon University, School of Computer Science, </institution> <address> Pittsburgh, PA, 15232, USA, </address> <year> 1994. </year>
Reference-contexts: The maximum phone-level value found within the word is reported. * acNormS [7] <ref> [8] </ref>: The ratio between the best possible acoustic score within the segmentation of the word and the actual acoustic score of the word. * AvgPhAcNormS: acNormS calculated for each phone in the word, then averaged over the number of phones in the word. * DelNormS: The difference between acNormS and AvgPhAc-NormS.
References-found: 8

