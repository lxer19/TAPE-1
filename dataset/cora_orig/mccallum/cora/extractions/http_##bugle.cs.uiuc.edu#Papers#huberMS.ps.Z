URL: http://bugle.cs.uiuc.edu/Papers/huberMS.ps.Z
Refering-URL: http://bugle.cs.uiuc.edu/Papers/huberMS.html
Root-URL: http://www.cs.uiuc.edu
Title: PPFS: AN EXPERIMENTAL FILE SYSTEM FOR HIGH PERFORMANCE PARALLEL INPUT/OUTPUT  
Author: BY JAMES VALENTINE HUBER, JR. 
Degree: 1992 THESIS Submitted in partial fulfillment of the requirements for the degree of Master of Science in Computer Science in the Graduate College of the  
Address: 1995 Urbana, Illinois  
Affiliation: B.S., Northern Illinois University,  University of Illinois at Urbana-Champaign,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Arendt, J. W. </author> <title> Parallel Genome Sequence Comparison Using an iPSC/2 with a Concurrent File System. </title> <type> Master's thesis, </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Jan. </month> <year> 1991. </year>
Reference-contexts: Prefetching must not be overly aggressive, otherwise it may cause the cache to thrash by ejecting data which will soon be needed in favor of data that will be needed further in the future <ref> [1] </ref>. 7 2.1.2.3 Write Behind Write behind can often reduce the amount of data written to disk, by eliminating multiple write operations to the same location. It can also reduce the number of write operations by combining smaller continguous operations into one larger write. <p> The algorithm produces a homology score, which is a numeric value indicating how closely two sequences match. The I/O behavior of this code running on the Intel iPSC/860 and Paragon XP/S has been studied in <ref> [1, 28] </ref>. 4.2.1 Application Features The purpose of the genome code is to search a large database of sequences, identifying those which closely match a target sequence.
Reference: [2] <author> Aydt, R. A. </author> <title> A User's Guide to Pablo I/O Instrumentation. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Figures 4.6 and 4.6 illustrate the speedup of both versions of the code, based on the number of client nodes. 4.2.3 Performance Analysis To understand the performance differences between these two versions of the genome code, we instrumented them with the Pablo instrumentation library <ref> [26, 23, 2] </ref>. We gathered detailed information about the frequency, duration, and size of various (system level) I/O operations. The resulting data, taken from additional runs with 16 I/O nodes and 64 clients, is presented in Figures 4.7 and 4.8.
Reference: [3] <author> Cabrera, L.-F., and Long, D. D. E. </author> <title> Exploiting Multiple I/O Streams to Provide High Data-Rates. </title> <booktitle> In Proceedings of the 1991 Summer Usenix Conference (1991), </booktitle> <pages> pp. 31-48. </pages>
Reference-contexts: These provide data striping and a small set of parallel file access modes. In many cases, these access modes do not allow the application enough control to extract good performance from the input/output system. Distributed file systems, such as Zebra [12] and Swift <ref> [3] </ref>, stripe data over distributed input/output servers, but do not provide distribution or policy control to the application layer. Further, because the performance requirements in this environment are quite different (users are not as willing to tune for input/output performance), these systems provide little control to the application program.
Reference: [4] <author> Choudary, A., Bordawekar, R., Harry, M., Krishnaiyer, R., Ponnusamy, R., Singh, T., and Thakur, R. </author> <title> PASSION: Parallel And Scalable Software for Input-Output. </title> <type> Tech. rep., </type> <institution> Department of Electrical and Computer Engineering, Syracuse University, </institution> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Unlike PPFS, PIOUS enforces sequential consistency on file accesses to provide fault-tolerance on unreliable, distributed systems. In PPFS, because the primary focus is on parallel systems, data consistency can be controlled by the application, enabling higher performance in many cases. PASSION <ref> [4] </ref> is another user-level library that provides many features found in PPFS. However, 2 PASSION focuses on out-of-core problems and support for compiler-generated input/output op-erations, rather than multiple, autonomous input/output streams. The Vesta [5] parallel file system, developed by IBM, allows applications to define logical partitions, and some access information.
Reference: [5] <author> Corbett, P., and Feitelson, D. </author> <title> Design and Implementation of the Vesta Parallel File System. </title> <booktitle> In Scalable High-Performance Computing Conference (May 1994), </booktitle> <pages> pp. 63-70. </pages>
Reference-contexts: PASSION [4] is another user-level library that provides many features found in PPFS. However, 2 PASSION focuses on out-of-core problems and support for compiler-generated input/output op-erations, rather than multiple, autonomous input/output streams. The Vesta <ref> [5] </ref> parallel file system, developed by IBM, allows applications to define logical partitions, and some access information. Applications can also exercise some control over the data distribution.
Reference: [6] <author> Crandall, P. E., Chien, A. A., and Reed, D. A. </author> <title> I/O Characteristics of a Production-Scale Run of the SMC Electron-Scattering Program. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Open, close, and read operations combined amount to less than five percent of the total I/O time, while writes and seeks involve 42 percent and 54 percent, respectively <ref> [6] </ref>. We therefore concern ourselves with the write and seek operations and ignore all other I/O.
Reference: [7] <author> Davison, D. </author> <title> Sequence Similarity ('Homology') Searching for Molecular Biologists. </title> <journal> In Bulletin of Mathematical Biology 47 (1985), </journal> <volume> vol. 4, </volume> <pages> pp. 437-474. </pages>
Reference-contexts: of PPFS, this startup time is a negligable fraction of the time a scientific application runs, and for shorter programs, one can leave the system running and pay the startup price once. 4.2 Genome Pattern Matching Algorithms that compare sequences of nucleotide bases are used to study pathology and heredity <ref> [7] </ref>. Because genetic sequencing methods are error-prone, approximate matching algorithms are employed to find similar sequences. We experimented with a parallel FORTRAN program 3 which uses a generalized Needleman, Wunsch, and Sellars (NWS) sequence comparison algorithm. This dynamic programming algorithm, developed by Needleman and Wunsch [22], finds optimal sequence alignments.
Reference: [8] <author> Ellis, C. S., and Kotz, D. </author> <title> Prefetching in File Systems for MIMD Multiprocessors. </title> <booktitle> In Proceedings of the 1989 International Conference on Parallel Processing (August 1989), </booktitle> <pages> pp. </pages> <month> I:306-314. </month>
Reference-contexts: Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems [25, 15]. In [15], Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in <ref> [16, 8, 17] </ref>. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments. Chapter 2 presents the logical organization of PPFS. Chapter 3 describes the current implementation of PPFS.
Reference: [9] <author> Fox, G., Hiranandani, S., Kennedy, K., Kremer, U., and Tseng, C. </author> <title> Fortran D Language Specification. </title> <type> Tech. Rep. </type> <institution> COMP TR90-141, Rice University, Houston, Texas, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: A distribution can be seen as a one-to-one mapping from file record number to a pair containing segment number and segment record number. PPFS provides built-in support for common distributions such as striped and blocked, as well as other HPF <ref> [9] </ref> distributions. 8 2.1.3.3 Indexing To support parallel files with records of different lengths, another level of data placement is required. A parallel file may have an optional indexing scheme which determines the length and location of records. <p> The StripeDist class supports striped distributions with an arbitrary striping factor. A striping factor of one corresponds to unit striping, and a stripe factor of N places sequences of N contiguous records in each segment, in round-robin fashion. The UniformDist class provides for HPF <ref> [9] </ref> style distributions, and anything else can be implemented with the generic FnDist which allows the user to specify function pointers for the map () and unmap () operations.
Reference: [10] <author> French, J. C., Pratt, T. W., and Das, M. </author> <title> Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube. </title> <journal> Journal of Parallel and Distributed Computing 17, </journal> <note> 1-2 (January and February 1993), 115-121. </note>
Reference-contexts: Applications can also exercise some control over the data distribution. Related work also includes a number of commercial parallel file systems | the CM-5 Scalable Parallel File System [19, 18], the Intel Concurrent File System <ref> [10] </ref> for the iPSC/2 and iPSC/860, and the Intel Paragon's Parallel File System [14]. These provide data striping and a small set of parallel file access modes. In many cases, these access modes do not allow the application enough control to extract good performance from the input/output system.
Reference: [11] <author> Geist, G., and Sunderam, V. </author> <title> Network Based Concurrent Computing on the PVM System. Concurrent: </title> <note> Practice and Experience June (1992). </note>
Reference-contexts: Several message-passing libraries were considered, but most lacked what were considered vital functionality. For instance, MPI [21] lacks process management, and PVM <ref> [11] </ref> has no support for interrupt-driven messages. Both of these important features are provided by NXLib [29], which is a port of NX to various workstation platforms.
Reference: [12] <author> Hartman, J. H., and Ousterhout, J. K. </author> <title> Zebra: A Striped Network File System. </title> <booktitle> In Proceedings of the Usenix File Systems Workshop (May 1992), </booktitle> <pages> pp. 71-78. </pages>
Reference-contexts: These provide data striping and a small set of parallel file access modes. In many cases, these access modes do not allow the application enough control to extract good performance from the input/output system. Distributed file systems, such as Zebra <ref> [12] </ref> and Swift [3], stripe data over distributed input/output servers, but do not provide distribution or policy control to the application layer.
Reference: [13] <author> Huber, Jr., J. V., Elford, C. L., Reed, D. A., Chien, A. A., and Blumenthal, D. S. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In 9th ACM International Conference on Supercomputing (submitted for publication 1994). </booktitle>
Reference-contexts: Using programs that consist solely of input/output operations, we can compare PPFS with the native file system <ref> [13] </ref>. The benchmarks consist of two programs: one creates and writes a 64 MB file, and the other reads the same file. In the first case, each node writes a disjoint set of records.
Reference: [14] <author> Intel Supercomputer Systems Division. </author> <title> Paragon XP/S Product Overview. </title> <address> Beaver-ton, OR, </address> <month> Nov. </month> <year> 1991. </year>
Reference-contexts: Until this gap is closed, input/output will continue to be one of the primary potential bottlenecks in high-performance parallel computing. To improve I/O subsystem performance, many parallel systems employ a set of disks in parallel. The Intel Paragon XP/S, for example, supports multiple redundant arrays of inexpensive disks (RAIDs) <ref> [24, 14] </ref>. Systems such as this have an impressive peak I/O throughput, equal to the product of the throughput of each device and the number of devices. <p> Applications can also exercise some control over the data distribution. Related work also includes a number of commercial parallel file systems | the CM-5 Scalable Parallel File System [19, 18], the Intel Concurrent File System [10] for the iPSC/2 and iPSC/860, and the Intel Paragon's Parallel File System <ref> [14] </ref>. These provide data striping and a small set of parallel file access modes. In many cases, these access modes do not allow the application enough control to extract good performance from the input/output system. <p> The data server is split into two separate programs, thus the server can service requests out of its cache while a blocking input/output operation is in progress on the associated server slave. 3.1.1 Programming Platforms Two platforms were selected for the initial implementation: the Intel Paragon XP/S <ref> [14] </ref>, and a cluster of Sun SPARC workstations. The workstations served as a development platform, as they provide a more stable and productive programming environment than the Paragon.
Reference: [15] <author> Kotz, D. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <type> Tech. rep., </type> <institution> Department of Computer Science, Dartmouth College, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: Further, because the performance requirements in this environment are quite different (users are not as willing to tune for input/output performance), these systems provide little control to the application program. Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems <ref> [25, 15] </ref>. In [15], Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in [16, 8, 17]. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments. <p> Further, because the performance requirements in this environment are quite different (users are not as willing to tune for input/output performance), these systems provide little control to the application program. Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems [25, 15]. In <ref> [15] </ref>, Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in [16, 8, 17]. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments.
Reference: [16] <author> Kotz, D., and Ellis, C. S. </author> <title> Caching and writeback policies in parallel file systems. </title> <booktitle> In 1991 IEEE Symposium on Parallel and Distributed Processing (December 1991), </booktitle> <pages> pp. 60-67. </pages>
Reference-contexts: Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems [25, 15]. In [15], Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in <ref> [16, 8, 17] </ref>. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments. Chapter 2 presents the logical organization of PPFS. Chapter 3 describes the current implementation of PPFS.
Reference: [17] <author> Kotz, D., and Ellis, C. S. </author> <title> Practical prefetching techniques for parallel file systems. </title> <booktitle> In Proceedings of the First International Conference on Parallel and Distributed Information Systems (December 1991), </booktitle> <pages> pp. 182-189. </pages>
Reference-contexts: Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems [25, 15]. In [15], Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in <ref> [16, 8, 17] </ref>. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments. Chapter 2 presents the logical organization of PPFS. Chapter 3 describes the current implementation of PPFS.
Reference: [18] <author> Kwan, T. T., and Reed, D. A. </author> <title> Performance of the CM-5 Scalable File System. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing (July 1994). </booktitle>
Reference-contexts: The Vesta [5] parallel file system, developed by IBM, allows applications to define logical partitions, and some access information. Applications can also exercise some control over the data distribution. Related work also includes a number of commercial parallel file systems | the CM-5 Scalable Parallel File System <ref> [19, 18] </ref>, the Intel Concurrent File System [10] for the iPSC/2 and iPSC/860, and the Intel Paragon's Parallel File System [14]. These provide data striping and a small set of parallel file access modes.
Reference: [19] <author> LoVerso, S. J., Isman, M., Nanopoulos, A., Nesheim, W., Milne, E. D., and Wheeler, R. sfs: </author> <title> A Parallel File System for the CM-5. </title> <booktitle> In Proceedings of the 1993 Summer Usenix Conference (1993), </booktitle> <pages> pp. 291-305. 59 </pages>
Reference-contexts: The Vesta [5] parallel file system, developed by IBM, allows applications to define logical partitions, and some access information. Applications can also exercise some control over the data distribution. Related work also includes a number of commercial parallel file systems | the CM-5 Scalable Parallel File System <ref> [19, 18] </ref>, the Intel Concurrent File System [10] for the iPSC/2 and iPSC/860, and the Intel Paragon's Parallel File System [14]. These provide data striping and a small set of parallel file access modes.
Reference: [20] <author> Moyer, S. A., and Sundaram, V. S. </author> <title> PIOUS: A Scalable Parallel I/O System for Distributed Computing Environments. </title> <booktitle> In 1994 Scalable High Performance Computing Conference (May 1994), </booktitle> <pages> pp. 71-78. </pages>
Reference-contexts: We believe PPFS has a richer interface than these systems, supporting application control and extension of input/output system policies, and declaration of access information. PIOUS (Parallel Input/OUtput System) <ref> [20] </ref> is a portable input/output system primarily for use with PVM. Unlike PPFS, PIOUS enforces sequential consistency on file accesses to provide fault-tolerance on unreliable, distributed systems.
Reference: [21] <author> MPI. </author> <title> MPI: A Message Passing Interface Standard. </title> <type> Tech. rep., Message Passing Interface Forum, </type> <month> May </month> <year> 1994. </year>
Reference-contexts: Several message-passing libraries were considered, but most lacked what were considered vital functionality. For instance, MPI <ref> [21] </ref> lacks process management, and PVM [11] has no support for interrupt-driven messages. Both of these important features are provided by NXLib [29], which is a port of NX to various workstation platforms. <p> We also suspect that the scattering code uses more synchronization than is truly necessary, but the only general solution to this problem is for PPFS to provide efficient global operations. This would be possible if PPFS were implemented in a message-passing environment that supports user-defined communication groups. MPI <ref> [21] </ref> and PVM citepvm are two such message-passing platforms. 4.4 Summary We have demonstrated that the flexible design of PPFS, which allows the file system to be tuned to the application, yields promising results with simple benchmarks as well as production scientific applications. <p> In addition to completing the implementation, we also plan to port PPFS to other platforms. We have partially completed the port to our next target environment, the IBM SP/2. The SP/2 supports several message-passing libraries, including the emerging standard, MPI <ref> [21] </ref>. We are testing PPFS under MPI running on a network of workstations, and plan to use move to the SP/2 soon. 57
Reference: [22] <author> Needleman, S. B., and Wunsch, C. D. </author> <title> An Efficient Method Applicable to the Search for Similarities in the Amino Acid Sequences of Two Proteins. </title> <journal> Journal of Molecular Biology 48 (1970), </journal> <pages> 444-453. </pages>
Reference-contexts: Because genetic sequencing methods are error-prone, approximate matching algorithms are employed to find similar sequences. We experimented with a parallel FORTRAN program 3 which uses a generalized Needleman, Wunsch, and Sellars (NWS) sequence comparison algorithm. This dynamic programming algorithm, developed by Needleman and Wunsch <ref> [22] </ref>, finds optimal sequence alignments. Enhancements made by Sellers find good local alignments of subsequences [27].
Reference: [23] <author> Noe, R. J. </author> <title> Pablo Instrumentation Environment Users Guide. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Dec. </month> <year> 1994. </year>
Reference-contexts: Figures 4.6 and 4.6 illustrate the speedup of both versions of the code, based on the number of client nodes. 4.2.3 Performance Analysis To understand the performance differences between these two versions of the genome code, we instrumented them with the Pablo instrumentation library <ref> [26, 23, 2] </ref>. We gathered detailed information about the frequency, duration, and size of various (system level) I/O operations. The resulting data, taken from additional runs with 16 I/O nodes and 64 clients, is presented in Figures 4.7 and 4.8.
Reference: [24] <author> Patterson, D., Gibson, G., and Katz, R. </author> <title> A Case For Redundant Arrays of Inexpensive Disks (RAID). </title> <booktitle> In Proceedings of ACM SIGMOD (December 1988). </booktitle>
Reference-contexts: Until this gap is closed, input/output will continue to be one of the primary potential bottlenecks in high-performance parallel computing. To improve I/O subsystem performance, many parallel systems employ a set of disks in parallel. The Intel Paragon XP/S, for example, supports multiple redundant arrays of inexpensive disks (RAIDs) <ref> [24, 14] </ref>. Systems such as this have an impressive peak I/O throughput, equal to the product of the throughput of each device and the number of devices.
Reference: [25] <author> Patterson, D., Gibson, G., and Satyanarayanan, M. </author> <title> A Status Report on Research in Transparent Informed Prefetching. </title> <type> Tech. Rep. </type> <institution> CMU-CS-93-113, Department of Computer Science, Carnegie-Mellon University, </institution> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: Further, because the performance requirements in this environment are quite different (users are not as willing to tune for input/output performance), these systems provide little control to the application program. Several groups have proposed schemes for exploiting access pattern information both in sequential and parallel systems <ref> [25, 15] </ref>. In [15], Kotz uses pattern predictors to anticipate an application's future access patterns. A variety of data management strategies for parallel input/output systems are explored in [16, 8, 17]. 1.3 Organization The rest of this thesis presents the design of our PPFS and its use in input/output experiments.
Reference: [26] <author> Reed, D. A., Aydt, R. A., Madhyastha, T. M., Noe, R. J., Shields, K. A., and Schwartz, B. W. </author> <title> An Overview of the Pablo Performance Analysis environment. </title> <type> Tech. rep., </type> <institution> University of Illinois at Urbana-Champaign, Department of Computer Science, </institution> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Figures 4.6 and 4.6 illustrate the speedup of both versions of the code, based on the number of client nodes. 4.2.3 Performance Analysis To understand the performance differences between these two versions of the genome code, we instrumented them with the Pablo instrumentation library <ref> [26, 23, 2] </ref>. We gathered detailed information about the frequency, duration, and size of various (system level) I/O operations. The resulting data, taken from additional runs with 16 I/O nodes and 64 clients, is presented in Figures 4.7 and 4.8.
Reference: [27] <author> Sellers, P. H. </author> <title> On the Theory and Computation of Evolutionary Distances. </title> <note> SIAM Journal of Applied Mathematics 26 4 (June 1974), 787-793. </note>
Reference-contexts: We experimented with a parallel FORTRAN program 3 which uses a generalized Needleman, Wunsch, and Sellars (NWS) sequence comparison algorithm. This dynamic programming algorithm, developed by Needleman and Wunsch [22], finds optimal sequence alignments. Enhancements made by Sellers find good local alignments of subsequences <ref> [27] </ref>. A K-tuple heuristic [31] is used to prune the search space and compensate 3 This code was originally developed for the Cray X-MP by Dan Davidson. 44 for the quadratic time of the basic comparison algorihtm.
Reference: [28] <author> Shields, K., Tavera, L., Scullin, W. H., Elford, C. L., and Reed, D. A. </author> <title> Virtual Reality for Parallel Computer Systems Analysis. </title> <booktitle> In ACM SIGGRAPH '94 Visual Proceedings (July 1994), </booktitle> <address> p. </address> <month> 261. </month>
Reference-contexts: The algorithm produces a homology score, which is a numeric value indicating how closely two sequences match. The I/O behavior of this code running on the Intel iPSC/860 and Paragon XP/S has been studied in <ref> [1, 28] </ref>. 4.2.1 Application Features The purpose of the genome code is to search a large database of sequences, identifying those which closely match a target sequence.
Reference: [29] <author> Stellner, G., Bode, A., Lamberts, S., and Ludwig, T. </author> <title> Developing Applications for Multicomputer Systems on Workstation Clusters. </title> <booktitle> In HPCN Europe, The International Conference and Exhibition on High-Performance Computing and Networking (1994). </booktitle> <pages> 60 </pages>
Reference-contexts: Several message-passing libraries were considered, but most lacked what were considered vital functionality. For instance, MPI [21] lacks process management, and PVM [11] has no support for interrupt-driven messages. Both of these important features are provided by NXLib <ref> [29] </ref>, which is a port of NX to various workstation platforms. Thus, NXLib was chosen for the SPARC platform. 3.2 System Processes Each of the PPFS elements defined in x2.2 is implemented by one or more system processes. The run process spawns the other system processes as described in x3.1.1.
Reference: [30] <author> Stroustrup, B. </author> <title> The C++ Programming Language, Second Edition. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1991. </year>
Reference-contexts: To achieve these goals, and to ease the burden of crafting a complex software system, the C++ programming language <ref> [30] </ref> was chosen to implement PPFS. The general availability of C++ makes it a reasonable choice for a portable system. C++ offers type safety, encapsulation, inheritance, and polymorphism, which allow the design concepts of PPFS to be expressed naturally in the implementation.
Reference: [31] <author> Wilbur, W. J., and Lipman, D. J. </author> <title> Rapid Similarity Searches of Nucleic Acid and Protein Data Banks. </title> <booktitle> In Proceedings of the National Academy of Sciences (1983), </booktitle> <volume> vol. 80, </volume> <pages> pp. 726-730. </pages>
Reference-contexts: We experimented with a parallel FORTRAN program 3 which uses a generalized Needleman, Wunsch, and Sellars (NWS) sequence comparison algorithm. This dynamic programming algorithm, developed by Needleman and Wunsch [22], finds optimal sequence alignments. Enhancements made by Sellers find good local alignments of subsequences [27]. A K-tuple heuristic <ref> [31] </ref> is used to prune the search space and compensate 3 This code was originally developed for the Cray X-MP by Dan Davidson. 44 for the quadratic time of the basic comparison algorihtm. The algorithm produces a homology score, which is a numeric value indicating how closely two sequences match.
Reference: [32] <author> Winstead, C., and McKoy, V. </author> <title> Studies of Electron-Molecule Collisions on Massively Parallel Computers. In Modern Electronic Structure Theory, </title> <editor> D. R. Yarkony, Ed., </editor> <volume> vol. 2. </volume> <publisher> World Scientific, </publisher> <year> 1994. </year> <month> 61 </month>
Reference-contexts: The Schwinger Multichannel (SMC) method is based on Schwinger's variational principle for scattering amplitude <ref> [32] </ref>. This method obtains low-energy electron-molecule cross sections by calculating scattering amplitudes using computationally intense matrix operations involving integrals, some of which must be handled using numerical quadrature. We have studied a parallel implementation 6 of the SMC method running on the Intel Paragon XP/S.
References-found: 32

