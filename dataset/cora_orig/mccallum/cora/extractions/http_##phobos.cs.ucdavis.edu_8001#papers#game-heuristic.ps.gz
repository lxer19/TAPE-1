URL: http://phobos.cs.ucdavis.edu:8001/papers/game-heuristic.ps.gz
Refering-URL: http://www.ai.univie.ac.at/~juffi/lig/lig-bib.html
Root-URL: 
Email: prieditis@cs.ucdavis.edu  
Phone: (916) 752-6958  
Title: Learning Approximately Admissible Heuristics in Two-Player Games  
Author: Evan Fletcher and Armand Prieditis 
Date: August 25, 1994  
Note: Submitted to the Machine Learning Journal  
Address: Davis, CA 95616  
Affiliation: Department of Computer Science University of California  
Abstract: Admissible heuristics are important in single-agent search algorithms such as IDA * because they guarantee correct forward pruning. One source of admissible heuristics is from simplifications of the original problem; the length of the shortest solution for the simplified problem is an admissible heuristic for the original problem. Recently, a new two-player search algorithm, DTC * , has been developed that uses two-player admissible heuristics (lower-bounds on the minimax value) to guarantee correct forward pruning. Unfortunately, most simplifications in two-player games result in admissible heuristics that are still too expensive to compute online (in actual play). This article describes an offline method of learning an approximately admissible heuristic for two-player games from the results of a small set of offline searches in a simplified space, given only the raw board state. Previous approaches to learning heuristic for two-player games were not aimed at learning admissible heuristics. Using neural nets as the learning vehicle, our results show that it is possible to learn an approximately admissible heuristic outplayed a non-learning opponent in 4x4 Tic-Tac-Toe. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> B. Abramson and R. Korf. </author> <title> A model of two-player evaluation functions. </title> <booktitle> In Proceedings AAAI-87, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1987. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Another approach is to learn the probability of winning from a given state by sampling a search tree where both players make random moves, given a set of predefined state features <ref> [1] </ref>. A third approach is to learn the probability of winning from actual games, given a set of predefined board features. The fourth approach is to learn the probably of winning from actual games, given only the raw board state [28]. <p> Learning can take place on-line (during play) or off-line. The probabilistic approach attempts to estimate the probability of a win from a given state, given a set of predefined state features. For example, Korf and Abramson estimate this probability based on the assumption that each player makes random moves <ref> [1] </ref>. Since each player is assumed to make random moves, they used a sampling rather than search algorithm to compute this frequency. They also used linear regression on the state features to approximate this frequency. <p> Heuristics that estimate the probability of winning, such as those of Tesauro [28] and Korf and Abramson <ref> [1] </ref> are more rigorous than the other previous notions of heuristics. However, they approximate a different objective function than the one we use.
Reference: [2] <author> H. Berliner. </author> <title> The b* search algorithm: A best-first proof procedure. </title> <journal> Artificial Intelligence, </journal> <volume> 12(1) </volume> <pages> 23-40, </pages> <year> 1979. </year> <note> Also in Readings in Artificial Intelligence, </note> <editor> Webber, B. L. and Nilsson, N. J., </editor> <publisher> (Eds.). </publisher>
Reference-contexts: This term is an extension of the term two-player admissible, except that now a depth-bound is included in the definition. Put simply, an algorithm is depth-bounded two-player admissible if it always computes the correct minimax value with respect to a given depth. Notice that algorithms such as B fl <ref> [2] </ref> and Best First Minimax [11] are not depth-bounded two-player admissible because they compute a different objective function. Heuristics h 1 and h 2 are called depth-bounded two-player admissible iff (8s 2 S 1 )(8d 2 @)fh 1 (s) h fl 2 (s; d)g.
Reference: [3] <author> J. Christensen and R. Korf. </author> <title> A unified theory of heuristic evaluation functions and its applications to learning. </title> <booktitle> In Proceedings AAAI-86, </booktitle> <pages> pages 148-152, </pages> <address> Philadelphia, Pennsylvania, </address> <month> August </month> <year> 1986. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: An evaluation polynomial is a linear combinations of state features, having the form P = C 1 F 1 + ::: + C n F n where the C i 's are coefficients and the F i 's are the features <ref> [3, 4, 25, 13] </ref>. The features are defined by human experts, and learning involves finding values of the coefficients using training data derived from minimax state space search.
Reference: [4] <author> P. Frey. </author> <title> Algorithmic strategies for improving the performance of game-playing programs. In Evolution, Games, and Learning. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1986. </year>
Reference-contexts: An evaluation polynomial is a linear combinations of state features, having the form P = C 1 F 1 + ::: + C n F n where the C i 's are coefficients and the F i 's are the features <ref> [3, 4, 25, 13] </ref>. The features are defined by human experts, and learning involves finding values of the coefficients using training data derived from minimax state space search.
Reference: [5] <author> J. Gaschnig. </author> <title> A problem-similarity approach to devising heuristics. </title> <booktitle> In Proceedings IJCAI-6, </booktitle> <pages> pages 301-307, </pages> <address> Tokyo, Japan, </address> <year> 1979. </year> <booktitle> International Joint Conferences on Artificial Intelligence. </booktitle> <pages> 16 </pages>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [6] <author> G. Guida and M. Somalvico. </author> <title> A method for computing heuristics in problem solving. </title> <journal> Information Sciences, </journal> <volume> 19 </volume> <pages> 251-259, </pages> <year> 1979. </year>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [7] <author> M. James. </author> <title> Classification Algorithms. </title> <publisher> William Collins Sons and Company, </publisher> <address> London, England, </address> <year> 1985. </year>
Reference-contexts: To ameliorate such overestimate problems, we modified the neural net output using a penalty matrix. The penalty matrix, which is a standard way to modify the results of a classifier <ref> [7] </ref>, contains a set of penalties associated with each type of misclassification: the entry P ij gives the penalty for misclassifying a state of class i as class j. <p> We found that high-inadmissible estimates from the learned heuristic caused DTC * to ignore more promising moves. As a result, we needed a way to control inadmissibility. We used a penalty matrix, a standard domain-independent technique to adjust errors in pattern classification <ref> [7] </ref>, to increase admissibility at some cost to accuracy. The penalty-adjusted heuristic proved to be much better in tournament play. To see if accuracy was also important we played the penalty-adjusted learned heuristic against a penalty-adjusted random-valued heuristic.
Reference: [8] <author> D. Kibler. </author> <title> Natural generation of heuristics by transforming the problem representation. </title> <type> Technical Report TR-85-20, </type> <institution> Computer Science Department, UC-Irvine, </institution> <year> 1985. </year>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [9] <author> R. Korf. </author> <title> Depth-first iterative-deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27(2) </volume> <pages> 97-109, </pages> <year> 1985. </year>
Reference-contexts: Such heuristics do not guarantee correct forward pruning. Heuristics h 1 and h 2 are called two-player admissible iff (8s 2 S 1 )fh 1 (s) h fl 1 (s)g and 2 (s)g. The term two-player admissible heuristic is consistent with the term admissible heuristic in single-agent search <ref> [9] </ref>, except that in our case we have two objective functions and hence the two heuristics. It is unfortunate that the term admissible is associated with both algorithms and heuristics in single-agent search, but that is the tradition.
Reference: [10] <author> R. Korf. </author> <title> Real-time heuristic search: New results. </title> <publisher> AAAI, </publisher> <year> 1988. </year>
Reference-contexts: The particular simplification that we use guarantees admissibility; we use the term approximately admissible to emphasize that the learned function is an approximation of the data. In contrast, Korf's LRTA * learns a single-agent admissible heuristic in the form of a lookup table for all states <ref> [10] </ref>. The entries in the lookup table are updated as a result of lookahead searches. Unfortunately, since state space size is generally enormous, such a lookup table cannot be stored for most problems. For example, the table would require 1:046139 fi 10 13 entries for the Fifteen puzzle. <p> To reduce the expense of computing the heuristic on-line, a lookup table of h fl 1 (s; k) can be stored for a sample set of states encountered on-line or generated off-line. However, this method is generally too space-expensive, as is the case with Korf's LRTA * <ref> [10] </ref>. Such a table could be approximated by using a learning algorithm and a set of examples. For example, Tesauro's system used actual game-playing rather than search to generate the examples for learning [28].
Reference: [11] <author> R. Korf and D. Chickering. </author> <title> Best-first minimax search: Othello results. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: Put simply, an algorithm is depth-bounded two-player admissible if it always computes the correct minimax value with respect to a given depth. Notice that algorithms such as B fl [2] and Best First Minimax <ref> [11] </ref> are not depth-bounded two-player admissible because they compute a different objective function. Heuristics h 1 and h 2 are called depth-bounded two-player admissible iff (8s 2 S 1 )(8d 2 @)fh 1 (s) h fl 2 (s; d)g.
Reference: [12] <author> K-F Lee and S. Mahajan. </author> <title> A pattern classification approach to evaluation function learning. </title> <journal> Artificial Intelligence, </journal> <volume> 36(1) </volume> <pages> 1-25, </pages> <year> 1988. </year>
Reference-contexts: They also used linear regression on the state features to approximate this frequency. Similarly, Lee and Mahajan used multivariate normal Bayesian learning with four state features to approximate this frequency for the game of Othello <ref> [12] </ref>. Instead of assuming random play, they used the outcome of expert play to compute the frequency. Lee and Mahajan describe several advantages of their approach. The learning is completely automatic, involving no human intervention (other than specifying the state features). <p> Second, our definition of a two-player heuristic for both Min and Max is rigorously defined with respect to the ultimate objective function. In contrast, most previous notions of heuristicsnotions that include measures of worth [15] merit and strength [16], quality [30], goodness <ref> [12] </ref>, and promise [24]do not appear to be approximating any clearly stated objective function. Heuristics that estimate the probability of winning, such as those of Tesauro [28] and Korf and Abramson [1] are more rigorous than the other previous notions of heuristics.
Reference: [13] <author> D. Mitchell. </author> <title> Using features to evaluate experts' and novices' othello games. </title> <type> Master's thesis, </type> <institution> Northwestern University, </institution> <year> 1984. </year>
Reference-contexts: An evaluation polynomial is a linear combinations of state features, having the form P = C 1 F 1 + ::: + C n F n where the C i 's are coefficients and the F i 's are the features <ref> [3, 4, 25, 13] </ref>. The features are defined by human experts, and learning involves finding values of the coefficients using training data derived from minimax state space search.
Reference: [14] <author> J. Mostow and A. </author> <title> Prieditis. Discovering admissible heuristics by abstracting and optimizing. </title> <booktitle> In Proceedings IJCAI-11, </booktitle> <address> Detroit, MI, </address> <month> August </month> <year> 1989. </year> <booktitle> International Joint Conferences on Artificial Intelligence. </booktitle>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [15] <author> N. J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: This is because Min's ultimate objective function, which relies on Max's ultimate objective, is supposed to return a result relative to Min, not Max. This definition of the objective function is also somewhat more general than those in standard AI texts <ref> [16, 15] </ref> in that it allows for variable arc costs and terminal values, which can include symbolic values such as Draw and Loss, provided +, and &lt; are defined over such symbolic values as if those symbolic values were infinite numbers. <p> Heuristic evaluation from Min's perspective is central to forward-pruning in two-player search [22]. Our notion of a heuristic as approximation of the ultimate objective function is consistent with that of single-player search, where a heuristic is an estimate of an objective function that returns the least cost path <ref> [15] </ref>. Although our notion of a two-player heuristic is consistent with single-agent search, it conceptually differs from previous notions of two-player heuristics in two ways. First, it involves two heuristics (one for Min and one for Max), which are not symmetric. <p> Second, our definition of a two-player heuristic for both Min and Max is rigorously defined with respect to the ultimate objective function. In contrast, most previous notions of heuristicsnotions that include measures of worth <ref> [15] </ref> merit and strength [16], quality [30], goodness [12], and promise [24]do not appear to be approximating any clearly stated objective function. Heuristics that estimate the probability of winning, such as those of Tesauro [28] and Korf and Abramson [1] are more rigorous than the other previous notions of heuristics.
Reference: [16] <author> J. Pearl. </author> <title> Heuristics: Intelligent Search Strategies for Computer Problem-Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem. <p> This is because Min's ultimate objective function, which relies on Max's ultimate objective, is supposed to return a result relative to Min, not Max. This definition of the objective function is also somewhat more general than those in standard AI texts <ref> [16, 15] </ref> in that it allows for variable arc costs and terminal values, which can include symbolic values such as Draw and Loss, provided +, and &lt; are defined over such symbolic values as if those symbolic values were infinite numbers. <p> This term is consistent with admissible search in single-agent search algorithms <ref> [16] </ref>. Of course, computing such a function is typically feasible only in endgameswhere the distance to terminals is low. For example, it can be used to solve chess problems that involve verifying that White can win in 5 moves. <p> First, it involves two heuristics (one for Min and one for Max), which are not symmetric. Typically, heuristics for Min are the negative of heuristics for Max under the assumption that what is good for Min is bad for Max <ref> [16] </ref>. In contrast to the standard formulation of heuristics in two-player search, which is from each player's perspective, our heuristics are only from Min's perspective. Second, our definition of a two-player heuristic for both Min and Max is rigorously defined with respect to the ultimate objective function. <p> Second, our definition of a two-player heuristic for both Min and Max is rigorously defined with respect to the ultimate objective function. In contrast, most previous notions of heuristicsnotions that include measures of worth [15] merit and strength <ref> [16] </ref>, quality [30], goodness [12], and promise [24]do not appear to be approximating any clearly stated objective function. Heuristics that estimate the probability of winning, such as those of Tesauro [28] and Korf and Abramson [1] are more rigorous than the other previous notions of heuristics.
Reference: [17] <author> D. Pomerleau. ALVINN: </author> <title> An autonomous land vehicle in a neural network. </title> <booktitle> In Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 305-313. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1989. </year>
Reference-contexts: A training set consisting of hypothetical anticipated moves would leave the learned heuristic vulnerable to any opponent that made different moves. A similar principle was used in the ALVINN project, which trained a net to drive a car <ref> [17] </ref>. Using only training data from good driving made ALVINN perform poorly whenever it encountered a bad driving situation, such as swerving onto the shoulder. To avoid this outcome, ALVINN was trained on both good and bad driving situations.
Reference: [18] <author> A. </author> <title> Prieditis. Discovering Effective Admissible Heuristics by Abstraction and Speedup: A Transformational Approach. </title> <type> PhD thesis, </type> <institution> Rutgers University, </institution> <year> 1990. </year>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [19] <author> A. </author> <title> Prieditis. Machine discovery of effective admissible heuristics. </title> <booktitle> In Proceedings IJCAI-12, </booktitle> <address> Sydney, Australia, </address> <month> August </month> <year> 1991. </year> <booktitle> International Joint Conferences on Artificial Intelligence. </booktitle>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [20] <author> A. </author> <title> Prieditis. Machine discovery of effective admissible heuristics. </title> <journal> Machine Learning, </journal> <volume> 12 </volume> <pages> 117-141, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [21] <author> A. </author> <title> Prieditis. Extending admissibility to two-player search, </title> <month> August </month> <year> 1994. </year> <note> Submitted to the Journal of Artificial Intelligence. </note>
Reference-contexts: The function maps a state in the original problem to one in the simplified problem. 2 Recently, a new two-player search algorithm, DTC * , has been developed that uses two-player admissible heuristics to guarantee correct forward pruning <ref> [21] </ref>. Such heuristics, which are lower-bounds on the minimax value, are two-player analogues of admissible heuristics in single-agent search. Unfortunately, most simplifications in two-player games result in heuristics that are still too expensive for actual (on-line) play. <p> Heuristics h 1 and h 2 are called depth-bounded two-player admissible iff (8s 2 S 1 )(8d 2 @)fh 1 (s) h fl 2 (s; d)g. This term simply extends two-player admissible heuristics by including a depth bound. Depth-bounded two-player admissible heuristics guarantee that DTC * is depth-bounded admissible <ref> [21] </ref>. 3.3 Simplifications as a Source of Admissible Heuristics in Two-Player Games Recall that our notion of heuristics is that they are approximations of the ultimate (non-depth-bounded) minimax objective function h fl 1 (s).
Reference: [22] <author> A. </author> <title> Prieditis. Extending admissible heuristics two-player endgames search, </title> <month> August </month> <year> 1994. </year> <note> Submitted to the Journal of Artificial Intelligence Research. </note>
Reference-contexts: A heuristic for Max, denoted h 2 (s), is an approximation of Max's objective function h fl 2 (s). Note that h 2 (s) is still from player 1's (Min's) perspective. Heuristic evaluation from Min's perspective is central to forward-pruning in two-player search <ref> [22] </ref>. Our notion of a heuristic as approximation of the ultimate objective function is consistent with that of single-player search, where a heuristic is an estimate of an objective function that returns the least cost path [15]. <p> In keeping with tradition, we use the same terms to describe both two-player algorithms and heuristics with certain properties. Admissible heuristics guarantee admissibility in a non-depth-bounded version of DTC * <ref> [22] </ref>.
Reference: [23] <author> A. Prieditis and B. Janakiraman. </author> <title> Generating effective admissible heuristics by reconstitution. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <address> Washington, DC, </address> <month> August </month> <year> 1993. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: A heuristic is admissible if it gives a lower bound to the objective function computed by the search heuristic. One rich source of admissible heuristics in single-agent search is from simplifications of a problem <ref> [5, 6, 8, 14, 16, 18, 19, 20, 23] </ref>. As Figure 1 shows, the cost of a least cost solution in the simplified problem is the admissible heuristic. The intuitive reason why simplifications generate admissible heuristics is because they add short-cut solution paths by simplifying the original problem.
Reference: [24] <author> E. Rich and K. Knight. </author> <booktitle> Artificial Intelligence: Second Edition. </booktitle> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, </address> <year> 1991. </year>
Reference: [25] <author> A. L. Samuel. </author> <title> Some studies of machine learning using the game of checkers. </title> <booktitle> In Computers and Thought, </booktitle> <pages> pages 71-105. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: One approach is to learn the worth or merit of a state by adjusting the coefficients of a polynomial function of a set of predefined state features <ref> [25] </ref>. Another approach is to learn the probability of winning from a given state by sampling a search tree where both players make random moves, given a set of predefined state features [1]. <p> An evaluation polynomial is a linear combinations of state features, having the form P = C 1 F 1 + ::: + C n F n where the C i 's are coefficients and the F i 's are the features <ref> [3, 4, 25, 13] </ref>. The features are defined by human experts, and learning involves finding values of the coefficients using training data derived from minimax state space search.
Reference: [26] <author> S. Smith and D. Nau. </author> <title> An analysis of forward pruning. </title> <booktitle> In Proceedings AAAI-93, </booktitle> <address> Seattle, WA, </address> <month> August </month> <year> 1994. </year> <journal> American Association for Artificial Intelligence. </journal>
Reference-contexts: In contrast, random trees, which are sometimes used to evaluate two-player search algorithms <ref> [26] </ref>, have no structure to their states on which the learning algorithm can train. We chose neural nets as the learning vehicle because they were readily available off-the-shelf and because we believed that other learning algorithms would produce comparable results.
Reference: [27] <author> R. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3(1) </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: In contrast, evaluation polynomials that are linear combinations of the features cannot capture feature dependencies because they necessarily model the features as mutually independent. Tesauro used temporal difference learning <ref> [27] </ref> with a neural net as the learning vehicle for Backgammon [28]. His program used self-play to produce a training set whose elements consisted of the raw board state and the outcome of playing to the end from that state.
Reference: [28] <author> G. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: A third approach is to learn the probability of winning from actual games, given a set of predefined board features. The fourth approach is to learn the probably of winning from actual games, given only the raw board state <ref> [28] </ref>. None of these approaches guarantees that the resulting heuristics will be admissible or even approximately admissible. Again, the property of admissibility is important because it guarantees correct forward pruning in a recently developed search algorithm called DTC * . <p> In contrast, evaluation polynomials that are linear combinations of the features cannot capture feature dependencies because they necessarily model the features as mutually independent. Tesauro used temporal difference learning [27] with a neural net as the learning vehicle for Backgammon <ref> [28] </ref>. His program used self-play to produce a training set whose elements consisted of the raw board state and the outcome of playing to the end from that state. As play progressed towards the end, successive moves yielded more and more information. <p> In contrast, most previous notions of heuristicsnotions that include measures of worth [15] merit and strength [16], quality [30], goodness [12], and promise [24]do not appear to be approximating any clearly stated objective function. Heuristics that estimate the probability of winning, such as those of Tesauro <ref> [28] </ref> and Korf and Abramson [1] are more rigorous than the other previous notions of heuristics. However, they approximate a different objective function than the one we use. <p> However, this method is generally too space-expensive, as is the case with Korf's LRTA * [10]. Such a table could be approximated by using a learning algorithm and a set of examples. For example, Tesauro's system used actual game-playing rather than search to generate the examples for learning <ref> [28] </ref>. However, his system required millions of training examples to yield effective heuristic because he used actual games rather than lookahead searches. In this article, we focus on a technique that requires a small set of training examples to produce heuristics by using searches in a simplified space.
Reference: [29] <author> S. Weiss and C. </author> <title> Kulikowski. Computer Systems That Learn. </title> <publisher> Morgan Kaufman Publishers, Inc., </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: We used leave-one-out because it is a standard technique which has been shown to be a nearly unbiased estimator of the true error of the classifier <ref> [29] </ref>.
Reference: [30] <author> P. Winston. </author> <booktitle> Artificial Intelligence: Third Edition. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1992. </year> <month> 18 </month>
Reference-contexts: Second, our definition of a two-player heuristic for both Min and Max is rigorously defined with respect to the ultimate objective function. In contrast, most previous notions of heuristicsnotions that include measures of worth [15] merit and strength [16], quality <ref> [30] </ref>, goodness [12], and promise [24]do not appear to be approximating any clearly stated objective function. Heuristics that estimate the probability of winning, such as those of Tesauro [28] and Korf and Abramson [1] are more rigorous than the other previous notions of heuristics.
References-found: 30

