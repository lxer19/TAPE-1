URL: http://www.cs.huji.ac.il/papers/IP/caip.ps.gz
Refering-URL: http://www.cs.huji.ac.il/papers/IP/index.html
Root-URL: 
Title: Robust Recovery of Ego-Motion  
Author: Michal Irani Benny Rousso Shmuel Peleg 
Address: 91904 Jerusalem, ISRAEL  
Affiliation: Institute of Computer Science, The Hebrew University of Jerusalem  
Abstract: A robust method is introduced for computing the camera motion (the ego-motion) in a static scene. The method is based on detecting a single planar surface in the scene directly from image intensities, and computing its 2D motion in the image plane. The detected 2D motion of the planar surface is used to register the images, so that the planar surface appears stationary. The resulting displacement field for the entire scene in such registered frames is affected only by the 3D translation of the camera, which is computed by finding the focus-of-expansion in the registered frames. This step is followed by computing the 3D rotation to complete the computation of the ego-motion. This 3D motion computation is based on a motion computation scheme which handles the difficult case when multiple image motions are present. This multiple motion analysis is performed together with object segmen tation by using a temporal integration approach.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> G. Adiv. </author> <title> Determining three-dimensional motion and structure from optical flow generated by several moving objects. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 7(4) </volume> <pages> 384-401, </pages> <month> July </month> <year> 1985. </year>
Reference-contexts: We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [4, 6, 7]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [6, 5]) and projective transformation (8 parameters <ref> [1, 5] </ref>). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations. <p> The 3D scene structure can then be reconstructed from the computed 3D motion parameters of the camera (using a scheme similar to that suggested in [10]). 3.1 Projected 2D Motion When the field of view is not very large and the rotation is relatively small <ref> [1] </ref>, a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by [5]: v = f c ( T X Z + y Z x 2 Y f c Z X <p> By perspective projection, this yields: 1 Z = ff + fi x + fl y where: (x; y) are image coordinates, and ff = 1 A ; fi = B f c A : Therefore, Eq. (1) can be rewritten as <ref> [1, 5] </ref>: v s = a + b x + c y + g x 2 + h xy where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y d = f c ffT
Reference: 2. <author> G. Adiv. </author> <title> Inherent ambiguities in recovering 3D motion and structure from a noisy flow field. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 11 </volume> <pages> 477-489, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: The registration of an image region which corresponds to a planar surface in the scene, and examining the motion in the registered sequence, helps to overcome the ambiguities in computing 3D motion only from the image motion of a planar surface <ref> [2, 19] </ref>. Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene.
Reference: 3. <author> Y. Aloimonos and Z. Duric. </author> <title> Active egomotion estimation: A qualitative approach. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 497-510, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 4. <author> J.R. Bergen and E.H. Adelson. </author> <title> Hierarchical, computationally efficient motion estimation algorithm. </title> <journal> J. Opt. Soc. Am. A., </journal> <volume> 4:35, </volume> <year> 1987. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [4, 6, 7] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [6, 5]) and projective transformation (8 parameters [1, 5]). Detecting the First Object.
Reference: 5. <author> J.R. Bergen, P. Anandan, K.J. Hanna, and R. Hingorani. </author> <title> Hierarchical model-based motion estimation. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 237-252, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [4, 6, 7]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, <ref> [6, 5] </ref>) and projective transformation (8 parameters [1, 5]). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [4, 6, 7]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [6, 5]) and projective transformation (8 parameters <ref> [1, 5] </ref>). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis. <p> Projected 2D Motion When the field of view is not very large and the rotation is relatively small [1], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed by <ref> [5] </ref>: v = f c ( T X Z + y Z x 2 Y f c Z X ) x Z + y T Z f c + y 2 X # where, (X; Y; Z) denote the Cartesian coordinates of the scene point projected onto (x; y), (T X <p> By perspective projection, this yields: 1 Z = ff + fi x + fl y where: (x; y) are image coordinates, and ff = 1 A ; fi = B f c A : Therefore, Eq. (1) can be rewritten as <ref> [1, 5] </ref>: v s = a + b x + c y + g x 2 + h xy where: a = f c ffT X f c Y e = Z f c fiT Y c = Z f c flT X g = Y d = f c ffT
Reference: 6. <author> J.R. Bergen, P.J. Burt, K. Hanna, R. Hingorani, P. Jeanne, and S. Peleg. </author> <title> Dynamic multiple-motion computation. In Y.A. </title> <editor> Feldman and A. Bruckstein, editors, </editor> <booktitle> Artificial Intelligence and Computer Vision: Proceedings of the Israeli Conference, </booktitle> <pages> pages 147-156. </pages> <publisher> Elsevier, </publisher> <year> 1991. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [4, 6, 7] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [6, 5]) and projective transformation (8 parameters [1, 5]). Detecting the First Object. <p> We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation [4, 6, 7]. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, <ref> [6, 5] </ref>) and projective transformation (8 parameters [1, 5]). Detecting the First Object. The motion parameters of a single object in the image plane can be recovered by applying the iterative detection method to the entire region of analysis.
Reference: 7. <author> J.R. Bergen, P.J. Burt, R. Hingorani, and S. Peleg. </author> <title> Computing two motions from three frames. </title> <booktitle> In International Conference on Computer Vision, </booktitle> <pages> pages 27-32, </pages> <address> Os-aka, Japan, </address> <month> December </month> <year> 1990. </year>
Reference-contexts: This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. We have chosen to use an iterative, multi-resolution, gradient-based approach for motion computation <ref> [4, 6, 7] </ref>. The parametric motion models used in our current implementation are: pure 2D translation (2 parameters), 2D affine transformation (6 parameters, [6, 5]) and projective transformation (8 parameters [1, 5]). Detecting the First Object.
Reference: 8. <author> P.J. Burt, R. Hingorani, and R.J. Kolczynski. </author> <title> Mechanisms for isolating component patterns in the sequential analysis of multiple motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 187-193, </pages> <address> Princeton, New Jersey, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: This can be done even in the presence of other differently moving objects in the region of analysis, and with no prior knowledge of their regions of support <ref> [8, 14] </ref>. Once a motion has been determined, we would like to identify the region having this motion. To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions.
Reference: 9. <author> R. Guissin and S. Ullman. </author> <title> Direct computation of the focus of expansion from velocity field measurements. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 146-155, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 10. <author> K. Hanna. </author> <title> Direct multi-resolution estimation of ego-motion and structure from motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 156-162, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 3, 9, 17, 18, 21, 20], or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [10, 12, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1. <p> The 3D scene structure can then be reconstructed from the computed 3D motion parameters of the camera (using a scheme similar to that suggested in <ref> [10] </ref>). 3.1 Projected 2D Motion When the field of view is not very large and the rotation is relatively small [1], a 3D motion of the camera between two image frames creates a 2D displacement (u; v) of an image point (x; y) in the image plane, which can be expressed <p> T Z was therefore set to the correct size 12cm, and the other parameters were then scaled accordingly). Once the 3D motion parameters of the camera were computed, the 3D scene structure was reconstructed using a scheme similar to that suggested in <ref> [10] </ref>. In Fig. 4, the computed inverse depth map of the scene ( 1 Z (x;y) ) is displayed. 4 Concluding Remarks A method is introduced for computing ego-motion in static scenes.
Reference: 11. <author> B.K.P. Horn. </author> <title> Relative orientation. </title> <journal> International Journal of Computer Vision, </journal> <volume> 4(1) </volume> <pages> 58-78, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 3, 9, 17, 18, 21, 20], or the correspondence of previously extracted distinguished features (points, lines, contours) <ref> [11, 22] </ref>. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
Reference: 12. <author> B.K.P. Horn and E.J. Weldon. </author> <title> Direct methods for recovering motion. </title> <journal> International Journal of Computer Vision, </journal> <volume> 2(1) </volume> <pages> 51-76, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 3, 9, 17, 18, 21, 20], or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [10, 12, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
Reference: 13. <author> M. Irani and S. Peleg. </author> <title> Image sequence enhancement using multiple motions analysis. </title> <booktitle> In IEEE Conference on Computer Vision and Pattern Recognition, </booktitle> <address> Cham-paign, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [13, 15, 14, 16] </ref>. ? This research was supported by the Israel Science Foundation. M. Irani and B.
Reference: 14. <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Detecting and tracking multiple moving objects using temporal integration. </title> <booktitle> In European Conference on Computer Vision, </booktitle> <pages> pages 282-287, </pages> <address> Santa Margarita Ligure, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [13, 15, 14, 16] </ref>. ? This research was supported by the Israel Science Foundation. M. Irani and B. <p> This can be done even in the presence of other differently moving objects in the region of analysis, and with no prior knowledge of their regions of support <ref> [8, 14] </ref>. Once a motion has been determined, we would like to identify the region having this motion. To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions.
Reference: 15. <author> M. Irani, B. Rousso, and S. Peleg. </author> <note> Computing occluding and transparent motions. To appear in International Journal of Computer Vision, </note> <year> 1993. </year>
Reference-contexts: Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [13, 15, 14, 16] </ref>. ? This research was supported by the Israel Science Foundation. M. Irani and B. <p> Temporal integration is then used to track detected objects throughout the image sequence. More details can be found in <ref> [15] </ref>. It is assumed that the projected 3D motions of the objects can be approximated by some 2D parametric transformation in the image plane. This assumption is valid when the differences in depth caused by the motions are small relative to the distances of the objects from the camera. <p> To simplify the problem, the two images are registered using the detected motion. The motion of the corresponding region is therefore canceled, and the problem becomes that of identifying the stationary regions. Detection of stationary regions is described in <ref> [15] </ref>. Tracking by Temporal Integration. Once an object has been detected, it can be tracked throughout the image sequence. This is done by using temporal integration of images registered with respect to the tracked motion. The temporally integrated image serves as a dynamic internal representation image of the tracked object. <p> The temporally integrated image serves as a dynamic internal representation image of the tracked object. Let fI (t)g denote the image sequence, and let M (t) denote the segmentation mask of the tracked object computed for frame I (t), using the segmentation method described in <ref> [15] </ref>. Initially, M (0) is the entire region of analysis.
Reference: 16. <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization. </title> <type> Technical Report 93-11, </type> <institution> Institute of Computer Science, The Hebrew University, Jerusalem, Israel, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene. More details can be found in <ref> [13, 15, 14, 16] </ref>. ? This research was supported by the Israel Science Foundation. M. Irani and B.
Reference: 17. <author> A.D. </author> <title> Jepson and D.J. Heeger. A fast subspace algorithm for recovering rigid motion. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 124-131, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 18. <author> D.T. Lawton and J.H. Rieger. </author> <title> The use of difference fields in processing sensor motion. </title> <booktitle> In DARPA IUWorkshop, </booktitle> <pages> pages 78-83, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 19. <author> H.C. Longuet-Higgins. </author> <title> Visual ambiguity of a moving plane. </title> <journal> Proceedings of The Royal Society of London B, </journal> <volume> 223 </volume> <pages> 165-175, </pages> <year> 1984. </year>
Reference-contexts: The registration of an image region which corresponds to a planar surface in the scene, and examining the motion in the registered sequence, helps to overcome the ambiguities in computing 3D motion only from the image motion of a planar surface <ref> [2, 19] </ref>. Sect. 2 describes briefly a method for detecting and tracking the differently moving objects in the sequence. Sect. 3 describes the method for computing the 3D motion of the camera (the ego-motion) in a static scene.
Reference: 20. <author> F. Meyer and P. Bouthemy. </author> <title> Estimation of time-to-collision maps from first order motion models and normal flows. </title> <booktitle> In International Conference on Pattern Recognition, </booktitle> <pages> pages 78-82, </pages> <address> The Hague, </address> <year> 1992. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 21. <author> S. Negahdaripour and S. Lee. </author> <title> Motion recovery from image sequences using first-order optical flow information. </title> <booktitle> In IEEE Workshop on Visual Motion, </booktitle> <pages> pages 132-139, </pages> <address> Princeton, NJ, </address> <month> October </month> <year> 1991. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames <ref> [1, 3, 9, 17, 18, 21, 20] </ref>, or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations.
Reference: 22. <author> F. Lustman O.D. Faugeras and G. Toscani. </author> <title> Motion and structure from motion from point and line matching. </title> <booktitle> In Proc. 1st International Conference on Computer Vision, </booktitle> <pages> pages 25-34, </pages> <address> London, </address> <year> 1987. </year>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 3, 9, 17, 18, 21, 20], or the correspondence of previously extracted distinguished features (points, lines, contours) <ref> [11, 22] </ref>. Methods for computing the ego-motion directly from image intensities were also suggested [10, 12, 23], but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
Reference: 23. <author> M.A. Taalebinezhaad. </author> <title> Direct recovery of motion and shape in the general case by fixation. </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> 14 </volume> <pages> 847-853, </pages> <month> August </month> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Previous works on 3D motion estimation use the optical or normal flow field derived between two frames [1, 3, 9, 17, 18, 21, 20], or the correspondence of previously extracted distinguished features (points, lines, contours) [11, 22]. Methods for computing the ego-motion directly from image intensities were also suggested <ref> [10, 12, 23] </ref>, but each method has its limitations. In this section we propose the following scheme in order to use the robustness of the 2D motion computation for computing 3D motion: 1.
References-found: 23

