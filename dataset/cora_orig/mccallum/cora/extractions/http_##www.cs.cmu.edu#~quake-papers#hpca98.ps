URL: http://www.cs.cmu.edu/~quake-papers/hpca98.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs.cmu.edu/project/quake/public/www/papers.html
Root-URL: 
Email: fdroh,jrs,trgg@cs.cmu.edu  
Title: Fourth Symposium on High Performance Computer Architeture, February 1998, Las Vegas, NV Architectural Implications of
Author: David O'Hallaron, Jonathan Richard Shewchuk, and Thomas Gross 
Address: Pittsburgh, PA, USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Irregular applications based on sparse matrices are at the core of many important scientific computations. Since the importance of such applications is likely to increase in the future, high-performance parallel and distributed systems must provide adequate support for such applications. We characterize a family of irregular scientific applications and derive the demands they will place on the communication systems of future parallel systems. Running time of these applications is dominated by repeated sparse matrix vector product (SMVP) operations. Using simple performance models of the SMVP, we investigate requirements for bisection bandwidth, sustained bandwidth on each processing element (PE), burst bandwidth during block transfers, and block latencies for PEs under different assumptions about sustained computational throughput. Our model indicates that block latencies are likely to be the most problematic engineering challenge for future communication networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Amza, A. Cox, S. Dwarkadas, C. Hyams, Z. Li, and W. Zwaenepoel. Treadmarks: </author> <title> Shared memory computing on networks of workstations. </title> <journal> IEEE Computer, </journal> <volume> 29(2):1828, </volume> <month> Feb. </month> <year> 1996. </year>
Reference-contexts: Blocks may be fixed-sized or variable-sized. For example, a block might be a cache line in a CC-NUMA system [11], a message in a message passing system [13], a bulk asynchronous data transfer between two PEs' memories [17], or a page in a software distributed shared memory system <ref> [1] </ref>. The transfer time for a block i of l i words is T l + l i T w , where T l is the constant block latency, T w is the constant marginal cost of transferring each additional word, and T 1 w is the burst bandwidth.
Reference: [2] <author> H. Bao, J. Bielak, O. Ghattas, D. O'Hallaron, L. Kallivokas, J. Shewchuk, and J. Xu. </author> <title> Earthquake ground motion modeling on parallel computers. </title> <booktitle> In Proc. Supercomputing '96, </booktitle> <address> Pitts-burgh, PA, </address> <month> Nov. </month> <year> 1996. </year> <note> See also www.cs.cmu.edu/~quake/. </note>
Reference-contexts: The Quake applications, described in Section 2, simulate the motion of the ground during strong earthquakes. They were developed as part of an ongoing project at Carnegie Mellon to model earthquakes in the Los Angeles basin and other alluvial valleys <ref> [2] </ref>. Running time of the Quake applications is dominated by a sparse matrix-vector product (SMVP) operation that is repeated thousands of times, and the SMVP is the only operation besides I/O that requires the transfer of data between processors. <p> communication latency will need to be a central focus of future efforts to engineer effective communication networks and software. 2 The family of Quake applications The Quake applications are unstructured finite element codes that were developed to predict ground motion in the San Fernando Valley of Southern California during earthquakes <ref> [2] </ref>. There are four Quake applications, denoted sf10, sf5, sf2, and sf1. The sf is an abbreviation for San Fernando, and the number indicates the period (in seconds) of the highest frequency wave that the simulation is able to resolve. <p> The simulations are parallelized using Archimedes, a domain-specific tool chain for finite element problems <ref> [2, 18] </ref>. To generate a simulation that will run on p PEs, Archimedes partitions the mesh into p disjoint sets of elements. Each set is called a subdomain; a one-to-one mapping is established between PEs and subdomains. <p> For example, even though the optimal throughput of strided copies on the Cray T3D is 3040 MBytes/sec [19], current implementations of sf2 achieve at best a measured and sustained bandwidth of 10 MBytes/sec using the C interface to the vendor-supplied MPI library <ref> [2] </ref>. Even more daunting is the goal of running the Quake applications at say 80% efficiency on high-speed networks of workstations.
Reference: [3] <author> S. Barnard and H. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstructured problems. </title> <type> Technical Report RNR-92-033, </type> <institution> NASA Ames Research Center, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case. Furthermore, the SMVP operations are the only operations (besides I/O) that require interprocessor communication.
Reference: [4] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, E. Santos, K. Schauser, R. Subramonian, and T. von Eicken. </author> <title> LogP: A practical model of parallel computation. </title> <journal> Communications of the ACM, </journal> <volume> 39(11):7985, </volume> <month> Nov. </month> <year> 1996. </year>
Reference-contexts: In Section 4, we use this model to explore bandwidth and latency tradeoffs in communication systems as the base microprocessors continue to improve. The models in Equations (1) and (2) are similar in some ways and different in others to the LogP model <ref> [4] </ref>. LogP is a general performance model for bulk-synchronous parallel (BSP) computations [20], where a program is viewed as a series of supersteps separated by barrier synchronizations. During a superstep, each PE performs local computation and transfers a limited number of messages.
Reference: [5] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Ar-chitetural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proc. 20th Intl. Symp. Computer Arch., </booktitle> <pages> pages 213. </pages> <address> ACM/IEEE, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It is crucial to understand communication requirements because some parts of a high-performance communication system cannot be commodity, and will therefore be expensive. In general it is important to understand the communication requirements of real applications <ref> [5, 10] </ref>, and these requirements are especially difficult to characterize for the important class of irregular scientific applications that manipulate sparse matrices. <p> For systems with a sustained computational rate of 200 MFLOPS, PEs will need about 300 MBytes/sec of sustained bandwidth and 600 MBytes/sec of burst bandwidth to run irregular codes with good efficiency. Our work is similar in spirit to that of Cypher, Ho, Kon-stantinidou, and Messina <ref> [5] </ref>, who characterize eight regular and irregular scientific applications in terms of memory, processing, communication, and I/O requirements, and build scalability models for three of the simpler regular applica tions. However, our approach is different in that our goal is depth rather than breadth. <p> To show that the generality of our model seems to extend beyond this single family of irregular applications, we observe that there is some evidence that the Quake applications are typical of unstructured finite element simulations. For example, the EXFLOW application from Cypher et al. <ref> [5] </ref> is a 3D unstructured finite element program that simulates a fluid dynamics problem on 512 PEs. Interestingly, EXFLOW has nearly identical computational properties as a similarly sized Quake application (sf2/128, which resolves a wave with a two-second period on 128 PEs). <p> Irregular finite element applications like EXFLOW and Quake have an average total communication volume similar to that of the regular applications studied earlier by Cypher et al. <ref> [5] </ref>, but they transfer more messages having a smaller average size than most of those regular applications.
Reference: [6] <author> S. Dwarkadas, A. Cox, and W. Zwaenepoel. </author> <title> An integrated compile-time/run-time software distributed shared memory system. </title> <booktitle> In Proc. Sixth Intl. Conf. on Architectural Support for Prog. Languages and Operating Systems (ASPLOS VI), </booktitle> <pages> pages 186197, </pages> <address> Boston, </address> <month> Oct. </month> <year> 1996. </year> <note> ACM. </note>
Reference-contexts: This is the case in message passing systems or DSMs that aggregate blocks <ref> [6] </ref>. The interesting point about this graph is that latency matters for the SMVP. Even if burst bandwidth is driven to infinity, observed block latency must not exceed 3 s if the code is to run at 90% efficiency.
Reference: [7] <author> J. Gilbert, G. Miller, and S.-H. Teng. </author> <title> Geometric mesh partitioning: Implementation and experiments. </title> <booktitle> In 9th International Parallel Processing Symposium, </booktitle> <pages> pages 418427, </pages> <address> Santa Barbara, CA, </address> <month> Apr. </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case. Furthermore, the SMVP operations are the only operations (besides I/O) that require interprocessor communication.
Reference: [8] <author> B. Hendrickson and R. Leland. </author> <title> The Chaco user's guide Version 2.0. </title> <type> Technical Report SAND95-2344, </type> <institution> Sandia National Laboratories, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: The geometric partitioning algorithm has provable asymptotic upper bounds on the number of shared nodes, and in practice generates partitions that are competitive with those produced by other modern partitioning algorithms <ref> [7, 3, 8] </ref>. 2.3 The Parallel SMVP The running time of the Quake applications is dominated by the execution of SMVP operations, which consume over 80% of the total running time in the sequential case. Furthermore, the SMVP operations are the only operations (besides I/O) that require interprocessor communication.
Reference: [9] <author> J. L. Hennessy and D. A. Patterson. </author> <booktitle> Computer Architecture: </booktitle>
Reference-contexts: This separation of factors is similar in spirit to the familiar CPI model for uniprocessor perfor-mance <ref> [9] </ref>.
References-found: 9

