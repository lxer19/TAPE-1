URL: ftp://ftp.cs.virginia.edu/pub/techreports/CS-91-23.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Giving CANDY to Children: User-Tailored Gesture Input Driving an Articulator-Based Speech Synthesizer  
Author: Randy Pausch and Ronald D. Williams 
Note: This work was supported in part by the National Science Foundation, the Science Applications International Corporation, the Virginia Engineering Foundation, the Virginia Center for Innovative Technology, and the United Cerebral Palsy Foundation.  
Date: October 7, 1991  
Affiliation: Computer Science  
Pubnum: Report No. TR-91-23  
Abstract-found: 0
Intro-found: 1
Reference: [Becker] <author> A. Becker, </author> <title> Design Case Study: Private Eye, Information Display, </title> <month> March, </month> <year> 1990. </year>
Reference-contexts: We expect some of our mappings to be less geometrically obvious. We are initially concentrating on target curves and surfaces that can visualized by the therapist. We have created a low-cost virtual reality type head-mounted display using Private Eye displays <ref> [Becker] </ref> and a PowerGlove TM which will eventually allow therapists to easily manipulate target surfaces [Pausch 91]. Later target curves and surfaces will be in spaces not easily visualized; in those cases, we will create the mapping entirely in software.
Reference: [Bolt] <author> R. </author> <title> Bolt, Put-That-There: Voice & Gesture at the Graphics Interface, </title> <booktitle> Computer Graphics, 14,3 (1980), </booktitle> <pages> 262-270. </pages>
Reference-contexts: Recognition of three-dimensional gestures has also been attempted, but again the main emphasis has been on converting the body motions into discrete symbols that are interpreted as commands to the system <ref> [Bolt, Buxton] </ref>. Systems have attempted to recognize static gestures for the deaf alphabet and motions for a subset of American Sign Language. All of these approaches are based on converting three-dimensional signals into a discrete stream of tokens.
Reference: [Buxton] <author> W. Buxton, E. Fiume, R. Hill, A. Lee and C. Woo, </author> <title> Continuous hand-gesture driven input, </title> <booktitle> Proceedings of Graphics Interface 83, </booktitle> <pages> 191-195. </pages>
Reference-contexts: Recognition of three-dimensional gestures has also been attempted, but again the main emphasis has been on converting the body motions into discrete symbols that are interpreted as commands to the system <ref> [Bolt, Buxton] </ref>. Systems have attempted to recognize static gestures for the deaf alphabet and motions for a subset of American Sign Language. All of these approaches are based on converting three-dimensional signals into a discrete stream of tokens.
Reference: [Childers] <author> D. G. Childers, K. Wu and D. M. Hicks, </author> <title> Factors in Voice Quality: Acoustic Features Related to Gender, </title> <booktitle> Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing, </booktitle> <address> New York, </address> <year> 1987, </year> <pages> 293-296. </pages>
Reference-contexts: The word same might be synthesized by Tongue Tip Position Top Bottom Back Forward Tongue Base Position Vocal Tract Configuration Parameters concatenating the /s/, /!EY!/, and /m/ units. Between these units, transients can occur that make the speech sound unnatural <ref> [Childers] </ref>. The simplified articulator driven system requires a time trajectory between any two sounds. This trajectory will have synthesis data along its path, so the transitions are continuous, with interpolation being used to smooth these transitions.
Reference: [Coker] <author> C. H. Coker, </author> <title> Synthesis by Rule from Articulatory Parameters, in Speech Synthesis, </title> <editor> J. L. Flanagan and L. R. Rabiner (editors), Dowden, Hutchinson & Ross, </editor> <publisher> Inc., </publisher> <address> Stroudsburg, PA, </address> <year> 1973, </year> <pages> 396-399. </pages>
Reference-contexts: Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature <ref> [Coker, Haggard, Henke] </ref>. Our new limitation is that disabled individuals must be able to produce the articular control parameters in real-time. Articulator driven synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech.
Reference: [Connolly] <author> C. Connolly, </author> <title> Compensating for Cerebral Palsy: A Tailorable Mapping from Voluntary Movement to Synthetic Speech in Real Time, </title> <institution> Master of Science Thesis, University of Virginia Department of Electrical Engineering, </institution> <address> Charlottesville, VA, </address> <month> August, </month> <year> 1990. </year>
Reference-contexts: We are currently in our third year of the project and have produced a set of interim results in both our speech synthesizer and gesture-based input. Our initial target population is children with cerebral palsy (CP), of whom there are approximately 400,000 in the United States alone <ref> [Connolly] </ref>. When adults are included in the count, the number of Americans affected by CP is approximately 700,000 [UCP]. Persons with other disabilities, such as strokes and Alzheimers disease, may also eventually benefit from our system. CP can be broadly defined as brain damage that impairs motor control.
Reference: [Dramer] <author> J. Dramer, </author> <title> The Talking Glove in Action, </title> <journal> Communications of the ACM 32,4 (April, </journal> <year> 1989), </year> <month> 515. </month>
Reference-contexts: Existing gesture recognition and speech synthesis systems are based on symbols. For example, several systems have attempted to synthesize speech using the deaf alphabet and/or a subset of American Sign Language as user input <ref> [Dramer, Loomis] </ref>. These systems attempt to understand or interpret gestures, and are commonly referred to as gesture recognition systems. Our approach is to map continuous data from one or more sensors to a set of continuous device control signals with no intermediate symbols.
Reference: [Foley] <author> J. D. Foley, </author> <title> Interfaces for Advanced Computing, </title> <publisher> Scientific American, </publisher> <month> October, </month> <year> 1987, </year> <pages> 127-135. </pages>
Reference-contexts: The pilots faceshield contains targeting crosshairs, and as the pilots helmet moves rigidly with his head, the system computes the angle of his gaze [Furness]. More detailed tracking is performed in three dimensional drawing or sculpting applications [Schmandt], and virtual reality systems, where sensors attached to gloves <ref> [Foley] </ref> provide three-dimensional signals that are mapped into motions in synthetic worlds shown on traditional or head-mounted displays. These systems perform mappings from position and orientation information, but the mappings are significantly less complicated than those we propose. The Experimental Setup Our experimental setup is shown in Figure 3.
Reference: [Furness] <author> T. A. Furness, </author> <title> Super Cockpit: Virtual Crew Systems, </title> <institution> Armstrong Aerospace Medical Research Laboratory, </institution> <year> 1988. </year>
Reference-contexts: For example, advanced military systems exist that map pilot head motion into weapon trajectories. The pilots faceshield contains targeting crosshairs, and as the pilots helmet moves rigidly with his head, the system computes the angle of his gaze <ref> [Furness] </ref>. More detailed tracking is performed in three dimensional drawing or sculpting applications [Schmandt], and virtual reality systems, where sensors attached to gloves [Foley] provide three-dimensional signals that are mapped into motions in synthetic worlds shown on traditional or head-mounted displays.
Reference: [Girson] <author> A. Girson and R. Williams, </author> <title> Articulator-Based Synthesis For Conversational Speech, </title> <booktitle> International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <month> April, </month> <year> 1990. </year>
Reference-contexts: This trajectory will have synthesis data along its path, so the transitions are continuous, with interpolation being used to smooth these transitions. The current implementation can be used to produce crude, monotone speech by using a joystick to navigate the tongue position grid <ref> [Girson] </ref>. The joystick is a temporary testing device, as the target population does not have the dexterity to control a standard joystick. Early attempts to build interfaces for the synthesizer focused on building analog input devices, such as levers to be placed against the cheek or arm.
Reference: [Haggard] <author> M. Haggard, </author> <title> Experience and Perspectives in Articulatory Synthesis, </title> <booktitle> Frontiers of Speech Communication Research, </booktitle> <address> London, UK, </address> <year> 1979, </year> <pages> 259-274. </pages>
Reference-contexts: Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature <ref> [Coker, Haggard, Henke] </ref>. Our new limitation is that disabled individuals must be able to produce the articular control parameters in real-time. Articulator driven synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech.
Reference: [Henke] <author> W. L. Henke, </author> <title> Preliminaries to Speech Synthesis Based upon an Articulatory Model, </title> <booktitle> Proceedings of the IEEE Conference on Speech Communication and Processing, </booktitle> <address> New York, </address> <year> 1967, </year> <note> 170- 182. </note>
Reference-contexts: Our articulator driven speech synthesizer produces sounds using the positions and motions of implied articulators in a simulated vocal tract. This form of speech synthesis has been discussed previously in the literature <ref> [Coker, Haggard, Henke] </ref>. Our new limitation is that disabled individuals must be able to produce the articular control parameters in real-time. Articulator driven synthesis is unnecessary and constraining in the text-to-speech environment, but this approach is directly analogous to the mechanisms of speech production used for normal human conversational speech.
Reference: [Loomis] <author> J. Loomis, H. Poizner, U. Bellugi, A. Blakemore and J. Hollerbach, </author> <title> Computer Graphic Modeling of American Sign Language, </title> <journal> Computer Graphics 17,3 (July 1983). </journal>
Reference-contexts: Existing gesture recognition and speech synthesis systems are based on symbols. For example, several systems have attempted to synthesize speech using the deaf alphabet and/or a subset of American Sign Language as user input <ref> [Dramer, Loomis] </ref>. These systems attempt to understand or interpret gestures, and are commonly referred to as gesture recognition systems. Our approach is to map continuous data from one or more sensors to a set of continuous device control signals with no intermediate symbols.
Reference: [Pausch 90] <author> R. Pausch and R. D. Williams, Tailor: </author> <title> Creating Custom User Interfaces Based on Gesture, </title> <booktitle> UIST 90: Proceedings of the Annual ACM SIGGRAPH Symposium on User Interface Software and Technology, </booktitle> <month> October, </month> <year> 1990. </year>
Reference-contexts: In this way, we can quickly turn any existing physical object into a input device. The limiting factor is that the user must have a comfortable range of motion over the surface of the object <ref> [Pausch 90] </ref>. In order to adapt for fatigue over time, our initial plan is to continue to add all user tracking points to the cumulative cloud as the user controls the device. As the cloud shifts, we will make our heuristics and genetic algorithms adjust the mapping in real time.
Reference: [Pausch 91] <author> R. Pausch, </author> <title> Virtual Reality on Five Dollars a Day, </title> <booktitle> Proceedings of the ACM SIGCHI Human Factors in Computer Systems Conference, </booktitle> <month> April, </month> <year> 1991. </year>
Reference-contexts: We are initially concentrating on target curves and surfaces that can visualized by the therapist. We have created a low-cost virtual reality type head-mounted display using Private Eye displays [Becker] and a PowerGlove TM which will eventually allow therapists to easily manipulate target surfaces <ref> [Pausch 91] </ref>. Later target curves and surfaces will be in spaces not easily visualized; in those cases, we will create the mapping entirely in software. Often the tracker motion is not best interpreted in an absolute coordinate system.
Reference: [Schmandt] <author> C. Schmandt, </author> <title> Spatial Input/Display Correspondence in a Stereoscopic Computer Graphic Work Station, </title> <journal> Computer Graphics 17,3 (July, </journal> <year> 1983), </year> <pages> 253-261. </pages>
Reference-contexts: The pilots faceshield contains targeting crosshairs, and as the pilots helmet moves rigidly with his head, the system computes the angle of his gaze [Furness]. More detailed tracking is performed in three dimensional drawing or sculpting applications <ref> [Schmandt] </ref>, and virtual reality systems, where sensors attached to gloves [Foley] provide three-dimensional signals that are mapped into motions in synthetic worlds shown on traditional or head-mounted displays. These systems perform mappings from position and orientation information, but the mappings are significantly less complicated than those we propose.
Reference: [UCP] <institution> What Everyone Should Know About Cerebral Palsy, United Cerebral Palsy Association of Westchester County, Inc., </institution> <year> 1985. </year>
Reference-contexts: Our initial target population is children with cerebral palsy (CP), of whom there are approximately 400,000 in the United States alone [Connolly]. When adults are included in the count, the number of Americans affected by CP is approximately 700,000 <ref> [UCP] </ref>. Persons with other disabilities, such as strokes and Alzheimers disease, may also eventually benefit from our system. CP can be broadly defined as brain damage that impairs motor control. Persons with CP are not necessarily mentally retarded, but they do exhibit wide variation in physical abilities.
Reference: [Zemlin] <author> W. R. Zemlin, </author> <title> Speech and Hearing Science, </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1968. </year>
Reference-contexts: The articulators used to produce speech are shown in moved from one position to another in continuous motions which give speech its continuous, uid quality. The tongue is the most important articulator. The jaw, lips, and velum are less important for shaping the speech spectrum <ref> [Zemlin] </ref>.
References-found: 18

