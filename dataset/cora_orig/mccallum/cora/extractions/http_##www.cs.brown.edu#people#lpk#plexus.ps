URL: http://www.cs.brown.edu/people/lpk/plexus.ps
Refering-URL: http://www.cs.brown.edu/people/lpk/
Root-URL: 
Email: ftld, lpk, jak, aeng@cs.brown.edu  
Title: Planning Under Time Constraints in Stochastic Domains  
Author: Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, Ann Nicholson 
Address: Providence, RI 02912  
Affiliation: Department of Computer Science Brown University,  
Abstract: We provide a method, based on the theory of Markov decision processes, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods require time at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables). By using information about the starting state, the reward function, and the transition probabilities of the domain, we restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal. Using this restricted set of states, the planner can generate more or less complete plans depending on the time it has available. Our approach employs several iterative refinement routines for solving different aspects of the decision making problem. We describe the meta-level control problem of deliberation scheduling, allocating computational resources to these routines. We provide different models corresponding to optimization problems that capture the different circumstances and computational strategies for decision making under time constraints. We consider precursor models in which all decision making is performed prior to execution and recurrent models in which decision making is performed in parallel with execution, accounting for the states observed during execution and anticipating future states. We describe experimental results for both the precursor and recurrent problems that demonstrate planning times that grow slowly as a function of domain size and compare their performance to other relevant algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: [ Adams et al., 1988 ] <author> Adams, J.; Balas, E.; and Zawack, D. </author> <year> 1988. </year> <title> The shifting bottleneck procedure for job shop scheduling. </title> <booktitle> Management Science 34(3) </booktitle> <pages> 391-401. </pages>
Reference-contexts: Our method uses Monte Carlo simulation to identify a subset of reachable states and bottleneck-centered heuristics from operations research <ref> [ Adams et al., 1988 ] </ref> to guide in incrementally refining conditional schedules.
Reference: [ Barto et al., 1993 ] <author> Barto, Andrew G.; Bradtke, Steven J.; and Singh, Satinder P. </author> <year> 1993. </year> <title> Learning to act using real-time dynamic programming. </title> <type> Technical Report 93-02, </type> <institution> Department of Computer and Information Science, University of Massachusetts, Amherst, Massachusetts. </institution> <note> To appear in Artificial Intelligence. </note>
Reference-contexts: Sutton's dyna system [ Sutton, 1990 ] explores this connection between planning and learning, and uses a version of value iteration to interleave model learning and policy updating. Barto, Bradtke, and Singh <ref> [ Barto et al., 1993 ] </ref> investigate this approach further, developing the RTDP (Real-Time Dynamic Programming) algorithm, which has the same fundamental motivations as this work. <p> RTDP The real-time dynamic programming (RTDP) algorithm was developed by Barto, Bradtke, and Singh <ref> [ Barto et al., 1993 ] </ref> . It is a generalization of Korf 's Learning-Real-Time-A* algorithm to stochastic problems. In our application of RTDP, the goal state has reward 0 and all other states have reward -1. <p> In addition to Barto, Bradtke, and Singh's real-time dynamic programming (rtdp) algorithm <ref> [ Barto et al., 1993 ] </ref> , which is described in Section 6.3.1, there are a number of other relevant algorithms. Learning real-time A* [ Korf, 1988 ] is the deterministic special case of real-time dynamic programming.
Reference: [ Bellman, 1957 ] <author> Bellman, Richard 1957. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: The research presented in this paper is based on sequential decision making, stochastic control, and reinforcement learning; we review this work in Section 9. 2 Markov Decision Models Following the work on Markov decision processes <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> , we model the entire environment as a stochastic automaton. Let S be the finite set of world states; we assume that they can be reliably identified by the agent. <p> Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm [ Howard, 1960 ] or the value iteration algorithm <ref> [ Bellman, 1957 ] </ref> . We use the policy iteration algorithm 5 because it is guaranteed to converge in a finite number of steps|generally a small number of steps in the domains that we have experimented with|and thus simplifies debugging our computational experiments. <p> metering gate assignment problem, evening rush hour arrival times for commuter aircraft are notoriously unreliable and so it often does not pay to expend considerable computational resources refining a detailed schedule for such periods. 9 Related Work Our primary interest is in applying the sequential decision making techniques of Bellman <ref> [ Bellman, 1957 ] </ref> and Howard [ Howard, 1960 ] in time-critical applications. Our initial motivation for the methods discussed here came from the work of Drummond and Bresina [ Drummond and Bresina, 1990 ] .
Reference: [ Bertsekas, 1987 ] <author> Bertsekas, Dimitri P. </author> <year> 1987. </year> <title> Dynamic Programming. </title> <publisher> Prentice-Hall, </publisher> <address> En-glewood Cliffs, N.J. </address>
Reference-contexts: The research presented in this paper is based on sequential decision making, stochastic control, and reinforcement learning; we review this work in Section 9. 2 Markov Decision Models Following the work on Markov decision processes <ref> [ Bellman, 1957, Bertsekas, 1987 ] </ref> , we model the entire environment as a stochastic automaton. Let S be the finite set of world states; we assume that they can be reliably identified by the agent.
Reference: [ Boddy, 1991 ] <author> Boddy, </author> <title> Mark 1991. Anytime problem solving using dynamic programming. </title> <booktitle> In Proceedings AAAI-91. AAAI. </booktitle> <pages> 738-743. </pages>
Reference-contexts: For an overview of resource-bounded decision making methods, see Chapter 8 of the text by Dean and Wellman [ Dean and Wellman, 1991 ] . Boddy <ref> [ Boddy, 1991 ] </ref> describes solutions to related problems involving dynamic programming.
Reference: [ Cassandra et al., 1994 ] <author> Cassandra, Anthony R.; Kaelbling, Leslie Pack; and Littman, Michael L. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <address> Seattle, WA. </address>
Reference-contexts: Most importantly, if an agent adopts the optimal policy for the belief mdp, the resulting behavior will be optimal for the partially observable process. The remaining difficulty is that the belief process is continuous and therefore not susceptible to standard methods for finding policies. Cassandra, Kaelbling, and Littman <ref> [ Cassandra et al., 1994 ] </ref> review existing algorithms from the operations research literature and sketch a new one that is more computationally tractable. In addition, they show that in many cases approximately-optimal finite-state control automata can be extracted from the resulting policies.
Reference: [ Christiansen and Goldberg, 1990 ] <author> Christiansen, Alan and Goldberg, </author> <title> Ken 1990. Robotic manipulation planning with stochastic actions. In DARPA Workshop on Innovative Approaches to Planning, Scheduling and Control. </title> <address> San Diego,California. </address>
Reference-contexts: Hansson and Mayer's Bayesian Problem Solver (BPS) [ Hansson and Mayer, 1989 ] supports general state-space search with decision-theoretic control of inference; it may be that BPS could be used as the basis for envelope extension thus providing more fine-grained decision-theoretic control. Christiansen and Goldberg <ref> [ Christiansen and Goldberg, 1990 ] </ref> and Kushmerick, Hanks, and Weld [ Kushmerick et al., 1993 ] also address the problem of planning in stochastic domains.
Reference: [ Davis et al., 1991 ] <author> Davis, Thomas Jr.; Erzberger, H.; Green, S. M.; and Nedell, W. </author> <year> 1991. </year> <title> Design and evaluation of an air traffic control final approach spacing tool. </title> <journal> AIAA Journal of Guidance, Control, and Dynamics 14 </journal> <pages> 848-854. 49 </pages>
Reference-contexts: For example, there is ongoing work at NASA in collaboration with the FAA on building systems for the automated management and control of U.S. airports <ref> [ Davis et al., 1991 ] </ref> . Such systems face daunting combinatorial problems and the need to respond to unforeseen situations (e.g., snowstorms and fog) in a timely manner. One component of such systems involves the assignment of metering gates for unloading and loading planes.
Reference: [ Dean and Boddy, 1988 ] <author> Dean, Thomas and Boddy, </author> <title> Mark 1988. An analysis of time--dependent planning. </title> <booktitle> In Proceedings AAAI-88. AAAI. </booktitle> <pages> 49-54. </pages>
Reference-contexts: For states outside of the envelope, the policy is defined by a set of reflexes that implement some default behavior for the agent. The algorithm is implemented as an anytime algorithm <ref> [ Dean and Boddy, 1988 ] </ref> , one that can be interrupted at any point during execution to return an answer whose value, at least in certain classes of stochastic processes, improves in expectation as a function of the computation time. <p> 1990 ] and work by Whitehead and Ballard [ Whitehead and Ballard, 1989 ] studied learning by interleaving actions in the real world with "simulated" actions that peform value updates on the policy. 9.3 Time-Critical Planning Methods The approach described in this paper represents a particular instance of time-dependent planning <ref> [ Dean and Boddy, 1988 ] </ref> and borrows from, among others, Horvitz' [ Horvitz, 1988 ] approach to flexible computation. For an overview of resource-bounded decision making methods, see Chapter 8 of the text by Dean and Wellman [ Dean and Wellman, 1991 ] .
Reference: [ Dean and Kanazawa, 1989 ] <author> Dean, Thomas and Kanazawa, </author> <title> Keiji 1989. A model for reasoning about persistence and causation. </title> <booktitle> Computational Intelligence 5(3) </booktitle> <pages> 142-150. </pages>
Reference-contexts: In order for very large domains to be amenable to planning, they must have internal regularities that allow them to be represented more compactly. Markov chains and the Bayesian network formalism [ Pearl, 1988 ] have been shown to be equivalent <ref> [ Dean and Kanazawa, 1989, Nunez, 1989 ] </ref> . The following simplified version of the Bayesian network formalism is well suited to specifying stochastic state transition and reward models for our purposes.
Reference: [ Dean and Wellman, 1991 ] <author> Dean, Thomas and Wellman, </author> <title> Michael 1991. Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: For an overview of resource-bounded decision making methods, see Chapter 8 of the text by Dean and Wellman <ref> [ Dean and Wellman, 1991 ] </ref> . Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming.
Reference: [ Drummond and Bresina, 1990 ] <author> Drummond, Mark and Bresina, </author> <title> John 1990. Anytime synthetic projection: Maximizing the probability of goal satisfaction. </title> <booktitle> In Proceedings AAAI-90. AAAI. </booktitle> <pages> 138-144. </pages>
Reference-contexts: In more complicated models, called recurrent-deliberation models, we assume that the agent periodically replans and executes the resulting policy in parallel with planning the next policy. Our approach is motivated by the intuitively appealing work of Drummond and Bresina on `anytime synthetic projection' <ref> [ Drummond and Bresina, 1990 ] </ref> . <p> Our initial motivation for the methods discussed here came from the work of Drummond and Bresina <ref> [ Drummond and Bresina, 1990 ] </ref> . <p> In the following three subsections, we describe the connection to the work of Drummond and Bresina, the relationship to work in the area of reinforcement learning and adaptive control, and discuss other related work in time-critical decision making. 9.1 Anytime Synthetic Projection Drummond and Bresina's anytime synthetic projection algorithm <ref> [ Drummond and Bresina, 1990 ] </ref> incrementally constructs conditional plans for stochastic domains. Their work provided the initial motivation for our research.
Reference: [ Drummond et al., 1994 ] <author> Drummond, M.; Swanson, K.; and Bresina, J. </author> <year> 1994. </year> <title> Robust scheduling and execution for automatic telescopes. In Zweben, </title> <editor> M. and Fox, M., editors 1994, </editor> <title> Knowledge-Based Scheduling. </title> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, California. </address>
Reference-contexts: Our method uses Monte Carlo simulation to identify a subset of reachable states and bottleneck-centered heuristics from operations research [ Adams et al., 1988 ] to guide in incrementally refining conditional schedules. In this work, we have built on the work of Drummond et al. <ref> [ Drummond et al., 1994 ] </ref> who apply a variant of anytime synthetic projection to the problem of scheduling experiments on automatic telescopes and the work of Muscettola and Smith [ Muscettola and Smith, 1987 ] who have demonstrated how to apply bottleneck-centered heuristics in handling uncertainty in arrival and departure
Reference: [ Drummond, 1989 ] <author> Drummond, </author> <title> Mark 1989. Situated control rules. </title> <editor> In Brachman, Ronald J.; Levesque, Hector J.; and Reiter, Raymond, editors 1989, </editor> <booktitle> Proceedings of the First International Conference on Principles of Knowledge Representation and Reasoning. </booktitle> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, California. </address>
Reference-contexts: The search space is a graph in which each node corresponds to a possible situation and each arc to an applicable operator. The plan that results from synthetic projection is represented as a set of situated control rules <ref> [ Drummond, 1989 ] </ref> which constitute a partial policy by mapping situations to operators corresponding to actions. The resulting conditional plan is similar to the triangle tables of Fikes et al. [ Fikes et al., 1972 ] . 45 Simplifying somewhat, the synthetic projection involves two basic subroutines.
Reference: [ Fikes and Nilsson, 1971 ] <author> Fikes, Richard E. and Nilsson, Nils J. </author> <year> 1971. </year> <title> Strips: A new approach to the application of theorem proving to problem solving. </title> <booktitle> Artificial Intelligence 2 </booktitle> <pages> 189-208. </pages> <note> Reprinted in Readings in Planning, </note> <editor> J. Allen, J. Hendler, and A. Tate, eds., </editor> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: In nondeterministic worlds, planners must address the question of what to do when things do not go as expected. The method of triangle tables <ref> [ Fikes and Nilsson, 1971 ] </ref> made plans that could be executed robustly in any circumstance along the nominal trajectory of world states, allowing for certain classes of failures and serendipitous events.
Reference: [ Fikes et al., 1972 ] <author> Fikes, Richard E.; Hart, Peter E.; and Nilsson, Nils J. </author> <year> 1972. </year> <title> Learning and executing generalized robot plans. </title> <booktitle> Artificial Intelligence 3 </booktitle> <pages> 251-288. </pages>
Reference-contexts: The plan that results from synthetic projection is represented as a set of situated control rules [ Drummond, 1989 ] which constitute a partial policy by mapping situations to operators corresponding to actions. The resulting conditional plan is similar to the triangle tables of Fikes et al. <ref> [ Fikes et al., 1972 ] </ref> . 45 Simplifying somewhat, the synthetic projection involves two basic subroutines.
Reference: [ Greenwald and Dean, 1994a ] <author> Greenwald, Lloyd and Dean, </author> <title> Thomas 1994a. Anticipating computational demands when solving time-critical decision-making problems. </title> <booktitle> In Workshop on the Algorithmic Foundations of Robotics. </booktitle>
Reference-contexts: The combinatorics in such problems is quite daunting and some of our recent work involves anticipating computational demands off-line for processes that exhibit some degree of regularity in order to support online deliberation scheduling <ref> [ Greenwald and Dean, 1994a, Greenwald and Dean, 1994c ] </ref> .
Reference: [ Greenwald and Dean, 1994b ] <author> Greenwald, Lloyd and Dean, </author> <title> Thomas 1994b. Monte carlo simulation and bottleneck-centered heuristics for time-critical scheduling in stochastic domains. </title> <booktitle> In ARPI Planning Initiative Workshop. </booktitle> <pages> 50 </pages>
Reference-contexts: Large action spaces make it impractical to apply standard policy improvement algorithms that require quantifying over the entire action space. A variant of the method described in this paper can be applied to scheduling problems such as metering gate assignment <ref> [ Greenwald and Dean, 1994b ] </ref> . This variant method involves approximation algorithms for both value determination (estimating the expected value of a given conditional schedule) and policy improvement (refining a given conditional schedule to improve its expected value).
Reference: [ Greenwald and Dean, 1994c ] <author> Greenwald, Lloyd and Dean, </author> <title> Thomas 1994c. Solving time--critical decision-making problems with predictable computational demands. </title> <booktitle> In Second International Conference on AI Planning Systems. </booktitle>
Reference-contexts: The combinatorics in such problems is quite daunting and some of our recent work involves anticipating computational demands off-line for processes that exhibit some degree of regularity in order to support online deliberation scheduling <ref> [ Greenwald and Dean, 1994a, Greenwald and Dean, 1994c ] </ref> .
Reference: [ Hansson and Mayer, 1989 ] <author> Hansson, Othar and Mayer, </author> <title> Andrew 1989. Heuristic search as evidential reasoning. </title> <booktitle> In Proceedings of the Fifth Workshop on Uncertainty in AI. </booktitle> <pages> 152-161. </pages>
Reference-contexts: For an overview of resource-bounded decision making methods, see Chapter 8 of the text by Dean and Wellman [ Dean and Wellman, 1991 ] . Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming. Hansson and Mayer's Bayesian Problem Solver (BPS) <ref> [ Hansson and Mayer, 1989 ] </ref> supports general state-space search with decision-theoretic control of inference; it may be that BPS could be used as the basis for envelope extension thus providing more fine-grained decision-theoretic control.
Reference: [ Horvitz, 1988 ] <author> Horvitz, Eric J. </author> <year> 1988. </year> <title> Reasoning under varying and uncertain resource constraints. </title> <booktitle> In Proceedings AAAI-88. AAAI. </booktitle> <pages> 111-116. </pages>
Reference-contexts: 1989 ] studied learning by interleaving actions in the real world with "simulated" actions that peform value updates on the policy. 9.3 Time-Critical Planning Methods The approach described in this paper represents a particular instance of time-dependent planning [ Dean and Boddy, 1988 ] and borrows from, among others, Horvitz' <ref> [ Horvitz, 1988 ] </ref> approach to flexible computation. For an overview of resource-bounded decision making methods, see Chapter 8 of the text by Dean and Wellman [ Dean and Wellman, 1991 ] . Boddy [ Boddy, 1991 ] describes solutions to related problems involving dynamic programming.
Reference: [ Howard, 1960 ] <author> Howard, Ronald A. </author> <year> 1960. </year> <title> Dynamic Programming and Markov Processes. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts. </address>
Reference-contexts: Given a state-transition model, a reward function, and a value for fl, it is possible to compute the optimal policy using either the policy iteration algorithm <ref> [ Howard, 1960 ] </ref> or the value iteration algorithm [ Bellman, 1957 ] . <p> hour arrival times for commuter aircraft are notoriously unreliable and so it often does not pay to expend considerable computational resources refining a detailed schedule for such periods. 9 Related Work Our primary interest is in applying the sequential decision making techniques of Bellman [ Bellman, 1957 ] and Howard <ref> [ Howard, 1960 ] </ref> in time-critical applications. Our initial motivation for the methods discussed here came from the work of Drummond and Bresina [ Drummond and Bresina, 1990 ] .
Reference: [ Kabanza, 1990 ] <author> Kabanza, F. </author> <year> 1990. </year> <title> Synthesis of reactive plans for multi-path environments. </title> <booktitle> In Proceedings AAAI-90. AAAI. </booktitle> <pages> 164-169. </pages>
Reference-contexts: Christiansen and Goldberg [ Christiansen and Goldberg, 1990 ] and Kushmerick, Hanks, and Weld [ Kushmerick et al., 1993 ] also address the problem of planning in stochastic domains. Kabanza <ref> [ Kabanza, 1990 ] </ref> describes a method for planning in nondeterministic environments that relies on exploring a small portion of the set of all possible action sequences and representing the resulting restricted automaton using a propositional branching time logic.
Reference: [ Kemeny and Snell, 1960 ] <author> Kemeny, J. G. and Snell, J. L. </author> <year> 1960. </year> <title> Finite Markov Chains. </title> <address> D. </address> <publisher> Van Nostrand, </publisher> <address> New York. </address>
Reference-contexts: A policy is a mapping from S to A, specifying an action to be taken in each situation. An environment combined with a policy for choosing actions in that environment yields a Markov chain <ref> [ Kemeny and Snell, 1960 ] </ref> . A reward function is a mapping from S to R, specifying the instantaneous reward that the agent derives from being in each state.
Reference: [ Kirman, Forthcoming ] <author> Kirman, Jak oming. </author> <title> What Makes Planning in Stochastic Domains Hard? Ph.D. </title> <type> Dissertation, </type> <institution> Brown University. </institution>
Reference-contexts: A more detailed investigation and empirical analysis of the domain characteristics for robot-navigation and other domains including air-traffic scheduling and traffic-light control is currently being undertaken <ref> [ Kirman, Forthcoming ] </ref> . We have outlined a number of extensions to the basic planning approach which we are currently exploring. These include relaxing the assumptions of complete observability, using compositional representations and extending the approach to work with multiple levels of abstraction and larger action spaces.
Reference: [ Korf, 1988 ] <author> Korf, Richard 1988. </author> <title> Real-time heuristic search: New results. </title> <booktitle> In Proceedings AAAI-88. AAAI. </booktitle> <pages> 139-144. </pages>
Reference-contexts: In addition to Barto, Bradtke, and Singh's real-time dynamic programming (rtdp) algorithm [ Barto et al., 1993 ] , which is described in Section 6.3.1, there are a number of other relevant algorithms. Learning real-time A* <ref> [ Korf, 1988 ] </ref> is the deterministic special case of real-time dynamic programming.
Reference: [ Kushmerick et al., 1993 ] <author> Kushmerick, Nicholas; Hanks, Steve; and Weld, </author> <title> Daniel 1993. An algorithm for probabilistic planning. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: It also might be appropriate to use Kushmerick et al.'s method for generating plausible initial policies <ref> [ Kushmerick et al., 1993 ] </ref> . 4.2.3 Envelope Alteration Envelope alteration can be classified in terms of three basic operations on the envelope: trajectory planning, envelope extension, and envelope pruning. <p> Christiansen and Goldberg [ Christiansen and Goldberg, 1990 ] and Kushmerick, Hanks, and Weld <ref> [ Kushmerick et al., 1993 ] </ref> also address the problem of planning in stochastic domains.
Reference: [ Littman, 1994 ] <author> Littman, Michael L. </author> <year> 1994. </year> <title> Memoryless policies: Theoretical limitations and practical results. </title> <booktitle> In From Animals to Animats 3, </booktitle> <address> Brighton, UK. </address>
Reference-contexts: The result is that even an 41 optimal policy of this form can have arbitrarily poor performance. Further, determining whether a policy with a given level of performance exists for this type of non-Markov decision problem is NP-hard <ref> [ Littman, 1994 ] </ref> . Instead, we introduce a kind of internal state for the agent. A belief state is a discrete probability distribution over the set of world states, representing for each state the agent's belief that it is currently occupying that state.
Reference: [ Lovejoy, 1991 ] <author> Lovejoy, William S. </author> <year> 1991. </year> <title> A survey of algorithmic methods for partially observed markov decision processes. </title> <journal> Annals of Operations Research 28(1) </journal> <pages> 47-65. </pages>
Reference-contexts: include relaxing the assumptions of complete observabiltiy, exploring compositional representations of the state space and state transition model, and dealing with domains in which the number of actions is very large. 8.1 Partial Observability We are exploring an extension of the mdp model called partially observable Markov decision processes (pomdps) <ref> [ Lovejoy, 1991, Monahan, 1982 ] </ref> , which, like the mdp model, was developed within the context of operations research.
Reference: [ Monahan, 1982 ] <author> Monahan, George E. </author> <year> 1982. </year> <title> A survey of partially observable markov decision processes: Theory, models, and algorithms. </title> <booktitle> Management Science 28(1) </booktitle> <pages> 1-16. 51 </pages>
Reference-contexts: include relaxing the assumptions of complete observabiltiy, exploring compositional representations of the state space and state transition model, and dealing with domains in which the number of actions is very large. 8.1 Partial Observability We are exploring an extension of the mdp model called partially observable Markov decision processes (pomdps) <ref> [ Lovejoy, 1991, Monahan, 1982 ] </ref> , which, like the mdp model, was developed within the context of operations research.
Reference: [ Moore and Atkeson, 1993 ] <author> Moore, Andrew W. and Atkeson, Christopher G. </author> <year> 1993. </year> <title> Memory-based reinforcement learning: Efficient computation with prioritized sweeping. </title> <booktitle> In Advances in Neural Information Processing 5, </booktitle> <address> San Mateo, California. </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: Learning real-time A* [ Korf, 1988 ] is the deterministic special case of real-time dynamic programming. Value iteration can be made much more efficient by performing updates in an intelligent order; this idea is pursued in Moore and Atkeson's prioritized sweeping algorithm <ref> [ Moore and Atkeson, 1993 ] </ref> and in Peng and Williams' Dyna-Queue algorithm [ Peng and Williams, 1993 ] . These methods are not as directed as rtdp, however, because they do not take into account information about what the starting state is.
Reference: [ Muscettola and Smith, 1987 ] <author> Muscettola, Nicola and Smith, </author> <title> Steve 1987. A probabilistic framework for resource-constrained multiagent planning. </title> <booktitle> In Proceedings IJCAI 10. </booktitle> <address> IJ-CAII. </address>
Reference-contexts: In this work, we have built on the work of Drummond et al. [ Drummond et al., 1994 ] who apply a variant of anytime synthetic projection to the problem of scheduling experiments on automatic telescopes and the work of Muscettola and Smith <ref> [ Muscettola and Smith, 1987 ] </ref> who have demonstrated how to apply bottleneck-centered heuristics in handling uncertainty in arrival and departure times. In our case, we employ an explicit model representing the dynamics governing the ar 44 rival and departure of planes.
Reference: [ Nicholson and Kaelbling, 1994 ] <author> Nicholson, Ann and Kaelbling, </author> <title> Leslie Pack 1994. Toward approximate planning in very large stochastic domains. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Decision Theoretic Planning, </booktitle> <address> Stanford, California. </address>
Reference-contexts: Even with a compact representation of the dynamics of the entire world, we will rarely want or need to work with the whole model. Given different goals, different time constraints, or different current world states, we might want to take very different views of the world. Nicholson and Kaelbling <ref> [ Nicholson and Kaelbling, 1994 ] </ref> investigate the construction of different world views by specifying only a subset of the possible variables in the complete world model. In some cases, these abstract views capture all of the world dynamics relevant to the problem at hand.
Reference: [ Nilsson, 1986 ] <author> Nilsson, </author> <title> Nils 1986. Probabilistic logic. </title> <booktitle> Artificial Intelligence 28 </booktitle> <pages> 71-88. </pages>
Reference-contexts: Thiebaux et al. [ Thiebaux et al., 1994 ] attempt to extend our work and that of Drummond and Bresina to use probabilistic logic <ref> [ Nilsson, 1986 ] </ref> for 47 planning under uncertainty.
Reference: [ Nunez, 1989 ] <author> Nunez, </author> <title> Linda Mensinger 1989. On the relationship between temporal bayes networks and Markov chains. </title> <type> Masters Thesis, </type> <institution> Brown University. </institution>
Reference-contexts: In order for very large domains to be amenable to planning, they must have internal regularities that allow them to be represented more compactly. Markov chains and the Bayesian network formalism [ Pearl, 1988 ] have been shown to be equivalent <ref> [ Dean and Kanazawa, 1989, Nunez, 1989 ] </ref> . The following simplified version of the Bayesian network formalism is well suited to specifying stochastic state transition and reward models for our purposes.
Reference: [ Pearl, 1988 ] <author> Pearl, </author> <title> Judea 1988. Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California. </address>
Reference-contexts: This approach works well in small state spaces, but can very quickly become intractable. In order for very large domains to be amenable to planning, they must have internal regularities that allow them to be represented more compactly. Markov chains and the Bayesian network formalism <ref> [ Pearl, 1988 ] </ref> have been shown to be equivalent [ Dean and Kanazawa, 1989, Nunez, 1989 ] . The following simplified version of the Bayesian network formalism is well suited to specifying stochastic state transition and reward models for our purposes.
Reference: [ Peng and Williams, 1993 ] <author> Peng, Jing and Williams, Ronald J. </author> <year> 1993. </year> <title> Efficient learning and planning within the dyna framework. </title> <booktitle> Adaptive Behavior 1(4) </booktitle> <pages> 437-454. </pages>
Reference-contexts: Value iteration can be made much more efficient by performing updates in an intelligent order; this idea is pursued in Moore and Atkeson's prioritized sweeping algorithm [ Moore and Atkeson, 1993 ] and in Peng and Williams' Dyna-Queue algorithm <ref> [ Peng and Williams, 1993 ] </ref> . These methods are not as directed as rtdp, however, because they do not take into account information about what the starting state is.
Reference: [ Schoppers, 1987 ] <author> Schoppers, Marcel J. </author> <year> 1987. </year> <title> Universal plans for reactive robots in unpredictable environments. </title> <booktitle> In Proceedings IJCAI 10. IJCAII. </booktitle> <pages> 1039-1046. </pages>
Reference-contexts: Many systems (sipe, for example [ Wilkins, 1988 ] ) can monitor for plan "failures" and initiate replanning. Replanning is often too slow to be useful in time-critical domains, however. Schoppers, in his universal plans <ref> [ Schoppers, 1987 ] </ref> , gives a method for generating a reaction for every possible situation that could transpire during plan execution; these plans are robust and fast to execute, but can be very large and expensive to generate. There is an inherent contradiction in all of these approaches.
Reference: [ Shachter, 1986 ] <author> Shachter, Ross D. </author> <year> 1986. </year> <title> Evaluating influence diagrams. </title> <journal> Operations Research 34(6) </journal> <pages> 871-882. </pages>
Reference-contexts: The value of this node depends deterministically on the values of the nodes to which it is connected; it plays the role of a value node in an influence diagram ( <ref> [ Shachter, 1986 ] </ref> ). Although it would be possible to compute the entire state transition function from the Bayesian network representation and store it in a table, it will be intractable to do so for domains of the size we are interested.
Reference: [ Smallwood and Sondik, 1973 ] <author> Smallwood, Richard D. and Sondik, Edward J. </author> <year> 1973. </year> <title> The optimal control of partially observable markov processes over a finite horizon. </title> <journal> Operations Research 21 </journal> <pages> 1071-1088. </pages>
Reference-contexts: The key to finding optimal policies in the partially observable case is that the problem can be cast as a completely observable continuous-space mdp in which the states are the belief states. The belief mdp is Markov <ref> [ Smallwood and Sondik, 1973 ] </ref> and having information about previous belief states cannot improve the choice of action. Most importantly, if an agent adopts the optimal policy for the belief mdp, the resulting behavior will be optimal for the partially observable process.
Reference: [ Sutton, 1990 ] <author> Sutton, Richard S. </author> <year> 1990. </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <address> Austin, Texas. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 52 </pages>
Reference-contexts: The mdp process model has been the basis of a large amount of work in the reinforcement 3 learning community. In the context of model-based learning, in which an MDP model of the world is learned, it is necessary to generate a policy from a model. Sutton's dyna system <ref> [ Sutton, 1990 ] </ref> explores this connection between planning and learning, and uses a version of value iteration to interleave model learning and policy updating. <p> These methods are not as directed as rtdp, however, because they do not take into account information about what the starting state is. Finally, Sutton's dyna system <ref> [ Sutton, 1990 ] </ref> and work by Whitehead and Ballard [ Whitehead and Ballard, 1989 ] studied learning by interleaving actions in the real world with "simulated" actions that peform value updates on the policy. 9.3 Time-Critical Planning Methods The approach described in this paper represents a particular instance of time-dependent
Reference: [ Thiebaux et al., 1994 ] <author> Thiebaux, Sylvie; Hertzberg, Joachim; Shoaf, William; and Schnei--der, </author> <title> Moti 1994. A stochastic model of actions and plans for for anytime planning under uncertainty. </title> <editor> In Sandewall, E. and Backstrom, C., editors 1994, </editor> <booktitle> Current Trends in AI Planning. </booktitle> <publisher> IOS Press, Amesterdam. </publisher>
Reference-contexts: Kabanza's approach does not make use of any probabilistic information regarding state transitions and makes no attempt to construct even an approximately optimal plan for any measure of performance. Thiebaux et al. <ref> [ Thiebaux et al., 1994 ] </ref> attempt to extend our work and that of Drummond and Bresina to use probabilistic logic [ Nilsson, 1986 ] for 47 planning under uncertainty.
Reference: [ Whitehead and Ballard, 1989 ] <author> Whitehead, Steven D. and Ballard, Dana H. </author> <year> 1989. </year> <title> A role for anticipation in reactive systems that learn. </title> <booktitle> In Proceedings of the Sixth International Workshop on Machine Learning, </booktitle> <address> Ithaca, New York. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 354-357. </pages>
Reference-contexts: These methods are not as directed as rtdp, however, because they do not take into account information about what the starting state is. Finally, Sutton's dyna system [ Sutton, 1990 ] and work by Whitehead and Ballard <ref> [ Whitehead and Ballard, 1989 ] </ref> studied learning by interleaving actions in the real world with "simulated" actions that peform value updates on the policy. 9.3 Time-Critical Planning Methods The approach described in this paper represents a particular instance of time-dependent planning [ Dean and Boddy, 1988 ] and borrows from,
Reference: [ Wilkins, 1988 ] <author> Wilkins, David E. </author> <year> 1988. </year> <title> Practical Planning: Extending the Classical AI Planning Paradigm. </title> <publisher> Morgan-Kaufmann, </publisher> <address> Los Altos, California. </address> <month> 53 </month>
Reference-contexts: It is often the case, however, that an execution error will move the world to a situation that has not been previously considered by the planner. Many systems (sipe, for example <ref> [ Wilkins, 1988 ] </ref> ) can monitor for plan "failures" and initiate replanning. Replanning is often too slow to be useful in time-critical domains, however.
References-found: 44

