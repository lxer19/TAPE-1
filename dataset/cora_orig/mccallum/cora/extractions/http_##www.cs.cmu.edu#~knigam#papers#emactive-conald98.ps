URL: http://www.cs.cmu.edu/~knigam/papers/emactive-conald98.ps
Refering-URL: http://www.cs.cmu.edu/~knigam/
Root-URL: http://www.cs.cmu.edu/~jr6b
Email: knigam@cs.cmu.edu  mccallum@justresearch.com  
Title: Pool-Based Active Learning for Text Classification  
Author: Kamal Nigam Andrew McCallum zy 
Address: Pittsburgh, PA 15213  4616 Henry Street Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  Just Research  
Abstract: This paper shows how a text classifier's need for labeled training documents can be reduced by employing a large pool of unlabeled documents. We modify the Query-by-Committee (QBC) method of active learning to use the unlabeled pool by explicitly estimating document density when selecting examples for labeling. Then active learning is combined with Expectation-Maximization in order to "fill in" the class labels of those documents that remain unlabeled. Experimental results show that the improvements to active learning reduce the need for labelings by one-third over previous QBC approaches, and that the combination of EM and active learning requires only slightly more than half as many labeled training examples to achieve the same accuracy as either EM or active learning alone. 
Abstract-found: 1
Intro-found: 1
Reference: <author> D. Cohn, Z. Ghahramani, and M. Jordan. </author> <title> Active learning with statistical models. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 129-145, </pages> <year> 1996. </year>
Reference: <author> D. Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <booktitle> In NIPS 6, </booktitle> <year> 1994. </year>
Reference: <author> I. Dagan and S. Engelson. </author> <title> Committee-based sampling for training probabilistic classifiers. </title> <booktitle> In ICML-95, </booktitle> <year> 1995. </year>
Reference: <author> A. P. Dempster, N. M. Laird, and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM. algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference: <author> P. Domingos and M. Pazzani. </author> <title> Beyond independence: Conditions for the optimality of the simple Bayesian classifier. </title> <journal> Machine Learning, </journal> <volume> 29 </volume> <pages> 103-130, </pages> <year> 1997. </year>
Reference: <author> Y. Freund, H. Seung, E. Shamir, and N. Tishby. </author> <title> Selective sampling using the query by committee algorithm. </title> <journal> Machine Learning, </journal> <volume> 28 </volume> <pages> 133-168, </pages> <year> 1997. </year>
Reference: <author> Z. Ghahramani and M. Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In NIPS 6, </booktitle> <year> 1994. </year>
Reference: <author> T. Joachims. </author> <title> A probabilistic analysis of the Roc-chio algorithm with TFIDF for text categorization. </title> <booktitle> In ICML-97, </booktitle> <year> 1997. </year>
Reference: <author> D. Lewis and W. Gale. </author> <title> A sequential algorithm for training text classifiers. </title> <booktitle> In Proceedings of ACM SIGIR, </booktitle> <year> 1994. </year>
Reference: <author> D. D. Lewis. </author> <title> A sequential algorithm for training text classifiers: Corrigendum and additional data. </title> <journal> SIGIR Forum, </journal> <volume> 29(2) </volume> <pages> 13-19, </pages> <year> 1995. </year>
Reference: <author> R. Liere and P. Tadepalli. </author> <title> Active learning with committees for text categorization. </title> <booktitle> In AAAI-97, </booktitle> <year> 1997. </year>
Reference: <author> D. J. Miller and H. S. Uyar. </author> <title> A mixture of experts classifier with learning based on both labelled and unlabelled data. </title> <booktitle> In NIPS 9, </booktitle> <year> 1997. </year>
Reference: <author> K. Nigam, A. McCallum, S. Thrun, and T. Mitchell. </author> <title> Learning to classify text from labeled and unlabeled documents. </title> <booktitle> In AAAI-98, </booktitle> <year> 1998. </year>
Reference: <author> F. Pereira, N. Tishby, and L. Lee. </author> <title> Distributional clustering of English words. </title> <booktitle> In Proc. of the 31st ACL, </booktitle> <year> 1993. </year>
Reference: <author> B. Shahshahani and D. Landgrebe. </author> <title> The effect of unlabeled samples in reducing the small sample size problem and mitigating the Hughes phenomenon. </title> <journal> IEEE Trans. on Geoscience and Remote Sensing, </journal> <volume> 32(5) </volume> <pages> 1087-1095, </pages> <month> Sept </month> <year> 1994. </year>
References-found: 15

