URL: http://www.enee.umd.edu/medlab/filter/papers/survey.ps
Refering-URL: http://www.cs.jhu.edu/~weiss/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: christos@cs.umd.edu oard@eng.umd.edu  
Title: A Survey of Information Retrieval and Filtering Methods  
Author: Christos Faloutsos and Douglas Oard 
Address: College Park, MD 20742  
Affiliation: University of Maryland  
Abstract: We survey the major techniques for information retrieval. In the first part, we provide an overview of the traditional ones (full text scanning, inversion, signature files and clustering). In the second part we discuss attempts to include semantic information (natural language processing, latent semantic indexing and neural networks).
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A.V. Aho and M.J. Corasick. </author> <title> Fast pattern matching: an aid to bibliographic search. </title> <journal> CACM, </journal> <volume> 18(6) </volume> <pages> 333-340, </pages> <month> June </month> <year> 1975. </year>
Reference-contexts: Again, it requires some (O (m)) preprocessing of the search string. Recent variations on the basic algorithm have been suggested by Sunday [71]. Another approach to this problem is based on automata theory. Aho and Corasick [1975] <ref> [1] </ref> proposed a method that is based on a finite automaton and allows searching for several strings simultaneously. The search time is O (n) and the construction time of the automaton is linear on the sum of characters in the strings.
Reference: [2] <author> Brian T. Bartell, Garrison W. Cottrell, and Richard K. Belew. </author> <title> Latent semantic indexing is an optimal special case of multidimensional scaling. </title> <editor> In Nicholas Belkin et al., editors, </editor> <booktitle> Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 161-167. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Applications of LSI to information filtering and retrieval are reported in [27, 21, 19] and the technique is further developed in <ref> [17, 18, 2] </ref>. We begin with a basic implementation which captures the essence of the technique. From the complete collection of documents a term-document matrix is formed in which each entry consists of an integer representing the number of occurrences of a specific term in a specific document.
Reference: [3] <author> I.J. Barton, S.E. Creasey, M.F. Lynch, and M.J. Snell. </author> <title> An information-theoretic approach to text searching in direct access systems. </title> <journal> CACM, </journal> <volume> 17(6) </volume> <pages> 345-350, </pages> <month> June </month> <year> 1974. </year>
Reference-contexts: They also used a numeric procedure as a hashing function, instead of a look-up table. Harrison [28] used the signature file approach in order to speed up the substring testing. He suggests using consecutive letters ("n-grams") as input to the hashing function. Barton et al. [1974] <ref> [3] </ref> suggest using equi-frequent text segments instead of n-grams. Thus, the distribution of "1"s in the signature will be uniform. The method proposed by Tsichritzis and Christodoulakis [73] tries to use signature files without superimposed coding. There, the signature of the document consists of the concatenation of each word signature.
Reference: [4] <author> Richard K. Belew. </author> <title> Adaptive information retrieval: Using a connectionist representation to retrieve and learn about documents. </title> <editor> In N. J. Belkin and C. J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Twelfth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 11-20. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew <ref> [4] </ref>, Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [5] <author> R.S. Boyer and J.S. Moore. </author> <title> A fast string searching algorithm. </title> <journal> CACM, </journal> 20(10) 762-772, October 1977. 
Reference-contexts: The method needs some preprocessing of the search string, to detect recurring sequences of letters. The time required for preprocessing is O (m). The fastest known algorithm was proposed by Boyer and Moore <ref> [5] </ref>. Their idea is to perform character comparisons from right to left; if a mismatch occurs, the search string may be shifted up to m positions to the right.
Reference: [6] <author> Eric W. Brown, James P. Callan, and W. Bruce Croft. </author> <title> Fast incremental indexing for full-text information retrieval. </title> <booktitle> Proc. of VLDB Conf., </booktitle> <pages> pages 192-202, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Despite their size, we want to have fast insertions. Techniques to achieve fast insertions incrementally include the work by Tomasic et al., [72]; Cutting and Pedersen [12] and Brown et. al. <ref> [6] </ref>. These efforts typically exploit the skewness of the distribution of postings lists, treating the short lists different than the long ones. Compression methods have also been suggested, to manage the problem of index size: Zobel et al. [83] use Elias's [22] compression scheme for postings lists.
Reference: [7] <author> K. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> Proc. of the Second Conf. on Applied Natural Language Processing, </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year>
Reference-contexts: The commonly used stoplists, for example, are intended to remove words with low semantic content. Use of phrases as indexing terms is another example of integration of a simple natural language processing technique with more traditional information retrieval methods. Croft et. al. [10] suggest using a coarse parser <ref> [7] </ref> to detect sentences, and then use sentences for indexing (as oppose to single terms).
Reference: [8] <author> Paul R. Cohen and Rick Kjeldsen. </author> <title> Information retrieval by constrained spreading activation in semantic networks. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 23(4) </volume> <pages> 255-268, </pages> <year> 1987. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen <ref> [8] </ref>. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [9] <author> W.S. Cooper. </author> <title> On deriving design equations for information retrieval systems. </title> <booktitle> JASIS, </booktitle> <pages> pages 385-395, </pages> <month> November </month> <year> 1970. </year>
Reference-contexts: A dictionary of synonyms that helps to assign each word-stem to a concept class. In this way each document is represented by a tdimensional vector, where t is the number of permissible index terms (concepts). Absence of a term is indicated by a 0 (or by -1 <ref> [9] </ref>). Presence of a term is indicated by 1 (binary document vectors) or by a positive number (term weight), which reflects the importance of the term for the document. Several weighting functions have been proposed: - F REQ ik : the occurrence frequency of term k in document i.
Reference: [10] <author> W. Bruce Croft, Howard R. Turtle, and David D. Lewis. </author> <title> The use of phrases and structured queries in information retrieval. </title> <booktitle> Proc. of ACM SIGIR, </booktitle> <pages> pages 32-45, </pages> <month> October </month> <year> 1991. </year> <month> 16 </month>
Reference-contexts: The commonly used stoplists, for example, are intended to remove words with low semantic content. Use of phrases as indexing terms is another example of integration of a simple natural language processing technique with more traditional information retrieval methods. Croft et. al. <ref> [10] </ref> suggest using a coarse parser [7] to detect sentences, and then use sentences for indexing (as oppose to single terms). <p> Indexing on phrases provides some small improvements (from negative up to 20% savings <ref> [10] </ref>) on the precision/recall performance, at the expense of more elaborate preprocessing of the documents (full or partial parsing and syntax analysis).
Reference: [11] <author> W.B. Croft. </author> <title> A model of cluster searching based on classification. </title> <journal> Information Systems, </journal> <volume> 5 </volume> <pages> 189-195, </pages> <year> 1980. </year>
Reference-contexts: Then they suggest continuing the search in those clusters that seem to contain enough qualifying documents. Experimental results of their method are presented in [62] where it can be observed that the proposed method performs almost the same as the cosine similarity function (which is simpler). Croft <ref> [11] </ref> uses pattern recognition methods and derives a linear discriminant function, which is essentially a cluster-to-query similarity function. He uses the logarithm of the document frequency as a weight for each term in a cluster.
Reference: [12] <author> Doug Cutting and Jan Pedersen. </author> <title> Optimizations for dynamic inverted index maintenance. </title> <booktitle> Proc. SIGIR, </booktitle> <pages> pages 405-411, </pages> <year> 1990. </year>
Reference-contexts: Despite their size, we want to have fast insertions. Techniques to achieve fast insertions incrementally include the work by Tomasic et al., [72]; Cutting and Pedersen <ref> [12] </ref> and Brown et. al. [6]. These efforts typically exploit the skewness of the distribution of postings lists, treating the short lists different than the long ones.
Reference: [13] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Landauer, and Richard Harsh-man. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 41(6) </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: Next we present a brief but rigorous description of the mathematical details of LSI. Readers interested in the principles underlying the development of LSI or the existence and uniqueness of the singular value decomposition are encouraged to consult Deerwester, et.al. for details which have been omitted here for brevity <ref> [13] </ref>. Applications of LSI to information filtering and retrieval are reported in [27, 21, 19] and the technique is further developed in [17, 18, 2]. We begin with a basic implementation which captures the essence of the technique. <p> The features which are retained are those which have the greatest influence on the position of the document vector in m-space. Deerwester, et.al. suggest that this choice captures the underlying semantic structure (i.e. the concepts) in the term-document matrix while rejecting the "noise" that results from term usage variations <ref> [13] </ref>. In other words, the elimination of the small singular values reduces the document feature space into a "document concept space." Removing these small singular values reduces the SVD to ^ X = T SD T .
Reference: [14] <author> U. Deppisch. S-tree: </author> <title> a dynamic balanced signature index for office retrieval. </title> <booktitle> Proc. of ACM "Research and Development in Information Retrieval", </booktitle> <pages> pages 77-87, </pages> <month> September </month> <year> 1986. </year>
Reference-contexts: Two-level signature files have been suggested [55, 54], with improved search speed; trees of signatures <ref> [14] </ref> and partitioning based on signatures [39] have also been proposed, without timing results on real databases, though. Research on the design and performance of superimposed coding methods started long ago. The first who applied superimposed coding for retrieval is C.N. Mooers [46].
Reference: [15] <author> Tamas E. Doszkocs, James Reggia, and Xia Lin. </author> <title> Connectionist models and information retrieval. </title> <editor> In Martha E. Williams, editor, </editor> <booktitle> Annual Review of Information Science and Technology (ARIST), </booktitle> <volume> volume 25, </volume> <pages> pages 209-260. </pages> <publisher> Elsevier, </publisher> <year> 1990. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. <ref> [15] </ref>, Kwok [38], Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [16] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: The underlying reason is the so-called cluster hypothesis: closely associated documents tend to be relevant to the same requests. Grouping similar documents accelerates the searching. Clustering has attracted much attention in information retrieval and library science [61] [75] as well as in pattern recognition <ref> [16] </ref>. Although the emphasis in pattern recognition is not on document clustering, it uses some methods and ideas that are applicable to our environment. Note that clustering can be applied to terms, instead of documents. Thus, terms can be grouped and form classes of co-occurring terms.
Reference: [17] <author> Susan T. Dumais. </author> <title> Enhancing performance in latent semantic indexing (LSI) retrieval. </title> <type> Technical Memorandum TM-ARH-017527, </type> <institution> Bellcore, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: Applications of LSI to information filtering and retrieval are reported in [27, 21, 19] and the technique is further developed in <ref> [17, 18, 2] </ref>. We begin with a basic implementation which captures the essence of the technique. From the complete collection of documents a term-document matrix is formed in which each entry consists of an integer representing the number of occurrences of a specific term in a specific document.
Reference: [18] <author> Susan T. Dumais. </author> <title> Improving the retrieval of information from external sources. Behavior Research Methods, </title> <journal> Instruments and Computers, </journal> <volume> 23(2) </volume> <pages> 229-236, </pages> <year> 1991. </year>
Reference-contexts: Applications of LSI to information filtering and retrieval are reported in [27, 21, 19] and the technique is further developed in <ref> [17, 18, 2] </ref>. We begin with a basic implementation which captures the essence of the technique. From the complete collection of documents a term-document matrix is formed in which each entry consists of an integer representing the number of occurrences of a specific term in a specific document.
Reference: [19] <author> Susan T. Dumais. </author> <title> LSI meets TREC: A status report. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The First Text Retrieval Conference (TREC-1), </booktitle> <pages> 500-207, pages 137-152, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1993. </year> <note> NIST, NIST. Special Publication 500-207. </note>
Reference-contexts: Readers interested in the principles underlying the development of LSI or the existence and uniqueness of the singular value decomposition are encouraged to consult Deerwester, et.al. for details which have been omitted here for brevity [13]. Applications of LSI to information filtering and retrieval are reported in <ref> [27, 21, 19] </ref> and the technique is further developed in [17, 18, 2]. We begin with a basic implementation which captures the essence of the technique.
Reference: [20] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (LSI) and TREC-2. </title> <type> Technical Memorandum TM-ARH-023878, </type> <institution> Bellcore, </institution> <address> 445 South St., Morristown, NJ 07960, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: Of course, the query might contain concepts which are not preserved by LSI on the existing document collection, so the alignment of the concept space may not be well-suited to a particular query. Dumais reports promising results using this technique on the large TREC-2 document collection <ref> [20] </ref>. 3.2.3 Term Matching Before examining the consequences of this interpretation, it is also useful to interpret the adjoints of these functions in a meaningful way. The matrix XX T is composed of inner products of vectors of term frequencies across documents.
Reference: [21] <author> Susan T. Dumais and Jacob Nielsen. </author> <title> Automating the assignment of submitted manuscripts to reviewers. </title> <editor> In Nicholas Belkin et al., editors, </editor> <booktitle> Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 233-244. </pages> <publisher> ACM Press, </publisher> <month> June </month> <year> 1992. </year>
Reference-contexts: Readers interested in the principles underlying the development of LSI or the existence and uniqueness of the singular value decomposition are encouraged to consult Deerwester, et.al. for details which have been omitted here for brevity [13]. Applications of LSI to information filtering and retrieval are reported in <ref> [27, 21, 19] </ref> and the technique is further developed in [17, 18, 2]. We begin with a basic implementation which captures the essence of the technique.
Reference: [22] <author> P. Elias. </author> <title> Universal codeword sets and representations of integers. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> IT-21:194-203, </volume> <year> 1975. </year>
Reference-contexts: These efforts typically exploit the skewness of the distribution of postings lists, treating the short lists different than the long ones. Compression methods have also been suggested, to manage the problem of index size: Zobel et al. [83] use Elias's <ref> [22] </ref> compression scheme for postings lists. Finally, the `glimpse' package [44] uses a coarse index plus the `agrep' [78] package for approximate matching. 2.4 Vector Model and Clustering The basic idea in clustering is that similar documents are grouped together to form clusters.
Reference: [23] <author> C. Faloutsos and S. Christodoulakis. </author> <title> Design of a signature file method that accounts for nonuniform occurrence and query frequencies. </title> <booktitle> In Proc. 11th International Conference on VLDB, </booktitle> <pages> pages 165-170, </pages> <address> Stockholm, Sweden, </address> <month> August </month> <year> 1985. </year> <month> 17 </month>
Reference-contexts: He suggests creating the signatures in such a way that terms that appear frequently in queries are treated specially. However, Roberts did not try to provide any mathematical analysis towards this direction. Such an attempt can be found in <ref> [23] </ref> where it is shown that if the access patterns and occurrence frequencies 3 of words are known in advance and are skewed enough (80-20 rule), the signature file can be designed in such a way that we can avoid approximately 50% of the false drops of an ordinary signature file
Reference: [24] <author> C. Faloutsos and H.V. Jagadish. </author> <title> Hybrid index organizations for text databases. </title> <booktitle> EDBT '92, </booktitle> <pages> pages 310-327, </pages> <month> March </month> <year> 1992. </year> <note> Also available as UMIACS-TR-91-33 and CS-TR-2621. </note>
Reference-contexts: This means that a few vocabulary words will appear very often, while the majority of vocabulary words will appear once or twice. To remedy this problem, there have been proposed hybrid methods <ref> [24] </ref>, as well as algorithms to grow the postings lists adaptively [25]. * the fact that the indices may be huge, spanning several Megabytes or even GigaBytes. Despite their size, we want to have fast insertions.
Reference: [25] <author> Christos Faloutsos and H.V. Jagadish. </author> <title> On b-tree indices for skewed distributions. </title> <booktitle> In 18th VLDB Conference, </booktitle> <pages> pages 363-374, </pages> <address> Vancouver, British Columbia, </address> <month> August </month> <year> 1992. </year> <note> Also available as. </note>
Reference-contexts: This means that a few vocabulary words will appear very often, while the majority of vocabulary words will appear once or twice. To remedy this problem, there have been proposed hybrid methods [24], as well as algorithms to grow the postings lists adaptively <ref> [25] </ref>. * the fact that the indices may be huge, spanning several Megabytes or even GigaBytes. Despite their size, we want to have fast insertions. Techniques to achieve fast insertions incrementally include the work by Tomasic et al., [72]; Cutting and Pedersen [12] and Brown et. al. [6].
Reference: [26] <author> J.R. </author> <title> Files and H.D. Huskey. An information retrieval system based on superimposed coding. </title> <booktitle> Proc. AFIPS FJCC, </booktitle> <volume> 35 </volume> <pages> 423-432, </pages> <year> 1969. </year>
Reference-contexts: In this method, each document yields a bit string ('signature'), using hashing on its words and superimposed coding. The resulting document signatures are stored sequentially in a separate file (signature file); which is much smaller than the original file, and can be searched much faster. Files and Huskey <ref> [26] </ref> applied this method on a database of bibliographic entries. They used a stop list to discard the common words and an automatic procedure to reduce each non-common word to its stem. They also used a numeric procedure as a hashing function, instead of a look-up table.
Reference: [27] <author> Peter W. Foltz. </author> <title> Using latent semantic indexing for information filtering. </title> <editor> In Frederick H. Lochovsky and Robert B. Allen, editors, </editor> <booktitle> Conference on Office Information Systems, </booktitle> <pages> pages 40-47. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1990. </year>
Reference-contexts: Readers interested in the principles underlying the development of LSI or the existence and uniqueness of the singular value decomposition are encouraged to consult Deerwester, et.al. for details which have been omitted here for brevity [13]. Applications of LSI to information filtering and retrieval are reported in <ref> [27, 21, 19] </ref> and the technique is further developed in [17, 18, 2]. We begin with a basic implementation which captures the essence of the technique.
Reference: [28] <author> M.C. Harrison. </author> <title> Implementation of the substring test by hashing. </title> <journal> CACM, </journal> <volume> 14(12) </volume> <pages> 777-779, </pages> <month> December </month> <year> 1971. </year>
Reference-contexts: They used a stop list to discard the common words and an automatic procedure to reduce each non-common word to its stem. They also used a numeric procedure as a hashing function, instead of a look-up table. Harrison <ref> [28] </ref> used the signature file approach in order to speed up the substring testing. He suggests using consecutive letters ("n-grams") as input to the hashing function. Barton et al. [1974] [3] suggest using equi-frequent text segments instead of n-grams. Thus, the distribution of "1"s in the signature will be uniform.
Reference: [29] <author> R.L. Haskin. </author> <title> Special-purpose processors for text retrieval. </title> <journal> Database Engineering, </journal> <volume> 4(1) </volume> <pages> 16-29, </pages> <month> September </month> <year> 1981. </year>
Reference-contexts: Lesk [40] uses an over-loaded hash table with separate chaining, in order to achieve fast retrieval in a database of bibliographic entries. The disadvantages of this method are: the storage overhead (which can reach up to 300% of the original file size <ref> [29] </ref>), the cost of updating and reorganizing the index, if the environment is dynamic, and the cost of merging the lists, if they are too long or too many.
Reference: [30] <author> L.A. Hollaar, K.F. Smith, W.H. Chow, </author> <title> P.A. Emrath, and R.L. Haskin. Architecture and operation of a large, full-text information-retrieval system. </title> <editor> In D.K. Hsiao, editor, </editor> <booktitle> Advanced Database Machine Architecture, </booktitle> <pages> pages 256-299. </pages> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: The price is the bad response time. This might be severe for large data bases. Therefore, full text scanning is usually carried out by special purpose hardware [Hollaar et al. 1983] <ref> [30] </ref> or it is used in cooperation with another access method (e.g., inversion) that would restrict the scope of searching. 2.2 Signature Files The signature file approach has attracted much interest. In this method, each document yields a bit string ('signature'), using hashing on its words and superimposed coding.
Reference: [31] <author> J.E. Hopcroft and J.D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison Wesley, </publisher> <address> Reading, Mass., </address> <year> 1979. </year>
Reference-contexts: We shall not examine searching methods for general regular expressions. This subject is discussed in Automata Theory [Hopcroft and Ullman 1979, pp. 29-35]. <ref> [31] </ref> Given a regular expression, a finite state automaton can be built, which is able to detect the occurrence of the given expression in a document.
Reference: [32] <author> IBM. IBM System/370 (OS/VS), </author> <title> Storage and Information Retrieval System / Vertical Storage (STAIRS/VS). </title> <publisher> IBM World Trade Corporation. </publisher>
Reference-contexts: This method is followed by almost all the commercial systems [61]. More sophisticated methods can be used to organize the index file, such as: B-trees, TRIEs, hashing or variations and combinations of these (e.g., see [36] pp. 471-542). STAIRS <ref> [32] </ref> uses two levels for the index file. Words that start with the same pair of letters are stored together in the second level, 4 while the first level contains pointers to the second level, one pointer for each letter pair.
Reference: [33] <author> Paul S. Jacobs and Lisa F. Rau. </author> <title> Natural language techniques for intelligent information retrieval. </title> <editor> In Yves Chiaramella, editor, </editor> <booktitle> 11th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 85-99, </pages> <address> Grenoble, France, </address> <month> June </month> <year> 1988. </year> <institution> Presses Universitaires de Grenoble. </institution>
Reference-contexts: syntactic information and natural language processing in general (b) the 'Latent Semantic Indexing' method and (c) methods using neural networks and specifically spreading activation models. 3.1 Natural Language Processing Natural language processing techniques seek to enhance performance by matching the semantic content of queries with the semantic content of documents <ref> [33, 49, 76] </ref>. Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus [70, 41, 69].
Reference: [34] <author> Andrew Jennings and Hideyuki Higuchi. </author> <title> A user model neural network for a personal news service. </title> <booktitle> User Modeling and Uaer-Adapted Interaction, </booktitle> <volume> 3(1) </volume> <pages> 1-25, </pages> <year> 1993. </year>
Reference-contexts: Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in <ref> [34] </ref>.
Reference: [35] <author> W.H. Kautz and R.C. </author> <title> Singleton. Nonrandom binary superimposed codes. </title> <journal> IEEE Trans. Inform. Theory, </journal> <volume> IT-10:363-377, </volume> <month> October </month> <year> 1964. </year>
Reference-contexts: Orosz and Tackacs [47] used Jordan's theorem and gave a closed form formula for the probability distribution of the number of "1"'s in a document signature. Kautz and Singleton <ref> [35] </ref> discussed the problem of designing a system of signatures that will not have false drops. They attacked the problem from the point of view of coding and information theory.
Reference: [36] <author> D.E. Knuth. </author> <booktitle> The Art of Computer Programming, </booktitle> <volume> Vol. </volume> <month> 3: </month> <title> Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass, </address> <year> 1973. </year> <month> 18 </month>
Reference-contexts: This method is followed by almost all the commercial systems [61]. More sophisticated methods can be used to organize the index file, such as: B-trees, TRIEs, hashing or variations and combinations of these (e.g., see <ref> [36] </ref> pp. 471-542). STAIRS [32] uses two levels for the index file. Words that start with the same pair of letters are stored together in the second level, 4 while the first level contains pointers to the second level, one pointer for each letter pair.
Reference: [37] <author> D.E. Knuth, J.H. Morris, and V.R. Pratt. </author> <title> Fast pattern matching in strings. </title> <journal> SIAM J. Comput, </journal> <volume> 6(2) </volume> <pages> 323-350, </pages> <month> June </month> <year> 1977. </year>
Reference-contexts: Although simple to implement, this algorithm is too slow. If m is the length of the search string and n is the length of the document (in characters), then it needs up to O (m fl n) comparisons. Knuth, Morris and Pratt <ref> [37] </ref> proposed an algorithm that needs O (m + n) comparisons. Their main idea is to shift the search string by more than one characters to the right, whenever a mismatch is predictable. The method needs some preprocessing of the search string, to detect recurring sequences of letters.
Reference: [38] <author> K. L. Kwok. </author> <title> A neural network for probabilistic information retrieval. </title> <editor> In N. J. Belkin and C. J. van Rijsbergen, editors, </editor> <booktitle> Proceedings of the Twelfth Annual International ACMSIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 21-30. </pages> <publisher> ACM, </publisher> <month> June </month> <year> 1989. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok <ref> [38] </ref>, Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [39] <author> D.L. Lee and C.-W. Leng. </author> <title> Partitioned signature file: Designs and performance evaluation. </title> <journal> ACM Trans. on Information Systems (TOIS), </journal> <volume> 7(2) </volume> <pages> 158-180, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Two-level signature files have been suggested [55, 54], with improved search speed; trees of signatures [14] and partitioning based on signatures <ref> [39] </ref> have also been proposed, without timing results on real databases, though. Research on the design and performance of superimposed coding methods started long ago. The first who applied superimposed coding for retrieval is C.N. Mooers [46].
Reference: [40] <author> M.E. Lesk. </author> <title> Some Applications of Inverted Indexes on the UNIX System. </title> <institution> Bell Laboratories, </institution> <address> Murray Hill, New Jersey, </address> <year> 1978. </year>
Reference-contexts: STAIRS [32] uses two levels for the index file. Words that start with the same pair of letters are stored together in the second level, 4 while the first level contains pointers to the second level, one pointer for each letter pair. Lesk <ref> [40] </ref> uses an over-loaded hash table with separate chaining, in order to achieve fast retrieval in a database of bibliographic entries.
Reference: [41] <author> David Lewis and Alan Smeaton. </author> <booktitle> Workshop on: Use of natural language processing at TREC. </booktitle> <editor> In D. K. Harman, editor, </editor> <booktitle> The First Text Retrieval Conference (TREC-1), </booktitle> <pages> pages 365-366, </pages> <address> Gaithers-burg, MD, </address> <month> March </month> <year> 1993. </year> <title> NIST, </title> <type> U. </type> <institution> S. Department of Commerce. </institution>
Reference-contexts: Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus <ref> [70, 41, 69] </ref>. Although it has often been claimed that deeper semantic interpretation of texts and/or queries will be required before information retrieval can reach its full potential, a significant perfromance improvement from automated semantic analysis techniques has yet to be demonstrated.
Reference: [42] <author> David Dolan Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> University of Massachusetts, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Experiments indicate that the above method gives excellent results after only two or three iterations [56]. 3 Using Semantic Information The information retrieval techniques we have described use only a small amount of the information associated with a document as the basis for relevance decisions <ref> [42] </ref>. Despite this inherent limitation, they often achieve acceptable precision because the full text of a document contains a significant amount of redundancy. Next we survey recent methods that try to capture more information about each document, to achieve better performance.
Reference: [43] <author> Dekang Lin and Randy Goebel. </author> <title> Context-free grammar parsing by message passing. </title> <booktitle> In Proceedings of PACLING 93, </booktitle> <year> 1993. </year>
Reference-contexts: Considerable advances have been made in recent years in syntactic modeling of natural language, and efficient parsers with a broad domain have recently become available <ref> [43] </ref>. Semantic analysis is less well understood, but progress is being made with a syntax-directed semantic technique called lexical compositional semantics. Deeper semantic interpretation appears to require extensive knowledge engineering, limiting the breadth of systems which depend on natural language processing.
Reference: [44] <author> Udi Manber and Sun Wu. Glimpse: </author> <title> a tool to search through entire file systems. </title> <booktitle> Proc. of USENIX Techn. Conf., </booktitle> <year> 1994. </year> <note> Also available as TR 93-94, </note> <institution> Dept. of Comp. Sc., Univ. of Arizona, Tucson, </institution> <note> or through anonymous ftp (ftp://cs.arizona.edu/glimpse/glimpse.ps.Z). </note>
Reference-contexts: Compression methods have also been suggested, to manage the problem of index size: Zobel et al. [83] use Elias's [22] compression scheme for postings lists. Finally, the `glimpse' package <ref> [44] </ref> uses a coarse index plus the `agrep' [78] package for approximate matching. 2.4 Vector Model and Clustering The basic idea in clustering is that similar documents are grouped together to form clusters.
Reference: [45] <author> Michael L. Mauldin. </author> <title> Performance in ferret: a conceptual information retrieval system. </title> <booktitle> Proc. of ACM SIGIR, </booktitle> <pages> pages 347-355, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In the same vein, Rau and Jacobs [50] suggest grouping the keywords to achieve better precision/recall, with the help of a lexicon for the parsing. Mauldin <ref> [45] </ref> used a "skimming" parser (ie., a 'quick-and-dirty' parser) to turn documents in to 'case frames'; compared to a simple keyword system, the method typically improves the precision/recall performance, although it sometimes offers worse results.
Reference: [46] <author> C. Mooers. </author> <title> Application of random codes to the gathering of statistical information. Bulletin 31, </title> <publisher> Zator Co, </publisher> <address> Cambridge, Mass, </address> <year> 1949. </year> <title> based on M.S. </title> <type> thesis, </type> <institution> MIT, </institution> <month> January </month> <year> 1948. </year>
Reference-contexts: Research on the design and performance of superimposed coding methods started long ago. The first who applied superimposed coding for retrieval is C.N. Mooers <ref> [46] </ref>. He invented an ingenious mechanical device that was based on edge-notched cards and needles. This device was able to handle conjunctive queries on a database of bibliographic entries very fast. The keyword extraction was performed manually and the hashing function utilized a look-up table.
Reference: [47] <author> G. Orosz and L. Tackacs. </author> <title> Some probability problems concerning the marking of codes into the superimposed field. </title> <journal> J. of Documentation, </journal> <volume> 12(4) </volume> <pages> 231-234, </pages> <month> December </month> <year> 1956. </year>
Reference-contexts: Stiassny [68] suggested using pairs of letters to create each word signature. He also proved that, for a given signature size, the false drop probability is minimized if the number of "1"'s is equal to the number of "0"'s in the document signatures. Orosz and Tackacs <ref> [47] </ref> used Jordan's theorem and gave a closed form formula for the probability distribution of the number of "1"'s in a document signature. Kautz and Singleton [35] discussed the problem of designing a system of signatures that will not have false drops.
Reference: [48] <author> F. Rabitti and J. Zizka. </author> <title> Evaluation of access methods to text documents in office systems. </title> <booktitle> Proc. 3rd Joint ACM-BCS Symposium on Research and Development in Information Retrieval, </booktitle> <year> 1984. </year>
Reference-contexts: The method proposed by Tsichritzis and Christodoulakis [73] tries to use signature files without superimposed coding. There, the signature of the document consists of the concatenation of each word signature. This way, the positioning information is preserved. Rabitti and Zizka <ref> [48] </ref> report that this method is expected to be more heavily CPU bound than superimposed coding. Other researchers have adopted similar approaches for formatted records. Some of these papers suggest ideas potentially useful for text retrieval. Roberts [1979] [52] used a one-level signature file for a telephone directory application.
Reference: [49] <author> Ashwin Ram. </author> <title> Interest-based information filtering and extraction in natural language understanding systems. </title> <booktitle> In Proceedings of the Bellcore Workshop on High Performance Information Filtering, </booktitle> <month> November </month> <year> 1991. </year> <month> 19 </month>
Reference-contexts: syntactic information and natural language processing in general (b) the 'Latent Semantic Indexing' method and (c) methods using neural networks and specifically spreading activation models. 3.1 Natural Language Processing Natural language processing techniques seek to enhance performance by matching the semantic content of queries with the semantic content of documents <ref> [33, 49, 76] </ref>. Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus [70, 41, 69].
Reference: [50] <author> Lisa F. Rau and Paul S. Jacobs. </author> <title> Creating segmented databases from free text for text retrieval. </title> <booktitle> Proc. of ACM SIGIR, </booktitle> <pages> pages 337-346, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: The benefit of using phrases as terms is that phrases carry greater semantic content, but the risk is that the greater specificity of a phrase can reduce the performance of ranking or matching algorithms which depend on generality. In the same vein, Rau and Jacobs <ref> [50] </ref> suggest grouping the keywords to achieve better precision/recall, with the help of a lexicon for the parsing.
Reference: [51] <author> Ellen Riloff. </author> <title> Using cases to represent context for text classification. </title> <editor> In Bharat Bhargava, Timothy Finin, and Yalena Yesha, editors, </editor> <booktitle> Proceedings of the Second International Conference on Information and Knowledge Management, </booktitle> <pages> pages 105-113. </pages> <publisher> ACM, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Deeper semantic interpretation appears to require extensive knowledge engineering, limiting the breadth of systems which depend on natural language processing. Case frame analysis, an artificial intelligence technique, has been successfully applied in limited domains <ref> [51] </ref>. 3.2 Latent Semantic Indexing Latent Semantic Indexing (LSI) is a vector space information retrieval method which has demonstrated improved performance over the traditional vector space techniques used in Salton's SMART system. Next we present a brief but rigorous description of the mathematical details of LSI.
Reference: [52] <author> C.S. Roberts. </author> <title> Partial-match retrieval via the method of superimposed codes. </title> <journal> Proc. IEEE, </journal> <volume> 67(12) </volume> <pages> 1624-1642, </pages> <month> December </month> <year> 1979. </year>
Reference-contexts: This way, the positioning information is preserved. Rabitti and Zizka [48] report that this method is expected to be more heavily CPU bound than superimposed coding. Other researchers have adopted similar approaches for formatted records. Some of these papers suggest ideas potentially useful for text retrieval. Roberts [1979] <ref> [52] </ref> used a one-level signature file for a telephone directory application.
Reference: [53] <author> J.J. Rocchio. </author> <title> Performance indices for document retrieval. </title> <editor> In G. Salton, editor, </editor> <title> The SMART Retrieval System Experiments in Automatic Document Processing. </title> <publisher> Prentice-Hall Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year> <note> Chapter 3. </note>
Reference-contexts: He compares his function against the cosine function experimentally and he reports that his method performs better. The vector representation of queries and documents allows the so-called relevance feedback, which increases the effectiveness of the search <ref> [53] </ref> The user pinpoints the relevant documents among the retrieved ones and the system re-formulates the query vector and starts the searching from the beginning.
Reference: [54] <author> R. Sacks-Davis, A. Kent, and K. Ramamohanarao. </author> <title> Multikey access methods based on superimposed coding techniques. </title> <journal> ACM Trans. on Database Systems (TODS), </journal> <volume> 12(4) </volume> <pages> 655-696, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: Two-level signature files have been suggested <ref> [55, 54] </ref>, with improved search speed; trees of signatures [14] and partitioning based on signatures [39] have also been proposed, without timing results on real databases, though. Research on the design and performance of superimposed coding methods started long ago. The first who applied superimposed coding for retrieval is C.N.
Reference: [55] <author> R. Sacks-Davis and K. Ramamohanarao. </author> <title> A two level superimposed coding scheme for partial match retrieval. </title> <journal> Information Systems, </journal> <volume> 8(4) </volume> <pages> 273-280, </pages> <year> 1983. </year>
Reference-contexts: Two-level signature files have been suggested <ref> [55, 54] </ref>, with improved search speed; trees of signatures [14] and partitioning based on signatures [39] have also been proposed, without timing results on real databases, though. Research on the design and performance of superimposed coding methods started long ago. The first who applied superimposed coding for retrieval is C.N.
Reference: [56] <author> G. Salton. </author> <title> Relevance feedback and the optimization of retrieval effectiveness. </title> <editor> In G. Salton, editor, </editor> <title> The SMART Retrieval System Experiments in Automatic Document Processing. </title> <publisher> Prentice-Hall Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year> <note> Chapter 15. </note>
Reference-contexts: The usual way to carry out the query re-formulation is by adding (vector addition) to the query vector 9 the (weighted) vectors of the relevant documents and by subtracting the non-relevant ones. Experiments indicate that the above method gives excellent results after only two or three iterations <ref> [56] </ref>. 3 Using Semantic Information The information retrieval techniques we have described use only a small amount of the information associated with a document as the basis for relevance decisions [42].
Reference: [57] <author> G. Salton. </author> <title> The SMART Retrieval System Experiments in Automatic Document Processing. </title> <publisher> Prentice-Hall Inc, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1971. </year>
Reference-contexts: The searching proceeds from the most similar clusters, i.e., those whose similarity with the query vector exceeds a threshold. A cluster-to-query similarity function has to be selected; a popular choice is the cosine function <ref> [57] </ref> Yu and Luk [80] proposed a modification to the above search strategy: Given a (binary) query vector and (binary) cluster vectors, they derive a formula for the expected number of qualifying documents in each specific cluster.
Reference: [58] <author> G. Salton. </author> <title> Experiments in automatic thesaurus construction for information retrieval. </title> <booktitle> Information Processing 71, </booktitle> <pages> pages 115-123, </pages> <year> 1972. </year>
Reference-contexts: Co-occurring terms are usually relevant to each other and are sometimes synonyms. This grouping of terms is useful in automatic thesaurus construction and in dimensionality reduction. Automatic thesaurus construction is based on statistical criteria and thus it 5 is conceptually identical with the document clustering methods. However, Salton <ref> [58] </ref> states that the effectiveness of automatic term grouping algorithms is in doubt and he recommends semi-automatic methods. Document clustering involves two procedures: The cluster generation and the cluster search. First we discuss the cluster generation methods and classify them.
Reference: [59] <author> G. Salton. </author> <title> Recent studies in automatic text analysis and document retrieval. </title> <journal> JACM, </journal> <volume> 20(2) </volume> <pages> 258-278, </pages> <month> April </month> <year> 1973. </year>
Reference-contexts: Each document is represented as a vector; it is processed and some keywords are assigned to it. This is the "indexing" procedure and it can be carried out either manually or automatically. Comparison performed by Salton <ref> [59] </ref> shows that simple automatic indexing methods perform at least as well as manual methods in the laboratory environment.
Reference: [60] <author> G. Salton. </author> <title> Dynamic Information and Library Processing. </title> <publisher> Prentice-Hall Inc, </publisher> <address> Englewood Cliffs, N.J, </address> <year> 1975. </year>
Reference-contexts: The remainder of the documents are assigned to the existing clusters using a fast assignment strategy. Analysis on the execution time of some iterative cluster generation methods has been carried out by Salton <ref> [60] </ref> Assuming that the average number of clusters is logn or n=logn, he shows that the methods operate in O (nlogn) or O (n 2 =logn ) on the average.
Reference: [61] <author> G. Salton and M.J. McGill. </author> <title> Introduction to Modern Information Retrieval. </title> <publisher> McGraw-Hill, </publisher> <year> 1983. </year>
Reference-contexts: Fast retrieval can be achieved if we invert on those keywords. The keywords are stored, eg., alphabetically, in the 'index file'; for each keyword we maintain a list of pointers to the qualifying documents in the 'postings file'. This method is followed by almost all the commercial systems <ref> [61] </ref>. More sophisticated methods can be used to organize the index file, such as: B-trees, TRIEs, hashing or variations and combinations of these (e.g., see [36] pp. 471-542). STAIRS [32] uses two levels for the index file. <p> For the above reasons, the inversion method has been adopted in most of the commercial systems (DIALOG, BRS, MEDLARS, ORBIT, STAIRS <ref> [61] </ref> ch. 2). Recent developments and challenges include the following: * the skeweness of the distribution (Zipf's law) [82] of the postings lists. This means that a few vocabulary words will appear very often, while the majority of vocabulary words will appear once or twice. <p> The underlying reason is the so-called cluster hypothesis: closely associated documents tend to be relevant to the same requests. Grouping similar documents accelerates the searching. Clustering has attracted much attention in information retrieval and library science <ref> [61] </ref> [75] as well as in pattern recognition [16]. Although the emphasis in pattern recognition is not on document clustering, it uses some methods and ideas that are applicable to our environment. Note that clustering can be applied to terms, instead of documents. <p> The simplest and fastest one seems to be the "single pass" method [62]. Each document is processed once and is either assigned to one (or more, if overlap is allowed) of the existing clusters, or it creates a new cluster. Hybrid methods may be used. Salton and McGill <ref> [61] </ref> suggest using an iterative method to create a rough partition of the documents into clusters and then applying a graph-theoretic method to subdivide each of the previous clusters. Another hybrid approach is mentioned by Van-Rijsbergen [75].
Reference: [62] <author> G. Salton and A. Wong. </author> <title> Generation and search of clustered files. </title> <journal> ACM TODS, </journal> <volume> 3(4) </volume> <pages> 321-346, </pages> <month> December </month> <year> 1978. </year>
Reference-contexts: Many iterative methods have appeared in the literature. A brief survey can be found in ([75], pp. 51-53). The simplest and fastest one seems to be the "single pass" method <ref> [62] </ref>. Each document is processed once and is either assigned to one (or more, if overlap is allowed) of the existing clusters, or it creates a new cluster. Hybrid methods may be used. <p> Then they suggest continuing the search in those clusters that seem to contain enough qualifying documents. Experimental results of their method are presented in <ref> [62] </ref> where it can be observed that the proposed method performs almost the same as the cosine similarity function (which is simpler). Croft [11] uses pattern recognition methods and derives a linear discriminant function, which is essentially a cluster-to-query similarity function.
Reference: [63] <author> Gerard Salton, James Allan, and Chris Buckley. </author> <title> Automatic structuring and retrieval of large text files. </title> <journal> Comm. of ACM (CACM), </journal> <volume> 37(2) </volume> <pages> 97-108, </pages> <month> February </month> <year> 1994. </year> <month> 20 </month>
Reference-contexts: Mauldin [45] used a "skimming" parser (ie., a 'quick-and-dirty' parser) to turn documents in to 'case frames'; compared to a simple keyword system, the method typically improves the precision/recall performance, although it sometimes offers worse results. Salton et. al. <ref> [63] </ref> suggest using document vectors for a first filtering, followed by a comparison of section, paragraph and sentence vectors. 10 The first step in a more complete natural language processing information retrieval system would likely be automatic syntactic analysis.
Reference: [64] <author> Gerard Salton and Chris Buckley. </author> <title> On the use of spreading activation methods in automatic in-formation retrieval. </title> <editor> In Yves Chiaramella, editor, </editor> <booktitle> 11th International Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 147-160. </pages> <booktitle> ACM SIGIR, </booktitle> <month> June </month> <year> 1988. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew [4], Salton and Buckley <ref> [64] </ref> and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [65] <author> J. C. Scholtes. </author> <title> Neural nets and their relevance for information retrieval. </title> <institution> ITLI Prepublication CL-91-02, University of Amsterdam, Institute for Language, Logic and Information, Department of Computational Linguistics, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Johannes Scholtes's 1993 doctoral dissertation at the University of Amsterdam is the most recent comprehensive treatment of spreading activation methods for information retrieval. An earlier technical report he produced contains an excellent research review and extensive references <ref> [65] </ref>. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in [77]. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [66] <author> K. Sparck-Jones. </author> <title> A statistical interpretation of term specificity and its application in retrieval. </title> <journal> J. of Documentation, </journal> <volume> 28(1) </volume> <pages> 11-20, </pages> <month> March </month> <year> 1972. </year>
Reference-contexts: Several weighting functions have been proposed: - F REQ ik : the occurrence frequency of term k in document i. It is easy to obtain and more effective than the binary weight. - "Term specificity" <ref> [66] </ref> : logN log (DOCF REQ k ) +1 where DOCF REQ k is the number of documents that contain the term k and N is the total number of documents. It is relatively easy to obtain and it is more effective than the binary weight.
Reference: [67] <author> C. Stanfill and B. Kahle. </author> <title> Parallel free-text search on the connection machine system. </title> <journal> CACM, </journal> <volume> 29(12) </volume> <pages> 1229-1239, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The advantages are the simplicity of its implementation, the efficiency in handling insertions, the ability to handle queries on parts of words, ability to support a growing file, and tolerance of typing and spelling errors. In addition, the method is easily parallelizable (see <ref> [67] </ref> for an implementation on the Connection Machine). 2.3 Inversion Each document can be represented by a list of (key)words, which describe the contents of the document for retrieval purposes. Fast retrieval can be achieved if we invert on those keywords.
Reference: [68] <author> S. Stiassny. </author> <title> Mathematical analysis of various superimposed coding methods. </title> <journal> American Documentation, </journal> <volume> 11(2) </volume> <pages> 155-169, </pages> <month> February </month> <year> 1960. </year>
Reference-contexts: This device was able to handle conjunctive queries on a database of bibliographic entries very fast. The keyword extraction was performed manually and the hashing function utilized a look-up table. This method of edge-notched cards attracted a lot of interest. Stiassny <ref> [68] </ref> suggested using pairs of letters to create each word signature. He also proved that, for a given signature size, the false drop probability is minimized if the number of "1"'s is equal to the number of "0"'s in the document signatures.
Reference: [69] <author> Tomek Strzalkowski and Jose Perez Carballo. </author> <title> Recent developments in natural language text retrieval. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text Retrieval Conference (TREC-2), </booktitle> <pages> pages 123-136, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1994. </year> <pages> NIST. </pages>
Reference-contexts: Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus <ref> [70, 41, 69] </ref>. Although it has often been claimed that deeper semantic interpretation of texts and/or queries will be required before information retrieval can reach its full potential, a significant perfromance improvement from automated semantic analysis techniques has yet to be demonstrated.
Reference: [70] <author> Tomek Strzalowski. </author> <title> Natural language processing in large-scale text retrieval tasks. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The First Text Retrieval Conference (TREC-1), </booktitle> <pages> pages 173-187, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1993. </year> <title> NIST, </title> <type> U.S. </type> <institution> Department of Commerce. </institution>
Reference-contexts: Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus <ref> [70, 41, 69] </ref>. Although it has often been claimed that deeper semantic interpretation of texts and/or queries will be required before information retrieval can reach its full potential, a significant perfromance improvement from automated semantic analysis techniques has yet to be demonstrated.
Reference: [71] <author> D.M. Sunday. </author> <title> A very fast substring search algorithm. </title> <journal> Comm. of ACM (CACM), </journal> <volume> 33(8) </volume> <pages> 132-142, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Again, it requires some (O (m)) preprocessing of the search string. Recent variations on the basic algorithm have been suggested by Sunday <ref> [71] </ref>. Another approach to this problem is based on automata theory. Aho and Corasick [1975] [1] proposed a method that is based on a finite automaton and allows searching for several strings simultaneously.
Reference: [72] <author> Anthony Tomasic, Hector Garcia-Molina, and Kurt Shoens. </author> <title> Incremental updates of inverted lists for text document retrieval. </title> <booktitle> ACM SIGMOD, </booktitle> <pages> pages 289-300, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: Despite their size, we want to have fast insertions. Techniques to achieve fast insertions incrementally include the work by Tomasic et al., <ref> [72] </ref>; Cutting and Pedersen [12] and Brown et. al. [6]. These efforts typically exploit the skewness of the distribution of postings lists, treating the short lists different than the long ones.
Reference: [73] <author> D. Tsichritzis and S. Christodoulakis. </author> <title> Message files. </title> <journal> ACM Trans. on Office Information Systems, </journal> <volume> 1(1) </volume> <pages> 88-98, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: He suggests using consecutive letters ("n-grams") as input to the hashing function. Barton et al. [1974] [3] suggest using equi-frequent text segments instead of n-grams. Thus, the distribution of "1"s in the signature will be uniform. The method proposed by Tsichritzis and Christodoulakis <ref> [73] </ref> tries to use signature files without superimposed coding. There, the signature of the document consists of the concatenation of each word signature. This way, the positioning information is preserved. Rabitti and Zizka [48] report that this method is expected to be more heavily CPU bound than superimposed coding.
Reference: [74] <author> C.J. Van-Rijsbergen. </author> <title> An algorithm for information structuring and retrieval. </title> <journal> Computer Journal, </journal> <volume> 14(4) </volume> <pages> 407-412, </pages> <year> 1971. </year>
Reference-contexts: One way to achieve this is by applying the above method for several decreasing 7 values of the threshold. A better algorithm that builds such a hierarchy can be found in <ref> [74] </ref>. This method uses the single-link (nearest neighbor) criterion for clustering. Experiments with 200 documents indicate that the execution time for the proposed algorithm is quadratic.
Reference: [75] <author> C.J. Van-Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, England, </address> <year> 1979. </year> <note> 2nd edition. </note>
Reference-contexts: The underlying reason is the so-called cluster hypothesis: closely associated documents tend to be relevant to the same requests. Grouping similar documents accelerates the searching. Clustering has attracted much attention in information retrieval and library science [61] <ref> [75] </ref> as well as in pattern recognition [16]. Although the emphasis in pattern recognition is not on document clustering, it uses some methods and ideas that are applicable to our environment. Note that clustering can be applied to terms, instead of documents. <p> Hybrid methods may be used. Salton and McGill [61] suggest using an iterative method to create a rough partition of the documents into clusters and then applying a graph-theoretic method to subdivide each of the previous clusters. Another hybrid approach is mentioned by Van-Rijsbergen <ref> [75] </ref>. Some documents are sampled from the document collection and a core-clustering is constructed using an O (n 2 ) method for the sample of documents. The remainder of the documents are assigned to the existing clusters using a fast assignment strategy.
Reference: [76] <author> Edgar B. Wendlandt and James R. Driscoll. </author> <title> Incorporating a semantic analysis into a document retrieval strategy. </title> <editor> In A. Bookstein, Y. Chiaramella, G. Salton, and V. V. Raghavan, editors, </editor> <booktitle> Proceedings of the Fourteenth Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 270-279. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1991. </year> <month> 21 </month>
Reference-contexts: syntactic information and natural language processing in general (b) the 'Latent Semantic Indexing' method and (c) methods using neural networks and specifically spreading activation models. 3.1 Natural Language Processing Natural language processing techniques seek to enhance performance by matching the semantic content of queries with the semantic content of documents <ref> [33, 49, 76] </ref>. Natural language techniques have been applied with some success on the large Text Retrieval Conference (TREC) corpus [70, 41, 69].
Reference: [77] <author> Ross Wilkinson and Philip Hingston. </author> <title> Using the cosine measure in a neural network for document retrieval. </title> <editor> In A. Bookstein, Y. Chiaramella, G. Salton, and V. V. Raghavan, editors, </editor> <booktitle> Proceedings of the Fourteenth Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 202-210. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: An earlier technical report he produced contains an excellent research review and extensive references [65]. Earlier work on the topic includes the papers by Doszkocs et. al. [15], Kwok [38], Belew [4], Salton and Buckley [64] and Cohen and Kjeldsen [8]. Implementation details are discussed in <ref> [77] </ref>. Jennings and Higuchi have reported results for a system designed to filter USENET news articles in [34].
Reference: [78] <author> Sun Wu and Udi Manber. </author> <title> Agrep a fast approximate pattern searching tool. </title> <booktitle> In USENIX Conference, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: The search time is O (n) and the construction time of the automaton is linear on the sum of characters in the strings. Searching algorithms that can tolerate typing errors have been developed by Wu and Manber <ref> [78] </ref>. The idea is to scan the database one character at a time, keeping track of the currently matched characters in a clever bit-encoding. The method is fast (a few seconds for a few Megabytes of text on 2 a SUN-class workstation) and flexible. <p> Compression methods have also been suggested, to manage the problem of index size: Zobel et al. [83] use Elias's [22] compression scheme for postings lists. Finally, the `glimpse' package [44] uses a coarse index plus the `agrep' <ref> [78] </ref> package for approximate matching. 2.4 Vector Model and Clustering The basic idea in clustering is that similar documents are grouped together to form clusters. The underlying reason is the so-called cluster hypothesis: closely associated documents tend to be relevant to the same requests. Grouping similar documents accelerates the searching.
Reference: [79] <author> C.T. Yu, K. Lam, and G. Salton. </author> <title> Term weighting in information retrieval using the term precision model. </title> <journal> JACM, </journal> <volume> 29(1) </volume> <pages> 152-170, </pages> <month> January </month> <year> 1982. </year>
Reference-contexts: Under certain conditions, this is the theoretically optimal weight <ref> [79] </ref> and experimental evidence ([61], p. 207) seems to confirm it. A problem with this approach is that it requires relevance assessments for every possible term over the whole document collection, which requires human experts and takes much time.
Reference: [80] <author> C.T. Yu and W.S. Luk. </author> <title> Analysis of effectiveness of retrieval in clustered files. </title> <journal> JACM, </journal> <volume> 24(4) </volume> <pages> 607-622, </pages> <month> October </month> <year> 1977. </year>
Reference-contexts: The searching proceeds from the most similar clusters, i.e., those whose similarity with the query vector exceeds a threshold. A cluster-to-query similarity function has to be selected; a popular choice is the cosine function [57] Yu and Luk <ref> [80] </ref> proposed a modification to the above search strategy: Given a (binary) query vector and (binary) cluster vectors, they derive a formula for the expected number of qualifying documents in each specific cluster. Then they suggest continuing the search in those clusters that seem to contain enough qualifying documents.
Reference: [81] <author> C.T. Zahn. </author> <title> Graph-theoretical methods for detecting and describing gestalt clusters. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-20(1):68-86, </volume> <month> January </month> <year> 1971. </year>
Reference-contexts: This constant greatly affects the final partitioning and therefore imposes a structure on the given data, instead of detecting any existing structure. The method proposed by Zahn <ref> [81] </ref> is an attempt to circumvent this problem. He suggests finding a minimum spanning tree for the given set of points (documents) and then deleting the "inconsistent" edges. An edge is inconsistent if its length l is much larger than the average length l avg of its incident edges.
Reference: [82] <author> G.K. Zipf. </author> <title> Human Behavior and Principle of Least Effort: an Introduction to Human Ecology. </title> <publisher> Addison Wesley, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1949. </year>
Reference-contexts: For the above reasons, the inversion method has been adopted in most of the commercial systems (DIALOG, BRS, MEDLARS, ORBIT, STAIRS [61] ch. 2). Recent developments and challenges include the following: * the skeweness of the distribution (Zipf's law) <ref> [82] </ref> of the postings lists. This means that a few vocabulary words will appear very often, while the majority of vocabulary words will appear once or twice.
Reference: [83] <author> Justin Zobel, Alistair Moffat, and Ron Sacks-Davis. </author> <title> An efficient indexing technique for full-text database systems. </title> <booktitle> VLDB, </booktitle> <pages> pages 352-362, </pages> <month> August </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: These efforts typically exploit the skewness of the distribution of postings lists, treating the short lists different than the long ones. Compression methods have also been suggested, to manage the problem of index size: Zobel et al. <ref> [83] </ref> use Elias's [22] compression scheme for postings lists. Finally, the `glimpse' package [44] uses a coarse index plus the `agrep' [78] package for approximate matching. 2.4 Vector Model and Clustering The basic idea in clustering is that similar documents are grouped together to form clusters.
References-found: 83

