URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/methodslearning.ps.Z
Refering-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Root-URL: http://www.ruhr-uni-bochum.de/lmi/mschmitt/
Email: mschmitt@igi.tu-graz.ac.at  
Title: On Methods to Keep Learning Away from Intractability (Extended Abstract)  
Author: Michael Schmitt 
Address: Klosterwiesgasse 32/2, A-8010 Graz, Austria,  
Affiliation: Institute for Theoretical Computer Science, Technische Universitat Graz  
Note: In F. Fogelman-Soulie and P. Gallinari, editors, Proceedings of the International Conference on Artificial Neural Networks ICANN'95, volume 1, pages 211-216, EC2 Cie., Paris, 1995.  
Abstract: We investigate the complexity of learning from restricted sets of training examples. With the intention to make learning easier we introduce two types of restrictions that describe the permitted training examples. The strength of the restrictions can be tuned by choosing specific parameters. We ask how strictly their values must be limited to turn NP-complete learning problems into polynomial-time solvable ones. Results are presented for Perceptrons with binary and arbitrary weights. We show that there exist bounds for the parameters that sharply separate efficiently solvable from intractable learning problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. Amaldi. </author> <title> On the complexity of training Perceptrons. </title> <editor> In T. Kohonen, K. Makisara, O. Simula, and J. Kangas, editors, </editor> <booktitle> Artificial Neural Networks, </booktitle> <pages> pages 55-60. </pages> <publisher> Elsevier (North-Holland), </publisher> <year> 1991. </year>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. <p> Consistency for B has been shown NP-complete by Pitt and Valiant [16, Section 5]. This implies that maximum consistency for B is also NP-complete. Consistency for L is well known to be solvable in polynomial time by methods of linear programming [12]. Amaldi <ref> [1] </ref> has shown that maximum consistency for L with threshold t = 0 is NP-complete. The constraint t = 0 was eliminated by Hoffgen and Simon [10]. We now state our results in the form of so-called dichotomy theorems.
Reference: [2] <author> M. Anthony and N. Biggs. </author> <title> Computational Learning Theory. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science. </booktitle> <publisher> Cambridge University Press, </publisher> <address> Cambridge, </address> <year> 1992. </year>
Reference-contexts: Alternative terms frequently used in the literature are training problem [4], loading problem [11], and fitting problem [15]. The name consistency problem often occurs in the context of Valiant's PAC-learning paradigm <ref> [2, 5] </ref>. Let B be the set of Boolean functions computable by single neurons (Perceptrons, threshold gates) with binary weights and arbitrary threshold. Further, let C and H be arbitrary natural numbers. The classes of consistency problems for B with bounded coincidence and heaviness are defined as follows.
Reference: [3] <author> B. Aspvall, M. F. Plass, and R. E. Tarjan. </author> <title> A linear-time algorithm for testing the truth of certain quantified Boolean formulas. </title> <journal> Information Processing Letters, </journal> <volume> 8 </volume> <pages> 121-123, </pages> <year> 1979. </year>
Reference-contexts: Figure 1 shows the algorithm. 2-SATISFIABILITY can be decided in linear time <ref> [3, 6] </ref>. The reduction and the first two parts (t 1; t 3) can easily be seen to run in linear time.
Reference: [4] <author> A. L. Blum and R. L. Rivest. </author> <title> Training a 3-node neural network is NP-complete. </title> <booktitle> Neural Networks, </booktitle> <volume> 5 </volume> <pages> 117-127, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. <p> The consistency problem for a set of functions F is the problem to decide if for a given set of examples S there exists a function f 2 F that is consistent with S. Alternative terms frequently used in the literature are training problem <ref> [4] </ref>, loading problem [11], and fitting problem [15]. The name consistency problem often occurs in the context of Valiant's PAC-learning paradigm [2, 5]. Let B be the set of Boolean functions computable by single neurons (Perceptrons, threshold gates) with binary weights and arbitrary threshold.
Reference: [5] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Alternative terms frequently used in the literature are training problem [4], loading problem [11], and fitting problem [15]. The name consistency problem often occurs in the context of Valiant's PAC-learning paradigm <ref> [2, 5] </ref>. Let B be the set of Boolean functions computable by single neurons (Perceptrons, threshold gates) with binary weights and arbitrary threshold. Further, let C and H be arbitrary natural numbers. The classes of consistency problems for B with bounded coincidence and heaviness are defined as follows.
Reference: [6] <author> S. Even, A. Itai, and A. Shamir. </author> <title> On the complexity of timetable and multicommodity flow problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 5 </volume> <pages> 691-703, </pages> <year> 1976. </year>
Reference-contexts: Figure 1 shows the algorithm. 2-SATISFIABILITY can be decided in linear time <ref> [3, 6] </ref>. The reduction and the first two parts (t 1; t 3) can easily be seen to run in linear time.
Reference: [7] <author> M. R. Garey and D. S. Johnson. </author> <title> Computers and Intractability: A Guide to the Theory of NP-Completeness. </title> <editor> W. H. </editor> <publisher> Freeman, </publisher> <address> San Francisco, </address> <year> 1979. </year>
Reference-contexts: Question: Is there a truth assignment fi : U ! f0; 1g such that each subset in C has exactly one true variable? POSITIVE 1-IN-3SAT, which does not have the condition jc " dj 1, is already known to be NP-complete <ref> [7, p. 259] </ref>. We show that it remains NP-complete on the requirement of almost disjointness. Lemma 4 ALMOST DISJOINT POSITIVE 1-IN-3SAT is NP-complete. Proof. (Omitted.) Theorem 5 CONSISTENCY FOR B WITH COINCIDENCE 1 AND HEAVINESS 4 is NP-complete. Proof. (Outline.) The proof is by reduction from ALMOST DISJOINT POSITIVE 1-IN-3SAT. <p> Proof. (Outline.) The proof is an adaptation of a proof by Hoffgen and Simon [10, Theorem 3.1]. It turns out that a modification of their construction is sufficient for our purpose. The reduction is from VERTEX COVER <ref> [7, p. 190] </ref>. Let (V; E); K be an instance of the latter where V = fv 1 ; : : : ; v n g. The set of examples S where dom (S) f0; 1g 2n is constructed as follows.
Reference: [8] <author> S. Haykin. </author> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: References [1, 4, 10, 11, 14, 19] are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. Techniques to modify architectures (see, e.g., <ref> [8, Section 6.17] </ref> and [9, Section 6.6] for comprehensive descriptions) or to initialize weights [13] are designed to reduce learning times. Their success, however, can only be stated after completion of the training process. In this paper we pursue a different approach.
Reference: [9] <author> J. Hertz, A. Krogh, and R. G. Palmer. </author> <title> Introduction to the Theory of Neural Computation, volume I of Santa Fe Institute studies in the sciences of complexity. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, CA, </address> <year> 1991. </year>
Reference-contexts: References [1, 4, 10, 11, 14, 19] are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. Techniques to modify architectures (see, e.g., [8, Section 6.17] and <ref> [9, Section 6.6] </ref> for comprehensive descriptions) or to initialize weights [13] are designed to reduce learning times. Their success, however, can only be stated after completion of the training process. In this paper we pursue a different approach.
Reference: [10] <author> K.-U. Hoffgen and H.-U. Simon. </author> <title> Robust trainability of single neurons. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 428-439. </pages> <publisher> ACM Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. <p> Consistency for L is well known to be solvable in polynomial time by methods of linear programming [12]. Amaldi [1] has shown that maximum consistency for L with threshold t = 0 is NP-complete. The constraint t = 0 was eliminated by Hoffgen and Simon <ref> [10] </ref>. We now state our results in the form of so-called dichotomy theorems. Theorem 1 CONSISTENCY FOR B WITH COINCIDENCE C AND HEAVINESS H is NP-complete whenever C 1 and H 4. <p> Theorem 6 MAXIMUM CONSISTENCY FOR B WITH COINCIDENCE 1 AND HEAVINESS 2 is NP-complete. Proof. (Outline.) The proof is an adaptation of a proof by Hoffgen and Simon <ref> [10, Theorem 3.1] </ref>. It turns out that a modification of their construction is sufficient for our purpose. The reduction is from VERTEX COVER [7, p. 190]. Let (V; E); K be an instance of the latter where V = fv 1 ; : : : ; v n g. <p> For each e 2 E we define two edge examples (1 e ; 0); (0; 1 e ) 2 pos (S). We omit the rest of the proof. Hoffgen and Simon have investigated single neurons with arbitrary weights. The following statement is an immediate consequence of their proof <ref> [10, Theorem 3.1] </ref>. Corollary 7 MAXIMUM CONSISTENCY FOR L WITH COINCIDENCE 1 AND HEAVINESS 2 is NP-complete. 4 Learning problems solvable in linear time In this section we show that all (maximum) consistency problems complementary to those of the previous section have linear-time algorithms on a random access machine.
Reference: [11] <author> J. S. Judd. </author> <title> Neural Network Design and the Complexity of Learning. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, CA, </address> <year> 1990. </year> <month> 8 </month>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. <p> The consistency problem for a set of functions F is the problem to decide if for a given set of examples S there exists a function f 2 F that is consistent with S. Alternative terms frequently used in the literature are training problem [4], loading problem <ref> [11] </ref>, and fitting problem [15]. The name consistency problem often occurs in the context of Valiant's PAC-learning paradigm [2, 5]. Let B be the set of Boolean functions computable by single neurons (Perceptrons, threshold gates) with binary weights and arbitrary threshold. Further, let C and H be arbitrary natural numbers.
Reference: [12] <author> H. Karloff. </author> <title> Linear Programming. </title> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1991. </year>
Reference-contexts: The latter result also holds if the weights are allowed to be arbitrary real numbers. Finally, total agreement for arbitrary weights is widely known to be computable in polynomial time by methods of linear programming <ref> [12] </ref>. Thus we are able to completely characterize all defined restrictions of learning problems for this architecture with respect to their complexity. In Section 2 we give definitions of example properties and learning problems and present the main results. In Section 3 we sketch the NP-completeness results. <p> Consistency for B has been shown NP-complete by Pitt and Valiant [16, Section 5]. This implies that maximum consistency for B is also NP-complete. Consistency for L is well known to be solvable in polynomial time by methods of linear programming <ref> [12] </ref>. Amaldi [1] has shown that maximum consistency for L with threshold t = 0 is NP-complete. The constraint t = 0 was eliminated by Hoffgen and Simon [10]. We now state our results in the form of so-called dichotomy theorems.
Reference: [13] <author> J. F. Kolen and J. B. Pollack. </author> <title> Backpropagation is sensitive to initial conditions. </title> <journal> Complex Systems, </journal> <volume> 4 </volume> <pages> 269-280, </pages> <year> 1990. </year>
Reference-contexts: In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm. Techniques to modify architectures (see, e.g., [8, Section 6.17] and [9, Section 6.6] for comprehensive descriptions) or to initialize weights <ref> [13] </ref> are designed to reduce learning times. Their success, however, can only be stated after completion of the training process. In this paper we pursue a different approach. We try to bypass intractability of training a specific architecture by restricting the set of permitted training examples.
Reference: [14] <author> J.-H. Lin and J. S. Vitter. </author> <title> Complexity results on learning by neural nets. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 211-230, </pages> <year> 1991. </year>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm.
Reference: [15] <author> B. K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: Alternative terms frequently used in the literature are training problem [4], loading problem [11], and fitting problem <ref> [15] </ref>. The name consistency problem often occurs in the context of Valiant's PAC-learning paradigm [2, 5]. Let B be the set of Boolean functions computable by single neurons (Perceptrons, threshold gates) with binary weights and arbitrary threshold. Further, let C and H be arbitrary natural numbers.
Reference: [16] <author> L. Pitt and L. G. Valiant. </author> <title> Computational limitations on learning from examples. </title> <journal> Journal of the ACM, </journal> <volume> 35 </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: They are defined correspondingly by replacing B by L in the above definitions. 2.2 Main results Before we state our results for the restricted learning problems we recall known results for the unrestricted versions from the literature. Consistency for B has been shown NP-complete by Pitt and Valiant <ref> [16, Section 5] </ref>. This implies that maximum consistency for B is also NP-complete. Consistency for L is well known to be solvable in polynomial time by methods of linear programming [12]. Amaldi [1] has shown that maximum consistency for L with threshold t = 0 is NP-complete.
Reference: [17] <author> N. J. Redding, A. Kowalczyk, and T. Downs. </author> <title> Constructive higher-order network algorithm that is polynomial time. </title> <booktitle> Neural Networks, </booktitle> <volume> 6 </volume> <pages> 997-1010, </pages> <year> 1993. </year>
Reference-contexts: The approach was to introduce criteria for the complexity of example sets to define simpler problem instances for known hard problems. This is different from a recently proposed polynomial-time learning algorithm by Redding et al. <ref> [17] </ref> who make use of higher order networks of varying size and node fan-in. We confined ourselves to fixed architectures showing that extensive knowledge about the examples one has to deal with can lead to highly specialized and fast learning algorithms.
Reference: [18] <author> M. Schmitt. </author> <title> On the complexity of consistency problems for neurons with binary weights. </title> <type> Ulmer Informatik-Berichte 94-01, </type> <institution> Universitat Ulm, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Linear-time algorithms and sketches thereof are presented in Section 4. The last section contains some concluding remarks and discussions. Proofs in this paper are sketched very coarsely or omitted at all. The interested reader is referred to a more comprehensive technical report <ref> [18] </ref>. 2 Learning from restricted example sets 2.1 Definitions Given a set of examples S = f (x (1) ; a (1) ); : : : ; (x (m) ; a (m) )g where x (i) 2 f0; 1g n and a (i) 2 f0; 1g, we denote by pos (S)
Reference: [19] <author> J. Sima. </author> <title> Loading deep networks is hard. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 842-850, </pages> <year> 1994. </year> <month> 9 </month>
Reference-contexts: 1 Introduction Since the advent of learning algorithms for neural networks many problems have been shown computationally intractable. References <ref> [1, 4, 10, 11, 14, 19] </ref> are just to mention a few. In order to cope with complexity various methods have been invented to incorporate knowledge about the functions being learned into the algorithm.
References-found: 19

