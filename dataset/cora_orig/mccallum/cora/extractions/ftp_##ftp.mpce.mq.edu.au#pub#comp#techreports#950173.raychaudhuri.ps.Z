URL: ftp://ftp.mpce.mq.edu.au/pub/comp/techreports/950173.raychaudhuri.ps.Z
Refering-URL: http://www.ai.mit.edu/people/cohn/SAL95/papers.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: tirthank@mpce.mq.edu.au  
Title: An Algorithm for Active Data Collection for Learning Feasibility Study with Neural Networks.  
Author: Tirthankar RayChaudhuri Leonard G.C. Hamey Tirthankar RayChaudhuri and Leonard G.C. Hamey 
Degree: All rights reserved  
Date: May 1995  
Web: URL ftp://ftp.mpce.mq.edu.au/pub/comp/techreports  
Note: This publication is available at the following  Copyright c 1995  
Abstract: Macquarie University Technical Report No. 95-173C Department of Computing School of MPCE, Macquarie University, New South Wales, Australia 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Cohn, L. Atlas, and L. Ladner. </author> <title> Training connectionist networks with queries and selective sampling. </title> <editor> In D. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Califor-nia, </address> <year> 1990. </year>
Reference: [2] <author> David A. Cohn. </author> <title> Neural network exploration using optimal experiment design. </title> <type> Technical Report AIM-1491, </type> <institution> Department of Brain and Cognitive Sciences, Massachusetts Institute of Technology, </institution> <month> June </month> <year> 1994. </year>
Reference-contexts: We propose a means of achieving both objectives simultaneously in this work. We have based our ideas upon concepts already introduced by researchers such as Cohn et al at MIT <ref> [2, 3] </ref> and Krogh and Vedelsby [7]. 2 Existing Methods and a Data Minimising Ap proach Recent proposed schemes of active learning in the neural network literature have covered both active data subset selection as well as active selection of unlabeled data. 2.1 Active Data Subset Selection Tamburini and Davoli [14] <p> Cohn et al <ref> [2, 3] </ref> have carried out active learning by choosing unlabeled inputs that minimise the expected value of the learner's mean squared error. We use the `query-by-committee' approach.
Reference: [3] <author> David A. Cohn, Zoubin Ghahramani, and Michael I. Jordan. </author> <title> Active learning with statistical models. </title> <editor> In G.Tesauro, D.Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <address> Cambridge, MA, 1995. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Such query-based learning is often referred to as `active learning' | the learner having the ability to select its own training data <ref> [3] </ref>. Apart from obtaining improved generalisation there is another major factor which motivates research in active learning methods | the high expense of data collection and measurement. <p> We propose a means of achieving both objectives simultaneously in this work. We have based our ideas upon concepts already introduced by researchers such as Cohn et al at MIT <ref> [2, 3] </ref> and Krogh and Vedelsby [7]. 2 Existing Methods and a Data Minimising Ap proach Recent proposed schemes of active learning in the neural network literature have covered both active data subset selection as well as active selection of unlabeled data. 2.1 Active Data Subset Selection Tamburini and Davoli [14] <p> Cohn et al <ref> [2, 3] </ref> have carried out active learning by choosing unlabeled inputs that minimise the expected value of the learner's mean squared error. We use the `query-by-committee' approach.
Reference: [4] <author> Howard Demuth and Mark Beale. </author> <title> Neural Network Toolbox for Use with MATLAB. </title> <publisher> The Math Works Inc., </publisher> <month> January </month> <year> 1994. </year> <note> User's Guide. </note>
Reference-contexts: The architectures of the ten identifying networks were identical with that used to generate the plant, but their weights were initialised with random values every time. Using the Matlab Neural Network Toolbox, fast backpropagation with the Levenberg-Marquardt algorithm <ref> [4] </ref> performed quick training. The initial data consisted of ten points (input-output pairs) collected randomly from the plant.
Reference: [5] <author> Y. Freund, H.S. Seung, E. Shamir, and N. Tishby. </author> <title> Information, prediction, and query by committee. </title> <booktitle> In Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5. </volume> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1993. </year>
Reference: [6] <author> L.K. Hansen and P. Salamon. </author> <title> Neural network ensembles. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(10) </volume> <pages> 993-1001, </pages> <month> October </month> <year> 1990. </year>
Reference: [7] <author> Anders Krogh and Jesper Vedelsby. </author> <title> Neural network ensembles, cross validation and active learning. </title> <editor> In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year>
Reference-contexts: We propose a means of achieving both objectives simultaneously in this work. We have based our ideas upon concepts already introduced by researchers such as Cohn et al at MIT [2, 3] and Krogh and Vedelsby <ref> [7] </ref>. 2 Existing Methods and a Data Minimising Ap proach Recent proposed schemes of active learning in the neural network literature have covered both active data subset selection as well as active selection of unlabeled data. 2.1 Active Data Subset Selection Tamburini and Davoli [14] have suggested that if those training <p> Cohn et al [2, 3] have carried out active learning by choosing unlabeled inputs that minimise the expected value of the learner's mean squared error. We use the `query-by-committee' approach. Krogh and Vedelsby <ref> [7] </ref> have successfully applied this idea to neural network learning; however their emphasis has been upon reducing the generalisation error of a neural network ensemble rather than minimising data collection. They began training with one `example' and added one point at a time corresponding to maximum `ensemble ambiguity'. <p> The algorithm converged, but as is only to be expected, 5 more points were collected than in the experiment with active learning. Figure 6 (a) shows the final sampled points and Figure 6 (b) the corresponding plot of final variance. This is a case of `passive' learning <ref> [7] </ref> | the selection of the additional labeled sample each time is unbiased. 3.2 Experiments with Noisy Data So far we had worked with clean data which is mathematically easier to handle. Noise was now added to the output of the plant (Figure 2 (b)).
Reference: [8] <author> D. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 590-604, </pages> <year> 1992. </year>
Reference: [9] <author> M. Plutowski and H. White. </author> <title> Selecting concise training sets from clean data. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2) </volume> <pages> 305-318, </pages> <year> 1993. </year>
Reference-contexts: Plutowski and White <ref> [9, 10] </ref> have implemented a technique of selecting `training exemplars' which is based upon a derivation of the integrated MSE criterion | to obtain points with maximum information from already-labeled data.
Reference: [10] <author> Mark Plutowski. </author> <title> Selected Training Exemplars for Neural Network Learning. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <address> San Diego, </address> <year> 1994. </year>
Reference-contexts: This applies to machine learning algorithms as well. The challenge is to obtain good generalisation from a limited amount of data. The traditional approach has been to study generalisation from random examples. However it has been found that random examples contain progressively less new information as learning proceeds <ref> [10, 13] </ref>. In order to improve generalisation therefore, it is necessary to make learning query-based, i.e., to set up a criterion which will select only those training examples which contain maximal information about the system being learned. <p> Plutowski and White <ref> [9, 10] </ref> have implemented a technique of selecting `training exemplars' which is based upon a derivation of the integrated MSE criterion | to obtain points with maximum information from already-labeled data.
Reference: [11] <author> H.S. Seung, M. Opper, and H. Sompolinsky. </author> <title> Query by committee. </title> <booktitle> In Proceedings of the Fifth Workshop on Computaional Learning Theory, </booktitle> <pages> pages 287-294, </pages> <address> San Mateo, California, 1992. </address> <publisher> Morgan Kaufmann. </publisher> <pages> 10 </pages>
Reference: [12] <author> Peter Sollich. </author> <title> Query construction, entropy and generalisation in neural network models. </title> <journal> Physical Review E, </journal> <volume> 49 </volume> <pages> 4637-4651, </pages> <year> 1994. </year>
Reference: [13] <author> Peter Sollich and David Saad. </author> <title> Learning from queries for maximum information gain in imperfectly learnable problems. </title> <editor> In G. Tesauro, D.S. Touretzky, and T.K. Leen, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 7. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge MA, </address> <year> 1995. </year>
Reference-contexts: This applies to machine learning algorithms as well. The challenge is to obtain good generalisation from a limited amount of data. The traditional approach has been to study generalisation from random examples. However it has been found that random examples contain progressively less new information as learning proceeds <ref> [10, 13] </ref>. In order to improve generalisation therefore, it is necessary to make learning query-based, i.e., to set up a criterion which will select only those training examples which contain maximal information about the system being learned. <p> Generalisation is greatly enhanced by active data subset selection, i.e., using querying methods to resample those points which contain the most information about the system. Compared to random repetitive sampling this approach greatly reduces computational activity <ref> [13] </ref>, but does little to minimise data gathering expenditure. 2.2 An Algorithm to minimise Data Collection It is desirable to `actively' reduce the considerable expense of data collection and at the same time not increase our generalisation error, i.e., retain modelling accuracy.
Reference: [14] <author> Fabio Tamburini and Renzo Davoli. </author> <title> An algorithmic method to build good training sets for neural-network classifiers. </title> <type> Technical Report UBLCS-94-18, </type> <institution> Laboratory for Computer Science, University of Bologna, </institution> <month> July </month> <year> 1994. </year>
Reference-contexts: [2, 3] and Krogh and Vedelsby [7]. 2 Existing Methods and a Data Minimising Ap proach Recent proposed schemes of active learning in the neural network literature have covered both active data subset selection as well as active selection of unlabeled data. 2.1 Active Data Subset Selection Tamburini and Davoli <ref> [14] </ref> have suggested that if those training patterns which exhibit the highest LMS error upon an initial classification attempt be added to the training data, then a better training set is obtained.
Reference: [15] <author> Sebastian B. Thrun and Knut Moller. </author> <title> Active exploration in dynamic environments. </title> <editor> In John E. Moody, Steven J. Hanson, and Richard P. Lippman, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, California, </address> <year> 1992. </year>

References-found: 15

