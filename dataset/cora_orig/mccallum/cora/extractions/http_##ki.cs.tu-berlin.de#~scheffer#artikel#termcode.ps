URL: http://ki.cs.tu-berlin.de/~scheffer/artikel/termcode.ps
Refering-URL: http://ki.cs.tu-berlin.de/~scheffer/publications.html
Root-URL: 
Email: musia@cs.tu-berlin.de  tobiass@cs.tu-berlin.de  
Title: A Term-Based Genetic Code for Artificial Neural Networks  
Author: Marek Musial and Tobias Scheffer 
Address: Berlin), Schottburger Str. 11 a, D-12305 Berlin,  Berlin), Jahnstr. 65, D-12347 Berlin,  
Affiliation: (TU  (TU  
Abstract: We developed a well-structured term-based language for the structural specification of artificial neural networks. The language achieves an intuitive and compact representation even for very large networks, making it interesting as an input language for network simulators. Since it describes neural networks on a logical level, it is very well suited as a "genetic code" for the optimization of network architectures by genetic algorithms, allowing well-controllable mutation operators and a powerful crossover operation that is able to recombine functional blocks of any shape instead of destroying them. We define the language formally, give examples of its application and present some results of its use as a genetic code for finding network architec tures.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Cybenko, </author> <title> "Approximation by Superpositions of a Sigmoidal Function", Mathematics of Control, Signals, </title> <booktitle> and Systems 2, </booktitle> <pages> pp. 303-314, </pages> <year> 1989 </year>
Reference-contexts: 1 Introduction Although it can be proven that there exists an artificial neural network (ANN) approximating every continuous, bounded function (e.g. Theorem of Cybenko <ref> [1] </ref>), no universal algorithm that determines the network's parameters is known. Given an architecture of an ANN, i.e. the number of units and their connectivity, the weights can be adapted by several training algorithms, e.g. backpropagation [2]. <p> N ser (t 1 ; : : : ; t n ) 2 T; iff t 1 ; : : : ; t n 2 T; n &gt; 1 smul (n; t) 2 T; iff n 2 N n f1g; t 2 T tf (r) 2 T; iff r 2 <ref> [0; 1] </ref> Where N is the set of positive natural numbers. 3.2 Informal Semantics A term consists of a function symbol followed by a list of arguments. Depending on the function symbol, the arguments may also be terms, and their semantics are networks. <p> We do not draw these edges at all, instead we draw all members directly into their parent blocks. Each node is assigned a type. The (infinite) alphabet of types is N fi f*; par; tf (r)g; r 2 <ref> [0; 1] </ref> The first component of the type indicates the position of the block within its parent, the second determines whether the block is a unit (which is a block, too), a structured non-unit block or a topological filter. If it is a topological filter, the real-valued filter-factor is given. <p> We do not believe that using named terms is actually necessary, because non-structured networks can be approximated by structured networks; for example, if only one edge is omitted in figure 9, the network can again be expressed as a term. With respect to the Theorem of Cybenko <ref> [1] </ref> we have reason to hope that every function that is learned by a non-structured network can be learned by a similar structured network as well.
Reference: [2] <author> D. E. Rummelhart, G. E. Hinton, R. J. Williams: </author> <title> "Learning Internal Representations by Error Propagation", Parallel Distributed Processing: Explorations in Microstructures of Cognition, Vol I, </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: Theorem of Cybenko [1]), no universal algorithm that determines the network's parameters is known. Given an architecture of an ANN, i.e. the number of units and their connectivity, the weights can be adapted by several training algorithms, e.g. backpropagation <ref> [2] </ref>. Yet, the architecture decides whether a set of weights can be found by the BP algorithm. Genetic algorithms (Goldberg [3]) can be used to find or optimize network structures. A similar optimization strategy is evolution strategy (Rechenberg [4] [5]). <p> Termination criteria are the error on the training set and an upper bound to the number of BP epochs. The following enhancements to "traditional" back-propagation (e.g. <ref> [2] </ref>) are used: * Dynamic adaptation of the (learning rate) and ff (momentum) parameters after Salomon [10].
Reference: [3] <author> D. E. Goldberg: </author> <title> "Genetic Algorithms in Search, Optimization, and Machine Learning", </title> <publisher> Addison-Wesley, </publisher> <address> Reading, </address> <year> 1989 </year>
Reference-contexts: Given an architecture of an ANN, i.e. the number of units and their connectivity, the weights can be adapted by several training algorithms, e.g. backpropagation [2]. Yet, the architecture decides whether a set of weights can be found by the BP algorithm. Genetic algorithms (Goldberg <ref> [3] </ref>) can be used to find or optimize network structures. A similar optimization strategy is evolution strategy (Rechenberg [4] [5]). Evolution strategy is better understood and a more extensive theory is available, parts of which can be applied to the similar genetic optimization strategy.
Reference: [4] <editor> I. Rechenberg: "Evolutionsstrategie", Frommann-Holzboog, </editor> <address> Stuttgart, </address> <year> 1973 </year>
Reference-contexts: Yet, the architecture decides whether a set of weights can be found by the BP algorithm. Genetic algorithms (Goldberg [3]) can be used to find or optimize network structures. A similar optimization strategy is evolution strategy (Rechenberg <ref> [4] </ref> [5]). Evolution strategy is better understood and a more extensive theory is available, parts of which can be applied to the similar genetic optimization strategy.
Reference: [5] <author> I. </author> <title> Rechenberg: </title> <type> "Evolutionsstrategie '94", </type> <institution> Frommann-Holzboog, Stuttgart, </institution> <year> 1994 </year>
Reference-contexts: Yet, the architecture decides whether a set of weights can be found by the BP algorithm. Genetic algorithms (Goldberg [3]) can be used to find or optimize network structures. A similar optimization strategy is evolution strategy (Rechenberg [4] <ref> [5] </ref>). Evolution strategy is better understood and a more extensive theory is available, parts of which can be applied to the similar genetic optimization strategy.
Reference: [6] <author> J. R. Koza, J. P. Rice: </author> <title> "Genetic Generation of Both the Weights and Architecture for a Neural Network", </title> <journal> IEEE Press, </journal> <volume> Vol II, </volume> <year> 1991 </year>
Reference-contexts: Jorn Hopf (Editor) Genetic Algorithms within the Framework of Neural Computation: Proceedings of the KI94-Workshop, Max-Planck-Institut fur Informatik, Saarbrucken, 1994 There are two possible ways of optimizing ANNs by means of genetic algorithms: Architecture and weights can be optimized simultaneously (e.g. Koza and Rice <ref> [6] </ref>), or only architecture is optimized by a genetic algorithm, while the weights are adapted by a local search strategy like back-propagation for each individual (e. g. Harp [7]).
Reference: [7] <author> S. A. Harp, T. Samad, A. Guha: </author> <title> "Designing Application-Specific Neural Network Structure Using the Genetic Algorithm", </title> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kauf-mann, </publisher> <address> San Mateo, </address> <year> 1990 </year>
Reference-contexts: Koza and Rice [6]), or only architecture is optimized by a genetic algorithm, while the weights are adapted by a local search strategy like back-propagation for each individual (e. g. Harp <ref> [7] </ref>). Due to the results of the theory of evolution strategy, we focused on the second approach. 1.1 Genetic Algorithms Genetic algorithms optimize a population of individuals (e.g.
Reference: [8] <author> U. Utecht, K. Trint: </author> <title> "Mutation Operators for Structure Evolution of Neural Networks", proposed to be published in Proceedings of the Third Conference on Parallel Problem Solving from Nature, </title> <address> Jerusalem, </address> <year> 1994 </year>
Reference-contexts: Although it is possible to define the mutation operators on the phenotype level (Utecht and Trint <ref> [8] </ref>), the individuals are usually represented by a genetic code. The mutation and crossover operators are then defined on that code. This representation of an individual is called its genotype.
Reference: [9] <author> F. Gruau: </author> <title> "Cellular Encoding as a Graph Grammar", </title> <journal> ISM, </journal> <volume> 7/93 </volume>
Reference-contexts: Various forms of genetic representation of ANNs have been invented. They can be divided into direct encoding schemes, which encode complete and detailed information about the network's architecture, and indirect encoding schemes, which encode either rules for the generation of the phenotype <ref> [9] </ref> or only those parts of the network's architecture that are considered to be of relevance. Direct encoding schemes, the simplest of which is the direct encoding of the interconnection matrix, tend to result in large and redundant genotypes.
Reference: [10] <author> R. Salomon: "Verbesserung konnektionistischer Lernverfahren, </author> <title> die nach der Gradientenmethode arbeiten", </title> <type> Dissertation, </type> <institution> Technical University of Berlin, </institution> <year> 1991 </year>
Reference-contexts: Termination criteria are the error on the training set and an upper bound to the number of BP epochs. The following enhancements to "traditional" back-propagation (e.g. [2]) are used: * Dynamic adaptation of the (learning rate) and ff (momentum) parameters after Salomon <ref> [10] </ref>. This method removes the necessity of choosing the "correct" parameters by hand and is capable of adjusting the parameters for varying local properties of the error space. * Gradient normalization, i.e. the steps in the weight space depend on the gradient's direction but not on its magnitude.
Reference: [11] <author> S. E. Fahlmann, C. Lebiere: </author> <booktitle> "The Cascade-Correlation Learning Architecture", Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, </address> <year> 1990 </year>
Reference-contexts: Actually, some results indicate that short-cut connections could be necessary to succeed, and standard BP has been reported to require at least 20,000 cycles for a solution (Fahlmann, Lebiere <ref> [11] </ref>). The problem can be outlined as follows: There are two interlocking spirals in the input plane, each going around a common centre point three times.
Reference: [12] <author> Y. Le Cun, B. Boser, J. S. Denker, D. Hendersen, R.E Howard, W. Hubbard, L. D. Jackel: </author> <title> "Backpropagation Applied to Handwritten Zip Code Recognition"; Neural Computation 1, </title> <journal> pp. </journal> <volume> 541ff, </volume> <year> 1989 </year>
Reference-contexts: The ZIP code reader by LeCun et al. <ref> [12] </ref> can be considered a suitable real-world problem. The network consists of 1256 units and more than 50,000 edges, most of them with shared weights. The units are arranged in overlapping feature detectors in a regular but non-trivial pattern.
Reference: [13] <author> A. Zell, N. Mache, R. Hubner, M. Schmalzl, T. Som-mer, G. Mamier, M. Vogt: </author> <title> "SNNS User Manual", </title> <institution> University of Stuttgart, </institution> <type> Report 8/92 </type>
Reference-contexts: The network consists of 1256 units and more than 50,000 edges, most of them with shared weights. The units are arranged in overlapping feature detectors in a regular but non-trivial pattern. While the interconnection matrix takes several megabytes of memory and the BIGNET-specification <ref> [13] </ref> still fills a couple of pages, the term representation is quite handy: ser ( mul (12; ser (tf (0:3125); smul (8; ser (tf (0:3125); smul (8; 1))))); tf (0:6667); mul (12; ser (tf (0:625); smul (4; ser (tf (0:625); smul (4; 1))))); 30; 10 Modelling networks can be made easier
References-found: 13

