URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P622.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts96.htm
Root-URL: http://www.mcs.anl.gov
Title: SUPERLINEAR CONVERGENCE OF AN INTERIOR-POINT METHOD DESPITE DEPENDENT CONSTRAINTS  
Author: DANIEL RALPH AND STEPHEN J. WRIGHT 
Note: AMS(MOS) subject classifications. 90C33, 90C30, 49M45  
Affiliation: MATHEMATICS AND COMPUTER SCIENCE DIVISION, ARGONNE NATIONAL LABORATORY  
Pubnum: PREPRINT ANL/MCS-P622-1196,  
Abstract: We show that an interior-point method for monotone variational inequalities exhibits superlinear convergence provided that all the standard assumptions hold except for the well-known assumption that the Jacobian of the active constraints has full rank at the solution. We show that superlinear convergence occurs even when the constant rank condition on the Jacobian assumed in an earlier work does not hold. 1. Introduction. We consider the following monotone variational inequality 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. F. Bonnans, </author> <title> Local study of Newton type algorithms for constrained problems, </title> <booktitle> in Optimization-Fifth French-German Conference, </booktitle> <editor> S. Dolecki, ed., </editor> <volume> no. </volume> <booktitle> 1405 in Lecture Notes in Mathematics, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1989, </year> <pages> pp. </pages> <month> 13-24. </month> <title> SUPERLINEAR CONVERGENCE WITHOUT NONDEGENERACY 15 </title>
Reference-contexts: A formal statement of these assumptions and further details are given in Section 2.2. Superlinear convergence has been proved for other methods for nonlinear programming without the strict complementarity assumption, but these results typically require the Jacobian of active constraints to have full rank (see Pang [5], Bonnans <ref> [1] </ref>, and Facchinei, Fischer, and Kanzow [2]). Possibly the best known application of (1) is the convex programming problem defined by min OE (z) subject to z 2 C;(4) where OE : IR n ! IR is C 2 and convex. Let = DOE. <p> The safe-step procedure is defined as follows. safe (z; ; y; t; fl; fi): choose ~oe 2 [oe; 1 2 ], ff 0 2 <ref> [ ff; 1] </ref>; solve (11) to find (z; ; y); choose ff to be the first element in the sequence ff 0 ; Off 0 ; O 2 ff 0 ; : : :, such that the following conditions are satisfied: i (ff)y i (ff) fl (ff); kr g (z (ff);
Reference: [2] <author> F. Facchinei, A. Fischer, and C. Kanzow, </author> <title> On the accurate identification of active constraints, </title> <type> tech. rep., </type> <institution> Universita' di Roma "La Sapienza", </institution> <year> 1996. </year>
Reference-contexts: Superlinear convergence has been proved for other methods for nonlinear programming without the strict complementarity assumption, but these results typically require the Jacobian of active constraints to have full rank (see Pang [5], Bonnans [1], and Facchinei, Fischer, and Kanzow <ref> [2] </ref>). Possibly the best known application of (1) is the convex programming problem defined by min OE (z) subject to z 2 C;(4) where OE : IR n ! IR is C 2 and convex. Let = DOE.
Reference: [3] <author> P. T. Harker and J.-S. Pang, </author> <title> Finite-dimensional variational inequality and nonlinear complementarity problems: A survey of theory, </title> <booktitle> algorithms and applications, Mathematical Programming, 48 (1990), </booktitle> <pages> pp. 161-220. </pages>
Reference-contexts: nonlinear complementarity (NCP) problem: Find the vector triple (z; ; y) 2 IR n+2m such that y = f (z; ) where f : IR n+m ! IR n is the C 1 function defined by f (z; ) = (z) + Dg (z) T :(3) It is well known <ref> [3] </ref> that, under suitable conditions on g such as the Slater constraint qualification, z solves (1) if and only if there exists a multiplier such that (z; ) solves (2).
Reference: [4] <author> R. D. C. Monteiro and S. J. Wright, </author> <title> Local convergence of interior-point algorithms for degenerate monotone LCP, </title> <journal> Computational Optimization and Applications, </journal> <volume> 3 (1994), </volume> <pages> pp. 131-155. </pages>
Reference-contexts: The strict complementarity condition is essential for superlinear convergence in a number of contexts besides NCP and nonlinear programming. See, for example Wright [8, Chapter 7] for an analysis of linear programming and Monteiro and Wright <ref> [4] </ref> for asymptotic properties of interior-point methods for monotone linear complementarity problems.
Reference: [5] <author> J.-S. Pang, </author> <title> Convergence of splitting and Newton methods for complementarity problems: An application of some sensitivity results, </title> <journal> Mathematical Programming, </journal> <volume> 58 (1993), </volume> <pages> pp. 149-160. </pages>
Reference-contexts: A formal statement of these assumptions and further details are given in Section 2.2. Superlinear convergence has been proved for other methods for nonlinear programming without the strict complementarity assumption, but these results typically require the Jacobian of active constraints to have full rank (see Pang <ref> [5] </ref>, Bonnans [1], and Facchinei, Fischer, and Kanzow [2]). Possibly the best known application of (1) is the convex programming problem defined by min OE (z) subject to z 2 C;(4) where OE : IR n ! IR is C 2 and convex. Let = DOE.
Reference: [6] <author> D. Ralph and S. J. Wright, </author> <title> Superlinear convergence of an interior-point method for monotone variational inequalities, </title> <type> Preprint MCS-P556-0196, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <month> January </month> <year> 1996. </year> <title> To appear in Proceedings of International Conference on Complementarity Problems: Engineering and Economic Applications and Computational Methods, </title> <publisher> SIAM, </publisher> <year> 1997. </year>
Reference-contexts: To show superlinear (local) convergence in methods for nonlinear programs, on eusually makes several assumptions with regard to the solution point. Until recently, these assumptions included (local) uniqueness of the solution (z; ; y). This uniqueness condition was relaxed somewhat in <ref> [6] </ref> to allow for several multipliers corresponding fl Department of Mathematics, The University of Melbourne, Parkville, Victoria 3052, Australia. The work of this author was supported by the Australian Research Council. y Mathematics and Computer Science Division, Argonne National Laboratory, 9700 South Cass Avenue, Argonne, Illinois 60439, U.S.A. <p> The point of this article is to show that superlinear convergence holds in the previous setting <ref> [6] </ref> even when the constant rank condition does not hold. This result lends theoretical support to our numerical observations [6, Section 7]. <p> The point of this article is to show that superlinear convergence holds in the previous setting [6] even when the constant rank condition does not hold. This result lends theoretical support to our numerical observations <ref> [6, Section 7] </ref>. Moreover, we believe that the superlinear convergence result can be shown for other interior-point methods whose search directions are asymptotically the same as the pure Newton (affine-scaling) direction defined below (6). <p> If a constraint qualification holds, then solutions of (4) correspond, via Lagrange multipliers, to solutions of (2)-(3) and, in addition, solutions of (1) and (4) coincide. We consider the solution of (1) by the interior-point algorithm of Ralph and Wright <ref> [6] </ref>, which is in turn a natural extension of the safe-step/fast-step algorithm of Wright [7] for monotone linear complementarity problems. <p> SUPERLINEAR CONVERGENCE WITHOUT NONDEGENERACY 3 The duality measure defined by = T y=m is used frequently in our analysis as a measure of nonoptimality and infeasibility. To extend the superlinear convergence result of <ref> [6] </ref> without a constant rank condition on the active constraint Jacobian, we show that the affine-scaling step defined by (6) has size O (). Hence, the superlinearity result can be extended to most algorithms that take near-unit steps along directions that are asymptotically the same as the affine-scaling direction. <p> Hence, the superlinearity result can be extended to most algorithms that take near-unit steps along directions that are asymptotically the same as the affine-scaling direction. Since we are extending our work in <ref> [6] </ref>, much of the analysis in that earlier paper, much of the analysis in the earlier work carries over without modification to the present case, and we omit many of the details here. <p> The Algorithm. In this section, we review the notation, assumptions, and the statement of the algorithm from Ralph and Wright <ref> [6] </ref>. We also state the main global and superlinear convergence results, which differ from the corresponding theorems in [6] only in the absence of the constant rank assumption. 2.1. Notation and Terminology. <p> The Algorithm. In this section, we review the notation, assumptions, and the statement of the algorithm from Ralph and Wright <ref> [6] </ref>. We also state the main global and superlinear convergence results, which differ from the corresponding theorems in [6] only in the absence of the constant rank assumption. 2.1. Notation and Terminology. <p> The notation Dg fl refers to Dg (z fl ). 2.2. Assumptions. Here we give a formal statement of the assumptions needed for global and superlinear convergence. Some motivation is given here, but we refer the reader to the earlier paper <ref> [6] </ref> for further details. The first assumption ensures that the mapping f defined by (3) is monotone with respect to z and therefore that the mapping (z; ) ! (f (z; ); g (z)) is monotone. <p> Next, we make a smoothness assumption on and g in the neighborhood of the first component z fl of the strictly complementary solution from Assumption 4. (We show in <ref> [6, Lemma 4.2] </ref> that, under this assumption, z fl is the first component of all solutions.) Assumption 5. The matrix-valued functions D and D 2 g i , i = 1; 2; : : :; m are Lipschitz continuous in a neighborhood of z fl . <p> In the statements of our results, we refer to a set of "standing assumptions," which we define as follows: Standing Assumptions: Assumptions 1-6, together with an assumption that the algorithm of Ralph and Wright <ref> [6] </ref> applied to the problem (2) generates an infinite sequence f (z k ; k ; y k )g with a limit point. Along with Assumptions 1-6, the superlinear convergence result in Ralph and Wright [6] requires a constant rank constraint qualification to hold. <p> Standing Assumptions: Assumptions 1-6, together with an assumption that the algorithm of Ralph and Wright <ref> [6] </ref> applied to the problem (2) generates an infinite sequence f (z k ; k ; y k )g with a limit point. Along with Assumptions 1-6, the superlinear convergence result in Ralph and Wright [6] requires a constant rank constraint qualification to hold. <p> T j z 2 U g with H k ! H fl = Dg B (z fl ) T and all index sets J ae f1; 2; : : : ; jBjg, we have that rank H k J ! rank H fl J : However, in the analysis of <ref> [6] </ref>, this assumption is not invoked until Section 5.4, so we are justified in reusing many results from earlier sections of that paper here. Indeed, SUPERLINEAR CONVERGENCE WITHOUT NONDEGENERACY 5 we also reuse results from later sections of [6] by applying them to constant matrices (which certainly satisfy the constant rank <p> J ! rank H fl J : However, in the analysis of <ref> [6] </ref>, this assumption is not invoked until Section 5.4, so we are justified in reusing many results from earlier sections of that paper here. Indeed, SUPERLINEAR CONVERGENCE WITHOUT NONDEGENERACY 5 we also reuse results from later sections of [6] by applying them to constant matrices (which certainly satisfy the constant rank condition). <p> We assume that the sequence of iterates has a limit point, which we denote by (z fl ; ^ ; y fl ):(9) By <ref> [6, Theorem 3.2] </ref>, we have that (z fl ; ^ ; y fl ) 2 S. <p> We perform a backtracking line search along this direction, stopping when we identify a value of ff k that achieves a "sufficient decrease" in without leaving the set k+1 . The algorithm is parametrized by the following quantities whose roles are ex plained more fully in <ref> [6] </ref>. O 2 (0; 1); oe 2 (0; 1 fi min &gt; 0; fi max = fi min exp (3=2); 0 &lt; fl min &lt; fl max 1 fl 2 (0; 1 2 fl) 1=^o ; 1 )); where exp () is the exponential function. <p> Convergence of the Algorithm. The algorithm converges globally ac cording to the following theorem. Theorem 2.1. (Ralph and Wright <ref> [6, Theorem 3.2] </ref>) Suppose that Assumptions 1 and 2 hold. Then either (A) (z k ; k ; y k ) 2 S for some k &lt; 1, or (B) all limit points of f (z k ; k ; y k )g belong to S. <p> Here, however, our focus is on the following local superlinear convergence theorem. It is simply a restatement of <ref> [6, Theorem 3.3] </ref> without the constant rank condition on the active constraint Jacobian matrix [6, Assumption 7]. Theorem 2.2. <p> Here, however, our focus is on the following local superlinear convergence theorem. It is simply a restatement of [6, Theorem 3.3] without the constant rank condition on the active constraint Jacobian matrix <ref> [6, Assumption 7] </ref>. Theorem 2.2. Suppose that Assumptions 1, 2, 3, 4, 5, and 6 are satisfied and that the sequence f (z k ; k ; y k )g is infinite, with a limit point (z fl ; ^ ; y fl ) 2 S. <p> Our basic strategy for proving the estimate (12) is as follows. From <ref> [6, Section 5.3] </ref>, we have without assuming the constant rank condition that (t 0 ; u 0 ; v 0 ) = O () for all (z; ; y) 2 S (ffi), where ffi 2 (0; 1) is a positive constant. <p> The constant rank assumption is, however, needed in <ref> [6] </ref> to prove that the other step component (t; u; v) is also O (). <p> Our first result, proved in the earlier paper <ref> [6] </ref>, collects some bounds that are useful throughout this section. Lemma 3.1. [6, Lemma 5.1] Suppose that the standing assumptions hold. <p> Our first result, proved in the earlier paper [6], collects some bounds that are useful throughout this section. Lemma 3.1. <ref> [6, Lemma 5.1] </ref> Suppose that the standing assumptions hold. <p> Lemma 3.2. (cf. <ref> [6, Lemma 5.2] </ref>) Suppose that the standing assumptions are satisfied. <p> Proof. We claim first that the right-hand-side components of (17) and (19) are O (). From <ref> [6, Equation (78)] </ref>, we have that k (z; ) (z fl ; )k C 2;1 ; for all (z; ; y) 2 S (ffi 2 );(26) for some positive constants C 2;1 and ffi 2 2 (0; 1). <p> For the remaining right-hand-side component in (17), we have trivially that kflY ek 1 = m: Consider now the system (17). As in the proof of <ref> [6, Lemma 5.2] </ref>, we have that = O (ffi 3 ) for all (z; ; y) 2 S (ffi 3 ). <p> others following make use of the positive diagonal matrix D defined by D = fl 1=2 Y 1=2 :(31) From Lemma 3.1, there is a constant C 3 such that kDk C 3 1=2 ; kD 1 k C 3 1=2 ;(32) SUPERLINEAR CONVERGENCE WITHOUT NONDEGENERACY 11 Lemma 3.3. (cf. <ref> [6, Lemma 5.3] </ref>) Suppose that the standing assumptions are satisfied. Then for ^ ffi defined in Lemma 3.2, there is C 4 &gt; 0 such that kD c k C 4 1=2 ; kD 1 c yk C 4 1=2 ;(33) Proof. <p> First, let ^ ffi be defined in Lemma 3.2, but adjusted if necessary to ensure that (z; ; y) 2 S ( ^ ffi) ) 1:(34) The proof closely follows that of <ref> [6, Lemma 5.3] </ref>, but we spell out the details here because the analytical techniques are also needed in a later result (Theorem 3.8). <p> Since D 1 ~v = D ~u, the second part of (33) follows likewise. Bounds on some of the components of c and c y follow easily from Lemma 3.3. Theorem 3.4. (cf. <ref> [6, Theorem 5.4] </ref>) Suppose that the standing assumptions are satisfied. Then there are positive constants ^ ffi and C 5 such that k c N k C 5 ; k c y B k C 5 :(38) Proof. Let ^ ffi be as defined in Lemma 3.3. <p> Hence, by using (22), we obtain j c i j i 1=2 C 1 1=2 1=2 C 4 1=2 ; which proves that k c N k C 5 for an obvious choice of C 5 . The bound on c y B is derived similarly. Lemma 3.5. (cf. <ref> [6, Lemma 5.10] </ref>) Let ; 6= J ae B and ; 6= K ae N . <p> In addition, we have that dim ker (D z f) (Dg fl Dg fl 0 I K = dim ker (Dg fl Proof. This result differs from <ref> [6, Lemma 5.10] </ref> only in that z fl replaces z as the argument of Dg (). The proof is essentially unchanged. <p> The value ~ ffi = min ( ffi; ^ ffi), with ffi from (39) and ^ ffi from Theorem 3.4, suffices to prove this result. The technique of proof is by now familiar (it follows the proof of <ref> [6, Theorem 5.12] </ref> closely), and we omit the details. At this point, we have proved the first estimate in (21), as we summarize in the following theorem. Theorem 3.7. Suppose that the standing assumptions hold. <p> that there is a radius ffi 4 2 (0; ~ ffi) such that 2 (Dg Dg fl )( ) + (Dg Dg fl ) T c 0 5 = O ( 2 ); all (z; ; y) 2 S (ffi 4 ):(43) The remainder of the proof follows that of <ref> [6, Lemma 5.7] </ref>. <p> Proof. We have from Theorems 3.7 and 3.8 that (t; u; v) = O () for ffi defined as in Theorem 3.8. Moreover, it follows directly from <ref> [6, Section 5.3] </ref> that (t 0 ; u 0 ; v 0 ) = O (), possibly after some adjustment of ffi. Hence, the result follows from (14). 4. Conclusions. <p> Hence, the result follows from (14). 4. Conclusions. The result proved here explains the numerical experience reported in Section 7 of Ralph and Wright <ref> [6] </ref>, in which the convergence behavior of our test problems seemed to be the same regardless of whether the active constraint Jacobian satisfied the constant rank condition. We speculated in [6] about possible relaxation of the constant rank condition and have verified in this article that, in fact, this condition can <p> Conclusions. The result proved here explains the numerical experience reported in Section 7 of Ralph and Wright <ref> [6] </ref>, in which the convergence behavior of our test problems seemed to be the same regardless of whether the active constraint Jacobian satisfied the constant rank condition. We speculated in [6] about possible relaxation of the constant rank condition and have verified in this article that, in fact, this condition can be dispensed with altogether. Our results are possibly the first proofs of superlinear convergence in nonlinear programming without multiplier nondegeneracy or uniqueness.
Reference: [7] <author> S. J. Wright, </author> <title> A path-following interior-point algorithm for linear and quadratic optimization problems, </title> <journal> Annals of Operations Research, </journal> <volume> 62 (1996), </volume> <pages> pp. 103-130. </pages> <note> [8] , Primal-Dual Interior-Point Methods, SIAM, 1996. To appear. </note>
Reference-contexts: We consider the solution of (1) by the interior-point algorithm of Ralph and Wright [6], which is in turn a natural extension of the safe-step/fast-step algorithm of Wright <ref> [7] </ref> for monotone linear complementarity problems.
References-found: 7

