URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P204.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts91.htm
Root-URL: http://www.mcs.anl.gov
Title: Exploiting Parallelism in Automatic Differentiation  
Author: Christian Bischof Andreas Griewank David Juedes E. Houstis and Y. Muraoka, Eds., 
Date: 146-153, 1991.  
Note: Argonne Preprint MCS-P204-0191 Published in Proc. 1991 Int. Conf. on Supercomputing,  ACM Press, Baltimore, Md., pp.  
Address: Argonne, Illinois 60439-4801  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: The numerical methods employed in the solution of many scientific computing problems require the computation of first- or second-order derivatives of a function f : R n ! R m . We present an approach that, given a serial C program for the computation of f(x), derives a parallel execution schedule for the computation of f and its derivatives in a completely automatic fashion. This is achieved by overloading the computation of f (x) in C++ to obtain a trace of the computations to be performed and then transforming this trace into a data flow graph for the computation of f(x). In addition to the computation of f (x), this graph also allows us to exactly and inexpensively compute derivates of f by the repeated use of the chain rule. Parallelism is exploited in two ways: rows or columns of derivative matrices can be computed by independent passes through the computational graph, and parallelism within the processing of this computational graph can be exploited by processing independent subgraphs concurrently. We present experimental results that show that good performance on shared-memory machines can be obtained by using a graph interpreter approach. We then present some ideas that are currently under development for improving computational granularity and for implementing parallel automatic differentiation schemes in a portable and more efficient fashion. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Baur and V. Strassen. </author> <title> The complexity of partial derivatives. </title> <journal> Theoretical Computer Science, </journal> <volume> 22 </volume> <pages> 317-330, </pages> <year> 1983. </year>
Reference-contexts: Speelpenning [24] mentioned and Baur and Strassen <ref> [1] </ref> later published a proof that the number of operations required to compute a scalar function and its partial derivatives is bounded above by a fixed constant times the number of operations required to compute the function. This theoretical result leads to the more efficient reverse mode of derivative evaluation.
Reference: [2] <author> Christian H. Bischof. </author> <title> A parallel QR factorization algorithm with controlled local pivoting. </title> <type> Technical Report ANL/MCS-P21-1088, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1988. </year>
Reference-contexts: These issues are discussed in more detail in [12]. The situation is even more complicated if one wishes to exploit parallelism. Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines <ref> [2, 3, 21, 23] </ref>. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful [7, 19, 22], but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information.
Reference: [3] <author> Christian H. Bischof and Per Christian Hansen. </author> <title> Structure-preserving and rank-revealing QR factorizations. </title> <type> Technical Report MCS-P100-0989, </type> <institution> 7 Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <month> September </month> <year> 1989. </year>
Reference-contexts: These issues are discussed in more detail in [12]. The situation is even more complicated if one wishes to exploit parallelism. Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines <ref> [2, 3, 21, 23] </ref>. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful [7, 19, 22], but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information.
Reference: [4] <author> Christian H. Bischof and Brad N. Karp. </author> <title> Increasing the granularity of parallelism and reducing contention in automatic differentiation. </title> <type> Technical Report MCS-TM-142, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <month> November </month> <year> 1990. </year>
Reference-contexts: We call this operation hoisting. In general, we can hoist a node n into a node p if p is the only node that uses the result computed by n, 4 and p represents a unary operation. Details on how this transformation is implemented can be found in <ref> [4] </ref>. These transformations can have a significant effect on the number of nodes in the graph. For the Bratu problem, a classical problem in combustion modeling, the ADOL-C tape contained 1,142 nodes, of which 184 were assignments.
Reference: [5] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. i. nonlinear functional analysis approach. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2794-2802, </pages> <year> 1981. </year>
Reference: [6] <author> D. G. Cacuci. </author> <title> Sensitivity theory for nonlinear systems. ii. extension to additional classes of responses. </title> <journal> Journal of Mathematical Physics, </journal> <volume> 22(12) </volume> <pages> 2803-2812, </pages> <year> 1981. </year>
Reference: [7] <author> T. F. Coleman and J. J. </author> <title> More. Estimation of sparse Jacobian matrices and graph coloring problems. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 187-209, </pages> <year> 1983. </year>
Reference-contexts: Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines [2, 3, 21, 23]. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful <ref> [7, 19, 22] </ref>, but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information. This approach computes derivative information without truncation error, and in an automatic fashion. That is, a user can take a black-box view of the differentiation process.
Reference: [8] <author> Thomas F. Coleman. </author> <title> Large Sparse Numerical Optimization, </title> <booktitle> volume 165 of Lecture Notes in Computer Science. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivates of some objective function. Probably best known are gradient methods for optimization and New-ton's method for the solution of nonlinear systems <ref> [8, 10] </ref>. Other examples can be found in [9]. For example, fl This work was supported by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U. S. Department of Energy, under Contract W-31-109-Eng-38. The third author was also supported through NSF Cooperative Agreement No.
Reference: [9] <author> George F. Corliss. </author> <title> Applications of differentiation arithmetic. </title> <booktitle> In Reliability in Computing, </booktitle> <pages> pages 127-148. </pages> <publisher> Academic Press, </publisher> <year> 1988. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivates of some objective function. Probably best known are gradient methods for optimization and New-ton's method for the solution of nonlinear systems [8, 10]. Other examples can be found in <ref> [9] </ref>. For example, fl This work was supported by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U. S. Department of Energy, under Contract W-31-109-Eng-38. The third author was also supported through NSF Cooperative Agreement No.
Reference: [10] <author> John Dennis and Robert Schnabel. </author> <title> Numerical Methods for Unconstrained Optimization and Nonlin&lt;ear Equations. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction The methods employed for the solution of many scientific computing problems require the evaluation of derivates of some objective function. Probably best known are gradient methods for optimization and New-ton's method for the solution of nonlinear systems <ref> [8, 10] </ref>. Other examples can be found in [9]. For example, fl This work was supported by the Applied Mathematical Sciences subprogram of the Office of Energy Research, U. S. Department of Energy, under Contract W-31-109-Eng-38. The third author was also supported through NSF Cooperative Agreement No.
Reference: [11] <author> L. C. W. Dixon. </author> <title> Automatic differentiation and parallel processing in optimization. </title> <type> Technical Report No. 176, </type> <institution> The Hatfield Polytechnic, Hatfield, U.K., </institution> <year> 1987. </year>
Reference-contexts: Often Jacobi matrices are sparse, and sparse storage techniques can be employed rather advantageously. Then the ratio between the cost of evaluating F 0 and f is bounded by the maximum number of nonzeros in any row of the Jacobian (see for example <ref> [11] </ref>). We also mention that if one does not need J per se, but instead J v for some vector v, the additivity of differentiation allows us to compute this quantity in one pass by initializing d i = v i ; i = 1; : : : ; n.
Reference: [12] <author> Andreas Griewank. </author> <title> On automatic differentiation. </title> <booktitle> In Mathematical Programming: Recent Developments and Applications, </booktitle> <pages> pages 83-108. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Symbolic differentiation techniques currently are often not feasible, since they do not fully utilize common subexpressions, and therefore are com--putationally inefficient. These issues are discussed in more detail in <ref> [12] </ref>. The situation is even more complicated if one wishes to exploit parallelism. Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines [2, 3, 21, 23].
Reference: [13] <author> Andreas Griewank, David Juedes, and Jay Srini-vasan. ADOL-C, </author> <title> a package for the automatic differentiation of algorithms written in C/C++. </title> <type> Technical Report MCS-180-1190, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1990. </year>
Reference-contexts: While the tape itself may be quite large, it is always accessed in a purely sequential fashion, and RAM storage requirements are modest by exploiting the fact that only few temporary variables are active at any given point in time. Details can be found in <ref> [13] </ref>. We mention that the automatic differentiation of computer arithmetic has been investigated since before 1960. Since then there have been various implementations of automatic differentiation. Most of these implementations have concentrated on the simple forward evaluation of derivatives.
Reference: [14] <author> Roger Hoover. </author> <title> Incremental Graph Evaluation. </title> <type> PhD thesis, </type> <institution> Cornell University, Department of Computer Science, </institution> <year> 1987. </year>
Reference-contexts: This assumption is true for most optimization approaches. Even if the control flow should change, chances are that the change would be mostly local, so that incremental computation techniques like those described in <ref> [14] </ref> would be applicable. We have implemented a system that takes the "tape" produced by ADOL-C and converts it into a computational graph.
Reference: [15] <author> J. E. Horwedel, B. A. Worley, E. M. Oblow, and F. G. Pin. </author> <note> GRESS Version 0.0 Users Manual. Technical Report ORNL/TM 10835, </note> <institution> Oak Ridge National Laboratory, Engineering Physics and Mathematics Division, </institution> <year> 1988. </year>
Reference-contexts: This theoretical result leads to the more efficient reverse mode of derivative evaluation. Speelpenning [24], Iri and Kubota [16], and Horwedel et al. <ref> [15] </ref> have all implemented the reverse mode of evaluating derivatives in their respective Fortran precompilers. 3 Exploiting Parallelism While the computation of derivatives has been presented in a strictly serial framework until now, there is actually considerable scope for the exploitation of parallelism.
Reference: [16] <author> M. Iri and K. Kubota. </author> <title> Methods of fast automatic differentiation and applications. </title> <type> Technical Report Research Memorandum 87-0, </type> <institution> Department of Mathematical Engineering and Instrumentation Physics, Faculty of Engineering, University of Tokyo, </institution> <year> 1987. </year>
Reference-contexts: This theoretical result leads to the more efficient reverse mode of derivative evaluation. Speelpenning [24], Iri and Kubota <ref> [16] </ref>, and Horwedel et al. [15] have all implemented the reverse mode of evaluating derivatives in their respective Fortran precompilers. 3 Exploiting Parallelism While the computation of derivatives has been presented in a strictly serial framework until now, there is actually considerable scope for the exploitation of parallelism.
Reference: [17] <author> David Juedes and Andreas Griewank. </author> <title> Implementing automatic differentiation efficiently. </title> <type> Technical Report MCS-TM-140, </type> <institution> Argonne National Laboratory, Mathematics and Computer Sciences Division, </institution> <year> 1990. </year>
Reference-contexts: More difficult is exploitation of parallelism within the graph itself. However, if one evaluates the function or computes its gradient using the reverse mode, only one pass over the graph is performed, and thus this is the only chance for exploiting parallelism. Juedes and Griewank <ref> [17] </ref> produced a parallel implementation for the reverse mode of automatic differentiation on the Sequent Symmetry, a shared-memory multiprocessor. This parallel implementation of the reverse mode traverses the dependency graph, evaluating partial derivatives at each node. The embedded dependency information is inverted during the reverse sweep.
Reference: [18] <author> S. Linnainmaa. </author> <title> Taylor expansion of the accumulated rounding error. </title> <journal> BIT, </journal> <volume> 16 </volume> <pages> 146-160, </pages> <year> 1976. </year>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations, which has been used at least since the late sixties, especially in nuclear engineering [5,6], weather forecasting [25], and even neural networks [26]. The discrete analog used in automatic differentiation was apparently first discovered by Linnainmaa <ref> [18] </ref> in the context of rounding error estimates. Again we associate a scalar d i (say) with each intermediate quantity.
Reference: [19] <author> J. J. </author> <title> More. On the performance of algorithms for large-scale bound constrained problems. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <title> Large-Scale Numerical Optimization. </title> <publisher> SIAM, </publisher> <year> 1991. </year>
Reference-contexts: Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines [2, 3, 21, 23]. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful <ref> [7, 19, 22] </ref>, but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information. This approach computes derivative information without truncation error, and in an automatic fashion. That is, a user can take a black-box view of the differentiation process.
Reference: [20] <author> I. M. Navon and U. Muller. </author> <title> FESW a finite-element Fortran IV program for solving the shallow water equations. </title> <booktitle> Advances in Engineering Software, </booktitle> <volume> 1 </volume> <pages> 77-84, </pages> <year> 1979. </year>
Reference-contexts: For the Bratu problem, a classical problem in combustion modeling, the ADOL-C tape contained 1,142 nodes, of which 184 were assignments. Through elimination of assignments and hoisting, we arrived at a graph representation with 613 nodes, a savings of 46%. A shallow-water model for weather modeling <ref> [20] </ref> contained 281,805 operations on the tape. After eliminating the 61,236 assignments, we eliminated another 29,694 nodes through hoisting, for a final representation with 153,484 nodes again a savings of 46%.
Reference: [21] <author> Paul E. Plassmann. </author> <title> The Parallel Solution of Nonlinear Least-Squares Problems. </title> <type> PhD thesis, </type> <institution> Dept. of Applied Mathematics, Cornell University, </institution> <year> 1990. </year>
Reference-contexts: These issues are discussed in more detail in [12]. The situation is even more complicated if one wishes to exploit parallelism. Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines <ref> [2, 3, 21, 23] </ref>. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful [7, 19, 22], but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information.
Reference: [22] <author> Paul E. Plassmann. </author> <title> Sparse Jacobian estimation and factorization on a multiprocessor. </title> <editor> In T. F. Coleman and Y. Li, editors, </editor> <booktitle> Large-Scale Optimization, </booktitle> <pages> pages 152-179, </pages> <address> Philadelphia, </address> <year> 1990. </year> <note> SIAM. </note>
Reference-contexts: Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines [2, 3, 21, 23]. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful <ref> [7, 19, 22] </ref>, but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information. This approach computes derivative information without truncation error, and in an automatic fashion. That is, a user can take a black-box view of the differentiation process.
Reference: [23] <author> Alex Pothen and Padma Raghavan. </author> <title> Distributed orthogonal factorization: Givens and Householder algorithms. </title> <type> Technical Report CS-87-24, </type> <institution> The Pennsylvania State University, </institution> <year> 1987. </year>
Reference-contexts: These issues are discussed in more detail in [12]. The situation is even more complicated if one wishes to exploit parallelism. Considerable progress has been made in the implementation of linear algebra kernels, such as orthogonal factorizations, on parallel machines <ref> [2, 3, 21, 23] </ref>. With respect to the computation of derivative information, approaches to computing finite-difference approximations in parallel using graph coloring approaches have been successful [7, 19, 22], but again accuracy may be lost. We, in turn, suggest the use of automatic differentiation to compute derivative information.
Reference: [24] <author> B. Speelpenning. </author> <title> Compiling Fast Partial Derivatives of Functions Given by Algorithms. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1980. </year>
Reference-contexts: For scalar functions of the form y = F (x 1 ; :::x n ), the forward evaluation of partial derivatives requires O (n) times the execution time 3 1312 10 8 6 4 y_1 y_2 sqrt * sqrt * / * of the original function. Speelpenning <ref> [24] </ref> mentioned and Baur and Strassen [1] later published a proof that the number of operations required to compute a scalar function and its partial derivatives is bounded above by a fixed constant times the number of operations required to compute the function. <p> This theoretical result leads to the more efficient reverse mode of derivative evaluation. Speelpenning <ref> [24] </ref>, Iri and Kubota [16], and Horwedel et al. [15] have all implemented the reverse mode of evaluating derivatives in their respective Fortran precompilers. 3 Exploiting Parallelism While the computation of derivatives has been presented in a strictly serial framework until now, there is actually considerable scope for the exploitation of
Reference: [25] <author> O. Talagrand and P. Courtier. </author> <title> Variational assimilation of meteorological observations with the adjoint vorticity equation. i: Theory. </title> <editor> Q. J. R. Meteorological Society, </editor> <volume> 113 </volume> <pages> 1311-1328, </pages> <year> 1987. </year>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations, which has been used at least since the late sixties, especially in nuclear engineering [5,6], weather forecasting <ref> [25] </ref>, and even neural networks [26]. The discrete analog used in automatic differentiation was apparently first discovered by Linnainmaa [18] in the context of rounding error estimates. Again we associate a scalar d i (say) with each intermediate quantity.
Reference: [26] <author> P. Werbos. </author> <title> Applications of advances in nonlinear sensitivity analysis. </title> <booktitle> In Systems Modeling and Optimization, </booktitle> <pages> pages 762-777, </pages> <address> New York, 1982. </address> <publisher> Springer Verlag. </publisher> <pages> 8 </pages>
Reference-contexts: This approach is closely related to the adjoint sensitivity analysis for differential equations, which has been used at least since the late sixties, especially in nuclear engineering [5,6], weather forecasting [25], and even neural networks <ref> [26] </ref>. The discrete analog used in automatic differentiation was apparently first discovered by Linnainmaa [18] in the context of rounding error estimates. Again we associate a scalar d i (say) with each intermediate quantity.
References-found: 26

