URL: http://www.cs.columbia.edu/~sal/hpapers/mp.ps
Refering-URL: http://www.cs.columbia.edu/~sal/recent-papers.html
Root-URL: http://www.cs.columbia.edu
Email: mauricio@cs.columbia.edu sal@cs.columbia.edu  
Title: Real-world Data is Dirty: Data Cleansing and The Merge/Purge Problem  
Author: Mauricio A. Hernandez Salvatore J. Stolfo 
Keyword: data cleaning, data cleansing, duplicate elimination, semantic inte gration  
Note: This work has been supported in part by the New York State Science and Technology Foundation through the Center for Advanced Technology in Telecommunications at Polytechnic University, by NSF under grant IRI-94-13847, and by Citicorp. This author is now at the University of Illinois at Springfied. Work at Columbia University was supported by an AT&T Cooperative Research Program Fellowship.  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Abstract: The problem of merging multiple databases of information about common entities is frequently encountered in KDD and decision support applications in large commercial and government organizations. The problem we study is often called the Merge/Purge problem and is difficult to solve both in scale and accuracy. Large repositories of data typically have numerous duplicate information entries about the same entities that are difficult to cull together without an intelligent "equational theory" that identifies equivalent items by a complex, domain-dependent matching process. We have developed a system for accomplishing this Data Cleansing task and demonstrate its use for cleansing lists of names of potential customers in a direct marketing-type application. Our results for statistically generated data are shown to be accurate and effective when processing the data multiple times using different keys for sorting on each successive pass. Combing results of individual passes using transitive closure over the independent results, produces far more accurate results at lower cost. The system provides a rule programming module that is easy to program and quite good at finding duplicates especially in an environment with massive amounts of data. This paper details improvements in our system, and reports on the successful implementation for a "real-world" database that conclusively validates our results previously achieved for statistically generated data. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> ACM. </editor> <booktitle> SIGMOD record, </booktitle> <month> December </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogeneous representations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the literature have been called record linkage [12], the semantic integration problem <ref> [1] </ref> or the instance identification problem [23], and more recently the data cleansing problem regarded as a crucial first step in a KDD/DM process [11]. Business organizations call this problem the merge/purge problem.
Reference: [2] <author> R. Agrawal and H. V. Jagadish. </author> <title> Multiprocessor Transitive Closure Algorithms. </title> <booktitle> In Proc. Int'l Symp. on Databases in Parallel and Distributed Systems, </booktitle> <pages> pages 56-66, </pages> <month> December </month> <year> 1988. </year>
Reference-contexts: The more corrupted the data, more runs might be needed to capture the matching records. The transitive closure, however, is executed on pairs of tuple id's, each at most 30 bits, and fast 15 solutions to compute transitive closure exist <ref> [2] </ref>. From observing real world scenarios, the size of the data set over which the closure is computed is at least one order of magnitude smaller than the corresponding database of records, and thus does not contribute a large cost.
Reference: [3] <author> C. Batini, M. Lenzerini, and S. Navathe. </author> <title> A Comparative Analysis of Methodologies for Database Schema Integration. </title> <journal> ACM Computing Surverys, </journal> <volume> 18(4) </volume> <pages> 323-364, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: The first issue, where databases have different schema, has been addressed extensively in the literature and is known as the schema integration problem <ref> [3] </ref>. We are primarily interested in the second problem: heterogeneous representations of data and its implication when merging or joining multiple datasets.
Reference: [4] <author> D. Bitton and D. J. DeWitt. </author> <title> Duplicate Record Elimination in Large Data Files. </title> <journal> ACM Transactions on Database Systems, </journal> <volume> 8(2) </volume> <pages> 255-265, </pages> <month> June </month> <year> 1983. </year>
Reference-contexts: The problem of merging two or more databases has been tackled in a straightforward 1 fashion by a simple sort of the concatenated data sets followed by a duplicate elimination phase over the sorted list <ref> [4] </ref>. However, when the databases involved are heterogeneous, meaning they do not share the same schema, or that the same real-world entities are represented differently in the datasets, the problem of merging becomes more difficult.
Reference: [5] <author> B. P. Buckles and F. E. Petry. </author> <title> A fuzzy representation of data for relational databases. </title> <journal> Fuzzy Sets and Systems, </journal> <volume> 7 </volume> <pages> 213-226, </pages> <year> 1982. </year> <note> Generally regarded as the paper that originated Fuzzy Databases. </note>
Reference-contexts: We use a rule-based knowledge base to implement an equational theory. The problem of identifying similar instances of the same real-world entity by means of an inexact match has been studied by the Fuzzy Database <ref> [5] </ref> community. Much of the work has concentrated on the problem of executing a query Q over a fuzzy relational database. The answer for Q is the set of all tuples satisfying Q in a non-fuzzy relational database and all tuples that satisfy Q within a threshold value.
Reference: [6] <author> J. P. Buckley. </author> <title> A Hierarchical Clustering Strategy for Very Large Fuzzy Databases. </title> <booktitle> In Proceedings of the IEEE International Conference on Systems, Man and Cybernetics, </booktitle> <pages> pages 3573-3578, </pages> <year> 1995. </year>
Reference-contexts: Elsewhere we have treated the case of clustering in which sorting is replaced by a single-scan process [16]. This clustering resembles the hierarchical clustering strategy proposed in <ref> [6] </ref> to efficiently perform queries over large fuzzy relational databases. However, we demonstrate that, as one may expect, none of these basic approaches alone can guarantee high accuracy. Here, accuracy means how many of the actual duplicates appearing in the data have been matched and merged correctly.
Reference: [7] <author> K. W. Church and W. A. Gale. </author> <title> Probability Scoring for Spelling Correction. </title> <journal> Statistics and Computing, </journal> <volume> 1 </volume> <pages> 93-103, </pages> <year> 1991. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [21, 7, 17] </ref>.
Reference: [8] <author> T. K. Clark. </author> <title> Analyzing Foster Childrens' Foster Home Payments Database. </title> <editor> In KDD Nuggets 95:7 (http://info.gte.com/~kdd/nuggets/95/), Piatetsky-Shapiro, ed., </editor> <year> 1995. </year>
Reference-contexts: The State of Washington Department of Social and Health Services maintains large databases of transactions made over the years with state residents. In March of 1995 the Office of Children Administrative Research (OCAR) of the Department of Social and Health Services posted a request on the KDD-nuggets <ref> [8] </ref> asking for assistance analyzing one of their databases. We answered their request and this section details our results. OCAR analyzes the database of payments by the State to families and businesses that provide services to needy children.
Reference: [9] <author> T. Dietterich and R. Michalski. </author> <title> A Comparative Review of Selected Methods for Learning from Examples. </title> <editor> In R. Michalski, J. Carbonell, and T. Mitchell, editors, </editor> <booktitle> Machine Learning, </booktitle> <volume> volume 1, </volume> <pages> pages 41-81. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1983. </year>
Reference-contexts: In this strategy, the N latest elements are selected as prime-representatives. * Generalization: Generate the prime-representatives by generalizing the data collected from several positive examples (records) of the concept represented by the clus ter. Techniques for generalizing concepts are well known from machine learning <ref> [9, 18] </ref>. * Syntactic: Choose the largest or more complete record. * Utility: Choose the record that matched others more frequently. In this section we present initial results comparing the time and accuracy performance of incremental Merge/Purge with the basic Merge/Purge algorithm.
Reference: [10] <author> R. Dubes and A. Jain. </author> <title> Clustering Techniques: The User's Dilema. </title> <journal> Pattern Recognition, </journal> <volume> 8 </volume> <pages> 247-260, </pages> <year> 1976. </year>
Reference-contexts: Here prime-representatives are a set of records extracted from each cluster of records used to represent the information in its cluster. From the pattern recognition community, we can think of these prime-representatives as analogous to the "cluster centroids" <ref> [10] </ref> generally used to represent clusters of information, or as the base element of an equivalence class. Initially, no previous set of prime-representatives exists and the first increment is just the first input relation. The concatenation step has, therefore, no effect.
Reference: [11] <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> From Data Mining to Knowledge Discovery in Databases. </title> <journal> AI Magazine, </journal> <volume> 17(3), </volume> <month> Fall </month> <year> 1996. </year>
Reference-contexts: Instances of this problem appearing in the literature have been called record linkage [12], the semantic integration problem [1] or the instance identification problem [23], and more recently the data cleansing problem regarded as a crucial first step in a KDD/DM process <ref> [11] </ref>. Business organizations call this problem the merge/purge problem. In this paper we consider the data cleansing of very large databases of information that need to be processed as quickly, efficiently, and accurately as possible. For instance, one month is a typical business cycle in certain direct marketing operations.
Reference: [12] <author> I. Fellegi and A. Sunter. </author> <title> A Theory for Record Linkage. </title> <journal> American Statistical Association Journal, </journal> <pages> pages 1183-1210, </pages> <month> December </month> <year> 1969. </year>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogeneous representations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the literature have been called record linkage <ref> [12] </ref>, the semantic integration problem [1] or the instance identification problem [23], and more recently the data cleansing problem regarded as a crucial first step in a KDD/DM process [11]. Business organizations call this problem the merge/purge problem.
Reference: [13] <author> C. L. Forgy. </author> <title> OPS5 User's Manual. </title> <type> Technical Report CMU-CS-81-135, </type> <institution> Carnegie Mellon University, </institution> <month> July </month> <year> 1981. </year> <month> 35 </month>
Reference-contexts: Functions to compare these complex data types (e.g., sets, images, sound, etc.) could also be used within rules to perform the matching of complex tuples. For the purpose of experimental study, we wrote an OPS5 <ref> [13] </ref> rule program consisting of 26 rules for this particular domain of employee records and was tested repeatedly over relatively small databases of records.
Reference: [14] <author> R. George, F. E. Petry, B. P. Buckles, and R. Srikanth. </author> <title> Fuzzy Database Systems Challenges and Opportunities of a New Era. </title> <journal> International Journal of Intelligent Systems, </journal> <volume> 11 </volume> <pages> 649-659, </pages> <year> 1996. </year>
Reference-contexts: Fuzzy relational databases can explicitly store possibility distributions for each value in a tuple, or use possibility-based relations to determine how strongly records belong to the fuzzy set defined by a query <ref> [14] </ref>. The problem we study in this paper is closely related to the problem studied by the fuzzy database community.
Reference: [15] <author> S. Ghandeharizadeh. </author> <title> Physical Database Design in Multiprocessor Database Systems. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, University of Wisconsin - Madison, </institution> <year> 1990. </year>
Reference-contexts: cost of applying the window-scan method to the data in one block. 3 The 2 comes from the fact that we are counting both read and write operations. 19 Instead of sorting, we could divide the data into C buckets (e.g., hashing the records or using a multi-dimensional partitioning strategy <ref> [15] </ref>). We call this modification the clustering method. Assuming M = C + 1, (1 page for each bucket plus one page for processing an input block), we need one pass over the entire data to partition the records into C buckets (B blocks are read).
Reference: [16] <author> M. Hernandez and S. Stolfo. </author> <title> The Merge/Purge Problem for Large Databases. </title> <booktitle> In Proceedings of the 1995 ACM-SIGMOD Conference, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Elsewhere we have treated the case of clustering in which sorting is replaced by a single-scan process <ref> [16] </ref>. This clustering resembles the hierarchical clustering strategy proposed in [6] to efficiently perform queries over large fuzzy relational databases. However, we demonstrate that, as one may expect, none of these basic approaches alone can guarantee high accuracy. <p> Depending upon the complexity of the rule program and window size w, the last pass may indeed be the dominant cost. We introduced elsewhere <ref> [16] </ref> the means of speeding up this phase by processing "parallel windows" in the sorted list. We note with interest that the sorts of optimizations detailed in the AlphaSort paper [20] may of course be fruitfully applied here. <p> But note we pay a heavy price due to the number of sorts or clusterings of the original large data set. We presented some parallel implementation alternatives to reduce this cost in <ref> [16] </ref>. 3.2.1 Scaling Up Finally, we demonstrate that the sorted-neighborhood method scales well as the size of the database increases. Due to the limitations of our available disk space, we could only grow our databases to about 3,000,000 records. <p> Thus, even though there are some time savings in initially partitioning the data, the savings are small compared to the overall time cost. In <ref> [16] </ref> we describe parallel variants of the basic techniques (including clustering) to show that with a modest amount of "cheap" parallel hardware, we can speed-up the multi-pass approach to a level comparable to the time to do a single-pass approach, but with a very high accuracy, i.e. a few small windows
Reference: [17] <author> K. Kukich. </author> <title> Techniques for Automatically Correcting Words in Text. </title> <journal> ACM Computing Surveys, </journal> <volume> 24(4) </volume> <pages> 377-439, </pages> <year> 1992. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [21, 7, 17] </ref>.
Reference: [18] <author> M. Lebowitz. </author> <title> Not the Path to Perdition: The Utility of Similarity-Based Learning. </title> <booktitle> In Proceedings of 5th National Conference on Artificial Intelligence, </booktitle> <pages> pages 533-537, </pages> <year> 1986. </year>
Reference-contexts: In this strategy, the N latest elements are selected as prime-representatives. * Generalization: Generate the prime-representatives by generalizing the data collected from several positive examples (records) of the concept represented by the clus ter. Techniques for generalizing concepts are well known from machine learning <ref> [9, 18] </ref>. * Syntactic: Choose the largest or more complete record. * Utility: Choose the record that matched others more frequently. In this section we present initial results comparing the time and accuracy performance of incremental Merge/Purge with the basic Merge/Purge algorithm.
Reference: [19] <author> A. Monge and C. Elkan. </author> <title> An Efficient Domain-independent Algorithm for Detecting Approximate Duplicate Database Records. </title> <booktitle> In Proceedings of the 1997 SIGMOD Workshop on Research Issues on DMKD, </booktitle> <pages> pages 23-29, </pages> <year> 1997. </year>
Reference-contexts: The moral is simply that several distinct "cheap" passes over the data produce more accurate results than one "expensive" pass over the data. This result was verified independently by Monge and Elkan <ref> [19] </ref> who recently studied the same problem using a domain-independent matching algorithm as an equational theory. In section 4 we provide a detailed treatment of a real-world data set, provided by the Child Welfare Department of the State of Washington, which was used to establish the validity of these results.
Reference: [20] <author> C. Nyberg, T. Barclay, Z. Cvetanovic, J. Gray, and D. Lomet. AlphaSort: </author> <title> A RISC Machine Sort. </title> <booktitle> In Proceedings of the 1994 ACM-SIGMOD Conference, </booktitle> <pages> pages 233-242, </pages> <year> 1994. </year>
Reference-contexts: In this case, at least three passes would be needed, one pass for conditioning the data and preparing keys, at least a second pass, likely more, for a high speed sort like, for example, the AlphaSort <ref> [20] </ref>, and a final pass for window processing and application of the rule program for each record entering the sliding window. Depending upon the complexity of the rule program and window size w, the last pass may indeed be the dominant cost. <p> We introduced elsewhere [16] the means of speeding up this phase by processing "parallel windows" in the sorted list. We note with interest that the sorts of optimizations detailed in the AlphaSort paper <ref> [20] </ref> may of course be fruitfully applied here. We are more concerned with alternative process architectures that lead to higher accuracies in the computed results while also reducing the time complexity. <p> Thus, we consider alternative metrics for the purposes of merge/purge to include how accurately can you data cleanse for a fixed dollar and given time constraint, rather than the specific cost- and time-based metrics proposed in <ref> [20] </ref>. 2.2 Selection of Keys The effectiveness of the sorted-neighborhood method highly depends on the key selected to sort the records. Here a key is defined to be a sequence of a subset of attributes, or substrings within the attributes, chosen from the record.
Reference: [21] <author> J. J. Pollock and A. Zamora. </author> <title> Automatic spelling correction in scientific and scholarly text. </title> <journal> ACM Computing Surveys, </journal> <volume> 27(4) </volume> <pages> 358-368, </pages> <year> 1987. </year>
Reference-contexts: The errors introduced in the duplicate records range from small typographical mistakes, to complete change of last names and addresses. When setting the parameters for typographical errors, we used known frequencies from studies in spelling correction algorithms <ref> [21, 7, 17] </ref>.
Reference: [22] <author> T. Senator, H. Goldberg, J. Wooton, A. Cottini, A. Umar, C. Klinger, W. Llamas, M. Mar-rone, and R. Wong. </author> <title> The FinCEN Artificial Intelligence System: Identifying Potential Money Laundering from Reports of Large Cash Transactions. </title> <booktitle> In Proceedings of the 7th Conference on Innovative Applications of AI, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: The Justice Department and other law enforcement agencies seek to discover crucial links in complex webs of financial transactions to uncover sophisticated money laundering activities <ref> [22] </ref>. Errors due to data entry mistakes, faulty sensor readings or more malicious activities, provide scores of erroneous datasets that propagate errors in each successive generation of data.
Reference: [23] <author> Y. R. Wang and S. E. Madnick. </author> <title> The Inter-Database Instance Identification Problem in Integrating Autonomous Systems. </title> <booktitle> In Proceedings of the Sixth International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1989. </year> <month> 36 </month>
Reference-contexts: 1 Introduction Merging large databases acquired from different sources with heterogeneous representations of information has become an increasingly important and difficult problem for many organizations. Instances of this problem appearing in the literature have been called record linkage [12], the semantic integration problem [1] or the instance identification problem <ref> [23] </ref>, and more recently the data cleansing problem regarded as a crucial first step in a KDD/DM process [11]. Business organizations call this problem the merge/purge problem.
References-found: 23

