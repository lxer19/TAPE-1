URL: http://iew3.technion.ac.il:8080/~moshet/colearn.ps
Refering-URL: http://iew3.technion.ac.il:8080/~moshet/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Co-Learning and the Evolution of Social Activity  
Author: Yoav Shoham and Moshe Tennenholtz 
Note: This work was supported in part by AFOSR grants F49620-92-J-0547-P00001 and F49620-94-10090, and by NSF grant IRI-9220645.  
Address: Stanford, CA 94305  
Affiliation: Robotics Laboratory Department of Computer Science Stanford University  
Abstract: We introduce the notion of co-learning, which refers to a process in which several agents simultaneously try to adapt to one another's behavior so as to produce desirable global system properties. Of particular interest are two specific co-learning settings, which relate to the emergence of conventions and the evolution of cooperation in societies, respectively. We define a basic co-learning rule, called Highest Cumulative Reward (HCR), and show that it gives rise to quite nontrivial system dynamics. In general, we are interested in the eventual convergence of the co-learning system to desirable states, as well as in the efficiency with which this convergence is attained. Our results on eventual convergence are analytic; the results on efficiency properties include analytic lower bounds as well as empirical upper bounds derived from rigorous computer simulations. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. Altenberg and M. W. Feldman. </author> <title> Selection, Generalized Transmission, and the Evolution of Modifier Genes. I. The reduction principle. </title> <booktitle> Genetics, </booktitle> <pages> pages 559-572, </pages> <month> November </month> <year> 1987. </year>
Reference-contexts: This of course is quite unlike our own framework, the dynamics of the atomic changes are the basis for change, and any statistical properties are derived from these. Work in population genetics <ref> [1] </ref> is closer to ours in this sense. Here we have a set of individuals, each belonging to one of several types.
Reference: [2] <author> R. Axelrod. </author> <title> The Evolution of Cooperation. </title> <address> New York: </address> <publisher> Basic Books, </publisher> <year> 1984. </year>
Reference-contexts: Success: Joint actions in which all agents select the same action are success ful; the others are not. The second game we will consider corresponds to the well known prisoners' dilemma setting, of the sort studied for example in Axelrod's <ref> [2] </ref>. This game is a basic game for the study of cooperation. Definition 4: [the cooperation game, aka prisoners' dilemma]: Denote the actions available to the agents by c (for `cooperate') and d (for `defect'), and the payoff function for a particular agent by u.
Reference: [3] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Occam's Razor. </title> <journal> Information Processing Letters, </journal> <volume> 24 </volume> <pages> 377-380, </pages> <year> 1987. </year>
Reference-contexts: Research in ML is typically concerned with an agent that tries to adapt to an environment. In different areas of research in ML the environment has different structures: it might be a stochastic process that generates examples <ref> [3] </ref>, a teacher [14], a source of reinforcement feedback [13], and so on.
Reference: [4] <author> Bernardo A. Huberman and Tad Hogg. </author> <title> The Behavior of Computational Ecologies. </title> <editor> In Bernardo A. Huberman, editor, </editor> <booktitle> The Ecology of Computation. </booktitle> <publisher> Elsevier Science, </publisher> <year> 1988. </year>
Reference-contexts: These dynamic models have also have found applications in other fields. One is quantitative sociology, where the models have been used to predict opinion shifts over time within large populations [16]. Statistical mechanics also provided the inspiration to Computational Ecology <ref> [4] </ref>; this work is based on the idea that the existence of many agents in an advanced computerized framework creates a "computational ecology." A computational framework, similar in its spirit to quantitative sociology, is developed and analyzed using the tools of statistical mechanics.
Reference: [5] <author> M. Kandori, G. Mailath, and R. Rob. </author> <title> Learning, Mutation and Long Equilibria in Games. </title> <institution> Mimeo. University of Pennsylvania, </institution> <year> 1991. </year>
Reference-contexts: Section 5 is devoted to a discussion about related frameworks. Finally, Section 6 summarizes the main message of the article. 2 Social Games and the HCR Update Rule The basic framework we present shares some features with recent work in game theory (e.g., <ref> [5] </ref>), which in turn was inspired by work in theoretical biology. In spite of this similarity, there are significant differences between the approach we take and work in game theory. <p> We have discussed already in the paper why these notions do not a priori hold any special significance in our setting. The work in economics that is closest in spirit to ours is that on `evolutionarily stable strategies', or ess's <ref> [5] </ref>; this work is also the most strongly influenced by population genetics within economics. A typical setting in this line of research looks as follows. Agents within a given population meet each other randomly, and when they do so they play some particular pre-defined game (such as the prisoners' dilemma).
Reference: [6] <author> R. Kinderman and S. L. Snell. </author> <title> Markov Random Fields and their Applications. </title> <publisher> American Mathematical Society, </publisher> <year> 1980. </year>
Reference-contexts: Statistical mechanics models are a powerful tool for explaining a variety of phenomena in physics. An important family of models in that area goes under the general name of the Ising model <ref> [6] </ref>. In a typical Ising model we have a set of spins, each of which can be in -1/1 state, and which are organized into some fixed spatial arrangement (such as a one-dimensional sequence or a two-dimensional grid).
Reference: [7] <author> David Lewis. </author> <title> Convention, A Philosophical Study. </title> <publisher> Harvard University Press, </publisher> <year> 1969. </year>
Reference-contexts: Intuitively, the first game describes a situation in which the goal is to reach homogeneity in the society, a goal that is reflected in both the evaluation criteria and the payoff structure; it is also a a basic game in Lewis' study of conventions <ref> [7] </ref>. Definition 3: [the convention game]: Denote the payoff function for a particular agent by u. Payoff: u (x) = 1 if the other agent performs x, and 1 otherwise. Success: Joint actions in which all agents select the same action are success ful; the others are not.
Reference: [8] <author> Y. Moses and M. Tennenholtz. </author> <title> On Computational Aspects of Artificial Social Systems. </title> <booktitle> In the Proceedings of DAI-92, </booktitle> <year> 1992. </year>
Reference-contexts: Some of these rules are social laws, designed and imposed ahead of time; traffic laws are an example. Previous work <ref> [8, 11] </ref> investigated some aspects of this off-line design of social law. However, not all rules can be legislated in advance. This is either because the characteristics of the society are unknown, or because they change over time.
Reference: [9] <author> G. Owen. </author> <title> Game Theory (2nd Ed.). </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: In the framework of co-learning there is a natural candidate for the feedback representation. The actions that the agents perform at a particular point can be treated as strategies in a game in the sense of economics (see <ref> [9] </ref>, and definitions in next section), with the agent's payoff at each point being interpreted as its feedback. This game-theoretic representation is quite general, and can be specialized by defining particular types of payoff functions; each restriction on the payoff function defines a particular game type. <p> This gives rise to interesting static notions such as dominance, and participates in the definition of other notions such as Nash equilibrium and Pareto-optimality <ref> [9] </ref>, and of dynamic notions such as evolutionary stable strategies (ess's) [12]. We do not follow this route, and instead stay closer to the spirit of reinforcement learning.
Reference: [10] <author> Y. Shoham and M. Tennenholtz. </author> <title> Emergent Conventions in Multi-Agent Systems: initial experimental results and observations. </title> <booktitle> In Proc. of the 3rd International Conference on Principles of Knowledge Representation and Reasoning, </booktitle> <pages> pages 225-231, </pages> <year> 1992. </year>
Reference-contexts: In this article we investigate the emergence of successful joint actions only in such `faceless masses,' and completely ignore the role of personal identities. We are now ready to start investigating useful action-update rules. In <ref> [10] </ref> we reported on preliminary results of experiments with a number of such rules. Here, however, we will concentrate on one particular update rule, called Highest Cumulative Reward. There are a few reasons we concentrate on this rule. <p> We concentrate on this update rule, although we investigated other update rules that led to interesting phenomena as well (see <ref> [10] </ref>), since it led to the most interesting phenomena and to efficient convention evolution (as will be discussed in 3.4). Unless stated otherwise, the experimental results appearing in this section and in the following section refer to experiments with 100 agents starting with random initial actions.
Reference: [11] <author> Y. Shoham and M. Tennenholtz. </author> <title> On the Synthesis of Useful Social Laws for Artificial Agent Societies. </title> <booktitle> In Proc. of AAAI-92, </booktitle> <pages> pages 276-281, </pages> <year> 1992. </year>
Reference-contexts: Some of these rules are social laws, designed and imposed ahead of time; traffic laws are an example. Previous work <ref> [8, 11] </ref> investigated some aspects of this off-line design of social law. However, not all rules can be legislated in advance. This is either because the characteristics of the society are unknown, or because they change over time.
Reference: [12] <author> John Mayrand Smith. </author> <title> Evolution and the Theory of Games. </title> <publisher> Cambridge University Press, </publisher> <year> 1982. </year> <month> 32 </month>
Reference-contexts: This gives rise to interesting static notions such as dominance, and participates in the definition of other notions such as Nash equilibrium and Pareto-optimality [9], and of dynamic notions such as evolutionary stable strategies (ess's) <ref> [12] </ref>. We do not follow this route, and instead stay closer to the spirit of reinforcement learning. In our framework, the payoff matrix is the designer's way of encoding the feedback given to the agents, but it is not accessible to the agents, nor does it uniquely determine their behavior.
Reference: [13] <author> R.S. Sutton. </author> <title> Special issue on reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <pages> 8(3-4), </pages> <year> 1992. </year>
Reference-contexts: Research in ML is typically concerned with an agent that tries to adapt to an environment. In different areas of research in ML the environment has different structures: it might be a stochastic process that generates examples [3], a teacher [14], a source of reinforcement feedback <ref> [13] </ref>, and so on.
Reference: [14] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Comm. ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: Research in ML is typically concerned with an agent that tries to adapt to an environment. In different areas of research in ML the environment has different structures: it might be a stochastic process that generates examples [3], a teacher <ref> [14] </ref>, a source of reinforcement feedback [13], and so on.
Reference: [15] <author> C.J.C.H. Watkins. </author> <title> Learning With Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cam-bridge University, </institution> <year> 1989. </year>
Reference-contexts: We refer to this update rule as the Highest Cumulative Reward (HCR) update rule. This is perhaps the simplest update rule that comes to mind; certainly it is simpler than update rules in the reinforcement-learning literature, such as Q-learning <ref> [15] </ref>. However, the simultaneous adaptation of the various agents will lead to highly nontrivial behaviors even with this relatively simple rule.
Reference: [16] <author> W. Weidlich and G. Haag. </author> <title> Concepts and Models of a Quantitative Sociology; The Dynamics of Interacting Populations. </title> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: However, both have been augmented to 28 include a dynamical system. These dynamic models have also have found applications in other fields. One is quantitative sociology, where the models have been used to predict opinion shifts over time within large populations <ref> [16] </ref>.
References-found: 16

