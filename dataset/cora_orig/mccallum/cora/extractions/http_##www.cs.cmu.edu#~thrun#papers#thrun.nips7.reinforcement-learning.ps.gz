URL: http://www.cs.cmu.edu/~thrun/papers/thrun.nips7.reinforcement-learning.ps.gz
Refering-URL: http://www.cs.cmu.edu/~thrun/papers/thrun.nips7.reinforcement-learning.html
Root-URL: 
Email: E-mail: thrun@carbon.informatik.uni-bonn.de  Email: schwartz@cs.stanford.edu  
Title: Finding Structure in Reinforcement Learning  
Author: Sebastian Thrun Anton Schwartz G. Tesauro, D. Touretzky, and T. Leen, 
Date: 1995  
Note: to appear in: Advances in Neural Information Processing Systems 7  eds.,  
Address: Romerstr. 164, D-53117 Bonn, Germany  Stanford, CA 94305  
Affiliation: University of Bonn Department of Computer Science III  Dept. of Computer Science Stanford University  
Abstract: Reinforcement learning addresses the problem of learning to select actions in order to maximize one's performance in unknown environments. To scale reinforcement learning to complex real-world tasks, such as typically studied in AI, one must ultimately be able to discover the structure in the world, in order to abstract away the myriad of details and to operate in more tractable problem spaces. This paper presents the SKILLS algorithm. SKILLS discovers skills, which are partially defined action policies that arise in the context of multiple, related tasks. Skills collapse whole action sequences into single operators. They are learned by minimizing the compactness of action policies, using a description length argument on their representation. Empirical results in simple grid navigation tasks illustrate the successful discovery of structure in reinforcement learning.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. G. Barto, S. J. Bradtke, and S. P. Singh. </author> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <note> to appear. </note>
Reference-contexts: Throughout this paper we will assume that the environment of the learner is a partially controllable Markov chain <ref> [1] </ref>. At any instant in time the learner can observe the state of the environment, denoted by s 2 S, and apply an action, a 2 A. Actions change the state of the environment, and also produce a scalar pay-off value, denoted by r s;a 2 &lt;. <p> In addition all usage values fu b;k jb 2 Bg are initialized randomly. This mechanism ensures that skills, once overturned by other skills, will not get lost forever. Usages. Unlike skill domains, which are discrete quantities, usages are real-valued numbers. Initially, they are chosen at random in <ref> [0; 1] </ref>. Usages are optimized by stochastic gradient descent in E. According to Eq. (8), the derivative of E (s) is the sum of @LOSS (s) @u b;k @DL (s) @u b;k .
Reference: [2] <author> J. A. Boyan. </author> <title> Generalization in reinforcement learning: Safely approximating the value function. Same volume. </title>
Reference-contexts: Using function approximators. In this paper, performance loss and description length has been defined based on table look-up representations of Q. Recently, various researchers have applied reinforcement learning in combination with generalizing function approximators, such as nearest neighbor methods or artificial neural networks (e.g., <ref> [2, 4, 12, 13] </ref>). In order to apply the SKILLS algorithm together with generalizing function approximators, the notions of skill domains and description length have to be modified.
Reference: [3] <author> P. Dayan and G. E. Hinton. </author> <title> Feudal reinforcement learning. </title> <editor> In J. E. Moody, S. J. Hanson, and R. P. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5,1993. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: example, abstractions have been built upon previously learned, simpler tasks [9, 10], previously learned low-level behaviors [7], subgoals, which are either known in advance [15] or determined at random [6], or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities <ref> [3] </ref>. For all these approaches, drastically improved problem solving capabilities have been reported, which are far beyond that of plain, unstructured reinforcement learning. This paper exclusively focuses on how to discover the structure inherent in a family of related tasks.
Reference: [4] <author> V. Gullapalli, J. A. Franklin, and Hamid B. </author> <title> Acquiring robot skills via reinforcement learning. </title> <journal> IEEE Control Systems, </journal> <volume> 272(1708), </volume> <year> 1994. </year>
Reference-contexts: Using function approximators. In this paper, performance loss and description length has been defined based on table look-up representations of Q. Recently, various researchers have applied reinforcement learning in combination with generalizing function approximators, such as nearest neighbor methods or artificial neural networks (e.g., <ref> [2, 4, 12, 13] </ref>). In order to apply the SKILLS algorithm together with generalizing function approximators, the notions of skill domains and description length have to be modified.
Reference: [5] <author> T. Jaakkola, M. I. Jordan, and S. P. Singh. </author> <title> On the convergence of stochastic iterative dynamic programming algorithms. </title> <type> Technical Report 9307, </type> <institution> Department of Brain and Cognitive Sciences, MIT, </institution> <month> July </month> <year> 1993. </year>
Reference-contexts: a lookup-table, as will be the case throughout this paper, the Q-Learning rule (3) has been shown 1 to converge to a value function Q opt (s; a) which measures the future discounted pay-off one can expect to receive upon applying action a in state s, and acting optimally thereafter <ref> [5, 14] </ref>.
Reference: [6] <author> L. P. Kaelbling. </author> <title> Hierarchical learning in stochastic domains: Preliminary results. </title> <editor> In Paul E. Utgoff, editor, </editor> <booktitle> Proceedings of the Tenth International Conference on Machine Learning, 1993. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Different approaches differ in the origin of the abstraction, and the way it is incorporated into learning. For example, abstractions have been built upon previously learned, simpler tasks [9, 10], previously learned low-level behaviors [7], subgoals, which are either known in advance [15] or determined at random <ref> [6] </ref>, or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities [3]. For all these approaches, drastically improved problem solving capabilities have been reported, which are far beyond that of plain, unstructured reinforcement learning.
Reference: [7] <author> L.-J. Lin. </author> <title> Self-supervised Learning by Reinforcement and Artificial Neural Networks. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: Different approaches differ in the origin of the abstraction, and the way it is incorporated into learning. For example, abstractions have been built upon previously learned, simpler tasks [9, 10], previously learned low-level behaviors <ref> [7] </ref>, subgoals, which are either known in advance [15] or determined at random [6], or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities [3].
Reference: [8] <author> M. </author> <title> Ring. Two methods for hierarchy learning in reinforcement environments. In From Animals to Animates 2: </title> <booktitle> Proceedings of the Second International Conference on Simulation of Adaptive Behavior. </booktitle> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference: [9] <author> S. P. Singh. </author> <title> Reinforcement learning with a hierarchy of abstract models. </title> <booktitle> In Proceeding of the Tenth National Conference on Artificial Intelligence AAAI-92, 1992. </booktitle> <publisher> AAAI Press/The MIT Press. </publisher>
Reference-contexts: Learning speed. In our experiments we found that the time required for finding useful skills is up to an order of magnitude larger than the time it takes to find close-to-optimal policies. more complex grid navigation task. Similar findings are reported in <ref> [9] </ref>. This is because discovering skills is much harder than learning control. Initially, nothing is know about the structure of the state space, and unless reasonably accurate Q-tables are available, SKILLS cannot discover meaningful skills. <p> In recent years, several researchers have recognized the importance of structuring reinforcement learning in order to build abstractions and action hierarchies. Different approaches differ in the origin of the abstraction, and the way it is incorporated into learning. For example, abstractions have been built upon previously learned, simpler tasks <ref> [9, 10] </ref>, previously learned low-level behaviors [7], subgoals, which are either known in advance [15] or determined at random [6], or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities [3].
Reference: [10] <author> S. P. Singh. </author> <title> Transfer of learning by composing solutions for elemental sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: This is because if tasks are related, as is the case in many natural learning environments, skills can be used to transfer knowledge from previously learned tasks to new tasks. In particular, if the learner faces tasks with increasing complexity, as proposed by Singh <ref> [10] </ref>, learning skills could conceivable reduce the learning time in complex tasks, and hence scale reinforcement learning techniques to more complex tasks. Using function approximators. In this paper, performance loss and description length has been defined based on table look-up representations of Q. <p> In recent years, several researchers have recognized the importance of structuring reinforcement learning in order to build abstractions and action hierarchies. Different approaches differ in the origin of the abstraction, and the way it is incorporated into learning. For example, abstractions have been built upon previously learned, simpler tasks <ref> [9, 10] </ref>, previously learned low-level behaviors [7], subgoals, which are either known in advance [15] or determined at random [6], or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities [3].
Reference: [11] <author> R. S. Sutton. </author> <title> Temporal Credit Assignment in Reinforcement Learning. </title> <type> PhD thesis, </type> <institution> Department of Computer and Information Science, University of Massachusetts, </institution> <year> 1984. </year>
Reference-contexts: In general, pay-off might be delayed. Therefore, in order to learn an optimal , one has to solve a temporal credit assignment problem <ref> [11] </ref>. To date, the single most widely used algorithm for learning from delayed pay-off is Q-Learning [14]. Q-Learning solves the problem of learning by learning a value function, denoted by Q : S fi A ! &lt;.
Reference: [12] <author> G. J. Tesauro. </author> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <year> 1992. </year>
Reference-contexts: Using function approximators. In this paper, performance loss and description length has been defined based on table look-up representations of Q. Recently, various researchers have applied reinforcement learning in combination with generalizing function approximators, such as nearest neighbor methods or artificial neural networks (e.g., <ref> [2, 4, 12, 13] </ref>). In order to apply the SKILLS algorithm together with generalizing function approximators, the notions of skill domains and description length have to be modified.
Reference: [13] <author> S. Thrun and A. Schwartz. </author> <title> Issues in using function approximation for reinforcement learning. </title> <editor> In M. Mozer, Pa. Smolensky, D. Touretzky, J. Elman, and A. Weigend, editors, </editor> <booktitle> Proceedings of the 1993 Connectionist Models Summer School, 1993. </booktitle> <publisher> Erlbaum Associates. </publisher>
Reference-contexts: Using function approximators. In this paper, performance loss and description length has been defined based on table look-up representations of Q. Recently, various researchers have applied reinforcement learning in combination with generalizing function approximators, such as nearest neighbor methods or artificial neural networks (e.g., <ref> [2, 4, 12, 13] </ref>). In order to apply the SKILLS algorithm together with generalizing function approximators, the notions of skill domains and description length have to be modified.
Reference: [14] <author> C. J. C. H. Watkins. </author> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> King's College, </institution> <address> Cambridge, England, </address> <year> 1989. </year>
Reference-contexts: In general, pay-off might be delayed. Therefore, in order to learn an optimal , one has to solve a temporal credit assignment problem [11]. To date, the single most widely used algorithm for learning from delayed pay-off is Q-Learning <ref> [14] </ref>. Q-Learning solves the problem of learning by learning a value function, denoted by Q : S fi A ! &lt;. Q maps states s 2 S and actions a 2 A to scalar values. <p> a lookup-table, as will be the case throughout this paper, the Q-Learning rule (3) has been shown 1 to converge to a value function Q opt (s; a) which measures the future discounted pay-off one can expect to receive upon applying action a in state s, and acting optimally thereafter <ref> [5, 14] </ref>.
Reference: [15] <author> S. Whitehead, J. Karlsson, and J. Tenenberg. </author> <title> Learning multiple goal behavior via task decomposition and dynamic policy merging. </title> <editor> In J. H. Connell and S. Mahadevan, editors, </editor> <title> Robot Learning. </title> <publisher> Kluwer Academic Publisher, </publisher> <year> 1993. </year>
Reference-contexts: Different approaches differ in the origin of the abstraction, and the way it is incorporated into learning. For example, abstractions have been built upon previously learned, simpler tasks [9, 10], previously learned low-level behaviors [7], subgoals, which are either known in advance <ref> [15] </ref> or determined at random [6], or based on a pyramid of different levels of perceptual resolution, which produces a whole spectrum of problem solving capabilities [3]. For all these approaches, drastically improved problem solving capabilities have been reported, which are far beyond that of plain, unstructured reinforcement learning.
References-found: 15

