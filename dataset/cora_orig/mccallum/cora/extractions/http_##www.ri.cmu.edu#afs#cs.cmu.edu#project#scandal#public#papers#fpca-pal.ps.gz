URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/fpca-pal.ps.gz
Refering-URL: http://www.ri.cmu.edu/afs/cs.cmu.edu/project/scandal/public/papers/fpca-pal.html
Root-URL: 
Email: (blelloch@cs.cmu.edu)  (jdg@cs.cmu.edu)  
Title: Parallelism in Sequential Functional Languages  
Author: Guy Blelloch John Greiner 
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: This paper formally studies the question of how much parallelism is available in call-by-value functional languages with no parallel extensions (i.e., the functional subsets of ML or Scheme). In particular we are interested in placing bounds on how much parallelism is available for various problems. To do this we introduce a complexity model, the PAL, based on the call-by-value -calculus. The model is defined in terms of a profiling semantics and measures complexity in terms of the total work and the parallel depth of a computation. We describe a simulation of the A-PAL (the PAL extended with arithmetic operations) on various parallel machine models, including the butterfly, hypercube, and PRAM models and prove simulation bounds. In particular the simulations are work-efficient (the processor-time product on the machines is within a constant factor of the work on the A-PAL), and for p processors the slowdown (time on the machines divided by depth on the A-PAL) is proportional to at most O(log p). We also prove bounds for simulating the PRAM on the A-PAL. Based on the model, we describe and analyze tree-based versions of quicksort and mergesort. We show that for an input of size n these algorithms run on the A-PAL model with O(n log n) work and O(log 2 n) depth (expected case for quicksort). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Samson Abramsky and R. Sykes. Secd-m: </author> <title> A virtual machine for applicative programming. </title> <editor> In Jean-Pierre Jouannaud, editor, </editor> <booktitle> Proceedings 2nd International Conference on Functional Programming Languages and Computer Architecture, number 201 in Lecture Notes in Computer Science, </booktitle> <pages> pages 81-98, </pages> <year> 1985. </year>
Reference-contexts: Goodrich and Kosaraju [18] introduced a parallel pointer machine (PPM), but this is quite different from our model since it assumes a fixed number of processors and allows side effecting of pointers. Another parallel version of the SECD machine was introduced by Abramsky and Sykes <ref> [1] </ref>, but their Secd-m machine was non-deterministic and based on the fair merge. 7 Conclusions This paper has discussed a complexity model based on the -calculus and shown various simulation results. A goal of this work is to bring a closer tie between parallel algorithms and functional languages.
Reference: [2] <author> Amir M. Ben-Amram and Zvi Galil. </author> <title> On pointers versus addresses. </title> <journal> Journal of the ACM, </journal> <volume> 39(3) </volume> <pages> 617-648, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The primitive functions are addition, multiplication, negation, division by two, and the test for positive integers. For syntactic simplicity, all primitive functions are curried. The choice of primitives is not important, but for the purpose of lower bounds proofs they should be incompressible <ref> [2] </ref>, which ensures that certain kinds of data encoding schemes cannot asymptotically im prove complexity bounds, e.g., encoding arrays as integers. This is why general division has been omitted. The ffi functions for these constants are given in Figure 3. <p> Proof outline: Follows from v e bounding the depth of each environment in the evaluation. 2 We note that Lemma 2 holds for a pointer machine <ref> [24, 44, 2] </ref> as well as a RAM since the simulation does not require random access to memory. Theorem 1 If [] ` e ! v; w; d, then a RAM can calculate v from e in no more than kv e w time, for some constant k. <p> The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines <ref> [2] </ref>). Since we don't know how to do better for the weaker models, we will base our results on the most powerful model, the CRCW PRAM with unit-time multiprefix sums (MP PRAM). <p> But for type inference to terminate, only special forms of recursion can be used, such as those of the Bird-Meertens formalism. There has been much work on comparing machine models within traditional complexity theory. The most closely related is that of Ben-Amram and Galil <ref> [2] </ref>, who show that a pointer machine incurs logarithmic overhead to simulate a RAM. The pointer machine [24, 44] is similar to the SECD machine in that it addresses memory only through pointers, but it lacks direct support for implementing higher-order functions.
Reference: [3] <author> Bror Bjerner and Soren Holmstrom. </author> <title> A compositional approach to time analysis of first order lazy functional programs. </title> <booktitle> In Proceedings 4th International Conference on Functional Programming Languages and Computer Architecture. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: However, if merging uses n= lg n splitters, rather than just the median, the depth complexities of merging and mergesort can each be improved by a factor of log n [7]. 6 Related Work Several researchers have used cost-augmented semantics for automatic time analysis of serial programs <ref> [3, 38, 39, 45] </ref>. This work was concerned with serial running time, and since they were primarily interested in automatically analyzing programs rather than defining complexity, they each altered the semantics of functions to simplify such analysis.
Reference: [4] <author> Guy Blelloch. </author> <note> An L1 User's Manual (Version 1.2: Draft), </note> <month> November </month> <year> 1989. </year>
Reference-contexts: The simulation we use gives the same results for the EREW, CREW, and CRCW PRAM as well as for the multi-prefix [32] and scan models <ref> [4] </ref>. The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines [2]).
Reference: [5] <author> Guy E. Blelloch. </author> <title> Vector Models for Data-Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages [11, 8, 10]. The work-step paradigm has been used for many years for informally describing parallel algorithms [42, 22]. It was first included in a formal model by Blelloch in the VRAM <ref> [5] </ref>. NESL [6], a data-parallel functional language, includes complexity measures based on work and steps and has been used for describing and teaching parallel algorithms. Skil-licorn [43] also introduced cost measures specified in terms of work and steps for a data-parallel language based on the Bird-Meertens formalism.
Reference: [6] <author> Guy E. Blelloch. NESL: </author> <title> A nested data-parallel language (version 2.6). </title> <type> Technical Report CMU-CS-93-129, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: The work-step paradigm has been used for many years for informally describing parallel algorithms [42, 22]. It was first included in a formal model by Blelloch in the VRAM [5]. NESL <ref> [6] </ref>, a data-parallel functional language, includes complexity measures based on work and steps and has been used for describing and teaching parallel algorithms. Skil-licorn [43] also introduced cost measures specified in terms of work and steps for a data-parallel language based on the Bird-Meertens formalism.
Reference: [7] <author> Guy E. Blelloch and John Greiner. </author> <title> A parallel complexity model for functional languages. </title> <type> Technical Report CMU-CS-94-196, </type> <institution> Carnegie Mellon University, </institution> <month> October </month> <year> 1994. </year>
Reference-contexts: The language is basically equivalent within constant factors of complexity to the functional subsets of eager languages such as ML or Scheme when the parallelism in those languages comes from evaluating arguments in parallel <ref> [7] </ref>. This correspondence allows us to use the simpler -calculus to prove results about the complexity model while using an ML-like language to prove results about algorithms. 2. <p> We chose the - calculus rather than a specific language since its simplicity makes the simulation results in Section 3 much cleaner, and many features of modern languages (e.g., data-types, conditionals, recursion, and local variables) can be simulated with constant overhead <ref> [7] </ref>, therefore not affecting asymptotic performance. The abstract syntax of the model is e 2 Expressions ::= c j x j x:e j e 1 e 2 where the meta-variable c ranges over a set of constants. <p> Applying a constant function is also assumed to evaluate with constant work and depth. This is a reasonable assumption for most constant functions, in cluding those used here. It is straightforward, however, to augment the model with constant functions whose work and depth is a function of their argument <ref> [7] </ref>. Definition 1 The PAL model is the -calculus with no constants and with the semantics defined by E ` e Adding Constants to the PAL Model We now extend the basic PAL model with arithmetic constants to obtain the Arithmetic-PAL model. <p> i) = add i ffi (mul; i) = mul i ffi (neg; i) = i ffi (div2; i) = bi=2c ffi (pos?; i) = if i &gt; 0 then cl ([]; x; y:x) else cl ([]; x; y:y) ings for the booleans and can be used to encode condition als <ref> [7] </ref>. Applying each of these constants requires constant work. <p> The full proof can be found in <ref> [7] </ref>. 2 In our variant of the SECD machine, environments are represented as balanced trees, such as AVL trees. Extending an environment creates a new environment sharing as much structure with the old environment as possible. <p> However, if merging uses n= lg n splitters, rather than just the median, the depth complexities of merging and mergesort can each be improved by a factor of log n <ref> [7] </ref>. 6 Related Work Several researchers have used cost-augmented semantics for automatic time analysis of serial programs [3, 38, 39, 45].
Reference: [8] <author> A. P. Willem Bohm and R. E. Hiromoto. </author> <title> The dataflow time and space complexity of FFTs. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3) </volume> <pages> 301-313, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages <ref> [11, 8, 10] </ref>. The work-step paradigm has been used for many years for informally describing parallel algorithms [42, 22]. It was first included in a formal model by Blelloch in the VRAM [5].
Reference: [9] <author> Richard P. Brent. </author> <title> The parallel evaluation of general arithmetic expressions. </title> <journal> Journal of the ACM, </journal> <volume> 21(2) </volume> <pages> 201-206, </pages> <year> 1974. </year>
Reference-contexts: Analogous results are true for the other models. Proof: The proof uses Brent's scheduling principle <ref> [9] </ref>. We prove it for the CREW PRAM, but the other proofs are almost identical. We assume that step i of the P-ECD pro cesses q i substates. We know from Lemma 3 that P i&lt;d w.
Reference: [10] <author> I. Checkland and C. Runciman. </author> <title> Perfect hash functions made parallel|lazy functional programming on a distributed multiprocessor. </title> <editor> In T. N. Mudge, V. Mi-lutinovic, and L. Hunter, editors, </editor> <booktitle> Proceedings 26th Hawaii International Conference on System Sciences, </booktitle> <volume> volume 2, </volume> <pages> pages 397-406, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages <ref> [11, 8, 10] </ref>. The work-step paradigm has been used for many years for informally describing parallel algorithms [42, 22]. It was first included in a formal model by Blelloch in the VRAM [5].
Reference: [11] <author> C. D. Clack and Simon Peyton Jones. </author> <title> Generating parallelism from strictness analysis. </title> <type> Technical Report Internal Note 1679, </type> <institution> Dept. Comp. Sci., University College London, </institution> <month> February </month> <year> 1985. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages <ref> [11, 8, 10] </ref>. The work-step paradigm has been used for many years for informally describing parallel algorithms [42, 22]. It was first included in a formal model by Blelloch in the VRAM [5].
Reference: [12] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. E. Schauser, E. Santos, R. Subramonian, and T. von Eicken. </author> <title> LogP: Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Can good bounds be placed on the use of memory? * Because it lacks random-access, can the A-PAL model be simulated more efficiently than the PRAM on machines that have less powerful communication (e.g., fixed-topology networks, parallel I/O models, or the LOGP model <ref> [12] </ref>), and can the complexity model be augmented to capture the notion of locality for these machines? 8 Acknowledgments This research was sponsored in part by the Wright Laboratory, Aeronautical Systems Center, Air Force Materiel Command, USAF, and the Advanced Research Projects Agency (ARPA) under grant number F33615-93-1-1330 and contract number
Reference: [13] <author> Vincent Dornic, Pierre Jouvelot, and David K. Gifford. </author> <title> Polymorphic time systems for estimating program complexity. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <volume> 1(1) </volume> <pages> 33-45, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Also neither formally showed relationship of their models to machine models. Part of the motivation of the work described in this paper was to formalize the mapping of complexity to machine models and to see how much parallelism is available without adding data-parallel primitives. Dornic, et al. <ref> [13] </ref> and Reistad and Gifford [35] explore adding time information to a functional language type system. But for type inference to terminate, only special forms of recursion can be used, such as those of the Bird-Meertens formalism. There has been much work on comparing machine models within traditional complexity theory.
Reference: [14] <author> John T. Feo, David C. Cann, and Rodney R. Olde-hoeft. </author> <title> A Report on the Sisal Language Project. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 10(4) </volume> <pages> 349-366, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Furthermore, researchers have presented many implementation techniques to take advantage of this parallelism, including data-flow [28], parallel graph reduction [20, 30], and various compiler techniques <ref> [14] </ref>. Such work has suggested that it might not be necessary to add explicit parallel constructs to functional languages to get adequate parallelism from functional languages.
Reference: [15] <author> Steven Fortune and James Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings 10th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: This correspondence allows us to use the simpler -calculus to prove results about the complexity model while using an ML-like language to prove results about algorithms. 2. We prove results on how the complexities in our model relate to complexities of various machine-based models, including the PRAM <ref> [15] </ref>, hypercube, and butterfly models. For the PRAM, we examine both the concurrent read, concurrent write (CRCW) and concurrent read, exclusive write (CREW) variants. The results are summarized in Figure 1. The proofs introduce a parallel version of the SECD machine [25], the P-ECD machine.
Reference: [16] <author> J. Gil, Yossi Matias, and Uzi Vishkin. </author> <title> Towards a theory of nearly constant time parallel algorithms. </title> <booktitle> In Proceedings Symposium on Foundations of Computer Science, </booktitle> <pages> pages 698-710, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Since the result array is a constant times larger than the total number of remaining substates, we will maintain the invariant mentioned earlier. Gil, Matias, and Vishkin <ref> [16] </ref> have shown that the linear approximate compaction problem can be solved on a p processor CRCW PRAM (arbitrary) in O (n=p + log fl p) 6 expected time (using a randomized solution).
Reference: [17] <author> T. Goldberg and U. Zwick. </author> <title> Optimal deterministic processor allocation. </title> <booktitle> In Proceedings 4th ACM-SIAM Symp. on Discrete Algorithms, </booktitle> <month> January </month> <year> 1995. </year>
Reference-contexts: Gil, Matias, and Vishkin [16] have shown that the linear approximate compaction problem can be solved on a p processor CRCW PRAM (arbitrary) in O (n=p + log fl p) 6 expected time (using a randomized solution). Goldberg and Zwick <ref> [17] </ref> have recently shown that the problem can be solved deterministically in O (n=p + log log p) time.
Reference: [18] <author> Michael T. Goodrich and S. Rao Kosaraju. </author> <title> Sorting on a parallel pointer machine with applications to set expression evaluation. </title> <booktitle> In 30th Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 190-195, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: We borrow from them the parameterization of models over incompressible data types and operations. Paige [29] also compares models similar to those used by Ben-Amram and Galil. Goodrich and Kosaraju <ref> [18] </ref> introduced a parallel pointer machine (PPM), but this is quite different from our model since it assumes a fixed number of processors and allows side effecting of pointers.
Reference: [19] <author> Paul Hudak and Steve Anderson. </author> <title> Pomset interpretations of parallel functional programs. </title> <booktitle> In Proceedings 3rd International Conference on Functional Programming Languages and Computer Architecture, number 274 in Lecture Notes in Computer Science, </booktitle> <pages> pages 234-256. </pages> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1987. </year>
Reference-contexts: One inconvenience with our model is the need to keep track of how many variable names are needed. In particular, our simulation bounds need to include the logarithm of the 1 We use the term to mean a fully speculative implementation <ref> [19] </ref>. number of independent variables (v e ) in order to account for variable lookup. Fortunately it is straightforward to show that the number of variables for algorithms, such as sorting, is independent of the size of the input, so that v e does not effect the asymptotic bounds. <p> It was not shown, however, whether the model exactly modeled the PRAM. In particular since it is not known until execution how many processors are needed, it is not clear whether the scheduling could be done on the fly. Hudak and Anderson <ref> [19] </ref> suggest modeling parallelism in functional languages using an extended operational semantics based on partially ordered multisets (pomsets). The semantics can be though of as keeping a trace of the computation as a partial order specifying what had to be computed before what else.
Reference: [20] <author> Paul Hudak and Eric Mohr. </author> <title> Graphinators and the Duality of SIMD and MIMD. </title> <booktitle> In ACM Conference on Lisp and Functional Programming, </booktitle> <pages> pages 224-234, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: 1 Introduction Many researchers have argued that an important aspect of purely functional languages is their inherent parallelism| since the languages lack side effects, subexpressions may safely be evaluated in parallel. Furthermore, researchers have presented many implementation techniques to take advantage of this parallelism, including data-flow [28], parallel graph reduction <ref> [20, 30] </ref>, and various compiler techniques [14]. Such work has suggested that it might not be necessary to add explicit parallel constructs to functional languages to get adequate parallelism from functional languages.
Reference: [21] <author> Neil D. Jones. </author> <title> Constant time factors do matter (extended abstract). </title> <booktitle> In Proceedings 25th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 602-611, </pages> <year> 1993. </year> <month> 11 </month>
Reference-contexts: They did not relate their model to other models of parallelism or describe how it would effect algorithms. Previous work on formally relating language-based models (languages with cost-augmented semantics) to machine models is sparse. Jones <ref> [21] </ref> related the time-augmented semantics of simple while-loop language to that of an equivalent machine language in order to study the effect of constant factors in time complexity. Seidl and Wilhelm [40] provide complexity bounds for an implementation of graph reduction on the PRAM.
Reference: [22] <author> R. M. Karp and V. Ramachandran. </author> <title> Parallel algorithms for shared memory machines. </title> <editor> In J. Van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science | Volume A: Algorithms and Complexity. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, Mass., </address> <year> 1990. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages [11, 8, 10]. The work-step paradigm has been used for many years for informally describing parallel algorithms <ref> [42, 22] </ref>. It was first included in a formal model by Blelloch in the VRAM [5]. NESL [6], a data-parallel functional language, includes complexity measures based on work and steps and has been used for describing and teaching parallel algorithms.
Reference: [23] <author> Richard Kennaway. </author> <title> A conflict between call-by-need computation and parallelism (extended abstract). </title> <booktitle> In Proceedings Conditional Term Rewriting Systems-94, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: The problem is that normal-order evaluation can have wide range of implementations, such as call-by-name, call-by-need (lazy), and call-by-speculation (lenient) 1 , and these implementations would have very different complexity models. The first two, call-by-name and call-by-need, actually offer no significant parallelism <ref> [23] </ref>. Call-by-speculation offers plenty of parallelism but does the same amount of work as applicative-order semantics. In particular, a model that uses call-by-speculation would give the same asymptotic work bounds as our model, although it might be possible to improve some depth bounds.
Reference: [24] <author> Donald E. Knuth. </author> <title> Fundamental Algorithms, </title> <booktitle> volume 1 of The Art of Computer Programming. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, MA, </address> <year> 1968. </year>
Reference-contexts: Proof outline: Follows from v e bounding the depth of each environment in the evaluation. 2 We note that Lemma 2 holds for a pointer machine <ref> [24, 44, 2] </ref> as well as a RAM since the simulation does not require random access to memory. Theorem 1 If [] ` e ! v; w; d, then a RAM can calculate v from e in no more than kv e w time, for some constant k. <p> There has been much work on comparing machine models within traditional complexity theory. The most closely related is that of Ben-Amram and Galil [2], who show that a pointer machine incurs logarithmic overhead to simulate a RAM. The pointer machine <ref> [24, 44] </ref> is similar to the SECD machine in that it addresses memory only through pointers, but it lacks direct support for implementing higher-order functions. We borrow from them the parameterization of models over incompressible data types and operations.
Reference: [25] <author> P. J. Landin. </author> <title> The mechanical evaluation of expressions. </title> <journal> Computer Journal, </journal> <volume> 6 </volume> <pages> 308-320, </pages> <year> 1964. </year>
Reference-contexts: For the PRAM, we examine both the concurrent read, concurrent write (CRCW) and concurrent read, exclusive write (CREW) variants. The results are summarized in Figure 1. The proofs introduce a parallel version of the SECD machine <ref> [25] </ref>, the P-ECD machine. A state of the P-ECD machine consists of a set of substates, and each state transition of the machine transforms this set into a new set of substates. On each step the substates are scheduled across the processors of the host machine. <p> We first describe the simulation on a serial RAM and then extend this for the simulation on a PRAM, butterfly network, and hypercube. To simulate the A-PAL on the RAM, we use a variant of the SECD machine <ref> [25, 31] </ref> as an intermediate step. We first show how the work complexity of an A-PAL program is related to the number of state transitions of the SECD machine and then show that each transition can be implemented within given bounds.
Reference: [26] <author> Yossi Matias and Uzi Vishkin. </author> <title> Converting high probability into nearly-constant time|with applications to parallel hashing. </title> <booktitle> In Proceedings ACM Symposium on Theory of Computing, </booktitle> <pages> pages 307-316, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: This will give a new array that is exactly the length of the number of new substates. On the CRCW PRAM the distribution into the new array can be done more efficiently using a solution to the linear approximate compaction problem <ref> [26] </ref>: given an array of n cells, m of which contain an object, place the m objects in distinct cells of an array of size km for some constant k &gt; 1.
Reference: [27] <author> Kurt Mehlhorn and Uzi Vishkin. </author> <title> Randomized and deterministic simulations of PRAMs by parallel machines with restricted granularity of parallel memory. </title> <journal> Acta Informatica, </journal> <volume> 21 </volume> <pages> 339-374, </pages> <year> 1984. </year>
Reference-contexts: On such a machine each of the p processors can access (read or write) n elements in O (n + log p) time with high probability <ref> [27, 33] </ref>. The O (log p) time is due to latency through the network. We also assume the butterfly network has simple integer adders in the switches, such that a prefix-sum computation can execute in O (log p) time.
Reference: [28] <author> Rishiyur S. Nikhil. </author> <note> ID Version 90.0 Reference Manual. Computation Structures Group Memo 284-1, </note> <institution> Laboratory for Computer Science, Massachusetts Institute of Technology, </institution> <month> July </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Many researchers have argued that an important aspect of purely functional languages is their inherent parallelism| since the languages lack side effects, subexpressions may safely be evaluated in parallel. Furthermore, researchers have presented many implementation techniques to take advantage of this parallelism, including data-flow <ref> [28] </ref>, parallel graph reduction [20, 30], and various compiler techniques [14]. Such work has suggested that it might not be necessary to add explicit parallel constructs to functional languages to get adequate parallelism from functional languages.
Reference: [29] <author> Robert Paige. </author> <title> Real-time simulation of a set machine on a RAM. </title> <editor> In W. Koczkodaj, editor, </editor> <booktitle> Proceedings International Conference on Computing and Information, </booktitle> <volume> volume 2, </volume> <pages> pages 68-73, </pages> <year> 1989. </year>
Reference-contexts: The pointer machine [24, 44] is similar to the SECD machine in that it addresses memory only through pointers, but it lacks direct support for implementing higher-order functions. We borrow from them the parameterization of models over incompressible data types and operations. Paige <ref> [29] </ref> also compares models similar to those used by Ben-Amram and Galil. Goodrich and Kosaraju [18] introduced a parallel pointer machine (PPM), but this is quite different from our model since it assumes a fixed number of processors and allows side effecting of pointers.
Reference: [30] <author> S. L. Peyton Jones. </author> <title> Parallel Implementations of Functional Programming Languages. </title> <journal> The Computer Journal, </journal> <volume> 32(2) </volume> <pages> 175-186, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Many researchers have argued that an important aspect of purely functional languages is their inherent parallelism| since the languages lack side effects, subexpressions may safely be evaluated in parallel. Furthermore, researchers have presented many implementation techniques to take advantage of this parallelism, including data-flow [28], parallel graph reduction <ref> [20, 30] </ref>, and various compiler techniques [14]. Such work has suggested that it might not be necessary to add explicit parallel constructs to functional languages to get adequate parallelism from functional languages.
Reference: [31] <author> Gordon D. Plotkin. </author> <title> Call-by-name, call-by-value and the lambda calculus. </title> <journal> Theoretical Computer Science, </journal> <volume> 1 </volume> <pages> 125-159, </pages> <year> 1975. </year>
Reference-contexts: We first describe the simulation on a serial RAM and then extend this for the simulation on a PRAM, butterfly network, and hypercube. To simulate the A-PAL on the RAM, we use a variant of the SECD machine <ref> [25, 31] </ref> as an intermediate step. We first show how the work complexity of an A-PAL program is related to the number of state transitions of the SECD machine and then show that each transition can be implemented within given bounds.
Reference: [32] <author> Abhiram G. Ranade. </author> <title> Fluent Parallel Computation. </title> <type> PhD thesis, </type> <institution> Yale University, Department of Computer Science, </institution> <address> New Haven, CT, </address> <year> 1989. </year>
Reference-contexts: The simulation we use gives the same results for the EREW, CREW, and CRCW PRAM as well as for the multi-prefix <ref> [32] </ref> and scan models [4]. The simulation is optimal in terms of work for all the PRAM variants. This is because it takes logarithmic work to simulate each random access into memory (this is the same as for pointer machines [2]).
Reference: [33] <author> Abhiram G. Ranade. </author> <title> How to emulate shared memory. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 42(3) </volume> <pages> 307-326, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: On such a machine each of the p processors can access (read or write) n elements in O (n + log p) time with high probability <ref> [27, 33] </ref>. The O (log p) time is due to latency through the network. We also assume the butterfly network has simple integer adders in the switches, such that a prefix-sum computation can execute in O (log p) time.
Reference: [34] <author> R. Reischuk. </author> <title> Probabilistic parallel algorithms for sorting and selection. </title> <journal> SIAM Journal of Computing, </journal> <volume> 14(2) </volume> <pages> 396-409, </pages> <year> 1985. </year>
Reference-contexts: We note that the worst case recursion depth is O (n) and that fewer than 1 out of n of the possible inputs will lead to a recursion depth greater than k log n <ref> [34] </ref>. To determine the total computational depth of qsort rec, we need to consider the computational depth along the longest path. We claim that this computational depth is at most O (d) times the recursion depth since each node along the recursion tree will require at most O (d) depth.
Reference: [35] <author> Brian Reistad and David K. Gifford. </author> <title> Static dependent costs for estimating execution time. </title> <booktitle> In Proceedings ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 65-78, </pages> <month> July </month> <year> 1994. </year>
Reference-contexts: Part of the motivation of the work described in this paper was to formalize the mapping of complexity to machine models and to see how much parallelism is available without adding data-parallel primitives. Dornic, et al. [13] and Reistad and Gifford <ref> [35] </ref> explore adding time information to a functional language type system. But for type inference to terminate, only special forms of recursion can be used, such as those of the Bird-Meertens formalism. There has been much work on comparing machine models within traditional complexity theory.
Reference: [36] <author> Paul Roe. </author> <title> Calculating lenient programs' performance. </title> <editor> In Simon L Peyton Jones, Graham Hutton, and Carsten Kehler Holst, editors, </editor> <booktitle> Proceedings Functional Programming, Glasgow 1990, Workshops in computing. </booktitle> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1990. </year>
Reference-contexts: Furthermore, none related their complexity models to more traditional machine models, although since the languages are serial this should not be hard. Roe <ref> [36, 37] </ref> and Zimmermann [46, 47] both studied profiling semantics for parallel languages. Roe formally defined a profiling semantics for an extended -calculus with lenient evaluation.
Reference: [37] <author> Paul Roe. </author> <title> Parallel Programming using Functional Languages. </title> <type> PhD thesis, </type> <institution> Department of Computing Science, University of Glasgow, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: Furthermore, none related their complexity models to more traditional machine models, although since the languages are serial this should not be hard. Roe <ref> [36, 37] </ref> and Zimmermann [46, 47] both studied profiling semantics for parallel languages. Roe formally defined a profiling semantics for an extended -calculus with lenient evaluation.
Reference: [38] <author> Mads Rosendahl. </author> <title> Automatic complexity analysis. </title> <booktitle> In Proceedings 4th International Conference on Functional Programming Languages and Computer Architecture. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1989. </year>
Reference-contexts: This relation needs to capture some aspects of the parallel implementation of the language. To address these issues this paper makes the following contributions: 1. We introduce a parallel model based on the pure - calculus using applicative order (call-by-value) evaluation and specified in terms of a profiling semantics <ref> [38, 39] </ref>. This semantics defines two measures of complexity. The work is the total amount of computation executed by a program. The computational depth (or simply depth) is the depth of the computation tree, assuming that the two subexpressions of an application e 1 e 2 are evaluated in parallel. <p> We keep track of the work in addition to the depth for the purpose of proving useful simulation bounds on parallel machines that have a fixed number of processors. We formalize the work and depth complexities in terms of a profiling semantics <ref> [38, 39] </ref>, which extends the standard operational semantics with cost measures. The judgment E ` e ! v; w; d reads as "In the environment E, the expression e evaluates to value v in work w and depth d." This relation is defined by the rules in Figure 2. <p> However, if merging uses n= lg n splitters, rather than just the median, the depth complexities of merging and mergesort can each be improved by a factor of log n [7]. 6 Related Work Several researchers have used cost-augmented semantics for automatic time analysis of serial programs <ref> [3, 38, 39, 45] </ref>. This work was concerned with serial running time, and since they were primarily interested in automatically analyzing programs rather than defining complexity, they each altered the semantics of functions to simplify such analysis.
Reference: [39] <author> David Sands. </author> <title> Calculi for Time Analysis of Functional Programs. </title> <type> PhD thesis, </type> <institution> University of London, Imperial College, </institution> <month> September </month> <year> 1990. </year>
Reference-contexts: This relation needs to capture some aspects of the parallel implementation of the language. To address these issues this paper makes the following contributions: 1. We introduce a parallel model based on the pure - calculus using applicative order (call-by-value) evaluation and specified in terms of a profiling semantics <ref> [38, 39] </ref>. This semantics defines two measures of complexity. The work is the total amount of computation executed by a program. The computational depth (or simply depth) is the depth of the computation tree, assuming that the two subexpressions of an application e 1 e 2 are evaluated in parallel. <p> We keep track of the work in addition to the depth for the purpose of proving useful simulation bounds on parallel machines that have a fixed number of processors. We formalize the work and depth complexities in terms of a profiling semantics <ref> [38, 39] </ref>, which extends the standard operational semantics with cost measures. The judgment E ` e ! v; w; d reads as "In the environment E, the expression e evaluates to value v in work w and depth d." This relation is defined by the rules in Figure 2. <p> However, if merging uses n= lg n splitters, rather than just the median, the depth complexities of merging and mergesort can each be improved by a factor of log n [7]. 6 Related Work Several researchers have used cost-augmented semantics for automatic time analysis of serial programs <ref> [3, 38, 39, 45] </ref>. This work was concerned with serial running time, and since they were primarily interested in automatically analyzing programs rather than defining complexity, they each altered the semantics of functions to simplify such analysis.
Reference: [40] <author> Helmut Seidl and Reinhard Wilhelm. </author> <title> Probabilistic load balancing for parallel graph reduction. </title> <booktitle> In Proceedings TENCON '89, 4th IEEE Region 10 International Conference, </booktitle> <pages> pages 879-884, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: Previous work on formally relating language-based models (languages with cost-augmented semantics) to machine models is sparse. Jones [21] related the time-augmented semantics of simple while-loop language to that of an equivalent machine language in order to study the effect of constant factors in time complexity. Seidl and Wilhelm <ref> [40] </ref> provide complexity bounds for an implementation of graph reduction on the PRAM. However, their implementation only considers a single step and requires that you know which graph nodes to execute in parallel in that step and that the graph has constant in-degree.
Reference: [41] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> Finding the maximum, merging and sorting in a parallel computation model. </title> <journal> Journal of Algorithms, </journal> <volume> 2(1) </volume> <pages> 88-102, </pages> <year> 1981. </year>
Reference-contexts: This accentuates the importance of storing data as trees rather than lists to take advantage of parallel implementations of functional languages. The merging in mergesort borrows ideas from algorithms designed for the PRAM <ref> [41] </ref>, but has some substantial changes to make up for the lack of random access. Both sorting algorithms require O (n log n) work and O (log 2 n) depth, and our work bounds are optimal for both merging and sorting, and our depth bounds are optimal for merging.
Reference: [42] <author> Yossi Shiloach and Uzi Vishkin. </author> <title> An O(n 2 log n) parallel Max-Flow algorithm. </title> <journal> J. Algorithms, </journal> <volume> 3 </volume> <pages> 128-146, </pages> <year> 1982. </year>
Reference-contexts: There have also been several experimental studies of how much parallelism is available in sequential functional languages [11, 8, 10]. The work-step paradigm has been used for many years for informally describing parallel algorithms <ref> [42, 22] </ref>. It was first included in a formal model by Blelloch in the VRAM [5]. NESL [6], a data-parallel functional language, includes complexity measures based on work and steps and has been used for describing and teaching parallel algorithms.
Reference: [43] <author> David B. Skillicorn and W. Cai. </author> <title> A cost calculus for parallel functional programming. </title> <note> To appear in the Journal of Parallel and Distributed Computing. </note>
Reference-contexts: It was first included in a formal model by Blelloch in the VRAM [5]. NESL [6], a data-parallel functional language, includes complexity measures based on work and steps and has been used for describing and teaching parallel algorithms. Skil-licorn <ref> [43] </ref> also introduced cost measures specified in terms of work and steps for a data-parallel language based on the Bird-Meertens formalism. In both cases the languages were not based on the pure -calculus but instead included array primitives. Also neither formally showed relationship of their models to machine models.
Reference: [44] <author> Robert E. Tarjan. </author> <title> A class of algorithms which require nonlinear time to maintain disjoint sets. </title> <journal> J. Comput. System Sci., </journal> <volume> 18 </volume> <pages> 110-127, </pages> <year> 1979. </year>
Reference-contexts: Proof outline: Follows from v e bounding the depth of each environment in the evaluation. 2 We note that Lemma 2 holds for a pointer machine <ref> [24, 44, 2] </ref> as well as a RAM since the simulation does not require random access to memory. Theorem 1 If [] ` e ! v; w; d, then a RAM can calculate v from e in no more than kv e w time, for some constant k. <p> There has been much work on comparing machine models within traditional complexity theory. The most closely related is that of Ben-Amram and Galil [2], who show that a pointer machine incurs logarithmic overhead to simulate a RAM. The pointer machine <ref> [24, 44] </ref> is similar to the SECD machine in that it addresses memory only through pointers, but it lacks direct support for implementing higher-order functions. We borrow from them the parameterization of models over incompressible data types and operations.
Reference: [45] <author> Philip Wadler. </author> <title> Strictness analysis aids time analysis. </title> <booktitle> In Proceedings 15th ACM Symposium on Principles of Programming Languages, </booktitle> <month> January </month> <year> 1988. </year>
Reference-contexts: However, if merging uses n= lg n splitters, rather than just the median, the depth complexities of merging and mergesort can each be improved by a factor of log n [7]. 6 Related Work Several researchers have used cost-augmented semantics for automatic time analysis of serial programs <ref> [3, 38, 39, 45] </ref>. This work was concerned with serial running time, and since they were primarily interested in automatically analyzing programs rather than defining complexity, they each altered the semantics of functions to simplify such analysis.
Reference: [46] <author> Wolf Zimmermann. </author> <title> Automatic worst case complexity analysis of parallel programs. </title> <type> Technical Report TR-90-066, </type> <institution> International Computer Science Institute, </institution> <month> Decem-ber </month> <year> 1990. </year>
Reference-contexts: Furthermore, none related their complexity models to more traditional machine models, although since the languages are serial this should not be hard. Roe [36, 37] and Zimmermann <ref> [46, 47] </ref> both studied profiling semantics for parallel languages. Roe formally defined a profiling semantics for an extended -calculus with lenient evaluation.
Reference: [47] <author> Wolf Zimmermann. </author> <title> Complexity issues in the design of functional languages with explicit parallelism. </title> <booktitle> In Proceedings International Conference on Computer Languages, </booktitle> <pages> pages 34-43, </pages> <year> 1992. </year> <month> 12 </month>
Reference-contexts: Furthermore, none related their complexity models to more traditional machine models, although since the languages are serial this should not be hard. Roe [36, 37] and Zimmermann <ref> [46, 47] </ref> both studied profiling semantics for parallel languages. Roe formally defined a profiling semantics for an extended -calculus with lenient evaluation.
References-found: 47

