URL: http://www.specbench.org/hpg/sas.ps
Refering-URL: http://www.specbench.org/hpg/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Targeting a Shared-Address-Space Version of the Seismic Benchmark Seis1.1  
Author: Bill Pottenger and Rudolf Eigenmann 
Keyword: Shared Memory, Shared Memory Multiprocessor, Distributed Shared Memory, Distributed Shared Memory Multiprocessor, Computational Applications, Seismic Processing, Parallel Processing, Parallel Languages, Language Design, Shared Address Space, Message Passing, Performance Evaluation, Benchmarks, SMP, DSM, SAS, MP  
Address: 1308 West Main Street, Urbana, Illinois 61801-2307  Building, West Lafayette, Indiana 47907  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  Purdue University School of Electrical and Computer Engineering 336 EE  
Email: potteng@csrd.uiuc.edu  eigenman@ecn.purdue.edu  
Phone: 217-333-6578, fax: 217-244-1351  317-494-1741, fax: 317-494-6440  
Date: January 15, 1996  
Abstract: We report on our experiences retargeting the seismic processing message-passing application Seis1.1 [11] to an SGI Challenge shared-memory multiprocessor. Our primary purpose in doing so is to provide a shared-address-space (SAS) version of the application for inclusion in the new high performance SPEChpc96 Benchmark Suite. As a result of this work we have determined the language constructs necessary to express Seis1.1 in the SAS programming model. In addition, we have characterized the performance of the SAS versus message-passing programming models for this application on a shared-memory multiprocessor.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> George Almasi and Allan Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Benjamin/Cummings, Redwood City, CA, </address> <year> 1989. </year>
Reference-contexts: The SAS version initially employed a barrier based on AT&T System V shared-memory semaphores which was provided with the Seis1.1 benchmark. However, performance of the IPC semaphore-based barriers was dismal, and we replaced them with a custom-coded barrier. Using an algorithm provided in <ref> [1] </ref>, a barrier based on the SGI atomic test-then-add primitive was developed. This proved to be much more efficient than the IPC-based barrier. Additional optimizations to the test-then-add barrier were investigated, including cache-line alignment of the barrier synchronization variable, and exponential backoff while spin-waiting at the barrier [10].
Reference: [2] <author> SGI Documentation. </author> <title> Fortran 77 Programmer's Guide and associated man pages. Silicon Graphics, </title> <publisher> Inc., </publisher> <year> 1994. </year>
Reference-contexts: There are two distinct communication paradigms used in the MP version of Seis1.1: PVM-based sends and receives, and optional shared-memory copies for AT&T System V shared-memory architectures. AT&T System V interprocess communication (IPC) facilities allow two or more processes to share segments of their virtual address spaces <ref> [2] </ref>. These facilities, when combined with shared-memory copy operations, can be used to replace PVM message-passing primitives. This functionality is optionally enabled for array transpose operations which take place in one of the computationally intensive phases of the application. The variable SHMEM selects between the two schemes at compile time. <p> This was accomplished using the SGI Fortran directive C$COPYIN item [,item], where item is a member of a local common block privatized using the -Xlocaldata loader switch. item can be a "variable, an array, an individual element of an array, or the entire common block" <ref> [2] </ref>. An entire common block is copied-in using the standard Fortran notation "/name/", where name is the name of the common block. <p> The final flag is CONF STHREADMISCON, a catch-all which means "single threaded miscellaneous on". This enables single threaded execution of miscellaneous routines in the C stdio library such as opendir, closedir, seekdir, etc. All three of these flags were set using the usconfig interface <ref> [2] </ref>. 4 Results Tables 1 and 2 summarize our timing results on an SGI Challenge multiprocessor. The measurements are based on runs made at the National Center for Supercomputing Applications (NCSA).
Reference: [3] <author> Rudolf Eigenmann and Siamak Hassanzadeh. </author> <title> Evaluating High-Performance Computer Technology through Industrially Significant Applications. </title> <journal> IEEE Computational Science & Engineering, </journal> <month> Spring </month> <year> 1996. </year>
Reference-contexts: Taken together with message-passing primitives, these routines are designed to offer opportunities for parallelism in a portable manner. Seis1.1 has been revised and adopted as a member of the new SPEChpc96 benchmark suite, which is being established under the sponsorship of the Standard Performance Evaluation Corporation (SPEC) <ref> [3] </ref>. SPEChpc96 is being defined by a joint effort of industrial members, high-performance computer vendors, and academic institutions. The primary goal is to determine a set of industrially significant applications that can be used to characterize the performance of high-performance computers across a wide range of machine organizations.
Reference: [4] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, version 1.0. </title> <type> Technical report, </type> <institution> Rice University, Houston Texas, </institution> <month> May </month> <year> 1993. </year>
Reference-contexts: For example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI [5], PVM [6]) * High Performance Fortran (HPF) <ref> [4] </ref> * High Performance C ++ [7] * Recent work on dpANS X3.252-199x (X3H5) [9] * Vendor-specific dialects (e.g., compiler directives, *Step [8]) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1.
Reference: [5] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <type> Technical report, </type> <institution> University of Tennessee, Knoxville, Tennessee, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: For example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI <ref> [5] </ref>, PVM [6]) * High Performance Fortran (HPF) [4] * High Performance C ++ [7] * Recent work on dpANS X3.252-199x (X3H5) [9] * Vendor-specific dialects (e.g., compiler directives, *Step [8]) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1.
Reference: [6] <author> G. A. Geist, A. L. Beguelin, J. J. Dongarra, W. Jiang, R. J. Manchek, and V. S. Sunderam. </author> <title> PVM: Parallel Virtual Machine A Users Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: For example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI [5], PVM <ref> [6] </ref>) * High Performance Fortran (HPF) [4] * High Performance C ++ [7] * Recent work on dpANS X3.252-199x (X3H5) [9] * Vendor-specific dialects (e.g., compiler directives, *Step [8]) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1. <p> The actual underlying message-passing layer employed in release 1.1 is PVM <ref> [6] </ref>. In order to port the MP/PVM based version to the SAS model, the message-passing layer had to be replaced. In order to understand how this replacement was done, some background is necessary on the different modes of communication employed in the suite.
Reference: [7] <institution> HPC++ Whitepapers and Draft Working Documents. The HPC++ working group. </institution> <type> Technical report, </type> <institution> Indiana University, </institution> <year> 1995. </year> <note> Presented at Supercomputing '95, available at http://extreme.indiana.edu/hpc++/index.html. </note>
Reference-contexts: For example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI [5], PVM [6]) * High Performance Fortran (HPF) [4] * High Performance C ++ <ref> [7] </ref> * Recent work on dpANS X3.252-199x (X3H5) [9] * Vendor-specific dialects (e.g., compiler directives, *Step [8]) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1.
Reference: [8] <author> Kuck & Associates Inc. </author> <note> flStep User's Guide, Version 1.0, </note> <month> November </month> <year> 1995. </year>
Reference-contexts: example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI [5], PVM [6]) * High Performance Fortran (HPF) [4] * High Performance C ++ [7] * Recent work on dpANS X3.252-199x (X3H5) [9] * Vendor-specific dialects (e.g., compiler directives, *Step <ref> [8] </ref>) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1. How can portable parallel programming be accomplished in a way that machine-dependent features can be exploited? 2.
Reference: [9] <author> Bruce Leisure. </author> <title> Parallel Processing Model for High Level Programming Languages. </title> <type> Technical report, </type> <institution> Kuck & Associates, Inc., Champaign, Illinois 61820, </institution> <note> April 5th, 1994. Available at http://www.kai.com/hints/csbparallelism.html. </note>
Reference-contexts: For example, the following languages and language extensions support shared-address-space (SAS) parallel programming: * Message passing for SAS machines (e.g., MPI [5], PVM [6]) * High Performance Fortran (HPF) [4] * High Performance C ++ [7] * Recent work on dpANS X3.252-199x (X3H5) <ref> [9] </ref> * Vendor-specific dialects (e.g., compiler directives, *Step [8]) These languages and extensions are quite diverse, and as a result the following questions remain as yet unanswered: 1. How can portable parallel programming be accomplished in a way that machine-dependent features can be exploited? 2.
Reference: [10] <author> John M. Mellor-Crummey and Michael L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol 9, No 1, </volume> <pages> pages 21-65, </pages> <month> Feburary </month> <year> 1991. </year>
Reference-contexts: This proved to be much more efficient than the IPC-based barrier. Additional optimizations to the test-then-add barrier were investigated, including cache-line alignment of the barrier synchronization variable, and exponential backoff while spin-waiting at the barrier <ref> [10] </ref>. The code is displayed in Figure 3. The global variable nt in Figure 3 stands for the number of threads, and is initialized prior to entry to the barrier. The global rollover is similarly initialized to 2 fl nt 1.
Reference: [11] <editor> Society for Exploration Geophysics. </editor> <title> A Benchmark Suite for Parallel Seismic Processing, </title> <year> 1991. </year>
Reference-contexts: 1 Introduction 1.1 Application Overview The computational application studied in this article is an industrial code representative of modern seismic processing programs used in the search for oil and gas. Seis1.1 <ref> [11] </ref> consists of a collection of seismic processing routines targeted at parallel processors. Fortran 77 routines for pre-stack and post-stack seismic processing and 3D finite difference modeling are included in the benchmark.
Reference: [12] <author> Supercomputing 1992. </author> <title> A Benchmark Suite for Parallel Seismic Processing, </title> <booktitle> 1992. </booktitle> <pages> 19 </pages>
Reference-contexts: One of the goals of the work described herein is to develop a third version that best exploits the strengths of machine architectures that provide a shared address space. Seis1.1 was originally written as a collection of serial Fortran codes which were later ported to a parallel, message-passing model <ref> [12] </ref>. In its current form, both the single and multiprocessor versions are generated from this explicitly parallel message-passing version. The message-passing version is based on a system independent message-passing layer which acts as an interface to commonly available communication libraries such as PVM.
References-found: 12

