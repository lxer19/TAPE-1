URL: http://www.cs.wustl.edu/cs/techreports/1991/wucs-91-37.ps.Z
Refering-URL: http://www.cs.wustl.edu/cs/cs/publications.html
Root-URL: 
Title: The Power of Self-Directed Learning  
Author: Sally A. Goldman Robert H. Sloan 
Note: Supported in part by an NSF CCR Research Initiation Award. Portions of this research were performed while the author was at the Harvard Aiken Computation Laboratory, supported by ARO grant DAAL 03-86-K-0171.  
Address: St. Louis, Missouri 63130  Chicago, IL 60680  
Affiliation: Department of Computer Science Washington University  Dept. of Electrical Engineering and Computer Science University of Illinois at Chicago  
Pubnum: WUCS-91-37  
Email: Net address: sloan@turing.eecs.uic.edu.  
Date: December 30, 1991  
Abstract: This paper studies self-directed learning, a variant of the on-line learning model in which the learner selects the presentation order for the instances. We give tight bounds on the complexity of self-directed learning for the concept classes of monomials, k-term DNF formulas, and orthogonal rectangles in f0; 1; ; n1g d . These results demonstrate that the number of mistakes under self-directed learning can be surprisingly small. We then prove that the model of self-directed learning is more powerful than all other commonly used on-line and query learning models. Next we explore the relationship between the complexity of self-directed learning and the Vapnik-Chervonenkis dimension. Finally, we explore a relationship between Mitchell's version space algorithm and the existence of self-directed learning algorithms that make few mistakes. fl Supported in part by a GE Foundation Junior Faculty Grant and NSF Grant CCR-9110108. Part of this research was conducted while the author was at the M.I.T. Laboratory for Computer Science and supported by NSF grant DCR-8607494 and a grant from the Siemens Corporation. Net address: sg@cs.wustl.edu. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342, </pages> <year> 1988. </year> <month> 20 </month>
Reference-contexts: A concept class C over X is maximum if for every finite subset Y X, the class C, when restricted to be a class over Y , contains d (jY j) concepts. 5 Finally, we describe membership and equivalence queries as originally defined by An- gluin <ref> [1] </ref>. * A membership query is a call to an oracle that on input x for any x 2 X classifies x as either a positive or negative instance according to the target concept c fl 2 C. * An equivalence query is a call to an oracle that on input <p> Thus it is important to determine how this new learning models relates to both standard on-line learning algorithms and the various models of learning with queries introduced by Angluin <ref> [1] </ref>. Maass and Turan's [12, 13] work comparing the complexity of learning under the commonly studied on-line and query learning models is quite useful.
Reference: [2] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learn--ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: The Vapnik-Chervonenkis dimension of C, denoted vcd (C), is defined to be the smallest d for which no set of d + 1 points is shattered by C. Blumer et al. <ref> [2] </ref> have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [17]. Related to the VC-dimension are the notions of maximal and maximum concept classes [4, 19].
Reference: [3] <author> A. Bundy, B. Silver, and D. Plummer. </author> <title> An analytical comparison of some rule-learning programs. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 137-181, </pages> <year> 1985. </year>
Reference-contexts: For example, the class of monotone monomials 5 has the property that the set S never contains more than one hypothesis <ref> [3] </ref>. Furthermore, for any monomial c, any minimal spanning set I c is just the single instance for which which all the variables in c are 1 and the rest are 0. Thus for the class of monotone monomials, I (C) = 1. <p> Thus at most I (C) mistakes are made when placing any new concept in h. Furthermore, at most k concepts are put in h. Thus we get the desired mistake bound. Applying the result of Bundy, Silver, and Plummer <ref> [3] </ref> to this theorem we get the result of Theorem 3.
Reference: [4] <author> Sally Floyd. </author> <title> On Space-bounded Learning and the Vapnik-Chervonenkis Dimension. </title> <type> PhD thesis, </type> <institution> International Computer Science Institute, </institution> <month> December </month> <year> 1989. </year>
Reference-contexts: Blumer et al. [2] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [17]. Related to the VC-dimension are the notions of maximal and maximum concept classes <ref> [4, 19] </ref>. A concept class is maximal if adding any concept to the class increases the VC dimension of the class. <p> Theorem 7 There exits a family of concept classes C n for which sdc (C n ) = vcd (C n ) + 1. Proof Sketch: We begin by describing concept classes induced by simple planar arrangements as defined in Floyd's thesis <ref> [4] </ref>. A simple planar arrangement G is the dissection of the plane into cells by a finite set of lines with the property that no two lines may be parallel and no three lines can intersect at a single point.
Reference: [5] <author> Sally A. Goldman, Ronald L. Rivest, and Robert E. Schapire. </author> <title> Learning binary relations and total orders. </title> <type> Technical Report MIT/LCS/TM-413, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> May </month> <year> 1990. </year> <note> A preliminary version is available in Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, pages 46-51, </note> <year> 1989. </year>
Reference-contexts: In the next section, we motivate the self-directed learning model. Then in Section 3 we formally describe the model of self-directed learning (as originally defined by Goldman, Rivest, and Schapire <ref> [5] </ref>). Next in Section 4, we discuss some related work. In Section 5 we give tight bounds on the complexity of self-directed learning for the concept classes of monomials, k-term DNF formulas, and orthogonal rectangles in f0; 1; ; n1g d . <p> Finally, in Section 8 we explore a relationship between Mitchell's version space algorithm [14] and the existence of self-directed learning algorithms that make few mistakes. 2 Motivation In this section we motivate the self-directed learning model by reviewing the allergist example given by Goldman, Rivest, and Schapire <ref> [5] </ref>. Consider an allergist with a set of patients to be tested for a given set of allergens. Each patient is either highly allergic, mildly allergic, or not allergic to any given allergen. <p> Sometimes this learning model appropriately captures the interaction between the learner and its environment, yet as indicated by the allergist example, sometimes it does not. In studying the problem of learning binary relations, Goldman, Rivest and Schapire <ref> [5] </ref> introduced the model of self-directed learning in which the learner selects the order in which the instances are presented to the learner. In selecting the instance to predict on trial t the learner may use polynomial time, and all information obtained in the first t 1 trials. <p> As we have mentioned, Goldman, Rivest, and Schapire <ref> [5] </ref> use this model to study the problems of learning binary relations and total orders.
Reference: [6] <author> David Haussler. </author> <title> Learning conjunctive concepts in structural domains. </title> <type> Technical Report UCSC-CRL-87-1, </type> <institution> Computer Research Laboratory, University of Santa Cruz, </institution> <month> February </month> <year> 1987. </year>
Reference-contexts: Initially let G contain only the concept containing all instances and let S contain only the empty concept. Then, for each example, both G and S are appropriately updated. See Haussler <ref> [6, 7] </ref>, and also Rivest and Sloan [15] for a discussion of connections between Mitchell's version space algorithm and the distribution-free learning model. We now describe a relation between the version space algorithm and situations in which the learner can perform well under self-directed learning.
Reference: [7] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36 </volume> <pages> 177-221, </pages> <year> 1988. </year>
Reference-contexts: Initially let G contain only the concept containing all instances and let S contain only the empty concept. Then, for each example, both G and S are appropriately updated. See Haussler <ref> [6, 7] </ref>, and also Rivest and Sloan [15] for a discussion of connections between Mitchell's version space algorithm and the distribution-free learning model. We now describe a relation between the version space algorithm and situations in which the learner can perform well under self-directed learning.
Reference: [8] <author> David Haussler, Nick Littlestone, and Manfred K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <booktitle> In Proceedings of the Twenty-Ninth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 100-109, </pages> <year> 1988. </year>
Reference-contexts: Finally, a hypothesis h for C is a rule that given any x 2 X outputs in polynomial time a prediction for c (x). We are now ready to define the absolute mistake-bound variant of the on-line learning model <ref> [8, 11] </ref>. An on-line algorithm (or prediction algorithm) for C is an algorithm that runs under the following scenario. A learning session consists of a set of trials. In each trial, an adversary 1 presents the learner with an unlabeled instance x 2 X.
Reference: [9] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing. </journal> <note> To appear. A preliminary version is available in Proceedings of the Third Annual Workshop on Computational Learning Theory, </note> <year> 1990. </year>
Reference-contexts: Let the concept class of multiples be defined as follows. Let the instance space be the natural numbers, and let the concept class be all multiples of i for each i 2 N . This class has infinite VC-dimension <ref> [9] </ref>. 3 Nevertheless, we now show that sdc (multiples) = 1. The algorithm for obtaining this mistake bound is shown in Figure 7. Clearly this procedure finds the target concept while making only a single mistake.
Reference: [10] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning nested differences of intersection-closed concept classes. </title> <journal> Machine Learning, </journal> <volume> 5 </volume> <pages> 165-196, </pages> <year> 1990. </year> <note> Special issue for COLT 89. </note>
Reference-contexts: concept c 2 C with respect to the class C is a set I c having the property that c is the unique most specific concept consistent with the instances in I. (This generalizes the notation of a spanning set of an intersection-closed class given by Helmbold, Sloan, and Warmuth <ref> [10] </ref>.) Finally we define I (C) for concept class C as follows: I (C) = max fjI c j : I c is a minimal spanning set for c with respect to Cg: To provide some intuition for our more general result, we first consider the simple case for which I <p> We conjecture that condition required in Theorem 8 holds for all intersection-closed concept classes. Combined with the result from Helmbold, Sloan, and Warmuth <ref> [10] </ref> that I (C) vcd (C) would give the following. Conjecture 1 For all intersection-closed concept classes sdc (C) vcd (C). We now extend Theorem 8 to concept classes that are made of the disjunction of concepts from a concept class for which it currently applies.
Reference: [11] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Finally, a hypothesis h for C is a rule that given any x 2 X outputs in polynomial time a prediction for c (x). We are now ready to define the absolute mistake-bound variant of the on-line learning model <ref> [8, 11] </ref>. An on-line algorithm (or prediction algorithm) for C is an algorithm that runs under the following scenario. A learning session consists of a set of trials. In each trial, an adversary 1 presents the learner with an unlabeled instance x 2 X. <p> We then use a standard trick to learn the sign of each variable at a cost of only one mistake <ref> [11] </ref>. For arbitrarily chosen instances, predict that each one is negative until a mistake is made. Let x be the positive instance obtained on the first mistake. The sign of each relevant variable is given by its assignment in x.
Reference: [12] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples. </title> <booktitle> In Proceedings of the Thirtieth Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <month> October </month> <year> 1989. </year>
Reference-contexts: Then in Section 6 we prove that the model of self-directed learning is more powerful than all other commonly used on-line and query learning models. In particular, we show that this model is more powerful than all of the models considered by Maass and Turan <ref> [12, 13] </ref>. Next in Section 7 we study the relationship between the optimal mistake bound under self-directed learning and the Vapnik-Chervonenkis dimension. We first show that the VC-dimension can be arbitrarily larger than the complexity of self-directed learning. <p> Thus it is important to determine how this new learning models relates to both standard on-line learning algorithms and the various models of learning with queries introduced by Angluin [1]. Maass and Turan's <ref> [12, 13] </ref> work comparing the complexity of learning under the commonly studied on-line and query learning models is quite useful. <p> the monomials corresponding to the incorrectly predicted instances. 5.3 Orthogonal Rectangles in f0; 1; ; n1g d Finally, in this section we consider the concept class box d n of orthogonal rectangles in f0; 1; ; n 1g d . (This class has previously been studied by Maass and Turan <ref> [12] </ref> for other learning models.) 8 Learn-box-corner (corner pt) Query the corner point, predicting it is negative While no mistake has occurred Choose any unseen point of minimum Manhattan distance (L 1 norm distance) from the corner point. Query this unseen point, predicting it is negative. n . <p> Furthermore, the learner can be forced to make a mistake in finding each corner. 6 Relation to Other On-line and Query Learning Models In this section we build on the results of Maass and Turan <ref> [12, 13] </ref> by showing that the model of self-directed learning is more powerful than all of the on-line and query learning models which they considered. <p> Thus, the number of mistakes made by A 0 is at most the number of partial equivalence queries made by A. Finally, note that we showed that sdc (box d n ) = 2, whereas lc-partial (box d n ) = fi (d log n) <ref> [12] </ref>.
Reference: [13] <author> Wolfgang Maass and Gyorgy Turan. </author> <title> On the complexity of learning from counterexamples and membership queries. </title> <booktitle> In Proceedings of the Thirty First Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 203-210, </pages> <month> October </month> <year> 1990. </year>
Reference-contexts: Then in Section 6 we prove that the model of self-directed learning is more powerful than all other commonly used on-line and query learning models. In particular, we show that this model is more powerful than all of the models considered by Maass and Turan <ref> [12, 13] </ref>. Next in Section 7 we study the relationship between the optimal mistake bound under self-directed learning and the Vapnik-Chervonenkis dimension. We first show that the VC-dimension can be arbitrarily larger than the complexity of self-directed learning. <p> Thus it is important to determine how this new learning models relates to both standard on-line learning algorithms and the various models of learning with queries introduced by Angluin [1]. Maass and Turan's <ref> [12, 13] </ref> work comparing the complexity of learning under the commonly studied on-line and query learning models is quite useful. <p> Furthermore, the learner can be forced to make a mistake in finding each corner. 6 Relation to Other On-line and Query Learning Models In this section we build on the results of Maass and Turan <ref> [12, 13] </ref> by showing that the model of self-directed learning is more powerful than all of the on-line and query learning models which they considered.
Reference: [14] <author> Thomas M. Mitchell. </author> <title> Version spaces: A candidate elimination approach to rule learning. </title> <booktitle> In Proceedings IJCAI-77, </booktitle> <pages> pages 305-310, </pages> <month> August </month> <year> 1977. </year>
Reference-contexts: We first show that the VC-dimension can be arbitrarily larger than the complexity of self-directed learning. We then give a family of concept classes for which the complexity of self-directed learning is larger than the VC-dimension. Finally, in Section 8 we explore a relationship between Mitchell's version space algorithm <ref> [14] </ref> and the existence of self-directed learning algorithms that make few mistakes. 2 Motivation In this section we motivate the self-directed learning model by reviewing the allergist example given by Goldman, Rivest, and Schapire [5]. <p> Is there some characterization for the situations in which the learner can perform so well? As a partial answer to this question, we describe a relation between this work and Mitchell's version space algorithm <ref> [14] </ref>. We begin by describing Mitchell's version space algorithm.
Reference: [15] <author> Ronald L. Rivest and Robert Sloan. </author> <title> Learning complicated concepts reliably and usefully. </title> <booktitle> In Proceedings AAAI-88, </booktitle> <pages> pages 635-639, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: Initially let G contain only the concept containing all instances and let S contain only the empty concept. Then, for each example, both G and S are appropriately updated. See Haussler [6, 7], and also Rivest and Sloan <ref> [15] </ref> for a discussion of connections between Mitchell's version space algorithm and the distribution-free learning model. We now describe a relation between the version space algorithm and situations in which the learner can perform well under self-directed learning.
Reference: [16] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: Define d (m) = &lt; P d i for m d If C is a concept class of VC-dimension d on a finite set X with jXj = m, then the cardinality of C is at most d (m) <ref> [16, 18] </ref>.
Reference: [17] <author> Leslie Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: Blumer et al. [2] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant <ref> [17] </ref>. Related to the VC-dimension are the notions of maximal and maximum concept classes [4, 19]. A concept class is maximal if adding any concept to the class increases the VC dimension of the class.
Reference: [18] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and Its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
Reference-contexts: We define the self-directed learning complexity of a concept class C (sdc (C)) as follows: sdc (C) = max fopt (c)g where opt (c) is the optimal mistake bound for learning c under self-directed learning. We now define the Vapnik-Chervonenkis dimension <ref> [18] </ref>. Let X be any instance space, and C be a concept class over X. A finite set Y X is shattered by C if fc"Y j c 2 Cg = 2 Y . <p> Define d (m) = &lt; P d i for m d If C is a concept class of VC-dimension d on a finite set X with jXj = m, then the cardinality of C is at most d (m) <ref> [16, 18] </ref>.
Reference: [19] <author> E. Welzl. </author> <title> Complete range spaces. </title> <type> Unpublished manuscript, </type> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Blumer et al. [2] have shown that this combinatorial measure of a concept class characterizes the number of examples required for learning any concept in the class under the distribution-free or PAC model of Valiant [17]. Related to the VC-dimension are the notions of maximal and maximum concept classes <ref> [4, 19] </ref>. A concept class is maximal if adding any concept to the class increases the VC dimension of the class.
References-found: 19

