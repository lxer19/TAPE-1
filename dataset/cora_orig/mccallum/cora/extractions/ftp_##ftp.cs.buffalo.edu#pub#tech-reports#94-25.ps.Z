URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/94-25.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: shu@cs.buffalo.edu  
Title: Runtime Incremental Parallel Scheduling on Distributed Memory Computers  
Author: Wei Shu 
Address: Buffalo, NY 14260  
Affiliation: Department of Computer Science State University of New York at Buffalo  
Abstract: Runtime Incremental Parallel Scheduling (RIPS) is an alternative strategy to the commonly used dynamic scheduling. In this scheduling strategy, the system scheduling activity alternates with the underlying computation work. RIPS utilizes advanced parallel scheduling techniques to produce a low-overhead, high-quality load balancing and adapts to applications of nonuniform structures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. C. Athas. </author> <title> Fine Grain Concurrent Computations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, California Institute of Technology, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: In Table III, we compared RIPS to two other dynamic load balancing strategies: random allocation and gradient model. A randomized allocation strategy dictates that each processor, when it generates a new task, should send it to a randomly chosen processor <ref> [1, 2, 9] </ref>. One advantage of this strategy is simplicity of implementation. No local load information needs to be maintained, nor is any load information sent to other processors. Statistical analysis shows that randomized allocation has a respectable performance.
Reference: [2] <author> W. C. Athas and C. L. Seitz. </author> <title> Multicomputers: Message-passing concurrent computers. </title> <journal> IEEE Computer, </journal> <volume> 21(8) </volume> <pages> 9-24, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In Table III, we compared RIPS to two other dynamic load balancing strategies: random allocation and gradient model. A randomized allocation strategy dictates that each processor, when it generates a new task, should send it to a randomly chosen processor <ref> [1, 2, 9] </ref>. One advantage of this strategy is simplicity of implementation. No local load information needs to be maintained, nor is any load information sent to other processors. Statistical analysis shows that randomized allocation has a respectable performance.
Reference: [3] <author> S. B. Baden. </author> <title> Dynamic load balancing of a vortex calculation running on multiprocessors. </title> <type> Technical Report Vol. 22584, </type> <institution> Lawrence Berkeley Lab., </institution> <year> 1986. </year>
Reference-contexts: It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures [14, 18]. Some other works also deals with geographically structured problems <ref> [8, 4, 3] </ref>. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size.
Reference: [4] <author> M.J. Berger and S. Bokhari. </author> <title> A partitioning strategy for non-uniform problems on multipro cessors. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-26:570-580, </volume> <year> 1987. </year>
Reference-contexts: It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures [14, 18]. Some other works also deals with geographically structured problems <ref> [8, 4, 3] </ref>. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size. <p> In general, the number of phases in the ANY policy is larger the depth of the tree. Note that 15 Every node n in the fat tree has the following information in node [n]: parent: its parent ID numChild:the number of its children child <ref> [4] </ref>: its children's ID level: its level in the tree, the root has the highest level a: being its parent's the ath child There is a global table used during the tree construction: nonLeaf [maxLevel][maxNodeinLevel]: the node ID in the fat tree Scheduling tree construction algorithm for a 4-ary fat tree:
Reference: [5] <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algo rithms on distributed memory machines. </title> <journal> Concurrency: Practice and Experience, </journal> <note> accepted for publication, </note> <year> 1991. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [18, 8, 5, 7, 24] </ref>. A parallel scheduling is stable because of its synchronous operation. It uses global load information stored at every processor and is able to accurately balance the load. Parallel scheduling is scalable to massively parallel systems. <p> It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures [14, 18]. Some other works also deals with geographically structured problems [8, 4, 3]. The project PARTI automates prescheduling for nonuniform problems <ref> [19, 5] </ref>. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size.
Reference: [6] <author> Y.C. Chung and S. Ranka. </author> <title> Applications and performance analysis of a compile-time opti mization approach for list scheduling algorithms on distributed memory multiprocessors. </title> <booktitle> In Supercomputer '92, </booktitle> <month> November </month> <year> 1992. </year>
Reference-contexts: The static scheduling utilizes the knowledge of problem characteristics to reach a global op 1 timal, or near optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [12, 25, 11, 26, 6] </ref>. However, static scheduling has not been widely used in real-world applications yet. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling.
Reference: [7] <author> G. Cybenko. </author> <title> Dynamic load balancing for distributed memory multiprocessors. </title> <journal> J. of Parallel Distrib. Comput., </journal> <volume> 7 </volume> <pages> 279-301, </pages> <year> 1989. </year> <month> 22 </month>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [18, 8, 5, 7, 24] </ref>. A parallel scheduling is stable because of its synchronous operation. It uses global load information stored at every processor and is able to accurately balance the load. Parallel scheduling is scalable to massively parallel systems. <p> Some other works also deals with geographically structured problems [8, 4, 3]. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure <ref> [7] </ref>. It balances load for independnt tasks with an equal grain size. The method has been extended by Willebeek-LeMair and Reeves [24] so that the algorithm can run incrementally to correct the imbalanced load due to varied task grain sizes.
Reference: [8] <author> K. M. Dragon and J. L. Gustafson. </author> <title> A low-cost hypercube load balance algorithm. </title> <booktitle> In Proc. of the 4th Conf. on Hypercube Concurrent Computers and Applications, </booktitle> <pages> pages 583-590, </pages> <year> 1989. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [18, 8, 5, 7, 24] </ref>. A parallel scheduling is stable because of its synchronous operation. It uses global load information stored at every processor and is able to accurately balance the load. Parallel scheduling is scalable to massively parallel systems. <p> It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures [14, 18]. Some other works also deals with geographically structured problems <ref> [8, 4, 3] </ref>. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size.
Reference: [9] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Trans. Software Eng., </journal> <volume> SE-12(5):662-674, </volume> <month> May </month> <year> 1986. </year>
Reference-contexts: In addition, it is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [9, 10, 21] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime. <p> In Table III, we compared RIPS to two other dynamic load balancing strategies: random allocation and gradient model. A randomized allocation strategy dictates that each processor, when it generates a new task, should send it to a randomly chosen processor <ref> [1, 2, 9] </ref>. One advantage of this strategy is simplicity of implementation. No local load information needs to be maintained, nor is any load information sent to other processors. Statistical analysis shows that randomized allocation has a respectable performance.
Reference: [10] <author> D. L. Eager, E. D. Lazowska, and J. Zahorjan. </author> <title> A comparison of receiver-initiated and sender initiated adaptive load sharing. </title> <booktitle> Performance Eval., </booktitle> <volume> 6(1) </volume> <pages> 53-68, </pages> <month> March </month> <year> 1986. </year>
Reference-contexts: In addition, it is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [9, 10, 21] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime.
Reference: [11] <author> H. El-Rewini and H. H. Ali. </author> <title> Scheduling conditional branching using representative task graphs. </title> <journal> The Journal of Combinatorial Mathematics and Combinatorial Computing, </journal> <year> 1991. </year>
Reference-contexts: The static scheduling utilizes the knowledge of problem characteristics to reach a global op 1 timal, or near optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [12, 25, 11, 26, 6] </ref>. However, static scheduling has not been widely used in real-world applications yet. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling.
Reference: [12] <author> H. El-Rewini and T. G. Lewis. </author> <title> Scheduling parallel program tasks onto arbitrary target machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <month> June </month> <year> 1990. </year>
Reference-contexts: The static scheduling utilizes the knowledge of problem characteristics to reach a global op 1 timal, or near optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [12, 25, 11, 26, 6] </ref>. However, static scheduling has not been widely used in real-world applications yet. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling.
Reference: [13] <author> H. El-Rewini, T. G. Lewis, and H. H. Ali. </author> <title> Task Scheduling in Parallel and Distributed Systems. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: 1. Introduction One of the challenges in programming distributed memory parallel machines is to schedule work to processors <ref> [13] </ref>. There are two types of application problem structures: problems with a predictable structure, also called static problems, and problems with an unpredictable structure, called dynamic problems.
Reference: [14] <author> G. C. Fox, M. A. Johnson, G. A. Lyzenga, S. W. Otto, J. K. Salmon, and D. W. Walker. </author> <title> Solving Problems on Concurrent Processors, volume I. </title> <publisher> Prentice-Hall, </publisher> <year> 1988. </year>
Reference-contexts: It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures <ref> [14, 18] </ref>. Some other works also deals with geographically structured problems [8, 4, 3]. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7].
Reference: [15] <author> L. V. Kale. </author> <title> Comparing the performance of two dynamic load distribution methods. </title> <booktitle> In Int'l Conf. on Parallel Processing, </booktitle> <pages> pages 8-12, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This leads to a higher communication load on large systems. Since the bandwidth consumed by a long-distance message is certainly larger, the system is more likely to be communication bound, compared to a system using other load balancing strategies that encourage locality. In the gradient model <ref> [17, 16, 15] </ref>, instead of trying to allocate a newly generated task to other processors, the task is queued at the generating processor and waits for some processor to request it. A separate, asynchronous process on each processor is responsible for balancing the load.
Reference: [16] <author> F. C. H. Lin. </author> <title> Load Balancing and Fault Tolerance in Applicative Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Utah, </institution> <month> August </month> <year> 1985. </year>
Reference-contexts: This leads to a higher communication load on large systems. Since the bandwidth consumed by a long-distance message is certainly larger, the system is more likely to be communication bound, compared to a system using other load balancing strategies that encourage locality. In the gradient model <ref> [17, 16, 15] </ref>, instead of trying to allocate a newly generated task to other processors, the task is queued at the generating processor and waits for some processor to request it. A separate, asynchronous process on each processor is responsible for balancing the load.
Reference: [17] <author> F. C. H. Lin and R. M. Keller. </author> <title> Gradient model: A demand-driven load balancing scheme. </title> <booktitle> In Int'l Conf. on Distributed Computing System, </booktitle> <pages> pages 329-336, </pages> <year> 1986. </year>
Reference-contexts: This leads to a higher communication load on large systems. Since the bandwidth consumed by a long-distance message is certainly larger, the system is more likely to be communication bound, compared to a system using other load balancing strategies that encourage locality. In the gradient model <ref> [17, 16, 15] </ref>, instead of trying to allocate a newly generated task to other processors, the task is queued at the generating processor and waits for some processor to request it. A separate, asynchronous process on each processor is responsible for balancing the load.
Reference: [18] <author> J.K. Salmon. </author> <title> Parallel hierarchical N-body methods. </title> <type> Technical report, Tech. Report, CRPC 90-14, </type> <institution> Center for Research in Parallel Computing, Caltech, </institution> <year> 1990., 1990. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [18, 8, 5, 7, 24] </ref>. A parallel scheduling is stable because of its synchronous operation. It uses global load information stored at every processor and is able to accurately balance the load. Parallel scheduling is scalable to massively parallel systems. <p> It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures <ref> [14, 18] </ref>. Some other works also deals with geographically structured problems [8, 4, 3]. The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7].
Reference: [19] <author> J. Saltz, R. Mirchandaney, R. Smith, D. Nicol, and K. Crowley. </author> <title> The PARTY parallel run time system. </title> <booktitle> In Proceedings of the SIAM Conference on Parallel Processing for Scientific Computing. </booktitle> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: It is sometimes referred as prescheduling which is more closely related to RIPS. Fox et al. first adapts prescheduling to application problems with geographical structures [14, 18]. Some other works also deals with geographically structured problems [8, 4, 3]. The project PARTI automates prescheduling for nonuniform problems <ref> [19, 5] </ref>. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size.
Reference: [20] <author> J. Shen and J. A. McCammon. </author> <title> Molecular dynamics simulation of superoxide interacting with superoxide dismutase. </title> <journal> Chemical Physics, </journal> <volume> 158 </volume> <pages> 191-198, </pages> <year> 1991. </year>
Reference-contexts: The number of tasks generated and the computation amount in each task are unpredictable. The second one, a molecular dynamics program named GROMOS, is a real application problem [23, 22]. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms <ref> [20] </ref>. The cutoff radius is predefined to 8 A, 12 A, and 16 A. GROMOS has a more predictable structure. The number of tasks is known with the given input data, but the computation amount in each task varies. Thus a load balancing mechanism is necessary.
Reference: [21] <author> Niranjan G. Shivaratri, Phillip Krieger, and Mukesh Singhal. </author> <title> Load distributing for locally distributed systems. </title> <journal> IEEE Computer, </journal> <volume> 25(12) </volume> <pages> 33-44, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: In addition, it is not able to balance the load for dynamic problems. Dynamic scheduling has certain advantages. It is a general approach suitable for a wide range of applications. It can adjust load distribution based on runtime system load information <ref> [9, 10, 21] </ref>. However, most runtime scheduling algorithms utilize neither the characteristics information of application problems, nor global load information for load balancing decision. Efforts to collect load information for a scheduling decision certainly compete the resource with the underlying computation during runtime. <p> This mechanism is 5 useful for an adaptive algorithm that changes between a one-queue policy and a two-queue policy. Typically, a runtime scheduling algorithm has four components: a transfer policy, a selection policy, a location policy, and an information policy <ref> [21] </ref>. The transfer policy determines whether a processor is in a suitable state to participate in a task transfer. The selection policy determines which tasks should be transferred. The location policy determines to which processor a task selected for transfer should be sent.
Reference: [22] <author> Reinhard v. Hanxleden and Ken Kennedy. </author> <title> Relaxing SIMD control flow constraints using loop transformations. </title> <type> Technical Report CRPC-TR92207, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: The first one, the N-queen problem in exhausted search, is of a very irregular and dynamic structure. The number of tasks generated and the computation amount in each task are unpredictable. The second one, a molecular dynamics program named GROMOS, is a real application problem <ref> [23, 22] </ref>. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms [20]. The cutoff radius is predefined to 8 A, 12 A, and 16 A. GROMOS has a more predictable structure.
Reference: [23] <author> W. F. van Gunsteren and H. J. C. Berendsen. GROMOS: </author> <title> GROningen MOlecular Simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: The first one, the N-queen problem in exhausted search, is of a very irregular and dynamic structure. The number of tasks generated and the computation amount in each task are unpredictable. The second one, a molecular dynamics program named GROMOS, is a real application problem <ref> [23, 22] </ref>. The test data for GROMOS is the bovine superoxide dismutase molecule (SOD), which has 6968 atoms [20]. The cutoff radius is predefined to 8 A, 12 A, and 16 A. GROMOS has a more predictable structure.
Reference: [24] <author> Marc Willebeek-LeMair and Anthony P. Reeves. </author> <title> Strategies for dynamic load balancing on highly parallel computers. </title> <journal> IEEE Trans. Parallel and Distributed System, </journal> <volume> 9(4) </volume> <pages> 979-993, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: This scheduling strategy should be able to generate well-balanced load without incurring large overhead. With advanced parallel scheduling techniques, this ideal scheduling becomes feasible. In a parallel scheduling, all processors are cooperated together to schedule work. Some parallel scheduling algorithms have been introduced in <ref> [18, 8, 5, 7, 24] </ref>. A parallel scheduling is stable because of its synchronous operation. It uses global load information stored at every processor and is able to accurately balance the load. Parallel scheduling is scalable to massively parallel systems. <p> The project PARTI automates prescheduling for nonuniform problems [19, 5]. The domain exchange method (DEM) is a parallel scheduling algorithm applied to application problems without geographical structure [7]. It balances load for independnt tasks with an equal grain size. The method has been extended by Willebeek-LeMair and Reeves <ref> [24] </ref> so that the algorithm can run incrementally to correct the imbalanced load due to varied task grain sizes. The DEM scheduling algorithm generates redundant communications, therefore leading to a large communication overhead.
Reference: [25] <author> M. Y. Wu and D. D. Gajski. Hypertool: </author> <title> A programming aid for message-passing systems. </title> <journal> IEEE Trans. Parallel and Distributed Systems, </journal> <volume> 1(3) </volume> <pages> 330-343, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: The static scheduling utilizes the knowledge of problem characteristics to reach a global op 1 timal, or near optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [12, 25, 11, 26, 6] </ref>. However, static scheduling has not been widely used in real-world applications yet. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling.
Reference: [26] <author> T. Yang and A. Gerasoulis. </author> <title> PYRROS: Static task scheduling and code generation for message passing multiprocessors. </title> <booktitle> The 6th ACM Int'l Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1992. </year> <month> 23 </month>
Reference-contexts: The static scheduling utilizes the knowledge of problem characteristics to reach a global op 1 timal, or near optimal, solution with well-balanced load. It has recently attracted considerable attention among the research community <ref> [12, 25, 11, 26, 6] </ref>. However, static scheduling has not been widely used in real-world applications yet. The quality of scheduling heavily relies on accuracy of weight estimation. The requirement of large memory space to store the task graph restricts the scalability of static scheduling.
References-found: 26

