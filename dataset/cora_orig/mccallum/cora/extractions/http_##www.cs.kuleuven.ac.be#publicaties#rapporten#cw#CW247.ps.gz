URL: http://www.cs.kuleuven.ac.be/publicaties/rapporten/cw/CW247.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/publicaties/rapporten/CW1997.html
Root-URL: 
Email: e-mail: fHendrik.Blockeel, Luc.DeRaedtg@cs.kuleuven.ac.be  
Title: Top-down Induction of Logical Decision Trees  
Author: Hendrik Blockeel and Luc De Raedt 
Note: Tilde system, is presented and experimentally evaluated.  
Date: January 21, 1997  
Address: Celestijnenlaan 200A 3001 Heverlee  
Affiliation: Katholieke Universiteit Leuven Department of Computer Science  
Abstract: Top-down induction of decision trees (TDIDT) is a very popular machine learning technique. Up till now, it has mainly been used for propositional learning, but seldomly for relational learning or inductive logic programming. The main contribution of this paper is the introduction of logical decision trees, which make it possible to use TDIDT in inductive logic programming. An implementation of this top-down induction of logical decision trees, the 
Abstract-found: 1
Intro-found: 1
Reference: [ Bergadano and Giordana, 1988 ] <author> F. Bergadano and A. Giordana. </author> <title> A knowledge in tensive approach to concept induction. </title> <booktitle> In Proceedings of the 5th International Workshop on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Within attribute value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. Yet, within first order approaches to concept-learning, only a few learning systems have made use of decision tree techniques ( <ref> [ Watanabe and Rendell, 1991; Bergadano and Giordana, 1988 ] </ref> ), and in the field of inductive logic programming, the approach has almost totally been ignored. With the exception of [ Bostrom, 1995 ] and some systems that transform ILP problems into propositional form (e.g.
Reference: [ Bostrom, 1995 ] <author> H. Bostrom. </author> <title> Covering vs. divide-and-conquer for top-down induc tion of logic programs. </title> <booktitle> In Proceedings of the 14th International Joint Conference on Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (such as CN2 [ Clark and Niblett, 1989 ] , AQ [ Michalski et al., 1986 ] ), which are based on covering strategies (cf. <ref> [ Bostrom, 1995 ] </ref> ). Within attribute value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. <p> With the exception of <ref> [ Bostrom, 1995 ] </ref> and some systems that transform ILP problems into propositional form (e.g. LINUS [ Lavrac and Dzeroski, 1994 ] , Indigo [ Geibel and Wysotzki, 1996 ] ) almost every ILP system uses a covering approach. <p> An interesting conclusion that can be drawn from these experiments is that, although several authors (e.g. <ref> [ Bostrom, 1995 ] </ref> , [ Watanabe and Rendell, 1991 ] ) have mentioned the fact that rule based systems return on the average a more compact theory than decision tree induction systems, the actual induction method that is used probably has more influence on this than the representation itself. <p> The fact that more than one literal can occur in a node of a logical decision tree allows Tilde to handle determinate literals, lookahead facilities etc. more easily than Watanabe's system could. This work is also related to <ref> [ Bostrom, 1995 ] </ref> , on induction of logic programs using the divide-and-conquer paradigm. The main difference is that we use a different logical setting (learning from interpretations instead of learning from entailment), aimed specifically at classification rather than logic program synthesis.
Reference: [ Bratko, 1990 ] <author> I. Bratko. </author> <title> Prolog Programming for Artificial Intelligence. </title> <publisher> Addison Wesley, </publisher> <year> 1990. </year> <note> 2nd Edition. </note>
Reference-contexts: Section 6 contains an empirical evaluation of this implementation, and finally, in Section 7 we conclude and touch upon related work. 2 The Learning Problem We assume familiarity with the Prolog programming language (see e.g. <ref> [ Bratko, 1990 ] </ref> ).
Reference: [ Catlett, 1991 ] <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 164-178. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: This has also yielded positive results in attribute value learning, cf. <ref> [ Catlett, 1991 ] </ref> . In our approach to discretization, the user has to identify the relevant queries and the variables for which the values are to be discretized using a template of the form : 12 to be discretized (Query, Varlist).
Reference: [ Clark and Niblett, 1989 ] <author> P. Clark and T. Niblett. </author> <title> The CN2 algorithm. </title> <journal> Machine Learning, </journal> <volume> 3(4) </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: 1 Introduction Top-down induction of decision trees [ Quinlan, 1986; Quinlan, 1993a ] is the best known and most succesful machine learning technique. It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (such as CN2 <ref> [ Clark and Niblett, 1989 ] </ref> , AQ [ Michalski et al., 1986 ] ), which are based on covering strategies (cf. [ Bostrom, 1995 ] ). Within attribute value learning (or propositional concept-learning) TDIDT is more popular than the covering approach.
Reference: [ De Raedt and Dehaspe, 1996 ] <author> L. De Raedt and L. Dehaspe. </author> <title> Clausal discovery. </title> <booktitle> Machine Learning, </booktitle> <year> 1996. </year> <note> To appear. </note>
Reference-contexts: The system will itself determine meaningful constants and generate clauses with these constants filled in. Tilde's dynamic refinement is inspired by the so-called "call handling" procedure implemented in the Claudien system <ref> [ De Raedt and Dehaspe, 1996; Muggle-ton and Page, 1994 ] </ref> .
Reference: [ De Raedt and Dzeroski, 1994 ] <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year> <month> 19 </month>
Reference-contexts: This results in the Tilde system, which is the main topic of this paper. Tilde works within the learning from interpretations paradigm introduced by <ref> [ De Raedt and Dzeroski, 1994 ] </ref> , and used in the ICL system of [ De Raedt and Van Laer, 1995 ] . 1 Within Tilde, we also incorporated two other novelties w.r.t. inductive logic programming: discretization of numeric attributes (based on [ Van Laer et al., 1996; Fayyad and <p> We essentially use the learning from interpretations paradigm for inductive logic programming, introduced by <ref> [ De Raedt and Dzeroski, 1994 ] </ref> , used in ICL [ De Raedt and Van Laer, 1995 ] , and related to other inductive logic programming settings in [ De Raedt, 1996 ] .
Reference: [ De Raedt and Van Laer, 1995 ] <author> L. De Raedt and W. Van Laer. </author> <title> Inductive constraint logic. </title> <booktitle> In Proceedings of the 5th Workshop on Algorithmic Learning Theory, volume 997 of Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: This results in the Tilde system, which is the main topic of this paper. Tilde works within the learning from interpretations paradigm introduced by [ De Raedt and Dzeroski, 1994 ] , and used in the ICL system of <ref> [ De Raedt and Van Laer, 1995 ] </ref> . 1 Within Tilde, we also incorporated two other novelties w.r.t. inductive logic programming: discretization of numeric attributes (based on [ Van Laer et al., 1996; Fayyad and Irani, 1993 ] ) and dynamic lookahead facilities. <p> We essentially use the learning from interpretations paradigm for inductive logic programming, introduced by [ De Raedt and Dzeroski, 1994 ] , used in ICL <ref> [ De Raedt and Van Laer, 1995 ] </ref> , and related to other inductive logic programming settings in [ De Raedt, 1996 ] . In this paradigm, each example is a Prolog knowledge base (i.e. a set of definite clauses), encoding the specific properties of the example.
Reference: [ De Raedt, 1996 ] <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year>
Reference-contexts: We essentially use the learning from interpretations paradigm for inductive logic programming, introduced by [ De Raedt and Dzeroski, 1994 ] , used in ICL [ De Raedt and Van Laer, 1995 ] , and related to other inductive logic programming settings in <ref> [ De Raedt, 1996 ] </ref> . In this paradigm, each example is a Prolog knowledge base (i.e. a set of definite clauses), encoding the specific properties of the example. Furthermore, each example is classified into one of a finite set of possible classes.
Reference: [ Dietterich et al., 1996 ] <author> Thomas G. Dietterich, Richard H. Lathrop, and Tomas Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, </journal> <note> 1996. In press. </note>
Reference-contexts: Then, a propositional decision tree induction system is used for the actual induction process. Figures for this table were copied from [ Quinlan, 1996 ] (FOIL, FFOIL, FORS) and [ Geibel and Wysotzki, 1996 ] (Indigo). 6.3 Musk Dataset The musk dataset was studied by Dietterich et al. <ref> [ Dietterich et al., 1996 ] </ref> , who donated it to the UCI repository [ Merz and Murphy, 1996 ] . <p> The multiple instance problem poses problems for propositional learners such as C4.5 or neural networks. There is a feature vector for each conformation, but the class of each conformation (musk / non-musk) is not known. Dietterich et al. <ref> [ Dietterich et al., 1996 ] </ref> developed a number of algorithms that explicitly deal with the multiple instance problem, thereby practically solving the problem for those cases where the theory can be represented as a set of axis-parallel rectangles (APR's). <p> 1 bound, L 2 = L 3 ; therefore the test with L 3 has been skipped). 1 bound 2 bounds 3 bounds 5 bounds L 1 79.3 % 85.9 % 83.7 % 83.7 % L 3 (77.3 %) 86.9 % 81.7 % 79.7 % The following table, taken from <ref> [ Dietterich et al., 1996 ] </ref> , allows to compare Tilde's performance with that of other algorithms. The algorithms marked with an asterisk are those that have been adapted specifically for the multiple-instance problem.
Reference: [ Dolsak and Muggleton, 1992 ] <author> B. Dolsak and S. Muggleton. </author> <title> The application of Inductive Logic Programming to finite element mesh design. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive logic programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: Table 2 suggests that lookahead may offer a slight advantage, but the results are not at all convincing. 6.2 Mesh Dataset The Mesh dataset, since its introduction in ILP by <ref> [ Dolsak and Muggleton, 1992 ] </ref> , has been used several times as a benchmark to compare ILP systems. In many engineering applications, objects are described using finite element meshes. The resolution of such a mesh depends on the shape of the object and of neighbouring objects.
Reference: [ Dougherty et al., 1995 ] <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretization of continuous features. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proc. Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: The resulting numeric attributes (indicated by Varlist) are then discretized using a simple modification of Fayyad and Irani's method. The details of the Fayyad and Irani's method can be found in [ Fayyad and Irani, 1993 ] and <ref> [ Dougherty et al., 1995 ] </ref> .
Reference: [ Fayyad and Irani, 1993 ] <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: the learning from interpretations paradigm introduced by [ De Raedt and Dzeroski, 1994 ] , and used in the ICL system of [ De Raedt and Van Laer, 1995 ] . 1 Within Tilde, we also incorporated two other novelties w.r.t. inductive logic programming: discretization of numeric attributes (based on <ref> [ Van Laer et al., 1996; Fayyad and Irani, 1993 ] </ref> ) and dynamic lookahead facilities. We also report on a number of encouraging experiments, in the domains of mutagenesis, finite element mesh design, and musk molecules. This text is organized as follows. <p> The resulting numeric attributes (indicated by Varlist) are then discretized using a simple modification of Fayyad and Irani's method. The details of the Fayyad and Irani's method can be found in <ref> [ Fayyad and Irani, 1993 ] </ref> and [ Dougherty et al., 1995 ] .
Reference: [ Geibel and Wysotzki, 1996 ] <author> P. Geibel and F. Wysotzki. </author> <title> Learning relational con cepts with decision trees. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 166-174, </pages> <month> July </month> <year> 1996. </year>
Reference-contexts: With the exception of [ Bostrom, 1995 ] and some systems that transform ILP problems into propositional form (e.g. LINUS [ Lavrac and Dzeroski, 1994 ] , Indigo <ref> [ Geibel and Wysotzki, 1996 ] </ref> ) almost every ILP system uses a covering approach. The main reason why divide-and-conquer approaches are not popular within inductive logic programming, lies in the discrepancies between the clausal representation employed within inductive logic programming and the structure underlying a decision tree. <p> Figures for Progol and FOIL have been taken from [ Srinivasan et al., 1995 ] , those for Indigo from <ref> [ Geibel and Wysotzki, 1996 ] </ref> . We conclude that Tilde's results are at a par with Progol's, as far as accuracy is concerned. The one result given for Indigo suggests, however, that it is possible to do better. <p> FOIL [ Quinlan, 1993b ] is a general-purpose ILP system. FFOIL [ Quinlan, 1996 ] is a variant of it that can only learn functional definitions, but is very good at that. FORS [ Karalic, 1995 ] is also specialized for learning functional definitions. Indigo <ref> [ Geibel and Wysotzki, 1996 ] </ref> uses the transformational approach to ILP: it first transforms the learning data into a propositional representation. Then, a propositional decision tree induction system is used for the actual induction process. <p> Then, a propositional decision tree induction system is used for the actual induction process. Figures for this table were copied from [ Quinlan, 1996 ] (FOIL, FFOIL, FORS) and <ref> [ Geibel and Wysotzki, 1996 ] </ref> (Indigo). 6.3 Musk Dataset The musk dataset was studied by Dietterich et al. [ Dietterich et al., 1996 ] , who donated it to the UCI repository [ Merz and Murphy, 1996 ] .
Reference: [ Karalic, 1995 ] <author> A. Karalic. </author> <title> First Order Regression. </title> <type> PhD thesis, </type> <institution> Faculty of Electrical Engineering and Computer Science, University of Ljubljana, Slovenia, </institution> <year> 1995. </year>
Reference-contexts: Table 3 compares Tilde's performance on this dataset with several state-of-the-art systems. FOIL [ Quinlan, 1993b ] is a general-purpose ILP system. FFOIL [ Quinlan, 1996 ] is a variant of it that can only learn functional definitions, but is very good at that. FORS <ref> [ Karalic, 1995 ] </ref> is also specialized for learning functional definitions. Indigo [ Geibel and Wysotzki, 1996 ] uses the transformational approach to ILP: it first transforms the learning data into a propositional representation. Then, a propositional decision tree induction system is used for the actual induction process.
Reference: [ Lavrac and Dzeroski, 1994 ] <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Program ming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: With the exception of [ Bostrom, 1995 ] and some systems that transform ILP problems into propositional form (e.g. LINUS <ref> [ Lavrac and Dzeroski, 1994 ] </ref> , Indigo [ Geibel and Wysotzki, 1996 ] ) almost every ILP system uses a covering approach.
Reference: [ Merz and Murphy, 1996 ] <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html] , 1996. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: Figures for this table were copied from [ Quinlan, 1996 ] (FOIL, FFOIL, FORS) and [ Geibel and Wysotzki, 1996 ] (Indigo). 6.3 Musk Dataset The musk dataset was studied by Dietterich et al. [ Dietterich et al., 1996 ] , who donated it to the UCI repository <ref> [ Merz and Murphy, 1996 ] </ref> . Dietterich used these data to study the so-called multiple instance problem: an example corresponds to multiple feature vectors, and is to be classified as positive if at least one of its feature vectors has certain properties.
Reference: [ Michalski et al., 1986 ] <author> R. Michalski, I. Mozetic, J. Hong, and N. Lavrac. </author> <title> The AQ15 inductive learning system: an overview and experiments. </title> <booktitle> In Proceedings of IMAL 1986, </booktitle> <address> Orsay, </address> <year> 1986. </year> <institution> Universite de Paris-Sud. </institution>
Reference-contexts: It has been used to solve numerous practical problems. It employs a divide-and-conquer strategy, and in this it differs from its rule-based competitors (such as CN2 [ Clark and Niblett, 1989 ] , AQ <ref> [ Michalski et al., 1986 ] </ref> ), which are based on covering strategies (cf. [ Bostrom, 1995 ] ). Within attribute value learning (or propositional concept-learning) TDIDT is more popular than the covering approach.
Reference: [ Muggleton and Page, 1994 ] <author> S. Muggleton and D. </author> <title> Page. Beyond first order learning: inductive learning with higher order logic. </title> <type> Technical report, </type> <institution> OUCL Programming Research Group Technical Report PRG-TR-13-94, </institution> <year> 1994. </year>
Reference: [ Muggleton, 1995 ] <author> S. Muggleton. </author> <title> Inverse entailment and progol. </title> <journal> New Generation Computing, </journal> <volume> 13, </volume> <year> 1995. </year>
Reference-contexts: Muggleton's Prolog system <ref> [ Muggleton, 1995 ] </ref> , for instance, allows the user to specify that a constant, and not a variable, should be put in a certain position in a literal. The system will itself determine meaningful constants and generate clauses with these constants filled in.
Reference: [ Quinlan, 1986 ] <author> J.R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1:81 106, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Top-down induction of decision trees <ref> [ Quinlan, 1986; Quinlan, 1993a ] </ref> is the best known and most succesful machine learning technique. It has been used to solve numerous practical problems.
Reference: [ Quinlan, 1993a ] <author> J. Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. Morgan Kaufmann series in machine learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Top-down induction of decision trees <ref> [ Quinlan, 1986; Quinlan, 1993a ] </ref> is the best known and most succesful machine learning technique. It has been used to solve numerous practical problems. <p> In this section, we discuss the heuritics and the language definition formalism that have been chosen, and also a number of other practical aspects. 5.1 The Heuristic Function The implementation of Tilde is strongly based on C4.5 <ref> [ Quinlan, 1993a ] </ref> , one of the most successful systems for learning decision trees in the attribute value context. This is, among other things, reflected in the heuristic value assigned to tests. As in 7 C4.5, the gain ratio of a test is used to evaluate it. <p> This will be larger for clauses that split the examples more evenly. The gain ratio is then defined as gain divided by splitting information. Quinlan <ref> [ Quinlan, 1993a ] </ref> mentions that this ratio is a better criterion for the usefulness of a clause than gain itself, because some tests inherently stand a good chance of leading to high gain, without really being relevant. <p> Quinlan notes that this is especially the case when the test can have more than two outcomes, but even when only two outcomes are possible (which is the case in our implementation, a clause can only succeed or fail), gainratio usually chooses better tests than gain ( <ref> [ Quinlan, 1993a ] </ref> , p. 24).
Reference: [ Quinlan, 1993b ] <author> J.R. Quinlan. </author> <title> FOIL: A midterm report. </title> <editor> In P. Brazdil, editor, </editor> <booktitle> Proceedings of the 6th European Conference on Machine Learning, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: It is possible that the constant generator succeeds multiple times, generating multiple values for L; in that case, each value gives rise to a new set of literals. This way, it is possible to simulate the behaviour of many ILP systems. FOIL <ref> [ Quinlan, 1993b ] </ref> , for instance, will determine constants based on the values that effectively occur in the data for that literal. For the comparison of ages, the constant occurring in the test is always an age that occurs in the data. <p> This implies that the gain ratio heuristic may not work very well for conjunctions introducing new variables. The problem has been acknowledged a long time ago, and greedy ILP systems sometimes have some method for alleviating it. The FOIL system <ref> [ Quinlan, 1993b ] </ref> , for instance, handles determinate literals (which cannot result in gain but introduce new, possibly interesting variables) by adding them automatically to the clause. <p> Because this number depends not only on the edge itself, but also on neighbouring edges, the learning task is a typical ILP task. Table 3 compares Tilde's performance on this dataset with several state-of-the-art systems. FOIL <ref> [ Quinlan, 1993b ] </ref> is a general-purpose ILP system. FFOIL [ Quinlan, 1996 ] is a variant of it that can only learn functional definitions, but is very good at that. FORS [ Karalic, 1995 ] is also specialized for learning functional definitions.
Reference: [ Quinlan, 1996 ] <author> J. R. Quinlan. </author> <title> Learning first-order definitions of functions. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 5 </volume> <pages> 139-161, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Because this number depends not only on the edge itself, but also on neighbouring edges, the learning task is a typical ILP task. Table 3 compares Tilde's performance on this dataset with several state-of-the-art systems. FOIL [ Quinlan, 1993b ] is a general-purpose ILP system. FFOIL <ref> [ Quinlan, 1996 ] </ref> is a variant of it that can only learn functional definitions, but is very good at that. FORS [ Karalic, 1995 ] is also specialized for learning functional definitions. <p> Indigo [ Geibel and Wysotzki, 1996 ] uses the transformational approach to ILP: it first transforms the learning data into a propositional representation. Then, a propositional decision tree induction system is used for the actual induction process. Figures for this table were copied from <ref> [ Quinlan, 1996 ] </ref> (FOIL, FFOIL, FORS) and [ Geibel and Wysotzki, 1996 ] (Indigo). 6.3 Musk Dataset The musk dataset was studied by Dietterich et al. [ Dietterich et al., 1996 ] , who donated it to the UCI repository [ Merz and Murphy, 1996 ] .
Reference: [ Srinivasan et al., 1995 ] <author> A. Srinivasan, S.H. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by inductive logic programming systems. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming. </booktitle> <publisher> IOS Press, </publisher> <year> 1995. </year>
Reference-contexts: The aim is to discriminate mutagenic molecules from non-mutagenic ones, by looking at their molecular structure (i.e. the atoms and bonds of which it consists). <ref> [ Srinivasan et al., 1995 ] </ref> introduces four levels of background knowledge that can be used with these data. Most experiments that have been done on the Mutagenesis dataset use one of these. We briefly describe the four backgrounds. <p> * B 3 : as B 2 , but 2 attributes describing the molecule as a whole have been added, of which experts know they are relevant * B 4 : as B 3 , with explicit knowledge about complex structures (benzene rings etc.) added 6.1.1 Comparison With Other Systems <ref> [ Srinivasan et al., 1995 ] </ref> compares the accuracies and complexities of the induced theories for the ILP systems FOIL and Progol, as well as the time consumed by the induction process. We have performed experiments on all four levels of background knowledge. <p> Figures for Progol and FOIL have been taken from <ref> [ Srinivasan et al., 1995 ] </ref> , those for Indigo from [ Geibel and Wysotzki, 1996 ] . We conclude that Tilde's results are at a par with Progol's, as far as accuracy is concerned. The one result given for Indigo suggests, however, that it is possible to do better.
Reference: [ Van Laer et al., 1996 ] <author> W. Van Laer, S. Dzeroski, and L. De Raedt. </author> <title> Multi-class problems and discretization in ICL (extended abstract). </title> <booktitle> In Proceedings of the MLnet Familiarization Workshop on Data Mining with Inductive Logic Programming (ILP for KDD), </booktitle> <year> 1996. </year>
Reference-contexts: the learning from interpretations paradigm introduced by [ De Raedt and Dzeroski, 1994 ] , and used in the ICL system of [ De Raedt and Van Laer, 1995 ] . 1 Within Tilde, we also incorporated two other novelties w.r.t. inductive logic programming: discretization of numeric attributes (based on <ref> [ Van Laer et al., 1996; Fayyad and Irani, 1993 ] </ref> ) and dynamic lookahead facilities. We also report on a number of encouraging experiments, in the domains of mutagenesis, finite element mesh design, and musk molecules. This text is organized as follows.
Reference: [ Watanabe and Rendell, 1991 ] <author> L. Watanabe and L. Rendell. </author> <title> Learning structural decision trees from examples. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 770-776, </pages> <year> 1991. </year> <month> 21 </month>
Reference-contexts: Within attribute value learning (or propositional concept-learning) TDIDT is more popular than the covering approach. Yet, within first order approaches to concept-learning, only a few learning systems have made use of decision tree techniques ( <ref> [ Watanabe and Rendell, 1991; Bergadano and Giordana, 1988 ] </ref> ), and in the field of inductive logic programming, the approach has almost totally been ignored. With the exception of [ Bostrom, 1995 ] and some systems that transform ILP problems into propositional form (e.g. <p> Our main contribution is the introduction of a logical decision tree representation that corresponds to a clausal representation. Logical decision trees upgrade the attribute value representations used within classical TDIDT algorithms, and also generalize the earlier work by <ref> [ Watanabe and Rendell, 1991 ] </ref> . Given the logical decision tree representation, it is easy to design and implement an algorithm for top-down induction of logical decision trees by adapting C4.5's heuristics. This results in the Tilde system, which is the main topic of this paper. <p> An interesting conclusion that can be drawn from these experiments is that, although several authors (e.g. [ Bostrom, 1995 ] , <ref> [ Watanabe and Rendell, 1991 ] </ref> ) have mentioned the fact that rule based systems return on the average a more compact theory than decision tree induction systems, the actual induction method that is used probably has more influence on this than the representation itself. <p> And finally, because their ability to learn several classes at once, they seem to be particularly advantageous for multi-class classification tasks. Logical decision trees are a generalization of Watanabe's structural decision trees <ref> [ Watanabe and Rendell, 1991 ] </ref> .
References-found: 27

