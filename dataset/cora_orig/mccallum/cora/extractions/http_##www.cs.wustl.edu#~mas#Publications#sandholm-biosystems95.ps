URL: http://www.cs.wustl.edu/~mas/Publications/sandholm-biosystems95.ps
Refering-URL: http://www.cs.wustl.edu/~mas/multiagent_learning.html
Root-URL: 
Email: fsandholm, critesg@cs.umass.edu  
Phone: Tel. 413-545-0675, Fax. 413-545-1249  
Title: Multiagent Reinforcement Learning in the Iterated Prisoner's Dilemma  
Author: Tuomas W. Sandholm and Robert H. Crites 
Note: Supported by ARPA contract N00014-92-J-1698. The content does not necessarily reflect the position or the policy of the Government and no official endorsement should be inferred. Also supported by the Finnish Culture Foundation, Honkanen Foundation, Ella and George Ehrnrooth Foundation, Finnish Science Academy, Leo and Regina Wainstein Foundation, Finnish Information Technology Research Foundation, and Jenny and Antti Wihuri Foundation. Supported by Air Force Office of Scientific Research Grant F49620-93-1-0269.  
Address: Amherst, MA 01003  
Affiliation: University of Massachusetts at Amherst Computer Science Department  
Abstract: Reinforcement learning (RL) is based on the idea that the tendency to produce an action should be strengthened (reinforced) if it produces favorable results, and weakened if it produces unfavorable results. Q-learning is a recent RL algorithm that does not need a model of its environment and can be used on-line. Therefore it is well-suited for use in repeated games against an unknown opponent. Most RL research has been confined to single agent settings or to multiagent settings where the agents have totally positively correlated payoffs (team problems) or totally negatively correlated payoffs (zero-sum games). This paper is an empirical study of reinforcement learning in the iterated prisoner's dilemma (IPD), where the agents' payoffs are neither totally positively nor totally negatively correlated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ashlock, D., Smucker, M. D., Stanley, E. A. and Tesfatsion, L. </author> <year> 1995. </year> <title> Preferential Partner Selection in an Evolutionary Study of Prisoner's Dilemma. </title> <note> This Issue. </note>
Reference: [2] <author> Axelrod, R. </author> <year> 1984. </year> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, NY. </address>
Reference: [3] <author> Barto, A. G., Sutton, R., and Anderson, C. W. </author> <year> 1983. </year> <title> Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13 </volume> <pages> 834-846. </pages>
Reference: [4] <author> Barto, A. </author> <year> 1989. </year> <title> From chemotaxis to cooperativity: Abstracted exercises in neuronal learning strategies. </title> <editor> In Durbin, R., Miall C. and Mitchison, G., eds., </editor> <booktitle> The Computing Neuron, </booktitle> <pages> 73-98. </pages> <publisher> Addison-Wesley. </publisher>
Reference: [5] <author> Barto, A. G., Bradtke, S. J. and Singh, S. P. </author> <year> 1995. </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72 </volume> <pages> 81-138. </pages>
Reference: [6] <author> Barto, A. G. and Anandan, P. </author> <year> 1985. </year> <title> Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 15 </volume> <pages> 360-375. </pages>
Reference: [7] <author> Barto, A. G. </author> <year> 1985. </year> <title> Learning by statistical cooperation of self-interested neuron-like adaptive elements. </title> <journal> Human Neurobiology, </journal> <volume> 4 </volume> <pages> 229-256. </pages>
Reference: [8] <author> Bertsekas, D. P. and Tsitsiklis, J. N. </author> <year> 1989. </year> <title> Parallel and Distributed Computation: Numerical Methods. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address> <month> 35 </month>
Reference: [9] <author> Bradtke, S. J. </author> <year> 1993. </year> <title> Distributed Adaptive Optimal Control of Flexible Struc--tures. </title> <institution> Computer Science Department, University of Massachusetts at Amherst. </institution> <note> Unpublished draft. </note>
Reference: [10] <author> Crites, R. </author> <year> 1994. </year> <title> Multi-Agent Reinforcement Learning. </title> <type> PhD dissertation proposal. </type> <institution> Computer Science Department, University of Massachusetts at Amherst. </institution>
Reference: [11] <author> Elman, J. </author> <year> 1990. </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211. </pages>
Reference: [12] <author> Fudenberg, D. and Tirole, J. </author> <year> 1991. </year> <title> Game Theory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: [13] <author> Haykin, S. </author> <year> 1994. </year> <title> Neural Networks: A Comprehensive Foundation. </title> <publisher> Macmillan, </publisher> <address> New York. </address>
Reference: [14] <author> Hecht-Nielsen, R. </author> <year> 1991. </year> <title> Neurocomputing. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA. </address>
Reference: [15] <author> Kinney, M. and Tsatsoulis, C. </author> <year> 1993. </year> <title> Learning Communication Strategies in Distributed Agent Environments. </title> <note> Working paper WP-93-4. </note> <institution> Intelligent Design Laboratory, University of Kansas. </institution>
Reference: [16] <author> Kreps, D. </author> <year> 1990. </year> <title> A Course in Microeconomic Theory. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, NJ. </address>
Reference: [17] <author> Lin, L-J. </author> <year> 1993. </year> <title> Reinforcement Learning for Robots Using Neural Networks. </title> <type> Ph.D. dissertation, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference: [18] <author> Littman, M. </author> <year> 1993. </year> <title> Markov games as a framework for multi-agent reinforcement learning. </title> <booktitle> In Machine Learning, Proceedings of the Eleventh International Conference, </booktitle> <pages> pp. 157-163, </pages> <institution> Rutgers University, NJ. </institution>
Reference: [19] <author> Littman, M. and Boyan, J. </author> <year> 1993. </year> <title> A Distributed Reinforcement Learning Scheme for Network Routing. </title> <type> Technical Report CMU-CS-93-165, </type> <institution> Carnegie Mellon University. </institution> <month> 36 </month>
Reference: [20] <author> Luce, D. and Raiffa, H. </author> <year> 1957. </year> <title> Games and Decisions. </title> <publisher> Reprint: Dover Publications, </publisher> <address> New York, </address> <year> 1989. </year>
Reference: [21] <author> Markey, K. L. </author> <year> 1993. </year> <title> Efficient Learning of Multiple Degree-of-Freedom Control Problems with Quasi-independent Q-agents. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School. </booktitle> <publisher> Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address>
Reference: [22] <author> Narendra, K. S. and Thathachar, M. A. L. </author> <year> 1989. </year> <title> Learning Automata: An Introduction. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: [23] <author> Nowak, M. and Sigmund, K. </author> <year> 1993. </year> <title> A strategy of win-stay, lose-shift that outperforms tit-for-tat in the Prisoner's Dilemma game. </title> <journal> Nature, </journal> <volume> 364: </volume> <pages> 56-58. </pages>
Reference: [24] <author> Rumelhart, D. E., Hinton, G. E. and Williams, R. J. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E. and McClellan, J. L., Eds., </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> 1, </volume> <pages> 318-362, </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: [25] <author> Samuel, A. L. </author> <year> 1959. </year> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal on Research and Development, </journal> <pages> 210-229. </pages> <note> Reprinted in Feigenbaum, </note> <editor> E. A. and Feldman, J., Eds., </editor> <booktitle> Computers and Thought, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference: [26] <author> Sandholm, T. </author> <year> 1993. </year> <title> An Implementation of the Contract Net Protocol Based on Marginal Cost Calculations. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence (AAAI-93), </booktitle> <pages> pp. 256-262, </pages> <address> Washington D.C. </address>
Reference: [27] <author> Sandholm T. and Lesser, V. </author> <year> 1994. </year> <title> Utility-Based Termination of Anytime Algorithms, </title> <booktitle> In Proceedings of the European Conference on Artificial Intelligence (ECAI-94) Workshop on Decision Theory for Distributed Artificial Intelligence Applications, </booktitle> <pages> pp. 88-99, </pages> <address> Amsterdam, The Netherlands. </address> <note> Extended version: </note> <institution> University of Massachusetts at Amherst, </institution> <note> Computer Science Technical Report 94-54. 37 </note>
Reference: [28] <author> Sandholm T. and Lesser, V. </author> <year> 1995a. </year> <title> Coalition Formation among Bounded Ratio--nal Agents. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada. </address>
Reference: [29] <author> Sandholm T. and Lesser, V. </author> <year> 1995b. </year> <title> Issues in Automated Negotiation and Electronic Commerce: Extending the Contract Net Framework. </title> <booktitle> In Proceedings of the First International Conference on Multiagent Systems (ICMAS-95), </booktitle> <address> San Fran-cisco, CA. </address>
Reference: [30] <author> Sandholm, T. and Nagendraprasad, M. </author> <year> 1993. </year> <title> Learning Pursuit Strategies. Class project for CmpSci 689 Machine Learning. </title> <institution> Computer Science Department, University of Massachusetts at Amherst, </institution> <month> Spring </month> <year> 1993. </year>
Reference: [31] <author> Sen, S., Sekaran, M. and Hale, J. </author> <year> 1994. </year> <title> Learning to coordinate without sharing information. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pp. 426-431, </pages> <address> Seattle, Washington. </address>
Reference: [32] <author> Shoham, Y. and Tennenholtz, M. </author> <year> 1993. </year> <title> Co-Learning and the Evolution of Coordinated Multi-Agent Activity. </title>
Reference: [33] <author> Sugawara, T. and Lesser, V. </author> <year> 1993. </year> <title> On-Line Learning of Coordination Plans. </title> <institution> Computer Science Technical Report 93-27, University of Massachusetts, Amherst. </institution>
Reference: [34] <author> Sutton, R. S. </author> <year> 1988. </year> <title> Learning to Predict by the Methods of Temporal Differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44. </pages>
Reference: [35] <author> Tan, M. </author> <year> 1993. </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents. In Machine Learning, Proceedings of the Tenth International Conference, </booktitle> <pages> pp. 330-337, </pages> <institution> University of Massachusetts, Amherst. </institution>
Reference: [36] <author> Tesauro, G. J. </author> <year> 1992. </year> <title> Practical issues in temporal difference learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 257-277. 38 </pages>
Reference: [37] <author> Tsetlin, M. L. </author> <year> 1973. </year> <title> Automaton Theory and Modeling of Biological Systems. </title> <publisher> Academic Press, </publisher> <address> New York, NY. </address>
Reference: [38] <author> Watkins, C. </author> <year> 1989. </year> <title> Learning from delayed rewards. </title> <type> PhD Thesis, </type> <institution> University of Cambridge, </institution> <address> England. </address>
Reference: [39] <author> Wei, G. </author> <year> 1993. </year> <title> Learning to Coordinate Actions in Multi-Agent Systems. </title> <booktitle> In Proceedings of the Thirteenth International Joint Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pp. 311-316, </pages> <address> Chambery, France. </address>
Reference: [40] <author> Williams, R. J. and Zipser, D. </author> <year> 1989. </year> <title> A learning algorithm for continually running fully recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 270-280. 39 </pages>
References-found: 40

