URL: http://www.cs.washington.edu/education/courses/590MV/papers/eff-sched.ps
Refering-URL: http://www.cs.washington.edu/education/courses/590MV/
Root-URL: 
Title: Using Runtime Measured Workload Characteristics in Parallel Processor Scheduling  
Author: Thu D. Nguyen, Raj Vaswani, and John Zahorjan 
Address: Box 352350  Seattle, WA 98195-2350 USA  
Affiliation: Department of Computer Science and Engineering,  University of Washington  
Abstract: Technical Report UW-CSE-95-10-01 October 15, 1995 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. Anderson, B. Bershad, E. Lazowska, and H. Levy. </author> <title> Scheduler Activations: Effective Kernel Support for the 13 User-Level Management of Parallelism. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 10(1), </volume> <month> Feb. </month> <year> 1992. </year>
Reference-contexts: The second problem we face is one of efficiency. Given an initial allocation of P processors, MGS normally starts searching within the the interval <ref> [1; P ] </ref>. While MGS converges relatively quickly (O (log (P ))), the cost of individual probes can be quite large (if the job has poor speedup at the probed number of processors). We address this by exploiting the fact that speedups cannot be super-linear 7 . <p> This allows us to estimate S (P ), the job's speedup with P processors. Since speedups can never be super-linear, we know that the globally best number of processors must fall in [S (P ); P ]. Our search therefore starts in this interval instead of <ref> [1; P ] </ref>. For applications with good speedup, the interval [S (P ); P ] will typically be small, allowing self-tuning to be performed with little overhead. For applications with poor speedup but only modest slowdown, speedup will be similar at all points between [1; P ] and so, again, self-tuning <p> starts in this interval instead of <ref> [1; P ] </ref>. For applications with good speedup, the interval [S (P ); P ] will typically be small, allowing self-tuning to be performed with little overhead. For applications with poor speedup but only modest slowdown, speedup will be similar at all points between [1; P ] and so, again, self-tuning can be carried out with little overhead. Only in the case where an application initially achieves good speedup but then slows down significantly as its allocation grows does self-tuning incur significant overhead. <p> It is clearly possible to do much more dynamic scheduling, e.g., <ref> [14, 1, 5, 8] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change and because we expect that ST-EQUI would perform even better when jobs are more responsive in responding to allocation changes (of the three policies, ST-EQUI reallocates processors most
Reference: [2] <author> M. Berry, D. Chen, P. Koss, D. Kuck, S. Lo, Y. Pang, L. Pointer, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Scharzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, R. Goodrum, and J. Martin. </author> <title> The PERFECT Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <year> 1989. </year>
Reference-contexts: We have implemented prototypes of both schedulers on a 50 node KSR-2. We evaluate the effectiveness of these prototypes using a number of workloads comprised of benchmarks from the SPLASH [15] and PERFECT Club <ref> [2] </ref> benchmark suites, the best applications available to us for this work.
Reference: [3] <author> E. C. Cooper and R. P. Draves. </author> <title> C Threads. </title> <type> Technical Report CMU-CS-88-154, </type> <institution> Department of Computer Science, Carnegie-Mellon University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: Thus, measuring communication and system overhead involves little more than periodically reading these counters. Measuring idleness is slightly more involved; we instrument all synchronization code in our runtime systems (KSR PRESTO [7] and CThreads <ref> [3] </ref>) to keep elapsed idle time using the wall-clock hardware counter 4 .
Reference: [4] <author> L. Dowdy. </author> <title> On the Partitioning of Multiprocessor Systems. </title> <type> Technical report, </type> <institution> Vanderbilt University, </institution> <month> June </month> <year> 1988. </year>
Reference-contexts: First, to address the local irregularities in the efficiency curve, we use an artificial curve extrapolated from the most recently measured efficiency alone. In particular, having just measured the efficiency of an application on p processors, we use the function (1+fi)=(p+fi), which is taken from <ref> [4] </ref>, choosing fi so that the function interpolates the most recent efficiency measurement. Next, we determine allocations by following an equal efficiency rule; that is, we allocate processors in a way that causes all applications to have about equal efficiencies according to our extrapolated curves 9 .
Reference: [5] <author> D. L. Eager and J. Zahorjan. Chores: </author> <title> Enhanced Run-Time Support for Shared-Memory Parallel Computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> Feb. </month> <year> 1993. </year>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [14, 1, 5, 8] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change and because we expect that ST-EQUI would perform even better when jobs are more responsive in responding to allocation changes (of the three policies, ST-EQUI reallocates processors most
Reference: [6] <author> D. G. Feitelson and B. Nitzberg. </author> <title> Job Characteristics of a Production Parallel Scientific Workload on the NASA Ames iPSC/860. In Job Scheduling Strategies for Parallel Processing, </title> <booktitle> IPPS '95 Workshop, </booktitle> <pages> pages 337-360. </pages> <publisher> Springer, </publisher> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: We then examine two distinct scheduling scenarios: interactive systems, where minimizing response time is the goal, and batch systems, where maximizing the rate at which useful work is completed is the goal. Both kinds of computing already have significant roles on existing large scale parallel platforms <ref> [6] </ref>. For the interactive environment, we propose a scheduler that uses measured speedups to adjust the processor allocation of each running job, attempting to maximize job speedup. For batch environments, we propose a scheduler that uses measured efficiencies to allocate processors in such a way as to maximize system efficiency. <p> This decision is supported by the measurements in <ref> [6] </ref>, which indicate that a multiprogramming levels of 2, 3, and 4 are the three most common during daytime hours in their production environment. <p> However, we used a single multiprogramming level of 3 in all experiments (a reduction from the maximum of 4 considered for the interactive environment) to reflect the likely larger size of jobs submitted for batch execution. (This change is supported by the measurements in <ref> [6] </ref>.) Additionally, we present here results only for those workloads that include a Barnes job, the representative from the class of jobs having good speedup.
Reference: [7] <institution> Kendall Square Research Inc., </institution> <address> 170 Tracer Lane, Waltham, MA 02154. </address> <booktitle> KSR/Series Principles of Operation, </booktitle> <year> 1994. </year>
Reference-contexts: As an example, consider the speedup of MP3D, an application from the SPLASH [15] benchmark suite, when run on the KSR-2 multiprocessor <ref> [7] </ref>. The KSR-2 has an interconnection network that is a hierarchy of rings. The basic communication time between two rings is roughly four times that for communication within any one. <p> Thus, measuring communication and system overhead involves little more than periodically reading these counters. Measuring idleness is slightly more involved; we instrument all synchronization code in our runtime systems (KSR PRESTO <ref> [7] </ref> and CThreads [3]) to keep elapsed idle time using the wall-clock hardware counter 4 .
Reference: [8] <author> E. Markatos and T. LeBlanc. </author> <title> Using Processor Affinity in Loop Scheduling on Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 379-400, </pages> <month> Apr. </month> <year> 1994. </year>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [14, 1, 5, 8] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change and because we expect that ST-EQUI would perform even better when jobs are more responsive in responding to allocation changes (of the three policies, ST-EQUI reallocates processors most
Reference: [9] <author> C. McCann, R. Vaswani, and J. Zahorjan. </author> <title> A Dynamic Processor Allocation Strategy for Multiprogrammed, Shared Memory Multiprocessors. </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Under EQUI, each currently executing job is allocated an equal number of processors. Processor reallocations take place on job arrival and departure times. EQUI is representative of the space sharing approach to processor allocation that has been found to perform well for multiprogrammed shared-memory multiprocessors <ref> [16, 9] </ref>. ST-EQUI: At the highest level, the specific policy we propose to take advantage of runtime estimated speedup allocates an equal number of processors to each executing job, just as with EQUI.
Reference: [10] <author> G. P. McCormick. </author> <title> Nonlinear Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1983. </year>
Reference-contexts: Our current implementation of self-tuning employs a heuristic-based optimization technique that is an adaptation of the method of golden sections (MGS) <ref> [10] </ref> to find the best allocation. MGS is a simple optimization procedure that finds the maximum of a unimodal function over a finite interval by iteratively computing and comparing function values and narrowing the interval in which the maximum may occur 6 .
Reference: [11] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> Maximizing Speedup Through Self-Tuning of Processor Allocation. </title> <booktitle> In Proceedings of the 10th International Parallel Processing Symposium, to appear, </booktitle> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: ST-EQUI: At the highest level, the specific policy we propose to take advantage of runtime estimated speedup allocates an equal number of processors to each executing job, just as with EQUI. However, each time a reallocation takes place, each affected job engages in a self-tuning procedure <ref> [11] </ref> to estimate how many of its allocated processors should actually be used to maximize its current speedup (we briefly describe self-tuning below). <p> To better understand this new source of overhead, we next present the self-tuning procedure in somewhat more detail. 3.2 Self-Tuning In this section, we present a brief overview of self-tuning. Comprehensive details can be found in <ref> [11] </ref>, which examines the use of this technique in a static (essentially uniprogramming) environment. <p> While this heuristic does not guarantee that self-tuning will always find the global maximum, the experiments in <ref> [11] </ref> show that this procedure works remarkably well, nearly always converging to a near optimal value. The second problem we face is one of efficiency. Given an initial allocation of P processors, MGS normally starts searching within the the interval [1; P ]. <p> Rather, we assume that some other mechanism, such as the feedback scheduling employed in sequential systems, is used for this purpose. 8 We show in <ref> [11] </ref> that self-tuning is effective for a much larger number of applications than the 3 representative applications used in this study. 6 (Parsons and Sevcik [13] present the design and evaluation of two such schemes, for example.) We consider the workload mixes we schedule to be the subset of a larger
Reference: [12] <author> T. D. Nguyen, R. Vaswani, and J. Zahorjan. </author> <title> On Scheduling Implications of Application Characteristics. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <note> in preparation. </note>
Reference-contexts: Our experience with a wide variety of benchmark programs shows that we can accurately predict application efficiency by measuring only idleness, communication, and system overhead; parallelization overhead is typically small <ref> [12] </ref>. Thus, we require only estimates of the first three components to accurately assess efficiency 2 . On the KSR-2, we rely on a combination of hardware and software support to measure inefficiencies. <p> Empirical evidence shows that successive iterations tend to behave similarly, so that measurements taken for a particular iteration are good predictors of near future behavior <ref> [12] </ref>. Thus, for such applications, we equate a measurement interval to an application iteration, providing a basis by which to reasonably compare a job's performance as its processor allocation is varied. Note, however, that in general, our approach does not require applications to be iterative. <p> The case is less clear for EQUI and ST-EQUI. ST-EQUI can outperform EQUI when one or more jobs 5 In <ref> [12] </ref>, we found that five of the ten SPLASH applications and all seven of the Perfect Club applications we could compile were iterative. 4 determine that they are better off using fewer than their fair share of processors and release excess processors back to the system. <p> Our previous detailed study of these applications <ref> [12] </ref> suggests that these programs can be divided into three broad classes: * Good speedup. Most of the hand-coded applications fall into this class, which is characterized by fairly good speedup that mostly rises monotonically as the job receives processors. <p> Most of these applications exhibit significant slowdown beyond a certain number of allocated processors. * Erratic speedup. This class consists of applications whose speedup is irregular, e.g., it varies over time or exhibits multiple local maxima. Such behavior can be observed in both hand-coded and compiler-parallelized applications <ref> [12] </ref>. Because it is infeasible to run experiments with all possible combinations from our benchmark suites, we instead use our taxonomy to reduce the number of jobs that must be considered.
Reference: [13] <author> E. W. Parsons and K. C. Sevcik. </author> <title> Multiprocessor Scheduling for High-Variability Service Time Distributions. In Job Scheduling Strategies for Parallel Processing, </title> <booktitle> IPPS '95 Workshop, </booktitle> <pages> pages 127-145. </pages> <publisher> Springer, </publisher> <month> Apr. </month> <year> 1995. </year>
Reference-contexts: Rather, we assume that some other mechanism, such as the feedback scheduling employed in sequential systems, is used for this purpose. 8 We show in [11] that self-tuning is effective for a much larger number of applications than the 3 representative applications used in this study. 6 (Parsons and Sevcik <ref> [13] </ref> present the design and evaluation of two such schemes, for example.) We consider the workload mixes we schedule to be the subset of a larger job mix chosen for current execution by such a mechanism. 3.4 Implementation At the user-level, we implement process control to avoid loss of efficiency due
Reference: [14] <author> C. Polychronopoulos and D. Kuck. </author> <title> Guided Self-Scheduling: A Practical Scheduling Scheme for Parallel Super computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1425-1439, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: It is clearly possible to do much more dynamic scheduling, e.g., <ref> [14, 1, 5, 8] </ref>; we did not do so because of the very large incremental implementation cost relative to our more restrictive change and because we expect that ST-EQUI would perform even better when jobs are more responsive in responding to allocation changes (of the three policies, ST-EQUI reallocates processors most
Reference: [15] <author> J. P. Singh, W.-D. Weber, and A. Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <year> 1992. </year>
Reference-contexts: As an example, consider the speedup of MP3D, an application from the SPLASH <ref> [15] </ref> benchmark suite, when run on the KSR-2 multiprocessor [7]. The KSR-2 has an interconnection network that is a hierarchy of rings. The basic communication time between two rings is roughly four times that for communication within any one. <p> We have implemented prototypes of both schedulers on a 50 node KSR-2. We evaluate the effectiveness of these prototypes using a number of workloads comprised of benchmarks from the SPLASH <ref> [15] </ref> and PERFECT Club [2] benchmark suites, the best applications available to us for this work.
Reference: [16] <author> A. Tucker and A. Gupta. </author> <title> Process Control and Scheduling Issues for Multiprogrammed Shared-Memory Multipro cessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating System Principles, </booktitle> <pages> pages 159-166, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: improve response time in interactive environments through the use of runtime gathered job characteristics. 3.1 Policies To evaluate whether runtime measurements can be used beneficially by a scheduler, we compare the multiprogramming performance of the following three policies: EQUI: The basic scheduling policy on which we build is dynamic equipartition <ref> [16] </ref>. Under EQUI, each currently executing job is allocated an equal number of processors. Processor reallocations take place on job arrival and departure times. EQUI is representative of the space sharing approach to processor allocation that has been found to perform well for multiprogrammed shared-memory multiprocessors [16, 9]. <p> Under EQUI, each currently executing job is allocated an equal number of processors. Processor reallocations take place on job arrival and departure times. EQUI is representative of the space sharing approach to processor allocation that has been found to perform well for multiprogrammed shared-memory multiprocessors <ref> [16, 9] </ref>. ST-EQUI: At the highest level, the specific policy we propose to take advantage of runtime estimated speedup allocates an equal number of processors to each executing job, just as with EQUI. <p> two such schemes, for example.) We consider the workload mixes we schedule to be the subset of a larger job mix chosen for current execution by such a mechanism. 3.4 Implementation At the user-level, we implement process control to avoid loss of efficiency due to mismatches between threads and processors <ref> [16] </ref>.
References-found: 16

