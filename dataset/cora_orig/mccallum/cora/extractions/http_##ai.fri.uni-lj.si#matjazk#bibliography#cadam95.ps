URL: http://ai.fri.uni-lj.si/matjazk/bibliography/cadam95.ps
Refering-URL: http://ai.fri.uni-lj.si/matjazk/bibliography/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: E-mail: matjaz.kukar@fer.uni-lj.si  
Phone: Tel: +386-61-1768386  
Title: Machine learning in blood group determination of Danish Jersey cattle (causal probabilistic network). Dobljene mreze
Author: Matjaz Kukar Povzetek Lene Kolind Rasmussen v svojem porocilu iz leta navaja izjemno dobro razumljivost postopka dolocanja krvnih skupin s pomocjo vzrocnih verjetnostnih mrez generiranega znanja lahko primerjajo vzrocnimi verjetnostnimi mrezami. 
Address: Trzaska 25  
Affiliation: University of Ljubljana Faculty of electrical engineering and computer science,  
Abstract: In the following paper we approach the problem with different machine learning algorithms and show that they can be compared with causal probabilistic networks in terms of performance and comprehensibility. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Andersen, S., Olesen, K., & Jensen, F. </author> <year> (1989). </year> <title> Hugin a shell for building Bayesian universes for expert systems. </title> <booktitle> In Proc. 11 th International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, USA. </address>
Reference-contexts: It is effective and relatively simple, as it controls only 4 out of 52 factors. She used causal probabilistic networks to determine the genotypes and verify the parentage of Danish Jersey cattle. The model they implemented using the Hugin software package <ref> (Andersen et al., 1989) </ref> was shown to be consistent with the human knowledge and reasoning when coping with this problem. In this paper we show that the machine learning approach can be used to successfully solve the problem.
Reference: <author> Cestnik, B. </author> <year> (1990). </year> <title> Estimating probabilities: A crucial task in machine learning. </title> <booktitle> In Proc. European Conference on Artificial Intelligence 1990, </booktitle> <pages> (pp. 147-149)., </pages> <address> Stockholm, Sweden. </address>
Reference: <author> Cestnik, B. & Bratko, I. </author> <year> (1991). </year> <title> On estimating probabilities in tree pruning. </title>
Reference: <editor> In Kodratoff, Y. (Ed.), </editor> <booktitle> Proc. European Working Session on Learning, </booktitle> <pages> (pp. 138-150)., </pages> <address> Porto, Portugal. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Cestnik, B., Kononenko, I., & Bratko, I. </author> <year> (1987). </year> <title> ASSISTANT 86: A knowledge elicitation tool for sophisticated users. </title> <editor> In I. Bratko & N. Lavrac (Eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: the learning set). 2 The algorithms used in experiments In our experiments we used the following learning algorithms: Assistant-R, Backpropagation Neural Networks, Semi Naive Bayesian Classifier and Looka head Feature Construction (LFC). 2.1 Assistant-R Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees <ref> (Cestnik et al., 1987) </ref>. The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. (1966) and reimplemented by several authors (see (Quinlan, 1986) for an overview).
Reference: <author> Cussens, J. </author> <year> (1993). </year> <title> Bayes and pseudo-Bayes estimates of conditional probabilities and their reliability. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> (pp. 136-152)., </pages> <address> Vienna, Austria. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Hunt, E., Martin, J., & Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992a). </year> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> In Proc. AAAI'92, </booktitle> <address> San Jose, CA. </address>
Reference-contexts: The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection. RELIEFF (Kononenko, 1994) is an extension of RELIEF <ref> (Kira & Rendell, 1992a, 1992b) </ref>. The key idea of RELIEF is to estimate attributes according to how well their values distinguish among the instances that are near to each other.
Reference: <author> Kira, K. & Rendell, L. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title>
Reference: <editor> In Sleeman, D. & Edwards, P. (Eds.), </editor> <booktitle> Proc. Intern. Conf. on Machine Learning, </booktitle> <pages> (pp. 249-256)., </pages> <address> Aberdeen, UK. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. & Bratko, I. </author> <year> (1991). </year> <title> Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 67-80. </pages>
Reference-contexts: A new instance is classified into the class with maximal calculated probability. The m-estimate of probabilities was used (see Section 2.1) and the parameter m was set to 2 in all experiments. The semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> addresses the independence assumption, which is often not justified. Usually, the attributes are defined by a human (for example in medical data), and are therefore relatively independent, as humans tend to think linearly. However, it is not always so. <p> All the algorithms, described in Sectinon 2 were run on all split sets. We measured not only the classification accuracy (percentage of correctly classified examples, but also the information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately deals with the probabilistic answers of a classifier.
Reference: <author> Kononenko, I. </author> <year> (1991). </year> <title> Semi-naive Bayesian classifier. </title> <editor> In Kodratoff, Y. (Ed.), </editor> <booktitle> Proc. European Working Session on Learning-91, </booktitle> <pages> (pp. 206-219)., </pages> <editor> Porto, Potrugal. </editor> <publisher> Springer-Verlag. </publisher>
Reference-contexts: A new instance is classified into the class with maximal calculated probability. The m-estimate of probabilities was used (see Section 2.1) and the parameter m was set to 2 in all experiments. The semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> addresses the independence assumption, which is often not justified. Usually, the attributes are defined by a human (for example in medical data), and are therefore relatively independent, as humans tend to think linearly. However, it is not always so. <p> All the algorithms, described in Sectinon 2 were run on all split sets. We measured not only the classification accuracy (percentage of correctly classified examples, but also the information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately deals with the probabilistic answers of a classifier.
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: Analysis and extensions of RELIEF. </title> <editor> In De Raedt, L. & Bergadano, F. (Eds.), </editor> <booktitle> Proc. European Conf. on Machine Learning, </booktitle> <pages> (pp. 171-182)., </pages> <address> Catania, Italy. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: The main features of the original Assistant are binarization of attributes, decision tree pruning, incomplete data handling and the use of naive Bayesian classifier in null leaves. The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection. RELIEFF <ref> (Kononenko, 1994) </ref> is an extension of RELIEF (Kira & Rendell, 1992a, 1992b). The key idea of RELIEF is to estimate attributes according to how well their values distinguish among the instances that are near to each other.
Reference: <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: The basic algorithm goes back to CLS (Concept Learning System) developed by Hunt et al. (1966) and reimplemented by several authors (see <ref> (Quinlan, 1986) </ref> for an overview). The main features of the original Assistant are binarization of attributes, decision tree pruning, incomplete data handling and the use of naive Bayesian classifier in null leaves. The main difference between Assistant and its reimplementation Assistant-R is that RELIEFF is used for attribute selection.
Reference: <author> Ragavan, H. & Rendell, L. </author> <year> (1993). </year> <title> Lookahead feature construction for learning hard concepts. </title> <booktitle> In Proc. 10 th Intern. Conf. on Machine Learning, </booktitle> <pages> (pp. 252-259)., </pages> <address> Amherst, MA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Ragavan, H., Rendell, L., Shaw, M., & Tessmer, A. </author> <year> (1993). </year> <title> Learning complex real-world concepts through feature construction. </title> <type> Technical report UIUC-BI-AI-93-03, </type> <institution> The Beckman Institute, University of Illinois. </institution>
Reference: <author> Rasmussen, L. K. </author> <year> (1992). </year> <title> Blood group determination of danish jersey cattle in the f-blood group system. </title> <type> Dina Reseach Report no. 8, </type> <institution> Dina Foulum. </institution>
Reference-contexts: Its purpose is primarily parentage verification for pedigree registration. In Denmark, 52 different blood factors are used for blood typing of cattle, controlled by 10 different blood group systems. In her paper, Lene Kolind Rasmussen <ref> (Rasmussen, 1992) </ref> referred to the F blood group system. It is effective and relatively simple, as it controls only 4 out of 52 factors. She used causal probabilistic networks to determine the genotypes and verify the parentage of Danish Jersey cattle. <p> Each allele contains a subset of the blood group factors contained in phenotype. However, the blood group factors occur only in small number of combinations, the so-called phenogroups, which represent from none to several inherited blood group factors as a unit <ref> (Rasmussen, 1992) </ref>. An example is given in Figure 1. <p> It consists of 742 examples, described with eight attributes (see Table 1). Many examples (335) were missing some attribute values, mostly those of the stated parents. The original task, as described in <ref> (Rasmussen, 1992) </ref> was: 1. to determine each animal's genotype (one of the following: F1/F1, F1/V1, F1/V2, V1/V1, V1/V2, V2/V2), see Table 2 2. to verify the assumed parentage by comparing the determined genotype with phenogroups of stated parents Attribute Possible values 1 Lysis 40 0 - 7 2 Lysis 41 0 <p> Phenogroup 2 of stated sire F1, V1, V2 Table 1: Attributes in original dataset Genotyope Frequency Relative frequency F1/F1 176 23.7% F1/V2 290 39.1% V1/V2 27 9.0% Table 2: Distibution of genotypes 3.3 Causal probabilistic network The structure of the causal probabilistic network to determine genotypes and verify assumed parentage <ref> (Rasmussen, 1992) </ref> is shown on Figure 2. The biological background is inheritance of the blood groups (one phenogroup is inherited from each parent), and assumptions concerning the risk of misstated parents. <p> If we add to this number the number of animals with one or both parents unknown (335), the percentage of animals, for which we cannot question the parentage reaches 90%, which is close to the estimate given in <ref> (Rasmussen, 1992) </ref>.
Reference: <author> Robnik, M. </author> <year> (1993). </year> <title> Constructive induction with decision trees. B.Sc. </title> <type> Thesis. </type> <institution> Ljubljana, Slovenia: University of Ljubljana, Faculty of electrical eng. & computer science. </institution> <note> In Slovene. </note>
Reference: <author> Rumelhart, D. & McClelland, J. L. </author> <year> (1986). </year> <title> Parallel Distributed Processing, </title> <booktitle> volume 1: Foundations. </booktitle> <address> Cambridge: </address> <publisher> MIT Press. </publisher>
Reference-contexts: To avoid this problem, the algorithm should detect the dependencies among attributes and join dependent attributes together. Besides, instead of joining whole attributes, only single values of different attributes can be joint, providing a more flexible solution. 2.3 Backpropagation with weight elimination The multilayered feedforward neural network <ref> (Rumelhart & McClelland, 1986) </ref> is a hierarchical network consisting of fully interconetced layers of processing units (often called neurons). The output of each unit is fed to the input of every unit in the next layer. Each network consists of at least two layers the input and the output one.
Reference: <author> Smyth, P., Goodman, R. M., & Higgins, C. </author> <year> (1990). </year> <title> A hybrid rule-based bayesian classifier. </title> <booktitle> In Proc.European Conf. on Artificial Intelligence, </booktitle> <pages> (pp. 610-615)., </pages> <address> Stockholm, Sweden. </address>
Reference: <author> Weigand, S., Huberman, A., & Rumelhart, D. E. </author> <year> (1990). </year> <title> Predicting the future: a connectionist approach. </title> <journal> International Journal of Neural Systems, </journal> <volume> 1(3). </volume>
Reference-contexts: The trained network becomes too specialised for describing learning instances, and is unable to successfully classify unseen instances. This phenomenon is usually a consequence of using too large network with too many hidden units. The weight elimination <ref> (Weigand et al., 1990) </ref> at least partially overcomes this problem.
References-found: 21

