URL: ftp://ftp.huji.ac.il/users/yish/yish.ijcai95.ps.gz
Refering-URL: http://www.cs.huji.ac.il/~yish/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: email: yish@cs.huji.ac.il, jeff@cs.huji.ac.il  
Title: Computationally Bounded Agents in Repeated Interactions addition, a variant of the repeated Prisoner's Dilemma game
Author: Yishay Mor Jeffrey S. Rosenschein ph: ---- 
Note: In  
Address: Givat Ram, Jerusalem, Israel  
Affiliation: Computer Science Department Hebrew University  
Abstract: This paper examines the incorporation of computational complexity and game theoretic models, as a framework for analyzing and designing multi-agent environments. The example focused on is the Prisoner's Dilemma, repeated for a finite length of time. We show that a minimal bound on the players' computational ability is sufficient to enable cooperative behavior. This paper has not already been accepted by and is not currently under review for a journal or another conference. Nor will it be submitted for such during IJCAI's review period. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> learning regular sets from queries and conterexamples. </title> <journal> Information and Computation, </journal> <volume> 75 </volume> <pages> 87-106, </pages> <year> 1987. </year>
Reference-contexts: If the opponent's strategy is not known, we might want to learn it during the course of the repeated game. Again, this problem in its general form was shown to be NP-hard for DFSs by Angluin <ref> [1] </ref> and others [12, 5]. The same researchers propose polynomial algorithms for learning automata under certain restrictions, but it is not clear whether those restrictions apply to the repeated game setting.
Reference: [2] <author> R. Axelrod. </author> <title> The Evolution of Cooperation. </title> <publisher> Basic Books, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: Perhaps the most well-known popular work on these topics is that of Axelrod <ref> [3, 2] </ref>, which reports on his famous computer tournament (programs, using different strategies, playing the repeated Prisoner's Dilemma) and analyzes social systems accordingly. The relationship between Artificial Intelligence (AI) and models of bounded rationality is long, and fundamental to the very nature of the field of AI.
Reference: [3] <author> R. Axelrod and W. Hamilton. </author> <title> The evolution of cooperation. </title> <journal> Science, </journal> <volume> 211(4489) </volume> <pages> 1390-1396, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: Perhaps the most well-known popular work on these topics is that of Axelrod <ref> [3, 2] </ref>, which reports on his famous computer tournament (programs, using different strategies, playing the repeated Prisoner's Dilemma) and analyzes social systems accordingly. The relationship between Artificial Intelligence (AI) and models of bounded rationality is long, and fundamental to the very nature of the field of AI.
Reference: [4] <author> Elchanan Ben-Porath. </author> <title> The complexity of computing a best response automaton in repeated games with mixed strategies. </title> <journal> Games and Economic Behavior, </journal> <volume> 2 </volume> <pages> 1-12, </pages> <year> 1990. </year>
Reference-contexts: In this section we discuss a few of these guidelines. Even when it is assumed that the opponent's strategy is realizable by a DFS automaton, computing a best response to it might be NP-complete (see <ref> [11, 4] </ref>). In cases where efficient algorithms exist, they are at best polynomial. Thus, if the opponent chooses a large enough automaton as his strategy, it might be impossible to compute the best response to it in one round of play, even if the strategy is known.
Reference: [5] <author> Y. Freund, M. J. Kearns, D. Ron, R. Rubinfeld, R. E. Schapire, and L. Sellie. </author> <title> Efficient learning of typical finite automata from random walks. </title> <booktitle> 13 In Proceedings of the 25th Annual ACM Symposium on Theory of Com--puting, </booktitle> <pages> pages 315-324, </pages> <address> San-Diego, CA, 1993. </address> <publisher> ACM press. </publisher>
Reference-contexts: If the opponent's strategy is not known, we might want to learn it during the course of the repeated game. Again, this problem in its general form was shown to be NP-hard for DFSs by Angluin [1] and others <ref> [12, 5] </ref>. The same researchers propose polynomial algorithms for learning automata under certain restrictions, but it is not clear whether those restrictions apply to the repeated game setting. Furthermore, in the our model, the game payoffs encompass the computation time as well as the payoffs for the different strategy combinations.
Reference: [6] <author> Ehud Kalai. </author> <title> Bounded rationality and strategic complexity in repeated games. </title> <editor> In T. Ichiishi, A. Neyman, and Y. Tauman, editors, </editor> <booktitle> Game Theory and Aplications, </booktitle> <pages> pages 131-157. </pages> <publisher> Academic Press, </publisher> <address> San Diego, </address> <year> 1990. </year>
Reference-contexts: We review some results obtained by this model in the context of the PD game, and show some possible applications in multi agent environments. 1.1 Related Work A thorough and comprehensive survey of the basic literature on bounded rationality and repeated PD appears in <ref> [6] </ref>. Perhaps the most well-known popular work on these topics is that of Axelrod [3, 2], which reports on his famous computer tournament (programs, using different strategies, playing the repeated Prisoner's Dilemma) and analyzes social systems accordingly. <p> Previous work <ref> [6] </ref> focused on finite or infinite iterated PD (IPD). The basic idea is that two players play PD for N rounds. At each round, once both have made their moves (effectively simultaneously), they get the payoffs defined by the PD game matrix.
Reference: [7] <author> N. Megiddo and A. Wigderson. </author> <title> On play by means of computing machines. </title> <booktitle> In Conference on Theoretical Aspects of Reasoning about Knowledge, </booktitle> <pages> pages 259-274, </pages> <address> Monterey California, </address> <month> March </month> <year> 1986. </year>
Reference-contexts: The mechanism by which two machines can be used to play a repeated game is similar to that described in <ref> [7] </ref>, with the exception that if at the end of a round of play, if one of the machines has not reached its action state (P AU SE in [7]), the action W is assigned to it by the game referee. <p> The mechanism by which two machines can be used to play a repeated game is similar to that described in <ref> [7] </ref>, with the exception that if at the end of a round of play, if one of the machines has not reached its action state (P AU SE in [7]), the action W is assigned to it by the game referee.
Reference: [8] <author> Yishay Mor. </author> <title> Computational approaches to rational choice. </title> <type> Master's thesis, </type> <institution> Hebrew University, </institution> <year> 1995. </year> <note> In preparation. </note>
Reference-contexts: If P 0 H then the games are weakly equivalent. 3. If P = 0 = H then the games are equilibrium equivalent. The proofs are rather technical (though straightforward), and can be found in <ref> [8] </ref>. For the rest of this paper, for simplicity's sake, we assume P = 0 = H. In [9] we introduced the notion of Complexity Bounded (CB) players. We showed that the set of CB strategies is a superset of the strategies realiz able by Turing machines. <p> A more detailed discussion, along with motivations for this perturbation, and a more formal description of the game, appear in [9] and in <ref> [8] </ref>. Here we will only present the model and the main results as a theoretical basis for the next section. The proofs of the theorems can also be found in [9]. Loosely speaking, the players are are offered a third choice (other than cooperate or defect): Opt Out.
Reference: [9] <author> Yishay Mor and Jeffrey S. Rosenschein. </author> <title> Time and the prisoner's dilemma, </title> <booktitle> 1994. Submited to the 1995 International Conference on Mul-tiagent Systems. </booktitle>
Reference-contexts: not attempting to provide an overview of this work, we will only men tion Simon [15, 16] as one of the earliest to investigate this topic, and Russell and Wefald [14] as some of the more recent to do so. 2 The models presented here were first laid out in <ref> [9] </ref>. A more extensive survey of the related literature can be found in that paper. 1.2 Outline of the Paper In Section 2 the basic concepts we use are defined, including the game setting, the notion of Nash Equilibrium, and the model of Complexity Bounded Players. <p> If P = 0 = H then the games are equilibrium equivalent. The proofs are rather technical (though straightforward), and can be found in [8]. For the rest of this paper, for simplicity's sake, we assume P = 0 = H. In <ref> [9] </ref> we introduced the notion of Complexity Bounded (CB) players. We showed that the set of CB strategies is a superset of the strategies realiz able by Turing machines. <p> Definition 5 A Cooperative Strategy is one that participates in a cooperative equilibrium. 5 Theorem 2 A cooperative player will Wait or Defect only if his opponent Defected or Waited at an earlier stage of the game. Proof. See <ref> [9] </ref>. Theorem 3 If R &gt; 0 and T &lt; 2 log 2 (N) k , where K is the maximal number of states in the CBTM agents, then there exists a cooperative equilibrium of the FTPD game. 2 Proof. Consider the two-state DFS strategy GRIM: 3 1. <p> The time needed to decrement the counter is not fixed, so the agent has to use his states to count rounds while he is counting, and needs at least 2t states to do so. 4 The agent will have to "read" through the whole counter at least 2 In <ref> [9] </ref>, we show a similar result for generalized CB players, with T = 1. 3 GRIM is a popular strategy in the literature of repeated Prisoner's Dilemma. 4 To be precise, if the agent needs s states to maintain his strategy without timing, he will need s fi t states with <p> A more detailed discussion, along with motivations for this perturbation, and a more formal description of the game, appear in <ref> [9] </ref> and in [8]. Here we will only present the model and the main results as a theoretical basis for the next section. The proofs of the theorems can also be found in [9]. <p> discussion, along with motivations for this perturbation, and a more formal description of the game, appear in <ref> [9] </ref> and in [8]. Here we will only present the model and the main results as a theoretical basis for the next section. The proofs of the theorems can also be found in [9]. Loosely speaking, the players are are offered a third choice (other than cooperate or defect): Opt Out. At any stage of the game, if one of the players chooses O (opt out), the pair is separated. <p> Theorem 6 If there is a probability of at least q &gt; 0 of being matched with a cooperative player at any stage of the game, then a player in the OPD game can ensure himself a security level (expected minimal payoff) of N fiRconst. In <ref> [9] </ref> we show that the optimal strategy cannot get a payoff higher by more than some constant amount. 4 Applying the Theory 4.1 Reliable Protocol Design The above discussion, while focusing on theoretical models of interacting agents, can have direct relevance to building real-world multi-agent systems.
Reference: [10] <author> A. Neyman. </author> <title> Bounded complexity justifies cooperation in finitely repeated prisoner's dilemma. </title> <journal> Economic Letters, </journal> <pages> pages 227-229, </pages> <year> 1985. </year>
Reference: [11] <author> Christos H. Papadimitriou. </author> <title> On players with a bounded ,number of states. </title> <journal> Games and Economic Behavior, </journal> <volume> 4 </volume> <pages> 122-131, </pages> <year> 1992. </year>
Reference-contexts: In this section we discuss a few of these guidelines. Even when it is assumed that the opponent's strategy is realizable by a DFS automaton, computing a best response to it might be NP-complete (see <ref> [11, 4] </ref>). In cases where efficient algorithms exist, they are at best polynomial. Thus, if the opponent chooses a large enough automaton as his strategy, it might be impossible to compute the best response to it in one round of play, even if the strategy is known.
Reference: [12] <author> R. Rivest and R. Schapire. </author> <title> Inference of finite automata using homing sequences. </title> <journal> Information and Computation, </journal> <volume> 103 </volume> <pages> 299-347, </pages> <year> 1993. </year>
Reference-contexts: If the opponent's strategy is not known, we might want to learn it during the course of the repeated game. Again, this problem in its general form was shown to be NP-hard for DFSs by Angluin [1] and others <ref> [12, 5] </ref>. The same researchers propose polynomial algorithms for learning automata under certain restrictions, but it is not clear whether those restrictions apply to the repeated game setting. Furthermore, in the our model, the game payoffs encompass the computation time as well as the payoffs for the different strategy combinations.
Reference: [13] <author> Jeffrey S. Rosenschein and Gilad Zlotkin. </author> <title> Rules of Encounter. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1994. </year>
Reference-contexts: Appropriate protocol design for agents (e.g., software agents) can have direct impact on the efficiency of reaching agreements, and on the quality of those agreements <ref> [13] </ref>. In a similar way, the analyses we are carrying out here can be used to design interaction environments for self-motivated agents. One of the interpretations for equilibria in games is that of "self-enforcing contracts".
Reference: [14] <author> Stuart Russell and Eric Wefald. </author> <title> Do the Right Thing. </title> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: The need to compromise between the attainable and the desirable leads to the notion of Bounded Rationality <ref> [14] </ref>. An agent with bounded rationality would act rationally, if he could. But then, how does he act? The question of bounded rationality lies at the intersection between Artificial intelligence, Cognitive Psychology, Political Science, Biology, and Game Theory. <p> While not attempting to provide an overview of this work, we will only men tion Simon [15, 16] as one of the earliest to investigate this topic, and Russell and Wefald <ref> [14] </ref> as some of the more recent to do so. 2 The models presented here were first laid out in [9].
Reference: [15] <author> Herbert A. Simon. </author> <booktitle> The Sciences of the Artificial. </booktitle> <publisher> The MIT Press, </publisher> <address> Cambridge Massachusetts, </address> <year> 1969. </year>
Reference-contexts: The relationship between Artificial Intelligence (AI) and models of bounded rationality is long, and fundamental to the very nature of the field of AI. While not attempting to provide an overview of this work, we will only men tion Simon <ref> [15, 16] </ref> as one of the earliest to investigate this topic, and Russell and Wefald [14] as some of the more recent to do so. 2 The models presented here were first laid out in [9].
Reference: [16] <author> Herbert A. Simon. </author> <title> Models of Bounded Rationality. </title> <publisher> The MIT Press, </publisher> <address> Cambridge Massachusetts, </address> <year> 1983. </year> <month> 14 </month>
Reference-contexts: The relationship between Artificial Intelligence (AI) and models of bounded rationality is long, and fundamental to the very nature of the field of AI. While not attempting to provide an overview of this work, we will only men tion Simon <ref> [15, 16] </ref> as one of the earliest to investigate this topic, and Russell and Wefald [14] as some of the more recent to do so. 2 The models presented here were first laid out in [9].
References-found: 16

