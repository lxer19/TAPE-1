URL: ftp://ftp.cs.toronto.edu/pub/mfalou/ICPADS/icpads.ps.Z
Refering-URL: http://www.cs.toronto.edu/~mfalou/papers.html
Root-URL: http://www.cs.toronto.edu
Email: mfalou@cs.toronto.edu mart@cs.ucr.edu  
Title: What Features Really Make Distributed Minimum Spanning Tree Algorithms Efficient?  
Author: Michalis Faloutsos Mart Molle 
Affiliation: University of Toronto University of California at Riverside Department of Computer Science Department of Computer Science  
Abstract: Since Gallager, Humblet and Spira first introduced the distributed Minimum Spanning Tree problem, many authors have suggested ways to enhance their basic algorithm to improve its performance. Most of these improved algorithms have even been shown to be very efficient in terms of reducing their worst-case communications and/or time complexity measures. In this paper, however, we take a different approach, basing our comparisons on measurements of the actual running times, numbers of messages sent, etc., when various algorithms are run on large numbers of test networks. We also propose the Distributed Information idea, that yields several new techniques for performance improvement. Simulation results show that contrary to the theoretical analysis, some of the techniques in the literature degrade the performance. Moreover, a simple technique that we propose seems to achieve the best time and message complexity among several algorithms tested. 
Abstract-found: 1
Intro-found: 1
Reference: [Afr90] <author> F. Afrati. </author> <title> Graph Algorithms. </title> <institution> National Technical University of Athens Press, </institution> <year> 1990. </year>
Reference-contexts: We found the package very useful though some extensions had to be made. The correctness of the solutions was tested with a non-distributed MST algorithm known as Prim's algorithm [Pri57], and described in <ref> [Afr90] </ref>. Given the wide variety of different algorithms that can be created, as stated already, we decided to test a representative group rather than exhaustively trying all combinations. We chose to test the following algorithms, each of which is identified by the nickname we gave it: 1.
Reference: [Awe87] <author> B. Awerbuch. </author> <title> Optimal distributed algorithms for minimum weight spanning tree, counting, leader election and related problems. </title> <booktitle> Proc. 19th Symp. on Theory of Computing, </booktitle> <pages> pages 230-240, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: Thus, we cannot simply send all the information to one node and solve a centralized problem, because this would involve a Leader Election problem which turns out to be comparably difficult <ref> [Awe87] </ref>. Fortunately, finding distributed MST algorithms is straightforward because the centralized MST problem can be easily solved by greedy algorithms. A distributed algorithm was proposed by Gallager et al [GHS83] requiring O (E + N log (N )) messages and O (N log (N )) time units. <p> Following [GHS83], various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity [CT85], [Gaf85], <ref> [Awe87] </ref>, [FM95] and [Fal95]. As it will become apparent later, there is a trade-off between termination time and messages. The first contribution of this paper is the introduction of further refinements to the basic algorithm, which are based on a novel approach called Distributed Information. <p> Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper [GHS83], Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including [CT85], [Gaf85], <ref> [Awe87] </ref>, [Fal95], and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST. <p> Notice that the communication complexity remains O (E + N log N ). 3.2 Estimating the Distance The basic idea behind the RS technique | namely triggering level increases whenever the current fragment size exceeds the next power of 2 | was carried further in <ref> [Awe87] </ref>, where two additional techniques were introduced 2 . These techniques were further refined in [FM95] after some problems were discovered in the original paper. The Root Distance (or RD) technique (originally called Root Update in [Awe87]) dominates the RS technique described above; it ensures that the root 1 Notice that <p> current fragment size exceeds the next power of 2 | was carried further in <ref> [Awe87] </ref>, where two additional techniques were introduced 2 . These techniques were further refined in [FM95] after some problems were discovered in the original paper. The Root Distance (or RD) technique (originally called Root Update in [Awe87]) dominates the RS technique described above; it ensures that the root 1 Notice that our two word names for the techniques follow a pattern where the first word denotes who is carrying out the procedure and the second one what it is estimating. <p> When an initiate message visits a node whose distance from the root is greater than 2 L+1 edges, the message "returns" to the root. The root then increases the level of the fragment and performs a Finding procedure at the new level (see <ref> [Awe87] </ref> [FM95] for more details). The other technique, which we call Leader Distance (or LD), works similarly, but is invoked by the leader of a submitted fragment. <p> For a detailed explanation see [FM95] [Fal95]. 3.3 Multiple Phase Operation The basic algorithm, together with any of the enhancements except LD, is message efficient but not time efficient, whereas LD makes the algorithm time efficient but sacrifices the message complexity. This difficulty was overcome by Awerbuch <ref> [Awe87] </ref>, who suggested an innovative three-phase algorithmic structure that switches from one sub-efficient version of the algorithm to another part way the execution, and thereby achieves efficiency in terms of both message and time complexity. To elaborate, in the first Counting phase, the algorithm simply determines N . <p> The third large fragment phase involves small numbers of large fragments, where limiting the execution time is most critical. Each fragment switches from the small fragment MST phase to the large MST phase as soon as its size reaches N log (N) . For more details see <ref> [Awe87] </ref> [FM95] [Fal95]. 4 Using Distributed Information In this section, we introduce several techniques that use stored information at each node to improve the performance of the algorithms. <p> Having an optiMSTic attitude, we hope to come across such configurations. The Communication Efficient techniques re quiring O (E + N log (N )) messages are: 1. Root Size ([CT85] [Gaf85]) 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance ([FM95] <ref> [Awe87] </ref>) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. <p> The former alone can not guarantee optimal time and they may also create cycles (or more details see [Fal95] [FM95]). Furthermore, we can create a number of efficient algorithms by combining the above techniques, and the multiple phase structure introduced by <ref> [Awe87] </ref>. <p> Awe-FM: third phase of <ref> [Awe87] </ref> with the corrections of [FM95], i.e., the basic plus the Root Distance and the Leader Distance techniques tech nique. 7. multiphase optiMST: a two phase algorithm with I) the basic II) optiMST, as described in the previous section. <p> Another major observation is that, in some cases, increasing the level faster can increase the termination time. Theoretically, most of the previous techniques try to increase the level as fast as they can, since the complexity proofs rely on the speed with which the level increases (see <ref> [Awe87] </ref> [FM95]). In practice, however, the repetition of the Finding procedure at a new level can cause big delays. In support of that we trace the level of the nodes during the execution.
Reference: [CT85] <author> F. Chin and H.F. Ting. </author> <title> An almost linear time and O(V logV +E) messages distributed algorithm for minimum weight spanning trees. </title> <booktitle> Proceedings of Foundations Of Computer Science (FOCS) Conference Portland, </booktitle> <address> Oregon, </address> <month> October </month> <year> 1985. </year>
Reference-contexts: Following [GHS83], various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity <ref> [CT85] </ref>, [Gaf85], [Awe87], [FM95] and [Fal95]. As it will become apparent later, there is a trade-off between termination time and messages. The first contribution of this paper is the introduction of further refinements to the basic algorithm, which are based on a novel approach called Distributed Information. <p> Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper [GHS83], Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including <ref> [CT85] </ref>, [Gaf85], [Awe87], [Fal95], and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> In the subsequent sections, we will describe the new features introduced in the later algorithms, without repeating features that do not change. 3.1 Size Estimation The technique that we will call Root Size 1 (or RS) was introduced in <ref> [CT85] </ref> [Gaf85]. The authors recognized that, although any fragment of level L in the basic algorithm obviously must have at least 2 L nodes, it may be much larger than that if it has accepted a lot of submissions. <p> The Communication Efficient techniques re quiring O (E + N log (N )) messages are: 1. Root Size (<ref> [CT85] </ref> [Gaf85]) 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance ([FM95] [Awe87]) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. Leader Size (optiMST) The time efficient algorithms are: the algorithm of the third phase in [FM95], and optiMST. <p> Avg optiMST Avg with 70 nodes. 0 10000 20000 30000 GHS var. Fast Report var. optiMST var. GHS time nodes: sparse graphs. O (N log (N )) time, but the implementation was not straightforward (see <ref> [CT85] </ref>). In an effort to approach a worst case scenario, we created a set of graphs borrowing ideas from our bad case example. These graphs consist mainly of a long line of nodes with increasing weights of edges between them.
Reference: [Fal95] <author> Michalis Faloutsos. </author> <title> Corrections, improvements, simulations and optiMSTic algorithms for the distributed minimum spanning tree problem. </title> <type> Master's thesis, </type> <institution> University of Toronto, Computer Science, </institution> <year> 1995. </year> <note> Technical Report CSRI-316. </note>
Reference-contexts: Following [GHS83], various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity [CT85], [Gaf85], [Awe87], [FM95] and <ref> [Fal95] </ref>. As it will become apparent later, there is a trade-off between termination time and messages. The first contribution of this paper is the introduction of further refinements to the basic algorithm, which are based on a novel approach called Distributed Information. <p> Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper [GHS83], Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including [CT85], [Gaf85], [Awe87], <ref> [Fal95] </ref>, and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST. <p> Unfortunately, the LD technique can increase the communication complexity to O (E + N 2 ). The problem appears when a large number of distinct fragments submit and activate LD procedures. For a detailed explanation see [FM95] <ref> [Fal95] </ref>. 3.3 Multiple Phase Operation The basic algorithm, together with any of the enhancements except LD, is message efficient but not time efficient, whereas LD makes the algorithm time efficient but sacrifices the message complexity. <p> The third large fragment phase involves small numbers of large fragments, where limiting the execution time is most critical. Each fragment switches from the small fragment MST phase to the large MST phase as soon as its size reaches N log (N) . For more details see [Awe87] [FM95] <ref> [Fal95] </ref>. 4 Using Distributed Information In this section, we introduce several techniques that use stored information at each node to improve the performance of the algorithms. <p> The second type of information is some measure of the distance from the root, which is used to accelerate the level increases of submitted fragments. We call this approach Distributed Information, and the interested reader can find a more detailed analysis of the new techniques in <ref> [Fal95] </ref>. 4.1 Storing Edge-MOE Information As we saw, the main question for every fragment is to find its MOE. Thus, it makes sense to keep track of the MOE of previous Reporting procedures, and avoid repeating parts of the procedure when the result can be predicted. <p> It is easy to see that the LS technique is equivalent to the earlier Leader Distance technique that we saw before, and leads to efficient time (O (N )) and non efficient communication (O (E + N 2 )) (see <ref> [Fal95] </ref> for more details). <p> Attention is needed, since the above time efficient techniques need the support of the communication efficient ones. The former alone can not guarantee optimal time and they may also create cycles (or more details see <ref> [Fal95] </ref> [FM95]). Furthermore, we can create a number of efficient algorithms by combining the above techniques, and the multiple phase structure introduced by [Awe87].
Reference: [FM95] <author> Michalis Faloutsos and Mart Molle. </author> <title> Optimal distributed algorithm for minimum spanning trees revisited. </title> <booktitle> Proceedings of Principles Of Distributed Computing (PODC), </booktitle> <year> 1995. </year>
Reference-contexts: Following [GHS83], various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity [CT85], [Gaf85], [Awe87], <ref> [FM95] </ref> and [Fal95]. As it will become apparent later, there is a trade-off between termination time and messages. The first contribution of this paper is the introduction of further refinements to the basic algorithm, which are based on a novel approach called Distributed Information. <p> Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper [GHS83], Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including [CT85], [Gaf85], [Awe87], [Fal95], and <ref> [FM95] </ref>. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. Thereafter, adjacent fragments join to form larger fragments by labeling their intermediate edge as a Branch of the MST. <p> These techniques were further refined in <ref> [FM95] </ref> after some problems were discovered in the original paper. <p> When an initiate message visits a node whose distance from the root is greater than 2 L+1 edges, the message "returns" to the root. The root then increases the level of the fragment and performs a Finding procedure at the new level (see [Awe87] <ref> [FM95] </ref> for more details). The other technique, which we call Leader Distance (or LD), works similarly, but is invoked by the leader of a submitted fragment. <p> The symmetry of the concept of the two procedures is apparent; they explore the same path from different ends. However, several issues have to be taken care of in order to guarantee the efficiency and the correctness of the resulting algorithm (see <ref> [FM95] </ref> for details). It is easy to see that the LD technique improves the time performance in our bad case example. Because of the LD procedure, fragment F 1 will reach L "faster", and thus fragment F will have an answer to its test message sooner. <p> Unfortunately, the LD technique can increase the communication complexity to O (E + N 2 ). The problem appears when a large number of distinct fragments submit and activate LD procedures. For a detailed explanation see <ref> [FM95] </ref> [Fal95]. 3.3 Multiple Phase Operation The basic algorithm, together with any of the enhancements except LD, is message efficient but not time efficient, whereas LD makes the algorithm time efficient but sacrifices the message complexity. <p> The third large fragment phase involves small numbers of large fragments, where limiting the execution time is most critical. Each fragment switches from the small fragment MST phase to the large MST phase as soon as its size reaches N log (N) . For more details see [Awe87] <ref> [FM95] </ref> [Fal95]. 4 Using Distributed Information In this section, we introduce several techniques that use stored information at each node to improve the performance of the algorithms. <p> Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance (<ref> [FM95] </ref> [Awe87]) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. Leader Size (optiMST) The time efficient algorithms are: the algorithm of the third phase in [FM95], and optiMST. Clearly, this plethora of choices raises the issue of which techniques should be used in practice. <p> (<ref> [FM95] </ref> [Awe87]) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. Leader Size (optiMST) The time efficient algorithms are: the algorithm of the third phase in [FM95], and optiMST. Clearly, this plethora of choices raises the issue of which techniques should be used in practice. Attention is needed, since the above time efficient techniques need the support of the communication efficient ones. <p> Attention is needed, since the above time efficient techniques need the support of the communication efficient ones. The former alone can not guarantee optimal time and they may also create cycles (or more details see [Fal95] <ref> [FM95] </ref>). Furthermore, we can create a number of efficient algorithms by combining the above techniques, and the multiple phase structure introduced by [Awe87]. <p> Awe-FM: third phase of [Awe87] with the corrections of <ref> [FM95] </ref>, i.e., the basic plus the Root Distance and the Leader Distance techniques tech nique. 7. multiphase optiMST: a two phase algorithm with I) the basic II) optiMST, as described in the previous section. <p> Another major observation is that, in some cases, increasing the level faster can increase the termination time. Theoretically, most of the previous techniques try to increase the level as fast as they can, since the complexity proofs rely on the speed with which the level increases (see [Awe87] <ref> [FM95] </ref>). In practice, however, the repetition of the Finding procedure at a new level can cause big delays. In support of that we trace the level of the nodes during the execution.
Reference: [Gaf85] <author> Eli Gafni. </author> <title> Improvements in the time complexity of two message-optimal election algorithms. </title> <booktitle> Proceedings of 1985 Principles Of Distributed Computing (PODC), </booktitle> <address> Ontario, </address> <month> August, </month> <year> 1985. </year>
Reference-contexts: Following [GHS83], various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity [CT85], <ref> [Gaf85] </ref>, [Awe87], [FM95] and [Fal95]. As it will become apparent later, there is a trade-off between termination time and messages. The first contribution of this paper is the introduction of further refinements to the basic algorithm, which are based on a novel approach called Distributed Information. <p> Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper [GHS83], Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including [CT85], <ref> [Gaf85] </ref>, [Awe87], [Fal95], and [FM95]. In this basic algorithm, each node is initially the root of its own fragment (a trivial connected subgraph of the MST) and all the edges are Unlabeled. <p> In the subsequent sections, we will describe the new features introduced in the later algorithms, without repeating features that do not change. 3.1 Size Estimation The technique that we will call Root Size 1 (or RS) was introduced in [CT85] <ref> [Gaf85] </ref>. The authors recognized that, although any fragment of level L in the basic algorithm obviously must have at least 2 L nodes, it may be much larger than that if it has accepted a lot of submissions. <p> Having an optiMSTic attitude, we hope to come across such configurations. The Communication Efficient techniques re quiring O (E + N log (N )) messages are: 1. Root Size ([CT85] <ref> [Gaf85] </ref>) 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance ([FM95] [Awe87]) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. <p> The Communication Efficient techniques re quiring O (E + N log (N )) messages are: 1. Root Size ([CT85] <ref> [Gaf85] </ref>) 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance ([FM95] [Awe87]) The communication efficient algorithms are: [GHS83], [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. Leader Size (optiMST) The time efficient algorithms are: the algorithm of the third phase in [FM95], and optiMST.
Reference: [Gbu95] <author> Pawel Gburzynski. </author> <title> Protocol design for local and metropolitan area networks. </title> <publisher> Prentice-Hall, </publisher> <year> 1995. </year> <title> (author's e-mail: </title> <publisher> pawel@cs.ualberta.ca). </publisher>
Reference-contexts: We created a simulator using SMURPH <ref> [Gbu95] </ref>, a 0 2000 4000 6000 40 60 80 100 120 140 160 180 200 Fast Report optiMST Root Size no FR optiMST 8000 12000 16000 20000 24000 GHS Fast Report optiMST Root Size no FR optiMST package on top of C++ for simulating low level communication protocols.
Reference: [GHS83] <author> R.G. Gallager, </author> <title> P.A. Humblet, and P.M. Spira. A distributed algorithm for minimum weight spanning trees. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 5(1) </volume> <pages> 66-77, </pages> <month> January </month> <year> 1983. </year>
Reference-contexts: Fortunately, finding distributed MST algorithms is straightforward because the centralized MST problem can be easily solved by greedy algorithms. A distributed algorithm was proposed by Gallager et al <ref> [GHS83] </ref> requiring O (E + N log (N )) messages and O (N log (N )) time units. We refer to this algorithm as the basic algorithm, and we outline it in the next section. <p> For the time complexity, it was proven [SB95] that a tighter bound of the basic algorithm is O ((D MST + d) log (N )) units, where D MST is the diameter of the resulting MST and d is the maximum degree of the nodes. Following <ref> [GHS83] </ref>, various authors have proposed enhancements to the basic algorithm to achieve efficient time and message complexity [CT85], [Gaf85], [Awe87], [FM95] and [Fal95]. As it will become apparent later, there is a trade-off between termination time and messages. <p> As we will see the results favour strongly algorithms with non-efficient worst case complexity, and especially an algorithm based on the Distributed Information approach. The rest of this paper is organized as follows. In Section 2, we describe the basic algorithm of <ref> [GHS83] </ref>, along with the path graph example that demonstrates its non optimal time complexity. We then list, in section 3, the innovations introduced by previous authors. Section 4 presents some new techniques and algorithms based on Distributed Information, and describes a methodology for creating optimal algorithm using non optimal ones. <p> Section 4 presents some new techniques and algorithms based on Distributed Information, and describes a methodology for creating optimal algorithm using non optimal ones. Finally, section 5 presents our experimental results. 2 The Basic Algorithm In their pioneering paper <ref> [GHS83] </ref>, Gallager, Hum-blet and Spira introduced the distributed MST problem and presented an algorithm that has formed the basis of much subsequent work in the area, including [CT85], [Gaf85], [Awe87], [Fal95], and [FM95]. <p> The leader sends a connect message along that edge and joins with the other fragment. Note that the leader is a "temporary root"; it makes the decisions for its fragment and its role ends when an initiate message from a root arrives. Even the basic algorithm presented in <ref> [GHS83] </ref> contains several subtleties. First, each fragment has a level, L, in addition to its unique fragment identifier F , denoted as a pair (F; L) for the rest of the paper. <p> Another noteworthy feature involves reducing the required number of messages by having a low-level node delay its response to any test message arriving from a high-level fragment, since the high-level fragment can not equi-join with or submit to a low-level fragment (see <ref> [GHS83] </ref> for more details). 2.1 A Bad Case Example F, L F , L F , L F , L 1 1 2 2 F ,L r-1 r-1 r r e x An intuitive feeling of how the time complexity can "go bad" can be obtained by the following pathological test <p> It is important to notice that the edge-MOE values may not be completely up-to-date. However, under the Joining policy introduced in <ref> [GHS83] </ref>, the weight of the current MOE reachable via a branch can only increase over time. In other words, edge-MOEs are non-decreasing in time, and thus each edge-MOE is always a lower bound to the current MOE reachable in the same direction. <p> Although this observation sounds trivial, in practice it results in a substantial speed-up by shortening the final (and most time consuming!) executions of the Finding procedure. The FR technique is compatible with the basic algorithm in <ref> [GHS83] </ref>, and hence can be added to most of the algorithms described in the previous section. However, FR may interfere with the node counting procedure in RS, since FR may complete the Reporting procedure before the counting is completed. <p> Thus, the FJ technique can take advantage of a "good" configuration, but in the worst case it does not do better than the basic algorithm. Although the FJ technique is compatible with the basic algorithm in <ref> [GHS83] </ref>, the addition of FJ results in a qualitatively different type of operation. <p> We find the combination of techniques Fast Report, Root Distance, Root Size and Leader Size in a single algorithm to be quite attractive, and denote it by the name optiMST. 3 4.5 Grouping the Techniques So far we have seen a number of different techniques that can be incorporated in <ref> [GHS83] </ref> and give rise to new algorithms. We can now group these techniques according to their performance characteristics. 3 The name comes from the fact that these techniques are able to exploit "favourable" configurations. Having an optiMSTic attitude, we hope to come across such configurations. <p> The Communication Efficient techniques re quiring O (E + N log (N )) messages are: 1. Root Size ([CT85] [Gaf85]) 2. Fast Report (Fast Report algorithm) 3. Local Decision to Join (autonoMST) 4. Root Distance ([FM95] [Awe87]) The communication efficient algorithms are: <ref> [GHS83] </ref>, [CT85], [Gaf85], Fast Report and autonoMST. The Time Efficient techniques requiring O (N ) time units are: 1. Leader Distance ([Awe87] with the corrections in [FM95]) 2. Leader Size (optiMST) The time efficient algorithms are: the algorithm of the third phase in [FM95], and optiMST. <p> Given the wide variety of different algorithms that can be created, as stated already, we decided to test a representative group rather than exhaustively trying all combinations. We chose to test the following algorithms, each of which is identified by the nickname we gave it: 1. GHS: the basic <ref> [GHS83] </ref> 2. FR: Fast Report as described in a previous section 3. optiMST: optiMST as described in a previous sec tion 4.
Reference: [Pri57] <author> R. C. Prim. </author> <title> Shortest connection networks and some generalizations. </title> <journal> Bell Syst. Tech. J. </journal> <volume> 36, </volume> <year> 1957. </year>
Reference-contexts: We found the package very useful though some extensions had to be made. The correctness of the solutions was tested with a non-distributed MST algorithm known as Prim's algorithm <ref> [Pri57] </ref>, and described in [Afr90]. Given the wide variety of different algorithms that can be created, as stated already, we decided to test a representative group rather than exhaustively trying all combinations.
Reference: [SB95] <author> Gurdip Singh and Arthur J. Bernstein. </author> <title> A highly asynchronous minimum spanning tree protocol. Distributed Computing, </title> <publisher> Springer Verlag 8(3), </publisher> <year> 1995. </year>
Reference-contexts: So the basic algorithm is message efficient. For the time complexity, it was proven <ref> [SB95] </ref> that a tighter bound of the basic algorithm is O ((D MST + d) log (N )) units, where D MST is the diameter of the resulting MST and d is the maximum degree of the nodes.
References-found: 10

