URL: http://www.icsi.berkeley.edu/ftp/global/pub/ai/picls/parc-tagger.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/ai/picls/
Root-URL: http://www.icsi.berkeley.edu
Title: A Practical Part-of-Speech Tagger  
Author: Doug Cutting and Julian Kupiec and Jan Pedersen and Penelope Sibun 
Address: 3333 Coyote Hill Road, Palo Alto, CA 94304, USA  
Affiliation: Xerox Palo Alto Research Center  
Abstract: We present an implementation of a part-of-speech tagger based on a hidden Markov model. The methodology enables robust and accurate tagging with few resource requirements. Only a lexicon and some unlabeled training text are required. Accuracy exceeds 96%. We describe implementation strategies and optimizations which result in high-speed operation. Three applications for tagging are described: phrase recognition; word sense disambiguation; and grammatical function assignment.
Abstract-found: 1
Intro-found: 1
Reference: [ Aho et al., 1986 ] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: The accepted approach is to specify token classes with regular expressions. These may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens <ref> [ Aho et al., 1986, Lesk, 1975 ] </ref> . In the context of tagging, we require at least two token classes: sentence boundary and word. Other classes may include numbers, paragraph boundaries and various sorts of punctuation (e.g., braces of various types, commas). <p> Just as with programming languages, with text it is not always possible to unambiguously specify the required token classes with regular expressions. However the addition of a simple lookahead mechanism which allows specification of right context ameliorates this <ref> [ Aho et al., 1986, Lesk, 1975 ] </ref> . For example, a sentence boundary in English text might be identified by a period, followed by white-space, followed by an uppercase letter. However the up percase letter must not be consumed, as it is the first com-ponent of the next token.
Reference: [ Baum, 1972 ] <author> L. E. Baum. </author> <title> An inequality and associated maximization technique in statistical estimation for probabilistic functions of a Markov process. </title> <journal> Inequalities, </journal> <volume> 3 </volume> <pages> 1-8, </pages> <year> 1972. </year>
Reference-contexts: These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. In this situation the Baum-Welch algorithm (also known as the forward-backward algorithm) can be used <ref> [ Baum, 1972 ] </ref> . Under this regime the model is called a hidden Markov model (HMM), as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger [ Jelinek, 1985 ] . <p> Maximum likelihood estimates (that is, estimates that maximize the probability of the training set) can be found through application of alternating expectation in a procedure known as the Baum-Welch, or forward-backward, algorithm <ref> [ Baum, 1972 ] </ref> . <p> It can be shown that this algorithm will converge, although possibly to a non-global maximum <ref> [ Baum, 1972 ] </ref> . Once a model has been estimated, selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S.
Reference: [ Church, 1988 ] <author> K. W. Church. </author> <title> A stochastic parts program and noun phrase parser for unrestricted text. </title> <booktitle> In Proceedings of the Second Conference on Applied Natural Language Processing (ACL), </booktitle> <pages> pages 136-143, </pages> <year> 1988. </year>
Reference-contexts: At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model. Church uses the tagged Brown corpus for training <ref> [ Church, 1988 ] </ref> . These models involve probabilities for each word in the lexicon, so large tagged corpora are required for reliable estimation. The second method of training does not require a tagged training corpus. <p> Eight iterations of training were used. This level of accuracy is comparable to the best achieved by other taggers <ref> [ Church, 1988, Merialdo, 1991 ] </ref> . The Brown Corpus contains fragments and ungrammat-icalities, thus providing a good demonstration of robustness. 5.3 Tunable and Reusable A tagger should be tunable, so that systematic tagging errors and anomalies can be addressed.
Reference: [ Cutting et al., 1991 ] <author> D.R. Cutting, J. Pedersen, and P.- K. Halvorsen. </author> <title> An object-oriented architecture for text retrieval. </title> <booktitle> In Conference Proceedings of RIAO'91, Intelligent Text and Image Handling, Barcelona, Spain, </booktitle> <pages> pages 285-298, </pages> <month> April </month> <year> 1991. </year> <note> Also available as Xerox PARC technical report SSL-90-83. </note>
Reference-contexts: This has the effect of disfavoring the indicated outcomes without disallowing them; sufficient converse data can rehabilitate these values. 4 Architecture In support of this and other work, we have developed a system architecture for text access <ref> [ Cutting et al., 1991 ] </ref> . This architecture defines five components for such systems: corpus, which provides text in a generic manner; analysis, which extracts terms from the text; index which stores term occurrence statistics; and search, which utilizes these statistics to resolve queries.
Reference: [ DeRose, 1988 ] <author> S. DeRose. </author> <title> Grammatical category disambiguation by statistical optimization. </title> <journal> Computational Linguistics, </journal> <volume> 14 </volume> <pages> 31-39, </pages> <year> 1988. </year>
Reference-contexts: TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [ Kosken-niemi, 1990 ] . Statistical methods have also been used (e.g., <ref> [ DeRose, 1988 ] </ref> , [ Garside et al., 1987 ] ). These provide the capability of resolving ambiguity on the basis of most likely interpretation.
Reference: [ Derouault and Merialdo, 1986 ] <author> A. M. Derouault and B. Merialdo. </author> <title> Natural language modeling for phoneme-to-text transcription. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8:742-749, </volume> <year> 1986. </year>
Reference-contexts: Two types of training (i.e., parameter estimation) have been used with this model. The first makes use of a tagged training corpus. Derouault and Merialdo use a bootstrap method for training <ref> [ Derouault and Merialdo, 1986 ] </ref> . At first, a relatively small amount of text is manually tagged and used to train a partially accurate model. The model is then used to tag more text, and the tags are manually corrected and then used to retrain the model.
Reference: [ Francis and Kucera, 1982 ] <author> W. N. Francis and F. Kucera. </author> <title> Frequency Analysis of English Usage. </title> <publisher> Houghton Mi*in, </publisher> <year> 1982. </year>
Reference-contexts: Greene and Rubin used a rule-based approach in the TAGGIT program [ Greene and Rubin, 1971 ] , which was an aid in tagging the Brown corpus <ref> [ Francis and Kucera, 1982 ] </ref> . TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [ Kosken-niemi, 1990 ] . <p> seen from these figures that training on a new corpus may be accomplished in a matter of minutes, and that tens of megabytes of text may then be tagged per hour. 5.2 Accurate and Robust When using a lexicon and tagset built from the tagged text of the Brown corpus <ref> [ Francis and Kucera, 1982 ] </ref> , training on one half of the corpus (about 500,000 words) and tagging the other, 96% of word instances were assigned the correct tag. Eight iterations of training were used.
Reference: [ Garside et al., 1987 ] <author> R. Garside, G. Leech, and G. Samp-son. </author> <title> The Computational Analysis of English. </title> <publisher> Longman, </publisher> <year> 1987. </year>
Reference-contexts: TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years. More recently, Koskenniemi also used a rule-based approach implemented with finite-state machines [ Kosken-niemi, 1990 ] . Statistical methods have also been used (e.g., [ DeRose, 1988 ] , <ref> [ Garside et al., 1987 ] </ref> ). These provide the capability of resolving ambiguity on the basis of most likely interpretation.
Reference: [ Greene and Rubin, 1971 ] <author> B. B. Greene and G. M. Rubin. </author> <title> Automatic grammatical tagging of English. </title> <type> Technical report, </type> <institution> Department of Linguistics, Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1971. </year>
Reference-contexts: Reusable The effort required to retarget a tagger to new corpora, new tagsets, and new languages should be minimal. 2 Methodology 2.1 Background Several different approaches have been used for building text taggers. Greene and Rubin used a rule-based approach in the TAGGIT program <ref> [ Greene and Rubin, 1971 ] </ref> , which was an aid in tagging the Brown corpus [ Francis and Kucera, 1982 ] . TAGGIT disambiguated 77% of the corpus; the rest was done manually over a period of several years.
Reference: [ Hearst, 1991 ] <author> M. A. Hearst. </author> <title> Noun homograph disambiguation using local context in large text corpora. </title> <booktitle> In The Proceedings of the 7th New OED Conference on Using Corpora, </booktitle> <pages> pages 1-22, </pages> <address> Oxford, </address> <year> 1991. </year>
Reference-contexts: But many words have multiple meanings even while occupying the same part of speech. To this end, the tagger has been used in the implementation of an experimental noun homograph disambiguation algorithm <ref> [ Hearst, 1991 ] </ref> . The algorithm (known as CatchWord) performs supervised training over a large text corpus, gathering lexical, orthographic, and simple syntactic evidence for each sense of the ambiguous noun.
Reference: [ Jelinek and Mercer, 1980 ] <author> F. Jelinek and R. L. Mercer. </author> <title> Interpolated estimation of markov source parameters from sparse data. </title> <booktitle> In Proceedings of the Workshop Pattern Recognition in Practice, </booktitle> <pages> pages 381-397, </pages> <address> Amster-dam, 1980. </address> <publisher> North-Holland. </publisher>
Reference-contexts: Jelinek has used this method for training a text tagger [ Jelinek, 1985 ] . Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from second-and first-order models and a uniform probability distribution <ref> [ Jelinek and Mercer, 1980 ] </ref> . Kupiec used word equivalence classes (referred to here as ambiguity classes) based on parts of speech, to pool data from individual words [ Ku-piec, 1989b ] . The most common words are still represented individually, as sufficient data exist for robust estimation.
Reference: [ Jelinek, 1985 ] <author> F. Jelinek. </author> <title> Markov source modeling of text generation. </title> <editor> In J. K. Skwirzinski, editor, </editor> <title> Impact of Processing Techniques on Communication. </title> <publisher> Nijhoff, </publisher> <address> Dor-drecht, </address> <year> 1985. </year>
Reference-contexts: Under this regime the model is called a hidden Markov model (HMM), as state transitions (i.e., part-of-speech categories) are assumed to be unobservable. Jelinek has used this method for training a text tagger <ref> [ Jelinek, 1985 ] </ref> . Parameter smoothing can be conveniently achieved using the method of deleted interpolation in which weighted estimates are taken from second-and first-order models and a uniform probability distribution [ Jelinek and Mercer, 1980 ] .
Reference: [ Knuth, 1973 ] <author> D. Knuth. </author> <title> The Art of Computer Programming, volume 3: Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: This class typically contains all the open class categories in the language. Dictionaries and suffix tables are both efficiently implementable as letter trees, or tries <ref> [ Knuth, 1973 ] </ref> , which require that each character of a word be examined only once during a lookup. 5 Performance In this section, we detail how our tagger meets the desiderata that we outlined in section 1. 5.1 Efficient The system is implemented in Common Lisp [ Steele, 1990
Reference: [ Koskenniemi, 1990 ] <author> K. Koskenniemi. </author> <title> Finte-state parsing and disambiguation. </title> <editor> In H. Karlgren, editor, </editor> <booktitle> COLING-90, </booktitle> <pages> pages 229-232, </pages> <address> Helsinki University, </address> <year> 1990. </year>
Reference: [ Kupiec, 1989a ] <author> J. M. Kupiec. </author> <title> Augmenting a hidden Markov model for phrase-dependent word tagging. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 92-98, </pages> <address> Cape Cod, MA, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). In <ref> [ Kupiec, 1989a ] </ref> , networks are used to selectively augment the context in a basic first-order model, rather than using uniformly second-order dependencies. 2.2 Our approach We next describe how our choice of techniques satisfies the criteria listed in section 1. <p> Since N and M are fixed by the model, the only parameter that can be varied to reduce storage costs is T . Now, adequate training requires processing from tens of thousands to hundreds of thousands of tokens <ref> [ Kupiec, 1989a ] </ref> . The training set can be considered one long sequence, it which case T is very large indeed, or it can be broken up into a number of smaller sequences at convenient boundaries.
Reference: [ Kupiec, 1989b ] <author> J. M. Kupiec. </author> <title> Probabilistic models of short and long distance word dependencies in running text. </title> <booktitle> In Proceedings of the 1989 DARPA Speech and Natural Language Workshop, </booktitle> <pages> pages 290-295, </pages> <address> Philadel-phia, 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Kupiec, 1992 ] <author> J. M. Kupiec. </author> <title> Robust part-of-speech tagging using a hidden markov model. </title> <note> submitted to Computer Speech and Language, </note> <year> 1992. </year>
Reference-contexts: However all other words are represented according to the set of possible categories they can assume. In this manner, the vocabulary of 50,000 words in the Brown corpus can be reduced to approximately 400 distinct ambiguity classes <ref> [ Kupiec, 1992 ] </ref> . To further reduce the number of parameters, a first-order model can be employed (this assumes that a word's category depends only on the immediately preceding word's category). <p> One obvious approach is simply to average. However, this fails if any two 4 An equivalent approach maintains a mapping from states i to non-zero symbol probabilities and simply avoids, in the inner iteration, computing products which must be zero <ref> [ Kupiec, 1992 ] </ref> . states are indistinguishable (in the sense that they had the same transition probabilities and the same symbol probabilities at start), because states are then not matched across trained models. <p> As a second stage, a language-specific method can be employed to guess ambiguity classes for unknown words. For many languages (e.g., English and French), word suffixes provide strong cues to words' possible categories. Prob-abalistic predictions of a word's category can be made by analyzing suffixes in untagged text <ref> [ Kupiec, 1992, Meteer et al., 1991 ] </ref> . As a final stage, if a word is not in the manually constructed lexicon, and its suffix is not recognized, a default ambiguity class is used. This class typically contains all the open class categories in the language.
Reference: [ Lesk, 1975 ] <author> M. E. Lesk. </author> <title> LEX | a lexical analyzer generator. </title> <type> Computing Science Technical Report 39, </type> <institution> AT&T Bell Laboratories, </institution> <address> Murray Hill, New Jersey, </address> <year> 1975. </year>
Reference-contexts: The accepted approach is to specify token classes with regular expressions. These may be compiled into a single deterministic finite state automaton which partitions character streams into labeled tokens <ref> [ Aho et al., 1986, Lesk, 1975 ] </ref> . In the context of tagging, we require at least two token classes: sentence boundary and word. Other classes may include numbers, paragraph boundaries and various sorts of punctuation (e.g., braces of various types, commas). <p> Just as with programming languages, with text it is not always possible to unambiguously specify the required token classes with regular expressions. However the addition of a simple lookahead mechanism which allows specification of right context ameliorates this <ref> [ Aho et al., 1986, Lesk, 1975 ] </ref> . For example, a sentence boundary in English text might be identified by a period, followed by white-space, followed by an uppercase letter. However the up percase letter must not be consumed, as it is the first com-ponent of the next token.
Reference: [ Levinson et al., 1983 ] <author> S. E. Levinson, L. R. Rabiner, and M. M. Sondhi. </author> <title> An introduction to the application of the theory of probabilistic functions of a Markov process to automatic speech recognition. </title> <journal> Bell System Technical Journal, </journal> <volume> 62 </volume> <pages> 1035-1074, </pages> <year> 1983. </year>
Reference-contexts: By using the fact that words are typically associated with only a few part-of-speech categories, and carefully ordering the computation, the algorithms have linear complexity (section 3.3). 3 Hidden Markov Modeling The hidden Markov modeling component of our tagger is implemented as an independent module following the specification given in <ref> [ Levinson et al., 1983 ] </ref> , with special attention to space and time efficiency issues. <p> difficulty occurs in equation 3 that can be cured by the addition of a new term, c t+1 , in each product of the upper sum: P T 1 P T1 = ^a ij : Numerical instability in the Viterbi algorithm can be ameliorated by operating on a logarithmic scale <ref> [ Levinson et al., 1983 ] </ref> .
Reference: [ Merialdo, 1991 ] <author> B. Merialdo. </author> <title> Tagging text with a proba-blistic model. </title> <booktitle> In Proceedings of ICASSP-91, </booktitle> <pages> pages 809-812, </pages> <address> Toronto, Canada, </address> <year> 1991. </year>
Reference-contexts: Eight iterations of training were used. This level of accuracy is comparable to the best achieved by other taggers <ref> [ Church, 1988, Merialdo, 1991 ] </ref> . The Brown Corpus contains fragments and ungrammat-icalities, thus providing a good demonstration of robustness. 5.3 Tunable and Reusable A tagger should be tunable, so that systematic tagging errors and anomalies can be addressed.
Reference: [ Meteer et al., 1991 ] <author> M. W. Meteer, R. Schwartz, and R. Weischedel. POST: </author> <title> Using probabilities in language processing. </title> <booktitle> In Proceedings of the 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 960-965, </pages> <year> 1991. </year>
Reference-contexts: As a second stage, a language-specific method can be employed to guess ambiguity classes for unknown words. For many languages (e.g., English and French), word suffixes provide strong cues to words' possible categories. Prob-abalistic predictions of a word's category can be made by analyzing suffixes in untagged text <ref> [ Kupiec, 1992, Meteer et al., 1991 ] </ref> . As a final stage, if a word is not in the manually constructed lexicon, and its suffix is not recognized, a default ambiguity class is used. This class typically contains all the open class categories in the language.
Reference: [ P754, 1981 ] <author> IEEE Task P754. </author> <title> A proposed standard for binary floating-point arithmetic. </title> <journal> Computer, </journal> <volume> 14(3) </volume> <pages> 51-62, </pages> <month> March </month> <year> 1981. </year>
Reference-contexts: However, this can be elegantly handled through the use of IEEE negative infinity <ref> [ P754, 1981 ] </ref> . 3.3 Reducing Time Complexity As can be seen from equations 1-5, the time cost of training is O (T N 2 ). Similarly, as given in equation 6, the Viterbi algorithm is also O (T N 2 ).
Reference: [ Rabiner and Juang, 1986 ] <author> L. R. Rabiner and B. H. Juang. </author> <title> An introduction to hidden markov models. </title> <journal> IEEE ASSP Magazine, </journal> <month> January </month> <year> 1986. </year>
Reference: [ Sibun, 1991 ] <author> P. Sibun. </author> <title> Grammatical function assignment in unrestricted text. </title> <type> internal report, </type> <institution> Xerox Palo Alto Research Center, </institution> <year> 1991. </year>
Reference-contexts: tagger in two ways: (i) to determine the part of speech of the target word (filtering out the non-noun usages) and (ii) as a step in the phrase recognition analysis of the context surrounding the noun. 6.3 Grammatical Function Assignment The phrase recognizers also provide input to a system, Sopa <ref> [ Sibun, 1991 ] </ref> , which recognizes nominal arguments of verbs, specifically, Subject, Object, and Predicative Arguments. Sopa does not rely on information (such as arity or voice) specific to the particular verbs involved.
Reference: [ Steele, 1990 ] <author> G. L. Steele, Jr. </author> <title> Common Lisp, The Language. </title> <note> Digital Press, second edition, </note> <year> 1990. </year>
Reference-contexts: or tries [ Knuth, 1973 ] , which require that each character of a word be examined only once during a lookup. 5 Performance In this section, we detail how our tagger meets the desiderata that we outlined in section 1. 5.1 Efficient The system is implemented in Common Lisp <ref> [ Steele, 1990 ] </ref> . All timings reported are for a Sun SPARCStation2. The English lexicon used contains 38 tags (M = 38) and 174 ambiguity classes (N = 174). Training was performed on 25,000 words in articles selected randomly from Grolier's Encyclopedia.
Reference: [ Viterbi, 1967 ] <author> A. J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimal decoding algorithm. </title> <journal> In IEEE Transactions on Information Theory, </journal> <pages> pages 260-269, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Once a model has been estimated, selecting the most likely underlying sequence of state transitions corresponding to an observation S can be thought of as a maximization over all sequences that might generate S. An efficient dynamic programming procedure, known as the Viterbi algorithm <ref> [ Viterbi, 1967 ] </ref> , arranges for this computation to proceed in time proportional to T .
References-found: 26

