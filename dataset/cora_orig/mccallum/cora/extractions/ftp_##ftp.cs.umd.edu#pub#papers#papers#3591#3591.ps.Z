URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3591/3591.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Email: fthkim,purtilog@cs.umd.edu  
Title: LOAD BALANCING FOR PARALLEL LOOPS IN WORKSTATION CLUSTERS  
Author: Tae-Hyung Kim and James M. Purtilo 
Keyword: Key words: Load balancing, Parallel loop scheduling, Workstation clusters, Distributed programs, Programming environments.  
Note: This research has been supported by Office of Naval Research and a grant from the Siemens Corporation.  
Address: College Park, Maryland 20742  
Affiliation: Institute for Advanced Computer Studies and Computer Science Department University of Maryland  
Abstract: Load imbalance is a serious impediment to achieving good performance in parallel processing. Global load balancing schemes are not adequately manage to balance parallel tasks generated from a single application. Dynamic loop scheduling methods are known to be useful in balancing parallel loops on shared-memory multiprocessor machines. However, their centralized nature causes a bottleneck for the relatively small number of processors in workstation clusters because of order-of-magnitude differences in communications overheads. Moreover, improvements of basic loop scheduling methods have not dealt effectively with irregularly distributed workloads in parallel loops, which commonly occur in applications for workstation clusters. In this paper, we present a new decentralized balancing method for parallel loops on workstation clusters. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. P. Belkhale and P. Banerjee. </author> <title> An approximate algorithm for the partitionable independent task scheduling problem. </title> <booktitle> In Proceedings of '90 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1990. </year>
Reference-contexts: If we cannot guarantee the faster processor finishes earlier, the migration paths must be bi-directional as in the following cases. For the latter case, we cannot initialize in the above way as the value of L (i) is unknown. The LPT (Largest Processing Time first) algorithm <ref> [1] </ref> is for this class of loop models. The tasks are sorted in descending order based on execution time L (i). Each processor should process the largest task first.
Reference: [2] <author> L. Bomans and D. Roose. </author> <title> Benchmarking the iPSC/2 hypercube multiprocessor. </title> <journal> Concur-rency: Practice and Experience, </journal> <volume> Vol. 1(1) </volume> <pages> 3-18, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: The conventional approach to modeling communication time for transferring a message of m bytes is a simple linear function, i.e. T comm = ff + fim, where ff is startup time and fi is transfer time per byte <ref> [2] </ref>. The empirical values for ff and fi under the PVM system [18] at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi [17].
Reference: [3] <author> Clemens H. Cap and Volker Strumpen. </author> <title> Efficient parallel computing in distributed workstation environments. </title> <journal> Parallel Computing, </journal> <volume> Vol. 19 </volume> <pages> 1221-1234, </pages> <year> 1993. </year>
Reference-contexts: if t new &lt; n n+1 t min , the execution time of (n + 1)-processors cluster is T (n+1)t new , which is longer than that of n processors! One may want to get around this problem by allocating tasks according to the known computing power of each processor <ref> [9, 3] </ref>. However, their methods were static, thus of limited usefulness.
Reference: [4] <author> N. Carriero and D. Gelernter. </author> <title> How to write parallel programs: A guide to the perplexed. </title> <journal> ACM Computing Surveys, </journal> <volume> Vol. 21(6) </volume> <pages> 322-356, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: Two major limitations are that it is static and that the loop heterogeneity model is linear. In this paper, we present a dynamic load balancing method for parallel loops of more general patterns, since many non-scientific applications such as the DNA sequence search problem <ref> [4] </ref> or the Mandelbrot set computation [8], which are good candidate applications for workstation clusters, often do not carry conventional regular loop patterns. <p> Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable. For example, the parallel tasks in the DNA sequence search problem <ref> [4] </ref> and the Mandelbrot set computation are all irregular, but the tasks in the first problem are predictable while the tasks in the second one are not. <p> loops are predictable (see Section 2.1), there are two cases: one is when we know the amount of the required computation exactly, as in Fig. 1 (a), (b), (c) and sometimes (d), and the other is when we can determine just the orderings, like in the DNA sequence search problem <ref> [4] </ref>. For the former case, as L (i) is known in advance, if we distribute proportionately according to each processor's throughput, we can reduce the likelihood of migration. In other words, the processor P i with t i will get t i P P k t k .
Reference: [5] <author> M. Cierniak, W. Li, and M. J. Zaki. </author> <title> Loop scheduling for heterogeneity. </title> <booktitle> In Proceedings of the 4th International Symposium on High-Performance Distributed Computing, </booktitle> <month> August </month> <year> 1995. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [19, 12, 15, 20, 5] </ref>. If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. <p> Recently, networks of workstations have emerged as viable candidates for running parallel applications. To our knowledge, the first work on parallel loop scheduling in a network of heterogeneous workstations was done by Cierniak et al. <ref> [5] </ref>. They considered three aspects of heterogeneity | loop, processor, and network | and developed algorithms for generating optimal and sub-optimal 2 schedules of loops. Two major limitations are that it is static and that the loop heterogeneity model is linear. <p> Finally, as in Fig. 1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [15, 20, 5] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable.
Reference: [6] <author> F. DeRemer and H. Kron. </author> <title> Programming-in-the-large versus programming-in-the-small. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 2(2), </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: From the concern of software engineering, module interconnection activity is understood to be an essentially distinct and different intellectual activity from that of implementing individual modules; that is "programming-in-the-large" is distinct from "programming-in-the-small" <ref> [6] </ref>. We apply this separating principle in order to isolate performance factors from the module programming level with the aid of the CORD that eventually intergrates all information to prepare executables.
Reference: [7] <author> Derek L. Eager, Edward D. Lazowska, and John Zahorjan. </author> <title> Adaptive load sharing in homogeneous distributed systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 12(5) </volume> <pages> 662-675, </pages> <month> May </month> <year> 1986. </year>
Reference-contexts: the throughput of W i , which is defined by the number of unit tasks per unit time. * fl ij : the amount of load to migrate from i to j. 3 LOAD BALANCING METHOD Two important components of dynamic load balancing schemes are transfer policy and location policy <ref> [7, 11] </ref>. The transfer policy determines whether a task should be processed locally or remotely by transferring it at a particular load state. The location policy determines which process initiates the migration and its source or destination. These are for global load balancing from the OS's viewpoints. <p> Multi-dimensional load vectors determine the load state of a processor. In our system, we aim to balance parallel loops in an application. A simple `demand' message is enough to initiate load migration rather than load state exchange [11] or random polling of candidate processors <ref> [7] </ref> because the only load vector is the number of sub-tasks in a processor. The transfer policy then becomes simple: if a processor receives a request message for transfer from a processor that is running out of sub-tasks to work on, it migrates some of its sub-tasks to that processor. <p> The thicker links denote higher level links; they will be used only if the load cannot be balanced through the lower links. 10 4 ANALYSIS OF MIGRATION BEHAVIORS There are two important concerns in devising a load balancing scheme <ref> [7] </ref>. First, the overhead should not negate the benefits of an improved load distribution. Next, the potential migration instability 1 , in which processors spend too much time transferring tasks, should be avoided. <p> To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form [13] or it is meaningless where distributed load patterns cannot be assumed to be known 19 in advance <ref> [7, 11] </ref>. More interesting topologies can be studied in the future. We have shown analytically that the overhead of our method is lower than that of the self-scheduling scheme when an "predictability" condition is given. We have also provided some experimental data for cases when the loop pattern is unpredictably irregular.
Reference: [8] <author> J. D. Foley, A. van. Dam, S. K. Feiner, J. F. Hughes, and R. L. Phillips. </author> <title> Introduction to Computer Graphics. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1993. </year>
Reference-contexts: In this paper, we present a dynamic load balancing method for parallel loops of more general patterns, since many non-scientific applications such as the DNA sequence search problem [4] or the Mandelbrot set computation <ref> [8] </ref>, which are good candidate applications for workstation clusters, often do not carry conventional regular loop patterns.
Reference: [9] <author> A. S. Grimshaw, J. B. Weissman, E. A. West, and Jr. E. C. Loyot. Metasystems: </author> <title> An approach combining parallel processing and heterogeneous distributed computing systems. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 21 </volume> <pages> 257-270, </pages> <year> 1994. </year> <month> 20 </month>
Reference-contexts: if t new &lt; n n+1 t min , the execution time of (n + 1)-processors cluster is T (n+1)t new , which is longer than that of n processors! One may want to get around this problem by allocating tasks according to the known computing power of each processor <ref> [9, 3] </ref>. However, their methods were static, thus of limited usefulness.
Reference: [10] <author> T.-H. Kim and J. M. Purtilo. </author> <title> Configuration-level optimization of RPC-based distributed programs. </title> <booktitle> In Proceedings of the 15th International Conference on Distributed Computing Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: Our work, called CORD (Configuration-level Optimization of RPC-based Distributed programs), is a framework for automatically generating all necessary executables from RPC-based distributed programs according to a configuration-level description intended for high performance <ref> [10] </ref>. From the concern of software engineering, module interconnection activity is understood to be an essentially distinct and different intellectual activity from that of implementing individual modules; that is "programming-in-the-large" is distinct from "programming-in-the-small" [6].
Reference: [11] <author> Philip Krueger and Niranjan G. Shivaratri. </author> <title> Adaptive location policies for global scheduling. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 20(6) </volume> <pages> 432-444, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: the throughput of W i , which is defined by the number of unit tasks per unit time. * fl ij : the amount of load to migrate from i to j. 3 LOAD BALANCING METHOD Two important components of dynamic load balancing schemes are transfer policy and location policy <ref> [7, 11] </ref>. The transfer policy determines whether a task should be processed locally or remotely by transferring it at a particular load state. The location policy determines which process initiates the migration and its source or destination. These are for global load balancing from the OS's viewpoints. <p> These are for global load balancing from the OS's viewpoints. Multi-dimensional load vectors determine the load state of a processor. In our system, we aim to balance parallel loops in an application. A simple `demand' message is enough to initiate load migration rather than load state exchange <ref> [11] </ref> or random polling of candidate processors [7] because the only load vector is the number of sub-tasks in a processor. <p> To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form [13] or it is meaningless where distributed load patterns cannot be assumed to be known 19 in advance <ref> [7, 11] </ref>. More interesting topologies can be studied in the future. We have shown analytically that the overhead of our method is lower than that of the self-scheduling scheme when an "predictability" condition is given. We have also provided some experimental data for cases when the loop pattern is unpredictably irregular.
Reference: [12] <author> C. P. Kruskal and A. Weiss. </author> <title> Allocating independent subtasks on parallel processors. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 11(10) </volume> <pages> 1001-1016, </pages> <month> October </month> <year> 1985. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [19, 12, 15, 20, 5] </ref>. If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. <p> It assigns a new iteration to a processor only when the processor becomes available. However, this method requires tremendous synchronization overhead; to be practical, hardware support to fast barrier synchronization primitives is desirable. Uniform-sized chunking (CSS) reduces such synchronization overhead by sending K iterations instead of one <ref> [12] </ref>. In this method, the overhead is amortized to 1=K, but the possibility of load imbalance increases when K is increased. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [12, 15, 20] </ref>, although the pure SS scheme is orthogonal to the loop patterns. 1.1 Programming Environment Networks of workstations have by nature easy-to-change configurations; programs must be adapted accordingly whenever the hardware configuration has been changed. <p> Of course, the above three loops are all predictable. 2.2 Workstation Cluster Model for Load Balancing Fig. 2 shows two representative topologies in the workstation cluster model for parallel loops. Fig. 2 (a) represents the topology of traditional loop scheduling methods <ref> [19, 12, 15, 20] </ref>, in which load migration is not performed. Instead, the main processor (shaded circle) prepares a set of 6 tasks and allocates them to each server whenever the server demands them.
Reference: [13] <author> Frank C. H. Lin and Robert M. Keller. </author> <title> The gradient model load balancing method. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. 13(1) </volume> <pages> 32-38, </pages> <month> January </month> <year> 1987. </year>
Reference-contexts: To our knowledge, migration topology for load balancing is considered for the first time. The topology has not been considered important heretofore because sometimes it is given in a hard-wired form <ref> [13] </ref> or it is meaningless where distributed load patterns cannot be assumed to be known 19 in advance [7, 11]. More interesting topologies can be studied in the future.
Reference: [14] <author> Bruce Martin, Charles Bergan, and Brian Russ. PARPC: </author> <title> A system for parallel remote procedure calls. </title> <booktitle> In Proceedings of the International Conferences on Parallel Processing, </booktitle> <pages> pages 449-452, </pages> <year> 1987. </year>
Reference-contexts: We assume such a distributed parallel application is written to the RPC (Remote Procedure Call) paradigm. Normally, RPC does not consider the case of multiple servers for the same function | except for some variations like PARPC <ref> [14] </ref> and MultiRPC [16]. Our work, called CORD (Configuration-level Optimization of RPC-based Distributed programs), is a framework for automatically generating all necessary executables from RPC-based distributed programs according to a configuration-level description intended for high performance [10].
Reference: [15] <author> C. D. Polychronopoulos and D. J. Kuck. </author> <title> Guided self-scheduling: A practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computer, </journal> <volume> Vol. C-36(12):1425-1439, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [19, 12, 15, 20, 5] </ref>. If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. <p> self-scheduling (GSS), the fixed chunk function (K) is replaced with a non-linearly decreasing chunk function in order to reduce the overhead at the beginning of a loop by allocating larger chunks, and also to reduce the chance of load imbalancing at the end of the loop by allocating smaller chunks <ref> [15] </ref>. Trapezoid self-scheduling (TSS) uses a linearly decreasing chunk function, which helps to reduce scheduling overhead while still maintaining a reasonable balance [20]. Recently, networks of workstations have emerged as viable candidates for running parallel applications. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [12, 15, 20] </ref>, although the pure SS scheme is orthogonal to the loop patterns. 1.1 Programming Environment Networks of workstations have by nature easy-to-change configurations; programs must be adapted accordingly whenever the hardware configuration has been changed. <p> Finally, as in Fig. 1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [15, 20, 5] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable. <p> Of course, the above three loops are all predictable. 2.2 Workstation Cluster Model for Load Balancing Fig. 2 shows two representative topologies in the workstation cluster model for parallel loops. Fig. 2 (a) represents the topology of traditional loop scheduling methods <ref> [19, 12, 15, 20] </ref>, in which load migration is not performed. Instead, the main processor (shaded circle) prepares a set of 6 tasks and allocates them to each server whenever the server demands them.
Reference: [16] <author> M. Satyanarayanan and E. H. Siegel. MultiRPC: </author> <title> A parallel remote procedure call mechanism. </title> <type> Technical Report CMU-CS-86-139, </type> <institution> Carnegie-Mellon University, </institution> <year> 1986. </year>
Reference-contexts: We assume such a distributed parallel application is written to the RPC (Remote Procedure Call) paradigm. Normally, RPC does not consider the case of multiple servers for the same function | except for some variations like PARPC [14] and MultiRPC <ref> [16] </ref>. Our work, called CORD (Configuration-level Optimization of RPC-based Distributed programs), is a framework for automatically generating all necessary executables from RPC-based distributed programs according to a configuration-level description intended for high performance [10].
Reference: [17] <author> B. K. Schmidt and V. S. Sunderam. </author> <title> Empirical analysis of overheads in cluster environments. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 6(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1994. </year>
Reference-contexts: The empirical values for ff and fi under the PVM system [18] at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi <ref> [17] </ref>. In Theorems 4.1 and 4.2, we compute the total number of migrated tasks (fi's multiplier) and the frequencies of migrations (ff's multiplier) in a cluster. Furthermore, we also illustrate an important characteristics of our method, which is that balance ratio gets improved as clustering happens at higher levels.
Reference: [18] <author> V. S. Sunderam. </author> <title> PVM: A framework for parallel distributed computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> Vol. 2(4) </volume> <pages> 315-339, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: T comm = ff + fim, where ff is startup time and fi is transfer time per byte [2]. The empirical values for ff and fi under the PVM system <ref> [18] </ref> at LAN-based clustered workstations are 4:527 msec, 0:0024 msec and 1:661 msec, 0:00157 msec for datagram and stream transmission cases, respectively, which imply ff fi [17].
Reference: [19] <author> P. Tang and P. C. Yew. </author> <title> Processor self-scheduling for multiple nested parallel loops. </title> <booktitle> In Proceedings of '86 International Conference on Parallel Processing, </booktitle> <pages> pages 528-535, </pages> <month> August </month> <year> 1986. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [19, 12, 15, 20, 5] </ref>. If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. <p> If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. Self-scheduling (SS) <ref> [19] </ref> is the simplest dynamic solution. It assigns a new iteration to a processor only when the processor becomes available. However, this method requires tremendous synchronization overhead; to be practical, hardware support to fast barrier synchronization primitives is desirable. <p> Of course, the above three loops are all predictable. 2.2 Workstation Cluster Model for Load Balancing Fig. 2 shows two representative topologies in the workstation cluster model for parallel loops. Fig. 2 (a) represents the topology of traditional loop scheduling methods <ref> [19, 12, 15, 20] </ref>, in which load migration is not performed. Instead, the main processor (shaded circle) prepares a set of 6 tasks and allocates them to each server whenever the server demands them.
Reference: [20] <author> T. H. Tzen and L. M. Ni. </author> <title> Dynamic loop scheduling for shared-memory multiprocessors. </title> <booktitle> In Proceedings of '91 International Conference on Parallel Processing, </booktitle> <pages> pages II:247-250, </pages> <month> August </month> <year> 1991. </year>
Reference-contexts: A loop is called a parallel loop (DOALL-loop) if there are no data dependences among all iterations. The question of how to allocate an iteration to a particular processor for minimizing the total execution time is known as a loop scheduling problem <ref> [19, 12, 15, 20, 5] </ref>. If there are I uniformly distributed iterations, and P identical processors, load can be balanced simply by assigning I=P iterations to each processor. Since both factors may not be known in advance or may vary substantially, such a static method is often difficult or inefficient. <p> Trapezoid self-scheduling (TSS) uses a linearly decreasing chunk function, which helps to reduce scheduling overhead while still maintaining a reasonable balance <ref> [20] </ref>. Recently, networks of workstations have emerged as viable candidates for running parallel applications. To our knowledge, the first work on parallel loop scheduling in a network of heterogeneous workstations was done by Cierniak et al. [5]. <p> The unpredictable patterns can even be detrimental to those improvements <ref> [12, 15, 20] </ref>, although the pure SS scheme is orthogonal to the loop patterns. 1.1 Programming Environment Networks of workstations have by nature easy-to-change configurations; programs must be adapted accordingly whenever the hardware configuration has been changed. <p> Finally, as in Fig. 1 (d), the workload may be quite irregular. Many non-scientific applications carry parallel loops of this type. The first three cases have been specially considered by conventional loop scheduling methods <ref> [15, 20, 5] </ref> in order to improve on the basic self-scheduling method. Particularly for irregular loops, we can distinguish between the two cases: predictable vs. unpredictable. <p> Of course, the above three loops are all predictable. 2.2 Workstation Cluster Model for Load Balancing Fig. 2 shows two representative topologies in the workstation cluster model for parallel loops. Fig. 2 (a) represents the topology of traditional loop scheduling methods <ref> [19, 12, 15, 20] </ref>, in which load migration is not performed. Instead, the main processor (shaded circle) prepares a set of 6 tasks and allocates them to each server whenever the server demands them.
References-found: 20

