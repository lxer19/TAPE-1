URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1993/GIT-CC-93-63.ps.Z
Refering-URL: http://www.cs.gatech.edu/tech_reports/index.93.html
Root-URL: 
Email: e-mail: rama@cc.gatech.edu  
Phone: Phone: (404) 894-5136 Fax: (404) 894-9442  
Title: Machine Abstractions and Locality Issues in Studying Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Keyword: Key words: Parallel Systems, Machine Abstractions, Locality, Execution-driven Simulation, Application driven Studies  
Note: This work has been funded in part by NSF grants MIPS-9058430 and MIPS-9200005, and an equipment grant from DEC.  
Address: Atlanta, Ga 30332-0280  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: Technical Report GIT-CC-93/63 October 1993 Abstract We define a set of overhead functions that capture the salient artifacts representing the interaction between parallel application characteristics and architectural features. An execution-driven simulation testbed is used to separate these overheads in a parallel system. Using this testbed and a set of applications, we address two important issues. The first concerns the use of machine abstractions for performance studies of parallel systems. The second deals with quantifying the impact of locality on the performance of applications. The key conclusions from this study are that the newly proposed model LogP is an effective one for abstracting the network, and that ignoring locality can significantly affect the application performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: For instance, some simulators [9, 11, 21] (including ours) have abstracted out the instruction-set of the processors since a detailed simulation of the instruction-set is not likely to contribute significantly to the performance analysis of parallel systems. Similarly, Agarwal <ref> [1] </ref> develops mathematical models for abstracting the network and studying network properties. Theoretical models such as the PRAM [15], and LogP [12] are abstractions of parallel machines used for algorithm development and analysis.
Reference: [2] <author> Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum. </author> <title> The MIT Alewife machine : A large scale Distributed-Memory Multiprocessor. In Scalable shared memory multiprocessors. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The performance impact of locality is determined by the interaction between the application characteristic and the hardware facility. However there have not been many attempts in quantifying this impact on application performance. Architectural studies <ref> [5, 2] </ref> have explored hardware facilities that would help exploit locality in applications. Gupta, et al. [22] have investigated the appropriate memory and cache sizes in a 7 parallel system based on the characteristics (such as the working set) of a set of applications.
Reference: [3] <author> Alok Aggarwal, Ashok K. Chandra, and Marc Snir. </author> <title> On Communication Latency in PRAM Computations. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 11-21, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency <ref> [3] </ref>, memory access conflicts [4], and cost of synchronization [16, 10]. Valiant's Bulk Synchronous Parallel (BSP) model [25] is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice.
Reference: [4] <author> H. Alt, T. Hagerup, K. Mehlhorn, and F. P. Preparata. </author> <title> Deterministic Simulation of Idealized Parallel Computers on More Realistic Ones. </title> <journal> SIAM Journal of Computing, </journal> <volume> 16(5) </volume> <pages> 808-835, </pages> <year> 1987. </year> <month> 20 </month>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [3], memory access conflicts <ref> [4] </ref>, and cost of synchronization [16, 10]. Valiant's Bulk Synchronous Parallel (BSP) model [25] is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice.
Reference: [5] <author> Robert Alverson, David Callahan, Daniel Cummings, Brian Koblenz, Allan Porterfield, and Burton Smith. </author> <title> The Tera Computer System. </title> <booktitle> In Proceedings of the ACM 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <address> Amsterdam, Netherlands, </address> <year> 1990. </year>
Reference-contexts: The performance impact of locality is determined by the interaction between the application characteristic and the hardware facility. However there have not been many attempts in quantifying this impact on application performance. Architectural studies <ref> [5, 2] </ref> have explored hardware facilities that would help exploit locality in applications. Gupta, et al. [22] have investigated the appropriate memory and cache sizes in a 7 parallel system based on the characteristics (such as the working set) of a set of applications.
Reference: [6] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to achieving Large Scale Computing Capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Parallel system overheads may be broadly classified into a purely algorithmic component (algorithmic overhead), and a component arising due to the interaction of the algorithm and the architecture (interaction overhead). The algorithmic overhead is due to the inherent serial part <ref> [6] </ref> and the work-imbalance in the algorithm, and is independent of the architectural 1 The term, parallel system, is used to denote an algorithm-architecture combination. characteristics. Work-imbalance could result if in a parallel phase, there is a differential amount of work done in the different threads.
Reference: [7] <author> Thomas E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: An application programmer may further define sub-modes if necessary. * BARRIER: Phase corresponding to a barrier synchronization operation. * MUTEX: Even though the simulated hardware provides only a test&set operation, mutual exclusion lock (implemented using test-test&set <ref> [7] </ref>) is available as a library function in SPASM. A program enters this mode during lock operations. With this mechanism, we can separate the overheads due to the synchronization operations from the rest of the program execution. * PGM SYNC : Parallel programs may use Signal-Wait semantics for pairwise synchronization. <p> Each processor uses this partial sum in calculating the partial sums for the chunk of global buckets allotted to it (phase 5) which is again a local operation. At the completion of this phase, a processor sets a lock (test-test&set lock <ref> [7] </ref>) 11 for each global bucket, subtracts the value found in the corresponding local bucket, updates the local bucket with this new value in the global bucket, and unlocks the bucket (phase 7). <p> Similarly, a MUTEX mode in an application which uses the test-test&set primitive <ref> [7] </ref>, would behave like an ordinary test&set operation in the LogP execution thus resulting in an increase of network accesses.
Reference: [8] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: Three of them (EP, IS and CG) are from the NAS parallel benchmark suite <ref> [8] </ref>; CHOLESKY is from the SPLASH benchmark suite [23]; and FFT is the well-known Fast Fourier Transform algorithm.
Reference: [9] <author> Eric A. Brewer, Chrysanthos N. Dellarocas, Adrian Colbrook, and William E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Similarly, developing algorithms for parallel architectures is also hard if one has to grapple with all these aspects. There have been several attempts to abstract artifacts of a parallel machine both from the point of view of performance analysis as well as program development. For instance, some simulators <ref> [9, 11, 21] </ref> (including ours) have abstracted out the instruction-set of the processors since a detailed simulation of the instruction-set is not likely to contribute significantly to the performance analysis of parallel systems. Similarly, Agarwal [1] develops mathematical models for abstracting the network and studying network properties.
Reference: [10] <author> Richard Cole and Ofer Zajicek. </author> <title> The APRAM: Incorporating Asynchrony into the PRAM Model. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 169-178, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [3], memory access conflicts [4], and cost of synchronization <ref> [16, 10] </ref>. Valiant's Bulk Synchronous Parallel (BSP) model [25] is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice.
Reference: [11] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Similarly, developing algorithms for parallel architectures is also hard if one has to grapple with all these aspects. There have been several attempts to abstract artifacts of a parallel machine both from the point of view of performance analysis as well as program development. For instance, some simulators <ref> [9, 11, 21] </ref> (including ours) have abstracted out the instruction-set of the processors since a detailed simulation of the instruction-set is not likely to contribute significantly to the performance analysis of parallel systems. Similarly, Agarwal [1] develops mathematical models for abstracting the network and studying network properties.
Reference: [12] <author> David Culler et al. </author> <title> LogP : Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Using this framework we explore two important questions: (a) can machine abstractions be used in an execution-driven simulator for the performance studies of parallel systems?, and (b) can locality be ignored in the performance prediction or algorithm development without significantly affecting the validity of the results? We show that LogP <ref> [12] </ref> is an effective model for abstracting the network, and that ignoring locality can significantly affect the application performance. <p> Similarly, Agarwal [1] develops mathematical models for abstracting the network and studying network properties. Theoretical models such as the PRAM [15], and LogP <ref> [12] </ref> are abstractions of parallel machines used for algorithm development and analysis. There is a growing awareness for studying parallel systems with real applications due to the dynamic nature of the interaction between applications and architectures. Execution-driven simulation is an important technique for enabling such studies. <p> Valiant's Bulk Synchronous Parallel (BSP) model [25] is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice. The LogP model recently developed by Culler et al. <ref> [12] </ref>, inspired by BSP, improves on the BSP model and is considered a more realistic one since it incorporates the two important properties of any network, namely, latency and contention. Therefore, we use this model in our work to abstract the network. <p> The application is implemented in 3 main phases. In phases 1 and 5, processors perform the radix-2 butterfly computation on N=P local points. Phase 3 is the only communication phase in which the cyclic layout of data is changed to a blocked layout as described in <ref> [12] </ref>. It involves an all-to-all communication step where each processor distributes its local data equally among the P processors. <p> Thus the LogP characterization is equivalent to an ordinary NUMA machine such as the BBN Butterfly GP1000. The L parameter for a message is chosen to be 1.6 microseconds assuming 32-byte messages. Similar to the method used in <ref> [12] </ref>, the g parameter is calculated using the cross-section bandwidth available per processor for each of the above network configurations. <p> The difference that can be observed in the absolute values can be explained as follows. The g-parameter in cLogP is estimated using the bisection bandwidth of the network as suggested in <ref> [12] </ref>, leading to a pessimistic estimate because of not accounting for communication locality. This pessimism increases as the diameter of the network increases as can be seen in Figures 16, 17, and 18. <p> Our framework using the overhead functions has been useful in validating the LogP model. One can experimentally determine the accuracy of the performance predicted by the LogP model as is done in <ref> [12] </ref> using CM-5. However, this approach does not validate the individual parameters abstracted using the model. Using the overhead functions in the framework of an execution-driven allows validation of the individual parameters. simulator addresses both of these deficiencies.
Reference: [13] <author> Zarka Cvetanovic. </author> <title> The effects of problem partitioning, allocation, and granularity on the performance of multiple-processor systems. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 36(4) </volume> <pages> 421-432, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [18, 26, 13] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic [13] develop models for regular iterative algorithms. <p> For instance, models developed in [18, 26, 13] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic <ref> [13] </ref> develop models for regular iterative algorithms. However, there exist several applications [22] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication.
Reference: [14] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year> <month> 21 </month>
Reference-contexts: A similar approach has also been used by other researchers in studying the impact of application characteristics on architectural requirements 2 We do not distinguish between the terms, process, processor and thread, and use them synonymously in this paper. 3 <ref> [14, 22] </ref>. 3.1 Approaches for Measuring Overheads Experimentation, simulation, and analytical models are techniques that can be used for measuring overheads. But each has its own limitations.
Reference: [15] <author> S. Fortune and J. Wyllie. </author> <title> Parallelism in random access machines. </title> <booktitle> In Proceedings of the 10th Annual Symposium on Theory of Computing, </booktitle> <pages> pages 114-118, </pages> <year> 1978. </year>
Reference-contexts: Similarly, Agarwal [1] develops mathematical models for abstracting the network and studying network properties. Theoretical models such as the PRAM <ref> [15] </ref>, and LogP [12] are abstractions of parallel machines used for algorithm development and analysis. There is a growing awareness for studying parallel systems with real applications due to the dynamic nature of the interaction between applications and architectures. Execution-driven simulation is an important technique for enabling such studies.
Reference: [16] <author> Phillip B. Gibbons. </author> <title> A More Practical PRAM Model. </title> <booktitle> In Proceedings of the First Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 158-168, </pages> <year> 1989. </year>
Reference-contexts: The PRAM model assumes conflict-free accesses to shared memory (assigning unit cost for memory accesses) and zero cost for synchronization. The PRAM model has been augmented with additional parameters to account for memory access latency [3], memory access conflicts [4], and cost of synchronization <ref> [16, 10] </ref>. Valiant's Bulk Synchronous Parallel (BSP) model [25] is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice.
Reference: [17] <institution> Intel Corporation, Oregon. Intel iPSC/2 and iPSC/860 User's Guide, </institution> <year> 1989. </year>
Reference-contexts: Otherwise, the number of columns is twice the number of rows (we restrict the number of processors to a power of 2 in this study). Messages are circuit-switched and 14 use a transmission scheme similar to the one used on the Intel iPSC/860 <ref> [17] </ref>. A circuit is set up between the source and the destination, and the message is then sent in a single packet. Message-sizes can vary upto 32 bytes. We assume that the switching time for setting up a circuit (in a contention free scenario) is negligible.
Reference: [18] <author> Sridhar Madala and James B. Sinclair. </author> <title> Performance of Synchronous Parallel Algorithms with Regular Structures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(1) </volume> <pages> 105-116, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [18, 26, 13] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic [13] develop models for regular iterative algorithms. <p> These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in [18, 26, 13] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair <ref> [18] </ref> confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic [13] develop models for regular iterative algorithms. However, there exist several applications [22] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.
Reference: [19] <author> Janak H. Patel. </author> <title> Analysis of multiprocessors with private cache memories. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 31(4) </volume> <pages> 296-304, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models. Similarly, several other models make assumptions about architectural characteristics. For instance, the model developed in <ref> [19] </ref> ignores data inconsistency that can arise in a cache-based multiprocessor during the course of execution of an application. In this study we use execution-driven simulation for separating and quantifying the overhead functions.
Reference: [20] <author> U. Ramachandran, G. Shah, S. Ravikumar, and J. Muthukumarasamy. </author> <title> Scalability study of the KSR-1. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel Processing, </booktitle> <pages> pages I-237-240, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: A list of 64K integers with 2K buckets is chosen for this study. An implementation of the algorithm is described in <ref> [20] </ref> and Table 2 summarizes its characteristics. The input list is equally partitioned among the processors. Each processor maintains two sets of buckets. One set of buckets (of size 2K) is used to maintain the information for the portion of the list local to it.
Reference: [21] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Similarly, developing algorithms for parallel architectures is also hard if one has to grapple with all these aspects. There have been several attempts to abstract artifacts of a parallel machine both from the point of view of performance analysis as well as program development. For instance, some simulators <ref> [9, 11, 21] </ref> (including ours) have abstracted out the instruction-set of the processors since a detailed simulation of the instruction-set is not likely to contribute significantly to the performance analysis of parallel systems. Similarly, Agarwal [1] develops mathematical models for abstracting the network and studying network properties.
Reference: [22] <author> Edward Rothberg, Jaswinder Pal Singh, and Anoop Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A similar approach has also been used by other researchers in studying the impact of application characteristics on architectural requirements 2 We do not distinguish between the terms, process, processor and thread, and use them synonymously in this paper. 3 <ref> [14, 22] </ref>. 3.1 Approaches for Measuring Overheads Experimentation, simulation, and analytical models are techniques that can be used for measuring overheads. But each has its own limitations. <p> Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic [13] develop models for regular iterative algorithms. However, there exist several applications <ref> [22] </ref> with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters. Further, an application may be structured to hide a particular overhead such as latency by overlapping computation with communication. It may be difficult to capture such dynamic program behavior using analytical models. <p> In this study we use execution-driven simulation for separating and quantifying the overhead functions. Several recent studies have also stressed the importance of using simulation with realistic workloads as the appropriate method of studying parallel systems <ref> [22] </ref>. However, simulation also has its limitations. For example, it may not always be possible to simulate large systems owing to resource (time and space) constraints in using architectural simulators. <p> However there have not been many attempts in quantifying this impact on application performance. Architectural studies [5, 2] have explored hardware facilities that would help exploit locality in applications. Gupta, et al. <ref> [22] </ref> have investigated the appropriate memory and cache sizes in a 7 parallel system based on the characteristics (such as the working set) of a set of applications. <p> In relation to the locality question, we confine ourselves to shared memory style parallel programs. Locality in such programs manifests itself implicitly through the memory access pattern for shared data. Most modern shared memory multiprocessors have sufficiently large-size caches, capable of holding the working set of most parallel applications <ref> [22] </ref>. Therefore, we use a LogP machine augmented with a cache, referred to as a cLogP machine, in our simulator to answer the two questions. Shared memory machines with private caches usually employ a protocol to maintain cache coherence.
Reference: [23] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: Three of them (EP, IS and CG) are from the NAS parallel benchmark suite [8]; CHOLESKY is from the SPLASH benchmark suite <ref> [23] </ref>; and FFT is the well-known Fast Fourier Transform algorithm. The characteristics include the data access pattern, the synchronization pattern, the communication pattern, the computation granularity (the amount of work done) and data granularity (the amount of data communicated) for each phase of the 9 program. <p> The sparse nature of the input matrix results in an algorithm with a data dependent dynamic access pattern. The algorithm requires an initial symbolic factorization of the input matrix which is done sequentially because it requires only a small fraction of the total compute time. Only numerical factorization <ref> [23] </ref> is parallelized and analyzed. Sets of columns having similar non-zero structure are combined into supernodes at the end of symbolic factorization. Processors get tasks from a central task queue. Each supernode is a potential task which is used to modify subsequent supernodes.
Reference: [24] <author> Anand Sivasubramaniam, Aman Singla, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <type> Technical Report GIT-CC-93/27, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: Further, simulation can also be used to validate existing 4 models using real applications. 3.2 SPASM SPASM (Simulator for Parallel Architectural Scalability Measurements) is an execution-driven simulator written in CSIM that we have developed for measuring the overhead functions. The reader is referred to <ref> [24] </ref> for a detailed description of the implementation of SPASM. <p> This mode is used to differentiate such accesses from normal load/store accesses. Thus the metrics identified by SPASM quantify the algorithmic overhead and the interesting components of the interaction overhead. 4 Using the Framework There are several applications for the framework described in this paper. In <ref> [24] </ref>, we use it to study the scalability of parallel systems.
Reference: [25] <author> Leslie G. Valiant. </author> <title> A Bridging Model for Parallel Computation. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 103-111, </pages> <month> August </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: The PRAM model has been augmented with additional parameters to account for memory access latency [3], memory access conflicts [4], and cost of synchronization [16, 10]. Valiant's Bulk Synchronous Parallel (BSP) model <ref> [25] </ref> is a departure from the PRAM models, and is one of the first attempt to realistically bridge the gap between theory and practice.
Reference: [26] <author> D. F. Vrsalovic, D. P. Siewiorek, Z. Z. Segall, and E. Gehringer. </author> <title> Performance Prediction and Cali--bration for a Class of Multiprocessors. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> 37(11) </volume> <pages> 1353-1365, </pages> <month> November </month> <year> 1988. </year>
Reference-contexts: In general, such models tend to make simplistic assumptions about program behavior and architectural characteristics to make the analysis using the model tractable. These assumptions restrict their applicability for capturing complex interactions between algorithms and architectures. For instance, models developed in <ref> [18, 26, 13] </ref> are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. [26] and Cvetanovic [13] develop models for regular iterative algorithms. <p> For instance, models developed in [18, 26, 13] are mainly applicable to algorithms with regular communication structures that can be predetermined before the execution of the algorithm. Madala and Sinclair [18] confine their studies to synchronous algorithms while Vrsalovic et al. <ref> [26] </ref> and Cvetanovic [13] develop models for regular iterative algorithms. However, there exist several applications [22] with irregular data access, communication, and synchronization characteristics which cannot always be captured by such simple parameters.
Reference: [27] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1979. </year> <title> 23 Execution Time Graphs 24 Execution Time Graphs (contd) 25 Latency Graphs 26 Contention Graphs 27 Contention Graphs (contd) 28 Contention Graphs (contd) Full : Contention 29 </title>
Reference-contexts: Algorithmic overhead is the difference between the linear curve and that which would be obtained (the ideal curve in Figure 1) by executing the algorithm on an ideal machine such as PRAM (Parallel Random Access Machine) <ref> [27] </ref>. Such a machine idealizes the parallel architecture by assuming an infinite number of processors, and unit costs for communication and synchronization. A real execution could deviate significantly from the ideal execution due to the overheads such as latency, contention, synchronization, scheduling and cache effects. <p> We associate an overhead function with each such overhead that results in non-ideal behavior of the system. The algorithmic overhead is quantified by computing the time taken for execution of a given parallel program on an ideal machine such as the PRAM <ref> [27] </ref> and measuring its deviation from a linear speedup curve. Further, we separate this overhead into that due to the serial part (serial overhead) and that due to work imbalance (work-imbalance overhead).
References-found: 27

