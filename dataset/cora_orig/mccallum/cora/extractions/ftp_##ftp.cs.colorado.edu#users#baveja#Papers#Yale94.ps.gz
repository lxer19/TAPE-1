URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Yale94.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: sutton@gte.com  singh@psyche.mit.edu  
Title: On Step-Size and Bias in Temporal-Difference Learning  
Author: Richard S. Sutton Satinder P. Singh 
Address: Cambridge, MA 02139  
Affiliation: Brain Cognitive Sciences Dept. (E10) Massachusetts Institute of Technology  
Note: As appeared in the Proceedings of the Eighth Yale Workshop on Adaptive and Learning Systems, pp. 91-96. Center for Systems Science, Yale University, 1994.  
Abstract: We present results for three new algorithms for setting the step-size parameters, ff and , of temporal-difference learning methods such as TD(). The overall task is that of learning to predict the outcome of an unknown Markov chain based on repeated observations of its state trajectories. The new algorithms select step-size parameters online in such a way as to eliminate the bias normally inherent in temporal-difference methods. We compare our algorithms with conventional Monte Carlo methods. Monte Carlo methods have a natural way of setting the step size: for each state s they use a step size of 1=n s , where n s is the number of times state s has been visited. We seek and come close to achieving comparable step-size algorithms for TD(). One new algorithm uses a = 1=n s schedule to achieve the same effect as processing a state backwards with TD(0), but remains completely incremental. Another algorithm uses a at each time equal to the estimated transition probability of the current transition. We present empirical results showing improvement in convergence rate over Monte Carlo methods and conventional TD(). A limitation of our results at present is that they apply only to tasks whose state trajectories do not contain cycles. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A., Duff, M. </author> <title> (1994) Monte Carlo Matrix Inversion and Reinforcement Learning. </title> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle>
Reference-contexts: We will rule out this certainty equivalent (CE) solution because of its prodigious requirements for memory and computation. 3 The conventional Monte Carlo (MC) predictions <ref> (e.g., see Barto & Duff, 1994) </ref> are the average of the actual outcomes that have followed visits to the state: V s = k2S s z k (2) where s is a nonterminal state, S s is the set of all previously observed sequences that contain s, and jS s j
Reference: <author> Dayan, P. </author> <title> (1992) The Convergence of TD() for General . Machine Learning 8, </title> <type> 341-362. </type>
Reference: <author> Klopf, A.H. </author> <title> (1982) The Hedonistic Neuron: A Theory of Memory, Learning, </title> <booktitle> and Intelligence, </booktitle> <address> Washing-ton DC: </address> <publisher> Hemisphere/Harper & Row. </publisher>
Reference: <author> Sutton, </author> <title> R.S. (1988) Learning to Predict by the Methods of Temporal Differences. </title> <booktitle> Machine Learning 3, </booktitle> <pages> 9-44. </pages>
Reference-contexts: 1 Introduction The defining feature of temporal-difference (TD) learning <ref> (Sutton, 1988) </ref> is that it involves learning predictions on the basis of other predictions. Suppose you are using observations of an absorbing Markov process to predict where it will absorb as a function of its current state. <p> If this update rule is followed, then (2) will hold at each step. Thus, the estimates will be unbiased and their variance will decrease in the usual way according to the law of large numbers. We call this algorithm the Monte Carlo (MC) rule. It is well known <ref> (Sutton, 1988) </ref> that TD (1), the TD () algorithm with = 1, can achieve an overall effect similar to that of an error correction rule such as (3).
Reference: <author> Watkins, C.J.C.H. </author> <title> (1989) Learning From Delayed Rewards. </title> <type> Cambridge University PhD Thesis (Psychology). </type>
References-found: 5

