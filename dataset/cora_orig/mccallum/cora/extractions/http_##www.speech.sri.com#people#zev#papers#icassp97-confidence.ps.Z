URL: http://www.speech.sri.com/people/zev/papers/icassp97-confidence.ps.Z
Refering-URL: http://www.speech.sri.com/people/zev/publications.html
Root-URL: 
Email: e-mail: mw,francois,zev,konig,stolcke@speech.sri.com  
Title: NEURAL NETWORK BASED MEASURES OF CONFIDENCE FOR WORD RECOGNITION  
Author: Mitch Weintraub, Fran~coise Beaufays, Ze'ev Rivlin, Yochai Konig, Andreas Stolcke 
Address: Menlo Park, CA.  
Affiliation: Speech Technology and Research Laboratory SRI International,  
Abstract: This paper proposes a probabilistic framework to define and evaluate confidence measures for word recognition. We describe a novel method to combine different knowledge sources and estimate the confidence in a word hypothesis, via a neural network. We also propose a measure of the joint performance of the recognition and confidence systems. The definitions and algorithms are illustrated with results on the Switchboard Corpus. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.J. Godfrey and E. C. Holliman and J. McDaniel, </author> <title> "SWITCHBOARD: Telephone Speech Corpus for Research and Development", </title> <journal> ICASSP'92, </journal> <volume> vol. I, </volume> <pages> pp. 517-520. </pages>
Reference-contexts: The definitions and algorithms proposed in the paper are illustrated with experiments on the Switchboard Corpus <ref> [1] </ref>. 2. WORD CORRECTNESS Given a reference and a hypothesis word string, each word in the hypothesis can be labeled as correct or incorrect. Many applications (e.g. speaker identification, adaptation of acoustic models, ...) require a definition of word correctness that involves time-alignments. We provide such a definition.
Reference: [2] <author> E.A.Wan, </author> <title> "Neural Network Classification: A Bayesian Interpretataion", </title> <journal> IEEE Trans. Neural Networks, </journal> <volume> vol. 1, no. 4, </volume> <pages> pp. 303-305. </pages>
Reference-contexts: Under these conditions, and assuming that the net is trained to minimize the MSE or maximize the CREP, the output of the net estimates the confidence in the word hypothesis (according to our definition) <ref> [2, 3] </ref>. 6.1. Features The performance of the neural network clearly depends on the quality of the features used as knowledge sources. In our experiments, we implemented 13 features. These are by no means exhaustive. 6.1.1.
Reference: [3] <author> M. D. Richard and R. P. Lippmann, </author> <title> "Neural Network Classifiers Estimate Bayesian a posteriori Probabilities", </title> <journal> Neural Computation, </journal> <volume> vol. 3, no. 4, </volume> <pages> pp. 461-483, </pages> <year> 1991. </year>
Reference-contexts: Under these conditions, and assuming that the net is trained to minimize the MSE or maximize the CREP, the output of the net estimates the confidence in the word hypothesis (according to our definition) <ref> [2, 3] </ref>. 6.1. Features The performance of the neural network clearly depends on the quality of the features used as knowledge sources. In our experiments, we implemented 13 features. These are by no means exhaustive. 6.1.1.
Reference: [4] <author> Z. Rivlin, M. Cohen, V. Abrash, Th. Chung, </author> <title> "A Phone-Dependent Confidence Measure for Utterance Rejection", </title> <journal> it ICASSP'96, </journal> <volume> vol. I, </volume> <pages> pp. 515-519. </pages>
Reference-contexts: Acoustic Features The acoustic features we implemented measure the normalized log-likelihood (LL) of the acoustic realization of each word. They differ by the models used to normalize the LLs and by the way the frame-level LLs are combined. Normalization was done either with context-independent HMMs (CI HMMs) <ref> [4] </ref> or with a Gaussian mixture model (GMM). The frame-level LLs were combined at the word level, phone level, or phone-state level. This gives six possible features of which five were implemented.
Reference: [5] <author> R. C. Rose and D. B. Paul, </author> <title> "A Hidden Markov Model Based Keyword Recognition System", </title> <booktitle> ICASSP'90, </booktitle> <pages> pp. 129-132. </pages>
Reference: [6] <author> R. A. Sukkar and J. G. Wilpon, </author> <title> "A Two Pass Classifier Utterance Rejection in Keyword Spotting", </title> <journal> ICASSP'93, </journal> <volume> vol. II, </volume> <pages> pp. 451-454. </pages>
Reference: [7] <author> H. Gish and K. Ng, </author> <title> "A Segmental Speech Model with Applications to Word Spotting", </title> <journal> ICASSP'93, </journal> <volume> vol. II, </volume> <pages> pp. 447-450. </pages>
Reference: [8] <author> Sh. R. Young, </author> <title> "Detecting Misrecognitions and Out-of-Vocabulary Words", </title> <journal> ICASSP'94, </journal> <volume> vol. II, </volume> <pages> pp. 21-24. </pages>
Reference: [9] <author> E. Eide, H. Gish, Ph. Jeanrenaud, A. Mielke, </author> <title> "Understanding and Improving Speech Recognition Performance Through the Use of Diagnostic Tools", </title> <journal> vol. </journal> <volume> I, </volume> <pages> pp. 221-224. </pages>
Reference: [10] <author> M. Weintraub, </author> <title> "LVCSR Log-Likelihood Ratio Scoring for Keyword Spotting", </title> <journal> ICASSP'95, </journal> <volume> vol. I, </volume> <pages> pp. 297-300. </pages>
Reference-contexts: This generated three features. The first one is defined below; the other two are similar. log P (WjX; LM) = log HYPs j W 2 HYP P (HYPjX; LM) P all HYPs P (HYPjX; LM) ; where X is the acoustic realization of the word W <ref> [10] </ref>. 6.1.4. Other Features We also used the number of phones in the word as a feature, with the intuition that shorter words tend to be more often incorrect than longer words. 7.
Reference: [11] <author> E. Lleida and R.C.Rose, </author> <title> "Efficient Decoding and Training Procedures for Utterance Verification in Continuous Speech Recognition", </title> <journal> ICASSP'96, </journal> <volume> vol. 1, </volume> <pages> pp. 507-510. </pages>
References-found: 11

