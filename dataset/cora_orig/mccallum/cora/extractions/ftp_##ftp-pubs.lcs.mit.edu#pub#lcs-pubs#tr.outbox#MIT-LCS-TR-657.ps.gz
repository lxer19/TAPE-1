URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-657.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr600.html
Root-URL: 
Title: Noise Tolerant Algorithms for Learning and Searching  
Author: by Javed Alexander Aslam Ronald L. Rivest 
Degree: Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of Doctor of Philosophy at the  Signature of Author  Certified by  Professor of Computer Science Thesis Supervisor Accepted by Frederic R. Morgenthaler Chairman, Departmental Committee on Graduate Students  
Note: c Massachusetts Institute of Technology  
Date: (1992)  (1987)  February 1995  1995  January 27, 1995  
Affiliation: S.M., Electrical Engineering and Computer Science Massachusetts Institute of Technology  B.S.E.E., Electrical and Computer Engineering University of Notre Dame  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dana Angluin. </author> <title> Computational learning theory: Survey and selected bibliography. </title> <booktitle> In Proceedings of the Twenty-Fourth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 351-369, </pages> <month> May </month> <year> 1992. </year>
Reference: [2] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference: [3] <author> Dana Angluin and Leslie G. Valiant. </author> <title> Fast probabilistic algorithms for Hamiltonian circuits and matchings. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 155-193, </pages> <month> April </month> <year> 1979. </year>
Reference: [4] <author> Martin Anthony and Norman Biggs. </author> <title> Computational Learning Theory. </title> <booktitle> Cambridge Tracts in Theoretical Computer Science (30). </booktitle> <publisher> Cambridge University Press, </publisher> <year> 1992. </year>
Reference: [5] <author> Javed A. Aslam and Aditi Dhagat. </author> <title> On-line algorithms for 2-coloring hypergraphs via chip games. </title> <journal> Theoretical Computer Science, </journal> <volume> 112(2) </volume> <pages> 355-369, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: A comparison of this work with the best known previous results can be found in Figures 7.1 and 7.2. Our results are obtained by looking at the search problem in the framework of chip games. These chip games have also proved useful in modeling a hypergraph 2-coloring problem <ref> [5] </ref>. In general, chip games model computational problems in such a way that winning strategies for the players translate into bounds on the critical resource. This critical resource is represented by some aspect of the chip game, such as number of chips used or number of moves in the game.
Reference: [6] <author> Avrim Blum, Merrick Furst, Jeffery Jackson, Michael Kearns, Yishay Mansour, and Steven Rudich. </author> <title> Weakly learning DNF and characterizing statistical query learning using fourier analysis. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, </booktitle> <year> 1994. </year> <note> To Appear. </note>
Reference: [7] <author> Anselm Blumer, Andrzej Ehrenfeucht, David Haussler, and Manfred K. Warmuth. </author> <title> Learn-ability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year> <note> 121 122 Bibliography </note>
Reference: [8] <author> Ryan S. Borgstrom and S. Rao Kosaraju. </author> <title> Comparison-based search in the presence of errors. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 130-136, </pages> <year> 1993. </year>
Reference-contexts: Two questions arise directly from this work: 1. In the linearly bounded error model, can we show a logarithmic upper bound on the number of comparison questions required when the error rate is between 1/3 and 1/2? Using techniques similar to ours, Borgstrom and Kosaraju <ref> [8] </ref> have recently shown that this is the case. 2.
Reference: [9] <author> Scott E. Decatur. </author> <title> Statistical queries and faulty PAC oracles. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference: [10] <author> Aditi Dhagat, Peter Gacs, and Peter Winkler. </author> <title> On playing twenty questions with a liar. </title> <booktitle> In Proceedings of the Third Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1992. </year>
Reference-contexts: Spencer and Winkler [32] have also examined this problem. They have arrived independently at one of the theorems in this paper using different proof techniques. Their paper as well as one by Dhagat, Gacs, and Winkler <ref> [10] </ref> considers another linearly bounded model of errors. We begin in Section 7.2 by developing the framework of chip games within which we solve the search problem.
Reference: [11] <author> Andrzej Ehrenfeucht, David Haussler, Michael Kearns, and Leslie Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <month> September </month> <year> 1989. </year>
Reference: [12] <author> U. Feige, D. Peleg, P. Raghavan, and E. Upfal. </author> <title> Computing with unreliable information. </title> <booktitle> In Proceedings of the Twenty-Second Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 128-137, </pages> <year> 1990. </year>
Reference-contexts: Finally, using standard Chernoff bound techniques, Feige et al. <ref> [12] </ref> showed that O (lg n) questions are sufficient for any p &lt; 1=2. Our contribution here is a formal reduction from the problem of searching in the probabilistic error model to that of searching in the linearly bounded error model. <p> n] 2 ) We thus bound the unknown number n by at most n 2 using O ([lg lg n] 2 ) (comparison) questions. 9.2.2 Stage 2 We can now simply apply the bounded searching techniques for membership questions described in previous section or the bounds of Feige et al. <ref> [12] </ref> for comparison questions. We can thus obtain the correct answer (with high probability) in an additional O (lg n 2 ) = O (lg n) comparison or membership questions.
Reference: [13] <author> Michael Frazier. </author> <title> Searching with a non-constant number of lies. </title> <type> Unpublished manuscript, </type> <year> 1990. </year>
Reference-contexts: Frazier <ref> [13] </ref> 1 The term lg n denotes log 2 n throughout this thesis. 2 Typically, the complexity of such algorithms depends on log (1=ffi), as does the complexity of our algorithm. 7.1 Introduction 93 Membership Questions Comparison Questions 0 &lt; r &lt; 1 1 2 0 &lt; r &lt; 1 1 <p> This bound comes from an analysis of a "brute-force" binary search, where each question of the search is asked enough times so that the correct answer can be determined by majority. A simple argument <ref> [13, 32] </ref> shows that the search problem cannot be solved (with either membership or comparison questions) if r 1=2. We show significantly improved bounds in the linearly bounded error model which hold for the entire range 0 &lt; r &lt; 1=2. <p> We first show an (n lg 1 12r ) lower bound for a "brute-force" strategy. Strategies similar to this "brute-force" method are given by Pelc [24] and Frazier <ref> [13] </ref>, and these were the best known results for 1=3 r &lt; 1=2 prior to this work. 8.1 A Brute-Force Strategy To determine an unknown number x 2 f1; : : : ; ng, a "brute-force" strategy simply performs a binary search, repeating each question enough times so that majority gives
Reference: [14] <author> Yoav Freund. </author> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 202-216. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [15] <author> Yoav Freund. </author> <title> An improved boosting algorithm and its implications on learning complexity. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 391-398. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference: [16] <author> Sally A. Goldman, Michael J. Kearns, and Robert E. Schapire. </author> <title> On the sample complexity of weak learning. </title> <booktitle> In Proceedings of COLT '90, </booktitle> <pages> pages 217-231. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference: [17] <author> David Helmbold, Robert Sloan, and Manfred K. Warmuth. </author> <title> Learning integer lattices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 21(2) </volume> <pages> 240-266, </pages> <year> 1992. </year>
Reference: [18] <author> W. Hoeffding. </author> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <volume> 58 </volume> <pages> 13-30, </pages> <year> 1963. </year> <note> Bibliography 123 </note>
Reference-contexts: If the correct chip never crosses the boundary line, then the linearly bounded error algorithm must return the correct chip when it terminates, and hence the correct answer will be obtained. Our analysis makes use of Hoeffding's Inequality <ref> [18] </ref> to approximate the tail of a binomial distribution. Let GE (p; m; n) be the probability of at least n successes in m Bernoulli trials, where each trial has probability of success p.
Reference: [19] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference: [20] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 267-280, </pages> <address> Chicago, Illinois, </address> <month> May </month> <year> 1988. </year>
Reference: [21] <author> Philip D. Laird. </author> <title> Learning from Good and Bad Data. </title> <booktitle> Kluwer international series in engineering and computer science. </booktitle> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1988. </year>
Reference: [22] <editor> F.J. MacWilliams and N.J.A. Sloane. </editor> <booktitle> The Theory of Error-Correcting Codes, </booktitle> <volume> volume 1, </volume> <pages> page 310. </pages> <publisher> North Holland Publishing Company, </publisher> <year> 1977. </year>
Reference-contexts: In order to determine c 1 , we make use of the following bound <ref> [22] </ref>: bkc X i 2 kH () where 0 &lt; &lt; 1=2 and H (r) is the binary entropy function. 1 We now have: brmc X i=0 n brmc X i=0 m n = n2 m (H (r)1) This last quantity is O (1) when m = lg n 1H (r)
Reference: [23] <author> Colin McDiarmid. </author> <title> On the method of bounded differences. </title> <editor> In J. Siemons, editor, </editor> <booktitle> Surveys in Combinatorics, </booktitle> <pages> pages 149-188. </pages> <publisher> Cambridge University Press, </publisher> <address> Cambridge, 1989. </address> <publisher> London Mathematical Society LNS 141. </publisher>
Reference: [24] <author> Andrzej Pelc. </author> <title> Searching wih known error probability. </title> <journal> Theoretical Computer Science, </journal> <volume> 63 </volume> <pages> 185-202, </pages> <year> 1989. </year>
Reference-contexts: Since ffi is previously known and fixed, we consider ffi a constant for the purpose of measuring the complexity of the searching algorithm. 2 Pelc <ref> [24] </ref> showed that in the probabilistic error model, with error probability p &lt; 1=2, O (lg 2 n) questions are sufficient to search in the bounded domain. <p> O (lg n) bound in the bounded domain. We also generalize this bound to the unbounded domain. 3 In the linearly bounded error model, Pelc <ref> [24] </ref> showed that x can be determined exactly in O (lg n) questions in both the bounded and unbounded domains. However, these bounds only hold for r &lt; 1=3. <p> We first show an (n lg 1 12r ) lower bound for a "brute-force" strategy. Strategies similar to this "brute-force" method are given by Pelc <ref> [24] </ref> and Frazier [13], and these were the best known results for 1=3 r &lt; 1=2 prior to this work. 8.1 A Brute-Force Strategy To determine an unknown number x 2 f1; : : : ; ng, a "brute-force" strategy simply performs a binary search, repeating each question enough times so
Reference: [25] <author> R. L. Rivest, A. R. Meyer, D. J. Kleitman, K. Winklmann, and J. Spencer. </author> <title> Coping with errors in binary search procedures. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 20 </volume> <pages> 396-404, </pages> <year> 1980. </year>
Reference-contexts: Bounded: x 2 f1; : : : ; ng, for some known n. Unbounded: x may be any positive integer. Much research has been devoted to the subject of searching in the presence of errors. Rivest et al. <ref> [25] </ref> have shown that in the bounded domain with at most k errors, x can be determined exactly with lg n + k lg lg n + O (k lg k) comparison questions. 1 Here k can be a function of n, but not of the number of questions asked.
Reference: [26] <author> Yasubumi Sakakibara. </author> <title> Algorithmic Learning of Formal Languages and Decision Trees. </title> <type> PhD thesis, </type> <institution> Tokyo Institute of Technology, </institution> <month> October </month> <year> 1991. </year> <institution> (International Institute for Advanced Study of Social Information Science, Fujitsu Laboratories Ltd, </institution> <note> Research Report IIAS-RR-91-22E). </note>
Reference: [27] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory Series A, </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference: [28] <author> Robert E. Schapire. </author> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5(2) </volume> <pages> 197-227, </pages> <year> 1990. </year>
Reference: [29] <author> Robert E. Schapire. </author> <title> The Design and Analysis of Efficient Learning Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year> <note> 124 Bibliography </note>
Reference: [30] <author> Hans Ulrich Simon. </author> <title> General bounds on the number of examples needed for learning probabilistic concepts. </title> <booktitle> In Proceedings of the Sixth Annual ACM Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference: [31] <author> Joel Spencer. </author> <title> Ten Lectures on the Probabilistic Method, </title> <booktitle> chapter 4, </booktitle> <pages> pages 32-35. </pages> <publisher> SIAM, </publisher> <year> 1987. </year>
Reference-contexts: Corresponding to the aforementioned error models, we consider both a probabilistic and an adversarial linearly bounded liar. The game against a linearly bounded liar can now be further reformulated as a Chip Game between two players: the Pusher and the Chooser. Pusher-Chooser games were first used by Spencer <ref> [31] </ref> to solve a different problem in his notes on the probabilistic method. The Chip Game starts with a unidimensional board marked in levels from 0 on upwards (see Figure 7.3). We start with n chips on level 0, each chip representing one number in f1; : : :; ng.
Reference: [32] <author> Joel Spencer and Peter Winkler. </author> <title> Three thresholds for a liar. Combinatorics, </title> <journal> Probability and Computing, </journal> <volume> 1 </volume> <pages> 81-93, </pages> <year> 1992. </year>
Reference-contexts: This bound comes from an analysis of a "brute-force" binary search, where each question of the search is asked enough times so that the correct answer can be determined by majority. A simple argument <ref> [13, 32] </ref> shows that the search problem cannot be solved (with either membership or comparison questions) if r 1=2. We show significantly improved bounds in the linearly bounded error model which hold for the entire range 0 &lt; r &lt; 1=2. <p> This critical resource is represented by some aspect of the chip game, such as number of chips used or number of moves in the game. Spencer and Winkler <ref> [32] </ref> have also examined this problem. They have arrived independently at one of the theorems in this paper using different proof techniques. Their paper as well as one by Dhagat, Gacs, and Winkler [10] considers another linearly bounded model of errors.
Reference: [33] <author> S. M. Ulam. </author> <title> Adventures of a Mathematician. </title> <publisher> Charles Scribner's Sons, </publisher> <address> 1 edition, </address> <year> 1976. </year>
Reference-contexts: Clearly, dlg ne questions are sufficient if Carole always answers truthfully. The problem of searching with errors thus translates into playing "Twenty Questions" with a liar <ref> [33] </ref>. Corresponding to the aforementioned error models, we consider both a probabilistic and an adversarial linearly bounded liar. The game against a linearly bounded liar can now be further reformulated as a Chip Game between two players: the Pusher and the Chooser.
Reference: [34] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <month> November </month> <year> 1984. </year>
Reference: [35] <author> Leslie G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In Proceedings IJCAI-85, </booktitle> <pages> pages 560-566. </pages> <booktitle> International Joint Committee for Artificial Intelligence, </booktitle> <publisher> Morgan Kaufmann, </publisher> <month> August </month> <year> 1985. </year>
Reference: [36] <author> V. N. Vapnik and A. Ya. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. Theory of Probability and its Applications, </title> <address> XVI(2):264-280, </address> <year> 1971. </year>
References-found: 36

