URL: file://ftp.cc.gatech.edu/pub/groups/architecture/TASS/sigmetrics-94.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Architecture/TASS/tass.html
Root-URL: 
Email: fanand, aman, rama, venkatg@cc.gatech.edu  
Title: An Approach to Scalability Study of Shared Memory Parallel Systems  
Author: Anand Sivasubramaniam Aman Singla Umakishore Ramachandran H. Venkateswaran 
Date: 171-180, May 1994.  
Note: In Proceedings of the ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, pages  
Address: Atlanta, GA 30332-0280.  
Affiliation: College of Computing Georgia Institute of Technology  
Abstract: The overheads in a parallel system that limit its scalability needto be identified and separated in order to enable parallel algorithm design and the development of parallel machines. Such overheads may be broadly classified into two components. The first one is intrinsic to the algorithm and arises due to factors such as the work-imbalance and the serial fraction. The second one is due to the interaction between the algorithm and the architecture and arises due to latency and contention in the network. A top-down approach to scalability study of shared memory parallel systems is proposed in this research. We define the notion of overhead functions associated with the different algorithmic and architectural characteristics to quantify the scalability of parallel systems; we isolate the algorithmic overhead and the overheads due to network latency and contention from the overall execution time of an application; we design and implement an execution-driven simulation platform that incorporates these methods for quantifying the overhead functions; and we use this simulator to study the scalability characteristics of five applications on shared memory platforms with different communication topologies. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Several performance studies address issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 15] and the limits on interconnection network performance <ref> [1, 16] </ref> are examples of such studies. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [2] <author> G. M. </author> <title> Amdahl. Validity of the Single Processor Approach to achieving Large Scale Computing Capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: Performance metrics such as speedup <ref> [2] </ref>, scaled speedup [11], sizeup [25], experimentally determined serial fraction [12], and isoefficiency function [13] have been proposed for quantifying the scalability of parallel systems.
Reference: [3] <author> T. E. Anderson. </author> <title> The Performance of Spin Lock Alternatives for Shared-Memory Multiprocessors. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(1) </volume> <pages> 6-16, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The growth of overhead functions will provide key insights on the scalability of a parallel system by suggesting application restructuring, as well as architectural enhancements. Several performance studies address issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 15] </ref> and the limits on interconnection network performance [1, 16] are examples of such studies. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance. <p> An application programmer may further define sub-modes if necessary. * BARRIER: Mode corresponding to a barrier synchronization operation. * MUTEX: Even though the simulated hardware provides only a test&set operation, mutual exclusion lock (implemented using test-test&set <ref> [3] </ref>) is available as a library function in SPASM. A program enters this mode during lock operations. With this mechanism, we can separate the overheads due to the synchro nization operations from the rest of the program execution. * PGM SYNC: Parallel programs may use Signal-Wait semantics for pairwise synchronization.
Reference: [4] <author> D. Bailey et al. </author> <title> The NAS Parallel Benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <year> 1991. </year>
Reference-contexts: In all three synchronization modes, the latency and contention overheads together represent the actual time incurred in accessing synchronization variables. 3 Application Characteristics Three of the applications (EP, IS and CG) are from the NAS parallel benchmark suite <ref> [4] </ref>; CHOLESKY is from the SPLASH benchmark suite [19]; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio.
Reference: [5] <author> E. A. Brewer, C. N. Dellarocas, A. Colbrook, and W. E. Weihl. </author> <title> PROTEUS : A high-performance parallel-architecture simulator. </title> <type> Technical Report MIT-LCS-TR-516, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA 02139, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 17] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions (such as LOADS and STORES) that may potentially involve a network access are simulated.
Reference: [6] <author> D. Chen, H. Su, and P. Yew. </author> <title> The Impact of Synchronization and Granularity on Parallel Systems. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 239-248, </pages> <year> 1990. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [10] and the impact of synchronization and task granularity on parallel system performance <ref> [6] </ref>. Cypher et al. [9] identify the architectural requirements such as floating point operations, communication, and input/output for message-passing scientific applications. Rothberg et al. [18] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [7] <author> R. G. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair. </author> <title> The Rice parallel processing testbed. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1988 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 4-11, </pages> <address> Santa Fe, NM, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 17] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions (such as LOADS and STORES) that may potentially involve a network access are simulated.
Reference: [8] <author> D. Culler et al. </author> <title> LogP : Towards a realistic model of parallel computation. </title> <booktitle> In Proceedings of the 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 1-12, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The latency and contention overheads (Figures 14 and 15) incurred in this mode are insignificant compared to the total execution time, despite the growth of contention overhead with increasing number of processors. The communication in FFT has been optimized as suggested in <ref> [8] </ref> into a single phase where every processor accesses the data of all the other processors in a skewed manner.
Reference: [9] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina. </author> <title> Architectural requirements of parallel scientific applications with explicit communication. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-13, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance [10] and the impact of synchronization and task granularity on parallel system performance [6]. Cypher et al. <ref> [9] </ref> identify the architectural requirements such as floating point operations, communication, and input/output for message-passing scientific applications. Rothberg et al. [18] conduct a similar study towards identifying the cache and memory size requirements for several applications.
Reference: [10] <author> S. J. Eggers and R. H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <address> Boston, Massachusetts, </address> <month> April </month> <year> 1989. </year>
Reference-contexts: There are studies that use real applications to address specific issues like the effect of sharing in parallel programs on the cache and bus performance <ref> [10] </ref> and the impact of synchronization and task granularity on parallel system performance [6]. Cypher et al. [9] identify the architectural requirements such as floating point operations, communication, and input/output for message-passing scientific applications.
Reference: [11] <author> J. L. Gustafson, G. R. Montry, and R. E. Benner. </author> <title> Development of Parallel Methods for a 1024-node Hypercube. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9(4) </volume> <pages> 609-638, </pages> <year> 1988. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup <ref> [11] </ref>, sizeup [25], experimentally determined serial fraction [12], and isoefficiency function [13] have been proposed for quantifying the scalability of parallel systems.
Reference: [12] <author> A. H. Karp and H. P. Flatt. </author> <title> Measuring Parallel processor Performance. </title> <journal> Communications of the ACM, </journal> <volume> 33(5) </volume> <pages> 539-543, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup [25], experimentally determined serial fraction <ref> [12] </ref>, and isoefficiency function [13] have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide adequate information needed to understand the reason why an application does not scale well on an architecture.
Reference: [13] <author> V. Kumar and V. N. Rao. </author> <title> Parallel Depth-First Search. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16(6) </volume> <pages> 501-519, </pages> <year> 1987. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup [25], experimentally determined serial fraction [12], and isoefficiency function <ref> [13] </ref> have been proposed for quantifying the scalability of parallel systems. While these metrics are extremely useful for tracking performance trends, they do not provide adequate information needed to understand the reason why an application does not scale well on an architecture.
Reference: [14] <author> F. H. McMahon. </author> <title> The Livermore Fortran Kernels : A Computer Test of the Numerical Performance Range. </title> <type> Technical Report UCRL-53745, </type> <institution> Lawrence Livermore National Laboratory, Livermore, </institution> <address> CA, </address> <month> December </month> <year> 1986. </year>
Reference-contexts: Such abstractions of real applications that capture the main phases of the computation are called kernels. One can go even lower than kernels by abstracting the main loops in the computation (like the Lawrence Livermore loops <ref> [14] </ref>) and evaluating their performance. As one goes lower, the outcome of the evaluation becomes less realistic. Even though an application may be abstracted by the kernels inside it, the sum of the times spent in the underlying kernels may not necessarily yield the time taken by the application.
Reference: [15] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: The growth of overhead functions will provide key insights on the scalability of a parallel system by suggesting application restructuring, as well as architectural enhancements. Several performance studies address issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware <ref> [3, 15] </ref> and the limits on interconnection network performance [1, 16] are examples of such studies. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [16] <author> G. F. Pfister and V. A. Norton. </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <journal> IEEE Transactions on Computer Systems, </journal> <volume> C-34(10):943-948, </volume> <month> Oc-tober </month> <year> 1985. </year>
Reference-contexts: Several performance studies address issues such as latency, contention and synchronization. The scalability of synchronization primitives supported by the hardware [3, 15] and the limits on interconnection network performance <ref> [1, 16] </ref> are examples of such studies. While such issues are extremely important, it is necessary to put the impact of these factors into perspective by considering them in the context of overall application performance.
Reference: [17] <author> S. K. Reinhardt et al. </author> <title> The Wisconsin Wind Tunnel : Virtual prototyping of parallel computers. </title> <booktitle> In Proceedings of the ACM SIGMETRICS 1993 Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 48-60, </pages> <address> Santa Clara, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Overhead functions can be used to study the growth of system overheads for any of these scaling strategies. In our simulation experiments, we limit ourselves to the constant problem size scaling model. 2.2 SPASM SPASM is an execution-driven simulator written in CSIM. As with other recent simulators <ref> [5, 7, 17] </ref>, the bulk of the instructions in the parallel program is executed at the speed of the native processor (SPARC in this study) and only the instructions (such as LOADS and STORES) that may potentially involve a network access are simulated.
Reference: [18] <author> E. Rothberg, J. P. Singh, and A. Gupta. </author> <title> Working sets, cache sizes and node granularity issues for large-scale multiprocessors. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 14-25, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Cypher et al. [9] identify the architectural requirements such as floating point operations, communication, and input/output for message-passing scientific applications. Rothberg et al. <ref> [18] </ref> conduct a similar study towards identifying the cache and memory size requirements for several applications. However, there have been very few attempts at quantifying the effects of algorithmic and architectural interactions in a parallel system.
Reference: [19] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <type> Technical Report CSL-TR-91-469, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <year> 1991. </year>
Reference-contexts: In all three synchronization modes, the latency and contention overheads together represent the actual time incurred in accessing synchronization variables. 3 Application Characteristics Three of the applications (EP, IS and CG) are from the NAS parallel benchmark suite [4]; CHOLESKY is from the SPLASH benchmark suite <ref> [19] </ref>; and FFT is the well-known Fast Fourier Transform algorithm. EP and FFT are well-structured applications with regular communication patterns determinable at compile-time, with the difference that EP has a higher computation to communication ratio.
Reference: [20] <author> A. Sivasubramaniam, U. Ramachandran, and H. Venkat-eswaran. </author> <title> Message-Passing: Computational Model, Programming Paradigm, and Experimental Studies. </title> <type> Technical Report GIT-CC-91/11, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> February </month> <year> 1991. </year>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory [21] and message-passing <ref> [20] </ref> platforms. In [24], we illustrated the top-down approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems.
Reference: [21] <author> A. Sivasubramaniam, G. Shah, J. Lee, U. Ramachandran, and H. Venkateswaran. </author> <title> Experimental Evaluation of Algorithmic Performance on Two Shared Memory Multiprocessors. In Norihisa Suzuki, editor, </title> <booktitle> Shared Memory Multiprocessing, </booktitle> <pages> pages 81-107. </pages> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory <ref> [21] </ref> and message-passing [20] platforms. In [24], we illustrated the top-down approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems.
Reference: [22] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> An Approach to Scalability Study of Shared Memory Parallel Systems. </title> <type> Technical Report GIT-CC-93/62, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: While a certain number of rows of the matrix in CG is assigned to a processor at compile time (static scheduling), CHOLESKY uses a dynamically maintained queue of runnable tasks. The reader is referred to <ref> [22] </ref> for further details of the applications. 4 Architectural Characteristics Since uniprocessor architecture is getting standardized with the advent of RISC technology, we fix most of the processor characteristics by using a 33 MHz SPARC chip as the baseline for each processor in a parallel system. <p> For each of these kernels, we quantify the different interaction overheads responsible for the deviation during each execution mode of the kernel. Only the results for IS, FFT and CHOLESKY are discussed in this section due to space constraints. Details on the other kernels can be found in <ref> [22] </ref>. In the following subsections, we show for each kernel the execution time, the latency, and the contention overhead graphs for the mesh platform. The first shows the total execution time, while the latter two show the communication overheads ignoring the computation time. <p> 129:3=p 0:7 129:3=p 0:7 129:3=p 0:7 Latency (ms) 13:2 (1 1 p ) 13:2 (1 1 p ) Contention (ms) N egligible 4:0 log p 0:9p Table 1: IS : Overhead Functions Parallelization of this kernel increases the amount of work to be done for a given problem size (see <ref> [22] </ref>). This inherent algorithmic overhead causes a deviation of the ideal curve from the linear curve (see Figure 3). This is also confirmed in Table 1, where the computation time does not decrease linearly with the number of processors. This indicates the kernel is not scalable for small problem sizes.
Reference: [23] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> Machine Abstractions and Locality Issues in Studying Parallel Systems. </title> <type> Technical Report GIT-CC-93/63, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> October </month> <year> 1993. </year>
Reference-contexts: In [24], we illustrated the top-down approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems. In a companion paper <ref> [23] </ref> we evaluate the use of abstractions for the network and locality in the context of simulating cache-coherent shared memory multiprocessors. The top-down approach and the overhead functions are elaborated in Section 2.
Reference: [24] <author> A. Sivasubramaniam, A. Singla, U. Ramachandran, and H. Venkateswaran. </author> <title> A Simulation-based Scalability Study of Parallel Systems. </title> <type> Technical Report GIT-CC-93/27, </type> <institution> College of Computing, Georgia Institute of Technology, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: In our earlier work, we studied issues such as task granularity, data distribution, scheduling, and synchronization, by implementing frequently used parallel algorithms on shared memory [21] and message-passing [20] platforms. In <ref> [24] </ref>, we illustrated the top-down approach for the scalability study of message-passing systems. In this paper, we conduct a similar study for shared memory systems. In a companion paper [23] we evaluate the use of abstractions for the network and locality in the context of simulating cache-coherent shared memory multiprocessors.
Reference: [25] <author> X-H. Sun and J. L. Gustafson. </author> <title> Towards a better Parallel Performance Metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <year> 1991. </year>
Reference-contexts: Performance metrics such as speedup [2], scaled speedup [11], sizeup <ref> [25] </ref>, experimentally determined serial fraction [12], and isoefficiency function [13] have been proposed for quantifying the scalability of parallel systems.
Reference: [26] <author> J. C. Wyllie. </author> <title> The Complexity of Parallel Computations. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1979. </year>
Reference-contexts: The algorithmic overhead is quantified by computing the time taken for execution of a given parallel program on an ideal machine such as the PRAM <ref> [26] </ref> and measuring its deviation from a linear speedup curve. A real execution could deviate significantly from the ideal execution due to overheads such as latency, contention, synchronization, scheduling and cache effects. These overheads are lumped together as the interaction overhead. <p> The algorithmic overhead is quantified by computing the time taken for execution of a given parallel program on an ideal machine such as the PRAM <ref> [26] </ref> and measuring its deviation from a linear speedup curve. The interaction overhead is also separated into its component parts. We currently do not address scheduling overheads 1 .
References-found: 26

