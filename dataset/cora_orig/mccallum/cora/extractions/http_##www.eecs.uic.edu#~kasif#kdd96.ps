URL: http://www.eecs.uic.edu/~kasif/kdd96.ps
Refering-URL: http://www.eecs.uic.edu/~kasif/learn-research.html
Root-URL: 
Email: trux@cs.jhu.edu  kasif@cs.jhu.edu  salzberg@cs.jhu.edu  waltz@research.nj.nec.com  
Title: Local Induction of Decision Trees: Towards Interactive Data Mining  
Author: Truxton Fulton Simon Kasif Steven Salzberg David Waltz 
Keyword: Local Learning, Decision Trees, Data Mining.  
Abstract: Decision trees are an important data mining tool with many applications. Like many classification techniques, decision trees process the entire data base in order to produce a generalization of the data that can be used subsequently for classification. Large, complex data bases are not always amenable to such a global approach to generalization. This paper explores several methods for extracting data that is local to a query point, and then using the local data to build generalizations. These adaptively constructed neighborhoods can provide additional information about the query point. Three new algorithms are presented, and experiments using these algorithms are described. 
Abstract-found: 1
Intro-found: 1
Reference: [AMS95] <author> Christopher Atkeson, Andrew Moore, and Stefan Schaal. </author> <title> Locally weighted learning. </title> <note> submitted for publication, </note> <month> April </month> <year> 1995. </year>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, CS93], robotics <ref> [AMS95, Atk89, MAS95] </ref>, computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in [BV92, VB93].
Reference: [Atk89] <author> C. Atkeson. </author> <title> Using local models to control movement. </title> <booktitle> In Neural Information Processing Systems Conf., </booktitle> <year> 1989. </year>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, CS93], robotics <ref> [AMS95, Atk89, MAS95] </ref>, computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in [BV92, VB93].
Reference: [BFOS84] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth Internatl. Group, </publisher> <year> 1984. </year>
Reference-contexts: For example, modern medical databases contain enormous quantities of patient data, which can provide great value in the treatment of future patients. In order to gain the maximum value from such databases, data mining tools are essential. One popular and successful data mining technique is the decision tree classifier <ref> [BFOS84, Qui93, MKS94] </ref> which can be used to classify new examples as well as providing a relatively concise description of the database. In this paper we describe a notion of interactive data mining where we wait for the user to provide a "query" that specifies a neighborhood to be mined. <p> In these experiments, the decision tree induction algorithm is implemented using standard axis-parallel splits and information gain as the goodness criterion. This algorithm is thus very similar to Quinlan's C4.5 [Qui93]. The decision trees are pruned using standard cost- complexity pruning <ref> [BFOS84] </ref> with a portion of the training set set aside as a pruning set. The purpose of these experiments is to determine whether local induction voting is an improvement over full induction. The graphs presented in this section show overall accuracy as a function of the parameter K varies.
Reference: [BV92] <author> L. Bottou and V. Vapnik. </author> <title> Local learning algorithms. </title> <journal> Neural Computation, </journal> <volume> 4 </volume> <pages> 888-900, </pages> <year> 1992. </year>
Reference-contexts: Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in <ref> [BV92, VB93] </ref>.
Reference: [CMSW92] <author> R. Creecy, B. Masand, S. Smith, and D. L. Waltz. </author> <title> Trading MIPS and memory for knowledge engineering. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 48-64, </pages> <year> 1992. </year>
Reference-contexts: Instead of building a complex statistical model that describes the entire space, we construct a simpler model that describes the space in a particular neighborhood. Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data <ref> [CMSW92] </ref>, software agents [MK93], computational biology [YL93, CS93], robotics [AMS95, Atk89, MAS95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94].
Reference: [CS93] <author> S. Cost and S. Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology <ref> [YL93, CS93] </ref>, robotics [AMS95, Atk89, MAS95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94].
Reference: [Ede94] <author> S. Edelman. </author> <title> Representation, similarity and the chorus of prototypes. Minds and Machines, </title> <year> 1994. </year>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, CS93], robotics [AMS95, Atk89, MAS95], computer vision <ref> [Ede94] </ref>, and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in [BV92, VB93].
Reference: [FKSW96] <author> T. Fulton, S. Kasif, S. Salzberg, and D. Waltz. </author> <title> Local induction of decision trees: Towards interactive data mining. </title> <type> Technical report, </type> <institution> Johns Hopkins University, </institution> <address> Baltimore, MD 21218, </address> <year> 1996. </year>
Reference-contexts: However, it turns out that we can utilize computational geometry techniques to reduce the running time to O (N log d1 N ) obtaining a practical improvement when the dimensionality is small. We provide a very rough sketch of the algorithm for two dimensions below. See <ref> [FKSW96] </ref> for a complete description. Given a point x we compute the set of monochromatic rectangles that are defined by x and another point in the database. Recall these are the points that we include in the local neighborhood of x. <p> This algorithm is sketched in Figure 3. The size of such a search tree is exponential in the number of dimensions of the feature space, and therefore we experimented with several greedy approximation algorithms (see <ref> [FKSW96] </ref>). 4 Experiments In this section we focus on the performance of the local induction voting algorithm used for several scientific domains: breast cancer diagnosis, star/galaxy classification, and identification of coding regions in DNA. We also performed experiments with the other algorithms using artificial datasets.
Reference: [Fri94] <author> J. H. Friedman. </author> <title> Flexible metric nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford Uni- versity, Statistics Dept., </institution> <month> November </month> <year> 1994. </year>
Reference-contexts: Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms <ref> [HT94, Fri94] </ref>. See also the very relevant theoretical framework for local learning described in [BV92, VB93].
Reference: [HT94] <author> T. Hastie and R. Tibshirani. </author> <title> Discriminant adaptive nearest neighbor classification. </title> <type> Technical report, </type> <institution> Stanford University, Statistics Dept., </institution> <month> December </month> <year> 1994. </year>
Reference-contexts: Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms <ref> [HT94, Fri94] </ref>. See also the very relevant theoretical framework for local learning described in [BV92, VB93]. <p> The algorithm is running in time O (N log N ). 3.3 Choosing a local neighborhood via an adaptive boundary Our third algorithm was inspired by a method of Hastie and Tibshirani <ref> [HT94] </ref>, who defined a technique for creating an ellipsoidal neighborhood around a query. We have implemented an iterative search for a rectangular neighborhood around a query.
Reference: [MAS95] <author> A. Moore, C. Atkeson, and S. Schaal. </author> <title> Memory-based learning for control. </title> <note> Artificial Intelligence Review (to appear), </note> <year> 1995. </year>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology [YL93, CS93], robotics <ref> [AMS95, Atk89, MAS95] </ref>, computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in [BV92, VB93].
Reference: [MK93] <author> P. Maes and R. Kozierok. </author> <title> Learning interface agents. </title> <booktitle> In Proc. of the Eleventh National Conf. on Artificial Intelligence, </booktitle> <pages> pages 459-465, </pages> <address> Washington, D.C., 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Instead of building a complex statistical model that describes the entire space, we construct a simpler model that describes the space in a particular neighborhood. Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents <ref> [MK93] </ref>, computational biology [YL93, CS93], robotics [AMS95, Atk89, MAS95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94].
Reference: [MKS94] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-33, </pages> <month> August </month> <year> 1994. </year>
Reference-contexts: For example, modern medical databases contain enormous quantities of patient data, which can provide great value in the treatment of future patients. In order to gain the maximum value from such databases, data mining tools are essential. One popular and successful data mining technique is the decision tree classifier <ref> [BFOS84, Qui93, MKS94] </ref> which can be used to classify new examples as well as providing a relatively concise description of the database. In this paper we describe a notion of interactive data mining where we wait for the user to provide a "query" that specifies a neighborhood to be mined.
Reference: [MLW92] <author> B. Masand, G. Linoff, and D. L. Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of the SIGIR, </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: Instead of building a complex statistical model that describes the entire space, we construct a simpler model that describes the space in a particular neighborhood. Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles <ref> [MLW92] </ref>, census data [CMSW92], software agents [MK93], computational biology [YL93, CS93], robotics [AMS95, Atk89, MAS95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94].
Reference: [PS85] <author> F. P. Preparata and M. I. Shamos. </author> <title> Computational Geometry: An Introduction. </title> <publisher> Springer- Verlag, </publisher> <address> New York, NY, </address> <year> 1985. </year>
Reference: [Qui93] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: For example, modern medical databases contain enormous quantities of patient data, which can provide great value in the treatment of future patients. In order to gain the maximum value from such databases, data mining tools are essential. One popular and successful data mining technique is the decision tree classifier <ref> [BFOS84, Qui93, MKS94] </ref> which can be used to classify new examples as well as providing a relatively concise description of the database. In this paper we describe a notion of interactive data mining where we wait for the user to provide a "query" that specifies a neighborhood to be mined. <p> In these experiments, the decision tree induction algorithm is implemented using standard axis-parallel splits and information gain as the goodness criterion. This algorithm is thus very similar to Quinlan's C4.5 <ref> [Qui93] </ref>. The decision trees are pruned using standard cost- complexity pruning [BFOS84] with a portion of the training set set aside as a pruning set. The purpose of these experiments is to determine whether local induction voting is an improvement over full induction.
Reference: [VB93] <author> V. Vapnik and L. Bottou. </author> <title> Local learning algorithms for pattern recognition and dependency estimation. </title> <journal> Neural Computation, </journal> <volume> 5 </volume> <pages> 893-909, </pages> <year> 1993. </year>
Reference-contexts: Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94]. See also the very relevant theoretical framework for local learning described in <ref> [BV92, VB93] </ref>.
Reference: [YL93] <author> T.-M. Yi and E. Lander. </author> <title> Protein secondary structure prediction using nearest-neighbor methods. </title> <journal> Journal of Molecular Biology, </journal> <volume> 232 </volume> <pages> 1117-1129, </pages> <year> 1993. </year> <month> 10 </month>
Reference-contexts: Local learning is a special case of memory-based reasoning (MBR). Applications of MBR include classification of news articles [MLW92], census data [CMSW92], software agents [MK93], computational biology <ref> [YL93, CS93] </ref>, robotics [AMS95, Atk89, MAS95], computer vision [Ede94], and many other pattern recognition and machine learning applications. Recent work in statistics addresses the issue of adaptive neighborhood to a given query to improve K-nearest neighbour algorithms [HT94, Fri94].
References-found: 18

