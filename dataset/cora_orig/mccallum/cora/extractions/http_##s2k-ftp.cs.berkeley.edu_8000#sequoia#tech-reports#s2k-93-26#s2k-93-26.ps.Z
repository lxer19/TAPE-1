URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-93-26/s2k-93-26.ps.Z
Refering-URL: http://s2k-ftp.cs.berkeley.edu:8000/sequoia/tech-reports/s2k-93-26/
Root-URL: http://www.cs.berkeley.edu
Email: chris@bliss.berkeley.edu marti@cs.berkeley.edu  
Title: Subtopic Structuring for Full-Length Document Access  
Author: Marti A. Hearst Christian Plaunt 
Address: 571 Evans Hall, UC Berkeley 207c SLIS, UC Berkeley Berkeley, CA 94720 USA Berkeley, CA 94720 USA  Center  
Affiliation: Computer Science Division Library and Information Studies  and Xerox Palo Alto Research  
Note: In the Proceedings of SIGIR '93, Pittsburgh, PA  
Abstract: We argue that the advent of large volumes of full-length text, as opposed to short texts like abstracts and newswire, should be accompanied by corresponding new approaches to information access. Toward this end, we discuss the merits of imposing structure on full-length text documents; that is, a partition of the text into coherent multi-paragraph units that represent the pattern of subtopics that comprise the text. Using this structure, we can make a distinction between the main topics, which occur throughout the length of the text, and the subtopics, which are of only limited extent. We discuss why recognition of subtopic structure is important and how, to some degree of accuracy, it can be found. We describe a new way of specifying queries on full-length documents and then describe an experiment in which making use of the recognition of local structure achieves better results on a typical information retrieval task than does a standard IR measure. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Croft, W. Bruce, Robert Krovetz, & H. Turtle. </author> <year> 1990. </year> <title> Interactive retrieval of complex documents. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26(5) </volume> <pages> 593-616. </pages>
Reference-contexts: Crucial to a system's decision to retrieve this document is the knowledge that a dense discussion of volcanic activity, rather than a passing reference, appears. Since <ref> (Croft et al. 1990) </ref> describes work that takes advantage of this kind of information. volcanism is not one of the text's two main topics, we shouldn't expect the number of references to this term to dominate the statistics of a vector space model.
Reference: <author> Hahn, Udo. </author> <year> 1990. </year> <title> Topic parsing: Accounting for text macro structures in full-text analysis. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 26(1) </volume> <pages> 135-170. </pages>
Reference-contexts: In our experiments, however, using unmotivated segments worked less well overall than using paragraphs or motivated segments (see Section 4). TextTiling We are interested in exploring the performance of motivated segmentation, i.e., segmentation that reflects the text's true underlying subtopic structure, which often spans paragraph boundaries. <ref> (Hahn 1990) </ref> has eloquently addressed the need for imposing structure on full-length documents in order to improve information retrieval, but proposes a knowledge-intensive, strongly domain dependent approach, which is difficult to scale to sizeable text collections.
Reference: <author> Hearst, Marti A. </author> <year> 1993a. </year> <title> Cases as structured indexes for full-length documents. </title> <booktitle> In Proceedings of the 1993 AAAI Spring Symposium on Case-based Reasoning and Information Retrieval, </booktitle> <address> Stanford,CA. </address>
Reference-contexts: These ideas are still in the experimental stage and have not yet been implemented. <ref> (Hearst 1993a) </ref> discusses in more detail innovative ways to use this bipartite indexing strategy. 4 Using Subtopic Structure to Improve Retrieval: An Exper iment Our approach to retrieval should reflect our assumption that full-length text is meaningfully different in structure from abstracts and short articles.
Reference: <author> Hearst, Marti A. </author> <year> 1993b. </year> <title> TextTiling: A quantitative approach to discourse segmentation. </title> <type> Technical Report 93/24, Sequoia 2000 Technical Report, </type> <institution> University of California, Berkeley. </institution>
Reference-contexts: In contrast, we want an algorithm that can be implemented and tested on a large, diverse text collection. Toward this end, we have developed TextTiling, a method for partitioning full-length text documents into coherent multi-paragraph units <ref> (Hearst 1993b) </ref>. Text-Tiling approximates the subtopic structure of a document by using patterns of lexical connectivity to find coherent subdiscussions. The layout of the `tiles' is meant to reflect the pattern of subtopics contained in an expository text.
Reference: <author> Liddy, Elizabeth. </author> <year> 1991. </year> <title> The discourse level structure of empirical abstracts an exploratory study. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 27(1) </volume> <pages> 55-81. </pages>
Reference-contexts: That is, a term's weight is the same no matter where it occurs in the text. 1 Many researchers assume this is a valid assumption when working with abstracts, since it is a fair approximation to say that the location of the term does not significantly effect its import (although <ref> (Liddy 1991) </ref> discusses the usefulness of understanding the structure of an abstract when using a natural-language based IR approach). These comments apply as well to short news articles, another text type commonly studied in information retrieval research.
Reference: <author> Rabiner, Lawrence R. & Ronald W. Schafer. </author> <year> 1978. </year> <title> Digital processing of speech signals. </title> <address> New Jersey: </address> <publisher> Prentice-Hall, Inc. </publisher>
Reference-contexts: from the calculation. 4 The authors are grateful to Michael Braverman for proving that the smoothing algorithm is equivalent to this convolution. tion h k (), where: h k (i) k 2 (k jij); jij k 1 0; otherwise The result is smoothed further with a simple median smoothing algorithm <ref> (Rabiner & Schafer 1978) </ref>, with a window of size three, to eliminate small local minima. Tile boundaries are determined by locating the lowermost portions of valleys in the resulting plot. The actual values of the similarity measures are not taken into account; the relative differences are what are of consequence.
Reference: <author> Ro, Jung Soon. </author> <year> 1988a. </year> <title> An evaluation of the applicability of ranking algorithms to improve the effectiveness of full-text retrieval. I. on the effectiveness of full-text retrieval. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 39(2) </volume> <pages> 73-78. </pages>
Reference-contexts: Their focus is on how to find similarity among blocks of text of greatly differing length, and not so much on the role of the text block in the document that it is a part of. <ref> (Ro 1988a) </ref> has performed experiments addressing the issue of retrieval from full texts in contrast to using controlled vocabulary, abstracts, and paragraphs alone.
Reference: <author> Ro, Jung Soon. </author> <year> 1988b. </year> <title> An evaluation of the applicability of ranking algorithms to improve the effectiveness of full-text retrieval. II. on the effectiveness of ranking algorithms on full-text retrieval. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> 39(3) </volume> <pages> 147-160. </pages>
Reference-contexts: Performing boolean retrieval for a set of nine queries against business management journal articles, Ro found that retrieving against full text produced the highest recall but the lowest precision of all the methods. In subsequent experiments, <ref> (Ro 1988b) </ref> tried various weighting schemes in an attempt to show that retrieving against full text would perform better than against paragraphs alone, but did not achieve significant results to this effect.
Reference: <author> Salton, Gerard. </author> <year> 1988. </year> <title> Automatic text processing : the transformation, analysis, </title> <booktitle> and retrieval of information by computer. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Actual paragraphs are not used because their lengths can be highly irregular, leading to unbalanced comparisons. Similarity is measured by putting a twist on the tf.idf measurement <ref> (Salton 1988) </ref>. In standard tf.idf, terms that are frequent in an individual document but relatively infrequent throughout the corpus are considered to be good distinguishers of the contents of the individual document.
Reference: <author> Salton, Gerard & Chris Buckley. </author> <year> 1991a. </year> <title> Automatic text structuring and retrieval: Experiments in automatic encyclopedia searching. </title> <booktitle> In Proceedings of SIGIR, </booktitle> <pages> 21-31. </pages>
Reference-contexts: In both cases this entails using the orthographic marking supplied by the author to determine topic boundaries. Salton and Buck-ley <ref> (Salton & Buckley 1991a) </ref>,(Salton & Buckley 1991b) have done the most comprehensive work to date on issues pertaining to full-length text. They have compared paragraphs within a large document (e.g., Salton's book), articles within an online encyclopedia, and electronic mail messages (inquiries and their replies). <p> Retrieve against the segmented document collec tion 4. Recombine the retrieved segments by parent text retaining only a predetermined number of the top scoring texts 5. Compare the results with standard query/full doc ument retrieval In the experimental runs themselves, we followed the example of <ref> (Salton & Buckley 1991a) </ref> by retrieving a threshold number of documents for each query; we used cutoffs of 5, 10, 15, 20, 25, and 30 retrieved documents. Though similar to standard vector-space retrieval, there are few notable differences here. <p> The average segment length was approximately 164 words, the average paragraph length was approximately 53 words, and the average full text length was 3,557 words. 4.3 Weighting Schemes For our experiments, we used two variants of tf.idf term weighting schemes. For full length documents, following the suggestion of <ref> (Salton & Buckley 1991a) </ref> we used then "enhanced" atc weights for query/document similarity comparison. We also used this weight for indexing and calculating inter-segment similarity.
Reference: <author> Salton, Gerard & Chris Buckley. </author> <year> 1991b. </year> <title> Global text matching for information retrieval. </title> <journal> Science, </journal> <volume> 253 </volume> <pages> 1012-1015. </pages>
Reference: <author> Stanfill, Craig & David L. Waltz. </author> <year> 1992. </year> <title> Statistical methods, </title> <booktitle> artificial intelligence, and information retrieval. In Text-based intelligent systems: Current research and practice in information extraction and retrieval, </booktitle> <editor> ed. by Paul S. Jacobs, </editor> <address> 215-226. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: These experiments are not entirely relevant to the efforts described here because of their focus on the boolean paradigm. Using Even-Sized Blocks Another way to approximate local structure in long documents is to hack the documents into even-size pieces, without regard for any boundaries. <ref> (Stanfill & Waltz 1992) </ref> report on such a technique, using the efficiency of a massively parallel computer. They divide the documents into 30-word segments and compare the queries to each segment.
Reference: <author> Tenopir, Carol & Jung Soon Ro. </author> <year> 1990. </year> <title> Full text databases. New Directions in Information Management. </title> <publisher> Greenwood Press. </publisher>
Reference-contexts: 1 Introduction Full-length documents have only recently become available online in large quantities, although technical abstracts and short newswire texts have been accessible for many years <ref> (Tenopir & Ro 1990) </ref>. For this reason, most information retrieval methods are better suited for accessing abstracts than longer documents. In this paper, we argue that the advent of full-length text should be accompanied by corresponding new approaches to information access. Abstracts are compact and information-dense.
Reference: <author> Turtle, Howard. </author> <year> 1991. </year> <title> Evaluation of an inference network-based retrieval model. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 9(3) </volume> <pages> 187-222. </pages>
Reference-contexts: Nevertheless, we find these results to be quite encouraging and plan to pursue variations on the summing approach in the near future, perhaps incorporating sophisticated evidence combination methods, such as those explored in <ref> (Turtle 1991) </ref>, instead of simple summing. 5 Conclusions We have discussed the importance of recognizing the structure of full-length text for the purposes of information retrieval, emphasizing that most existing similarity-based techniques implicitly assume that the documents being queried against are of uniform structure and import.
Reference: <author> Yarowsky, David. </author> <year> 1992. </year> <title> Word sense disambiguation using statistical models of roget's categories trained on large corpora. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Computational Linguistics, </booktitle> <pages> 454-460, </pages> <address> Nantes, France. </address> <month> 10 </month>
Reference-contexts: We plan to use a category-based lexical disambiguation algorithm based on that of <ref> (Yarowsky 1992) </ref> for the purpose of assigning groups of terms to higher level categories, to facilitate disambiguated term expansion within subtopic segments.
References-found: 15

