URL: ftp://ftp.cim.mcgill.ca/pub/techrep/CIM-95-11.ps.gz
Refering-URL: ftp://ftp.cim.mcgill.ca/pub/techrep/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Active Recognition: Using Uncertainty to Reduce Ambiguity  
Author: Francesco G. Callari and Frank P. Ferrie 
Address: Montreal, Quebec, Canada Postal Address: 3480 University Street, Montreal, Quebec, Canada H3A 2A7  
Affiliation: Centre for Intelligent Machines McGill University  
Pubnum: TR-CIM-95-11  
Email: Email: cim@cim.mcgill.ca  
Phone: Telephone: (514) 398-6319 Telex: 05 268510 FAX: (514) 398-7348  
Date: Sept. 26, 1995 Revised Jan. 10, 1996  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. H. Barr. </author> <title> Superquadrics and angle preserving transformations. </title> <journal> IEEE Computer Graphics and Applications, </journal> <volume> 1(1) </volume> <pages> 11-23, </pages> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: In the present implementation of the autonomous explorer, the measurements are 3D coordinates of object surface points, measured with a laser rangefinder probe mounted on a variety of robot devices, and the highly nonlinear models used are superellipsoids <ref> [1] </ref> fitted via least-squares optimization to the segmented parts of the object. 5 The above prescription specifies where on the object the next measurement 4 Strictly speaking, we should say "in a small neighborhood of the current model estimate m N ": a continuity argument is necessarily involved here. 5 The <p> a m k (s 2 ) ;(8) where s 2 = g T a C g a is the (scalar, in this case) variance of a (), and k (s 2 ) , 1 + s 2 =8 2 :(9) This result is intuitively satisfactory: k (s 2 ) 2 <ref> [0; 1] </ref>, hence the uncertainty in the input reduces the confidence in the prediction by "pulling" the argument of f () toward 0, and hence the estimated class probability toward 1 2 . This is demonstrated in Figure 4 (left).
Reference: [2] <author> R. Bolles. </author> <title> On optimally combining pieces of information, with application to estimating 3-d complex-object position from range data. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-8(5):619-638, </volume> <year> 1986. </year>
Reference-contexts: A few works have actually cast the object recognition problem in a probabilistic perspective <ref> [2, 13] </ref>, yet usually pursuing an "identification" approach, aimed at producing definite statements about model matching versus a database of objects, usually via MAP estimation weighted by statistical risk.
Reference: [3] <author> J. S. Bridle. </author> <title> Probabilistic interpretation of feedforward classification networks outputs, with relationship to statistical pattern recognition. </title> <editor> In F. Fogelman-Soulie and J. Herault, editors, </editor> <booktitle> NATO ASI, volume F68. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Gaussian approximations "in the large") are likely to fail. 3 However, such a probabilistic recognition framework can be cast in terms of more powerful connectionist learning and classification algorithms in a rigorous and structured manner <ref> [3, 7, 14] </ref>, leading to improved classifier performance.
Reference: [4] <author> V. Fedorov. </author> <title> Theory of optimal experiments. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: Furthermore, model uncertainty, if properly quantified, can be propagated to the prediction themselves, thus augmenting them with predictive error bars. Intuition and sound statistical arguments (the theory of optimal experiments <ref> [4] </ref>) then suggest that the best places to collect data from in order to refine the model are those where the error bars are large, since these are the data-gathering points that are most likely to provide useful information. <p> From a theoretical point of view, this strategy is provably optimal since it maximizes the information gain in the model parameters across successive data-gathering steps [6], thus being equivalent to a `Q-optimal' experiment design <ref> [4] </ref>. An efficient data collecting strategy aimed at confident object recognition, however, is arguably different from the above even if it's guided by the same optimality criterion, and even if the recognition of the observed objects, i.e. their partition into different classes, is based on their shape alone. <p> Hence a prescription for efficiently increasing the model accuracy by collecting new data follows: Take further measurements where the predicted errorbars are largest, until they equal the noise level. This is the same as Fedorov's "Q-optimal" experiment design <ref> [4] </ref>. For linear models the sensitivity g m depends only on the locations, hence an entire exploration plan can be devised even before taking a single measurement.
Reference: [5] <author> S. F. </author> <title> Gull. Bayesian data analysis:straight-line fitting. </title> <editor> In J. Skilling, editor, </editor> <booktitle> Maximum entropy and Bayesian methods, </booktitle> <pages> pages 511-518. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1989. </year>
Reference-contexts: As far as learning is concerned, this alone would make the problem hardly tractable: Bayesian interpolation of data with noise in the inputs is notoriously difficult, and solutions have so far been proposed for the linear case only <ref> [5] </ref>. Yet we can easily (although not cheaply) negotiate this difficulty for training purposes, since we can use the shape-driven explorer, and merge data from multiple views until the model uncertainty is made negligible.
Reference: [6] <author> D. J. MacKay. </author> <title> Information-based objective functions for active data selection. </title> <journal> Neural Computation, </journal> <volume> 4(4) </volume> <pages> 589-603, </pages> <year> 1991. </year>
Reference-contexts: From a theoretical point of view, this strategy is provably optimal since it maximizes the information gain in the model parameters across successive data-gathering steps <ref> [6] </ref>, thus being equivalent to a `Q-optimal' experiment design [4]. <p> The classifier parameters w of a particular classifier structure are optimized by means of a training set of labelled data. The training procedure and the structure selection technique we use closely follow the hierarchical Bayesian methods proposed by MacKay in his seminal works of 1991-92 <ref> [6, 7, 8] </ref>.
Reference: [7] <author> D. J. MacKay. </author> <title> A practical Bayesian framework for backprop networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 448-472, </pages> <year> 1991. </year>
Reference-contexts: Gaussian approximations "in the large") are likely to fail. 3 However, such a probabilistic recognition framework can be cast in terms of more powerful connectionist learning and classification algorithms in a rigorous and structured manner <ref> [3, 7, 14] </ref>, leading to improved classifier performance. <p> The classifier parameters w of a particular classifier structure are optimized by means of a training set of labelled data. The training procedure and the structure selection technique we use closely follow the hierarchical Bayesian methods proposed by MacKay in his seminal works of 1991-92 <ref> [6, 7, 8] </ref>.
Reference: [8] <author> D. J. MacKay. </author> <title> Bayesian methods for adaptive models. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, Pasadena, </institution> <address> CA, USA, </address> <year> 1992. </year>
Reference-contexts: The classifier used for the experiments reported in this paper has been implemented with a connectionist architecture, trained and selected using the Bayesian methods for classification networks proposed by MacKay <ref> [8] </ref> (our experimental results give indications of the adequacy this choice to the task at hand). However, the proposed formalism can be applied to a much wider family of supervised classification methods. In the following sections we will elaborate on these issues in broader detail. <p> The classifier parameters w of a particular classifier structure are optimized by means of a training set of labelled data. The training procedure and the structure selection technique we use closely follow the hierarchical Bayesian methods proposed by MacKay in his seminal works of 1991-92 <ref> [6, 7, 8] </ref>. <p> P (Rjm), in the course of a recognition-driven exploration around an object of R (ound) type. 10 Active Recognition: Using Uncertainty to Reduce Ambiguity The above approximation falls graciously for a 2 s 2 , and is formally identical to MacKay's <ref> [8] </ref>. The difference is in the uncertain variables the marginalization is made over: the classifier parameters w in his case, the classifier inputs m in ours. These marginalized, or "moderated" predictions are used in the exploration technique proposed in the next section. 4.
Reference: [9] <author> P. MacKenzie and G. Dudek. </author> <title> Precise positioning using model-based maps. </title> <booktitle> In Proceedings of the International Conference on Robotics and Automation, </booktitle> <address> San Diego, CA, 1994. </address> <publisher> IEEE Press. </publisher>
Reference-contexts: The recovery of models from the data can rapidly become a highly demanding task when high bandwidth sensory information is used, as is the case in 3D vision-based navigation <ref> [9] </ref>. This is particularly true for vision-based systems that demand "rich" models, i.e. general models with high descriptive capability (Figure 1), usually because the agent interacts with poorly structured environments, upon which few assumptions can be made a-priori.
Reference: [10] <author> A. Pentland and S. Sclaroff. </author> <title> Closed form solutions for physically based shape modelling and recognition. </title> <editor> In T. Kanade and K. Ikeuchi, </editor> <title> editors, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence: Special Issue on Physical Modeling in Computer Vision, </journal> <volume> volume 13(7), </volume> <pages> pages 715-729, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: Although active object recognition, in its various forms, has traditionally attracted considerable attention, the above problems can still be regarded as open. Several schemes in the literature <ref> [10, 13] </ref> pose the recognition problem as one of matching 1 Of course each one of these steps may well be the subject of whole shelves of literature. <p> have working solutions for them [15, 16], hence we tend to regard them as being dealt with by a subsystem within the architecture subject of the present work. 3 Active Recognition: Using Uncertainty to Reduce Ambiguity recovered volumetric models with those of a database, using distance measures like dot product <ref> [10] </ref> or Mahalanobis distance.
Reference: [11] <author> F. Solina and R. </author> <title> Bajcsy. Recovery of parametric models from range images: The case for superquadrics with global deformations. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 12(2) </volume> <pages> 131-147, </pages> <month> Feb. </month> <year> 1990. </year>
Reference-contexts: object is based on the following idea: when a new datum x N+1 is added to the dataset fxg, to which model M (x; m N ) has been fitted, and a new optimal 3 In the case of superellipsoid models, for example, the inadequacy is due to view-dependent degeneracies <ref> [11] </ref>, that cause the posterior model distribution to variably be uni- or multi-modal, depending on the object and the chosen views. 5 Active Recognition: Using Uncertainty to Reduce Ambiguity fit is computed, the shape and the volume of the uncertainty bubble (the ellipsoids of confidence) in the model parameters m N
Reference: [12] <author> D. J. Spiegelharter and S. L. Lauritzen. </author> <title> Sequential updating of conditional probabilities on directed graphical structures. </title> <journal> NETWORKS, </journal> <volume> 20 </volume> <pages> 579-605, </pages> <year> 1990. </year>
Reference-contexts: The integral of a logistic (or softmax) function times a Gaussian cannot be solved in closed form, though it can easily be evaluated numeri cally. However, a well behaved approximation has been given <ref> [12] </ref> for the logistic case 7 Technically, this is a consequence of the normalization of probabilities, which causes the mapping from models to class probabilities to be necessarily nonlinear. 9 Active Recognition: Using Uncertainty to Reduce Ambiguity (i.e. binary classification), from which useful insight can be drawn, P (O i jm;
Reference: [13] <author> J. Subrahmonia, D. B. Cooper, and D. Keren. </author> <title> Practical reliable bayesian recognition of 2D and 3D objects using implicit polynomials and algebraic invariants. </title> <type> LEMS 107, </type> <institution> Brown University LEMS, Laboratory fo Engineering Man/Machine systems, Division of Engineering, Brown University, </institution> <address> Providence, RI 02912, USA, </address> <year> 1992. </year>
Reference-contexts: Although active object recognition, in its various forms, has traditionally attracted considerable attention, the above problems can still be regarded as open. Several schemes in the literature <ref> [10, 13] </ref> pose the recognition problem as one of matching 1 Of course each one of these steps may well be the subject of whole shelves of literature. <p> A few works have actually cast the object recognition problem in a probabilistic perspective <ref> [2, 13] </ref>, yet usually pursuing an "identification" approach, aimed at producing definite statements about model matching versus a database of objects, usually via MAP estimation weighted by statistical risk.
Reference: [14] <author> N. Tishby, E. Levin, and S. Solla. </author> <title> Consistent inference of probabilities in layered networks. </title> <booktitle> In Proc. Internat. Joint Conf. on Neural Networks, </booktitle> <address> Washington, </address> <year> 1989. </year>
Reference-contexts: Gaussian approximations "in the large") are likely to fail. 3 However, such a probabilistic recognition framework can be cast in terms of more powerful connectionist learning and classification algorithms in a rigorous and structured manner <ref> [3, 7, 14] </ref>, leading to improved classifier performance.
Reference: [15] <author> P. Whaite and F. P. Ferrie. </author> <title> From uncertainty to visual exploration. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> 13(10) </volume> <pages> 1038-1049, </pages> <month> Oct. </month> <year> 1991. </year>
Reference-contexts: Several schemes in the literature [10, 13] pose the recognition problem as one of matching 1 Of course each one of these steps may well be the subject of whole shelves of literature. We group them together simply because we do have working solutions for them <ref> [15, 16] </ref>, hence we tend to regard them as being dealt with by a subsystem within the architecture subject of the present work. 3 Active Recognition: Using Uncertainty to Reduce Ambiguity recovered volumetric models with those of a database, using distance measures like dot product [10] or Mahalanobis distance. <p> Along the same line of thought Whaite and Ferrie have proposed a technique of autonomous exploration <ref> [15, 16, 17] </ref> for model-based shape reconstruction from laser rangefinder images. <p> In Section 5 we present experimental demonstrations of these techniques, concluding in Section 6 with a discussion of the results obtained and further research. 2. Autonomous exploration for shape reconstruction Whaite and Ferrie's <ref> [15, 16, 17] </ref> technique for actively modelling the shape, pose and position of an object is based on the following idea: when a new datum x N+1 is added to the dataset fxg, to which model M (x; m N ) has been fitted, and a new optimal 3 In the <p> The controlling software is based on the previous Whaite and Ferrie's autonomous modeller/explorer <ref> [15, 16, 17] </ref>, augmented with a purposely designed Bayesian connectionist simulator and auxiliary software in order to implement the new exploration strategy (class probability estimation, class sensitivity computation, exploration step planning, performance statistics collection, etc).
Reference: [16] <author> P. Whaite and F. P. Ferrie. </author> <title> Active exploration: Knowing when we're wrong. </title> <booktitle> In PROC. Fourth International Conference on Computer Vision, </booktitle> <pages> pages 41-48, </pages> <address> Berlin, Germany, </address> <month> May 11-14 </month> <year> 1993. </year> <booktitle> Computer Society of the IEEE, </booktitle> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: Several schemes in the literature [10, 13] pose the recognition problem as one of matching 1 Of course each one of these steps may well be the subject of whole shelves of literature. We group them together simply because we do have working solutions for them <ref> [15, 16] </ref>, hence we tend to regard them as being dealt with by a subsystem within the architecture subject of the present work. 3 Active Recognition: Using Uncertainty to Reduce Ambiguity recovered volumetric models with those of a database, using distance measures like dot product [10] or Mahalanobis distance. <p> Along the same line of thought Whaite and Ferrie have proposed a technique of autonomous exploration <ref> [15, 16, 17] </ref> for model-based shape reconstruction from laser rangefinder images. <p> In Section 5 we present experimental demonstrations of these techniques, concluding in Section 6 with a discussion of the results obtained and further research. 2. Autonomous exploration for shape reconstruction Whaite and Ferrie's <ref> [15, 16, 17] </ref> technique for actively modelling the shape, pose and position of an object is based on the following idea: when a new datum x N+1 is added to the dataset fxg, to which model M (x; m N ) has been fitted, and a new optimal 3 In the <p> The controlling software is based on the previous Whaite and Ferrie's autonomous modeller/explorer <ref> [15, 16, 17] </ref>, augmented with a purposely designed Bayesian connectionist simulator and auxiliary software in order to implement the new exploration strategy (class probability estimation, class sensitivity computation, exploration step planning, performance statistics collection, etc).
Reference: [17] <author> P. Whaite and F. P. Ferrie. </author> <title> Autonomous exploration: Driven by uncertainty. </title> <booktitle> In Proceedings, Conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 339-346, </pages> <address> Seattle, Washington, </address> <month> June 21-23 </month> <year> 1994. </year> <institution> Computer Society of the IEEE, IEEE Computer Society Press. Centre for Intelligent Machines, McGill University, 3480 University St., Mon-tre al, </institution> <address> Que., Canada, H3A 2A7 E-mail address: franco@cim.mcgill.ca, ferrie@cim.mcgill.ca 18 </address>
Reference-contexts: Along the same line of thought Whaite and Ferrie have proposed a technique of autonomous exploration <ref> [15, 16, 17] </ref> for model-based shape reconstruction from laser rangefinder images. <p> In Section 5 we present experimental demonstrations of these techniques, concluding in Section 6 with a discussion of the results obtained and further research. 2. Autonomous exploration for shape reconstruction Whaite and Ferrie's <ref> [15, 16, 17] </ref> technique for actively modelling the shape, pose and position of an object is based on the following idea: when a new datum x N+1 is added to the dataset fxg, to which model M (x; m N ) has been fitted, and a new optimal 3 In the <p> The controlling software is based on the previous Whaite and Ferrie's autonomous modeller/explorer <ref> [15, 16, 17] </ref>, augmented with a purposely designed Bayesian connectionist simulator and auxiliary software in order to implement the new exploration strategy (class probability estimation, class sensitivity computation, exploration step planning, performance statistics collection, etc).
References-found: 17

