URL: http://www.research.att.com/~schapire/papers/FreundScSiWa97.ps.Z
Refering-URL: http://www.research.att.com/~schapire/publist.html
Root-URL: 
Email: fyoav, schapire, singerg@research.att.com  manfred@cse.ucsc.edu  
Title: Using and combining predictors that specialize  
Author: Yoav Freund Robert E. Schapire Yoram Singer Manfred K. Warmuth 
Address: 600 Mountain Avenue, Murray Hill, NJ 07974  Santa Cruz, CA 95064  
Affiliation: AT&T Labs  University of California  
Note: Proceedings of the Twenty-Ninth Annual ACM Symposium on the Theory of Computing, 1997.  
Abstract: We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms, sometimes called experts. These simple algorithms belong to the multiplicative weights family of algorithms. The performance of these algorithms degrades only logarithmically with the number of experts, making them particularly useful in applications where the number of experts is very large. However, in applications such as text categorization, it is often natural for some of the experts to abstain from making predictions on some of the instances. We show how to transform algorithms that assume that all experts are always awake to algorithms that do not require this assumption. We also show how to derive corresponding loss bounds. Our method is very general, and can be applied to a large family of online learning algorithms. We also give applications to various prediction models including decision graphs and switching experts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Avrim Blum. </author> <title> Empirical support for winnow and weighted-majority based algorithms: results on a calendar scheduling domain. </title> <booktitle> In ml95, </booktitle> <pages> pages 64-72, </pages> <year> 1995. </year>
Reference-contexts: Furthermore, the dependence of such a bound on the number of experts is only logarithmic, making such algorithms applicable even when the number of experts is enormous. In this paper, we study an extension of the online prediction framework first proposed by Blum <ref> [1] </ref>. The added feature is that we allow experts to abstain from making a pre fl AT&T Labs is planning to move from Murray Hill in 1997. The new address will be: 180 Park Avenue, Florham Park, NJ 07932-0971. <p> We define online learning with specialists as a game that is played between the prediction algorithm and an adversary. We assume that there are N specialists, indexed by f1; : : : ; N g. We assume that predictions and outcomes are real-valued numbers from a bounded range <ref> [0; 1] </ref>. 1 We define a loss function L : [0; 1] fi [0; 1] ! [0; 1) that associates a non-negative loss to each pair of prediction and outcome. <p> We assume that there are N specialists, indexed by f1; : : : ; N g. We assume that predictions and outcomes are real-valued numbers from a bounded range <ref> [0; 1] </ref>. 1 We define a loss function L : [0; 1] fi [0; 1] ! [0; 1) that associates a non-negative loss to each pair of prediction and outcome. The game proceeds in iterations t = 1; : : : ; T , each consisting of the following five steps: 1. <p> We assume that there are N specialists, indexed by f1; : : : ; N g. We assume that predictions and outcomes are real-valued numbers from a bounded range <ref> [0; 1] </ref>. 1 We define a loss function L : [0; 1] fi [0; 1] ! [0; 1) that associates a non-negative loss to each pair of prediction and outcome. The game proceeds in iterations t = 1; : : : ; T , each consisting of the following five steps: 1. <p> Since in most interesting cases the loss function is convex, bounds of this form imply bounds of the previous form but not vice versa. Bounds of this second form are harder to achieve. In his work on predicting using specialists <ref> [1] </ref>, Blum proves a bound on the performance of a variant of the Winnow algorithm [13]. This algorithm is used for making binary predictions and Blum made the additional assumption that a non-empty subset of the specialists never make a mistake. <p> We start with a simple case and then describe a general transformation which we then apply to other, more complex cases. A few preliminaries: Recall that D N denotes the set of probability vectors of dimension N , i.e., D N = fp 2 <ref> [0; 1] </ref> N : i p i = 1g. <p> In this case, the predictions are from the range <ref> [0; 1] </ref>, the out comes are from f0; 1g and the loss is the log loss, or coding length, defined as L ( y; y) = ln y if y = 1 Note that this loss is always nonnegative, but may be infinite (for instance, if y = 0 and y <p> Bayes algorithm is described on the left side of Figure 1. This algorithm maintains a probability vector p t over the N experts. 4 On each round t, each expert i provides a prediction x t;i 2 <ref> [0; 1] </ref>. <p> We focus in this section on algorithms which, like Bayes, maintain a distribution vector p t 2 D N . In general, such algorithms consist of two parts: 1. a prediction function pred N : D N fi <ref> [0; 1] </ref> N ! [0; 1] which maps the current weight vector p t and instance x t to a prediction y t ; and 2. an update function update N : D N fi [0; 1] N fi [0; 1] ! D N which maps the current weight vector p <p> We focus in this section on algorithms which, like Bayes, maintain a distribution vector p t 2 D N . In general, such algorithms consist of two parts: 1. a prediction function pred N : D N fi <ref> [0; 1] </ref> N ! [0; 1] which maps the current weight vector p t and instance x t to a prediction y t ; and 2. an update function update N : D N fi [0; 1] N fi [0; 1] ! D N which maps the current weight vector p t , instance x <p> algorithms consist of two parts: 1. a prediction function pred N : D N fi <ref> [0; 1] </ref> N ! [0; 1] which maps the current weight vector p t and instance x t to a prediction y t ; and 2. an update function update N : D N fi [0; 1] N fi [0; 1] ! D N which maps the current weight vector p t , instance x t and outcome y t to a new weight vector p t+1 . When clear from context, we drop the subscript on pred N and update N . <p> parts: 1. a prediction function pred N : D N fi <ref> [0; 1] </ref> N ! [0; 1] which maps the current weight vector p t and instance x t to a prediction y t ; and 2. an update function update N : D N fi [0; 1] N fi [0; 1] ! D N which maps the current weight vector p t , instance x t and outcome y t to a new weight vector p t+1 . When clear from context, we drop the subscript on pred N and update N . <p> The conversion of such an algorithm to the specialist framework in which some of the experts may be sleeping is fairly straightforward. First, for any nonempty subset E f1; : : : ; N g of awake specialists and instance x 2 <ref> [0; 1] </ref> N , let x E 2 [0; 1] jEj denote the restriction of x to the components of E. <p> First, for any nonempty subset E f1; : : : ; N g of awake specialists and instance x 2 <ref> [0; 1] </ref> N , let x E 2 [0; 1] jEj denote the restriction of x to the components of E. Formally, if E = fi 1 ; : : : ; i jEj g with i 1 &lt; &lt; i jEj then x E j = x i j . <p> Algorithm SAbs Do for t = 1; 2; : : : ; T 1. Predict with: y t = F i2E t p t;i x t;i i2E t p t;i where F : <ref> [0; 1] </ref> ! [0; 1] is any function which satis fies, for all 0 r 1: 1+ 2 ln 2 F (r) 2 ln 2 2. Observe outcome y t and incur loss L ( y t ; y t ) = j y t y t j. 3. <p> Algorithm SAbs Do for t = 1; 2; : : : ; T 1. Predict with: y t = F i2E t p t;i x t;i i2E t p t;i where F : <ref> [0; 1] </ref> ! [0; 1] is any function which satis fies, for all 0 r 1: 1+ 2 ln 2 F (r) 2 ln 2 2. Observe outcome y t and incur loss L ( y t ; y t ) = j y t y t j. 3. <p> For this loss function, it is natural to interpret y 2 <ref> [0; 1] </ref> as a randomized prediction in f0; 1g which is 1 with probability y and 0 otherwise. Then the loss j y yj is the probability of a mistake, and the cumulative loss measures the expected number of mistakes in a sequence of randomized predictions. <p> The interior nodes are associated with tests on the input instance z t . Each interior node has two outgoing edges, one for each possible outcome of the test. Each terminal node is associated with a prediction in <ref> [0; 1] </ref>. The prediction associated with an instance is calculated in the natural way.
Reference: [2] <author> Nicolo Cesa-Bianchi, Yoav Freund, David P. Helmbold, David Haus-sler, Robert E. Schapire, and Manfred K. Warmuth. </author> <title> How to use expert advice. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 382-391, </pages> <year> 1993. </year> <note> To appear, Journal of the Association for Computing Machinery. </note>
Reference-contexts: We now give several applications of this bound for specific loss functions. In addition to those included in this abstract, the method can be applied to many other online algorithms, including all the algorithms derived for the expert setting <ref> [19, 8, 2] </ref>. This is possible because the analysis of all of these algorithms can be rewritten using the relative entropy as a measure of progress. 5 Parameters: Prior distribution p 1 2 D N ; learning rate &gt; 0; number of trials T . <p> Then the loss j y yj is the probability of a mistake, and the cumulative loss measures the expected number of mistakes in a sequence of randomized predictions. For the absolute loss, we can apply the transformation of Section 3.2 to the algorithm of Cesa-Bianchi et al. <ref> [2] </ref> which is based on the work of Vovk [19]. This yields an algorithm that is similar but somewhat more complex than SBayes, which we call SAbs, and which is shown in Figure 3. <p> There are two main differences between SAbs and SBayes. First, SAbs has a parameter &gt; 0, sometimes called a learning rate, that has to be set before the sequence is observed (see Cesa-Bianchi et al. <ref> [2] </ref> for a detailed discussion of how to choose ). Second, the prediction is not a weighted average of the predictions of the experts, but rather a function of this average which also depends on . <p> Second, the prediction is not a weighted average of the predictions of the experts, but rather a function of this average which also depends on . To analyze SAbs, we first rewrite the analysis of this algorithm <ref> [19, 2] </ref> using the notation from Section 3.2. The coefficients in the instantiation of Equation (6) that apply to Parameters: Prior distribution p 1 2 D N ; learning rate &gt; 0; number of trials T .
Reference: [3] <author> William W. Cohen and Yoram Singer. </author> <title> Context-sensitivelearning methods for text categorization. </title> <booktitle> In sigir96, </booktitle> <pages> pages 307-315, </pages> <year> 1996. </year>
Reference-contexts: This leads to very efficient algorithms that can deal with huge vocabularies and make very good predictions. This was demonstrated by Cohen and Singer <ref> [3] </ref> who used one of the specialist algorithms described in this paper for such a text-classification task. Thus, our results generalize a theoretical foundation to an algorithm that has already been shown to be of practical value.
Reference: [4] <author> Thomas M. </author> <title> Cover. Universal portfolios. </title> <journal> Mathematical Finance, </journal> <volume> 1(1) </volume> <pages> 1-29, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: E f1; : : : ; N g, we define u (E) = P 3.1 Log loss One of the simplest and best known online prediction algorithms is the Bayes algorithm, which has been rediscovered many times, for instance, in the context of universal coding, Bayesian estimation and investment management <ref> [4, 6, 7] </ref>.
Reference: [5] <author> Thomas M. Cover and Aaron Shenhar. </author> <title> Compound Bayes predictors for sequences with apparent Markov structure. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-7(6):421-424, </volume> <month> June </month> <year> 1977. </year>
Reference-contexts: We included this example as a simple illustration of the general method. The result is not new; for instance, the same loss bound and time and space complexity can be achieved using a variant of Cover and Shenhar's <ref> [5] </ref> method of parti tioning the data sequence.
Reference: [6] <author> Alfredo DeSantis, George Markowsky, and Mark N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328, </pages> <year> 1988. </year>
Reference-contexts: E f1; : : : ; N g, we define u (E) = P 3.1 Log loss One of the simplest and best known online prediction algorithms is the Bayes algorithm, which has been rediscovered many times, for instance, in the context of universal coding, Bayesian estimation and investment management <ref> [4, 6, 7] </ref>.
Reference: [7] <author> Robert G. Gallager. </author> <title> Information Theory and Reliable Communication. </title> <publisher> John Wiley & Sons, </publisher> <year> 1968. </year>
Reference-contexts: E f1; : : : ; N g, we define u (E) = P 3.1 Log loss One of the simplest and best known online prediction algorithms is the Bayes algorithm, which has been rediscovered many times, for instance, in the context of universal coding, Bayesian estimation and investment management <ref> [4, 6, 7] </ref>.
Reference: [8] <author> David Haussler, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Tight worst-case loss bounds for predicting with expert advice. </title> <booktitle> In Computational Learning Theory: Second European Conference, </booktitle> <volume> Euro-COLT '95, </volume> <pages> pages 69-83. </pages> <publisher> Springer-Verlag, </publisher> <year> 1995. </year>
Reference-contexts: We now give several applications of this bound for specific loss functions. In addition to those included in this abstract, the method can be applied to many other online algorithms, including all the algorithms derived for the expert setting <ref> [19, 8, 2] </ref>. This is possible because the analysis of all of these algorithms can be rewritten using the relative entropy as a measure of progress. 5 Parameters: Prior distribution p 1 2 D N ; learning rate &gt; 0; number of trials T .
Reference: [9] <author> David P. Helmbold, Jyrki Kivinen, and Manfred K. Warmuth. </author> <title> Worst-case loss bounds for sigmoided neurons. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <pages> pages 309-315, </pages> <year> 1995. </year>
Reference-contexts: This conversion also works for all other on-line algorithms derivable from the relative entropy such as the versions of EG where the loss of the algorithm is compared to the loss of the best sigmoided linear neuron <ref> [9] </ref>. 4 Applications In this section, we describe several applications of the spe cialist framework. For concreteness, we focus for each appli cation on a specific loss function.
Reference: [10] <author> David P. Helmbold and Robert E. Schapire. </author> <title> Predicting nearly as well as the best pruning of a decision tree. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 61-68, </pages> <year> 1995. </year>
Reference-contexts: Specifically, we apply this decomposition to the problem of predicting almost as well as the best pruning of a decision graph. This generalizes previous work on predicting almost as well as the best pruning of a decision tree <ref> [21, 10] </ref>. We also apply our methods to the problem of predicting in a model in which the best expert may change with time. <p> This bound is essentially optimal for general decision graphs. In the special case that the decision graph is actually a decision tree, we could instead apply the techniques of Willems, Shtarkov and Tjalkens [21] and Helmbold and Schapire <ref> [10] </ref>.
Reference: [11] <author> Mark Herbster and Manfred Warmuth. </author> <title> Tracking the best expert. </title> <booktitle> In Proceedings of the Twelfth International Conference on Machine Learning, </booktitle> <pages> pages 286-294, </pages> <year> 1995. </year>
Reference-contexts: We also apply our methods to the problem of predicting in a model in which the best expert may change with time. We derive a specialist-based algorithm for this problem that is as fast as the best known algorithm of Herbster and Warmuth <ref> [11] </ref> and achieves almost as good a loss bound. However, unlike their algorithm, ours does not require prior knowledge of the length of the sequence and the number of switches. 2 The specialist framework We now give a formal definition of the framework. <p> Our goal is to perform well relative to the best segmentation. This prediction problem was first studied by Littlestone and Warmuth [14]. Currently, the best of the known algorithms for this problem are due to Herbster and Warmuth <ref> [11] </ref>. Although the algorithms we obtain are just as efficient as theirs (O (N ) time per iteration), our bounds are slightly weaker than Herbster and Warmuth's. However, their algo 8 rithms require estimates of k and T and their bounds degrade as the quality of the estimates degrades.
Reference: [12] <author> Jyrki Kivinen and Manfred K. Warmuth. </author> <title> Additive versus exponentiated gradient updates for linear prediction. </title> <booktitle> In stoc95, </booktitle> <pages> pages 209-218, </pages> <year> 1995. </year> <note> See also technical report UCSC-CRL-94-16, </note> <institution> University of Cal-ifornia, Santa Cruz, Computer Research Laboratory. </institution>
Reference-contexts: This transformation can be applied to a large family of learning problems and algorithms, including all those that fall within Vovk's [18] very general framework of online learning, as well as the algorithms belonging to the exponentiated gradient family of algorithms introduced by Kivinen and Warmuth <ref> [12] </ref>. The feature common to the analysis of all these algorithms is that they use an amortized analysis in which relative entropy is the potential function. In the second part of the paper we show that using specialists is a powerful way for decomposing complex prediction problems. <p> In this section, we show how to get a more powerful bound in terms of L II u (x; y) using a different family of algorithms, called the exponentiated gradient (EG) algorithms. This family was introduced by Kivinen and Warmuth <ref> [12] </ref> and is derived and analyzed using the relative entropy. It thus fits within the framework of Section 3.2. The EG algorithm is similar to the algorithms based on Vovk's work in that they maintain one weight per input and update these weights multiplicatively. <p> Like SAbs, this algorithm has a parameter &gt; 0 that needs to be tuned. At the core of the relative loss bound for EG, there is again an inequality of the form given in Equation (6). Kivinen and Warmuth <ref> [12, Lemma 5.8] </ref> prove that such an inequality holds for a = , b = 2 and L II therefore can apply our general results to obtain the bound T X u (E t )( y t y t ) 2 2 t=1 u (E t )(u E t x t
Reference: [13] <author> Nick Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Bounds of this second form are harder to achieve. In his work on predicting using specialists [1], Blum proves a bound on the performance of a variant of the Winnow algorithm <ref> [13] </ref>. This algorithm is used for making binary predictions and Blum made the additional assumption that a non-empty subset of the specialists never make a mistake. It is assumed that at any iteration at least one of these infallible specialists is awake.
Reference: [14] <author> Nick Littlestone and Manfred K. Warmuth. </author> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 </volume> <pages> 212-261, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms, sometimes called experts. Starting with the work of Vovk [19] and Littlestone and Warmuth <ref> [14] </ref>, many algorithms have been developed in recent years which use multiplicative weight updates. These algorithms enjoy theoretical performance guarantees which can be proved without making any statistical assumptions. <p> The sequence of segments and its associated sequence of best experts is called a segmentation. Our goal is to perform well relative to the best segmentation. This prediction problem was first studied by Littlestone and Warmuth <ref> [14] </ref>. Currently, the best of the known algorithms for this problem are due to Herbster and Warmuth [11]. Although the algorithms we obtain are just as efficient as theirs (O (N ) time per iteration), our bounds are slightly weaker than Herbster and Warmuth's.
Reference: [15] <author> Neri Merhav, Meir Feder, and Michael Gutman. </author> <title> Some properties of sequential predictors for binary Markov sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 39(3) </volume> <pages> 887-892, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We call the decision graph that is derived in such a way a pruning of the original decision graph. An important example is the well-studied <ref> [16, 20, 15, 17, 21] </ref> variable-length Markov model, in which the order of decisions is fixed in advance, but the depth of the decision process might depend on the instance.
Reference: [16] <author> Jorma Rissanen and Glen G. Langdon, Jr. </author> <title> Universal modeling and coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-27(1):12-23, </volume> <month> January </month> <year> 1981. </year>
Reference-contexts: We call the decision graph that is derived in such a way a pruning of the original decision graph. An important example is the well-studied <ref> [16, 20, 15, 17, 21] </ref> variable-length Markov model, in which the order of decisions is fixed in advance, but the depth of the decision process might depend on the instance.
Reference: [17] <author> Dana Ron, Yoram Singer, and Naftali Tishby. </author> <title> Learning probabilistic automata with variable memory length. </title> <booktitle> In Proceedings of the Seventh Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 35-46, </pages> <year> 1994. </year>
Reference-contexts: We call the decision graph that is derived in such a way a pruning of the original decision graph. An important example is the well-studied <ref> [16, 20, 15, 17, 21] </ref> variable-length Markov model, in which the order of decisions is fixed in advance, but the depth of the decision process might depend on the instance.
Reference: [18] <author> V. G. Vovk. </author> <title> A game of prediction with expert advice. </title> <booktitle> In Proceedings of the Eighth Annual Conference on Computational Learning Theory, </booktitle> <year> 1995. </year>
Reference-contexts: This transformation can be applied to a large family of learning problems and algorithms, including all those that fall within Vovk's <ref> [18] </ref> very general framework of online learning, as well as the algorithms belonging to the exponentiated gradient family of algorithms introduced by Kivinen and Warmuth [12]. <p> While some results can be presented in the much more general online prediction framework of Vovk <ref> [18] </ref>, we chose to simplify this paper by making these more restrictive choices. iteration. In order to give a meaningful bound, we consider the difference between the total loss of the algorithm and the total loss of the experts.
Reference: [19] <author> Volodimir G. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction We study online learning algorithms that predict by combining the predictions of several subordinate prediction algorithms, sometimes called experts. Starting with the work of Vovk <ref> [19] </ref> and Littlestone and Warmuth [14], many algorithms have been developed in recent years which use multiplicative weight updates. These algorithms enjoy theoretical performance guarantees which can be proved without making any statistical assumptions. <p> We now give several applications of this bound for specific loss functions. In addition to those included in this abstract, the method can be applied to many other online algorithms, including all the algorithms derived for the expert setting <ref> [19, 8, 2] </ref>. This is possible because the analysis of all of these algorithms can be rewritten using the relative entropy as a measure of progress. 5 Parameters: Prior distribution p 1 2 D N ; learning rate &gt; 0; number of trials T . <p> For the absolute loss, we can apply the transformation of Section 3.2 to the algorithm of Cesa-Bianchi et al. [2] which is based on the work of Vovk <ref> [19] </ref>. This yields an algorithm that is similar but somewhat more complex than SBayes, which we call SAbs, and which is shown in Figure 3. Like SBayes, SAbs maintains a weight for each specialist which it updates by multiplicative factors after each iteration. <p> Second, the prediction is not a weighted average of the predictions of the experts, but rather a function of this average which also depends on . To analyze SAbs, we first rewrite the analysis of this algorithm <ref> [19, 2] </ref> using the notation from Section 3.2. The coefficients in the instantiation of Equation (6) that apply to Parameters: Prior distribution p 1 2 D N ; learning rate &gt; 0; number of trials T . <p> Using the algorithm for on-line prediction with square loss described by Vovk <ref> [19] </ref>, we can derive an algorithm whose bound is in terms of the comparison loss L I u (x; y). <p> Without applying the specialist framework, we could use, for instance, Vovk's <ref> [19] </ref> (insomniac) expert-prediction algorithm in which we maintain one expert for each table-lookup function. Naively, this would require maintenance of 2 2 k weights, all of which must be updated on every trial.
Reference: [20] <author> Marcelo J. Weinberger, Abraham Lempel, and Jacob Ziv. </author> <title> A sequential algorithm for the universal coding of finite-memory sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38(3) </volume> <pages> 1002-1014, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: We call the decision graph that is derived in such a way a pruning of the original decision graph. An important example is the well-studied <ref> [16, 20, 15, 17, 21] </ref> variable-length Markov model, in which the order of decisions is fixed in advance, but the depth of the decision process might depend on the instance.
Reference: [21] <author> Frans M. J. Willems, Yuri M. Shtarkov, and Tjalling J. Tjalkens. </author> <title> The context tree weighting method: basic properties. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 41(3) </volume> <pages> 653-664, </pages> <year> 1995. </year> <month> 10 </month>
Reference-contexts: Specifically, we apply this decomposition to the problem of predicting almost as well as the best pruning of a decision graph. This generalizes previous work on predicting almost as well as the best pruning of a decision tree <ref> [21, 10] </ref>. We also apply our methods to the problem of predicting in a model in which the best expert may change with time. <p> We call the decision graph that is derived in such a way a pruning of the original decision graph. An important example is the well-studied <ref> [16, 20, 15, 17, 21] </ref> variable-length Markov model, in which the order of decisions is fixed in advance, but the depth of the decision process might depend on the instance. <p> This bound is essentially optimal for general decision graphs. In the special case that the decision graph is actually a decision tree, we could instead apply the techniques of Willems, Shtarkov and Tjalkens <ref> [21] </ref> and Helmbold and Schapire [10].
References-found: 21

