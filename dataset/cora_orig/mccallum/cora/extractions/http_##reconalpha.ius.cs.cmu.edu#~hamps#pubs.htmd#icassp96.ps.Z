URL: http://reconalpha.ius.cs.cmu.edu/~hamps/pubs.htmd/icassp96.ps.Z
Refering-URL: http://reconalpha.ius.cs.cmu.edu/~hamps/technoTopics.htmd/icassp96.htmd/index.html
Root-URL: 
Email: hamps@ece.cmu.edu  dwatola@bvd.jpl.nasa.gov  
Title: DIAGNOSING AND CORRECTING SYSTEM ANOMALIES WITH A ROBUST CLASSIFIER  
Author: J. B. Hampshire II D. A. Watola 
Address: Pittsburgh, PA 15213-3890  4800 Oak Grove Drive Pasadena, CA 91109-8099  
Affiliation: Dept. of Electrical Computer Engineering Carnegie Mellon Universtiy  Jet Propulsion Laboratory, 238-420 California Institute of Technology  
Date: MAY, 1996.  
Note: REPRINTED FROM THE IEEE PROCEEDINGS OF THE 1996 INTERNATIONAL CONFERENCE OF ACOUSTICS, SPEECH, AND SIGNAL PROCESSING,  
Abstract: If a robust statistical model has been developed to classify the ``health'' of a system, a well-known Taylor series approximation technique forms the basis of a diagnostic/recovery procedure that can be initiated when the system's health degrades or fails altogether. This procedure determines a ranked set of probable causes for the degraded health state, which can be used as a prioritized checklist for isolating system anomalies and quantifying corrective action. The diagnostic/recovery procedure is applicable to any classifier known to be robust; it can be applied to both neural network and traditional parametric pattern classifiers generated by a supervised learning procedure in which an empirical risk/benefit measure is optimized. We describe the procedure mathematically and demonstrate its ability to detect and diagnose the cause(s) of faults in NASA's Deep Space Communications Complex at Goldstone, California. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. B. </author> <title> Hampshire II. A Differential Theory of Learning for Efficient Statistical Pattern Recognition. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, Department of Electrical & Computer Engineering, Hammerschlag Hall, </institution> <address> Pittsburgh, PA 15213-3890, </address> <month> September </month> <year> 1993. </year> <note> An abridged version is available via anonymous ftp from speech1.cs.cmu.edu (128.2.254.145): retrieve the file README for directions. </note>
Reference-contexts: The analytical procedure for determining the cause is fl THIS RESEARCH WAS FUNDED BY THE NATIONAL AERONAUTICS AND SPACE ADMINISTRATION VIA THE JET PROPULSION LABORATORY'S OFFICE OF TELECOMMUNICATIONS AND DATA ACQUISITION, RTOP-310-30-72. 1 The classifier need not be a good probabilistic model to be robust; see, <ref> [1, ch. 2] </ref>. based on the following simple notion: since the classifier expresses a unique mapping from feature vector space to health state space, we need only invert this mapping to determine the cause of an anomaly. Actually, the process is somewhat more complicated than a simple function inversion. <p> The formalism described above can be paired with many empirical risk/benefit measures. Our implementation uses the synthetic Classification Figure of Merit (CFM) objective function, which engenders a provably efficient, minimum-complexity form of learning known as Differential Learning <ref> [1, 2] </ref>. Consequently, we refer to our implementation of the diagnosis procedure described above as Differential Discriminative Diagnosis. A few observations regarding computational efficiency and robustness are worth mentioning here.
Reference: [2] <author> J. B. Hampshire II and B. V. K. Vijaya Kumar. </author> <title> Differentially Generated Neural Network Classifiers are Efficient. </title> <editor> In C. A. Kamm, G. M. Kuhn, B. Yoon, R. Chellappa, and S. Y. Kung, editors, </editor> <booktitle> Neural Networks for Signal Processing III: Proceedings of the 1993 IEEE Workshop, pages 151--160, </booktitle> <address> New York, </address> <month> September </month> <year> 1993. </year> <institution> The Institute of Electrical and Electronic Engineers, Inc. </institution>
Reference-contexts: The formalism described above can be paired with many empirical risk/benefit measures. Our implementation uses the synthetic Classification Figure of Merit (CFM) objective function, which engenders a provably efficient, minimum-complexity form of learning known as Differential Learning <ref> [1, 2] </ref>. Consequently, we refer to our implementation of the diagnosis procedure described above as Differential Discriminative Diagnosis. A few observations regarding computational efficiency and robustness are worth mentioning here.
Reference: [3] <author> B. Hassibi and D. G. Stork. </author> <title> Second-order derivatives for network pruning: Optimal brain surgeon. </title> <editor> In S. J. Hanson, J. D. Cowan, and C. L. Giles, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 5, </volume> <pages> pages 164--171. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: THE MATHEMATICAL FORMALISM FOR DIAGNOSIS AND CORRECTION Readers familiar with the Taylor series expansion that forms the basis of most optimization procedures notably the backpropagation supervised learning procedure [6, 7] and the optimal brain damage (OBD)/optimal brain surgeon (OBS) complexity reduction procedures <ref> [4, 3] </ref> in the case of neural network classifiers will recognize it below: when the system's health is anomalous, the expansion can be used during diagnosis to determine the relevance of causal factors to which the health anomaly is responsive, just as the OBD procedure can be used to determine the
Reference: [4] <author> Y. LeCun, J. Denker, and S. Solla. </author> <title> Optimal brain damage. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> vol. 2, </volume> <pages> pages 598--605. </pages> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: THE MATHEMATICAL FORMALISM FOR DIAGNOSIS AND CORRECTION Readers familiar with the Taylor series expansion that forms the basis of most optimization procedures notably the backpropagation supervised learning procedure [6, 7] and the optimal brain damage (OBD)/optimal brain surgeon (OBS) complexity reduction procedures <ref> [4, 3] </ref> in the case of neural network classifiers will recognize it below: when the system's health is anomalous, the expansion can be used during diagnosis to determine the relevance of causal factors to which the health anomaly is responsive, just as the OBD procedure can be used to determine the
Reference: [5] <author> B. A. Pearlmutter. </author> <title> Fast Exact Multiplication by the Hessian. </title> <journal> Neural Computation, </journal> <volume> 6(1):147 -- 160, </volume> <month> January </month> <year> 1994. </year>
Reference-contexts: A few observations regarding computational efficiency and robustness are worth mentioning here. The second-order (Hessian) computations of (2) are done efficiently using Pearlmutter's method for computing the product of an m fi m matrix and an m dimensional vector with only O [ m ] computations <ref> [5] </ref>. The objective function's change, approximated to second order in (2) and (3), can be evaluated explicitly. If the explicit value of the change matches the approximated value, we have added confidence that the relevance terms yielded by the approximation reflect reality a simple yet effective cross-check of the diagnosis.
Reference: [6] <author> D. E. Rumelhart, G. E. Hinton, and R. J. Williams. </author> <title> Learning Representations by Backpropagation Errors. </title> <booktitle> Nature, </booktitle> <address> 323:533 -536, </address> <month> October </month> <year> 1986. </year>
Reference-contexts: THE MATHEMATICAL FORMALISM FOR DIAGNOSIS AND CORRECTION Readers familiar with the Taylor series expansion that forms the basis of most optimization procedures notably the backpropagation supervised learning procedure <ref> [6, 7] </ref> and the optimal brain damage (OBD)/optimal brain surgeon (OBS) complexity reduction procedures [4, 3] in the case of neural network classifiers will recognize it below: when the system's health is anomalous, the expansion can be used during diagnosis to determine the relevance of causal factors to which the health
Reference: [7] <editor> D. E. Rumelhart, J. L. McClelland, et al. </editor> <booktitle> Parallel Distributed Processing, </booktitle> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: THE MATHEMATICAL FORMALISM FOR DIAGNOSIS AND CORRECTION Readers familiar with the Taylor series expansion that forms the basis of most optimization procedures notably the backpropagation supervised learning procedure <ref> [6, 7] </ref> and the optimal brain damage (OBD)/optimal brain surgeon (OBS) complexity reduction procedures [4, 3] in the case of neural network classifiers will recognize it below: when the system's health is anomalous, the expansion can be used during diagnosis to determine the relevance of causal factors to which the health
References-found: 7

