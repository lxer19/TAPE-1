URL: http://www.cs.umn.edu/Research/Agassiz/Paper/jin.thrashing.ps.Z
Refering-URL: http://www.cs.umn.edu/Research/Agassiz/agassiz_pubs.html
Root-URL: http://www.cs.umn.edu
Title: An Efficient Solution to the Cache Thrashing Problem  
Author: Guohua Jin Zhiyuan Li Fujie Chen 
Keyword: Multiprocesors, cache thrashing, parallel threads, loop transformations, parallelizing compilers.  
Note: Work supported by a National Science Foundation CAREER Award.  Work sup ported by a grant from the National Natural Science Foundation, China.  
Address: 200 Union Street S.E., Minneapolis, MN 55455, USA.  Changsha, Hunan 410073, CHINA.  
Affiliation: University of Minnesota, Department of Computer Science,  Changsha Institute of Technology, Department of Computer Science,  
Date: March 4, 1996  
Abstract: When parallel programs are executed on multiprocessors with private caches, a set of data may be repeatedly used and modified by different threads. Such data sharing can often result in cache thrashing which degrades the memory performance. This paper presents and evaluates a method to eliminate or reduce such cache thrashing due to true data sharing in nested parallel loops. With this method, the staggering relation and compacting relation are determined and accordingly the nested loop are restructured at compile time. Due to its simplicity, the method can be efficiently implemented in any parallel compiler. Experimental results show quite significant performance improvement over existing static and dynamic scheduling methods. fl University of Minnesota, Department of Computer Science, 200 Union Street S.E., Minneapolis, MN 55455, USA, or Changsha Institute of Technology, Department of Computer Science, Changsha, Hunan 410073, CHINA. Work supported by a grant from the National Natural Science Foundation, China. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Abraham and D. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherence traffic. </title> <journal> IEEE Transactions on Parallel Distributed Systems, </journal> <volume> 2(3), </volume> <month> July </month> <year> 1991. </year>
Reference-contexts: Hudak and Abraham <ref> [1, 17] </ref> developed a static partitioning approach called adaptive data partitioning (ADP) to reduce interprocessor communication for iterative data parallel loops. In their work, loops are also assumed to be perfectly nested.
Reference: [2] <author> W. Abu-Sufah, D. Kuck, and D. Lawrie. </author> <title> On the performance enhancement of paging systems through program analysis and transformations. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-30(5), </volume> <month> May </month> <year> 1981. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references <ref> [2] </ref>. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. Gallivan, Jalby and Gannon defined a reference window for a dependence as the variables referenced by both the source and the sink of the dependence [14, 15].
Reference: [3] <author> J. R. Allen and K. Kennedy. </author> <title> Automatic loop interchange. </title> <booktitle> In Proceedings of the SIGPLAN'84 Symposium on Compiler Construction, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1984. </year> <month> 24 </month>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [10, 11, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [5, 23, 3, 24] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [4] <author> J. Baer and W.Wang. </author> <title> Multilevel cache hierarchies: organizations, protocols, and performance. </title> <journal> Journal of Parallel and Distributed Computing, Vol.6, </journal> <volume> pp.451-476, </volume> <year> 1989. </year>
Reference-contexts: 1 Introduction Parallel processing systems with memory hierarchies become quite common today. A cache is usually used to bridge the speed gap between the processor and the main memory in hardware design. Most multiprocessor systems have a local cache in each processor, and some of them use multi-level caches <ref> [4, 13] </ref>. Very often, a copy-back snoopy cache protocol is employed to maintain cache coherence in these multiprocessor systems. Certain supercomputers also use a local memory which can be viewed as a program controlled cache.
Reference: [5] <author> U. Banerjee. </author> <title> Dependence analysis for supercomputing. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [10, 11, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [5, 23, 3, 24] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [6] <author> D. Callahan, S. Carr, and K. Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN'90 Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> After executing the source of the dependence, saving the associated reference window in the cache until the sink had been executed might improve the number of cache hits. Carr, Callahan and Kennedy <ref> [6, 7] </ref> discussed options for compiler control of a uniprocessor memory hierarchy. Wolf and Lam developed an algorithm that estimates all temporal and spatial reuse for a given loop permutation [33]. These optimizations all attempt to maximize the reuse of cached data on a single processor.
Reference: [7] <author> S. Carr and K. Kennedy. </author> <title> Compiling scientific code for complex memory hierarchies. </title> <booktitle> In Proceedings of Hawaii International Conference on System Sciences, </booktitle> <address> pp.536-544, </address> <year> 1991. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> After executing the source of the dependence, saving the associated reference window in the cache until the sink had been executed might improve the number of cache hits. Carr, Callahan and Kennedy <ref> [6, 7] </ref> discussed options for compiler control of a uniprocessor memory hierarchy. Wolf and Lam developed an algorithm that estimates all temporal and spatial reuse for a given loop permutation [33]. These optimizations all attempt to maximize the reuse of cached data on a single processor.
Reference: [8] <author> E. D'Hollander. </author> <title> Partitioning and labeling of loops by unimodular transformations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(4), </volume> <month> July </month> <year> 1992. </year>
Reference-contexts: In contrast, our work considers a multiprocessor environment where each processor has its own local cache or local memory and different processors may share data. Peir and Cytron [28], and Shang and Fortes [30], and D'Hollander's work <ref> [8] </ref> shared a common goal of partitioning an index set into independent execution sets so that each of which could run on a distinct processor without interprocessor communication.
Reference: [9] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating False Sharing. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Unlike these previous works, our work 4 focuses on the nested constructs in which loops are not necessarily perfectly nested , loop bounds can be array variables, and array subscript expressions can be of more general forms. Many researchers have studied the cache false sharing problem <ref> [9, 16, 18, 32] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although they do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [10] <author> J. Fang and M. Lu. </author> <title> A solution of cache ping-pong problem in RISC based parallel processing systems. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: J. Fang and M. Lu studied a large number of programs including the LINPACK benchmarks, the PERFECT Club benchmarks, and programs for mechanical CAE, computational chemistry, image and signal processing, and petroleum applications <ref> [10] </ref>, and they reported that almost all of the most time-consuming loop nests contain at least three loop levels, about 60% of which contain at least one parallel loop. <p> We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Our work considers the cache thrashing problem which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [10, 11, 26, 12] </ref>. In their work, the iteration space is partitioned into a set of equivalent classes, and each processor uses a formula to determine which iterations belong to the same equivalent class at execution time. Each processor then executes the corresponding iterations so as to eliminate cache thrashing. <p> We then restructure the nested loop to eliminate cache thrashing due to true data sharing. We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [10, 11, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [5, 23, 3, 24]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 . <p> Fang and Lu <ref> [10] </ref> reported that arrays involved in nested loops are usually two-dimensional, or three-dimensional with a small size in the third dimension which can be treated as a small number of two-dimensional arrays.
Reference: [11] <author> Z. Fang. </author> <title> Cache or local memory thrashing and compiler strategy in parallel processing systems. </title> <booktitle> In Proceedings of 1990 International Conference on Parallel Processing, </booktitle> <address> pp.271-275, </address> <month> Aug. </month> <year> 1990. </year>
Reference-contexts: If the threads of each list are assigned to different processors, the data of array A are unnecessarily moved back and forth between caches in the system, the true sharing thrashing problem <ref> [11] </ref> arises. <p> We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Our work considers the cache thrashing problem which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [10, 11, 26, 12] </ref>. In their work, the iteration space is partitioned into a set of equivalent classes, and each processor uses a formula to determine which iterations belong to the same equivalent class at execution time. Each processor then executes the corresponding iterations so as to eliminate cache thrashing. <p> We then restructure the nested loop to eliminate cache thrashing due to true data sharing. We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [10, 11, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [5, 23, 3, 24]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [12] <author> J. Fang and M. Lu. </author> <title> An iteration partition approach for cache or local memory thrashing on parallel processing. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-42, </volume> <month> May </month> <year> 1993. </year>
Reference-contexts: Our work considers the cache thrashing problem which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [10, 11, 26, 12] </ref>. In their work, the iteration space is partitioned into a set of equivalent classes, and each processor uses a formula to determine which iterations belong to the same equivalent class at execution time. Each processor then executes the corresponding iterations so as to eliminate cache thrashing.
Reference: [13] <author> M. Galles and E. Williams. </author> <title> Performance optimizations, implementation, and verification of the SGI Challenge multiprocessor. </title> <type> Technical Report. </type>
Reference-contexts: 1 Introduction Parallel processing systems with memory hierarchies become quite common today. A cache is usually used to bridge the speed gap between the processor and the main memory in hardware design. Most multiprocessor systems have a local cache in each processor, and some of them use multi-level caches <ref> [4, 13] </ref>. Very often, a copy-back snoopy cache protocol is employed to maintain cache coherence in these multiprocessor systems. Certain supercomputers also use a local memory which can be viewed as a program controlled cache.
Reference: [14] <author> K. Gallivan, W. Jalby, and D. Gannon. </author> <title> On the problem of optimizing data transfers for complex memory systems. </title> <booktitle> In Proceedings of Supercomputing'88, </booktitle> <address> pp.238-253, </address> <year> 1988. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. Gallivan, Jalby and Gannon defined a reference window for a dependence as the variables referenced by both the source and the sink of the dependence <ref> [14, 15] </ref>. After executing the source of the dependence, saving the associated reference window in the cache until the sink had been executed might improve the number of cache hits. Carr, Callahan and Kennedy [6, 7] discussed options for compiler control of a uniprocessor memory hierarchy.
Reference: [15] <author> D. Gannon, W. Jalby, and K. Gallivan. </author> <title> Strategies for cache and local memory management by global program transformation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol.5, </volume> <year> 1988. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. Gallivan, Jalby and Gannon defined a reference window for a dependence as the variables referenced by both the source and the sink of the dependence <ref> [14, 15] </ref>. After executing the source of the dependence, saving the associated reference window in the cache until the sink had been executed might improve the number of cache hits. Carr, Callahan and Kennedy [6, 7] discussed options for compiler control of a uniprocessor memory hierarchy.
Reference: [16] <author> M. Gupta and D. Padua. </author> <title> Effects of program parallelization and stripmining transformation on cache performance in a multiprocessor. </title> <booktitle> In Proceedings of the 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1991. </year>
Reference-contexts: Unlike these previous works, our work 4 focuses on the nested constructs in which loops are not necessarily perfectly nested , loop bounds can be array variables, and array subscript expressions can be of more general forms. Many researchers have studied the cache false sharing problem <ref> [9, 16, 18, 32] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although they do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [17] <author> D. Hudak and S. Abraham. </author> <title> Compiler techniques for data partitioning of sequentially iterated paralle loops. </title> <booktitle> In Proceedings of ACM International Conference on Supercomputing, </booktitle> <pages> pp. 187-200, </pages> <year> 1990. </year>
Reference-contexts: Hudak and Abraham <ref> [1, 17] </ref> developed a static partitioning approach called adaptive data partitioning (ADP) to reduce interprocessor communication for iterative data parallel loops. In their work, loops are also assumed to be perfectly nested.
Reference: [18] <author> T. E. Jeremiassen and S. J. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile-time data transformations. </title> <booktitle> In Proceedings of Fifth ACM SIGPLAN Symposium on Principals and Practice of Paralle Programming, </booktitle> <pages> pp. 179-188, </pages> <year> 1995. </year>
Reference-contexts: Unlike these previous works, our work 4 focuses on the nested constructs in which loops are not necessarily perfectly nested , loop bounds can be array variables, and array subscript expressions can be of more general forms. Many researchers have studied the cache false sharing problem <ref> [9, 16, 18, 32] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although they do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [19] <author> G. Jin and F. Chen. </author> <title> The design and the implementation of a knowledge-based parallelizing tool. </title> <booktitle> In Proceedings of the 2nd IES Information Technology Conference, </booktitle> <month> July </month> <year> 1991, </year> <institution> Singapore. </institution>
Reference-contexts: Cache thrashing due to true data sharing is thus eliminated. 20 5 Experimental Results The thread alignment techniques described in this paper have been implemented as backend optimizations in KD-PARPRO <ref> [19] </ref>, a knowledge-based parallelizing tool which can perform intra- and inter-procedural data dependence analysis and a large number of parallelizing transformations including loop interchange, loop distribution, loop skewing, cycle shrinking, for FORTRAN programs.
Reference: [20] <author> G. Jin, Z. Li, and F. Chen. </author> <title> An efficient solution to the cache thrashing problem (Extended Version). </title> <type> Technical Report TR 96-020, </type> <institution> Department of Computer Science, University of Minnesota, </institution> <year> 1996. </year>
Reference-contexts: Proof: <ref> [20] </ref>. In order to find the threads which have data dependence relations with thread T ij , we make the following definition. <p> In Figure 6 below we give the more general restructuring result for loops that may be imperfectly nested. For the sake of brevity, we consider L 2 6= 0 only. The code for L 2 = 0 is quite 9 similar <ref> [20] </ref>. There can be different ways to assign the columns in the RSRIS to the processors. <p> Proof: <ref> [20] </ref>. <p> Proof: <ref> [20] </ref>. <p> If the given loop nest is not perfectly nested, then the resulting code has two variants, one for g 6= 0 and the other for g 0 =0 . We show the code for g 0 6= 0 in Figure 10. The code for g 0 similar <ref> [20] </ref>. LB 1 and LB 2 are the lower bounds of I and J , U B 2 is the upper bound of the loop J, (g; g ) and d are the unified staggering parameter and the compacting parameter determined above. <p> (i 0 0 0 0 0 0 ) in the reduced iteration space, if there exist integers r 1 ; r 2 ; :::; r m , not all zero, such that k = j + fl=1 m X r fl L fl1 = 0 then S 0 0 Proof: <ref> [20] </ref>. Lemma 8: Given iterations (i; j) and (i 0 0 ) in the reduced iteration space, if (i 0 0 0 ij then S ij = S i 0 j 0 . Proof: [20]. <p> j + fl=1 m X r fl L fl1 = 0 then S 0 0 Proof: <ref> [20] </ref>. Lemma 8: Given iterations (i; j) and (i 0 0 ) in the reduced iteration space, if (i 0 0 0 ij then S ij = S i 0 j 0 . Proof: [20]. Theorem 9: Given staggering parameters (L 11 ; L 12 ); (L 21 ; L 22 ); :::; (L m1 ; L m2 ), we have S ij = S ij for any iteration (i; j) in the reduced iteration space. Proof: [20]. <p> Proof: <ref> [20] </ref>. Theorem 9: Given staggering parameters (L 11 ; L 12 ); (L 21 ; L 22 ); :::; (L m1 ; L m2 ), we have S ij = S ij for any iteration (i; j) in the reduced iteration space. Proof: [20]. Now we show why the compacting parameter defined above can be used together with a unified staggering parameter to form the iterations in each iteration set S 0 ij defined in Definition 8. We first introduce a lemma. <p> L i fl 1 = 0 (r 1 &gt; 0), then r 1 r 0 19 For any integers r 00 00 00 j satisfying j X r fl L i fl 1 = 0 (r 1 &gt; 0) there exists an integer k 1 such that r 00 Proof: <ref> [20] </ref>. <p> determined by Algorithm 1, and d 0 m P r fl L fl2 , where r 1 ; r 2 ; :::; r j are integers, not all zero, which satisfy m X r fl L fl1 = 0 then there exists an integer k such that d 0 Proof: <ref> [20] </ref>. Theorems 9 and 10 have shown that although the unified staggering parameters may not be unique, the final result of staggering and compacting is unique. Therefore the compacting factor d is also unique. <p> Proof: <ref> [20] </ref>. EXAMPLE: Consider a nested loop of the form described at the beginning of this subsection, with two linear functions ~ h 1 (i; j; k) = (j2i; j+3k) and ~ h 2 (i; j; k) = (i+j; j+k) in the loop body.
Reference: [21] <author> G. Jin and F. Chen. </author> <title> Loop restructuring techniques for thrashing problem. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Architectures and Languages Europe, </booktitle> <year> 1992. </year> <month> 25 </month>
Reference-contexts: Our analysis can also be extended to incorporate more machine parameters such as the levels of caches and the cache line size. 6 4 Solutions In this section, we begin with a simple case where all array subscripts in the loop body use the same linear function <ref> [21, 22] </ref>. We then discuss the more general case which allows multiple subscript functions and imperfectly nested loop with variable bounds. In both cases, the iteration space is staggered, regularized and compacted into a set of independent equivalent classes. The loop is restructured according to the rearranged iteration space. <p> Both perfectly and imperfectly nested loop can be restructured by rearranging the reduced iteration spaces described above. Code generation for perfectly nested loops with invariant bounds has been discussed in <ref> [21, 22] </ref>. In Figure 6 below we give the more general restructuring result for loops that may be imperfectly nested. For the sake of brevity, we consider L 2 6= 0 only. The code for L 2 = 0 is quite 9 similar [20]. <p> Chunks that are N P chunks apart are then assigned to the same processor. If the parallel loop is tightly nested in the outer sequential loop, then the load can be further balanced by compacting the RSRIS <ref> [21, 22] </ref>. With compacting, the RSRIS is first partitioned into multiple segments such that there are N 2 columns within each segment except the last one as shown in Figure 5 (c). These segments are then pieced together to form a rectangle.
Reference: [22] <author> G. Jin, X. Yang, and F. Chen. </author> <title> Loop staggering, loop staggering and loop compacting: restructuring techniques for thrashing problem. </title> <booktitle> In Proceedings of 1991 International Conference on Parallel Processing, </booktitle> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: Our analysis can also be extended to incorporate more machine parameters such as the levels of caches and the cache line size. 6 4 Solutions In this section, we begin with a simple case where all array subscripts in the loop body use the same linear function <ref> [21, 22] </ref>. We then discuss the more general case which allows multiple subscript functions and imperfectly nested loop with variable bounds. In both cases, the iteration space is staggered, regularized and compacted into a set of independent equivalent classes. The loop is restructured according to the rearranged iteration space. <p> Both perfectly and imperfectly nested loop can be restructured by rearranging the reduced iteration spaces described above. Code generation for perfectly nested loops with invariant bounds has been discussed in <ref> [21, 22] </ref>. In Figure 6 below we give the more general restructuring result for loops that may be imperfectly nested. For the sake of brevity, we consider L 2 6= 0 only. The code for L 2 = 0 is quite 9 similar [20]. <p> Chunks that are N P chunks apart are then assigned to the same processor. If the parallel loop is tightly nested in the outer sequential loop, then the load can be further balanced by compacting the RSRIS <ref> [21, 22] </ref>. With compacting, the RSRIS is first partitioned into multiple segments such that there are N 2 columns within each segment except the last one as shown in Figure 5 (c). These segments are then pieced together to form a rectangle.
Reference: [23] <author> D. Kuck. </author> <title> The structure of computers and computations. Vol.1, </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [10, 11, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [5, 23, 3, 24] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [24] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symposium on Principle of Programming Languages (POPL), </booktitle> <year> 1981. </year>
Reference-contexts: We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations [10, 11, 26]. 3 Basic Concepts and Assumptions Data dependences between statements are defined in <ref> [5, 23, 3, 24] </ref>. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [25] <author> B. Leasure, et al. </author> <title> PCF Fortran: </title> <booktitle> language definition(version 1). The Parallel Computing Forum, </booktitle> <publisher> Kuck & Associates Inc., </publisher> <month> Aug. </month> <year> 1988. </year>
Reference-contexts: Certain supercomputers also use a local memory which can be viewed as a program controlled cache. When programs with parallel nested loops are executed on such parallel processing systems, a set of data may be used and modified by different threads <ref> [25] </ref> of the nested loop.
Reference: [26] <author> M. Lu and J. Fang. </author> <title> A solution of the cache ping-pong problem in multiprocessor systems. </title> <journal> Journal of Parallel and Distributed Computing 16, </journal> <month> Oct. </month> <year> 1992. </year>
Reference-contexts: Our work considers the cache thrashing problem which is due to the true sharing of data words. Our work is most closely related to the research done by Fang and Lu <ref> [10, 11, 26, 12] </ref>. In their work, the iteration space is partitioned into a set of equivalent classes, and each processor uses a formula to determine which iterations belong to the same equivalent class at execution time. Each processor then executes the corresponding iterations so as to eliminate cache thrashing. <p> We then restructure the nested loop to eliminate cache thrashing due to true data sharing. We have performed experiments on a commercial multiprocessor, namely a Silicon Graphics Challenge Cluster, thereby we obtain real data regarding cache thrashing and its reduction. In contrast, previous data were mainly from simulations <ref> [10, 11, 26] </ref>. 3 Basic Concepts and Assumptions Data dependences between statements are defined in [5, 23, 3, 24]. If a statement S 1 uses the result of another statement S 2 , then S 1 is flow dependent on S 2 .
Reference: [27] <author> I. Nivan, et al. </author> <title> An introduction to the theory of numbers. </title> <publisher> John Wiley & Sons, </publisher> <address> Fourth Edition New York, Chichester Brisbane Toronto, </address> <year> 1980. </year>
Reference-contexts: following rules: 13 (1) If 8 (j; k) such that 1 j; k m and j 6= k, we have L k1 L k2 L j1 L j2 , then we call (g; L 12 L 11 g) the unified staggering parameter. (2) Otherwise, according to the theory of numbers <ref> [27] </ref>, there exist integers a 1 , a 2 , ..., a m that satisfy g = fl=1 Let g = fl=1 a fl L fl2 . We call (g; g 0 ) a unified staggering parameter.
Reference: [28] <author> J. Peir and R. Cytron. </author> <title> Minimum distance: a method for partitioning recurrences for multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-38, </volume> <month> Aug. </month> <year> 1989. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> In contrast, our work considers a multiprocessor environment where each processor has its own local cache or local memory and different processors may share data. Peir and Cytron <ref> [28] </ref>, and Shang and Fortes [30], and D'Hollander's work [8] shared a common goal of partitioning an index set into independent execution sets so that each of which could run on a distinct processor without interprocessor communication.
Reference: [29] <author> C. D. Polychronopoulos and D. Kuck. </author> <title> Guided self-scheduling: a practical scheduling scheme for parallel supercomputers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36, </volume> <month> Dec. </month> <year> 1987. </year>
Reference-contexts: With dynamic scheduling, the iterations are also divided into CHUNK-sized chunks. As each process finishes a chunk however, it enters a critical section to grab the next available chunk. With gss scheduling <ref> [29] </ref> , the chunk size is varied, depending on the number of iterations remaining. None of these SGI-provided methods consider task alignment. The programs we selected from LINPACK are SGEFA and SPODI. SGEFA factors a double precision matrix by Gaussian elimination.
Reference: [30] <author> W. Shang and J. Fortes. </author> <title> Time optimal linear schedules for algorithms with uniform dependencies. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-40, </volume> <month> June </month> <year> 1991. </year>
Reference-contexts: In contrast, our work considers a multiprocessor environment where each processor has its own local cache or local memory and different processors may share data. Peir and Cytron [28], and Shang and Fortes <ref> [30] </ref>, and D'Hollander's work [8] shared a common goal of partitioning an index set into independent execution sets so that each of which could run on a distinct processor without interprocessor communication.
Reference: [31] <author> Z. Shen, Z. Li, and P. -C. Yew. </author> <title> An empirical study of Fortran programs for parallelizing compilers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 1(3), </volume> <pages> pp. 356-364, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: bounds, N 2 and N 3 , are large enough to satisfy the following: N 2 &gt; max (j G:C:D:(b 1 ; c 1 ) c 2 j) N 3 &gt; max (j G:C:D:(b 1 ; c 1 ) b 2 j): These assumptions are almost always true in practice <ref> [31] </ref>. When they are not true, the parallel loops will be too small to be important. With these assumptions, we have the following theorem. Theorem 2: Suppose b 1 a 2 a 1 b 2 = 0.
Reference: [32] <author> J. Torrellas, M. S. Lam, and J. L. Hennessy. </author> <title> False sharing and spatial locality in multiprocessor caches. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-43, </volume> <month> June </month> <year> 1994. </year>
Reference-contexts: Unlike these previous works, our work 4 focuses on the nested constructs in which loops are not necessarily perfectly nested , loop bounds can be array variables, and array subscript expressions can be of more general forms. Many researchers have studied the cache false sharing problem <ref> [9, 16, 18, 32] </ref> in which cache thrashing occurs when different processors share the same cache line of multiple words, although they do not share the same word. Many algorithms have been proposed to reduce false sharing by better memory allocation, better thread scheduling, or by program transformations.
Reference: [33] <author> M. Wolf and M. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> In Proceedings of the ACM SIG-PLAN'91 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Carr, Callahan and Kennedy [6, 7] discussed options for compiler control of a uniprocessor memory hierarchy. Wolf and Lam developed an algorithm that estimates all temporal and spatial reuse for a given loop permutation <ref> [33] </ref>. These optimizations all attempt to maximize the reuse of cached data on a single processor. They also have a secondary effect of improving multiprocessor performance by reducing the bandwidth requirement of each processor, 3 thereby reducing contention in the memory system.
Reference: [34] <author> M. Wolfe. </author> <title> More iteration space tiling. </title> <booktitle> In Proceedings of Supercomputing'89, 1989. </booktitle> <volume> 26 22 23 </volume>
Reference-contexts: We then introduce some basic concepts and assumptions. After that, the solutions to the thrashing problem are given, and finally the experimental results conducted on a SGI multiprocessor system are shown. 2 Related Work Extensive research has been reported in the literature regarding efficient memory hierarchy <ref> [2, 6, 7, 10, 11, 14, 15, 28, 33, 34] </ref>. Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories [34]. <p> Abu-Sufah, Kuck and Lawrie used loop blocking to improve the paging performance by improving locality of references [2]. Wolfe proposed iteration space tiling as a way to improve data reuse in cache or local memories <ref> [34] </ref>. Gallivan, Jalby and Gannon defined a reference window for a dependence as the variables referenced by both the source and the sink of the dependence [14, 15].
References-found: 34

