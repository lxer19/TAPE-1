URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/dst/Skinnerbots/pubs/RAS-final-compact.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/Web/People/dst/Skinnerbots/index.html
Root-URL: 
Email: saksida@ri.cmu.edu, sr4r@andrew.cmu.edu, dst@cs.cmu.edu  
Title: Shaping Robot Behavior Using Principles from Instrumental Conditioning (1998) Shaping robot behavior using principles from
Author: Lisa M. Saksida ; Scott M. Raymond David S. Touretzky ; Sasksida, L.M., Raymond, S.M., and Touretzky, D.S. 
Note: Robotics and Autonomous Systems, 22(3/4):231  
Address: Pittsburgh, PA, USA 15213  
Affiliation: 1 Robotics Institute 2 Computer Science Department 3 Center for the Neural Basis of Cognition Carnegie Mellon University  Citation information:  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Asada, S. Noda, S. Tawaratsumida, and K. Hosoda. </author> <title> Purposive behavior acquisition for a real robot by vision-based reinforcement learning. </title> <booktitle> Machine Learning, </booktitle> <address> 23(2-3):279-303, </address> <year> 1996. </year>
Reference-contexts: In addition, Singh does not make an effort to replicate basic aspects of animal shaping such as stimulus control. Asada <ref> [1] </ref> has also provided a modification to Q-learning that has been compared to shaping because he trains the robot on simple tasks first and moves to progressively more difficult tasks once the easier ones have been mastered. <p> Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net.
Reference: [2] <author> S.A. Barnett. </author> <title> Modern Ethology. </title> <publisher> Oxford University Press, </publisher> <year> 1981. </year>
Reference-contexts: We do not assume that everything must be learned from scratch; instead we rely on built-in structure. Much evidence exists for the notion that animals have hardwired behavior sequences, and that there are "natural units" of behavior [19, 20]. Conventional in ethological research on learning <ref> [2] </ref>, and later accepted by neobehaviorist experimental psychologists [6], is the idea that conditioned behavior is synthesized out of instinctive acts.
Reference: [3] <author> D. A. Baxter, D. V. Buonomano, J. L. Raymond, D. G. Cook, F. M. Kuenzi, T. J. Carew, and J. H. Byrne. </author> <title> Empirically derived adaptive elements and networks simulate associative learning. </title> <editor> In M. L. Commons, S. Grossberg, and J. E. R. Staddon, editors, </editor> <booktitle> Neural Network Models of Conditioning and Action, </booktitle> <pages> pages 13-52. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1991. </year>
Reference-contexts: Efforts toward understanding instrumental learning through computer simulations have, to this point, addressed only elementary phenomena such as encouraging or suppressing a single motor action <ref> [3, 30] </ref>. A serious attempt to model phenomena described in the instrumental learning literature will serve the dual purpose of improving the abilities of robot learners while at the same time yielding a fresh, computationally-oriented perspective on animal learning.
Reference: [4] <editor> B. Blumberg. Action-selection in hamsterdam: </editor> <title> Lessons from ethology. </title> <booktitle> In Proceedings of the 3rd International Conference on the Simulation of Adaptive Behavior, </booktitle> <address> Brighton, </address> <year> 1994. </year>
Reference-contexts: Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net.
Reference: [5] <author> B. M. Blumberg, P. M. Todd, and Pattie Maes. </author> <title> No bad dogs: Ethological lessons for learning in hamsterdam. </title> <booktitle> In Proceedings of the 4th International Conference on the Simulation of Adaptive Behavior, </booktitle> <year> 1996. </year>
Reference-contexts: Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net. <p> Other differences between our work and that of Millan are that we use delayed reinforcement from a human trainer, while he uses immediate reinforcement from a reinforcement program, and we use visual sensory input, whereas he uses sonar and IR. Blumberg <ref> [5] </ref> describes an extensive action-selection system to which he has added some basic learning capabilities in the form of temporal difference learning [37]. This work provides a nice example of how innate knowledge is important for building complex behavior systems. 9.
Reference: [6] <author> K. Breland and M. Breland. </author> <title> The misbehavior of organisms. </title> <journal> American Psychologist, </journal> <volume> 16 </volume> <pages> 681-684, </pages> <year> 1961. </year>
Reference-contexts: Chicks can be taught to play a toy piano (peck out a key sequence until a reinforcement is received at the end of the tune) <ref> [6] </ref>, and rats have been conditioned to perform complex memory tasks which are analogs of human cognitive tests [8, 18]. These and many other complicated behaviors can be acquired as a result of the delivery of well-timed reinforcements from a human trainer. <p> Much evidence exists for the notion that animals have hardwired behavior sequences, and that there are "natural units" of behavior [19, 20]. Conventional in ethological research on learning [2], and later accepted by neobehaviorist experimental psychologists <ref> [6] </ref>, is the idea that conditioned behavior is synthesized out of instinctive acts. Adult animals come to a learning situation with a structured response hierarchy. 2 The goal of shaping is to use this response hierarchy to the trainer's advantage by molding pre-existing responses into more desired behaviors.
Reference: [7] <author> P.L Brown and H.M. Jenkins. </author> <title> Auto-shaping of the pigeon's keypeck. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 11 </volume> <pages> 1-8, </pages> <year> 1968. </year>
Reference-contexts: Associative Learning in Animals 2.1. Pavlovian and Instrumental Conditioning Two basic types of associative learning | Pavlovian (or classical) conditioning and instrumental (or operant) conditioning | are well established in the animal learning literature. Although the line between these two types of learning is often blurred <ref> [7] </ref>, they are considered to reflect different underlying processes [12]. Pavlovian conditioning results in the association of an arbitrary neutral stimulus called a conditioned stimulus (CS) with a second, unconditioned stimulus (US). The US is inherently pleasant or unpleasant, and thus provokes an innate, unconditioned response.
Reference: [8] <author> T. J. Bussey, J. L. Muir, and T. W. Robbins. </author> <title> A novel automated touchscreen procedure for assessing learning in the rat using computer graphic stimuli. </title> <journal> Neuroscience Research Communications, </journal> <volume> 15(2) </volume> <pages> 103-109, </pages> <year> 1994. </year>
Reference-contexts: Chicks can be taught to play a toy piano (peck out a key sequence until a reinforcement is received at the end of the tune) [6], and rats have been conditioned to perform complex memory tasks which are analogs of human cognitive tests <ref> [8, 18] </ref>. These and many other complicated behaviors can be acquired as a result of the delivery of well-timed reinforcements from a human trainer. This training strategy exploits a type of learning found from invertebrates to humans, in which an animal comes to associate its actions with the subsequent consequences.
Reference: [9] <author> CCI. </author> <title> The CCI Program. Canine Companions for Independence, </title> <address> Santa Rosa, CA, </address> <year> 1995. </year> <note> Informational page available at http://grunt.berkeley.edu/cci/cci.html. </note>
Reference-contexts: 1. Introduction Service dogs trained to assist a disabled person will respond to over 60 verbal commands to, for example, turn on lights, open a refrigerator door, or retrieve a dropped object <ref> [9] </ref>. Chicks can be taught to play a toy piano (peck out a key sequence until a reinforcement is received at the end of the tune) [6], and rats have been conditioned to perform complex memory tasks which are analogs of human cognitive tests [8, 18].
Reference: [10] <author> J. A. Clouse and P. E. Utgoff. </author> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Learning Through Interaction with a Human Trainer Several researchers have introduced methods in which a human trainer provides advice to a reinforcement learning agent <ref> [17, 23, 10, 38, 22] </ref>. These systems are similar to ours in that the domain knowledge of a human trainer provides additional information to the learner, which can speed up or improve learning.
Reference: [11] <author> M. Colombetti, M. Dorigo, and G. Borghi. </author> <title> Behavior analysis and training: A methodology for behavior engineering. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics Part B, </journal> <volume> 26(3) </volume> <pages> 365-380, </pages> <year> 1996. </year>
Reference-contexts: Mobile robot learning algorithms have yet to approach the sophistication and robustness of animal learning. While the idea of reinforcement for appropriate actions is commonplace in the machine learning literature [21], little of that work examines animal training methods. Several researchers have developed robot "shaping" systems <ref> [11, 28] </ref>, and these methods are a significant contribution to the robot learning literature, but they have not addressed behavioral shaping in animals in any detailed way. <p> Asada [1] has also provided a modification to Q-learning that has been compared to shaping because he trains the robot on simple tasks first and moves to progressively more difficult tasks once the easier ones have been mastered. Our work is most similar to that of Colombetti and Dorigo <ref> [11, 14] </ref> who make a point of exploiting trainer knowledge in order to speed up reinforcement learning. They also emphasize the adjustment of built-in actions through RL to tailor behaviors to specific environments. Our approach differs from theirs in several ways: 1. <p> Because our learning program runs on top of built-in reactive behaviors [35] which incorporate no learning and which have priority, the robot is fully operational and safe from danger during new task training. Colombetti and Dorigo <ref> [11] </ref> also mention the importance of combining low-level reactive behaviors with learned behaviors. However, due to their reinforcement strategy, this can lead to problems.
Reference: [12] <author> A. Dickinson. </author> <title> Instrumental conditioning. </title> <editor> In N. J. Mackintosh, editor, </editor> <booktitle> Handbook of Perception and Cognition. </booktitle> <volume> Volume 9. </volume> <publisher> Academic Press, </publisher> <address> Orlando, FL, </address> <year> 1995. </year>
Reference-contexts: Although the line between these two types of learning is often blurred [7], they are considered to reflect different underlying processes <ref> [12] </ref>. Pavlovian conditioning results in the association of an arbitrary neutral stimulus called a conditioned stimulus (CS) with a second, unconditioned stimulus (US). The US is inherently pleasant or unpleasant, and thus provokes an innate, unconditioned response.
Reference: [13] <author> Anthony Dickinson. </author> <title> Actions and habits: the development of behavioral autonomy. </title> <journal> Philosophical Transactions of the Royal Society of London, Series B, </journal> <volume> 308 </volume> <pages> 67-78, </pages> <year> 1985. </year>
Reference-contexts: Outcome devaluation: Much evidence exists for stimulus control being based on knowledge of the three-term relationship between the stimuli present during reinforcement (S), the action that was taken (A), and the outcome (O), rather than a simple S!R association between the stimuli and a response <ref> [13] </ref>. This can be shown by "devaluing" a reward (e.g., in rats, inducing nausea with a LiCl injection, which reduces the reward value of the type of food the rat had recently consumed) and showing that the rate of response to the controlling stimulus has diminished.
Reference: [14] <author> M. Dorigo and M. Columbetti. </author> <title> Robot shaping: Developing autonomous agents through learning. </title> <journal> Artificial Intelligence, </journal> <volume> 70(2) </volume> <pages> 321-370, </pages> <year> 1994. </year>
Reference-contexts: Asada [1] has also provided a modification to Q-learning that has been compared to shaping because he trains the robot on simple tasks first and moves to progressively more difficult tasks once the easier ones have been mastered. Our work is most similar to that of Colombetti and Dorigo <ref> [11, 14] </ref> who make a point of exploiting trainer knowledge in order to speed up reinforcement learning. They also emphasize the adjustment of built-in actions through RL to tailor behaviors to specific environments. Our approach differs from theirs in several ways: 1.
Reference: [15] <author> G. L. Drescher. </author> <title> Made-Up Minds. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: These are known as the preconditions for the state. Once activated, the state remains active until its postconditions are satisfied. Performing the action associated with the state (or one of its sub-states) should make the postconditions true. See <ref> [15] </ref> for a similar conception of states. <p> Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net.
Reference: [16] <author> C. R. Gallistel. </author> <title> The Organization of Action. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale,NJ, </address> <year> 1980. </year>
Reference-contexts: Consider an animal trainer teaching a dog to sit up and "beg". In order to produce this type of behavior, the trainer does not have to start from scratch, but instead can motivate the dog to emit an innate <ref> [16, 27] </ref> sequence of actions for pursuing an object (see Figure 1), and use appropriate reinforcement to mold the sequence into the begging behavior. 2 ======================================== ======================================== In order to get the dog to sit up on its haunches, the trainer dangles a piece of food above its head. <p> However, the model can be taught to make arbitrarily fine stimulus discriminations by tightening the tuning curve. Animals usually learn about certain types of stimuli more easily, or more quickly, than others <ref> [16] </ref>. In pigeons, for example, visual stimuli are more salient than auditory stimuli. In the model, each stimulus class has an innate salience, or associability (ff i ), that determines its relative strength in influencing the animal's behavior.
Reference: [17] <author> D. Gordon and D. Subramanian. </author> <title> A multistrategy learning scheme for agent knowledge acqui-sition. </title> <journal> Informatica, </journal> <volume> 17 </volume> <pages> 331-346, </pages> <year> 1994. </year>
Reference-contexts: Learning Through Interaction with a Human Trainer Several researchers have introduced methods in which a human trainer provides advice to a reinforcement learning agent <ref> [17, 23, 10, 38, 22] </ref>. These systems are similar to ours in that the domain knowledge of a human trainer provides additional information to the learner, which can speed up or improve learning.
Reference: [18] <author> R. E. Hampson, C. J. Heyser, and S. A. Deadwyler. </author> <title> Hippocampal cell firing correlates of delayed-match-to-sample performance in the rat. </title> <journal> Behavioral Neuroscience, </journal> <volume> 107(5) </volume> <pages> 715-739, </pages> <year> 1993. </year>
Reference-contexts: Chicks can be taught to play a toy piano (peck out a key sequence until a reinforcement is received at the end of the tune) [6], and rats have been conditioned to perform complex memory tasks which are analogs of human cognitive tests <ref> [8, 18] </ref>. These and many other complicated behaviors can be acquired as a result of the delivery of well-timed reinforcements from a human trainer. This training strategy exploits a type of learning found from invertebrates to humans, in which an animal comes to associate its actions with the subsequent consequences.
Reference: [19] <author> E. Hearst and H.M. Jenkins. </author> <title> Sign tracking: The stimulus-reinforcer relation and directed action. </title> <booktitle> The Psychonomic Society, </booktitle> <address> Austin, </address> <year> 1975. </year>
Reference-contexts: Guidance is accomplished in three ways: Modification of innate behavior sequences. We do not assume that everything must be learned from scratch; instead we rely on built-in structure. Much evidence exists for the notion that animals have hardwired behavior sequences, and that there are "natural units" of behavior <ref> [19, 20] </ref>. Conventional in ethological research on learning [2], and later accepted by neobehaviorist experimental psychologists [6], is the idea that conditioned behavior is synthesized out of instinctive acts.
Reference: [20] <author> H.M. Jenkins and B.R. Moore. </author> <title> The form of the autoshaped response with food or water reinforcers. </title> <journal> Journal of the Experimental Analysis of Behavior, </journal> <volume> 20 </volume> <pages> 163-181, </pages> <year> 1973. </year>
Reference-contexts: Guidance is accomplished in three ways: Modification of innate behavior sequences. We do not assume that everything must be learned from scratch; instead we rely on built-in structure. Much evidence exists for the notion that animals have hardwired behavior sequences, and that there are "natural units" of behavior <ref> [19, 20] </ref>. Conventional in ethological research on learning [2], and later accepted by neobehaviorist experimental psychologists [6], is the idea that conditioned behavior is synthesized out of instinctive acts.
Reference: [21] <author> L. Pack Kaelbling, M.L. Littman, and A.W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285, </pages> <year> 1996. </year>
Reference-contexts: One specific animal training technique which exploits this type of learning is shaping by successive approximations, or just "shaping". Mobile robot learning algorithms have yet to approach the sophistication and robustness of animal learning. While the idea of reinforcement for appropriate actions is commonplace in the machine learning literature <ref> [21] </ref>, little of that work examines animal training methods. Several researchers have developed robot "shaping" systems [11, 28], and these methods are a significant contribution to the robot learning literature, but they have not addressed behavioral shaping in animals in any detailed way.
Reference: [22] <author> L.-J. Lin. </author> <title> Self-improving reactive agents based on reinforcement learning, planning, and teaching. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 293-321, </pages> <year> 1992. </year>
Reference-contexts: Learning Through Interaction with a Human Trainer Several researchers have introduced methods in which a human trainer provides advice to a reinforcement learning agent <ref> [17, 23, 10, 38, 22] </ref>. These systems are similar to ours in that the domain knowledge of a human trainer provides additional information to the learner, which can speed up or improve learning.
Reference: [23] <author> R. Maclin and J. W. Shavlik. </author> <title> Creating advice-taking reinforcement learners. </title> <booktitle> Machine Learning, </booktitle> <address> 22(1-2-3):251-281, </address> <year> 1996. </year>
Reference-contexts: Learning Through Interaction with a Human Trainer Several researchers have introduced methods in which a human trainer provides advice to a reinforcement learning agent <ref> [17, 23, 10, 38, 22] </ref>. These systems are similar to ours in that the domain knowledge of a human trainer provides additional information to the learner, which can speed up or improve learning.
Reference: [24] <author> J. del R. Millan. </author> <title> Learning efficient reactive behavioral sequences from basic reflexes in a goal-directed autonomous robot. </title> <booktitle> In From Animals to Animats 3: Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pages 266-274, </pages> <address> Cambridge, MA, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net. <p> Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience [24, 1, 25, 4, 5, 15]. Millan <ref> [24, 25] </ref> describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net. <p> A similar idea for varying the distribution of generated actions is described in <ref> [24] </ref>. In addition to providing a model of animal learning, our system seeks to diminish some of the scaling problems that exist in more general reinforcement learning systems.
Reference: [25] <author> J. del R. Millan. </author> <title> Rapid, safe, and incremental learning of navigation strategies. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics Part B, </journal> <volume> 26(3) </volume> <pages> 408-420, </pages> <year> 1996. </year>
Reference-contexts: Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience <ref> [24, 1, 25, 4, 5, 15] </ref>. Millan [24, 25] describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net. <p> Learning through Refinement of Built-In Knowledge Other researchers have been concerned with establishing principles for deciding what types of behavior should be hardwired into the robot and what aspects of behavior the robot should learn from experience [24, 1, 25, 4, 5, 15]. Millan <ref> [24, 25] </ref> describes a robot learning system in which built-in basic reflexes are improved and refined through the use of RL and a neural net. <p> These basic reflexes suggest where to search for a possible action whenever the neural network fails to generalize correctly to previous experience with the current situation. In the implementation discussed in <ref> [25] </ref>, reflexes consist of move-forward actions, track object boundary, and avoid collision. These reflexes can solve the designated task (exiting a room while avoiding obstacles) without learning, but they are clumsy. After learning, the robot executes the behavior much more efficiently.
Reference: [26] <author> J.M. Pearce and G. Hall. </author> <title> A model for Pavlovian learning: Variations in effectiveness of conditioned but not unconditioned stimuli. </title> <journal> Paychological Review, </journal> <volume> 87(6) </volume> <pages> 532-552, </pages> <year> 1980. </year>
Reference-contexts: The salience of an individual stimulus instance also depends on its intensity, e.g., the brightness of a light, the loudness of a tone, or the proximity of a visually-tracked target <ref> [26] </ref>. Preconditions can be positive or negative, depending on the strength of association V i between stimulus match and reward. For example, if pressing a lever produces a food reward except when a light is on, the light would develop a significant negative associative strength.
Reference: [27] <author> S.M. Pellis, D.P. O'Brien, V.C. Pellis, P. Teitelbaum, D.L. Wolgin, and S. Kennedy. </author> <title> Escalation of feline predation along a gradient from avoidance through play to killing. </title> <journal> Behavioral Neuroscience, </journal> <volume> 102(5) </volume> <pages> 760-777, </pages> <year> 1988. </year>
Reference-contexts: Consider an animal trainer teaching a dog to sit up and "beg". In order to produce this type of behavior, the trainer does not have to start from scratch, but instead can motivate the dog to emit an innate <ref> [16, 27] </ref> sequence of actions for pursuing an object (see Figure 1), and use appropriate reinforcement to mold the sequence into the begging behavior. 2 ======================================== ======================================== In order to get the dog to sit up on its haunches, the trainer dangles a piece of food above its head.
Reference: [28] <author> Simon Perkins and Gillian Hayes. </author> <title> Robot shaping principles, methods, and architectures. </title> <booktitle> In Workshop on Learning in Robots and Animals, AISB '96, </booktitle> <year> 1996. </year>
Reference-contexts: Mobile robot learning algorithms have yet to approach the sophistication and robustness of animal learning. While the idea of reinforcement for appropriate actions is commonplace in the machine learning literature [21], little of that work examines animal training methods. Several researchers have developed robot "shaping" systems <ref> [11, 28] </ref>, and these methods are a significant contribution to the robot learning literature, but they have not addressed behavioral shaping in animals in any detailed way.
Reference: [29] <author> K. Pryor. </author> <title> Lads Before the Wind. </title> <publisher> Harper and Row, </publisher> <address> New York, </address> <year> 1975. </year>
Reference-contexts: Undesirable behaviors can sometimes be eliminated by bringing them under stimulus control and then not giving the command any more <ref> [29] </ref>. As mentioned previously, in our model stimulus control is the product of associability (ff i ), match strength (M i ), and associative strength (V i ).
Reference: [30] <author> J. L. Raymond, D. A. Baxter, D. V. Buonomano, and J. H. Byrne. </author> <title> A learning rule based on empirically derived activity-dependent neuromodulation supports operant conditning in a small network. </title> <booktitle> Neural Networks, </booktitle> <volume> 5(5) </volume> <pages> 789-803, </pages> <year> 1992. </year>
Reference-contexts: Efforts toward understanding instrumental learning through computer simulations have, to this point, addressed only elementary phenomena such as encouraging or suppressing a single motor action <ref> [3, 30] </ref>. A serious attempt to model phenomena described in the instrumental learning literature will serve the dual purpose of improving the abilities of robot learners while at the same time yielding a fresh, computationally-oriented perspective on animal learning.
Reference: [31] <author> R. A. Rescorla and A. R. Wagner. </author> <title> A theory of Pavlovian conditioning: Variations in the effectiveness of reinforcement and nonreinforcement. </title> <editor> In A. H. Black and W. F. Prokasy, editors, </editor> <title> Classical Conditioning II: </title> <booktitle> Theory and Research. </booktitle> <address> Appleton-Century-Crofts, New York, </address> <year> 1972. </year>
Reference-contexts: Therefore they can vary from trial to trial. The associative strength V i of a stimulus normally starts at zero and increases as the precondition is paired with reinforcement. It is adjusted in our model using the Rescorla-Wagner learning rule <ref> [31] </ref> (mathematically equivalent to the LMS or Widrow-Hoff learning rule [33]) when either a reward is received or an expected reward is not received. <p> It also allows us to replicate certain well-known phenomena in the classical conditioning literature, such as blocking and negative conditioning. 1 Our implementation of the rule differs from the original version described by Rescorla and Wagner <ref> [31] </ref> in two ways: first, we have a dynamic ff (which is described in the next section) and second, we use a continuous value for stimulus match strength instead of a boolean indicator of stimulus presence. 5.3.
Reference: [32] <author> G. S. Reynolds. </author> <title> A Primer of Operant Conditioning. Scott, </title> <address> Foresman, </address> <year> 1968. </year> <month> 22 </month>
Reference-contexts: That is, the probability of re-entering the state responsible for that action will be higher in the presence of the stimuli and lower in its absence <ref> [32] </ref>. In our model, stimuli gain control over behavior based on their relative match strengths, saliences, and associative strengths. We define the amount of control possessed by a stimulus feature to be C i (t) = M i (t) ff i (t) V i (2) of control. <p> That is, the probability of reentering the state responsible for that action will be higher in the presence of the stimuli and lower in its absence <ref> [32] </ref>. The stimuli are then known as controlling, or discriminative, stimuli. Discriminative stimuli are important because they allow the animal to restrict its actions to only those situations in which they will produce reinforcement.
Reference: [33] <author> F. Rosenblatt. </author> <title> Principles of Neurodynamics. </title> <publisher> Spartan, </publisher> <address> New York, </address> <year> 1962. </year>
Reference-contexts: The associative strength V i of a stimulus normally starts at zero and increases as the precondition is paired with reinforcement. It is adjusted in our model using the Rescorla-Wagner learning rule [31] (mathematically equivalent to the LMS or Widrow-Hoff learning rule <ref> [33] </ref>) when either a reward is received or an expected reward is not received.
Reference: [34] <author> R. Simmons. </author> <title> Structured control for autonomous robots. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 10(1) </volume> <pages> 34-43, </pages> <year> 1994. </year>
Reference-contexts: Computing power is provided by three on-board 11 Pentium processors; a 1 Mbps radio modem links Amelia to a network of high-end workstations that can contribute additional processing cycles. For our experiments, we ran the learning program in Allegro Common Lisp on a Sparc 5 and used TCA <ref> [34] </ref> to communicate with the robot. ======================================== ======================================== In order to allow the robot to operate in real time, we chose a fast and simple approach to robot vision. Objects were assumed to be monochromatic.
Reference: [35] <author> Reid Simmons, Richard Goodwin, Karen Haigh, Sven Koenig, and Joseph O'Sullivan. </author> <title> A modular architecture for office delivery robots. </title> <booktitle> In The First International Conference on Autonomous Agents, </booktitle> <month> Feb </month> <year> 1997. </year>
Reference-contexts: Another goal in our work is to enable these tasks to be learned while low-level reactive behaviors, such as obstacle avoidance, are intact. Because our learning program runs on top of built-in reactive behaviors <ref> [35] </ref> which incorporate no learning and which have priority, the robot is fully operational and safe from danger during new task training. Colombetti and Dorigo [11] also mention the importance of combining low-level reactive behaviors with learned behaviors. However, due to their reinforcement strategy, this can lead to problems.
Reference: [36] <author> S.P. Singh. </author> <title> Transfer of learning across sequential tasks. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 323-339, </pages> <year> 1992. </year>
Reference-contexts: Singh <ref> [36] </ref> was first in the robotics literature to use the term "shaping". In his system, complex behaviors are constructed from simple actions, that is, composite sequential decision tasks are formed by concatenating a number of elemental decision tasks.
Reference: [37] <author> R. S. Sutton and A. G. Barto. </author> <title> Toward a modern theory of adaptive networks: Expectation and prediction. </title> <journal> Psychological Review, </journal> <volume> 88 </volume> <pages> 135-170, </pages> <year> 1981. </year>
Reference-contexts: Blumberg [5] describes an extensive action-selection system to which he has added some basic learning capabilities in the form of temporal difference learning <ref> [37] </ref>. This work provides a nice example of how innate knowledge is important for building complex behavior systems. 9.
Reference: [38] <author> P. Utgoff and J. Clouse. </author> <title> Two kinds of training information for evaluation function learning. </title> <booktitle> In Proceeding of the Ninth National Conference on Artificial Intelligence (AAAI-91). </booktitle> <publisher> AAAI Press, </publisher> <year> 1991. </year>
Reference-contexts: Learning Through Interaction with a Human Trainer Several researchers have introduced methods in which a human trainer provides advice to a reinforcement learning agent <ref> [17, 23, 10, 38, 22] </ref>. These systems are similar to ours in that the domain knowledge of a human trainer provides additional information to the learner, which can speed up or improve learning.

References-found: 38

