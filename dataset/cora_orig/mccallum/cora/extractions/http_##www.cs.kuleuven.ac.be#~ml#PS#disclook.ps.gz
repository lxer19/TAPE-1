URL: http://www.cs.kuleuven.ac.be/~ml/PS/disclook.ps.gz
Refering-URL: http://www.cs.kuleuven.ac.be/~hendrik/publications.html
Root-URL: 
Email: e-mail: fHendrik.Blockeel, Luc.DeRaedtg@cs.kuleuven.ac.be  
Title: Lookahead and Discretization in ILP  
Author: Hendrik Blockeel and Luc De Raedt 
Address: Celestijnenlaan 200A 3001 Heverlee  
Affiliation: Katholieke Universiteit Leuven Department of Computer Science  
Abstract: We present and evaluate two methods for improving the performance of ILP systems. One of them is discretization of numerical attributes, based on Fayyad and Irani's text [9], but adapted and extended in such a way that it can cope with some aspects of discretization that only occur in relational learning problems (when indeterminate literals occur). The second technique is lookahead. It is a well-known problem in ILP that a learner cannot always assess the quality of a refinement without knowing which refinements will be enabled afterwards, i.e. without looking ahead in the refinement lattice. We present a simple method for specifying when lookahead is to be used, and what kind of lookahead is interesting. Both the discretization and lookahead techniques are evaluated experimentally. The results show that both techniques improve the quality of the induced theory, while computational costs are acceptable.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> H. Blockeel and L. De Raedt. </author> <title> Experiments with top-down induction of logical decision trees. </title> <type> Technical Report CW 247, </type> <institution> Dept. of Computer Science, K.U.Leuven, </institution> <month> January </month> <year> 1997. </year> <note> Also in Periodic Progress Report ESPRIT Project ILP2, January 1997. http://www.cs.kuleuven.ac.be/publicaties/rapporten/CW1997.html. </note>
Reference-contexts: Our experiments have been done with the ILP system Tilde <ref> [1] </ref>, which represents the induced hypotheses as logical decision trees (these are a first order logic upgrade of the classical decision trees used in propositional concept learning). Example 1. Suppose a number of machines are under revision.
Reference: 2. <author> J. Catlett. </author> <title> On changing continuous attributes into ordered discrete attributes. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the 5th European Working Session on Learning, volume 482 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 164-178. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: Tilde, however, discretizes numeric domains beforehand, which makes the induction process much more efficient. It is known that in a propositional learning context this does not necessarily decrease the quality of the induced hypothesis (it may even increase; see e.g. <ref> [2] </ref>). In our approach to discretization, the user can declaratively identify the relevant queries and the variables for which the values are to be discretized. For instance, to be discretized (atom (A,B,C,D), [D]) states that the fourth argument of atom should be discretized.
Reference: 3. <author> L. De Raedt. </author> <title> Induction in logic. In R.S. </title> <editor> Michalski and Wnek J., editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Multistrategy Learning, </booktitle> <pages> pages 29-38, </pages> <year> 1996. </year>
Reference-contexts: In Section 3 we discuss discretization, in Section 4 lookahead. Conclusions are presented in Section 5. 2 The Learning Setting We essentially use the learning from interpretations paradigm for inductive logic programming, introduced by [4], and related to other inductive logic programming settings in <ref> [3] </ref>. In this paradigm, each example is a Prolog knowledge base (i.e. a set of definite clauses), encoding the specific properties of the example. Furthermore, each example is classified into one of a finite set of possible classes.
Reference: 4. <author> L. De Raedt and S. Dzeroski. </author> <title> First order jk-clausal theories are PAC-learnable. </title> <journal> Artificial Intelligence, </journal> <volume> 70 </volume> <pages> 375-392, </pages> <year> 1994. </year>
Reference-contexts: In Section 2, we briefly discuss the ILP setting that is used. In Section 3 we discuss discretization, in Section 4 lookahead. Conclusions are presented in Section 5. 2 The Learning Setting We essentially use the learning from interpretations paradigm for inductive logic programming, introduced by <ref> [4] </ref>, and related to other inductive logic programming settings in [3]. In this paradigm, each example is a Prolog knowledge base (i.e. a set of definite clauses), encoding the specific properties of the example. Furthermore, each example is classified into one of a finite set of possible classes.
Reference: 5. <author> T. G. Dietterich, R. H. Lathrop, and T. Lozano-Perez. </author> <title> Solving the multiple-instance problem with axis-parallel rectangles. </title> <journal> Artificial Intelligence, </journal> <volume> 89(1-2):31-71, </volume> <year> 1997. </year>
Reference-contexts: Both datasets contain non-determinate numerical data, which makes them fit to test our discretization procedure on. We refer to <ref> [5] </ref> and [8] for precise descriptions of the datasets. On the Musk dataset, we have compared the discretization approaches using equality tests, and using inequality tests. On the Diterpene dataset, the interval and inequality approaches were compared with using no discretization at all. In that was given.
Reference: 6. <author> B. Dolsak and S. Muggleton. </author> <title> The application of Inductive Logic Programming to finite element mesh design. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive logic programming, </booktitle> <pages> pages 453-472. </pages> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: We refer to [14] and <ref> [6] </ref> respectively for more information on these datasets. Both datasets repeatedly were partitioned into 10 subsets. Two tenfold cross-validations were run based on each such partition; one without allowing looka-head, and one with lookahead.
Reference: 7. <author> J. Dougherty, R. Kohavi, and M. Sahami. </author> <title> Supervised and unsupervised discretiza-tion of continuous features. </title> <editor> In A. Prieditis and S. Russell, editors, </editor> <booktitle> Proc. Twelfth International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: For instance, to be discretized (atom (A,B,C,D), [D]) states that the fourth argument of atom should be discretized. The resulting numeric attributes are then discretized using a simple modification of Fayyad and Irani's method. The details of this method can be found in [9] and <ref> [7] </ref>. In short, the algorithm finds a threshold that partitions a set of examples into two subsets such that the average class entropy of the subsets is as small as possible. This procedure is applied recursively on S 1 and S 2 until some stopping criterion is reached.
Reference: 8. <author> S. Dzeroski, S. Schulze-Kremer, K. R. Heidtke, K. Siems, and D. Wettschereck. </author> <title> Applying ILP to diterpene structure elucidation from 13C NMR spectra. </title> <booktitle> In Proceedings of the 6th International Workshop on Inductive Logic Programming, </booktitle> <pages> pages 14-27, </pages> <month> August </month> <year> 1996. </year>
Reference-contexts: Both datasets contain non-determinate numerical data, which makes them fit to test our discretization procedure on. We refer to [5] and <ref> [8] </ref> for precise descriptions of the datasets. On the Musk dataset, we have compared the discretization approaches using equality tests, and using inequality tests. On the Diterpene dataset, the interval and inequality approaches were compared with using no discretization at all. In that was given.
Reference: 9. <author> U.M. Fayyad and K.B. Irani. </author> <title> Multi-interval discretization of continuous-valued attributes for classification learning. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> San Mateo, CA, 1993. </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference-contexts: In this paper, we discuss two such upgrades of propositional learning results to the ILP context. The first is discretization of continuous attributes. Irani and Fayyad <ref> [9] </ref> have presented a propositional method that divides a continuous domain into several subsets which can then be used as discrete values. <p> For instance, to be discretized (atom (A,B,C,D), [D]) states that the fourth argument of atom should be discretized. The resulting numeric attributes are then discretized using a simple modification of Fayyad and Irani's method. The details of this method can be found in <ref> [9] </ref> and [7]. In short, the algorithm finds a threshold that partitions a set of examples into two subsets such that the average class entropy of the subsets is as small as possible. This procedure is applied recursively on S 1 and S 2 until some stopping criterion is reached.
Reference: 10. <author> N. Lavrac and S. Dzeroski. </author> <title> Inductive Logic Programming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1994. </year>
Reference-contexts: Our work has of course heavily been influenced by several publications on discretization ([9, 7]) and especially [15]. The idea of using lookahead in ILP has been uttered several times before (e.g. <ref> [10] </ref>). Acknowledgements Hendrik Blockeel is supported by the Flemish Institute for the Promotion of Scientific and Technological Research in Industry (IWT). Luc De Raedt is supported by the Fund for Scientific Research of Flanders. This work is also part of the European Community Esprit project no. 20237, ILP2.
Reference: 11. <author> C.J. Merz and P.M. Murphy. </author> <title> UCI repository of machine learning databases [http://www.ics.uci.edu/~mlearn/mlrepository.html] , 1996. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: This corresponds to an interval test in the discrete domain. The three approaches have been used and compared in our experiments. 3.2 Experimental Evaluation We evaluate the effect of discretization on two datasets: the Musk dataset (available at the UCI repository <ref> [11] </ref>) and the Diterpene dataset, generously provided to us by Steffen Schulze-Kremer and Saso Dzeroski. Both datasets contain non-determinate numerical data, which makes them fit to test our discretization procedure on. We refer to [5] and [8] for precise descriptions of the datasets.
Reference: 12. <author> S. Muggleton. </author> <title> Inverse entailment and progol. </title> <journal> New Generation Computing, </journal> <volume> 13, </volume> <year> 1995. </year>
Reference: 13. <author> J.R. Quinlan. </author> <title> FOIL: A midterm report. </title> <editor> In P. Brazdil, editor, </editor> <booktitle> Proceedings of the 6th European Conference on Machine Learning, Lecture Notes in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: For greedy systems, this may heavily influence the induction process. Although some systems have some provisions to cope with the problem (e.g. FOIL <ref> [13] </ref> automatically adds determinate literals), it is still an open question how it can best be solved. A possible technique for coping with the problem, is to make the learner look ahead in the refinement lattice.
Reference: 14. <author> A. Srinivasan, S.H. Muggleton, and R.D. King. </author> <title> Comparing the use of background knowledge by inductive logic programming systems. </title> <editor> In L. De Raedt, editor, </editor> <booktitle> Proceedings of the 5th International Workshop on Inductive Logic Programming, </booktitle> <year> 1995. </year>
Reference-contexts: We refer to <ref> [14] </ref> and [6] respectively for more information on these datasets. Both datasets repeatedly were partitioned into 10 subsets. Two tenfold cross-validations were run based on each such partition; one without allowing looka-head, and one with lookahead.
Reference: 15. <author> W. Van Laer, S. Dzeroski, and L. De Raedt. </author> <title> Multi-class problems and discretiza-tion in ICL (extended abstract). </title> <booktitle> In Proceedings of the MLnet Familiarization Workshop on Data Mining with Inductive Logic Programming (ILP for KDD), </booktitle> <year> 1996. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: Although our experiments only concern Tilde, the proposed techniques are generally applicable; other ILP systems might profit from them as well. Our work has of course heavily been influenced by several publications on discretization ([9, 7]) and especially <ref> [15] </ref>. The idea of using lookahead in ILP has been uttered several times before (e.g. [10]). Acknowledgements Hendrik Blockeel is supported by the Flemish Institute for the Promotion of Scientific and Technological Research in Industry (IWT). Luc De Raedt is supported by the Fund for Scientific Research of Flanders.
References-found: 15

