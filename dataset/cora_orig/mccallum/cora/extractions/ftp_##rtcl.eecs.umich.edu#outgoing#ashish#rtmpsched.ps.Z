URL: ftp://rtcl.eecs.umich.edu/outgoing/ashish/rtmpsched.ps.Z
Refering-URL: http://www.eecs.umich.edu/~ashish/
Root-URL: http://www.cs.umich.edu
Email: Email: fashish,kgshing@eecs.umich.edu  
Title: QoS-Sensitive Protocol Processing in Shared-Memory Multiprocessor Multimedia Servers (Extended Abstract)  
Author: Ashish Mehra Kang G. Shin 
Address: Ann Arbor, MI 48109-2122  
Affiliation: Real-Time Computing Laboratory Department of Electrical Engineering and Computer Science University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Ferrari, </author> <title> "Client requirements for real-time communication services," </title> <journal> IEEE Communications Magazine, </journal> <pages> pp. 65-72, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: An application specifies its QoS requirements for each active connection in terms of several parameters such as end-to-end delay, delay jitter, and bandwidth; additional requirements regarding packet loss and in-order delivery can also be specified <ref> [1, 2] </ref>. Additionally, the application may need to specify its traffic characteristics on each connection in order for the host communication subsystem and network to provide the required guarantees.
Reference: [2] <author> M. Zitterbart, B. Stiller, and A. N. Tantawy, </author> <title> "A model for flexible high-performance communication systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 507-518, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: An application specifies its QoS requirements for each active connection in terms of several parameters such as end-to-end delay, delay jitter, and bandwidth; additional requirements regarding packet loss and in-order delivery can also be specified <ref> [1, 2] </ref>. Additionally, the application may need to specify its traffic characteristics on each connection in order for the host communication subsystem and network to provide the required guarantees.
Reference: [3] <author> D. C. Schmidt and T. Suda, </author> <title> "Transport system architecture services for high-performance communications systems," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 4, </volume> <pages> pp. 489-506, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Therefore, the communication subsystem must exercise fine-grain control over management of resources involved in communication, such as processors and the network adapter. Within the communication subsystem, the degree of sharing of processors amongst connections depends on the transport system architecture <ref> [3] </ref> and resource management policies employed. Resources such as processors and buffers must be allocated to individual connections in order to meet their QoS requirements. This allocation must attempt to reduce load imbalance, achieve high resource utilization, and maximize the number of connections that can be serviced. <p> Provision of multiple protocol processors facilitates scalable server design by increasing the processing capacity of the communication subsystem and allowing concurrent handling of different connections. Protocol processing is based on a vertical process architecture employing the process-per-connection model <ref> [3] </ref>. Each connection has associated with it a unique process (the connection "handler") and a first-in-first-out (FIFO) queue of messages waiting to be processed and transmitted; the connection handler processes the queued messages one at a time. <p> Vertical process architectures employing process-per-connection and process-per-message models have been proposed to exploit parallelism in protocol implementations <ref> [3] </ref>. A process-per-message model seems unsuitable for QoS-sensitive protocol processing. Assuming that each message's shepherd process is independently scheduled on the protocol processors, simultaneous processing of multiple messages from the same connection would lead to out-of-order consumption of protocol processing and transmission bandwidth.
Reference: [4] <author> A. Mehra, A. Indiresan, and K. G. Shin, </author> <title> "Design and evaluation of a QoS-sensitive communication subsystem," </title> <note> In preparation, </note> <month> July </month> <year> 1995. </year>
Reference-contexts: i.e., when new connections are established and cannot be accommodated on the available protocol processors. 3 Evaluating the Design Tradeoffs As a first step towards demonstrating the feasibility of the proposed architecture and evaluating the design tradeoffs highlighted, we have implemented a QoS-sensitive communication subsystem on a single protocol processor <ref> [4] </ref> residing in a small-scale bus-based multiprocessor host with a NUMA configuration [5]. The implementation has been done in the context of real-time channels, a paradigm for real-time communication services in packet-switched networks [6].
Reference: [5] <author> A. Indiresan, A. Mehra, and K. Shin, </author> <title> "Design tradeoffs in implementing real-time channels on bus-based multiprocessor hosts," </title> <type> Technical Report CSE-TR-238-95, </type> <institution> University of Michigan, </institution> <month> April </month> <year> 1995. </year>
Reference-contexts: available protocol processors. 3 Evaluating the Design Tradeoffs As a first step towards demonstrating the feasibility of the proposed architecture and evaluating the design tradeoffs highlighted, we have implemented a QoS-sensitive communication subsystem on a single protocol processor [4] residing in a small-scale bus-based multiprocessor host with a NUMA configuration <ref> [5] </ref>. The implementation has been done in the context of real-time channels, a paradigm for real-time communication services in packet-switched networks [6]. The communication executive on the protocol processor is derived from x-kernel 3.1 [7]; our implementation of the protocol stack decouples data transfer and control to minimize data copying. <p> However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic. We have studied the implications of network adapter characteristics for real-time communication on a Fibre Channel adapter manufactured by Ancor Communications <ref> [5] </ref>.
Reference: [6] <author> D. D. Kandlur, K. G. Shin, and D. Ferrari, </author> <title> "Real-time communication in multi-hop networks," </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> vol. 5, no. 10, </volume> <pages> pp. 1044-1056, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: The implementation has been done in the context of real-time channels, a paradigm for real-time communication services in packet-switched networks <ref> [6] </ref>. The communication executive on the protocol processor is derived from x-kernel 3.1 [7]; our implementation of the protocol stack decouples data transfer and control to minimize data copying.
Reference: [7] <author> N. C. Hutchinson and L. L. Peterson, </author> <title> "The x-Kernel: An architecture for implementing network protocols," </title> <journal> IEEE Trans. Software Engineering, </journal> <volume> vol. 17, no. 1, </volume> <pages> pp. 1-13, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The implementation has been done in the context of real-time channels, a paradigm for real-time communication services in packet-switched networks [6]. The communication executive on the protocol processor is derived from x-kernel 3.1 <ref> [7] </ref>; our implementation of the protocol stack decouples data transfer and control to minimize data copying. The implementation features a process-per-channel model with fixed-priority and earliest-deadline-first semi-preemptive scheduling of channel handlers; each channel handler relinquishes the CPU to a waiting higher-priority handler at evenly-spaced preemption points.
Reference: [8] <author> K. Maly et al., </author> <title> "Parallel TCP/IP for multiprocessor workstations," </title> <booktitle> in Proc. IFIP TC6/WG6.4 Fourth Int'l Conf. on High Performance Networking, </booktitle> <pages> pp. 103-118, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: Based on the insights gained, we are designing a powerful, parameterized, object-oriented SMMP simulator to study the proposed architecture and design tradeoffs identified in this paper. The design of the simulator is similar to the one developed in <ref> [8] </ref>, but specifically geared towards QoS-sensitive protocol processing. In particular, it features parameterized models for resources (processors, memory, processor-memory or processor-processor interconnect, network adapter), resource management and scheduling policies (connection admission control, processor-connection mapping, per-processor scheduling, link access scheduling), and resource usage (connection requests, per-connection traffic generation and protocol processing).
Reference: [9] <author> C. A. Thekkath, D. L. Eager, E. D. Lazowska, and H. M. Levy, </author> <title> "A performance analysis of network I/O in shared-memory multiprocessors," </title> <institution> Computer Science and Engineering Technical Report 92-04-04, University of Washington, </institution> <month> April </month> <year> 1992. </year>
Reference-contexts: In one approach, each processor executing a process also performs protocol processing for messages transmitted by that process <ref> [9] </ref>. In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems.
Reference: [10] <author> A. Garg, </author> <title> "Parallel STREAMS: A multiprocessor implementation," </title> <booktitle> in Winter 1990 USENIX Conference, </booktitle> <pages> pp. 163-176, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems. An alternative approach treats protocol processing as global work that can be scheduled uniformly on any available processor <ref> [10, 11] </ref>; this results in explicit sharing between the two subsystems. These approaches may not suffice for QoS-sensitive protocol processing since they introduce unpredictability in the availability and allocation of processing resources, and complicate global coordination for network access.
Reference: [11] <author> S. Khanna, M. Sebree, and J. Zolnowsky, </author> <title> "Realtime scheduling in SunOS 5.0," </title> <booktitle> in Winter USENIX Conference, </booktitle> <pages> pp. 375-390, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: In this model, protocol processing is treated as work strictly local to each processor, resulting in an implicit sharing between the computation and communication subsystems. An alternative approach treats protocol processing as global work that can be scheduled uniformly on any available processor <ref> [10, 11] </ref>; this results in explicit sharing between the two subsystems. These approaches may not suffice for QoS-sensitive protocol processing since they introduce unpredictability in the availability and allocation of processing resources, and complicate global coordination for network access.
Reference: [12] <author> A. N. Netravali, W. D. Roome, and K. Sabnani, </author> <title> "Design and implementation of a high-speed transport protocol," </title> <journal> IEEE Trans. Communications, </journal> <volume> vol. 38, no. 11, </volume> <pages> pp. 2010-2024, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: These approaches may not suffice for QoS-sensitive protocol processing since they introduce unpredictability in the availability and allocation of processing resources, and complicate global coordination for network access. Our proposal for static partitioning of processing resources is similar to those for multiprocessor front-ends <ref> [12] </ref>, except that a set of processors within the host are dedicated for protocol processing, as in [13]. Vertical process architectures employing process-per-connection and process-per-message models have been proposed to exploit parallelism in protocol implementations [3]. A process-per-message model seems unsuitable for QoS-sensitive protocol processing.
Reference: [13] <author> M. Bjorkman and P. Gunningberg, </author> <title> "Locking effects in multiprocessor implementations of protocols," </title> <booktitle> in Proc. of ACM SIGCOMM, </booktitle> <pages> pp. 74-83, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: Our proposal for static partitioning of processing resources is similar to those for multiprocessor front-ends [12], except that a set of processors within the host are dedicated for protocol processing, as in <ref> [13] </ref>. Vertical process architectures employing process-per-connection and process-per-message models have been proposed to exploit parallelism in protocol implementations [3]. A process-per-message model seems unsuitable for QoS-sensitive protocol processing.
Reference: [14] <author> E. M. Nahum, D. J. Yates, J. F. Kurose, and D. Towsley, </author> <title> "Performance issues in parallelized network protocols," </title> <booktitle> in Proc. USENIX Symp. on Operating Systems Design and Implementation, </booktitle> <pages> pp. 125-137, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: Our choice of the process-per-connection model and mapping of each connection handler to exactly one processor is based on these considerations. Recent results have shown 6 that connectional parallelism delivers comparatively more scalable performance than message parallelism <ref> [14, 15] </ref>. The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks has received significant attention recently, for FDDI [16] as well as ATM [17, 18] networks. However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic.
Reference: [15] <author> D. C. Schmidt and T. Suda, </author> <title> "Measuring the performance of parallel message-based process architectures," </title> <booktitle> in IEEE INFOCOM, </booktitle> <pages> pp. 624-633, </pages> <year> 1995. </year>
Reference-contexts: Our choice of the process-per-connection model and mapping of each connection handler to exactly one processor is based on these considerations. Recent results have shown 6 that connectional parallelism delivers comparatively more scalable performance than message parallelism <ref> [14, 15] </ref>. The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks has received significant attention recently, for FDDI [16] as well as ATM [17, 18] networks. However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic.
Reference: [16] <author> K. K. Ramakrishnan, </author> <title> "Performance considerations in designing network interfaces," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 203-219, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Recent results have shown 6 that connectional parallelism delivers comparatively more scalable performance than message parallelism [14, 15]. The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks has received significant attention recently, for FDDI <ref> [16] </ref> as well as ATM [17, 18] networks. However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic. We have studied the implications of network adapter characteristics for real-time communication on a Fibre Channel adapter manufactured by Ancor Communications [5].
Reference: [17] <author> B. Davie, </author> <title> "The architecture and implementation of a high-speed host interface," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 228-239, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Recent results have shown 6 that connectional parallelism delivers comparatively more scalable performance than message parallelism [14, 15]. The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks has received significant attention recently, for FDDI [16] as well as ATM <ref> [17, 18] </ref> networks. However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic. We have studied the implications of network adapter characteristics for real-time communication on a Fibre Channel adapter manufactured by Ancor Communications [5].
Reference: [18] <author> C. B. S. Traw and J. M. Smith, </author> <title> "Hardware/software organization of a high-performance ATM host interface," </title> <journal> IEEE Journal on Selected Areas in Communications, </journal> <volume> vol. 11, no. 2, </volume> <pages> pp. 240-253, </pages> <month> February </month> <year> 1993. </year> <month> 7 </month>
Reference-contexts: Recent results have shown 6 that connectional parallelism delivers comparatively more scalable performance than message parallelism [14, 15]. The design of high-speed network adapters, their performance characteristics, and implications for protocol stacks has received significant attention recently, for FDDI [16] as well as ATM <ref> [17, 18] </ref> networks. However, the design tradeoffs have been explored primarily in the context of uniprocessor workstations and best-effort network traffic. We have studied the implications of network adapter characteristics for real-time communication on a Fibre Channel adapter manufactured by Ancor Communications [5].
References-found: 18

