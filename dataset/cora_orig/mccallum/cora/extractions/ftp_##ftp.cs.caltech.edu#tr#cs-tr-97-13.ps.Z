URL: ftp://ftp.cs.caltech.edu/tr/cs-tr-97-13.ps.Z
Refering-URL: ftp://ftp.cs.caltech.edu/tr/INDEX.html
Root-URL: http://www.cs.caltech.edu
Email: (eric@cs.caltech.edu).  
Title: Similar Classifiers and VC Error Bounds  
Author: Eric Bax 
Address: ifornia, 91125  
Affiliation: Computer Science Department, California Institute of Technology 256-80, Pasadena, Cal  
Date: June 6, 1997  
Abstract: We improve error bounds based on VC analysis for classes with sets of similar classifiers. We apply the new error bounds to separating planes and artificial neural networks. Key words machine learning, learning theory, generalization, Vapnik-Chervonenkis, separating planes, neural networks. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Abu-Mostafa, Y.S. </author> <year> (1989). </year> <title> The Vapnik-Chervonenkis dimension: information versus complexity in learning. </title> <journal> Neural Computation, </journal> <volume> 1 (3), </volume> <pages> 312-317. </pages>
Reference-contexts: We use the in-sample data to select a classifier from the class, e.g., through training or data-fitting by other means. Vapnik-Chervonenkis analysis <ref> [17, 1, 3] </ref> provides probabilistic bounds on the test error of the chosen classifier. In this paper, we improve these bounds for several classes. In the next section, we derive VC-type bounds. First, we derive a bound for a single classifier chosen without reference to the data.
Reference: [2] <author> Baum, E. B., and Haussler, D. </author> <year> (1989). </year> <title> What size net gives valid generalization? Neural Computation, </title> <type> 1 (1), </type> <pages> 151-160. </pages>
Reference-contexts: To derive improved bounds for these networks, substitute the growth function of the particular architecture <ref> [2, 8] </ref> for m (n) in bound (40). It should not be difficult to extend the proof for separating planes to classes with other separating surfaces that can be continuously rotated and translated.
Reference: [3] <author> Blumer, A., Ehrenfeucht, A., and Haussler, D. </author> <year> (1989). </year> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36 (4), </volume> <pages> 929-965. </pages>
Reference-contexts: We use the in-sample data to select a classifier from the class, e.g., through training or data-fitting by other means. Vapnik-Chervonenkis analysis <ref> [17, 1, 3] </ref> provides probabilistic bounds on the test error of the chosen classifier. In this paper, we improve these bounds for several classes. In the next section, we derive VC-type bounds. First, we derive a bound for a single classifier chosen without reference to the data.
Reference: [4] <author> Bax, E., Cataltepe, Z., and Sill, J. </author> <year> (1997). </year> <title> Alternative error bounds for the classifier chosen by early stopping. </title> <publisher> CalTech-CS-TR-97-08. </publisher>
Reference-contexts: First, the set contains every output pattern produced by the classifiers in G. Second, for each pattern there are at least k 1 other patterns with r or fewer disagreements. Then Prf8g 2 Gj-0 &lt; - + D m (n) B (*) (23) Alternatively, we could use "central" classifiers <ref> [4] </ref>. In this case, it is not necessary for every representative classifier to have many neighbors. Instead, we require that a few classifiers form an r-disagreement covering of G.
Reference: [5] <author> Cover, T. M. </author> <year> (1965). </year> <title> Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> 14, </volume> <pages> 326-334. </pages>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see <ref> [6, 5] </ref>. Perceptrons [6, 11, 12, 13, 9, 10] are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane. <p> It should not be difficult to extend the proof for separating planes to classes with other separating surfaces that can be continuously rotated and translated. Future challenges include developing neighbor bounds for other classes and improving the bounds for separating planes. (Cover <ref> [5] </ref> has results that may be useful for these purposes.) Training by gradient descent requires classes with closely related classifiers.
Reference: [6] <author> Duda, R., and Hart, P. </author> <year> (1973). </year> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley & Sons, Inc. </publisher>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is <ref> [6] </ref> m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons [6, 11, 12, 13, 9, 10] are closely related to separating planes. <p> The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see <ref> [6, 5] </ref>. Perceptrons [6, 11, 12, 13, 9, 10] are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane. <p> The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [7] <author> Hoeffding, W. </author> <year> (1963). </year> <title> Probability inequalities for sums of bounded random variables. </title> <journal> Journal of the American Statistical Association, </journal> <pages> 13-30. </pages>
Reference-contexts: To simplify the analysis, we use the Hoeffding bound <ref> [7] </ref> 2e 1 2 * 2 D in place of the partition-based bound B (*). (The Hoeffding bound is smooth, and it is often used in VC analysis [15].) We find that the best disagreement rate goes to O ( 1 * ) as the number of examples increases.
Reference: [8] <author> Koiran, P. and Sontag, E. D. </author> <year> (1997). </year> <title> Neural networks with quadratic VC dimension. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 54, </volume> <pages> 190-198. </pages>
Reference-contexts: To derive improved bounds for these networks, substitute the growth function of the particular architecture <ref> [2, 8] </ref> for m (n) in bound (40). It should not be difficult to extend the proof for separating planes to classes with other separating surfaces that can be continuously rotated and translated.
Reference: [9] <author> Nilsson, H. J. </author> <year> (1965). </year> <title> Learning Machines: </title> <booktitle> Foundations of Trainable Pattern-Classifying Systems. </booktitle> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [10] <author> Minsky, M., and Papert, S. </author> <year> (1969). </year> <title> Perceptrons: An Introduction to Computational Geometry. </title> <address> Cambrdige, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [11] <author> McCulloch, W. S., and Pitts, W. H. </author> <year> (1965). </year> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 (1943): </volume> <pages> 115-133. </pages> <note> reprinted in: </note> <author> McCulloch, W. S. </author> <year> (1965). </year> <booktitle> Embodiments of Mind,(pp. </booktitle> <pages> 19-39). </pages> <address> Cambrdige, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [12] <author> Rosenblatt, F. </author> <year> (1957). </year> <title> The perceptron a perceiving and recognizing automaton. </title> <type> Report 85-460-1, </type> <institution> Cornell Aeronautical Laboratory. </institution>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [13] <author> Rosenblatt, F. </author> <year> (1962). </year> <title> Principles of Neurodynamics: Perceptrons and the Theory of Brain Mechanisms. </title> <address> Washington, D.C.: </address> <publisher> Spartan Books. </publisher>
Reference-contexts: The growth function m (n) for separating planes in d dimensions is [6] m (n) = 2 n n d + 1 P d n1 (26) Separating planes are a basic classifier set. For more information on separating planes, see [6, 5]. Perceptrons <ref> [6, 11, 12, 13, 9, 10] </ref> are closely related to separating planes. Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane.
Reference: [14] <editor> Rumelhart, D. E., McClelland, J. L., </editor> <booktitle> and the PDP Research Group (1986). Parallel Distributed Processing. </booktitle> <address> Cambrdige, MA: </address> <publisher> MIT Press. </publisher> <pages> 19 </pages>
Reference: [15] <author> Vapnik, V. N. </author> <year> (1982). </year> <title> Estimation of Dependences Based on Empirical Data (p.31). </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: To simplify the analysis, we use the Hoeffding bound [7] 2e 1 2 * 2 D in place of the partition-based bound B (*). (The Hoeffding bound is smooth, and it is often used in VC analysis <ref> [15] </ref>.) We find that the best disagreement rate goes to O ( 1 * ) as the number of examples increases.
Reference: [16] <author> Vapnik, V. N. </author> <year> (1995). </year> <title> The Nature of Statistical Learning Theory. </title> <address> New York: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Multilayer artificial neural networks often employ separating planes in their final layer. Thus, they can be viewed as first mapping the input space into some intermediate space, then applying a separating plane. This idea is the basis for support vector machines <ref> [16] </ref>. 4.1 Using Theorem 2 To use Theorem 2, we must show that for each fxg there is a multiset of m (n) or fewer output patterns that includes all output patterns produced by separating planes and fulfills a neighbor condition.
Reference: [17] <author> Vapnik, V. N. and Chervonenkis, A. </author> <year> (1971). </year> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and Its Applications, </journal> <volume> 16, </volume> <pages> 264-280. </pages>
Reference-contexts: We use the in-sample data to select a classifier from the class, e.g., through training or data-fitting by other means. Vapnik-Chervonenkis analysis <ref> [17, 1, 3] </ref> provides probabilistic bounds on the test error of the chosen classifier. In this paper, we improve these bounds for several classes. In the next section, we derive VC-type bounds. First, we derive a bound for a single classifier chosen without reference to the data. <p> Also, we extend the result for separating planes to many classes of artificial neural networks. 3 2 Uniform Error Bound We derive a uniform error bound by generalizing the bound at the heart of the original VC paper <ref> [17] </ref>. Let d be the number of in-sample examples, and let d 0 be the number of test examples. Let represent in-sample error, and let -0 represent test error. We begin with a test error bound for a single classifier chosen without reference to the data. <p> Gj-0 &lt; - + D + *jfxggp (fxg)dfxg (17) Use bound (16). fxg M (fxg) B (*))p (fxg)dfxg (18) Let m (n) = max fxg M (fxg),i.e., m (n) is the maximum number of distinct output patterns generated by classifiers in G. (This number is known as the growth function <ref> [17] </ref>.) fxg m (n) B (*))p (fxg)dfxg (19) The probability bound is constant with respect to fxg. = (1 k Z p (fxg)dfxg (20) 7 The integral is unity. Hence, Prf8g 2 Gj-0 &lt; - + D m (n) B (*) (21) We restate the result as a theorem.
References-found: 17

