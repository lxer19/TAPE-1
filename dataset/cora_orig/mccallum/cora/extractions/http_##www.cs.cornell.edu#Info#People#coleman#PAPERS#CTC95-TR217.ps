URL: http://www.cs.cornell.edu/Info/People/coleman/PAPERS/CTC95-TR217.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/coleman/papers.html
Root-URL: 
Title: SUBSPACE, INTERIOR, AND CONJUGATE GRADIENT METHOD FOR LARGE-SCALE BOUND CONSTRAINED MINIMIZATION PROBLEMS  
Author: MARY ANN BRANCHy THOMAS F. COLEMANz AND YUYING LIy 
Keyword: Key Words. Interior method, trust region method, negative curvature direction, inexact Newton step, conjugate gradients, bound-constrained problem, box-constraints  
Note: A  
Abstract: CORNELL THEORY CENTER TECHNICAL REPORT CTC95-TR217 Abstract. A subspace adaptation of the Coleman-Li trust region and interior method is proposed for solving large-scale bound constrained minimization problems. This method can be implemented with either sparse Cholesky factorization or conjugate gradient computation. Under reasonable conditions the convergence properties of this subspace trust region method are as strong as those of its full space version. Computational performance on various large test problems are reported; advantages of our approach are demonstrated. Our experience indicates our proposed method represents an efficient way to solve large bound constrained minimization problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Thomas F. Coleman and Yuying Li. </author> <title> An interior, trust region approach for nonlinear minimization subject to bounds. </title> <journal> SIAM Journal on Optimization, </journal> <volume> 6 </volume> <pages> 418-445, </pages> <year> 1996. </year>
Reference-contexts: 1. Introduction. Recently Coleman and Li <ref> [1, 2, 3] </ref> proposed two interior and reflective Newton methods to solve the bound constrained minimization problem, i.e., min ff (x) : l x ug;(1.1) where l 2 f&lt; [ f1gg n , u 2 f&lt; [ f1gg n , l &lt; u, and f is a smooth function, f : <p> These two methods differ in that a line search to update iterates is used in [2, 3] while a trust region idea is used in <ref> [1] </ref>. However, in both cases convergence is accelerated with the use of a novel reflection technique. The line search method version appears to be computationally viable for large-scale quadratic problems [3]. The trust region framework has more appeal, we feel, for nonlinear problems; however, the method given in [1] is not <p> used in <ref> [1] </ref>. However, in both cases convergence is accelerated with the use of a novel reflection technique. The line search method version appears to be computationally viable for large-scale quadratic problems [3]. The trust region framework has more appeal, we feel, for nonlinear problems; however, the method given in [1] is not suitable for the large-scale case. Our main objective here is to investigate solving large-scale bound constrained nonlinear minimization problems (1.1), using an adaptation of the Trust Region Interior Reflective (TIR) approach proposed in [1] suitable for large problems. The TIR method [1] generalizes the trust region idea for <p> framework has more appeal, we feel, for nonlinear problems; however, the method given in <ref> [1] </ref> is not suitable for the large-scale case. Our main objective here is to investigate solving large-scale bound constrained nonlinear minimization problems (1.1), using an adaptation of the Trust Region Interior Reflective (TIR) approach proposed in [1] suitable for large problems. The TIR method [1] generalizes the trust region idea for unconstrained minimization to bound constrained nonlinear minimization. An attractive feature of the TIR method [1] is that the main computation fl Research partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of <p> problems; however, the method given in <ref> [1] </ref> is not suitable for the large-scale case. Our main objective here is to investigate solving large-scale bound constrained nonlinear minimization problems (1.1), using an adaptation of the Trust Region Interior Reflective (TIR) approach proposed in [1] suitable for large problems. The TIR method [1] generalizes the trust region idea for unconstrained minimization to bound constrained nonlinear minimization. An attractive feature of the TIR method [1] is that the main computation fl Research partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research of the U.S. <p> solving large-scale bound constrained nonlinear minimization problems (1.1), using an adaptation of the Trust Region Interior Reflective (TIR) approach proposed in <ref> [1] </ref> suitable for large problems. The TIR method [1] generalizes the trust region idea for unconstrained minimization to bound constrained nonlinear minimization. An attractive feature of the TIR method [1] is that the main computation fl Research partially supported by the Applied Mathematical Sciences Research Program (KC-04-02) of the Office of Energy Research of the U.S. <p> Concluding remarks appear in x7. The convergence analysis of the STIR method is included in the appendix. 2. The TIR Method. In this section we briefly review the full space TIR method <ref> [1] </ref>, sketched in FIG. 1. This method closely resembles a typical trust region method for unconstrained minimization, min x2&lt; n f (x). The key difference is the presence of the affine scaling (diagonal) matrices D k and C k . <p> This method closely resembles a typical trust region method for unconstrained minimization, min x2&lt; n f (x). The key difference is the presence of the affine scaling (diagonal) matrices D k and C k . Next we briefly motivate these matrices and the TIR algorithm. 2 The TIR Method <ref> [1] </ref> [Let 0 &lt; &lt; &lt; 1, 0 &lt; L l &lt; L u and fl 1 &lt; 1 &lt; fl 2 be given. Let x 0 2 int (F ); D 0 &lt; L u .] 1. <p> Equation (2.3) suggests the use of the affine scaling transformation: x def = D k x. This transformation reduces the constrained problem (1.1) into an unconstrained problem: a local minimizer of (1.1) corre sponds to an unconstrained minimizer in the new coordinates x (for more details, see <ref> [1] </ref>). Therefore a reasonable way to improve x k is to solve the trust region subproblem min f k ( s) : k sk 2 D k g;(2.5) where k ( s) = g T 1 Let s = D 1 k s. <p> Let k 2 <ref> [ l ; 1] </ref> for some 0 &lt; l &lt; 1 and k 1 = O (kd k k). <p> Explicit conditions which yield first and second order optimality are analogous to those of trust region methods for unconstrained minimization <ref> [1] </ref>: (AS.3) k (s k ) &lt; fi fl k g k ], kD k s k k fi 0 D k ; x k + s k 2 int (F ). (AS.4) Assume that p k is a solution to min s2&lt; n f k (s) : kD k sk <p> Thus it appears that the effectiveness of the Steihaug idea decreases as nonlinearity increases. 4. The STIR Method. Supported by the discussion in x3, we propose a large-scale subspace adaptation of the TIR method <ref> [1] </ref> for the bound constrained problem (1.1). <p> The degeneracy definition is as in <ref> [1] </ref>. DEFINITION 4.2. <p> This is consistent with results presented in x3, e.g., see Table 4. 7. Conclusion. Based on the trust region interior reflective (TIR) method in <ref> [1] </ref>, we have proposed a subspace TIR method (STIR) suitable for large-scale minimization with bound constraints on the variables. In particular, we consider a two dimensional STIR in which a subspace is formed from the scaled gradient and (inexact or exact) Newton steps or a negative curvature direction.
Reference: [2] <author> Thomas F. Coleman and Yuying Li. </author> <title> On the convergence of reflective Newton methods for large-scale nonlinear minimization subject to bounds. </title> <journal> Mathematical Programming, </journal> <volume> 67 </volume> <pages> 189-224, </pages> <year> 1994. </year>
Reference-contexts: 1. Introduction. Recently Coleman and Li <ref> [1, 2, 3] </ref> proposed two interior and reflective Newton methods to solve the bound constrained minimization problem, i.e., min ff (x) : l x ug;(1.1) where l 2 f&lt; [ f1gg n , u 2 f&lt; [ f1gg n , l &lt; u, and f is a smooth function, f : <p> These two methods differ in that a line search to update iterates is used in <ref> [2, 3] </ref> while a trust region idea is used in [1]. However, in both cases convergence is accelerated with the use of a novel reflection technique. The line search method version appears to be computationally viable for large-scale quadratic problems [3]. <p> For example, one can choose s k so that k (s k ) is the minimum of the values fl k [p k ] and fl k g k ]. However, this does not lead to an efficient algorithm. In [3] and <ref> [2] </ref>, we utilized a reflection technique to permit further possible reduction of the objective function along a reflection path on the boundary. We found in [3] and [2] that this reflection process significantly enhances performance for minimizing a general quadratic function subject to simple bounds. <p> However, this does not lead to an efficient algorithm. In [3] and <ref> [2] </ref>, we utilized a reflection technique to permit further possible reduction of the objective function along a reflection path on the boundary. We found in [3] and [2] that this reflection process significantly enhances performance for minimizing a general quadratic function subject to simple bounds.
Reference: [3] <author> Thomas F. Coleman and Yuying Li. </author> <title> A reflective Newton method for minimizing a quadratic function subject to bounds on the variables. </title> <note> SIAM Journal on Optimization, to appear. </note>
Reference-contexts: 1. Introduction. Recently Coleman and Li <ref> [1, 2, 3] </ref> proposed two interior and reflective Newton methods to solve the bound constrained minimization problem, i.e., min ff (x) : l x ug;(1.1) where l 2 f&lt; [ f1gg n , u 2 f&lt; [ f1gg n , l &lt; u, and f is a smooth function, f : <p> These two methods differ in that a line search to update iterates is used in <ref> [2, 3] </ref> while a trust region idea is used in [1]. However, in both cases convergence is accelerated with the use of a novel reflection technique. The line search method version appears to be computationally viable for large-scale quadratic problems [3]. <p> However, in both cases convergence is accelerated with the use of a novel reflection technique. The line search method version appears to be computationally viable for large-scale quadratic problems <ref> [3] </ref>. The trust region framework has more appeal, we feel, for nonlinear problems; however, the method given in [1] is not suitable for the large-scale case. <p> For example, one can choose s k so that k (s k ) is the minimum of the values fl k [p k ] and fl k g k ]. However, this does not lead to an efficient algorithm. In <ref> [3] </ref> and [2], we utilized a reflection technique to permit further possible reduction of the objective function along a reflection path on the boundary. We found in [3] and [2] that this reflection process significantly enhances performance for minimizing a general quadratic function subject to simple bounds. <p> However, this does not lead to an efficient algorithm. In <ref> [3] </ref> and [2], we utilized a reflection technique to permit further possible reduction of the objective function along a reflection path on the boundary. We found in [3] and [2] that this reflection process significantly enhances performance for minimizing a general quadratic function subject to simple bounds.
Reference: [4] <author> Jorge J. Mor e and D.C. Sorensen. </author> <title> Computing a trust region step. </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 4 </volume> <pages> 553-572, </pages> <year> 1983. </year>
Reference-contexts: The method of Mor e and Sorensen <ref> [4] </ref> can be directly applied to (1.2) if sparse Cholesky factorizations can be computed efficiently. However, this method is unsuitable for large-scale problems if the Hessian matrix is not explicitly available or (sparse) Cholesky factorizations are too expensive.
Reference: [5] <author> D.C. Sorensen. </author> <title> Minimization of a large scale quadratic function subject to an ellipsoidal constraint. </title> <type> Technical Report TR94-27, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <year> 1994. </year>
Reference-contexts: The method of Mor e and Sorensen [4] can be directly applied to (1.2) if sparse Cholesky factorizations can be computed efficiently. However, this method is unsuitable for large-scale problems if the Hessian matrix is not explicitly available or (sparse) Cholesky factorizations are too expensive. Recently, Sorensen <ref> [5] </ref> proposed a new method for solving the subproblem (1.2) using matrix vector multiplications. Nonetheless, the effectiveness of this approach for large-scale minimization, particularly in the context of our trust region algorithm, is yet to be investigated.
Reference: [6] <author> T. Steihaug. </author> <title> The conjugate gradient method and trust regions in large scale optimization. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 20 </volume> <pages> 626-637, </pages> <year> 1983. </year>
Reference-contexts: Nonetheless, the effectiveness of this approach for large-scale minimization, particularly in the context of our trust region algorithm, is yet to be investigated. We take the view that solving the full space trust region subproblem (1.2) is too costly for a large-scale problem. This view is shared by Steihaug <ref> [6] </ref> who proposes an approximate (conjugate gradient) approach. Steihaug's approach to (1.2) seems viable although our computational experience (see Table 4) indicates that important negative curvature information can be missed, causing a significant increase in the number of minimization iterations. <p> We show that the reflection technique can substantially reduce the number of minimization iterations. Third, our computational experiments support the notion that the subspace trust region method is a promising way to solve large-scale bound constrained nonlinear minimization problems. Compared to the Steihaug <ref> [6] </ref> approach, the subspace approach appears more likely to capture negative curvature information: this leads to better computational performance. Finally, our subspace method is competitive with, and often superior to, the active set method in LANCELOT [8]. The paper is organized as follows. <p> uncon strained setting by min f k (s) : ksk 2 D k ; s 2 S k g;(3.1) where S k is a low dimensional subspace. (Our implementation employs a two dimensional choice for S k .) Another possible consideration for the approximation of (1.2) is the Steihaug idea <ref> [6] </ref>, also proposed in the large-scale unconstrained minimization setting. <p> We believe that a subspace trust region approach better captures the negative curvature information compared to the Steihaug approach <ref> [6] </ref>. To justify this we have conducted a limited computational study in the unconstrained minimization setting.
Reference: [7] <author> I.I. Dikin. </author> <title> Iterative solution of problems of linear and quadratic programming. </title> <journal> Doklady Akademiia Nauk SSSR, </journal> <volume> 174 </volume> <pages> 747-748, </pages> <year> 1967. </year>
Reference-contexts: In addition, we demonstrate the benefits of our affine scaling, reflection and subspace techniques with computational results. First, for (1.1), our affine scaling technique outperforms the classical Dikin scaling <ref> [7] </ref>, at least in the context of our algorithm. Second, we examine our method with and without reflection. We show that the reflection technique can substantially reduce the number of minimization iterations. <p> The scaling matrix used in our approach is related to, but different from, the scaling typically used in affine scaling methods for linear programming. The affine scaling matrix D affine k = diag (min (x k l k ; u k x k )) <ref> [7] </ref>, commonly used in linear programming, is formed from the distance of variables to their closest bounds.
Reference: [8] <author> A. R. Conn, N. I. M. Gould, and Ph. L. Toint. LANCELOT: </author> <title> A Fortran Package for Large-Scale Nonlinear Optimization (Release A). </title> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: Compared to the Steihaug [6] approach, the subspace approach appears more likely to capture negative curvature information: this leads to better computational performance. Finally, our subspace method is competitive with, and often superior to, the active set method in LANCELOT <ref> [8] </ref>. The paper is organized as follows. In x2, we briefly summarize the existing TIR method. Then we provide a computational comparison of the subspace trust region method and the Steihaug algorithm in the context of unconstrained minimization in x3. <p> Computational Experience. We demonstrate the computational performance of our STIR method given in FIG.4. Below we report our experience with the modified Cholesky and the conjugate gradient (MPCG) implementations. We examine the sensitivity of the STIR method to a starting point. Finally, some limited comparisons with SBMIN of LANCELOT <ref> [8] </ref> are also made. In the implementation of STIR, we compute s k using a reflective technique as shown in FIG.2. The exact trust region updating procedure is given in FIG.5. <p> This is in contrast to active set methods where the starting point can have a more dramatic effect on the iteration count. Last we contrast the performance of the STIR method using the conjugate gradient option with the SBMIN algorithm, an active set method, in the LANCELOT software package <ref> [8] </ref>. In particular, we choose problems where negative curvature is present or where it appears that the active set at the solution may be difficult to find. We expect our STIR method to outperform an active set method in these situations; indeed, we have found this to be the case. <p> The results strongly support the different components of our approach: the subspace idea, the use of our novel affine 22 scaling matrix, the modified Cholesky factorization and conjugate gradient variations, and the reflection technique. Moreover, preliminary experimental comparisons with code SBMIN, from LANCELOT <ref> [8] </ref>, indicate that our proposed STIR method can significantly outperform an active set approach for some large-scale problems. 23
Reference: [9] <author> Bruce A. Hendrickson. </author> <title> The molecule problem: Determining conformation from pairwise distances. </title> <type> Cornell University Ph.D thesis, </type> <institution> Computer Science, </institution> <year> 1991. </year>
Reference-contexts: We consider random problem instances of molecule minimization <ref> [9, 10] </ref>, which minimize a quartic subject to bounds on the variables. Tables 1 and 2 list the average number of iterations (over ten random test problem instances for each entry) required for the different techniques under comparison. <p> We used twenty different unconstrained nonlinear test problems. All but four are test problems described in [12], but with all the bound constraints removed. The problems EROSENBROCK and EPOWELL are taken from [13]. The last two problems, molecule problems MOLE1 and MOLE3, are described in <ref> [9, 10] </ref>. For all problems, the number of variables n is 260. The minimization algorithm terminates when kgk 2 10 6 . We use the parameter = 0:0005 in both FIG. 10 and FIG. 11.
Reference: [10] <author> Thomas F. Coleman. </author> <title> Large-scale numerical optimization: Introduction and overview. </title> <editor> In Allen Kent and James G. Williams, editors, </editor> <booktitle> Encyclopedia of Computer Science and Technology, </booktitle> <pages> pages 167-195. </pages> <publisher> Marcel Dekker, INC., </publisher> <year> 1993. </year>
Reference-contexts: We consider random problem instances of molecule minimization <ref> [9, 10] </ref>, which minimize a quartic subject to bounds on the variables. Tables 1 and 2 list the average number of iterations (over ten random test problem instances for each entry) required for the different techniques under comparison. <p> We used twenty different unconstrained nonlinear test problems. All but four are test problems described in [12], but with all the bound constraints removed. The problems EROSENBROCK and EPOWELL are taken from [13]. The last two problems, molecule problems MOLE1 and MOLE3, are described in <ref> [9, 10] </ref>. For all problems, the number of variables n is 260. The minimization algorithm terminates when kgk 2 10 6 . We use the parameter = 0:0005 in both FIG. 10 and FIG. 11.
Reference: [11] <author> Gerald A. Shultz, R. B. Schnabel, and Richard H. Byrd. </author> <title> A family of trust-region-based algorithms for unconstrained minimization with strong global convergence properties. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 22(1) </volume> <pages> 47-67, </pages> <year> 1985. </year>
Reference-contexts: A single reflection strategy was used for the constrained problems (with both scalings). 3. Approximation to the Trust Region Solution in Unconstrained Minimization. There are two possible ways to approximate a full space trust region solution in unconstrained minimization. Byrd, Schnabel, and Schultz <ref> [11] </ref> suggest replacing the full trust region subproblem in the uncon strained setting by min f k (s) : ksk 2 D k ; s 2 S k g;(3.1) where S k is a low dimensional subspace. (Our implementation employs a two dimensional choice for S k .) Another possible consideration
Reference: [12] <author> Andrew R. Conn, N. I. M. Gould, and Ph. L. Toint. </author> <title> Testing a class of methods for solving minimization problems with simple bounds on the variables. </title> <journal> Mathematics of Computation, </journal> <volume> 50(182) </volume> <pages> 399-430, </pages> <year> 1988. </year>
Reference-contexts: We let D 0 = 0:1kg 0 k where the k k 2 is used for the subspace method and k k G for the Steihaug method ([6]). We used twenty different unconstrained nonlinear test problems. All but four are test problems described in <ref> [12] </ref>, but with all the bound constraints removed. The problems EROSENBROCK and EPOWELL are taken from [13]. The last two problems, molecule problems MOLE1 and MOLE3, are described in [9, 10]. For all problems, the number of variables n is 260. <p> We also impose an upper bound of 600 on the number of iterations. We first report the results of the STIR method using the modified Cholesky factorization. Table 5 lists the number of major iterations required for some standard test problems (for details of these problems see <ref> [12] </ref>). (For all the results in this paper, the number of iterations is the same as the number of objective function evaluations.) The problem sizes vary from 100 to 10; 000. <p> STIR method with exact Newton steps at varied starting points The results in Table 8 were obtained using the conjugate gradient implementation, also on problems with 1000 variables. The starting points are as follows: original is the suggested starting point according to <ref> [12] </ref>; upper starts all variables at upper bounds; lower starts all variables at the lower bounds; middle starts at the midpoint between bounds; zero starts each variable at zero (the origin); upper-lower starts the odd variables at the upper and the even variables at the lower bounds; lower-upper is the reverse <p> Our proposed STIR method, a strictly interior method, moves directly to the solution without faltering when started at the same point. Table 10 summarizes the performances of STIR and SBMIN, on a set of constrained problems exhibiting negative curvature. (Again the problems are from <ref> [12] </ref> except the last two have been constrained differently to display negative curvature.) STIR is significantly better on these problems this is probably due to the fact that negative curvature is better exploited in our subspace trust region approach than in the Steihaug trust region method, which SBMIN employs.
Reference: [13] <author> Jorge J. Mor e, Burton S. Garbow, and Kenneth. E. Hillstrom. </author> <title> Testing unconstrained optimization software. </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 7 </volume> <pages> 17-41, </pages> <year> 1981. </year>
Reference-contexts: We used twenty different unconstrained nonlinear test problems. All but four are test problems described in [12], but with all the bound constraints removed. The problems EROSENBROCK and EPOWELL are taken from <ref> [13] </ref>. The last two problems, molecule problems MOLE1 and MOLE3, are described in [9, 10]. For all problems, the number of variables n is 260. The minimization algorithm terminates when kgk 2 10 6 . We use the parameter = 0:0005 in both FIG. 10 and FIG. 11.
Reference: [14] <author> Richard H. Byrd, Robert B. Schnabel, and Gerald A. Shultz. </author> <title> Approximate solution of the trust region problem by minimization over two-dimensional subspaces. </title> <journal> Mathematical Programming, </journal> <volume> 40 </volume> <pages> 247-263, </pages> <year> 1988. </year>
Reference-contexts: To guarantee that x fl is also a second order point, i.e., satisfies second order necessary conditions, the following conditions must be met. Firstly, it is clear that a sufficient negative curvature condition must be carried over from the unconstrained setting <ref> [14] </ref>.
Reference: [15] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> The Johns Hopkins University Press, </publisher> <year> 1989. </year>
Reference-contexts: Therefore, approximations, and short cuts, are in order. Can we compute approximate eigenvectors with the angle property? A good approximation to an eigenvector corresponding to an extreme eigenvalue can usually be obtained through a Lanczos process <ref> [15] </ref>.
Reference: [16] <author> Jane K. Cullum and Ralph A. Willoughby. </author> <title> Lanczos Algorithms for Large symmetric eigenvalue computations, </title> <booktitle> Vol. 1 Theory. Birkha user, </booktitle> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: In other words, in order to generate a negative curvature direction sequence with the angle property, orthogonality needs to be maintained in the Lanczos process. In practise maintaining orthogonality can be a delicate and expensive business <ref> [16] </ref>. A second (and cheaper) strategy is to employ a modified preconditioned conjugate gradient scheme, e.g., MPCG in FIG.11. Unfortunately, this process is not guaranteed to generate sufficient negative curvature; nonetheless, as indicated in [17], the MPCG output will satisfy the the angle property.
Reference: [17] <author> Mary Ann Branch. </author> <title> Inexact Reflective Newton Methods for Large-Scale Optimization Subject to Bound Constraints. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Cornell University, </institution> <year> 1995. </year>
Reference-contexts: In practise maintaining orthogonality can be a delicate and expensive business [16]. A second (and cheaper) strategy is to employ a modified preconditioned conjugate gradient scheme, e.g., MPCG in FIG.11. Unfortunately, this process is not guaranteed to generate sufficient negative curvature; nonetheless, as indicated in <ref> [17] </ref>, the MPCG output will satisfy the the angle property. Finally we consider a modified Cholesky factorization, e.g., [18], to obtain a negative curvature direction. LEMMA 3. Assume that f M k g is indefinite and fd k g is obtained from the modified Cholesky method.

References-found: 17

