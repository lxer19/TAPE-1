URL: http://www.sfs.nphil.uni-tuebingen.de/~abney/96o.ps.gz
Refering-URL: 
Root-URL: 
Email: abney@sfs.nphil.uni-tuebingen.de  
Title: Stochastic Attribute-Value Grammars  
Author: Steven Abney 
Address: Wilhelmstr. 113, 72074 Tubingen, Germany  
Web: http://www.sfs.nphil.uni-tuebingen.de/~abney/  
Note: Many of my papers are available in PostScript on my web page:  
Affiliation: University of Tubingen  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Chris Brew. </author> <title> Stochastic HPSG. </title> <booktitle> In Proceedings of EACL-95, </booktitle> <year> 1995. </year>
Reference-contexts: Attempts have been made to extend stochastic models developed for the regular and context-free cases to attribute-value grammars, but to date without success. 1 Brew <ref> [1] </ref> sketches a probabilistic version of HPSG, but admits 1 I confine my discussion here to Brew and Eisele because they aim to describe parametric models of probability distributions over the languages of constraint-based grammars, and to estimate the parameters of those models.
Reference: [2] <author> Andreas Eisele. </author> <title> Towards probabilistic extensions of constraint-based grammars. </title> <type> Technical Report Deliverable R1.2.B, </type> <institution> DYANA-2, </institution> <year> 1994. </year>
Reference-contexts: This interpretation avoids the need for normalization that Brew and Eisele face, though parameter estimation still remains to be addressed. 1 that his way of dealing with re-entrancies in feature structures is problematic. Eisele <ref> [2] </ref> attempts to translate stochastic context-free techniques to constraint-based grammar by assigning probabilities to SLD proof trees. Both Brew and Eisele propose associating weights with grammar-rule analogues (typed feature structures in Brew's case; Horn clauses in Eisele's case) and setting weights proportional to expected rule frequencies.
Reference: [3] <author> Kevin Mark, Michael Miller, Ulf Grenander, and Steve Abney. </author> <title> Parameter estimation for constrained context-free language models. </title> <booktitle> In Proceedings of the Fifth Darpa Workshop on Speech and Natural Language, </booktitle> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufman. </publisher>
Reference-contexts: They were originally studied by Gibbs, nearly a hundred years ago, as a model for statistical mechanics, and the general family of probability distributions involved is still known by his name. 2 To my knowledge, the first application of random fields to natural language was by Mark et al. <ref> [3] </ref>. The problem of interest was how to combine a stochastic context-free grammar with n-gram language models.
Reference: [4] <author> M.I. Miller and J.A. O'Sullivan. </author> <title> Entropies, combinatorics and probabilities of context-free branching processes. </title> <type> Technical report ESSRL-90-16, </type> <institution> Electronic Systems and Signals Research Laboratory, Washington University, </institution> <year> 1990. </year>
Reference-contexts: The uniform distribution maximizes entropy over a finite set. Maximizing entropy is more generally applicable, however, and can be applied to infinite sets as well. Maximum entropy distributions for context-free languages are discussed in a paper by Miller and O'Sullivan <ref> [4] </ref>, though a number of technical questions arise that I do not wish to pursue here. 5.2 Property Selection At each iteration, we select a new property f by considering all atomic properties and all complex properties that can be constructed from properties already in the field.
Reference: [5] <author> Stephen Della Pietra, Vincent Della Pietra, and John Lafferty. </author> <title> Inducing features of random fields. </title> <type> tech report CMU-CS-95-144, CMU, </type> <year> 1995. </year>
Reference-contexts: The resulting structures, e.g., (1), obviously involve re-entrancies and context-sensitivity. (1) It was clear at that time that a similar approach ought to succeed for general attribute-value grammars, but the issue was not pursued. Recent work by Della Pietra, Della Pietra, and Lafferty <ref> [5] </ref> (henceforth, DDL) also applies random fields to natural language processing. The application they consider is the induction of English orthographic constraints|inducing a grammar of possible English words. <p> Methods for doing both are given in a recent paper by Della Pietra, Della Pietra, and Lafferty <ref> [5] </ref>. 5 Field Induction In outline, the DDL algorithm is as follows: 1. Start (t = 0) with the null field (no properties). 2. Property Selection. Consider every property that might be added to the field q t and choose the best one. 3. Weight Adjustment.
Reference: [6] <author> Stefan Riezler. </author> <title> Quantitative constraint logic programming for weighted grammar applications. </title> <note> Talk given at LACL, </note> <month> September </month> <year> 1996. </year>
Reference-contexts: Other authors have assigned weights or preferences to constraint-based grammars but not discussed parameter estimation. One approach of the latter sort that I find of particular interest is that of Stefan Riezler <ref> [6] </ref>, who describes a weighted logic for constraint-based grammars that characterizes the languages of the grammars as fuzzy sets.
Reference: [7] <author> Gerhard Winkler. </author> <title> Image Analysis, Random Fields and Dynamic Monte Carlo Methods. </title> <publisher> Springer, </publisher> <year> 1995. </year> <month> 22 </month>
Reference-contexts: Winkler <ref> [7] </ref>). The final "acceptance" step intuitively serves the role of "punishing" dags that the p-sampler proposes more often than a q-sampler would, and 20 shifting their probability to dags that the p-sampler would propose less often than a q-sampler would.
References-found: 7

