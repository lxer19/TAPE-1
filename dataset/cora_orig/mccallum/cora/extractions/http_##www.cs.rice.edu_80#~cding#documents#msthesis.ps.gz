URL: http://www.cs.rice.edu:80/~cding/documents/msthesis.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~cding/publications.html
Root-URL: 
Title: Improving Software Pipelining with Unroll-and-Jam and Memory Reuse Analysis  
Author: By Chen Ding 
Degree: A THESIS Submitted in partial fulfillment of the requirements for the degree of MASTER OF SCIENCE IN COMPUTER SCIENCE  
Date: 1996  
Affiliation: MICHIGAN TECHNOLOGICAL UNIVERSITY  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> A. Aiken and A. Nicolau. </author> <title> Perfect Pipelining: A New Loop Optimization Technique. </title> <booktitle> In Proceedings of the 1988 European Symposium on Programming, Springer Verlag Lecture Notes in Computer Science, </booktitle> <volume> #300, </volume> <pages> pages 221-235, </pages> <address> Atlanta, GA, </address> <month> March </month> <year> 1988. </year>
Reference-contexts: This problem leads to sub-optimal resource usage in software pipelining. 14 Perfect Pipelining Perfect pipelining is motivated by a global compaction method, percolation scheduling [2] and assumes hardware support of multiple branches per cycle <ref> [1] </ref>. The algorithm first performs code motion. In scheduling, infinite resources are assumed and operations are scheduled as early as possible. To force a pattern to occur, the algorithm limits the scheduling scope to a fixed number of iterations. The fixed number is determined by experiment.
Reference: [2] <author> A. Aiken and A. Nicolau. </author> <title> A development environment for horizontal microcode. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 14(5) </volume> <pages> 584-594, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Moreover, if a pattern is not perfect, extra operations exist in the pattern and they do not represent resource requirements. This problem leads to sub-optimal resource usage in software pipelining. 14 Perfect Pipelining Perfect pipelining is motivated by a global compaction method, percolation scheduling <ref> [2] </ref> and assumes hardware support of multiple branches per cycle [1]. The algorithm first performs code motion. In scheduling, infinite resources are assumed and operations are scheduled as early as possible. To force a pattern to occur, the algorithm limits the scheduling scope to a fixed number of iterations.
Reference: [3] <author> V. H. Allan, U. R. Shah, and K. M. Reddy. </author> <title> Petri net versus modulo scheduling for software pipelining. </title> <booktitle> In Proceedings of the 28th International Symposium on Microarchitecture (MICRO-28), </booktitle> <pages> pages 125-133, </pages> <address> Ann Arbor, MI, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Rajagopalan and Allan's model can also handle resource constraints and predicates in a smooth way. In comparing with other methods, Rajagopalan and Allan claimed that their model achieved the best performance in unrolling-while-scheduling methods and performance similar to the best modulo scheduling method available [32] <ref> [3] </ref>. 2.2.2 Modulo Scheduling In contrast to scheduling-while-unrolling methods which unroll a loop and schedule multiple iterations, another class of scheduling methods, modulo scheduling, generates a pipelined schedule for a single iteration of a loop. Modulo scheduling imposes two restrictions on software pipelines. 15 * 1.
Reference: [4] <author> E.R. Altman, R. Govindarajan, and G.R. Gao. </author> <title> Scheduling and Mapping: Software Pipelining in the Presence of Structural Hazards. </title> <booktitle> In Proceedings of the ACM SIGPLAN '95 Conference on Programming Language Design and Implementation", </booktitle> <pages> pages 139-150, </pages> <address> La Jolla, CA, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: A leading heuristic-based method, iterative modulo scheduling, will be described in detail in Section 2.4 since it is the method used in this research. Integer Linear Programming In parallel with heuristic-based research on modulo scheduling, another work, Integer Linear Programming (LP) [21] <ref> [4] </ref>, was developed under a strict mathematical formulation, namely integer programming. Though the method has a possible exponential compilation time, it can 16 find a software pipeline schedule that uses a given number of resources at the smallest initiation interval while minimizing the number of registers it uses. <p> LP can also be used to generate pipelined schedules for imperfect hardware architectures. Altman et al. found that for many loops they tested, the compilation time is in seconds <ref> [4] </ref>. 2.3 Software Pipelining Constraints The II (initiation interval) of a loop is constrained by both the parallelism in the loop and the limited hardware resources. The amount of parallelism in the loop is determined by data recurrences. Hardware constraints are determined by the characteristics of a target machine. <p> However, it should yield improved software pipelining when compared to either an all-cache-hit or an all-cache-miss policy. 59 Chapter 5 Improving Modulo Scheduling Modulo scheduling is a effective scheduling technique for software pipelining. Experimental results have shown that it can achieve near-optimal II for most benchmark loops [33] <ref> [4] </ref>. This chapter discusses two enhancements to modulo scheduling. Previously, modulo scheduling could not eliminate all false recurrences without the hardware support of rotating registers. Section 1 proposes a compiler algorithm that can efficiently eliminate the effect of all false recurrences for conventional architectures.
Reference: [5] <author> D. Callahan, S. Carr, and Ken Kennedy. </author> <title> Improving register allocation for subscripted variables. </title> <booktitle> In PLDI90, </booktitle> <pages> pages 53-65, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Scalar replacement, or load-store-elimination is a technique that can be used to replace memory loads and stores with scalar operations [7] <ref> [5] </ref> [15] [10]. <p> This section describes how unroll-and-jam removes memory operations and how software pipelining resource constraints can be reduced by unroll-and-jam. 3.3.1 Removing Memory Operations Scalar replacement, or load-store-elimination, has been used to reduce the number of memory load/store operations by reusing values across innermost loop iterations [7] <ref> [5] </ref> [15] [10]. That is, instead of storing a value at this iteration and loading it at the next iteration, the value can be maintained in a register and the load and store operations of that value can be eliminated.
Reference: [6] <author> D. Callahan, K. Cooper, R. Hood, K. Kennedy, and L. Torczon. </author> <title> Parascope: A parallel programming environment. </title> <booktitle> In Proceedings of the First International Conference on Supercomputing, </booktitle> <address> Athens, Greese, </address> <year> 1987. </year>
Reference-contexts: Rocket is a retargetable optimizing compiler [40]. It performs traditional optimizations and program analyses. Rocket currently has two front ends, one for C and one for Fortran programs. The C front-end generates Rocket intermediate code directly. The Fortran front-end takes the intermediate code, ILoc, generated by ParaScope <ref> [6] </ref>, and converts it to Rocket intermediate code. All Rocket backend optimizations, including software pipelining, use Rocket intermediate code. The following subsections describe implementation related issues and tradeoffs in this research. 74 Loop Selection The current implementation only handles single-basic-block loops. <p> This research described here evaluates the performance of Fortran programs on the URM machine. 6.1.2 Instrumentation The experiment uses the implementation of unroll-and-jam and memory reuse analysis in Memoria, the implementation of array analysis in Parascope <ref> [6] </ref>, the software pipelining in Rocket, and the URM simulator [30]. The general experimental method is graphically described in and generates the transformed Fortran program with comments indicating the reuse information. Parascope compiles the Fortran program into Iloc intermediate code [6]. <p> analysis in Memoria, the implementation of array analysis in Parascope <ref> [6] </ref>, the software pipelining in Rocket, and the URM simulator [30]. The general experimental method is graphically described in and generates the transformed Fortran program with comments indicating the reuse information. Parascope compiles the Fortran program into Iloc intermediate code [6]. Parascope performs various global code optimizations, as well as array analysis. The result of array analyses, along with the result of memory reuse analysis generated by Memoria, are added as comments in the generated Iloc code.
Reference: [7] <author> D. Callahan and K. Kennedy. </author> <title> Estimating interlock and improving balance for pipelined machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 5 </volume> <pages> 334-358, </pages> <year> 1988. </year>
Reference-contexts: This research uses a different approach to exploit cross-loop parallelism for nested loops. It uses an outer-loop unrolling technique, unroll-and-jam <ref> [7] </ref>[9], before software pipelining. Unroll-and-jam unrolls an outer loop and then jams the resulting inner loops back together [7] [9]. By jamming iterations of outer loops into the innermost loop, unroll-and-jam brings cross-loop parallelism into the innermost loop. After unroll-and-jam, software pipelining can be performed on iterations from multiple loops and both inter-iteration and cross-loop parallelism can be utilized by software pipelining. <p> Scalar replacement, or load-store-elimination is a technique that can be used to replace memory loads and stores with scalar operations <ref> [7] </ref> [5] [15] [10]. <p> Thus, unroll-and-jam can exploit cross-loop parallelism for software pipelining without restrictions. Section 3 examines the other benefit of unroll-and-jam to software pipelining. Unroll-and-jam has been shown to be very effective in automatically removing memory bottlenecks <ref> [7] </ref> [9]. Section 3 shows that the removing of memory bottlenecks can significantly decrease the resource constraint of software pipelining and thus improve software pipelining's performance. One difficulty of using unroll-and-jam with software pipelining is that the prediction of register pressure of the transformed loop is difficult. <p> This section describes how unroll-and-jam removes memory operations and how software pipelining resource constraints can be reduced by unroll-and-jam. 3.3.1 Removing Memory Operations Scalar replacement, or load-store-elimination, has been used to reduce the number of memory load/store operations by reusing values across innermost loop iterations <ref> [7] </ref> [5] [15] [10]. That is, instead of storing a value at this iteration and loading it at the next iteration, the value can be maintained in a register and the load and store operations of that value can be eliminated. <p> Therefore, it is effective in removing the memory bottleneck. However, different loops need different degrees of unrolling and replacing to achieve optimal machine resource usage. This section describes the heuristics that are used to achieve the optimal resource utilization. The basic criteria was defined by Callahan et al <ref> [7] </ref>. 48 Carr and Kennedy gave the algorithm that automatically computes the optimal degree of unrolling for unroll-and-jam [9]. The criteria is the same when using unroll-and-jam to remove memory bottlenecks to software pipelining. <p> However, in computing the optimal degree of unrolling, the effect of overlapping execution needs to be considered. To illustrate the relation between the resource demand of a loop and the resources available in a machine, Callahan et al <ref> [7] </ref> defined the machine balance as fi M = number of memory operations canbe issued per cycle number of f loating point operations can be issued per cycle and the loop balance as fi L = number of memory operations in the loop number of f loating point operations in the <p> the machine balance as fi M = number of memory operations canbe issued per cycle number of f loating point operations can be issued per cycle and the loop balance as fi L = number of memory operations in the loop number of f loating point operations in the loop <ref> [7] </ref>. When fi L = fi M , the memory functional units and the floating-point functional units can all be fully used. However, when fi L fi M , a loop is memory bound and the floating-point functional units cannot be fully utilized due to the high memory resource demand.
Reference: [8] <author> S. Carr, C. Ding, and P. Sweany. </author> <title> Improving Software Pipelining With Unroll-and-Jam. </title> <booktitle> In 28th Hawaii International Conference on System Sciences, </booktitle> <year> 1996. </year> <month> 93 </month>
Reference-contexts: After register assignment, URM assembly code is generated. Performance results can be obtained by using the URM simulator. 76 6.2 Improvement Obtained by Using Unroll-and-Jam This section presents the result of combining unroll-and-jam with software pipelining. A similar experiment has been done previously <ref> [8] </ref>. The results of the previous experiment were not complete for the following reasons. First, the array analysis result was added into the compiler by hand, so it is not completely accurate. <p> Moreover, in the previous experiment, we did not perform register assignment and code generation. These shortcomings have been addressed in the latest experiment. Due to the above reasons, the results obtained in this experiment are more complete than previous results described in <ref> [8] </ref>. 6.2.1 Machine Model For our target architecture we chose a machine with four integer units and two floating-point units. Only one of the integer units can be used for memory operations.
Reference: [9] <author> S. Carr and K. Kennedy. </author> <title> Improving the ratio of memory operations to floating-point operations in loops. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 16(6) </volume> <pages> 1768-1810, </pages> <year> 1994. </year>
Reference-contexts: This research uses a different approach to exploit cross-loop parallelism for nested loops. It uses an outer-loop unrolling technique, unroll-and-jam [7]<ref> [9] </ref>, before software pipelining. Unroll-and-jam unrolls an outer loop and then jams the resulting inner loops back together [7] [9]. By jamming iterations of outer loops into the innermost loop, unroll-and-jam brings cross-loop parallelism into the innermost loop. After unroll-and-jam, software pipelining can be performed on iterations from multiple loops and both inter-iteration and cross-loop parallelism can be utilized by software pipelining. <p> However, unroll-and-jam can catch such reuse among iterations of different loops. Moreover, an unroll-and-jam algorithm developed by Carr and Kennedy can automatically compute the optimal degree of unrolling to remove memory bottlenecks for a given machine <ref> [9] </ref>. The effect of removing memory bottlenecks with unroll-and-jam has not been examined in the context of software pipelining. The second work of this research is to use unroll-and-jam with software pipelining, and measure the improvement in the performance of software pipelining after the removal of memory bottlenecks. <p> Too much register pressure would significantly degrade the performance of the transformed loop due to the high cost of register spilling. Carr and Kennedy developed a method that predicts the register pressure after unroll-and-jam and thus controls the degree of unrolling <ref> [9] </ref>. However, their formula does not consider the effect of software pipelining. This research examines the amount of additional register pressure in software pipelining and investigates a new prediction method. Chapter 3 explains how unroll-and-jam removes memory bottlenecks and investigates the register problem. <p> Thus, unroll-and-jam can exploit cross-loop parallelism for software pipelining without restrictions. Section 3 examines the other benefit of unroll-and-jam to software pipelining. Unroll-and-jam has been shown to be very effective in automatically removing memory bottlenecks [7] <ref> [9] </ref>. Section 3 shows that the removing of memory bottlenecks can significantly decrease the resource constraint of software pipelining and thus improve software pipelining's performance. One difficulty of using unroll-and-jam with software pipelining is that the prediction of register pressure of the transformed loop is difficult. <p> One difficulty of using unroll-and-jam with software pipelining is that the prediction of register pressure of the transformed loop is difficult. Previous register prediction schemes do not consider the effect of the overlapping execution in software pipelining <ref> [9] </ref>. Section 4 discusses several possible approaches that can help predict register pressure in software pipelining after unroll-and-jam. The final section compares the effect of unroll-and-jam on software pipelining with some other loop transformation techniques. 36 3.1 Unroll-and-Jam Unroll-and-jam is an outer-loop unrolling technique [7][9]. <p> This section describes the heuristics that are used to achieve the optimal resource utilization. The basic criteria was defined by Callahan et al [7]. 48 Carr and Kennedy gave the algorithm that automatically computes the optimal degree of unrolling for unroll-and-jam <ref> [9] </ref>. The criteria is the same when using unroll-and-jam to remove memory bottlenecks to software pipelining. However, in computing the optimal degree of unrolling, the effect of overlapping execution needs to be considered. <p> Given II, MinDist gives the minimum lifetime of each variable. This information can be very helpful in estimating the register pressure. Without considering the overlapping effect of software pipelining, the register pressure of the unrolled loop can be precisely estimated using the algorithm given by Carr and Kennedy <ref> [9] </ref>. So if the additional register demand caused by the overlapping effect is known, the register pressure after software pipelining can be precisely estimated. MinDist can be used to compute the effect of overlapping on register pressure for modulo scheduled software pipeline schedules.
Reference: [10] <author> S. Carr and K. Kennedy. </author> <title> Scalar replacement in the presence of conditional control flow. </title> <journal> Software Practice and Experience, </journal> <volume> 24(1) </volume> <pages> 51-77, </pages> <month> January </month> <year> 1994. </year>
Reference-contexts: Scalar replacement, or load-store-elimination is a technique that can be used to replace memory loads and stores with scalar operations [7] [5] [15] <ref> [10] </ref>. <p> This section describes how unroll-and-jam removes memory operations and how software pipelining resource constraints can be reduced by unroll-and-jam. 3.3.1 Removing Memory Operations Scalar replacement, or load-store-elimination, has been used to reduce the number of memory load/store operations by reusing values across innermost loop iterations [7] [5] [15] <ref> [10] </ref>. That is, instead of storing a value at this iteration and loading it at the next iteration, the value can be maintained in a register and the load and store operations of that value can be eliminated.
Reference: [11] <author> S. Carr, K.S. McKinley, and C.W. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <journal> In ACM SIGPLAN NOTICES, </journal> <volume> volume 29, </volume> <pages> pages 252-262, </pages> <month> Nov </month> <year> 1994. </year>
Reference-contexts: There are currently two popular compile-time analysis models that have been used to identify memory reuse; one is based upon sophisticated dependence analysis <ref> [11] </ref> and the other uses a linear algebra model [46]. The reader should refer to those resources for algorithm details. 4.3 Removing Hardware Misuse with Memory Reuse Analysis As discussed previously, the problem of uncertainty latencies is rooted in the deep memory hierarchy of modern microprocessors.
Reference: [12] <author> S. Carr and Q. Wu. </author> <title> An analysis of unroll-and-jam on the HP 715/50. </title> <type> Technical Report CS-95-03, </type> <institution> Department of Computer Science, Michigan Technological University, </institution> <address> Hougton, MI, </address> <year> 1995. </year>
Reference-contexts: If the estimation is higher than the actual register pressure, unroll-and-jam will restrict the degree of unrolling heuristically and unroll-and-jam's benefit will not be fully exploited. If the estimation is too low, unroll-and-jam will use too many registers and potentially cripple software pipelining due to register spilling <ref> [12] </ref>. Two register-pressure estimation methods for modulo scheduled loops, MaxLive and MinAvg, were described in Section 2.3.4. This section discusses the possible use of these two estimation methods with unroll-and-jam and proposes a new approach that may be more effective than simply using MaxLive or MinAvg.
Reference: [13] <author> A.E. Charlesworth. </author> <title> An approach to scientific array processing: The architectural design of the AP 120B/FPS 164 family. </title> <booktitle> Computer, </booktitle> <pages> pages 18-27, </pages> <month> Sept. </month> <year> 1981. </year>
Reference-contexts: Traditional compiler renaming is insufficient as well in that when a value is defined in the loop body, it does not assign a different variable to hold the value produced in each iteration [14]. Although special hardware support, namely a rotating register file, can eliminate all false recurrences <ref> [13] </ref> [36] [35], most of today's machines do not have the benefit of such rotating register files. A special software technique, modulo variable expansion (MVE)[25], was developed by Lam in order to eliminate false recurrences. But MVE has two shortcomings. <p> This type of renaming was used in vectorizing compilers [24], where the defined variable is expanded into a higher degree of array. However, this type of renaming is too expensive to be practical in conventional machines. One method of special hardware support, namely rotating registers (RR), has been suggested <ref> [13] </ref> [36]. Rau et al. used RRs for modulo scheduled loops [34]. In a machine with support for RRs, the index of registers are shifted every II cycles.
Reference: [14] <author> R. Cytron, J. Ferrante, B.K. Rosen, M.N. Wegman, and F.K. Zadeck. </author> <title> An efficient method of computing static single assignment form. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <year> 1989. </year>
Reference-contexts: Traditional compiler renaming is insufficient as well in that when a value is defined in the loop body, it does not assign a different variable to hold the value produced in each iteration <ref> [14] </ref>. Although special hardware support, namely a rotating register file, can eliminate all false recurrences [13] [36] [35], most of today's machines do not have the benefit of such rotating register files. A special software technique, modulo variable expansion (MVE)[25], was developed by Lam in order to eliminate false recurrences.
Reference: [15] <author> Evelyn Duesterwald, Rajiv Gupta, and Mary Lou Soffa. </author> <title> A practical data flow framework for array reference analysis and its use in optimizations. </title> <journal> SIGPLAN Notices, </journal> <volume> 28(6) </volume> <pages> 68-77, </pages> <month> June </month> <year> 1993. </year> <booktitle> Proceedings of the ACM SIGPLAN '93 Conference on Programming Language Design and Implementation. </booktitle>
Reference-contexts: Scalar replacement, or load-store-elimination is a technique that can be used to replace memory loads and stores with scalar operations [7] [5] <ref> [15] </ref> [10]. <p> This section describes how unroll-and-jam removes memory operations and how software pipelining resource constraints can be reduced by unroll-and-jam. 3.3.1 Removing Memory Operations Scalar replacement, or load-store-elimination, has been used to reduce the number of memory load/store operations by reusing values across innermost loop iterations [7] [5] <ref> [15] </ref> [10]. That is, instead of storing a value at this iteration and loading it at the next iteration, the value can be maintained in a register and the load and store operations of that value can be eliminated.
Reference: [16] <author> A. E. Eichenberger and E. S. Davidson. </author> <title> Stage scheduling: A technique to reduce the register requirement of a modulo schedule. </title> <booktitle> In Proceedings of the 28th International Symposium on Microarchitecture (MICRO-28), </booktitle> <pages> pages 338-349, </pages> <address> Ann Arbor, MI, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: The third goal of this research is to measure the benefit of using memory reuse analysis in software pipelining. In particular, since assuming all memory operations as cache misses is used by other researchers in studying register assignment for software pipelining [34] [22] [17] <ref> [16] </ref>, we want to measure the decrease of register overuse attributed to assuming all memory operations are cache misses. The result of this measurement will show the significance of using memory reuse analysis with software pipelining. <p> It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] [22] [35] [44] [34] <ref> [16] </ref>. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] [22] [35] [44] [34] <ref> [16] </ref>. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. Therefore, we have chosen iterative modulo scheduling as the basis for our software pipelining. Chapter 6 presents evaluation results obtained by using iterative modulo scheduling. <p> In his experiment, hardware support of rotating registers is assumed [22]. For conventional hardware, the register need is probably higher due to the effect of kernel unrolling. Stage Scheduling to Minimize Register Requirement Recently, a set of heuristics were developed by Eichenberger and Davidson <ref> [16] </ref> to find a near optimal register assignment. Their method, called stage scheduling, is a post-pass scheduler invoked after a software pipeline schedule has been found. They define a stage to be II cycles. <p> Analyses showed that assuming all-cache-hit can cause severe speed penalties because when a cache miss occurs, the software pipeline may stall until the cache miss is finished. To avoid this speed penalty, many researchers adopt an all-cache-miss assumption in software pipelining [34] [22] [17] <ref> [16] </ref>. However, assuming all-cache-miss leads to much higher register requirement than necessary. This research compares assuming all-cache-miss with using memory reuse analysis. For over a hundred benchmark loops tested, using memory reuse analysis decreased II from 10% to 43% and register pressure from 10% to 22%.
Reference: [17] <author> A. E. Eichenberger, E. S. Davidson, and S. G. Abraham. </author> <title> Minimum register requirements for a modulo schedule. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture (MICRO-27), </booktitle> <pages> pages 63-74, </pages> <address> San Jose, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: The third goal of this research is to measure the benefit of using memory reuse analysis in software pipelining. In particular, since assuming all memory operations as cache misses is used by other researchers in studying register assignment for software pipelining [34] [22] <ref> [17] </ref> [16], we want to measure the decrease of register overuse attributed to assuming all memory operations are cache misses. The result of this measurement will show the significance of using memory reuse analysis with software pipelining. <p> However, it is difficult for scheduling methods to estimate register usage or to predict the result of register assignment because until after scheduling, complete information about register lifetimes is unavailable. This section introduces three heuristic-based register-assignment approaches for software pipelining. For optimal methods, readers are referred to <ref> [17] </ref> [21]. Register Assignment after Register-Need-Insensitive Scheduling Rau et al. investigated the register assignment problem of software pipelining [34]. They studied various post-schedule register assignment methods. But they did not try to reduce the register need in the scheduling step. <p> Analyses showed that assuming all-cache-hit can cause severe speed penalties because when a cache miss occurs, the software pipeline may stall until the cache miss is finished. To avoid this speed penalty, many researchers adopt an all-cache-miss assumption in software pipelining [34] [22] <ref> [17] </ref> [16]. However, assuming all-cache-miss leads to much higher register requirement than necessary. This research compares assuming all-cache-miss with using memory reuse analysis. For over a hundred benchmark loops tested, using memory reuse analysis decreased II from 10% to 43% and register pressure from 10% to 22%.
Reference: [18] <author> M. Flynn. </author> <title> Very high-speed computing systems. </title> <booktitle> Proc. IEEE, </booktitle> <pages> pages 1901-1909, </pages> <year> 1966. </year>
Reference-contexts: Medium-grain parallelism describes searching for parallelism at the loop level. Fine-grain parallelism refers to attempts to overlap low-level operations such as adds, multiplies, loads and stores. The MIMD (multiple instruction multiple data) model of computation attempts to find coarse-grain parallelism <ref> [18] </ref>. In an MIMD machine, each processor runs a separate process and carries out a sub-task. Each processor has its own memory, registers and program counter. Processors communicate with each other by message passing. In contrast to MIMD parallelism, ILP (instruction-level parallelism) exploits the smallest granularity in parallel execution.
Reference: [19] <author> G.R. Gao, W-B. Wong, and Q. Ning. </author> <title> A Timed Petri-Net Model for Fine-Grain Loop Scheduling. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 204-218, </pages> <month> June 26-28 </month> <year> 1991. </year> <month> 94 </month>
Reference: [20] <author> Gina Goff, Ken Kennedy, and Chau-Wen Tseng. </author> <title> Practical dependence testing. </title> <booktitle> In PLDI, </booktitle> <pages> pages 15-29, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: To calculate RecII, dependence analysis is needed to provide the information of both loop-independent dependences and loop-carried dependences. Loop dependences can also be divided into scalar dependences and subscript-variable dependences. The computing of dependences can be done by well-known dependence analysis techniques [23] <ref> [20] </ref>. However, for software pipelining, not all scalar loop-carried dependences are needed. Pruning Loop-Carried Scalar Dependences This section shows that, for the purpose of computing RecII and thus, for performing modulo scheduling, many of the loop-carried scalar dependences can be ignored.
Reference: [21] <author> R. Govindarajan, E. R. Altman, and G. R. Gao. </author> <title> Minimizing Register Requirements under Resource-Constrained Rate-Optimal Software Pipelining. </title> <booktitle> In Proceedings of the 27th Microprogramming Workshop (MICRO-27), </booktitle> <pages> pages 85-94, </pages> <address> San Jose, CA, </address> <month> November </month> <year> 1994. </year>
Reference-contexts: A leading heuristic-based method, iterative modulo scheduling, will be described in detail in Section 2.4 since it is the method used in this research. Integer Linear Programming In parallel with heuristic-based research on modulo scheduling, another work, Integer Linear Programming (LP) <ref> [21] </ref> [4], was developed under a strict mathematical formulation, namely integer programming. Though the method has a possible exponential compilation time, it can 16 find a software pipeline schedule that uses a given number of resources at the smallest initiation interval while minimizing the number of registers it uses. <p> However, it is difficult for scheduling methods to estimate register usage or to predict the result of register assignment because until after scheduling, complete information about register lifetimes is unavailable. This section introduces three heuristic-based register-assignment approaches for software pipelining. For optimal methods, readers are referred to [17] <ref> [21] </ref>. Register Assignment after Register-Need-Insensitive Scheduling Rau et al. investigated the register assignment problem of software pipelining [34]. They studied various post-schedule register assignment methods. But they did not try to reduce the register need in the scheduling step.
Reference: [22] <author> R. A. Huff. </author> <title> Lifetime-Sensitive Modulo Scheduling. </title> <booktitle> In Conference Record of SIGPLAN Programming Language and Design Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: The third goal of this research is to measure the benefit of using memory reuse analysis in software pipelining. In particular, since assuming all memory operations as cache misses is used by other researchers in studying register assignment for software pipelining [34] <ref> [22] </ref> [17] [16], we want to measure the decrease of register overuse attributed to assuming all memory operations are cache misses. The result of this measurement will show the significance of using memory reuse analysis with software pipelining. <p> Various methods have been proposed to circumvent the above difficulties and find a good estimation for the register constraint. Two estimation methods, MaxLive, which is independent of register assignment methods [35], and, MinAvg, which is independent of scheduling <ref> [22] </ref>, are introduced in this section. 26 MaxLive is a lower bound on the register demand of a given schedule. At any point of the given schedule, there are certain number of values that are live at that point. <p> So when MaxLive for some schedules exceeds the number of available registers, we cannot conclude that no schedule is possible under the register constraint. To avoid the two disadvantages of MaxLive, Huff proposed a schedule-independent lower bound on register requirement for a given II, MinAvg <ref> [22] </ref>. MinAvg is computed based on the lower bound on the length of all lifetimes in the loop. <p> M inAvg = S 8value v 2loop M inLT (v) I I where M inLT (v) is the minimum lifetime of variable v. The minimum schedule-independent lifetime for each value is computed using MinDist (see Section 5.2) <ref> [22] </ref>. <p> As do other modulo scheduling 28 approaches, iterative modulo scheduling combines the consideration of recurrence and resource constraints in scheduling. It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] <ref> [22] </ref> [35] [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] <ref> [22] </ref> [35] [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. Therefore, we have chosen iterative modulo scheduling as the basis for our software pipelining. Chapter 6 presents evaluation results obtained by using iterative modulo scheduling. <p> It is possible that another schedule could have a lower MaxLive. Because the scheduler used in Rau et al.'s research was not sensitive to register demand, it did not try to find that schedule with the lowest MaxLive. Register Assignment after Lifetime-Sensitive Scheduling Huff <ref> [22] </ref> proposed a schedule-independent lower bound for register requirement. He also modified iterative modulo scheduling so that the scheduling step attempts to minimize the lifetime of values. The schedule-independent lower bound, MinAvg, has been described in Section 2.3.4. <p> Huff's experimental results showed that 46% of loops achieved a MaxLive equal to MinAvg and 93% of loops had a MaxLive within 10 registers of MinAvg. In his experiment, hardware support of rotating registers is assumed <ref> [22] </ref>. For conventional hardware, the register need is probably higher due to the effect of kernel unrolling. Stage Scheduling to Minimize Register Requirement Recently, a set of heuristics were developed by Eichenberger and Davidson [16] to find a near optimal register assignment. <p> The execution of this software pipeline will be 21 cycles per iteration, instead of 3 cycles per iteration, a slowdown of 700%. 4.3.2 All-Cache-Miss Assumption To avoid the possible severe speed penalty, Rau [33] and Huff <ref> [22] </ref> chose to assume the worst latency for every memory load, that is, all-cache-miss. <p> The current popular method proposed by Huff takes time as high as O (log 2 N fl N 3 ) in checking if a trial RecII is valid <ref> [22] </ref>. However, Huff's method is not sensitive to the complexity of the DDG; i.e. it uses same amount time in checking a trial RecII for DDGs of different complexity. <p> From the Equation 2.1, RecII can be computed if all loop-carried dependence cycles are known. However, finding all elementary cycles in a graph is generally difficult and computationally expensive because a graph can have an exponential number of cycles. Huff <ref> [22] </ref> formulated the problem as a minimal cost-time ratio problem [27]. Each dependence edge can be viewed as having a cost and a time. The cost is the negative of its timing constraint; and the time is its iteration difference. <p> The cost is the negative of its timing constraint; and the time is its iteration difference. The longest recurrence cycle, which determines RecII, is the one having the smallest cost per time value. Huff computed RecII by finding the smallest valid RecII <ref> [22] </ref> [33]. A valid RecII means that the RecII is higher than or equal to the length of any recurrence cycle in the DDG. A binary search can be used to find the smallest valid RecII to minimize the number of trials of RecII. <p> The lifetime of variables is not directly dependent on II, therefore it is more accurate assessment of the effect on register of the different latency assumptions. The lower-bound on the sum of the lifetime of all variables, MinLT, can be computed accurately using Huff's method <ref> [22] </ref>. MinLT is also independent of any particular software pipeline schedule. The result in the third column shows a more dramatic decrease, ranging from 33% to 54%, on MinLT when using memory reuse analysis over assuming all-cache-miss. <p> Analyses showed that assuming all-cache-hit can cause severe speed penalties because when a cache miss occurs, the software pipeline may stall until the cache miss is finished. To avoid this speed penalty, many researchers adopt an all-cache-miss assumption in software pipelining [34] <ref> [22] </ref> [17] [16]. However, assuming all-cache-miss leads to much higher register requirement than necessary. This research compares assuming all-cache-miss with using memory reuse analysis. For over a hundred benchmark loops tested, using memory reuse analysis decreased II from 10% to 43% and register pressure from 10% to 22%.
Reference: [23] <author> D. Kuck. </author> <title> The Structure of Computers and Computations Volume 1. </title> <publisher> John Wiley and Sons, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: To calculate RecII, dependence analysis is needed to provide the information of both loop-independent dependences and loop-carried dependences. Loop dependences can also be divided into scalar dependences and subscript-variable dependences. The computing of dependences can be done by well-known dependence analysis techniques <ref> [23] </ref> [20]. However, for software pipelining, not all scalar loop-carried dependences are needed. Pruning Loop-Carried Scalar Dependences This section shows that, for the purpose of computing RecII and thus, for performing modulo scheduling, many of the loop-carried scalar dependences can be ignored. <p> After using a different variable in each embedded loop body, the cross-loop dependences are removed, and so is the cross-loop dependence cycle. Shared-variable renaming is a special case of a well known technique, called tree height reduction <ref> [23] </ref>. Tree height reduction divides the computation of the shared variable in the loop and completes the computation in a minimum number of steps. <p> height reduction and loop interchange. 3.5.1 Comparison with Tree-Height Reduction Tree-height reduction (THR) is a general technique that considers the computation carried by the loop as a computing tree and restructures the computing tree so that more computations can be parallelized and the height of the tree can be reduced <ref> [23] </ref>. Tree-height reduction can be applied to not only nested loops but also single loops. It generally involves innermost loop unrolling and back-substitution. Tree-height reduction has recently been used and evaluated on ILP architectures [38] and in modulo scheduling [26].
Reference: [24] <author> D. J. Kuck, R. H. Kuhn, D. A. Padua, B. Leasure, and M. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <month> January </month> <year> 1981. </year>
Reference-contexts: Therefore, false recurrences may happen due to the reuse of the location of a by multiple iterations. If a value is defined in a loop, a complete renaming would require a separate variable for each iteration of the loop. This type of renaming was used in vectorizing compilers <ref> [24] </ref>, where the defined variable is expanded into a higher degree of array. However, this type of renaming is too expensive to be practical in conventional machines. One method of special hardware support, namely rotating registers (RR), has been suggested [13] [36].
Reference: [25] <author> M.S. Lam. </author> <title> Software Pipelining: An Effective Scheduling Technique for VLIW Machines. </title> <booktitle> In Proceedings of the SIGPLAN '88 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 318-328, </pages> <address> Atlanta, GA, </address> <month> June </month> <year> 1988. </year>
Reference-contexts: A special software technique, modulo variable expansion (MVE)<ref> [25] </ref>, was developed by Lam in order to eliminate false recurrences. But MVE has two shortcomings. First, it only handles a special class of false recurrences and cannot deal with the general case of false recurrences. For 9 example, modulo variable expansion [25] can not handle the case when the reused variable is not defined at the beginning of the loop. Second, MVE may unnecessarily increase the register pressure since not all false recurrences need renaming. MVE blindly eliminates the effect of all applicable false recurrences. <p> However, if a false recurrence is not more restrictive than other constraints, it does not need to be renamed. Excessive renaming causes unnecessary register pressure since renaming consumes additional registers. This research uses a leading software pipelining algorithm, iterative modulo scheduling <ref> [25] </ref> [33]. The final piece of this work is to develop a modified iterative modulo scheduling algorithm to (1) eliminate the effect of all false recurrence on software pipelining and (2) minimize the additional registers used by the renaming in modulo scheduling. Chapter 5 presents this new algorithm. <p> Unrolling cannot solve this problem when the exact achievable lower bound of II is unknown. However, scheduling-while-unrolling can naturally form a software pipeline with a non-integer II. The handling of branches is undoubtedly the greatest weakness of modulo scheduling. Though Hierarchical Reduction <ref> [25] </ref> and Enhanced Modulo Scheduling [44] can deal with branches without special hardware support, the amount of code expansion involved can be exponential. <p> As do other modulo scheduling 28 approaches, iterative modulo scheduling combines the consideration of recurrence and resource constraints in scheduling. It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied <ref> [25] </ref> [33] [22] [35] [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> To solve this problem of overlapping variable lifetimes, some have proposed a hardware solution, namely rotation registers (RRs) in which a machine can use multiple physical registers for one value. For conventional machines that do not have RRs, a compiler technique called modulo variable expansion <ref> [25] </ref>, or kernel unrolling, can be used to solve this problem. After the schedule of one iteration is found, a certain degree of kernel-unrolling is performed. The least amount of unrolling is determined by dividing the longest variable life time by II. <p> loop count 2 needs to be added either before the prelude or after the postlude of the software pipeline. 32 When there are multiple lifetimes that are greater than II and each lifetime requires a different degree of unrolling, the degree of kernel-unrolling can be determined by two different methods <ref> [25] </ref>. One needs a larger number of unrolling (code expansion) but requires fewer registers; the other requires more registers but less unrolling. After kernel unrolling, the code generation step generates prelude, kernel and postlude. Pre-conditioning or post-conditioning is needed for DO-loops if the kernel is unrolled. <p> Although RRs can eliminate all false recurrences caused by the reuse of variables, it is an expensive hardware feature that is not available on any of today's machines. Lam <ref> [25] </ref> proposed a software solution to this problem called Modulo Variable Expansion (MVE). The advantage of MVE is that it requires no hardware support not found on conventional architectures. <p> the renaming of the reused variable should make the define and the use in each iteration use two different variables so that the loop-independent anti-dependence can be removed. 5.1.2 Modulo Variable Expansion To eliminate the effect of certain false recurrences, Lam proposed a compiler renaming technique called Modulo Variable Expansion <ref> [25] </ref>. This section gives a detailed description of MVE and shows why MVE is neither complete nor efficient for eliminating all false recurrences. <p> scheduling implementation follows Rau's method [33] except that a modified method is used in computing RecII to make the computing faster. (See Section 5.2) Our implementation of modulo variable expansion uses the method of computing the unroll amount that restricts the degree of unrolling but does not minimize register usage <ref> [25] </ref>. The reason is to limit the size of the kernel generated. <p> These uncertain memory latencies can cause significant hardware misuse in software pipelining. Finally, the software pipelining algorithm may not be able to exploit all available parallelism in the innermost loop. In particular, the popular iterative modulo scheduling technique <ref> [25] </ref> [33] does not eliminate all the false recurrences which, in turn, hurts performance for most ILP machines. These four difficulties cause over-restrictive software pipelining constraints which unnecessarily limit the performance of software pipelining. <p> The final contribution of this research is a new method to efficiently eliminate the effect of all false recurrences in modulo scheduling. These new techniques, define-substitution and restrictive renaming, represent an improvement over the current method of modulo scheduling. Modulo variable expansion <ref> [25] </ref> is shown to be incomplete and inefficient for this purpose. Define-substitution can be used to eliminate all kinds of false recurrences; restrictive renaming eliminates a minimum number of false recurrences so that the register cost can be kept in minimum.
Reference: [26] <author> D. M. Lavery and W. W. Hwu. </author> <title> Unrolling-Based Optimizations for Modulo Scheduling. </title> <booktitle> In Proceedings of the 28th International Symposium on Microarchitecture (MICRO-28), </booktitle> <pages> pages 327-337, </pages> <address> Ann Arbor, MI, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Tree-height reduction can be applied to not only nested loops but also single loops. It generally involves innermost loop unrolling and back-substitution. Tree-height reduction has recently been used and evaluated on ILP architectures [38] and in modulo scheduling <ref> [26] </ref>. Both unroll-and-jam and THR increase the amount of parallelism in the innermost loop. However, the sources of the increased parallelism are totally different.
Reference: [27] <author> E. L. Lawler, H. Rinehart, and Winston Pub. </author> <title> Combinatorial Optimization: Networks and Matroids. </title> <year> 1976. </year>
Reference-contexts: However, finding all elementary cycles in a graph is generally difficult and computationally expensive because a graph can have an exponential number of cycles. Huff [22] formulated the problem as a minimal cost-time ratio problem <ref> [27] </ref>. Each dependence edge can be viewed as having a cost and a time. The cost is the negative of its timing constraint; and the time is its iteration difference. The longest recurrence cycle, which determines RecII, is the one having the smallest cost per time value.
Reference: [28] <author> Scott A. Mahlke, William Y. Chen, Wen-Mei W. Hwu, B. Ramak rishna Rau, and Michael S. Schlansker. </author> <title> Sentinel scheduling for VLIW and superscalar processors. </title> <booktitle> In ASPLOS5, </booktitle> <volume> volume 27, </volume> <pages> pages 238-247, </pages> <address> Boston, MA, </address> <month> Oct </month> <year> 1992. </year>
Reference: [29] <author> A. Nicolau and J.A. Fisher. </author> <title> Measuring the parallelism available for very long instruction word architectures. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 33(11) </volume> <pages> 968-976, </pages> <month> Nov </month> <year> 1984. </year>
Reference-contexts: These two factors make MIMD machines less cost-effective for general computing problems than alternative architectural paradigms such as ILP. 2 Instruction-level parallelism, although much more limited than MIMD parallelism, is more suited for general computing problems. Studies show that all programs exhibit a certain degree of instruction-level parallelism [43] <ref> [29] </ref>. More important, ILP is transparent to the programmer; existing sequential programs can be executed on ILP machines without programmer modifications. Rau and Fisher define ILP as a combined hardware and software approach to exploiting instruction-level parallelism in sequential programs [37]. <p> In two similar experiments, Nicolau and Fisher found parallelism of 90 in average with global scheduling whereas Tjaden reported average parallelism of 1.8 within basic blocks [43] <ref> [29] </ref>. Although global scheduling can exploit parallelism among basic blocks, it does not fully utilize parallelism in loops. Because loops consist of many iterations, we want to execute in parallel not only multiple operations of a single iteration, but also operations from multiple iterations.
Reference: [30] <author> David Poplawski. </author> <title> The unlimited resource machine (URM). </title> <type> Technical Report CS-TR 95-1, </type> <institution> Department of Computer Science, Michigan Technological University, </institution> <address> Hougton, MI, </address> <month> February </month> <year> 1995. </year> <month> 95 </month>
Reference-contexts: The front-end compiles both Fortran and C loops and the back-end 1 It is not necessary to know the number of iterations at compile time 75 generates code for a collection of architectures including a URM (Unlimited Resource Machine) machine <ref> [30] </ref>. This research described here evaluates the performance of Fortran programs on the URM machine. 6.1.2 Instrumentation The experiment uses the implementation of unroll-and-jam and memory reuse analysis in Memoria, the implementation of array analysis in Parascope [6], the software pipelining in Rocket, and the URM simulator [30]. <p> Resource Machine) machine <ref> [30] </ref>. This research described here evaluates the performance of Fortran programs on the URM machine. 6.1.2 Instrumentation The experiment uses the implementation of unroll-and-jam and memory reuse analysis in Memoria, the implementation of array analysis in Parascope [6], the software pipelining in Rocket, and the URM simulator [30]. The general experimental method is graphically described in and generates the transformed Fortran program with comments indicating the reuse information. Parascope compiles the Fortran program into Iloc intermediate code [6]. Parascope performs various global code optimizations, as well as array analysis.
Reference: [31] <author> M. Rajagopalan. </author> <title> A New Model for Software Pipelining Using Petri Nets. </title> <type> Master's thesis, </type> <institution> Department of Computer Science, Utah State University, Logan, </institution> <address> UT, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: In this model, each operation is a node and the issuing of a node happens when a token travels to this node. Dependences are modeled as rules that restrict the transition of tokens from one node to another. In the model of Rajagopalan and Allan <ref> [31] </ref>, the pace of different recurrences are synchronized to be the same in a Petri Net, so a pattern can always be formed. By checking the state information of the Petri Net, a repeating pattern can be found relatively easily.
Reference: [32] <author> M. Rajagopalan and V. H. Allan. </author> <title> Specification of Software Pipelining using Petri Nets. </title> <journal> International Journal on Parallel Processing, </journal> <volume> 3(22) </volume> <pages> 279-307, </pages> <year> 1994. </year>
Reference-contexts: Rajagopalan and Allan's model can also handle resource constraints and predicates in a smooth way. In comparing with other methods, Rajagopalan and Allan claimed that their model achieved the best performance in unrolling-while-scheduling methods and performance similar to the best modulo scheduling method available <ref> [32] </ref> [3]. 2.2.2 Modulo Scheduling In contrast to scheduling-while-unrolling methods which unroll a loop and schedule multiple iterations, another class of scheduling methods, modulo scheduling, generates a pipelined schedule for a single iteration of a loop. Modulo scheduling imposes two restrictions on software pipelines. 15 * 1.
Reference: [33] <author> B. R. Rau. </author> <title> Iterative modulo scheduling: An algorithm for software pipelining loops. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture (MICRO-27), </booktitle> <pages> pages 63-74, </pages> <address> San Jose, CA, </address> <month> December </month> <year> 1994. </year>
Reference-contexts: However, if a false recurrence is not more restrictive than other constraints, it does not need to be renamed. Excessive renaming causes unnecessary register pressure since renaming consumes additional registers. This research uses a leading software pipelining algorithm, iterative modulo scheduling [25] <ref> [33] </ref>. The final piece of this work is to develop a modified iterative modulo scheduling algorithm to (1) eliminate the effect of all false recurrence on software pipelining and (2) minimize the additional registers used by the renaming in modulo scheduling. Chapter 5 presents this new algorithm. <p> The following two reasons are a possible explanation for this experimental result. First, for loops that are not recurrence bounded and have no complicated recurrences, the difference in scheduling freedom is not very significant. More than half of the loops tested by Rau <ref> [33] </ref> are resource-bounded. The second possible reason is that current unrolling-while-scheduling methods cannot deal with resource constraints as effectively as modulo scheduling can. Treating resource constraints separately may result in sub-optimal performance. A significant restriction of modulo scheduling is that II must be an integer. <p> As do other modulo scheduling 28 approaches, iterative modulo scheduling combines the consideration of recurrence and resource constraints in scheduling. It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] <ref> [33] </ref> [22] [35] [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> Its scheduling, code generation and register assignment algorithms have been well-studied [25] <ref> [33] </ref> [22] [35] [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. Therefore, we have chosen iterative modulo scheduling as the basis for our software pipelining. Chapter 6 presents evaluation results obtained by using iterative modulo scheduling. <p> The execution of this software pipeline will be 21 cycles per iteration, instead of 3 cycles per iteration, a slowdown of 700%. 4.3.2 All-Cache-Miss Assumption To avoid the possible severe speed penalty, Rau <ref> [33] </ref> and Huff [22] chose to assume the worst latency for every memory load, that is, all-cache-miss. <p> However, it should yield improved software pipelining when compared to either an all-cache-hit or an all-cache-miss policy. 59 Chapter 5 Improving Modulo Scheduling Modulo scheduling is a effective scheduling technique for software pipelining. Experimental results have shown that it can achieve near-optimal II for most benchmark loops <ref> [33] </ref> [4]. This chapter discusses two enhancements to modulo scheduling. Previously, modulo scheduling could not eliminate all false recurrences without the hardware support of rotating registers. Section 1 proposes a compiler algorithm that can efficiently eliminate the effect of all false recurrences for conventional architectures. <p> The cost is the negative of its timing constraint; and the time is its iteration difference. The longest recurrence cycle, which determines RecII, is the one having the smallest cost per time value. Huff computed RecII by finding the smallest valid RecII [22] <ref> [33] </ref>. A valid RecII means that the RecII is higher than or equal to the length of any recurrence cycle in the DDG. A binary search can be used to find the smallest valid RecII to minimize the number of trials of RecII. <p> The focus of that experiment is measuring the decrease in register pressure available when using memory reuse analysis compared to assuming all-cache-miss. 6.1 General Experimental Setup The iterative modulo scheduling algorithm presented by Rau <ref> [33] </ref> is implemented in Rocket. Rocket is a retargetable optimizing compiler [40]. It performs traditional optimizations and program analyses. Rocket currently has two front ends, one for C and one for Fortran programs. The C front-end generates Rocket intermediate code directly. <p> First, loop-carried dependences are added. Rocket computes the loop-carried scalar dependences by itself and uses the dependence analysis result of Parascope for array dependences. Next, redundant scalar loop-carried dependences are pruned as described in Section 2.3.1. Iterative Modulo Scheduling Our iterative modulo scheduling implementation follows Rau's method <ref> [33] </ref> except that a modified method is used in computing RecII to make the computing faster. (See Section 5.2) Our implementation of modulo variable expansion uses the method of computing the unroll amount that restricts the degree of unrolling but does not minimize register usage [25]. <p> These uncertain memory latencies can cause significant hardware misuse in software pipelining. Finally, the software pipelining algorithm may not be able to exploit all available parallelism in the innermost loop. In particular, the popular iterative modulo scheduling technique [25] <ref> [33] </ref> does not eliminate all the false recurrences which, in turn, hurts performance for most ILP machines. These four difficulties cause over-restrictive software pipelining constraints which unnecessarily limit the performance of software pipelining.
Reference: [34] <author> B. R. Rau, M. Lee, P.P. Tirumalai, and M. S. Schlansker. </author> <title> Register Allocation for Software Pipelined Loops. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Programming Language Design and Implementation", </booktitle> <pages> pages 283-229, </pages> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: The third goal of this research is to measure the benefit of using memory reuse analysis in software pipelining. In particular, since assuming all memory operations as cache misses is used by other researchers in studying register assignment for software pipelining <ref> [34] </ref> [22] [17] [16], we want to measure the decrease of register overuse attributed to assuming all memory operations are cache misses. The result of this measurement will show the significance of using memory reuse analysis with software pipelining. <p> It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] [22] [35] [44] <ref> [34] </ref> [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> This section introduces three heuristic-based register-assignment approaches for software pipelining. For optimal methods, readers are referred to [17] [21]. Register Assignment after Register-Need-Insensitive Scheduling Rau et al. investigated the register assignment problem of software pipelining <ref> [34] </ref>. They studied various post-schedule register assignment methods. But they did not try to reduce the register need in the scheduling step. They tried different heuristics on both ordering lifetimes and assigning lifetimes to available registers. <p> Rau et al. found that for the loops they tested, the actual register demand of a schedule was rarely 5 registers more than its MaxLive for a register allocation method similar to graph-coloring register assignment <ref> [34] </ref>. Although MaxLive is a fairly precise estimation, it is not practical for unroll-and-jam to use it. Computing MaxLive can only be done after a software pipeline schedule is found, i.e. after generating an unrolled loop body and scheduling the generated loop body. <p> However, this type of renaming is too expensive to be practical in conventional machines. One method of special hardware support, namely rotating registers (RR), has been suggested [13] [36]. Rau et al. used RRs for modulo scheduled loops <ref> [34] </ref>. In a machine with support for RRs, the index of registers are shifted every II cycles. This shifting produces the effect that each II uses a different set of registers and the reuse of the same location can be avoided. <p> Analyses showed that assuming all-cache-hit can cause severe speed penalties because when a cache miss occurs, the software pipeline may stall until the cache miss is finished. To avoid this speed penalty, many researchers adopt an all-cache-miss assumption in software pipelining <ref> [34] </ref> [22] [17] [16]. However, assuming all-cache-miss leads to much higher register requirement than necessary. This research compares assuming all-cache-miss with using memory reuse analysis. For over a hundred benchmark loops tested, using memory reuse analysis decreased II from 10% to 43% and register pressure from 10% to 22%.
Reference: [35] <author> B. R. Rau, M. S. Schlansker, </author> <title> and P.P. Tirumalai. Code Generation Schema for Modulo Scheduled Loops. </title> <booktitle> In Proceedings of Micro-25, The 25th Annual International Symposium on Microarchitecture, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: Although special hardware support, namely a rotating register file, can eliminate all false recurrences [13] [36] <ref> [35] </ref>, most of today's machines do not have the benefit of such rotating register files. A special software technique, modulo variable expansion (MVE)[25], was developed by Lam in order to eliminate false recurrences. But MVE has two shortcomings. <p> If the restricted scheduling freedom does not prevent modulo scheduling from achieving software pipeline's performance limit, modulo scheduling is a very good choice for implementation. Also, a complete code generation algorithm of modulo scheduling methods has been published <ref> [35] </ref>. A leading heuristic-based method, iterative modulo scheduling, will be described in detail in Section 2.4 since it is the method used in this research. <p> Various methods have been proposed to circumvent the above difficulties and find a good estimation for the register constraint. Two estimation methods, MaxLive, which is independent of register assignment methods <ref> [35] </ref>, and, MinAvg, which is independent of scheduling [22], are introduced in this section. 26 MaxLive is a lower bound on the register demand of a given schedule. At any point of the given schedule, there are certain number of values that are live at that point. <p> However, MaxLive can serve as a fast estimation, since it is typically computed before doing any register assignment. Rau, et al., claimed that MaxLive is fairly accurate because for loops they tested, their best register allocation algorithm achieved an average of 1.3 registers more than MaxLive <ref> [35] </ref>. Due to the reason that computing of MaxLive must be based on a given schedule, there are two disadvantages, * The scheduler cannot compute MaxLive before scheduling. The scheduling effort may be wasted if it turns out that the schedule uses too many registers. <p> As do other modulo scheduling 28 approaches, iterative modulo scheduling combines the consideration of recurrence and resource constraints in scheduling. It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] [22] <ref> [35] </ref> [44] [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> After kernel unrolling, the code generation step generates prelude, kernel and postlude. Pre-conditioning or post-conditioning is needed for DO-loops if the kernel is unrolled. For WHILE-loops, special techniques have been developed by Rau et al <ref> [35] </ref>. If a loop has undergone if-conversion and the machine does not have hardware support for predicate execution, the branch constructs must be recovered in the code generation step. <p> For loops with multiple exits where the loop count is known prior to loop 91 entry, code generation schemes such as multiple-postlude can be used as described by Rau et al <ref> [35] </ref>. For loops with calls to small functions, we can inline the called function and then perform software pipelining. 92
Reference: [36] <author> B. R. Rau, D. W. L. Yen, W. Yen, and R. A. Towle. </author> <title> The Cydra 5 Departmental Supercomputer: Design Philosophies, Decisions, and Trade-offs. </title> <booktitle> The IEEE Computer, </booktitle> <pages> pages 12-25, </pages> <month> January </month> <year> 1989. </year>
Reference-contexts: Traditional compiler renaming is insufficient as well in that when a value is defined in the loop body, it does not assign a different variable to hold the value produced in each iteration [14]. Although special hardware support, namely a rotating register file, can eliminate all false recurrences [13] <ref> [36] </ref> [35], most of today's machines do not have the benefit of such rotating register files. A special software technique, modulo variable expansion (MVE)[25], was developed by Lam in order to eliminate false recurrences. But MVE has two shortcomings. <p> However, this type of renaming is too expensive to be practical in conventional machines. One method of special hardware support, namely rotating registers (RR), has been suggested [13] <ref> [36] </ref>. Rau et al. used RRs for modulo scheduled loops [34]. In a machine with support for RRs, the index of registers are shifted every II cycles.
Reference: [37] <author> B.R. Rau and J.A. Fisher. </author> <title> Instruction-level parallel processing: History, overview and per-specitve. </title> <journal> Journal of Supercomputing, </journal> 7(1/2):9-50, 1993. 
Reference-contexts: More important, ILP is transparent to the programmer; existing sequential programs can be executed on ILP machines without programmer modifications. Rau and Fisher define ILP as a combined hardware and software approach to exploiting instruction-level parallelism in sequential programs <ref> [37] </ref>. ILP hardware issues multiple operations every cycle and ILP compilers expose parallelism in a program so that the parallel hardware can be fully utilized.
Reference: [38] <author> M. Schlansker, V. Kathail, and S. Anik. </author> <title> Height reduction of control recurrences for ILP processors. </title> <booktitle> In Proceedings of the 27th International Symposium on Microarchitecture (MICRO-27), </booktitle> <pages> pages 40-51, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: Tree-height reduction can be applied to not only nested loops but also single loops. It generally involves innermost loop unrolling and back-substitution. Tree-height reduction has recently been used and evaluated on ILP architectures <ref> [38] </ref> and in modulo scheduling [26]. Both unroll-and-jam and THR increase the amount of parallelism in the innermost loop. However, the sources of the increased parallelism are totally different.
Reference: [39] <author> B. Su, S. Ding, J. Wang, and J. Xia. </author> <title> GURPR A Method for Global Software Pipelining. </title> <booktitle> In Proceedings of the 20th Microprogramming Workshop (MICRO-20), </booktitle> <pages> pages 97-105, </pages> <address> Colorado Springs, CO, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: Then a software pipeline schedule can be obtained by simply repeating this pattern <ref> [39] </ref>. For example, if a pattern contains 4 iterations and has a length of 6 cycles, the software pipeline will achieve a peak rate of finishing 4 iterations every 6 cycles, that is, a II of 3 2 .
Reference: [40] <author> Philip H. Sweany and Steven J. Beaty. </author> <title> Overview of the ROCKET retargetable C compiler. </title> <type> Technical Report CS-94-01, </type> <institution> Department of Computer Science, Michigan Technological University, </institution> <address> Hougton, MI, </address> <month> January </month> <year> 1994. </year>
Reference-contexts: The focus of that experiment is measuring the decrease in register pressure available when using memory reuse analysis compared to assuming all-cache-miss. 6.1 General Experimental Setup The iterative modulo scheduling algorithm presented by Rau [33] is implemented in Rocket. Rocket is a retargetable optimizing compiler <ref> [40] </ref>. It performs traditional optimizations and program analyses. Rocket currently has two front ends, one for C and one for Fortran programs. The C front-end generates Rocket intermediate code directly. The Fortran front-end takes the intermediate code, ILoc, generated by ParaScope [6], and converts it to Rocket intermediate code.
Reference: [41] <author> J. E. Thornton. </author> <title> Parallel operation in the control data 6600. </title> <booktitle> In Proc. AFIPS Fall Joint Computer Conference, </booktitle> <pages> pages 33-40, </pages> <year> 1964. </year> <month> 96 </month>
Reference-contexts: False recurrences can be eliminated by variable renaming, that is, using separate variables to avoid the reuse of the same variable. However, the overlapping execution of software pipelining makes it very difficult to perform adequate and efficient renaming. In software pipelining, traditional hardware renaming <ref> [41] </ref> is not sufficient for eliminating false recurrences because the definition of a new value may happen before the use of the old value in the overlapping execution. <p> Various software and hardware methods have been used to eliminate the effect of false dependences and recurrences. Hardware renaming is one method of eliminating anti-dependences that has been used for a long time <ref> [41] </ref>. In the realm of hardware renaming, an anti-dependence is known as a WAR (write-after-read) hazard. When the hardware detects a WAR hazard, it can copy the value to a temporary and let the second operation proceed without waiting for the first operation to finish.
Reference: [42] <author> P. Tirumalai, M. Lee, </author> <title> and M.S. Schlansker. Parallelization of Loops with exits on pipelined architectures. </title> <booktitle> In Proceedings of SuperComputing '90, </booktitle> <pages> pages 200-212, </pages> <month> November </month> <year> 1990. </year>
Reference: [43] <author> G.S. Tjaden and M.J. Flynn. </author> <title> Detection and parallel execution of parallel instructions. </title> <journal> IEEE Transaction on Computers, </journal> 19(10) 889-895, oct 1970. 
Reference-contexts: These two factors make MIMD machines less cost-effective for general computing problems than alternative architectural paradigms such as ILP. 2 Instruction-level parallelism, although much more limited than MIMD parallelism, is more suited for general computing problems. Studies show that all programs exhibit a certain degree of instruction-level parallelism <ref> [43] </ref> [29]. More important, ILP is transparent to the programmer; existing sequential programs can be executed on ILP machines without programmer modifications. Rau and Fisher define ILP as a combined hardware and software approach to exploiting instruction-level parallelism in sequential programs [37]. <p> In two similar experiments, Nicolau and Fisher found parallelism of 90 in average with global scheduling whereas Tjaden reported average parallelism of 1.8 within basic blocks <ref> [43] </ref> [29]. Although global scheduling can exploit parallelism among basic blocks, it does not fully utilize parallelism in loops. Because loops consist of many iterations, we want to execute in parallel not only multiple operations of a single iteration, but also operations from multiple iterations.
Reference: [44] <author> N.J. Warter, G.E. Haab, and J.W. Bockhaus. </author> <title> Enhanced Modulo Scheduling for Loops with Conditional Branches. </title> <booktitle> In Proceedings of the 25th Annual International Symposium on Mi-croarchitecture (MICRO-25), </booktitle> <pages> pages 170-179, </pages> <address> Portland, OR, </address> <month> December 1-4 </month> <year> 1992. </year>
Reference-contexts: Unrolling cannot solve this problem when the exact achievable lower bound of II is unknown. However, scheduling-while-unrolling can naturally form a software pipeline with a non-integer II. The handling of branches is undoubtedly the greatest weakness of modulo scheduling. Though Hierarchical Reduction [25] and Enhanced Modulo Scheduling <ref> [44] </ref> can deal with branches without special hardware support, the amount of code expansion involved can be exponential. <p> The following discussion of the recurrence constraint considers data dependences only for a straight-line loop body. For a description of how control constructs can be handled in software pipelining and how control dependences are converted to data dependences, the reader can refer to <ref> [44] </ref> [45]. Recurrence Initiation Interval The recurrence initiation interval (RecII) of a loop is defined as the lowest II that satisfies all recurrences of the loop. In order to describe the recurrences of a loop, we need to introduce an important program representation form, namely the data-dependence graph (DDG). <p> It is a heuristic-based method that is practical for use in a general purpose compiler. Its scheduling, code generation and register assignment algorithms have been well-studied [25] [33] [22] [35] <ref> [44] </ref> [34] [16]. Several experimental evaluations have shown that the iterative modulo scheduling method can achieve near-optimal performance for a large number of benchmark loops [33] [22] [16]. No other heuristic-based software pipelining algorithm has been shown to have a better performance than iterative modulo scheduling. <p> For conventional architectures that do not have PRs, a special code generation scheme, reverse if-conversion was developed for Enhanced Modulo Scheduling <ref> [44] </ref>. Reverse if-conversion generates all possible execution paths for a software pipeline after scheduling. To understand reverse if-conversion, assume a single branch construct is denoted as fAorAg, where A is the branch target basic block and A is the fall through basic block. <p> A conflict-free RRT will guarantee a conflict-free software pipeline. For an if-converted loop, the resource reservation table will consider the effect of branches by allowing operations of the same iteration but disjoint execution to share a resource <ref> [44] </ref>. 2.4.3 Kernel Unrolling and Code Generation After a schedule and an II have been found, some degree of kernel unrolling is needed because loop variants that have a lifetime longer than II will cause cross-loop value conflicts in the overlapped execution of the software pipeline. (The lifetime of a variable <p> For WHILE-loops, special techniques have been developed by Rau et al [35]. If a loop has undergone if-conversion and the machine does not have hardware support for predicate execution, the branch constructs must be recovered in the code generation step. Warter et al. describes this technique in <ref> [44] </ref>. 2.4.4 Register Allocation Current popular register assignment methods in ILP compilers are based on a graph coloring formulation in which each scalar is a graph node, and each edge indicates that the lifetimes of two values connected by the edge overlap with each other. <p> The current code generation scheme is designed with the assumption that the loop body is a single basic block. Reverse if-conversion needs to be implemented if there is no hardware support for predicate execution <ref> [44] </ref>.
Reference: [45] <author> N.J. Warter, Scott A. Mahlke, Wen-Mei W. Hwu, and B. R. Rau. </author> <title> Reverse If-Conversion. </title> <booktitle> In PLDI, </booktitle> <pages> pages 290-299, </pages> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: The following discussion of the recurrence constraint considers data dependences only for a straight-line loop body. For a description of how control constructs can be handled in software pipelining and how control dependences are converted to data dependences, the reader can refer to [44] <ref> [45] </ref>. Recurrence Initiation Interval The recurrence initiation interval (RecII) of a loop is defined as the lowest II that satisfies all recurrences of the loop. In order to describe the recurrences of a loop, we need to introduce an important program representation form, namely the data-dependence graph (DDG).
Reference: [46] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A data locality optimizing algorithm. </title> <booktitle> 26(6) </booktitle> <pages> 30-44, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: There are currently two popular compile-time analysis models that have been used to identify memory reuse; one is based upon sophisticated dependence analysis [11] and the other uses a linear algebra model <ref> [46] </ref>. The reader should refer to those resources for algorithm details. 4.3 Removing Hardware Misuse with Memory Reuse Analysis As discussed previously, the problem of uncertainty latencies is rooted in the deep memory hierarchy of modern microprocessors.
References-found: 46

