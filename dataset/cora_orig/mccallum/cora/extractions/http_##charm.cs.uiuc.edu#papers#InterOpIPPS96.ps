URL: http://charm.cs.uiuc.edu/papers/InterOpIPPS96.ps
Refering-URL: http://charm.cs.uiuc.edu/papers/InterOpIPPS96.html
Root-URL: http://www.cs.uiuc.edu
Email: fkale,milind,narain,sanjeev,jyelong@cs.uiuc.edu  
Title: Converse An Interoperable Framework for Parallel Programming  
Author: Laxmikant V. Kale, Milind Bhandarkar, Narain Jagathesan, Sanjeev Krishnan, Joshua Yelon 
Address: Urbana, IL 61801.  
Affiliation: Department of Computer Science, University of Illinois,  
Abstract: Many different parallel languages and paradigms have been developed, each with its own advantages. To benefit from all of them, it should be possible to link together modules written in different parallel languages in a single application. Since the paradigms sometimes differ in fundamental ways, this is difficult to accomplish. This paper describes a framework, Converse, that supports such multi-lingual interoperability. The framework is meant to be inclusive, and has been verified to support the SPMD programming style, message-driven programming, parallel object-oriented programming, and thread-based paradigms. The framework aims at extracting the essential aspects of the runtime support into a set of core components, so that language-specific code does not have to pay overhead for features that it does not need. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Agha. </author> <title> Actors: A Model of Concurrent Computation in Distributed Systems. </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented <ref> [1, 3, 4, 5, 11] </ref>, thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages.
Reference: [2] <author> J. Board, L. V. Kale, K. Schulten, R. Skeel, and T. Schlick. </author> <title> Modeling biomolecules: Larger scales, longer durations. </title> <journal> IEEE Computational Science and Engineering, </journal> <volume> 1(4), </volume> <year> 1994. </year>
Reference-contexts: For each part of the application, the most suitable language or paradigm can be used. * Pre-existing libraries written in different languages can be reused in a single application. For example, in a parallel molecular dynamics application <ref> [2] </ref> being developed using Charm++, we will be able to use an N-body module (based on the fast multiple algorithm), which happens to be written in PVM. * Development of parallel languages, coordination languages, or libraries based on new paradigms is simplified because commonly used runtime modules such as a scheduler,
Reference: [3] <author> F. Bodin, P. Beckman, D. Gannon, and S. Narayana, S. </author> <title> an d Yang. Distributed pC++: Basic Ideas for an Object Parallel Langua ge. </title> <journal> Scientific Programming, </journal> <volume> 2(3), </volume> <year> 1993. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel <ref> [9, 8, 3] </ref>, object-oriented [1, 3, 4, 5, 11], thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. <p> 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented <ref> [1, 3, 4, 5, 11] </ref>, thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages.
Reference: [4] <author> K.M. Chandy and C. Kesselman. </author> <title> Compositional C++: Compositional Parallel Programming. </title> <booktitle> In Proceedings of the Fourth Workshop on Parallel Computing and Compilers. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented <ref> [1, 3, 4, 5, 11] </ref>, thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages. <p> 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented [1, 3, 4, 5, 11], thread-based <ref> [7, 4, 10] </ref>, macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages.
Reference: [5] <author> A. Chien. </author> <title> Concurrent Aggregates. </title> <publisher> MIT Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented <ref> [1, 3, 4, 5, 11] </ref>, thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages.
Reference: [6] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke. </author> <title> Nexus: An interoperability toolkit for parallel and distributed computer systems. </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: Many thread packages and standards have been developed in the past few years <ref> [10, 6] </ref>. However, the gluing together of scheduling, concurrency control and other features with the mechanisms to suspend and resume threads is problematic for the requirements of interoperability. E.g. the particular scheduling strategy provided by the threads package may not be appropriate for the problem at hand.
Reference: [7] <author> M. Hainer, D. Cronk, and P. Mehrotra. </author> <title> On the design of Chant: A talking threads package. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <month> November </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented [1, 3, 4, 5, 11], thread-based <ref> [7, 4, 10] </ref>, macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages. <p> As an example, a small Chant-like <ref> [7] </ref> multi-threaded language that supports tagged messages was implemented with very little effort using Converse, because of the functionality provided by the above components.
Reference: [8] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification (Draft), </title> <address> 1.0 edition, </address> <month> January </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel <ref> [9, 8, 3] </ref>, object-oriented [1, 3, 4, 5, 11], thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. <p> For this, we need to support interoperability among multiple paradigms. Such interoperability is not currently possible, except for a spe fl This research was supported in part by the ARPA grant DACA 88-94-C-0019 and NSF grant ASC-93-18159. cific subset of language implementations designed for this purpose (e.g. HPF <ref> [8] </ref> and PVM [14]). This paper describes the design and rationale of Converse, an interoperable framework for combining multiple languages and their runtime libraries into a single parallel program.
Reference: [9] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine independent parallel programming in Fortran-D. </title> <publisher> Elsevier Science Publishers B.V., </publisher> <year> 1992. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel <ref> [9, 8, 3] </ref>, object-oriented [1, 3, 4, 5, 11], thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm.
Reference: [10] <institution> Draft Standard for Information TechnologyPortable Operat ing Systems Interface (Posix), </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented [1, 3, 4, 5, 11], thread-based <ref> [7, 4, 10] </ref>, macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages. <p> Many thread packages and standards have been developed in the past few years <ref> [10, 6] </ref>. However, the gluing together of scheduling, concurrency control and other features with the mechanisms to suspend and resume threads is problematic for the requirements of interoperability. E.g. the particular scheduling strategy provided by the threads package may not be appropriate for the problem at hand.
Reference: [11] <author> L.V. Kale and S. Krishnan. </author> <title> Charm++ : A portable concurrent object oriented system based on C++. </title> <booktitle> In Proceedings of the Conference on Object Oriented Programming Systems, Languages and Applications, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD [14, 12], data-parallel [9, 8, 3], object-oriented <ref> [1, 3, 4, 5, 11] </ref>, thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. It may be desirable to write different components of an application in different languages. <p> Prototype implementations of PVM messaging and SM (a simple messaging layer) are complete and simple multi-lingual programs are demonstrated to run on the above machines. The Charm and Charm++ <ref> [11] </ref> parallel object-oriented languages have been retargeted for Converse. The machine interface of Converse is meant to be implemented at the lowest level on individual machines. On some machines the lowest and most efficient layers of the system were available to us.
Reference: [12] <author> Message Passing Interface Forum. </author> <title> Document for a Standard Message-Passing Interface, </title> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD <ref> [14, 12] </ref>, data-parallel [9, 8, 3], object-oriented [1, 3, 4, 5, 11], thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm.
Reference: [13] <author> S. Pakin, M. Lauria, and A. Chien. </author> <title> High performance messaging on workstations: Illinois fast messages (fm) for myrinet. </title> <booktitle> In Proceedings of Supercomputing 1995, </booktitle> <month> dec </month> <year> 1995. </year>
Reference-contexts: These, however, are beyond the scope of this paper. 4 Status and Performance The basic Converse framework has been implemented on networks of Unix workstations connected by Ethernet/ATM, IBM SP, Intel Paragon, CM-5, Convex Exemplar, nCUBE/2, and on top of the Fast Messages layer <ref> [13] </ref> on the Cray T3D and Sun/Myrinet networks. Prototype implementations of PVM messaging and SM (a simple messaging layer) are complete and simple multi-lingual programs are demonstrated to run on the above machines. The Charm and Charm++ [11] parallel object-oriented languages have been retargeted for Converse.
Reference: [14] <author> V.S. Sunderam. </author> <title> PVM: A Framework for Parallel Distributed Computing. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 2(4), </volume> <month> December </month> <year> 1990. </year>
Reference-contexts: 1 Introduction Research on parallel computing has produced a number of different parallel programming paradigms, architectures and algorithms. There is a wealth of parallel programming paradigms such as SPMD <ref> [14, 12] </ref>, data-parallel [9, 8, 3], object-oriented [1, 3, 4, 5, 11], thread-based [7, 4, 10], macro-dataflow, functional languages, logic programming languages, and combinations of these. However, not all parallel algorithms can be efficiently implemented using a single parallel programming paradigm. <p> Such interoperability is not currently possible, except for a spe fl This research was supported in part by the ARPA grant DACA 88-94-C-0019 and NSF grant ASC-93-18159. cific subset of language implementations designed for this purpose (e.g. HPF [8] and PVM <ref> [14] </ref>). This paper describes the design and rationale of Converse, an interoperable framework for combining multiple languages and their runtime libraries into a single parallel program. It is based on a software architecture that allows programmers to compose multiple separately compiled modules written in different languages without losing performance.
Reference: [15] <author> J. Yelon and L. V. Kale. </author> <title> Thread primitives for an interoperable multiprocessor environment. </title> <type> Technical Report 95-15, </type> <institution> Parallel Programming Laboratory, Department of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> December </month> <year> 1995. </year> <note> Submitted for publication. </note>
Reference-contexts: Converse separates the capabilities of thread packages modularly. In particular, it provides the essential mechanisms for suspending and resuming threads as a separate component, which can be used with different thread sched-ulers and synchronization mechanisms, depending on the requirements of the parallel language or application <ref> [15] </ref>. Synchronization mechanisms: Locks are implemented by having queues attached to each lock. If a lock can be obtained, the thread trying to obtain the lock continues (after setting the lock to its locked state). If not, the thread is suspended and placed in a queue for the lock.
References-found: 15

