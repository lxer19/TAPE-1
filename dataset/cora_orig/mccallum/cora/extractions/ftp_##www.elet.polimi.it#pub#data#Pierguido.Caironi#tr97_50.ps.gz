URL: ftp://www.elet.polimi.it/pub/data/Pierguido.Caironi/tr97_50.ps.gz
Refering-URL: http://www.ph.tn.tudelft.nl/PRInfo/reports/msg00394.html
Root-URL: 
Email: email: caironi@elet.polimi.it  
Author: Pierguido V.C. CAIRONI 
Keyword: Gradient-Based Reinforcement Learning: Learning Combinations of Control Policies  
Date: October 15, 1997  
Note: Tech. Rep. 97.50  
Address: Piazza Leonardo da Vinci, 32 20133 Milano Italy  
Affiliation: Politecnico di Milano Dipartimento di Elettronica e Informazione  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Albus, J.S. </author> <year> (1972). </year> <title> Theoretical and Experimental Aspects of a Cerebellar Model, </title> <type> Ph.D. Dissertation, </type> <institution> University of Maryland, MD. </institution>
Reference-contexts: Since sigmoidal functions have very small derivatives with respect to their parameters when they reach saturation, this problem is mainly due to the choice of sigmoidal functions to represent motivations in Equations (4.22)-(4.24). Probably, the use of different families of base functions (e.g., radial basis, CMAC <ref> [1, 45] </ref>) which do not have this saturation problems should improve the performance and the convergence speed obtained during testing.
Reference: [2] <author> Anderson, C.W. </author> <year> (1987). </year> <title> Strategy Learning with Multilayer Connectionist Represent ations. </title> <booktitle> In Proceedings of the Fourth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 103-114. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79].
Reference: [3] <author> Anderson, C.W., & Hong, Z. </author> <year> (1994). </year> <title> Reinforcement Learning with Modular Neural Networks for Control. </title> <booktitle> In Proceedings of the IEEE International Workshop on Neural Networks Applied to Control and Image Processing (NNACIP'94), </booktitle> <publisher> IEEE Press, </publisher> <address> Pis-cataway, NJ. </address>
Reference-contexts: However, these approaches are either limited to softmax combinations <ref> [3] </ref> or to simple switching among base control policies [31, 62]. Chapter 4 Experiments Given the strong impact that the simple combination among hand-coded control policies may have on robotics, in this report I focus my experimental effort on the GREMLIN-M algorithm. I will test GREMLIN-MS in future works.
Reference: [4] <author> Baird III, L.C., & Klopf, A.H. </author> <year> (1996). </year> <title> Reinforcement Learning with High-Dimensional, Continuous Actions. </title> <institution> Technical Report WL-TR-93-1147 Wright-Patterson Air Force Base, OH. </institution>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [5] <author> Barto, A.G., Bradtke, S.J., & Singh, S.P. </author> <year> (1995). </year> <title> Learning to Act using Real-Time Dy namic Programming. </title> <journal> Artificial Intelligence, Special Volume: Computational Research on Interaction and Agency, </journal> <volume> 72(1), </volume> <pages> 81-138. </pages>
Reference-contexts: agent may still learn an experimental model of this function and apply the GREMLIN-M and GREMLIN-MS algorithms to this model. 1.5 Direct, Indirect and Predictive Critic Approach Both in adaptive control theory and in reinforcement learning there are two main approaches to the development of a controller: direct and indirect <ref> [5] </ref>. In the direct approach no model of the system is explicitly made and control parameters are tuned according to the results of a trial-and-error experimentation on the system to be controlled. Instead, in the indirect approach an explicit (possibly learned) model of the environment is assumed.
Reference: [6] <author> Barto, A.G., & Singh, S.P. </author> <year> (1990). </year> <title> On the Computational Economics of Reinforcement Learning. </title> <editor> In Touretzky, D.S., Elman, J.L., Sejnowski, T.J., & Hinton, G.E., eds., </editor> <booktitle> Proceedings of the 1990 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 35-44. </pages>
Reference-contexts: INTRODUCTION Which of the aforementioned approaches (direct, indirect or predictive critic) is best is still an open question and the results obtained are extremely application sensitive <ref> [6, 41, 65, 67, 68] </ref>.
Reference: [7] <author> Barto, A.G., Sutton, R.S., & Anderson, C.W. </author> <year> (1983). </year> <title> Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> 13(5), </volume> <pages> 834-846. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79]. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a
Reference: [8] <author> Bersini, H., & Gorrini, V. </author> <year> (1997). </year> <title> A Simplification of the Backpropagation-Through Time Algorithm for Optimal Neurocontrol. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 8(2), </volume> <pages> 437-441. </pages>
Reference-contexts: The same equations could have been applied to the past trajectory (as in <ref> [8, 52, 53, 35] </ref>); however, control optimization along the past is not useful if the agent has to plan an escape from a current bad situation. As an example, suppose that the agent is a small robot trapped inside a concave obstacle. <p> Some authors use back-propagation through time applying it to the past of the system (as in <ref> [8] </ref>) and some to the expected future (see [54] for a classification of all the possible approaches). 18 3.2. LEARNING COMBINATIONS OF FUNCTIONS 19 3.1.3 Forward Modeling Forward modeling has been proposed by Jordan and Rumelhart [35] in two versions: local and along trajectories.
Reference: [9] <author> Bertsekas, D.P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control, </title> <publisher> Athena Scientific, </publisher> <address> Belmont, MA. </address>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming <ref> [9] </ref> as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79].
Reference: [10] <author> Bertsekas, D.P. </author> <year> (1995). </year> <title> A Counterexample to Temporal Differences Learning. </title> <journal> Neural Computation, </journal> <volume> 7, </volume> <pages> 270-279. </pages>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [11] <author> Bertsekas, D.P., & Tsitsiklis, J.N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming, </title> <institution> Athena Sci entific, </institution> <address> Belmont, MA. 50 BIBLIOGRAPHY 51 </address>
Reference-contexts: In case of sums of squared errors, many approximate second order methods (such as quasi-Newton or Levenberg-Marquardt <ref> [11] </ref>) may be used to optimize the performance function of continuous-variable problems. <p> There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [12] <author> Bonarini, A. </author> <year> (1996). </year> <title> Evolutionary Learning of Fuzzy Rules: </title> <editor> Competition and Coopera tion. In Pedrycz, W., ed., </editor> <title> Fuzzy Modeling: Paradigms and Practice, </title> <publisher> Kluwer Academic, Norwell, </publisher> <address> MA, </address> <pages> 265-284. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [13] <editor> Booker, L.B., Goldberg, D.E., </editor> & <publisher> Holland, </publisher> <address> J.H. </address> <year> (1989). </year> <title> Classifier Systems and Genetic Algorithms. </title> <booktitle> Artificial Intelligence, </booktitle> <pages> 40(1-3), 235-282. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation <ref> [13, 24, 79] </ref>. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4.
Reference: [14] <author> Boyan, J.A., & Moore, A.W. </author> <year> (1995). </year> <title> Generalization in Reinforcement Learning: Safely Approximating the Value Function. </title> <editor> In Tesauro, G., Touretzky, D.S., & Leen, T.K., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 368-376. </pages>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [15] <author> Brody, C. </author> <year> (1992). </year> <title> Fast Learning with Predictive Forward Models. </title> <editor> In Moody, J.E., Hanson, S.J., & Lippmann, R.P., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 563-570. </pages>
Reference-contexts: Both of them are indirect supervised learning methods for learning a single control policy. Local forward modeling has no memory of the effects of previous control actions on the state of the system. Therefore, unless a predictive critic on reinforcements is used <ref> [15] </ref>, it is not able to solve the temporal credit assignment problem.
Reference: [16] <author> Brooks, R. </author> <year> (1989). </year> <title> A Robot that Walks: Emergent Behavior from a Carefully Evolved Network. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 253-262. </pages>
Reference-contexts: Thanks to generalizing function in control representation the agent does not fall into the complexity explosion of universal planning discussed in [27]. Moreover, the switching and especially the simultaneous combination of different control policy may generate emerging functionalities (see <ref> [16] </ref> and the "turn around the heat source" behavior described in Section 4.3.1). 2.8.3 Real Time The computational time required by each iteration of the two main loops of the GREMLIN-M and GREMLIN-MS algorithms is finite and constant, and may be computed during the design phase of the system. <p> The distance of the planning agent from obstacles is never lower than 0.3 m (see Figure 4.7) during the whole experiment. Moreover, as for concerns the heat source, an interesting emergent behavior <ref> [16] </ref> should be noted in Figure 4.4. While waiting its temper 30 CHAPTER 4. EXPERIMENTS ature to increase, the agent turns around the heat source. This behavior is the result of the simultaneous combination of the heat-seeking, obstacle-avoidance and, partially, of the target-following behaviors.
Reference: [17] <author> Cacciatore, T.W., & Nowlan, S.J. </author> <year> (1994). </year> <title> Mixture of Controllers for Jump Linear and Non-Linear Plants. </title> <editor> In Cowan, J.D., Tesauro, G., & Alspector, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> 719-726. </pages>
Reference-contexts: Moreover, mixture of experts is not able to deal with the temporal credit assignment problem, since the maximization of the log-likelihood function refers just to the current time step. 3.2.2 Mixture of Controllers Cacciatore and Nowlan <ref> [17] </ref> introduced a mixture of controllers which is basically a mixture of experts with a recurrent gating network. Mixture of controllers is limited to supervised learning of a predefined reference control function. 3.2.3 Switching Architecture Narendra, Balakrishnan and Ciliz [51] proposed a switching architecture among different controllers.
Reference: [18] <author> Caironi, P.V.C. </author> <year> (1997). </year> <title> Gradient Reinforcement Learning of Single and Multiple Con trol Policies: The Autonomous Agent Case, </title> <type> Ph.D. Thesis, </type> <institution> Politecnico di Milano, Di-partimento di Elettronica e Informazione, IT. </institution>
Reference-contexts: Case 1 reduces to a simple generalization of dynamic back propagation [52, 53] or backpropagation through time [56, 75] and is not discussed in this report, but in <ref> [18] </ref>. Cases 2 and 3 are solved by GREMLIN-M and GREMLIN-MS algorithms respectively. In the following two sections these two gradient ascent algorithms are detailed. <p> However, it is nonetheless interesting to study the behavior of the system when l tends to infinity. It is possible to show that if the learned controller is able to stabilize the model of the environment, Equations (2.9) and (2.14) are numerically stable (see <ref> [18] </ref>). Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems [58, 75, 78] especially in the neural-network [22, 29, 64] community. <p> In both the experiments the rand () and the randn () functions were respectively initialized to the same randomly chosen seeds 33840 and 44958. A statistical analysis of the results of whole populations of experiments obtained setting different seeds for the random number generators will be given in <ref> [18] </ref>. The experiments lasted for two hours of simulated time (7200 steps) during which the learning algorithm was always active. The results of the experiment referring to the planning agent are shown in Figures 4.4-4.12, while the results referring to the greedy agent are shown in Figures 4.13-4.21.
Reference: [19] <author> Clouse, J.A., & Utgoff, P.E. </author> <year> (1992). </year> <title> A teaching method for reinforcement learning. </title> <booktitle> In Proceedings of the Ninth International Conference of Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 92-101. </pages>
Reference-contexts: repeat behaviors that cause natural punishments (e.g., pain, discomfort, hunger, thirst). (For a discussion of a similar approach in the analysis of human behavior see [30].) In reinforcement learning for artificial agents the presence of a device called trainer or critic (which, in some case, may be a human being <ref> [19, 61] </ref>) is assumed.
Reference: [20] <author> Crites, R.H., & Barto, A.G. </author> <year> (1996). </year> <title> Improving Elevator Performance Using Reinforce ment Learning. </title> <editor> In Touretzky, D.S., Mozer, M.C., & Hasselmo, M.E., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 1017-1023. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [21] <author> Dayan, P., & Singh, S.P. </author> <year> (1996). </year> <title> Improving Policies without Measuring Merits. </title> <editor> In Touretzky, D.S., Mozer, M.C., & Hasselmo, M.E., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 1059-1065. </pages>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [22] <author> Deco, G., & Obradovic, D. </author> <year> (1996). </year> <title> An Information-Theoretic Approach to Neural Com puting, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY. </address>
Reference-contexts: Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems [58, 75, 78] especially in the neural-network <ref> [22, 29, 64] </ref> community.
Reference: [23] <author> Dickinson, A. </author> <year> (1980). </year> <title> Contemporary Animal Learning Theory, </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: RELEVANT PROPERTIES OF THE ALGORITHMS 17 connected to the different base control policies (see <ref> [23] </ref> for a definition of procedural and declarative knowledge and chapter 3 of [42] for a definition of utility). Thanks to generalizing function in control representation the agent does not fall into the complexity explosion of universal planning discussed in [27].
Reference: [24] <author> Dorigo, M., & Colombetti, M. </author> <year> (1997). </year> <title> Robot Shaping: An Experiment in Behavior Engineering, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation <ref> [13, 24, 79] </ref>. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. <p> There are many reasons to prefer a modular approach in the design and learning of a controller <ref> [24, 32] </ref>: * Many problems of control are intrinsically modular both in space (because very dif ferent kinds of controller have to be applied in different areas of the state space of the problem) and in time (because the behavior of the environment changes periodically going through a definite set of
Reference: [25] <author> Flake, G.W., Sun, G.-Z., Lee, Y.-C., & Chen, H.-H. </author> <year> (1994). </year> <title> Exploiting Chaos to Con trol the Future. </title> <editor> In Cowan, J.D., Tesauro, G., & Alspector, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, 646-654. 52 BIBLIOGRAPHY </address>
Reference-contexts: I would like to investigate further the relationship between the changing control parameters and the behavior obtained. I suspect that nontrivial changes of the behavior may be due to very small changes in the parameter values. This is typical of chaotic systems <ref> [25, 71] </ref>. 5.1.4 Inference and Reasoning As already said in Section 2.8.2 the algorithms proposed may be regarded as optimal planning algorithms doing a numerical search in the family of possible controllers within an horizon time limit of length l.
Reference: [26] <author> Fu, K.S., Gonzalez, R.C., & Lee, </author> <title> C.S.G. </title> <booktitle> (1987). Robotics: Control, Sensing, Vision, and Intelligence, </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: As an example, let us think to a learning system installed in a robot <ref> [26] </ref> with a black and white camera. This learning system cannot know if the world appears in black and white because the camera is made this way or because the word is actually black and white. As another example, let us consider a wheeled robot moving on a floor.
Reference: [27] <author> Ginsberg, </author> <title> M.L. (1989). Universal Planning: An (Almost) Universally Bad Idea. </title> <journal> AI Magazine, </journal> <volume> 10(4), </volume> <pages> 40-44. </pages>
Reference-contexts: Thanks to generalizing function in control representation the agent does not fall into the complexity explosion of universal planning discussed in <ref> [27] </ref>.
Reference: [28] <author> Harnad, S. </author> <year> (1990). </year> <title> The symbol grounding problem. </title> <journal> Physica D, </journal> <volume> 42, </volume> <pages> 335-346. </pages>
Reference-contexts: However, hand coding knowledge in PAA with noisy sensors is not an easy task and usually leads to problems of symbol grounding <ref> [28] </ref>.
Reference: [29] <author> Hertz, J., Krogh, A., & Palmer, R.G. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison Wesley, </publisher> <address> Redwood City, CA. </address>
Reference-contexts: From this point of view, the approach presented shares many elements with optimal adaptive control. 4 CHAPTER 1. INTRODUCTION In supervised learning (see definitions in the introduction of <ref> [29] </ref>) the learning system knows an explicit desired reference behavior, and, if the behavior space is normed, it adjusts the parameters of the control function to reduce the distance of the current behavior from the reference behavior. <p> Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems [58, 75, 78] especially in the neural-network <ref> [22, 29, 64] </ref> community.
Reference: [30] <author> Hobbes, T. </author> <title> (1651). Leviathan or the Matter Form of a Commonwealth, </title> <institution> Ecclesiastical and Civil. </institution>
Reference-contexts: inspired by ethological research [43] that shows that animals tend to repeat behaviors that lead to natural rewards (e.g., pleasure, satisfaction, satiety) and avoid to repeat behaviors that cause natural punishments (e.g., pain, discomfort, hunger, thirst). (For a discussion of a similar approach in the analysis of human behavior see <ref> [30] </ref>.) In reinforcement learning for artificial agents the presence of a device called trainer or critic (which, in some case, may be a human being [19, 61]) is assumed.
Reference: [31] <author> Huber, M., & Grupen, R.A. </author> <year> (1997). </year> <title> Learning to Coordinate Controllers: Reinforcement Learning on a Control Basis. </title> <booktitle> In Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> 1366-1371. </pages>
Reference-contexts: However, these approaches are either limited to softmax combinations [3] or to simple switching among base control policies <ref> [31, 62] </ref>. Chapter 4 Experiments Given the strong impact that the simple combination among hand-coded control policies may have on robotics, in this report I focus my experimental effort on the GREMLIN-M algorithm. I will test GREMLIN-MS in future works.
Reference: [32] <author> Jacobs, R.A., Jordan, M.I., & Barto, A.G. </author> <year> (1991). </year> <title> Task Decomposition Through Com petition in a Modular Connectionist Architecture: The What and Where Vision Tasks. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 219-250. </pages>
Reference-contexts: There are many reasons to prefer a modular approach in the design and learning of a controller <ref> [24, 32] </ref>: * Many problems of control are intrinsically modular both in space (because very dif ferent kinds of controller have to be applied in different areas of the state space of the problem) and in time (because the behavior of the environment changes periodically going through a definite set of <p> w max kr w Jk 2 otherwise w 0 w 0 + w return w + w 0 end-define 2.6 Exploration of the State Space Learning agents using controllers or models based on global base functions (e.g., sigmoidal neural net) suffer from a phenomenon of local over-fitting (also discussed in <ref> [32, 33] </ref> as temporal crosstalk ). This type of agents, while improving their performance in the current area of the state-action space, forget what they have learned in previously experienced areas of the space since they seem to start learning almost from scratch when taken back to these areas. <p> it iterates backward in time (compare Equations (2.7)-(2.11) and (2.12)-(2.16) with the Equations (15)-(19) in [35]). 3.2 Learning Combinations of Functions This section presents a quick review of the most relevant work about learning modular functions. 3.2.1 Mixture of Experts Mixture of experts has been introduced by Jacobs and Jordan <ref> [32, 33] </ref>. It is a supervised learning method to approximate a reference function with a convex combination (softmax function) of the output of several neural nets.
Reference: [33] <author> Jacobs, R.A., & Jordan, M.I. </author> <year> (1993). </year> <title> Learning Piecewise Control Strategies in a Modu lar Neural Network Architecture. </title> <journal> IEEE Transactions on System, Man and Cybernetics, </journal> <volume> 23(2), </volume> <pages> 337-345. </pages>
Reference-contexts: w max kr w Jk 2 otherwise w 0 w 0 + w return w + w 0 end-define 2.6 Exploration of the State Space Learning agents using controllers or models based on global base functions (e.g., sigmoidal neural net) suffer from a phenomenon of local over-fitting (also discussed in <ref> [32, 33] </ref> as temporal crosstalk ). This type of agents, while improving their performance in the current area of the state-action space, forget what they have learned in previously experienced areas of the space since they seem to start learning almost from scratch when taken back to these areas. <p> it iterates backward in time (compare Equations (2.7)-(2.11) and (2.12)-(2.16) with the Equations (15)-(19) in [35]). 3.2 Learning Combinations of Functions This section presents a quick review of the most relevant work about learning modular functions. 3.2.1 Mixture of Experts Mixture of experts has been introduced by Jacobs and Jordan <ref> [32, 33] </ref>. It is a supervised learning method to approximate a reference function with a convex combination (softmax function) of the output of several neural nets. <p> It is a supervised learning method to approximate a reference function with a convex combination (softmax function) of the output of several neural nets. Unless the control problem consists of learning the inverse dynamics of the controlled system <ref> [33] </ref>, mixture of experts cannot be applied to learn a control policy since it lacks an explicit model of the environment.
Reference: [34] <author> Jordan, M.I., & Jacobs, R.A. </author> <year> (1990). </year> <title> Learning to Control an Unstable System with Forward Modeling. </title> <editor> In Touretzky, D.S., ed., </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 324-331. </pages>
Reference-contexts: In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a
Reference: [35] <author> Jordan, M.I., & Rumelhart, D.E. </author> <year> (1992). </year> <title> Forward Models: Supervised Learning with a Distal Teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference-contexts: The same equations could have been applied to the past trajectory (as in <ref> [8, 52, 53, 35] </ref>); however, control optimization along the past is not useful if the agent has to plan an escape from a current bad situation. As an example, suppose that the agent is a small robot trapped inside a concave obstacle. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a <p> LEARNING COMBINATIONS OF FUNCTIONS 19 3.1.3 Forward Modeling Forward modeling has been proposed by Jordan and Rumelhart <ref> [35] </ref> in two versions: local and along trajectories. Both of them are indirect supervised learning methods for learning a single control policy. Local forward modeling has no memory of the effects of previous control actions on the state of the system. <p> The approach on which forward modeling along trajectories is based, namely, Lagrange multipliers, is different from the simple gradient used in the algorithms proposed in this report, besides, it iterates backward in time (compare Equations (2.7)-(2.11) and (2.12)-(2.16) with the Equations (15)-(19) in <ref> [35] </ref>). 3.2 Learning Combinations of Functions This section presents a quick review of the most relevant work about learning modular functions. 3.2.1 Mixture of Experts Mixture of experts has been introduced by Jacobs and Jordan [32, 33].
Reference: [36] <author> Kaelbling, </author> <title> L.P., Littman, M.L., & Moore, </title> <publisher> A.W. </publisher> <year> (1996). </year> <title> Reinforcement Learning: A Survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4(5), </volume> <pages> 237-285. </pages>
Reference-contexts: CONTROL POLICIES AND BEHAVIORS 3 system is controllable [37], observable [38] and free from perceptual aliasing problems <ref> [36] </ref>). <p> INTRODUCTION In supervised learning (see definitions in the introduction of [29]) the learning system knows an explicit desired reference behavior, and, if the behavior space is normed, it adjusts the parameters of the control function to reduce the distance of the current behavior from the reference behavior. Reinforcement learning <ref> [36] </ref> for artificial agents is inspired by ethological research [43] that shows that animals tend to repeat behaviors that lead to natural rewards (e.g., pleasure, satisfaction, satiety) and avoid to repeat behaviors that cause natural punishments (e.g., pain, discomfort, hunger, thirst). (For a discussion of a similar approach in the analysis <p> In my experience, a complete model of the environment can speed up learning considerably when the system is dynamic and a difficult temporal credit assignment problem <ref> [36] </ref> has to be solved since the model allows the learning system to look forward into the future comparing the outcomes of different control policies.
Reference: [37] <author> Levin, A.U., & Narendra, K.S. </author> <year> (1993). </year> <title> Control of Nonlinear Dynamical Systems Us ing Neural Networks: Controllability and Stabilization. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 4(2), </volume> <pages> 192-206. </pages>
Reference-contexts: CONTROL POLICIES AND BEHAVIORS 3 system is controllable <ref> [37] </ref>, observable [38] and free from perceptual aliasing problems [36]). <p> In the particular context of learning a control policy for a dynamic system it has been applied in [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] although many problems are still open about the properties of the controllers obtained by gradient methods <ref> [37, 38] </ref>. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a control policy for a dynamic system. 3.1.1 Dynamic Back-propagation The dynamic back-propagation algorithm has been proposed by Narendra in [52, 53].
Reference: [38] <author> Levin, A.U., & Narendra, K.S. </author> <year> (1996). </year> <title> Control of Nonlinear Dynamical Systems Using Neural Networks Part II: Observability, Identification and Control. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 7(1), </volume> <pages> 30-42. </pages>
Reference-contexts: CONTROL POLICIES AND BEHAVIORS 3 system is controllable [37], observable <ref> [38] </ref> and free from perceptual aliasing problems [36]). <p> In the particular context of learning a control policy for a dynamic system it has been applied in [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] although many problems are still open about the properties of the controllers obtained by gradient methods <ref> [37, 38] </ref>. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a control policy for a dynamic system. 3.1.1 Dynamic Back-propagation The dynamic back-propagation algorithm has been proposed by Narendra in [52, 53].
Reference: [39] <author> Lin, L.-J. </author> <year> (1993). </year> <title> Reinforcement Learning for Robots Using Neural Networks, </title> <type> Ph.D. Thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a
Reference: [40] <author> Mahadevan, S., & Connell, J. </author> <year> (1992). </year> <title> Automatic Programming of Behavior-Based Ro bots Using Reinforcement Learning. </title> <journal> Artificial Intelligence, </journal> <volume> 55, </volume> <pages> 311-365. BIBLIOGRAPHY 53 </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79].
Reference: [41] <author> Markey, K.L., & Mozer, M.C. </author> <year> (1992). </year> <title> Comparison of Reinforcement Algorithms on Discrete Functions: Learnability, Time Complexity, and Scaling. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <pages> 853-859. </pages>
Reference-contexts: INTRODUCTION Which of the aforementioned approaches (direct, indirect or predictive critic) is best is still an open question and the results obtained are extremely application sensitive <ref> [6, 41, 65, 67, 68] </ref>.
Reference: [42] <author> McFarland, D., & Bosser, T. </author> <year> (1993). </year> <title> Intelligent Behavior in Animals and Robots, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Consequently, the control policy developed by GREMLIN-M and GREMLIN-MS may be regarded as the result of a numerical (as opposed to symbolic) planning activity executed within an horizon time limit of l time steps. According to D. McFarland and T. Bosser <ref> [42] </ref> this planning capability makes the agents using the GREMLIN-M and GREMLIN-MS algorithms motivationally autonomous since they are able to decide current actions according to expected future rewards. <p> RELEVANT PROPERTIES OF THE ALGORITHMS 17 connected to the different base control policies (see [23] for a definition of procedural and declarative knowledge and chapter 3 of <ref> [42] </ref> for a definition of utility). Thanks to generalizing function in control representation the agent does not fall into the complexity explosion of universal planning discussed in [27].
Reference: [43] <author> McFarland, D., & Houston, A. </author> <year> (1981). </year> <title> Quantitative Ethology: The State-Space Ap proach, </title> <publisher> Pitman, </publisher> <address> London, UK. </address>
Reference-contexts: Reinforcement learning [36] for artificial agents is inspired by ethological research <ref> [43] </ref> that shows that animals tend to repeat behaviors that lead to natural rewards (e.g., pleasure, satisfaction, satiety) and avoid to repeat behaviors that cause natural punishments (e.g., pain, discomfort, hunger, thirst). (For a discussion of a similar approach in the analysis of human behavior see [30].) In reinforcement learning for
Reference: [44] <author> Millan, J. Del R., & Torras, C. </author> <year> (1992). </year> <title> A Reinforcement Connectionist Approach to Robot Path Finding in Non-Maze-Like Environments. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 363-395. </pages>
Reference-contexts: Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation <ref> [44] </ref>.
Reference: [45] <author> Miller, W.T., Glanz, F.H., & Kraft, L.G. </author> <year> (1990). </year> <title> CMAC: An Associative Neural Net work Alternative to Backpropagation. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10), </volume> <pages> 1561-1567. </pages>
Reference-contexts: Since sigmoidal functions have very small derivatives with respect to their parameters when they reach saturation, this problem is mainly due to the choice of sigmoidal functions to represent motivations in Equations (4.22)-(4.24). Probably, the use of different families of base functions (e.g., radial basis, CMAC <ref> [1, 45] </ref>) which do not have this saturation problems should improve the performance and the convergence speed obtained during testing.
Reference: [46] <author> Moore, A.W. </author> <year> (1991). </year> <title> Variable Resolution Dynamic Programming: Efficiently Learning Action Maps in Multivariate Real-valued State-spaces. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 333-337. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79]. <p> There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [47] <author> Moore, A.W. </author> <year> (1994). </year> <title> The Parti-game Algorithm for Variable Resolution Reinforcement Learning in Multidimensional State-spaces. </title> <editor> In Cowan, J.D., Tesauro, G., & Alspector, J., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <pages> 711-718. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [48] <author> Munos, R. </author> <year> (1997). </year> <title> A Convergent Reinforcement Learning Algorithm in the Continuous Case Based on a Finite Difference Methods. </title> <booktitle> In Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [49] <author> Musliner, D.J., Hendler, J.A., Agrawala, A.K., Durfee, E.H., Strosnider, J.K., & Paul, C.J. </author> <year> (1995). </year> <title> The Challenges of Real-Time AI. </title> <journal> IEEE Computer, </journal> <volume> 28(1), </volume> <pages> 58-66. </pages>
Reference-contexts: Therefore, the GREMLIN-M and GREMLIN-MS algorithms may be used for real-time learning <ref> [49] </ref>. 2.8.4 Numerical Stability Recursive Equations (2.9) and (2.14) are executed for a definite number of times (actually, l + 1 times) before the recursion is reinitialized, therefore, if l is sufficiently small, no problem of numerical stability should emerge.
Reference: [50] <author> Narendra, K.S. </author> <year> (1996). </year> <title> Neural Networks for Control: </title> <journal> Theory and Practice. Proceedings of the IEEE, </journal> <volume> 84(10), </volume> <pages> 1385-1406. </pages>
Reference-contexts: In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a
Reference: [51] <author> Narendra, K.S., Balakrishnan, J., & Ciliz, K. </author> <year> (1995). </year> <title> Adaptation and Learning Using Multiple Models, Switching and Tuning. </title> <journal> IEEE Control Systems Magazine, </journal> <month> June, </month> <pages> 37-51. </pages>
Reference-contexts: Mixture of controllers is limited to supervised learning of a predefined reference control function. 3.2.3 Switching Architecture Narendra, Balakrishnan and Ciliz <ref> [51] </ref> proposed a switching architecture among different controllers. Their approach is based on the identification of many environment models and in the switch to the optimal controller for the model that is currently producing the least prediction error on the environment.
Reference: [52] <author> Narendra, K.S., & Parthasarathy, K. </author> <year> (1990). </year> <title> Identification and Control of Dynamical Systems Using Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 1(1), </volume> <pages> 4-27. </pages>
Reference-contexts: GREMLIN-MS extends GREMLIN-M allowing the agent to learn simultaneously the parameters of the base control policies as well. The two algorithms are devised for agent-environment relationships that can be modeled by discrete-time models with continuous state and control variables. While being natural extensions of previously existing supervised learning algorithms <ref> [52, 53, 56] </ref>, GREMLIN-M and GREMLIN-MS improve the current state of art of reinforcement learning taking into account the temporal credit assignment problem for the combination of control policies. Furthermore, GREMLIN-M and GREMLIN-MS lend themselves to a motivational interpretation. <p> Learn simultaneously the base control function and their combination, which consists of finding the values of the parameters v and w i that maximizes J for any possible starting condition s (t) = s o . Case 1 reduces to a simple generalization of dynamic back propagation <ref> [52, 53] </ref> or backpropagation through time [56, 75] and is not discussed in this report, but in [18]. Cases 2 and 3 are solved by GREMLIN-M and GREMLIN-MS algorithms respectively. In the following two sections these two gradient ascent algorithms are detailed. <p> The same equations could have been applied to the past trajectory (as in <ref> [8, 52, 53, 35] </ref>); however, control optimization along the past is not useful if the agent has to plan an escape from a current bad situation. As an example, suppose that the agent is a small robot trapped inside a concave obstacle. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a <p> the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a control policy for a dynamic system. 3.1.1 Dynamic Back-propagation The dynamic back-propagation algorithm has been proposed by Narendra in <ref> [52, 53] </ref>. It is an indirect algorithm for supervised learning of single control policies.
Reference: [53] <author> Narendra, K.S., & Parthasarathy, K. </author> <year> (1991). </year> <title> Gradient Methods for the Optimization of Dynamical Systems Containing Neural Networks. </title> <journal> IEEE Transactions on Neural Networks, </journal> <volume> 2(2), </volume> <pages> 252-262. </pages>
Reference-contexts: GREMLIN-MS extends GREMLIN-M allowing the agent to learn simultaneously the parameters of the base control policies as well. The two algorithms are devised for agent-environment relationships that can be modeled by discrete-time models with continuous state and control variables. While being natural extensions of previously existing supervised learning algorithms <ref> [52, 53, 56] </ref>, GREMLIN-M and GREMLIN-MS improve the current state of art of reinforcement learning taking into account the temporal credit assignment problem for the combination of control policies. Furthermore, GREMLIN-M and GREMLIN-MS lend themselves to a motivational interpretation. <p> Learn simultaneously the base control function and their combination, which consists of finding the values of the parameters v and w i that maximizes J for any possible starting condition s (t) = s o . Case 1 reduces to a simple generalization of dynamic back propagation <ref> [52, 53] </ref> or backpropagation through time [56, 75] and is not discussed in this report, but in [18]. Cases 2 and 3 are solved by GREMLIN-M and GREMLIN-MS algorithms respectively. In the following two sections these two gradient ascent algorithms are detailed. <p> The same equations could have been applied to the past trajectory (as in <ref> [8, 52, 53, 35] </ref>); however, control optimization along the past is not useful if the agent has to plan an escape from a current bad situation. As an example, suppose that the agent is a small robot trapped inside a concave obstacle. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a <p> the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a control policy for a dynamic system. 3.1.1 Dynamic Back-propagation The dynamic back-propagation algorithm has been proposed by Narendra in <ref> [52, 53] </ref>. It is an indirect algorithm for supervised learning of single control policies. <p> The algorithms presented in this report may be considered as a natural extension of dynamic back-propagation to combinations of control policies and reinforcement learning (compare the equations presented in Section 2 with the equations discussed in Section IV of <ref> [53] </ref>) As noted in Section 2.2, the algorithms proposed in this report optimize the future expected performance function of the agent.
Reference: [54] <author> Nerrand, O., Roussel-Ragot, P., Personnaz, L., Dreyfus, G., & Marcos, S. </author> <year> (1993). </year> <title> Neural Networks and Nonlinear Adaptive Filtering: Unifying Concepts and New Algorithms. </title> <journal> Neural Computation, </journal> <volume> 5, </volume> <pages> 165-199. </pages>
Reference-contexts: Some authors use back-propagation through time applying it to the past of the system (as in [8]) and some to the expected future (see <ref> [54] </ref> for a classification of all the possible approaches). 18 3.2. LEARNING COMBINATIONS OF FUNCTIONS 19 3.1.3 Forward Modeling Forward modeling has been proposed by Jordan and Rumelhart [35] in two versions: local and along trajectories. Both of them are indirect supervised learning methods for learning a single control policy.
Reference: [55] <author> Newton, Sir I. </author> <note> (1707). Arithmetica Universalis. 54 BIBLIOGRAPHY </note>
Reference-contexts: It is possible to show that if the learned controller is able to stabilize the model of the environment, Equations (2.9) and (2.14) are numerically stable (see [18]). Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization <ref> [55] </ref> and many authors have used it for adaptive systems [58, 75, 78] especially in the neural-network [22, 29, 64] community.
Reference: [56] <author> Nguyen, D., & Widrow, B. </author> <year> (1990). </year> <title> Neural networks for self-learning control systems. </title> <journal> IEEE Control System Magazine, </journal> <volume> 10(3), </volume> <pages> 18-23. </pages>
Reference-contexts: GREMLIN-MS extends GREMLIN-M allowing the agent to learn simultaneously the parameters of the base control policies as well. The two algorithms are devised for agent-environment relationships that can be modeled by discrete-time models with continuous state and control variables. While being natural extensions of previously existing supervised learning algorithms <ref> [52, 53, 56] </ref>, GREMLIN-M and GREMLIN-MS improve the current state of art of reinforcement learning taking into account the temporal credit assignment problem for the combination of control policies. Furthermore, GREMLIN-M and GREMLIN-MS lend themselves to a motivational interpretation. <p> Case 1 reduces to a simple generalization of dynamic back propagation [52, 53] or backpropagation through time <ref> [56, 75] </ref> and is not discussed in this report, but in [18]. Cases 2 and 3 are solved by GREMLIN-M and GREMLIN-MS algorithms respectively. In the following two sections these two gradient ascent algorithms are detailed. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a <p> On the contrary dynamic back propagation computes the gradient of the performance function with respect to the past trajectory of the agent. 3.1.2 Back-Propagation Through Time Originally, back-propagation through time has been devised for supervised learning of recurrent functions [58]. However, <ref> [56] </ref> and [75] have proposed an extension of this algorithm to the supervised learning of control policies.
Reference: [57] <author> Rich, E., & Knight, K. </author> <year> (1991). </year> <booktitle> Artificial Intelligence (2nd ed.), </booktitle> <publisher> McGraw-Hill, </publisher> <address> New York, NY. </address>
Reference-contexts: But the analogies with traditional artificial intelligence <ref> [57] </ref> are probably more far-reaching than simple planning. As an example consider the heat-seeking behavior of Chapter 4.
Reference: [58] <author> Rumelhart, D.E., Hinton, G.E., & Williams, R.J. </author> <year> (1986). </year> <title> Learning Internal Represent ations by Error Propagation. </title> <editor> In Rumelhart, D.E., McClelland, J.L., </editor> & <booktitle> PDP Research Group, Parallel Distributed Processing: Explorations in the Microstructure of Cognition, Volume 1: Foundations, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 319-362. </pages>
Reference-contexts: Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems <ref> [58, 75, 78] </ref> especially in the neural-network [22, 29, 64] community. <p> On the contrary dynamic back propagation computes the gradient of the performance function with respect to the past trajectory of the agent. 3.1.2 Back-Propagation Through Time Originally, back-propagation through time has been devised for supervised learning of recurrent functions <ref> [58] </ref>. However, [56] and [75] have proposed an extension of this algorithm to the supervised learning of control policies.
Reference: [59] <author> Saad, M., Dessaint, L.A., Bigras, P., & Al-Haddad, K. </author> <year> (1994). </year> <title> Adaptive versus Neural Adaptive Control: Application to Robotics. </title> <journal> International Journal of Adaptive Control and Signal Processing, </journal> <volume> 8, </volume> <pages> 223-236. </pages>
Reference-contexts: In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a
Reference: [60] <author> Santamaria, J.C., Sutton, R.S., & Ram, A. </author> <year> (1996). </year> <title> Experiments with Reinforcement Learning in Problems with Continuous State and Action Spaces, </title> <type> Technical Report UM-CS-1996-088, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [61] <author> Shepanski, J.F., & Macy, S.A. </author> <year> (1987). </year> <title> Manual Training Techniques of Autonomous Systems Based on Artificial Neural Networks. </title> <booktitle> In Proceedings of the IEEE First Annual International Conference on Neural Networks, </booktitle> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <pages> 697-704. </pages>
Reference-contexts: repeat behaviors that cause natural punishments (e.g., pain, discomfort, hunger, thirst). (For a discussion of a similar approach in the analysis of human behavior see [30].) In reinforcement learning for artificial agents the presence of a device called trainer or critic (which, in some case, may be a human being <ref> [19, 61] </ref>) is assumed.
Reference: [62] <author> Singh, S.P. </author> <year> (1992). </year> <title> Transfer of Learning by Composing Solutions of Elemental Sequen tial Tasks. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 323-339. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79]. <p> However, these approaches are either limited to softmax combinations [3] or to simple switching among base control policies <ref> [31, 62] </ref>. Chapter 4 Experiments Given the strong impact that the simple combination among hand-coded control policies may have on robotics, in this report I focus my experimental effort on the GREMLIN-M algorithm. I will test GREMLIN-MS in future works.
Reference: [63] <author> Singh, S.P., Jaakkola, T., & Jordan, M.I. </author> <year> (1995). </year> <title> Reinforcement Learning with Soft State Aggregation. </title> <editor> In Tesauro, G., Touretzky, D.S., & Leen, T.K., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 7, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 361-368. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [64] <author> Sjoberg, J., Hjalmarsson, H., & Ljung, L. </author> <year> (1994). </year> <title> Neural Networks in System Identifica tion. </title> <booktitle> In Proceedings of the 10th IFAC Symposium on System Identification (SYSID'94), Copenhagen, DK, </booktitle> <volume> 2, </volume> <pages> 49-72. </pages>
Reference-contexts: Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems [58, 75, 78] especially in the neural-network <ref> [22, 29, 64] </ref> community.
Reference: [65] <author> Spall, J.C., & Cristion, J.A. </author> <year> (1995). </year> <title> Model-Free Control on Nonlinear Stochastic Sys tems in Discrete Time. </title> <booktitle> In Proceedings of the 34th Conference on Decision & Control, </booktitle> <publisher> IEEE Press, </publisher> <address> Piscataway, NJ, </address> <pages> 2199-2204. </pages>
Reference-contexts: INTRODUCTION Which of the aforementioned approaches (direct, indirect or predictive critic) is best is still an open question and the results obtained are extremely application sensitive <ref> [6, 41, 65, 67, 68] </ref>.
Reference: [66] <author> Sutton, R.S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Ma chine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79].
Reference: [67] <author> Sutton, R.S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 216-224. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79]. <p> INTRODUCTION Which of the aforementioned approaches (direct, indirect or predictive critic) is best is still an open question and the results obtained are extremely application sensitive <ref> [6, 41, 65, 67, 68] </ref>. <p> To avoid this problem and better exploit the possibility offered by the presence of an environment model, I use an approach close to Sutton's relaxation planning <ref> [67] </ref> which I call exploration of the state space.
Reference: [68] <author> Sutton, R.S. </author> <year> (1991). </year> <title> Planning by incremental dynamic programming. </title> <booktitle> In Proceedings of the Eighth International Workshop on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, 353-357. BIBLIOGRAPHY 55 </address>
Reference-contexts: INTRODUCTION Which of the aforementioned approaches (direct, indirect or predictive critic) is best is still an open question and the results obtained are extremely application sensitive <ref> [6, 41, 65, 67, 68] </ref>.
Reference: [69] <author> Sutton, R.S. </author> <year> (1996). </year> <title> Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding. </title> <editor> In Touretzky, D.S., Mozer, M.C., & Hasselmo, M.E., eds., </editor> <booktitle> Advances in Neural Information Processing Systems 8, </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <pages> 1039-1044. </pages>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [70] <author> Sutton, </author> <title> R.S., & Whitehead, S.D. (1993). Online learning with random representations. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <pages> 314-321. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation [13, 24, 79]. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces <ref> [4, 11, 12, 20, 46, 47, 60, 63, 70] </ref> is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [71] <author> Tani, J. </author> <year> (1996). </year> <title> Model-Based Learning for Mobile Robot Navigation from the Dynam ical Systems Perspective. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics Part B: Cybernetics, </journal> <volume> 26(3), </volume> <pages> 421-436. </pages>
Reference-contexts: I would like to investigate further the relationship between the changing control parameters and the behavior obtained. I suspect that nontrivial changes of the behavior may be due to very small changes in the parameter values. This is typical of chaotic systems <ref> [25, 71] </ref>. 5.1.4 Inference and Reasoning As already said in Section 2.8.2 the algorithms proposed may be regarded as optimal planning algorithms doing a numerical search in the family of possible controllers within an horizon time limit of length l.
Reference: [72] <author> The MathWorks, Inc. </author> <year> (1994). </year> <title> Matlab User's Guide, The MathWorks, </title> <publisher> Inc., </publisher> <address> Natick, MA. </address>
Reference-contexts: state of the agent and in other four states chosen randomly with a uniform distribution among the possible combinations of the values of the seven sensory variables. 4.3 Experimental Results The environment and the agent described in the previous sections have been simulated with a program implemented in Matlab Ver.4.2c.1 <ref> [72] </ref>. The standard Matlab functions rand () and randn () have been used to generate the stochastic elements of the simulation. In this section I analyze the results of two experiments obtained setting l = 6 (planning agent) and l = 0 (greedy agent).
Reference: [73] <author> Thrun, S., & Schwartz, A. </author> <year> (1993). </year> <title> Issues in Using Function Approximation for Re inforcement Learning. </title> <booktitle> In Proceedings of the Fourth Connectionist Models Summer School, </booktitle> <publisher> Lawrence Erlbaum Publisher, </publisher> <address> Hillsdale, NJ. </address>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [74] <author> Watkins, C.J.C.H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards, </title> <type> Ph.D. Thesis, </type> <institution> University of Cambridge, UK. </institution>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79]. <p> method is useful for jump-linear systems and other sorts of systems characterized by dramatic changes in the environment dynamics; however, it cannot sum different control policies simultaneously, just switch among them. 3.2.4 Other Combinations There are also some works about multiple controllers arbitrated by a gate network learned through Q-Learning <ref> [74] </ref>. However, these approaches are either limited to softmax combinations [3] or to simple switching among base control policies [31, 62].
Reference: [75] <author> Werbos, P.J. </author> <year> (1990). </year> <title> Backpropagation Through Time: What It Does and How to Do It. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 78(10), </volume> <pages> 1550-1559. </pages>
Reference-contexts: Case 1 reduces to a simple generalization of dynamic back propagation [52, 53] or backpropagation through time <ref> [56, 75] </ref> and is not discussed in this report, but in [18]. Cases 2 and 3 are solved by GREMLIN-M and GREMLIN-MS algorithms respectively. In the following two sections these two gradient ascent algorithms are detailed. <p> Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems <ref> [58, 75, 78] </ref> especially in the neural-network [22, 29, 64] community. <p> In the particular context of learning a control policy for a dynamic system it has been applied in <ref> [7, 34, 35, 39, 50, 52, 53, 56, 59, 75] </ref> although many problems are still open about the properties of the controllers obtained by gradient methods [37, 38]. 3.1 Learning a Control Policy for a Dynamic System This section presents a quick review of the most relevant work about learning a <p> On the contrary dynamic back propagation computes the gradient of the performance function with respect to the past trajectory of the agent. 3.1.2 Back-Propagation Through Time Originally, back-propagation through time has been devised for supervised learning of recurrent functions [58]. However, [56] and <ref> [75] </ref> have proposed an extension of this algorithm to the supervised learning of control policies. The algorithm resulting from this extension is quite close to dynamic back-propagation and to the algorithms presented in this report (apart from the elements strictly referring to combination of control policies and reinforcement learning).
Reference: [76] <author> Whitehead, S.D. </author> <year> (1991). </year> <title> A Complexity Analysis of Cooperative Mechanisms in Re inforcement Learning. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <pages> 607-613. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in <ref> [2, 7, 40, 46, 62, 66, 67, 74, 76] </ref> or evolutionary computation [13, 24, 79].
Reference: [77] <author> Williams, R.J., & Baird III, L.C. </author> <year> (1993). </year> <title> Tight Performance Bounds on Greedy Poli cies Based on Imperfect Value Functions. </title> <type> Technical Report NU-CCS-93-14, </type> <institution> College of Computer Science, Northeastern University, </institution> <address> Boston, MA. </address>
Reference-contexts: However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem <ref> [10, 14, 21, 48, 69, 73, 77] </ref>. Not much is present in the literature 1.4. INTERNAL OR EXTERNAL TRAINER 5 about reinforcement learning not explicitly based on dynamic programming or evolutionary computation [44].
Reference: [78] <author> Williams, R.J. & Zipser, D. </author> <year> (1989). </year> <title> A learning Algorithm for Continually Running Fully Recurrent Neural Networks. </title> <journal> Neural Computation, </journal> <volume> 1, </volume> <pages> 270-280. </pages>
Reference-contexts: Chapter 3 Related Work Gradient ascent is an old and well known method of doing optimization [55] and many authors have used it for adaptive systems <ref> [58, 75, 78] </ref> especially in the neural-network [22, 29, 64] community.
Reference: [79] <author> Wilson, S.W. </author> <year> (1995). </year> <title> Classifier Fitness Based on Accuracy. </title> <journal> Evolutionary Computation, </journal> <volume> 3(2), </volume> <pages> 149-175. </pages>
Reference-contexts: There are many approaches to reinforcement learning, much of them related to dynamic programming [9] as in [2, 7, 40, 46, 62, 66, 67, 74, 76] or evolutionary computation <ref> [13, 24, 79] </ref>. However, the possibility of extending dynamic programming or evolutionary computation to continuous or very large state-action spaces [4, 11, 12, 20, 46, 47, 60, 63, 70] is still an open research problem [10, 14, 21, 48, 69, 73, 77]. Not much is present in the literature 1.4.
References-found: 79

