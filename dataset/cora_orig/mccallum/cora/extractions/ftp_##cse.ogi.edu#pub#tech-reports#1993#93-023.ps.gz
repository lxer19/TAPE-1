URL: ftp://cse.ogi.edu/pub/tech-reports/1993/93-023.ps.gz
Refering-URL: http://www.cse.ogi.edu/~stoltz/www.dir/sparse.cprop.html
Root-URL: http://www.cse.ogi.edu
Email: stoltz@cse.ogi.edu  
Phone: (503) 690-1121 ext. 7404  
Title: Demand-Driven Constant Propagation  
Author: Eric Stoltz, Michael Wolfe, and Michael P. Gerlek 
Note: This research supported by NSF grant CCR9113885, ARPA grant F3062-92-C-135, and grants from Intel Corporation and Matsushita Electric Industrial.  
Date: 93-023  
Address: P.O. Box 91000 Portland, OR 97291-1000  
Affiliation: Department of Computer Science and Engineering Oregon Graduate Institute of Science Technology  
Pubnum: Technical Report  
Abstract: In this paper, we present a new method for detecting constants, based upon an optimistic demand-driven recursive solver, as opposed to more traditional iterative solvers. The problem with iterative solvers is that they may evaluate an expression many times, while our technique evaluates each expression only once. To consider conditional code, we augment the standard Static Single Assignment (SSA) form with merge operators called fl-functions, adapted from the interpretable Gated Single Assignment (GSA) model. We present preliminary experimental results which show the number of intra-procedural constants found in common high-performance Fortran programs. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Jean-Paul Tremblay and Paul G. Sorenson. </author> <title> The Theory and Practice of Compiler Writing. </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1985. </year> <month> 20 </month>
Reference-contexts: 1 Introduction Constant propagation is a static technique employed by the compiler to determine values which do not change regardless of the program path taken. In fact, it is a generalization of constant folding <ref> [1] </ref>, the deduction at compile time that the value of an expression is constant, and is frequently used as a preliminary to other optimizations. The results can often be propagated to other expressions, enabling further applications of the technique.
Reference: [2] <author> Michael P. Gerlek, Eric Stoltz, and Michael Wolfe. </author> <title> Beyond induction variables: Detecting and classifying sequences using a demand-driven SSA form. </title> <note> submitted for publication, </note> <month> September </month> <year> 1993. </year>
Reference-contexts: Propagation of real-valued expressions can be performed, but special care is required since operations on real-valued expressions are often architecturally dependent. The method outlined in this work also allows for arbitrary symbolic expression propagation <ref> [2] </ref>. S1: x = 2 + 3 Although in general constant propagation is an undecidable problem [3], it is nonetheless extremely useful and profitable for a number of optimizations. <p> Induction variables are traditionally detected as a precursor to strength reduction, and more recently for dependence analysis with regard to subscript expressions. We have developed methods for detecting and classifying induction variables (including non-linear induction variables [20, 21, 22]) based on strongly-connected regions in the SSA data-flow graph <ref> [2] </ref>. These techniques make use of an exit function, the -function, which holds the exit value of a variable assigned within the loop.
Reference: [3] <author> J. Kam and J. Ullman. </author> <title> Monotone data flow analysis frameworks. </title> <journal> Acta Informatica 7, </journal> <pages> pages 305-317, </pages> <year> 1977. </year>
Reference-contexts: The method outlined in this work also allows for arbitrary symbolic expression propagation [2]. S1: x = 2 + 3 Although in general constant propagation is an undecidable problem <ref> [3] </ref>, it is nonetheless extremely useful and profitable for a number of optimizations. These include dead code elimination [4], array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants [5].
Reference: [4] <author> Mark N. Wegman and F. Kenneth Zadeck. </author> <title> Constant propagation with conditional branches. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(2) </volume> <pages> 181-210, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: The method outlined in this work also allows for arbitrary symbolic expression propagation [2]. S1: x = 2 + 3 Although in general constant propagation is an undecidable problem [3], it is nonetheless extremely useful and profitable for a number of optimizations. These include dead code elimination <ref> [4] </ref>, array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants [5]. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers [6, 7, 8]. The paper is organized as follows. <p> Top (&gt;) is the initial state for all symbols. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard for many constant propagation methods <ref> [4, 9, 5] </ref>, originally introduced by Kildall [10]. Each symbol has its lattice value initialized to &gt;, which indicates that it has an as yet undetermined value. <p> By initializing lattice values to &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem [11], in which iterations continue until a fixed point is reached <ref> [4, 5] </ref>. <p> P ) then S3: y = 5 S4: else S5: y = z + 2 S6: endif (a) S8: if ( z &lt; 5 ) then S9: y = 5 S10: else S11: y = 2 S12: endif (b) 2.2 Previous Methods 2.2.1 Classification As explained by Wegman and Zadeck <ref> [4] </ref>, constant propagation algorithms can be classified in two ways: (i) using the entire graph or a sparse graph representation, and (ii) detecting simple or conditional constants. This naturally creates four classes of algorithms. <p> The distinction between the four types of algorithms is explained well by Wegman and 5 Zadeck <ref> [4] </ref>, and the reader is referred to their paper for more detail. We will look at the algo-rithm that they present, since it incorporates both sparse graph representation and conditional code. <p> Expressions are evaluated the first time a node is the destination of a flow edge, and also when the expression is the target of an SSA edge and at least one incoming flow edge is executable. More detail can be found in the original paper <ref> [4] </ref>. This algorithm finds all simple constants, plus additional constants that can be discovered when the predicate controlling a switch node is determined to be constant. The time complexity is proportional to the size of the SSA graph, and each SSA edge can be processed at most twice. <p> Practically, however, this is undesirable (managing the symbol table explosion alone precludes this option), so the SSA properties are maintained by providing links between each use and its one reaching definition. Instead of providing def-use links, as is the common implementation <ref> [4, 13] </ref>, we provide use-def links, giving rise to an SSA graph comprising factored use-def chains (FUD chains).
Reference: [5] <author> Dan Grove and Linda Torczon. </author> <title> Interprocedural constant propagation: A study of jump function implementations. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 90-99, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: These include dead code elimination [4], array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants <ref> [5] </ref>. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers [6, 7, 8]. The paper is organized as follows. In Section 2 we examine the standard framework employed to perform constant propagation and relate it to previous methods and algorithms. <p> Top (&gt;) is the initial state for all symbols. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard for many constant propagation methods <ref> [4, 9, 5] </ref>, originally introduced by Kildall [10]. Each symbol has its lattice value initialized to &gt;, which indicates that it has an as yet undetermined value. <p> By initializing lattice values to &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem [11], in which iterations continue until a fixed point is reached <ref> [4, 5] </ref>. <p> One important topic is interprocedural analysis and procedure integration, an area where we believe many constants will be found. Although some work has already been done in this area <ref> [5, 24, 25] </ref>, we would like to apply our demand-driven style to the problem. Dead-code can currently be identified with our technique, but we have not yet developed the algorithm fully.
Reference: [6] <author> Steve S. Muchnick. </author> <title> Optimizing compilers for SPARC. </title> <booktitle> Sun Technology, </booktitle> <pages> pages 161-173, </pages> <year> 1988. </year> <month> Summer. </month>
Reference-contexts: These include dead code elimination [4], array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants [5]. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers <ref> [6, 7, 8] </ref>. The paper is organized as follows. In Section 2 we examine the standard framework employed to perform constant propagation and relate it to previous methods and algorithms.
Reference: [7] <author> D. Blickstein, P. Craig, C. Davidson, R. Faiman, K. Glossop, R. Grove, S. Hobbs, and W. Noyce. </author> <title> The GEM optimizing compiler system. </title> <journal> Digital Technical Journal, </journal> <volume> 4 </volume> <pages> 121-136, </pages> <year> 1992. </year> <note> Special Issue. </note>
Reference-contexts: These include dead code elimination [4], array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants [5]. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers <ref> [6, 7, 8] </ref>. The paper is organized as follows. In Section 2 we examine the standard framework employed to perform constant propagation and relate it to previous methods and algorithms.
Reference: [8] <author> P. Lowney, SA. Freudenberger, T. Karzes, W. Lichtenstein, R. Nix, J. O'Donnell, and J. Ruttenberg. </author> <title> The Multiflow trace scheduling compiler. </title> <journal> The Journal of Supercomputing, </journal> <volume> 7 </volume> <pages> 51-142, </pages> <year> 1993. </year>
Reference-contexts: These include dead code elimination [4], array- and loop-bound propagation, and procedure integration and inlining, which we believe to be a major source of detectable constants [5]. Due to these benefits, constant propagation is an integral component of modern optimizing commercial compilers <ref> [6, 7, 8] </ref>. The paper is organized as follows. In Section 2 we examine the standard framework employed to perform constant propagation and relate it to previous methods and algorithms. <p> A sophisticated compiler may analyze the guard and determine that under the range of the true side of the conditional, x 1 will always be 1. This notion of a derived assertion is not new <ref> [8] </ref>, but to our knowledge has not yet been integrated into the SSA form. Using demand-driven SSA form, derived assertions can easily be captured by inserting dummy assignments. We propose a new SSA operator, the -function, which serves as the new definition of its variable.
Reference: [9] <author> David Callahan, Keith D. Cooper, Ken Kennedy, and Linda Torczon. </author> <title> Interprocedural constant propagation. </title> <booktitle> In Proceedings of Sigplan Symposium on Compiler Construction, </booktitle> <volume> volume 21, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: Top (&gt;) is the initial state for all symbols. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard for many constant propagation methods <ref> [4, 9, 5] </ref>, originally introduced by Kildall [10]. Each symbol has its lattice value initialized to &gt;, which indicates that it has an as yet undetermined value.
Reference: [10] <author> G. A. Kildall. </author> <title> A unified approach to global program optimization. </title> <booktitle> In Conference Record of the First ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 194-206, </pages> <month> October </month> <year> 1973. </year>
Reference-contexts: Top (&gt;) is the initial state for all symbols. When comparing two lattice element values, the meet operator (u) is applied, as given in Table 1. These foundations are standard for many constant propagation methods [4, 9, 5], originally introduced by Kildall <ref> [10] </ref>. Each symbol has its lattice value initialized to &gt;, which indicates that it has an as yet undetermined value. After analysis is complete, all symbols will have lattice value equal to ? (it cannot be determined to be constant), a constant value, or &gt; (unexecutable code). <p> The order in which basic blocks are visited is not important. * This is an optimistic solver, since all symbols are initialized to &gt;. We find the same class of simple constants as other non-conditional solvers, such as Kildall <ref> [10] </ref> and Reif and 9 8 t 2 tuples, lattice ( t ) = &gt; unvisited ( t ) = true Visit all basic blocks B in the program Visit all tuples t within B if unvisited ( t ) then propagate ( t ) propagate ( tuple t ) unvisited
Reference: [11] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1986. </year>
Reference-contexts: By initializing lattice values to &gt;, an optimistic approach is taken, which assumes all symbols can be determined to be constant until proven otherwise. Previous methods perform the analysis as an iterative data-flow problem <ref> [11] </ref>, in which iterations continue until a fixed point is reached [4, 5]. <p> This naturally creates four classes of algorithms. It is clear that propagating information about each symbol to every node in a graph is inefficient, since not all nodes contain references or definitions of the symbol under consideration. Sparse representations, on the other hand, such as def-use or use-def chains <ref> [11] </ref>, Static Single Assignment (SSA) [12], Dependence Flow Graphs (DFG) [13], or Program Dependence Graphs (PDG) [14], have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 2. <p> An irreducible graph contains loops with multiple entries this leads to problems both in loop detection (we classify loops according to the natural loop <ref> [11] </ref> definition), and working with control dependence (in an irreducible graph, the transitive control dependence of a node can skip over the immediate dominator). <p> If constant, we follow the indicated branch, propagating constant values as found. If not constant, we take the meet of its arguments. The revised algorithm is given in Figure 6. We may encounter -functions in a program with irreducible loops <ref> [11] </ref>. In this case, - functions cannot be converted to GSA form, but we can still detect simple constants.
Reference: [12] <author> Ron Cytron, Jeanne Ferrante, Barry K. Rosen, Mark N. Wegman, and F. Kenneth Zadeck. </author> <title> Efficiently computing Static Single Assignment form and the control dependence graph. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 13(4) </volume> <pages> 451-490, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: It is clear that propagating information about each symbol to every node in a graph is inefficient, since not all nodes contain references or definitions of the symbol under consideration. Sparse representations, on the other hand, such as def-use or use-def chains [11], Static Single Assignment (SSA) <ref> [12] </ref>, Dependence Flow Graphs (DFG) [13], or Program Dependence Graphs (PDG) [14], have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 2. <p> The -function is itself considered a new definition of the variable. For details on SSA graph construction the reader is referred to the paper by Cytron et al. <ref> [12] </ref>. A sample program converted into SSA form is shown in Figure 3. 2.2.3 A Closer Look at One Algorithm The algorithm used by Wegman and Zadeck operates on CFG edges. SSA def-use edges are added to the graph once the program has been transformed into SSA form.
Reference: [13] <author> Richard Johnson and Keshav Pingali. </author> <title> Dependence-based program analysis. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 78-89, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: Sparse representations, on the other hand, such as def-use or use-def chains [11], Static Single Assignment (SSA) [12], Dependence Flow Graphs (DFG) <ref> [13] </ref>, or Program Dependence Graphs (PDG) [14], have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 2. <p> Practically, however, this is undesirable (managing the symbol table explosion alone precludes this option), so the SSA properties are maintained by providing links between each use and its one reaching definition. Instead of providing def-use links, as is the common implementation <ref> [4, 13] </ref>, we provide use-def links, giving rise to an SSA graph comprising factored use-def chains (FUD chains). <p> It may well be that dead code is best identified using edges instead of nodes, as pointed out by Wegman and Zadeck. Traditional SSA form has been criticized for lacking a method to propagate constants de termined by predicate analysis <ref> [13] </ref>.
Reference: [14] <author> Jeanne Ferrante, Karl J. Ottenstein, and Joe D. Warren. </author> <title> The program dependence graph and its use in optimization. </title> <journal> ACM Trans. on Programming Languages and Systems, </journal> <volume> 9(3) </volume> <pages> 319-349, </pages> <month> July </month> <year> 1987. </year> <month> 21 </month>
Reference-contexts: Sparse representations, on the other hand, such as def-use or use-def chains [11], Static Single Assignment (SSA) [12], Dependence Flow Graphs (DFG) [13], or Program Dependence Graphs (PDG) <ref> [14] </ref>, have all shown the virtue of operating on a sparse graph for analysis. The distinction between simple (all paths) constants and conditional constants can be seen in Figure 2.
Reference: [15] <author> Michael Wolfe, Michael P. Gerlek, and Eric Stoltz. Nascent: </author> <title> A Next-Generation, High Performance Compiler. </title> <institution> Oregon Graduate Institute of Science & Technology unpublished, </institution> <year> 1993. </year>
Reference-contexts: Eventually, x 1 will evaluate to ?, as the merge for z becomes non-constant. It is this multiple expression evaluation which we seek to avoid. 3 SSA using FUD Chains for Simple Constants 3.1 FUD Chains In our restructuring compiler, Nascent <ref> [15] </ref>, we also convert the intermediate representation into SSA form. In order to achieve the single-assignment property each new definition of a variable receives a new name.
Reference: [16] <author> Eric Stoltz, Michael P. Gerlek, and Michael Wolfe. </author> <title> Extended SSA with factored use-def chains to support optimization and parallelism. </title> <booktitle> In 1994 ACM Conf. Proceedings Hawaii International Conference on System Sciences, </booktitle> <month> January </month> <year> 1994. </year> <note> to appear. </note>
Reference-contexts: Instead of providing def-use links, as is the common implementation [4, 13], we provide use-def links, giving rise to an SSA graph comprising factored use-def chains (FUD chains). This approach yields several advantages, such as constant space per node and an ideal form with which to perform demand-driven analysis <ref> [16] </ref>. 8 Our analysis of programs begins within a framework consisting of the CFG and an SSA data--flow graph. Each basic block contains a list of intermediate code tuples, which themselves are linked together as part of the data-flow graph.
Reference: [17] <author> John H. Reif and Harry R. Lewis. </author> <title> Symbolic evaluation and the global value graph. </title> <booktitle> In Conference Record of the Fourth ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 104-118, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: t ) = ? endif store: lattice ( t ) = lattice ( RHS ) -function: if loop-header then lattice ( t ) = ? else lattice ( t ) = u of -arguments of t endif default: lattice ( t ) = ? end case end propagate 10 Lewis <ref> [17] </ref>. * When at a merge node, we take the meet of the demanded classification of the - arguments.
Reference: [18] <author> Robert A. Ballance, Arthur B. Maccabe, and Karl J. Ottenstein. </author> <title> The program dependence web: A representation supporting control-, data-, and demand-driven interpretation of imperative languages. </title> <booktitle> In Proc. ACM SIGPLAN '90 Conf. on Programming Language Design and Implementation, </booktitle> <pages> pages 257-271, </pages> <address> White Plains, NY, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Examine Figure 3 (b). When attempting to classify x 1 , the value is demanded from the use-def SSA link of y 2 , which points to the - function. However, a -function is not interpretable <ref> [18] </ref>. Thus, we have no information about which path may or may not be taken. Since the predicate P in our example determines the path taken, if P is constant, we can determine which argument of the -function to evaluate. <p> If P is not constant, the best we can do is to take the meet of the -arguments. Augmentation of the -function is needed to include this additional information. We extend the SSA form to a gated single assignment form (GSA), introduced by Ballance et al. <ref> [18] </ref>, which allows us to evaluate conditionals based upon their predicates. Figure 3 shows a simple program converted to GSA form. Briefly, -functions are reclassified into and fl-functions. Most -functions contained within loop-header nodes are renamed -functions, while most other - functions are converted to fl-functions fl .
Reference: [19] <author> Paul Havlak. </author> <title> Construction of thinned gated single-assignment form. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Several important notes are necessary: * We provide the complete algorithm to convert -functions to fl- and -functions in Appendix A. A similar method employed by Havlak <ref> [19] </ref>, aimed at value-numbering, thins the fl-function to eliminate paths that cannot reach a merge point. Essentially, if all arguments save one are &gt;, then the entire argument structure is reduced to the one non-&gt; argument. <p> These are interesting tradeoffs, and remain an open question. Although not constant propagation per se, the structure of GSA lends itself particularly well to implementing value numbering, as has been shown by Havlak <ref> [19] </ref>. Finally, we want to extend our work into the area of non-integer and symbolic expression propagation.
Reference: [20] <author> Michael Wolfe. </author> <title> Beyond induction variables. </title> <booktitle> In Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 162-174, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The variables defined within these cycles are detected with induction variable analysis. Induction variables are traditionally detected as a precursor to strength reduction, and more recently for dependence analysis with regard to subscript expressions. We have developed methods for detecting and classifying induction variables (including non-linear induction variables <ref> [20, 21, 22] </ref>) based on strongly-connected regions in the SSA data-flow graph [2]. These techniques make use of an exit function, the -function, which holds the exit value of a variable assigned within the loop.
Reference: [21] <author> Mohammed R. Haghighat and Constantine D. Polychronopoulos. </author> <title> Symbolic program analysis and optimization for parallelizing compilers. </title> <booktitle> In Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 355-369, </pages> <year> 1992. </year>
Reference-contexts: The variables defined within these cycles are detected with induction variable analysis. Induction variables are traditionally detected as a precursor to strength reduction, and more recently for dependence analysis with regard to subscript expressions. We have developed methods for detecting and classifying induction variables (including non-linear induction variables <ref> [20, 21, 22] </ref>) based on strongly-connected regions in the SSA data-flow graph [2]. These techniques make use of an exit function, the -function, which holds the exit value of a variable assigned within the loop.
Reference: [22] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic paral-lelization of four Perfect-Benchmark programs. </title> <editor> In U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 65-83. </pages> <publisher> Spinger-Verlag, </publisher> <year> 1992. </year> <note> LNCS no. 589. </note>
Reference-contexts: The variables defined within these cycles are detected with induction variable analysis. Induction variables are traditionally detected as a precursor to strength reduction, and more recently for dependence analysis with regard to subscript expressions. We have developed methods for detecting and classifying induction variables (including non-linear induction variables <ref> [20, 21, 22] </ref>) based on strongly-connected regions in the SSA data-flow graph [2]. These techniques make use of an exit function, the -function, which holds the exit value of a variable assigned within the loop.
Reference: [23] <author> Michael P. Gerlek. </author> <title> Detecting induction variables using SSA form. </title> <type> Technical Report 93-014, </type> <institution> Oregon Graduate Institute of Science & Technology, </institution> <year> 1993. </year>
Reference-contexts: We are able to propagate constants through loops (single and nested) by taking advantage of specialized solvers which detect and classify a large assortment of linear and non-linear induction variables. Interested readers may obtain a description of this work via anonymous ftp to cse.ogi.edu <ref> [23] </ref>. 5 Experimental Results To gauge the effectiveness of our routines, we measured the number of constants (both simple and conditional) on Fortran scientific codes found in the PERFECT, RICEPS, and MENDEZ benchmark suites, and several miscellaneous but important routines.
Reference: [24] <author> Mary Hall. </author> <title> Managing Interprocedural Optimization. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Rice University, </institution> <year> 1991. </year>
Reference-contexts: One important topic is interprocedural analysis and procedure integration, an area where we believe many constants will be found. Although some work has already been done in this area <ref> [5, 24, 25] </ref>, we would like to apply our demand-driven style to the problem. Dead-code can currently be identified with our technique, but we have not yet developed the algorithm fully.
Reference: [25] <author> R. Metzger and S. Stroud. </author> <title> Interprocedural constant propagation: an empirical study. </title> <journal> ACM Letters on Programming Languages and Systems, </journal> <month> June </month> <year> 1992. </year> <month> 22 </month>
Reference-contexts: One important topic is interprocedural analysis and procedure integration, an area where we believe many constants will be found. Although some work has already been done in this area <ref> [5, 24, 25] </ref>, we would like to apply our demand-driven style to the problem. Dead-code can currently be identified with our technique, but we have not yet developed the algorithm fully.
References-found: 25

