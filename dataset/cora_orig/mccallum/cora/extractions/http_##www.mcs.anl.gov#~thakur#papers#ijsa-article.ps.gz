URL: http://www.mcs.anl.gov/~thakur/papers/ijsa-article.ps.gz
Refering-URL: http://www.mcs.anl.gov/~thakur/papers.html
Root-URL: http://www.mcs.anl.gov
Email: @mcs.anl.gov  
Title: I/O in Parallel Applications: The Weakest Link  
Author: Rajeev Thakur Ewing Lusk William Gropp fthakur, lusk, groppg 
Note: Appeared (with minor editorial changes) in The International Journal of High Performance Computing Applications, (12)4:389-395, Winter 1998. c 1998 Sage Publications, Inc.  
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Abstract: Parallel computers are increasingly being used to run large-scale applications that also have huge I/O requirements. However, many applications obtain poor I/O performance on modern parallel machines. This special issue of IJSA contains papers that describe the I/O requirements and the techniques used to perform I/O in real parallel applications. We first explain how the I/O application program interface (API) plays a critical role in enabling such applications to achieve high I/O performance. We describe how the commonly used Unix I/O interface is inappropriate for parallel I/O and how an explicitly parallel API with support for collective I/O can help the underlying I/O hardware and software perform I/O efficiently. We then describe MPI-IO, a recently defined, standard, portable API specifically designed for high-performance parallel I/O. We conclude with an overview of the papers in this special issue. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Applications Working Group of the Scalable I/O Initiative. </institution> <note> Preliminary Survey of I/O Intensive Applications. Scalable I/O Initiative Working Paper Number 1. On the World-Wide Web at http://www.cacr.caltech.edu/SIO/SIO apps.ps, </note> <year> 1994. </year>
Reference-contexts: Most of these applications also need to perform I/O for a number of reasons, such as reading initial data, writing the results, checkpointing, out-of-core data sets, scratch files for temporary storage, and visualization. (See <ref> [14, 1, 6] </ref> for a list of many such applications.) Since I/O is slow, the I/O speed, and not the CPU or communication speed, is often the bottleneck for such applications.
Reference: [2] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: However, as we explain below, using this interface often turns out to be very inefficient. The main reason is that the access patterns in parallel programs are quite different from those in uniprocess programs <ref> [21, 4, 2, 26] </ref>. In parallel programs, each process may need to access a noncontiguous data set. In many cases, the accesses of different processes may be interleaved in the file, and together they may span large, contiguous portions of the file.
Reference: [3] <author> P. Corbett et al. </author> <title> Proposal for a Common Parallel File System Programming Interface, </title> <note> Version 0.60. On the World-Wide Web at http://www.cs.princeton.edu/sio, June 1996. </note>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [4] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Although parallel computers with peak performance of 1 Tflops/sec or more are available, real applications running on parallel machines usually achieve I/O bandwidths of at most a few hundred Mbytes/sec. In fact, many applications achieve less than 10 Mbytes/sec <ref> [4] </ref>. As parallel computers get faster, scientists are increasingly using parallel computers to solve problems that require a large amount of computing power. <p> However, as we explain below, using this interface often turns out to be very inefficient. The main reason is that the access patterns in parallel programs are quite different from those in uniprocess programs <ref> [21, 4, 2, 26] </ref>. In parallel programs, each process may need to access a noncontiguous data set. In many cases, the accesses of different processes may be interleaved in the file, and together they may span large, contiguous portions of the file.
Reference: [5] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, </note> <month> 21(5) </month> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: If the API is able to specify the noncontiguous accesses of each process as well as the group of processes making such requests, the I/O system can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. <p> can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O [28, 11, 20, 17, 3, 10, 25, 9, 19].
Reference: [6] <author> J. del Rosario and A. Choudhary. </author> <title> High Performance I/O for Parallel Computers: Problems and Prospects. </title> <journal> Computer, </journal> <volume> 27(3) </volume> <pages> 59-68, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: Most of these applications also need to perform I/O for a number of reasons, such as reading initial data, writing the results, checkpointing, out-of-core data sets, scratch files for temporary storage, and visualization. (See <ref> [14, 1, 6] </ref> for a list of many such applications.) Since I/O is slow, the I/O speed, and not the CPU or communication speed, is often the bottleneck for such applications.
Reference: [7] <author> D. Feitelson, P. Corbett, S. Baylor, and Y. Hsu. </author> <title> Parallel I/O Subsystems in Massively Parallel Supercomputers. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 3(3) </volume> <pages> 33-47, </pages> <month> Fall </month> <year> 1995. </year>
Reference-contexts: The operating system schedules the file-system server on the compute nodes. However, these machines do have multiple disks and a file system that stripes files across the disks. For a good discussion of issues related to parallel I/O systems, see <ref> [7, 15] </ref>. 3 Application Program Interface The application program interface (API) plays a critical role in enabling the user to express I/O operations conveniently and also in conveying sufficient information about user-level access patterns to the I/O system so that the system can perform I/O efficiently.
Reference: [8] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIO|A Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year> <month> 8 </month>
Reference-contexts: If accesses overlap, the atomic mode should be used. Porting applications from Unix I/O to MPI-IO is easy, because MPI-IO provides functions that are equivalent to those in Unix I/O. For better performance, however, the special features of MPI-IO must be used. Many implementations of MPI-IO are in progress <ref> [30, 8, 13, 23, 22] </ref>, and most vendors of parallel machines plan to provide MPI-IO as part of their MPI-2 product. We ourselves are developing a portable MPI-IO implementation called ROMIO. ROMIO 1.0.0 is freely available from http://www.mcs.anl.gov/home/thakur/romio and works on most parallel computers and networks of workstations.
Reference: [9] <author> N. Galbreath, W. Gropp, and D. Levine. </author> <title> Applications-Driven Parallel I/O. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 462-471. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [10] <author> G. Gibson, D. Stodolsky, P. Chang, W. Courtwright II, C. Demetriou, E. Ginting, M. Hol-land, Q. Ma, L. Neal, R. Patterson, J. Su, R. Youssef, and J. Zelenka. </author> <title> The Scotch Parallel Storage Systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [11] <author> J. Huber, C. Elford, D. Reed, A. Chien, and D. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [12] <author> IEEE/ANSI Std. 1003.1. </author> <title> Portable Operating System Interface (POSIX)-Part 1: System Application Program Interface (API) [C Language], </title> <note> 1996 edition. </note>
Reference-contexts: However, it was designed mainly for uniprocessor file systems and for access patterns commonly found in uniprocess programs. Accordingly, it allows the user to access only a single chunk of data at a time. It has no notion of collective I/O requests from multiple processes. (POSIX <ref> [12] </ref> does define a function called lio listio that accepts a list of requests, but it is not supported on all machines, and it is not collective.) One can use the Unix I/O interface in parallel programs; each process can make Unix I/O calls on its own, independent of other processes.
Reference: [13] <author> T. Jones, R. Mark, J. Martin, J. May, E. Pierce, and L. Stanberry. </author> <title> An MPI-IO interface to HPSS. </title> <booktitle> In Proceedings of the Fifth NASA Goddard conference on Mass Storage Systems, </booktitle> <pages> pages I:37-50, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: If accesses overlap, the atomic mode should be used. Porting applications from Unix I/O to MPI-IO is easy, because MPI-IO provides functions that are equivalent to those in Unix I/O. For better performance, however, the special features of MPI-IO must be used. Many implementations of MPI-IO are in progress <ref> [30, 8, 13, 23, 22] </ref>, and most vendors of parallel machines plan to provide MPI-IO as part of their MPI-2 product. We ourselves are developing a portable MPI-IO implementation called ROMIO. ROMIO 1.0.0 is freely available from http://www.mcs.anl.gov/home/thakur/romio and works on most parallel computers and networks of workstations.
Reference: [14] <author> D. Kotz. </author> <title> Applications of Parallel I/O. </title> <type> Technical Report PCS-TR96-297, </type> <institution> Dept. of Computer Science, Dartmouth College, </institution> <month> October </month> <year> 1996. </year> <note> Release 1. Available on the World-Wide Web at http://www.cs.dartmouth.edu/reports/abstracts/TR96-297. </note>
Reference-contexts: Most of these applications also need to perform I/O for a number of reasons, such as reading initial data, writing the results, checkpointing, out-of-core data sets, scratch files for temporary storage, and visualization. (See <ref> [14, 1, 6] </ref> for a list of many such applications.) Since I/O is slow, the I/O speed, and not the CPU or communication speed, is often the bottleneck for such applications.
Reference: [15] <author> D. Kotz. </author> <title> Introduction to Multiprocessor I/O Architecture. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 4, </booktitle> <pages> pages 97-123. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The operating system schedules the file-system server on the compute nodes. However, these machines do have multiple disks and a file system that stripes files across the disks. For a good discussion of issues related to parallel I/O systems, see <ref> [7, 15] </ref>. 3 Application Program Interface The application program interface (API) plays a critical role in enabling the user to express I/O operations conveniently and also in conveying sufficient information about user-level access patterns to the I/O system so that the system can perform I/O efficiently.
Reference: [16] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year>
Reference-contexts: If the API is able to specify the noncontiguous accesses of each process as well as the group of processes making such requests, the I/O system can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. <p> can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O [28, 11, 20, 17, 3, 10, 25, 9, 19].
Reference: [17] <author> O. Krieger and M. Stumm. </author> <title> HFS: A Performance-Oriented Flexible File System Based on Building-Block Compositions. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [18] <author> Message-Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: Clearly, a single, standard, portable API designed specifically for parallel I/O is needed, together with high-performance implementations of it on all machines. Fortunately, there is now such an API, namely MPI-IO, the I/O chapter in MPI-2 <ref> [18] </ref>. MPI-IO has been designed based on experience with various existing APIs as well as knowledge of the I/O access patterns in parallel applications. MPI-IO can be considered as Unix I/O plus many features specifically intended for portable, high-performance parallel I/O.
Reference: [19] <author> J. Nieplocha and I. Foster. </author> <title> Disk Resident Arrays: An Array-Oriented I/O Library for Out-of-Core Computations. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 196-204. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year> <month> 9 </month>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [20] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley Parallel File System. </title> <booktitle> Parallel Computing, </booktitle> <address> 23(4):447--476, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [21] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: However, as we explain below, using this interface often turns out to be very inefficient. The main reason is that the access patterns in parallel programs are quite different from those in uniprocess programs <ref> [21, 4, 2, 26] </ref>. In parallel programs, each process may need to access a noncontiguous data set. In many cases, the accesses of different processes may be interleaved in the file, and together they may span large, contiguous portions of the file.
Reference: [22] <author> J. Prost. MPI-IO/PIOFS. </author> <note> World-Wide Web page at http://www.research.ibm.com/people/p/prost/sections/mpiio.html, 1996. </note>
Reference-contexts: If accesses overlap, the atomic mode should be used. Porting applications from Unix I/O to MPI-IO is easy, because MPI-IO provides functions that are equivalent to those in Unix I/O. For better performance, however, the special features of MPI-IO must be used. Many implementations of MPI-IO are in progress <ref> [30, 8, 13, 23, 22] </ref>, and most vendors of parallel machines plan to provide MPI-IO as part of their MPI-2 product. We ourselves are developing a portable MPI-IO implementation called ROMIO. ROMIO 1.0.0 is freely available from http://www.mcs.anl.gov/home/thakur/romio and works on most parallel computers and networks of workstations.
Reference: [23] <author> Darren Sanders, Yoonho Park, and Maciej Brodowicz. </author> <title> Implementation and Performance of MPI-IO File Access Using MPI Datatypes. </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: If accesses overlap, the atomic mode should be used. Porting applications from Unix I/O to MPI-IO is easy, because MPI-IO provides functions that are equivalent to those in Unix I/O. For better performance, however, the special features of MPI-IO must be used. Many implementations of MPI-IO are in progress <ref> [30, 8, 13, 23, 22] </ref>, and most vendors of parallel machines plan to provide MPI-IO as part of their MPI-2 product. We ourselves are developing a portable MPI-IO implementation called ROMIO. ROMIO 1.0.0 is freely available from http://www.mcs.anl.gov/home/thakur/romio and works on most parallel computers and networks of workstations.
Reference: [24] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: If the API is able to specify the noncontiguous accesses of each process as well as the group of processes making such requests, the I/O system can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. <p> can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O [28, 11, 20, 17, 3, 10, 25, 9, 19].
Reference: [25] <author> K. Seamons and M. Winslett. </author> <title> Multidimensional Array I/O in Panda 1.0. </title> <journal> The Journal of Supercomputing, </journal> <volume> 10(2) </volume> <pages> 191-211, </pages> <year> 1996. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [26] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: However, as we explain below, using this interface often turns out to be very inefficient. The main reason is that the access patterns in parallel programs are quite different from those in uniprocess programs <ref> [21, 4, 2, 26] </ref>. In parallel programs, each process may need to access a noncontiguous data set. In many cases, the accesses of different processes may be interleaved in the file, and together they may span large, contiguous portions of the file.
Reference: [27] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <journal> Scientific Programming, </journal> <volume> 5(4) </volume> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: If the API is able to specify the noncontiguous accesses of each process as well as the group of processes making such requests, the I/O system can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. <p> can access data efficiently by using a technique called collective I/O <ref> [5, 27, 16, 24] </ref>. A collective I/O implementation 3 tries to combine the noncontiguous requests of multiple processes into larger contiguous requests and access data in large chunks. Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O [28, 11, 20, 17, 3, 10, 25, 9, 19].
Reference: [28] <author> R. Thakur, A. Choudhary, R. Bordawekar, S. More, and S. Kuditipudi. </author> <title> Passion: Optimized I/O for Parallel Applications. </title> <journal> Computer, </journal> <volume> 29(6) </volume> <pages> 70-78, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Numerous studies have shown that collective I/O can improve performance significantly [5, 27, 16, 24]. However, collective I/O cannot be done with the Unix API. Over the past few years, many research parallel file systems and I/O libraries have been developed that perform various optimizations, including collective I/O <ref> [28, 11, 20, 17, 3, 10, 25, 9, 19] </ref>. Each of these, however, has a different API with varying degrees of portability and generality. The only standard, portable API that has been available on all machines is the Unix API.
Reference: [29] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <booktitle> In Proceedings of the 3rd International Conference of the Austrian Center for Parallel Computation (ACPC) with Special Emphasis on Parallel Databases and Parallel I/O, </booktitle> <pages> pages 24-35. </pages> <booktitle> Lecture Notes in Computer Science 1127. </booktitle> <publisher> Springer-Verlag., </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: We have seen impressive performance results with ROMIO's collective I/O implementation. For example, Figure 2 shows the I/O bandwidths obtained on the Intel Paragon at Caltech for an astrophysics application template, which has an access pattern similar to that in Figure 1. (This application is described in detail in <ref> [29] </ref>.) A three-dimensional array is block-distributed in 4 64 128 256 Processors 0.0 40.0 80.0 120.0 160.0 200.0 240.0 280.0 Mbytes/sec independent write collective write independent read collective read astrophysics application template all three dimensions and must be accessed (read/written) from a file containing the global array in column-major order.
Reference: [30] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> Users Guide for ROMIO: A High-Performance, Portable MPI-IO Implementation. </title> <type> Technical Report ANL/MCS-TM-234, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> October </month> <year> 1997. </year> <month> 10 </month>
Reference-contexts: If accesses overlap, the atomic mode should be used. Porting applications from Unix I/O to MPI-IO is easy, because MPI-IO provides functions that are equivalent to those in Unix I/O. For better performance, however, the special features of MPI-IO must be used. Many implementations of MPI-IO are in progress <ref> [30, 8, 13, 23, 22] </ref>, and most vendors of parallel machines plan to provide MPI-IO as part of their MPI-2 product. We ourselves are developing a portable MPI-IO implementation called ROMIO. ROMIO 1.0.0 is freely available from http://www.mcs.anl.gov/home/thakur/romio and works on most parallel computers and networks of workstations.
References-found: 30

