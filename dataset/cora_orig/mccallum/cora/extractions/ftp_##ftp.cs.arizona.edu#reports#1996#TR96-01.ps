URL: ftp://ftp.cs.arizona.edu/reports/1996/TR96-01.ps
Refering-URL: http://www.cs.arizona.edu/research/reports.html
Root-URL: http://www.cs.arizona.edu
Title: Efficient Support for Fine-Grain Parallelism on Shared-Memory Machines  
Author: Vincent W. Freeh David K. Lowenthal Gregory R. Andrews 
Abstract-found: 0
Intro-found: 1
Reference: [ABLL92] <author> T. E. Anderson, B. N. Bershad, E. D. Lazowska, and H. M. Levy. </author> <title> Scheduler activations: Effective kernel support for the user-level management of parallelism. </title> <journal> acmtocs, </journal> <volume> 10(1) </volume> <pages> 53-79, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: Several researchers have proposed ways to make standard thread packages more efficient. An-derson et al. [ALL89] discuss the gain from using local ready queues, and <ref> [ABLL92] </ref> shows how to do user-level scheduling. Schoenberg and Hummel [HS91] explain how to avoid allocating a stack per thread and switching contexts in nested parallel for loops. Markatos et al. [MB92] present a thorough study of the tradeoffs between load balancing and locality in shared memory machines.
Reference: [ALL89] <author> T.E. Anderson, E.D. Lazowska, and H.M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(12) </volume> <pages> 1631-1644, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: Several researchers have proposed ways to make standard thread packages more efficient. An-derson et al. <ref> [ALL89] </ref> discuss the gain from using local ready queues, and [ABLL92] shows how to do user-level scheduling. Schoenberg and Hummel [HS91] explain how to avoid allocating a stack per thread and switching contexts in nested parallel for loops.
Reference: [And91] <author> Gregory R. Andrews. </author> <title> Concurrent Programming: </title> <booktitle> Principles and Practice. </booktitle> <address> Benjamin/Cummings, Redwood City, California, </address> <year> 1991. </year>
Reference-contexts: In order to stress the load-balancing mechanism, we used the function f (x) = e x sin x. This function has more work at the right part of the interval because the oscillations are much sharper there. The coarse-grain adaptive quadrature program uses the dynamic "bag-of-tasks" paradigm <ref> [CGL86, And91] </ref> (which is not trivial to implement). In particular, there is one central bag of tasks, and each task in the bag specifies one subinterval. Initially, the bag contains the entire interval over which to integrate.
Reference: [BDSY92] <author> Peter A. Buhr, Glen Ditchfield, R.A. Stroobosscher, and B.M. Younger. </author> <title> uC++: concurrency in the object oriented language C++. </title> <journal> Software|Practice and Experience, </journal> <volume> 22(2) </volume> <pages> 137-172, </pages> <month> February </month> <year> 1992. </year>
Reference-contexts: The initial attempts to support efficient parallelism were lightweight thread packages such as Threads [Doe87], Presto [BLL88], System [BS90], C++ <ref> [BDSY92] </ref>, and Sun Lightweight Processes [SS92]. We will call these standard packages because they have a stack for each thread. The goal of standard packages is to provide the user with a natural thread abstraction and many of the usual concurrent programming primitives; different packages provide different primitives.
Reference: [BLL88] <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: a system for object-oriented parallel programming. </title> <journal> Software|Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The fine-grain quicksort program scales much better than the coarse-grain program; we believe this is because of contention for the bag. 5 Related Work There are many existing threads packages. The initial attempts to support efficient parallelism were lightweight thread packages such as Threads [Doe87], Presto <ref> [BLL88] </ref>, System [BS90], C++ [BDSY92], and Sun Lightweight Processes [SS92]. We will call these standard packages because they have a stack for each thread.
Reference: [BS90] <author> Peter A. Buhr and R.A. Stroobosscher. </author> <title> The uSystem: providing light-weight concurrency on shared memory multiprocessor computers running UNIX. </title> <journal> Software Practice and Experience, </journal> <pages> pages 929-964, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: The fine-grain quicksort program scales much better than the coarse-grain program; we believe this is because of contention for the bag. 5 Related Work There are many existing threads packages. The initial attempts to support efficient parallelism were lightweight thread packages such as Threads [Doe87], Presto [BLL88], System <ref> [BS90] </ref>, C++ [BDSY92], and Sun Lightweight Processes [SS92]. We will call these standard packages because they have a stack for each thread. The goal of standard packages is to provide the user with a natural thread abstraction and many of the usual concurrent programming primitives; different packages provide different primitives.
Reference: [CGL86] <author> Nicholas Carriero, David Gelernter, and Jerry Leichter. </author> <title> Distributed data structures in Linda. </title> <booktitle> In Thirteenth ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 236-242, </pages> <month> January </month> <year> 1986. </year>
Reference-contexts: In order to stress the load-balancing mechanism, we used the function f (x) = e x sin x. This function has more work at the right part of the interval because the oscillations are much sharper there. The coarse-grain adaptive quadrature program uses the dynamic "bag-of-tasks" paradigm <ref> [CGL86, And91] </ref> (which is not trivial to implement). In particular, there is one central bag of tasks, and each task in the bag specifies one subinterval. Initially, the bag contains the entire interval over which to integrate.
Reference: [CGSv93] <author> David E. Culler, Seth Copen Goldstein, Klaus Erik Schauser, and Thorsten von Eicken. </author> <title> TAM|a compiler controlled threaded abstract machine. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 18(3) </volume> <pages> 347-370, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: WorkCrews [VR88] supports fork/join parallelism on small-scale, shared-memory multiprocessors. WorkCrews introduced the concepts of pruning and of ordering queues to favor larger threads. Filaments has borrowed these ideas in its implementation of fork/join threads. TAM <ref> [CGSv93] </ref> is a compiler-controlled threaded abstract machine. It evolved from graph-based execution models for dataflow languages and provides a bridge between such models and the control flow models typically employed by standard multiprocessors.
Reference: [Dew85] <author> A. K. Dewdney. </author> <title> Computer recreations. </title> <publisher> Scientific American, </publisher> <pages> pages 16-24, </pages> <month> August </month> <year> 1985. </year>
Reference-contexts: The results are summarized in Table 4. Tests results for many of these applications on this platform and on a Sequent Symmetry are given in [Low93]. The Mandelbrot set is in the two-dimensional plane of complex numbers <ref> [Dew85] </ref>. The points in the Mandelbrot image take different amount of work to calculate and do not depend on any other points. Therefore, the programs can exhibit an imbalanced load and have no data locality.
Reference: [Doe87] <author> Thomas W. Doeppner. </author> <title> Threads: a system for the support of concurrent programming. </title> <type> Technical Report CS-87-11, </type> <institution> Brown University, </institution> <month> June </month> <year> 1987. </year>
Reference-contexts: The fine-grain quicksort program scales much better than the coarse-grain program; we believe this is because of contention for the bag. 5 Related Work There are many existing threads packages. The initial attempts to support efficient parallelism were lightweight thread packages such as Threads <ref> [Doe87] </ref>, Presto [BLL88], System [BS90], C++ [BDSY92], and Sun Lightweight Processes [SS92]. We will call these standard packages because they have a stack for each thread.
Reference: [EZ93] <author> Derek L. Eager and John Zahorjan. Chores: </author> <title> Enhanced run-time support for shared memory parallel computing. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(1) </volume> <pages> 1-32, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: This reduces the cost of running filaments, reduces the working set size to make more efficient use of the cache, and uses code that is easier for compilers to optimize. 3 Systems such as Chores <ref> [EZ93] </ref> and the Uniform System [TC88] have a fine-grain specification and a coarse-grain execution model, but use preprocessor support. Filaments generates different codes at compile time and chooses among them at run time. 6 Inlining consists of directly executing the body of each filament rather than making a procedure call. <p> TAM defines an abstract machine of self-scheduling parallel threads, which is used as an intermediate language that is mapped to existing processors, whereas Filaments defines a portable subroutine library, which is used to specify parallelism in a traditional, imperative way. Chores <ref> [EZ93] </ref> is similar to both Filaments and the Uniform System. Chores runs on top of Presto on a Sequent Symmetry. It uses a central ready queue, but servers take jobs in chunks. This amortizes the lock overhead of the central ready queue.
Reference: [FLA94] <author> Vincent W. Freeh, David K. Lowenthal, and Gregory R. Andrews. </author> <title> Distributed Filaments: Efficient fine-grain parallelism on a cluster of workstations. </title> <booktitle> In First Symposium on Operating Systems Design and Implementation, </booktitle> <pages> pages 201-212, </pages> <month> November </month> <year> 1994. </year>
Reference-contexts: It will be more important as networks of multiprocessors become more common. Our approach has been implemented in a portable software package called Filaments, which also supports fine-grain parallelism and a shared-memory programming model on both shared- and distributed-memory machines <ref> [FLA94] </ref>. It achieves efficiency in iterative computations by using stateless threads and implicitly coarsening granularity and in fork/join computations by pruning and automatically balancing the load. Filaments strives to make languages and compilers simpler by moving complexity into the run-time system. <p> However, Filaments directly supports efficient fine-grain parallelism, whereas Chores requires preprocessor support and the use of task generators in order to cluster fine-grain tasks into coarse-grain units. Distributed Filaments <ref> [FLA94] </ref> implemented the fine-grain model on a distributed-memory machine. [Shu95] and [NS95] describe later systems that allow the efficient management of many threads on distributed-memory machines. 6 Conclusion This paper has shown that fine-grain parallelism provides a simple programming model for parallel applications and has presented an efficient implementation for shared-memory <p> The Filaments package can be used as a stand-alone library, as here, or it can be used as a 13 compiler target. We have also developed an efficient implementation of Filaments for distributed--memory machines, including clusters of workstations <ref> [FLA94, LFA96] </ref>. The distributed implementation employs additional techniques for overlapping communication and computation and for adaptive data placement. We are continuing to work on the project to make the approach even more useful, portable, and efficient.
Reference: [Fre95] <author> Vincent W. Freeh. Writer-Owns: </author> <title> a new page consistency protocol for dynamically controlling thrashing on distributed-shared memory systems. </title> <month> December </month> <year> 1995. </year> <month> 14 </month>
Reference-contexts: Filaments strives to make languages and compilers simpler by moving complexity into the run-time system. It has been used as a runtime library for parallel programs written in C and as the runtime system for a compiler for the functional language Sisal <ref> [Fre95] </ref>. The remainder of the paper is organized as follows. The next section gives an overview of fine-grain parallelism and the Filaments package. Section 3 describes how the package is implemented. Section 4 gives performance results for a variety of applications. Section 5 discusses related work.
Reference: [HS91] <author> S.F. Hummel and E. Schonberg. </author> <title> Low-overhead scheduling of nested parallelism. </title> <journal> IBM Journel of Research and Development, </journal> <volume> 35(5) </volume> <pages> 743-765, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Several researchers have proposed ways to make standard thread packages more efficient. An-derson et al. [ALL89] discuss the gain from using local ready queues, and [ABLL92] shows how to do user-level scheduling. Schoenberg and Hummel <ref> [HS91] </ref> explain how to avoid allocating a stack per thread and switching contexts in nested parallel for loops. Markatos et al. [MB92] present a thorough study of the tradeoffs between load balancing and locality in shared memory machines.
Reference: [Kep93] <author> David Keppel. </author> <title> Tools and Techniques for Building Fast Portable Threads Packages. </title> <type> Technical Report UWCSE 93-05-06, </type> <institution> University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Schoenberg and Hummel [HS91] explain how to avoid allocating a stack per thread and switching contexts in nested parallel for loops. Markatos et al. [MB92] present a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Keppel <ref> [Kep93] </ref> describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Many threads packages support more fine-grain parallelism.
Reference: [LA96] <author> David K. Lowenthal and Gregory R. Andrews. </author> <title> An adaptive approach to data placement. </title> <booktitle> In Proceedings of the 10th International Symposium on Parallel Processing, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: See <ref> [LA96] </ref> for details. 3 is a special kind of variable with one copy per processor. Here, maxdiff is of type double and is shared among all filaments on a processor. The local copy of a reduction variable can be accessed directly.
Reference: [LFA96] <author> David K. Lowenthal, Vincent W. Freeh, and Gregory R. Andrews. </author> <title> Using fine-grain threads and run-time decision making in parallel computing. </title> <type> TR 96-2, </type> <institution> The University of Arizona, </institution> <month> January </month> <year> 1996. </year>
Reference-contexts: The Filaments package can be used as a stand-alone library, as here, or it can be used as a 13 compiler target. We have also developed an efficient implementation of Filaments for distributed--memory machines, including clusters of workstations <ref> [FLA94, LFA96] </ref>. The distributed implementation employs additional techniques for overlapping communication and computation and for adaptive data placement. We are continuing to work on the project to make the approach even more useful, portable, and efficient.
Reference: [Low93] <author> David K. Lowenthal. </author> <title> Performance experiments for the Filaments package. </title> <type> TR 93-13, </type> <institution> University of Arizona, </institution> <month> September </month> <year> 1993. </year>
Reference-contexts: The results are summarized in Table 4. Tests results for many of these applications on this platform and on a Sequent Symmetry are given in <ref> [Low93] </ref>. The Mandelbrot set is in the two-dimensional plane of complex numbers [Dew85]. The points in the Mandelbrot image take different amount of work to calculate and do not depend on any other points. Therefore, the programs can exhibit an imbalanced load and have no data locality.
Reference: [LS90] <author> Calvin Lin and Lawrence Snyder. </author> <title> A comparison of programming models for shared memory multiprocessors. </title> <journal> ICPP, </journal> <volume> 10(1) </volume> <pages> 163-170, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: The Uniform System also employs task generators (a related collection of tasks) and hence has a coarse-grain execution model. However, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin <ref> [LS90] </ref> and others have noted. Filaments can execute the fine-grain execution model efficiently in many cases, and when it needs to switch to a coarse-grain execution model, it does so at run time. WorkCrews [VR88] supports fork/join parallelism on small-scale, shared-memory multiprocessors.
Reference: [MB92] <author> Evangelos P. Markatos and Thomas L. Blanc. </author> <title> Load balancing vs. locality management in shared-memory multiprocessor. </title> <booktitle> In Proceedings of the 1992 International Conference on Parallel Processing, volume I, Architecture, pages I:258-267, </booktitle> <address> Boca Raton, Florida, August 1992. </address> <publisher> CRC Press. </publisher>
Reference-contexts: An-derson et al. [ALL89] discuss the gain from using local ready queues, and [ABLL92] shows how to do user-level scheduling. Schoenberg and Hummel [HS91] explain how to avoid allocating a stack per thread and switching contexts in nested parallel for loops. Markatos et al. <ref> [MB92] </ref> present a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Keppel [Kep93] describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Many threads packages support more fine-grain parallelism.
Reference: [NS95] <author> Richard Neves and Robert B. Schnabel. </author> <title> Runtime support for execution of fine grain parallel code on coarse-grain multiprocessors. </title> <booktitle> In Fifth Symposium on the Frontiers of Massively Parallel Computing, </booktitle> <month> February </month> <year> 1995. </year>
Reference-contexts: However, Filaments directly supports efficient fine-grain parallelism, whereas Chores requires preprocessor support and the use of task generators in order to cluster fine-grain tasks into coarse-grain units. Distributed Filaments [FLA94] implemented the fine-grain model on a distributed-memory machine. [Shu95] and <ref> [NS95] </ref> describe later systems that allow the efficient management of many threads on distributed-memory machines. 6 Conclusion This paper has shown that fine-grain parallelism provides a simple programming model for parallel applications and has presented an efficient implementation for shared-memory machines.
Reference: [PFTV88] <author> William H. Press, Brian P. Flannery, Saul A. Teukolsky, and William T. Vetterling. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <address> Cambrigde, </address> <year> 1988. </year>
Reference-contexts: There is very little difference in the execution times between the fine- and coarse-grain programs|always less than one second and at most 4.4%. LU decomposition is used to solve the linear system Ax = b <ref> [PFTV88] </ref>. Decompose A into lower-and upper-triangular matrices such that A = LU , and the linear system becomes A = LU x = b. The solution, x, is obtained by solving two triangular systems Ly = b and U x = y, using back-substitution.
Reference: [Shu95] <author> Wei Shu. </author> <title> Runtime support for user-level ultra lightweight threads on massively parallel distributed memory machines. </title> <booktitle> In Fifth Symposium on the Frontiers of Massively Parallel Computing, </booktitle> <month> Febru-ary </month> <year> 1995. </year>
Reference-contexts: However, Filaments directly supports efficient fine-grain parallelism, whereas Chores requires preprocessor support and the use of task generators in order to cluster fine-grain tasks into coarse-grain units. Distributed Filaments [FLA94] implemented the fine-grain model on a distributed-memory machine. <ref> [Shu95] </ref> and [NS95] describe later systems that allow the efficient management of many threads on distributed-memory machines. 6 Conclusion This paper has shown that fine-grain parallelism provides a simple programming model for parallel applications and has presented an efficient implementation for shared-memory machines.
Reference: [SS92] <author> D. Stein and D. Shah. </author> <title> Implementing lightweight threads. </title> <booktitle> In USENIX 1992, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: The initial attempts to support efficient parallelism were lightweight thread packages such as Threads [Doe87], Presto [BLL88], System [BS90], C++ [BDSY92], and Sun Lightweight Processes <ref> [SS92] </ref>. We will call these standard packages because they have a stack for each thread. The goal of standard packages is to provide the user with a natural thread abstraction and many of the usual concurrent programming primitives; different packages provide different primitives.
Reference: [TC88] <author> Robert H. Thomas and Will Crowther. </author> <title> The Uniform system: an approach to runtime support for large scale shared memory parallel processors. </title> <booktitle> In 1988 Conference on Parallel Processing, </booktitle> <pages> pages 245-254, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: This reduces the cost of running filaments, reduces the working set size to make more efficient use of the cache, and uses code that is easier for compilers to optimize. 3 Systems such as Chores [EZ93] and the Uniform System <ref> [TC88] </ref> have a fine-grain specification and a coarse-grain execution model, but use preprocessor support. Filaments generates different codes at compile time and chooses among them at run time. 6 Inlining consists of directly executing the body of each filament rather than making a procedure call. <p> Markatos et al. [MB92] present a thorough study of the tradeoffs between load balancing and locality in shared memory machines. Keppel [Kep93] describes a portable threads package that supports efficient barrier synchronization and non-preemptive threads. Many threads packages support more fine-grain parallelism. The Uniform System <ref> [TC88] </ref>, built for the BBN Butterfly, has several things in common with Filaments: There are no private stacks per 5 This is a simple application, and the Filaments program should perform well.
Reference: [VR88] <author> M. Vandevoorde and E. Roberts. Workcrews: </author> <title> an abstraction for controlling parallelism. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 17(4) </volume> <pages> 347-366, </pages> <month> August </month> <year> 1988. </year> <month> 15 </month>
Reference-contexts: This works well because the largest units of work tend to be at the front of processor lists <ref> [VR88] </ref>. For applications that do not exhibit much load imbalance|such as merge sort|the cost of acquiring data outweighs the gain of load balancing, so the programmer or compiler would disable the receiver-initiated phase. <p> However, the Uniform System cannot efficiently support thread-per-point decompositions for problems like Jacobi, as Lin [LS90] and others have noted. Filaments can execute the fine-grain execution model efficiently in many cases, and when it needs to switch to a coarse-grain execution model, it does so at run time. WorkCrews <ref> [VR88] </ref> supports fork/join parallelism on small-scale, shared-memory multiprocessors. WorkCrews introduced the concepts of pruning and of ordering queues to favor larger threads. Filaments has borrowed these ideas in its implementation of fork/join threads. TAM [CGSv93] is a compiler-controlled threaded abstract machine.
References-found: 26

