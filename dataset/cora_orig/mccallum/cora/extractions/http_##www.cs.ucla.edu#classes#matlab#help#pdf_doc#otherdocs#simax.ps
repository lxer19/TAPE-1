URL: http://www.cs.ucla.edu/classes/matlab/help/pdf_doc/otherdocs/simax.ps
Refering-URL: http://www.cs.ucla.edu/classes/matlab/help/fulldocset.html
Root-URL: http://www.cs.ucla.edu
Title: SPARSE MATRICES IN MATLAB: DESIGN AND IMPLEMENTATION  
Author: JOHN R. GILBERT CLEVE MOLER AND ROBERT SCHREIBER 
Keyword: Key words. Matlab, mathematical software, matrix computation, sparse matrix algorithms.  
Note: Dedicated to Gene Golub on the occasion of his 60th birthday.  AMS subject classifications. 65-04, 65F05, 65F20, 65F50, 68N15, 68R10.  
Abstract: We have extended the matrix computation language and environment Matlab to include sparse matrix storage and operations. The only change to the outward appearance of the Matlab language is a pair of commands to create full or sparse matrices. Nearly all the operations of Matlab now apply equally to full or sparse matrices, without any explicit action by the user. The sparse data structure represents a matrix in space proportional to the number of nonzero entries, and most of the operations compute sparse results in time proportional to the number of arithmetic operations on nonzeros. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Arioli, I. S. Duff, and P. P. M. de Rijk, </author> <title> On the augmented system approach to least-squares problems, </title> <journal> Numerische Mathematik, </journal> <volume> 55 (1989), </volume> <pages> pp. 667-684. </pages>
Reference-contexts: Instead, we use an apparently satisfactory substitute, ff = max ja ij j=1000: This approach has been used by several other authors, including Arioli et al. <ref> [1] </ref>, who do use a symmetric factorization and a similar heuristic for choosing ff. It is not clear whether augmented matrices, orthogonal factorizations, or iterative methods are preferable for least squares problems, from either an efficiency or an accuracy point of view.
Reference: [2] <author> C. Ashcraft, R. Grimes, J. Lewis, B. Peyton, and H. Simon, </author> <title> Recent progress in sparse matrix methods for large linear systems, </title> <journal> International Journal of Supercomputer Applications, </journal> <year> (1987), </year> <pages> pp. 10-30. </pages>
Reference-contexts: Cholesky factorization emphasizes simplicity and compatibility with the rest of sparse Matlab; thus it does not use some of the more sophisticated techniques such as the compressed index storage scheme [11, Sec. 5.4.2], or supernodal methods to take advantage of the clique structure of the chordal graph of the factor <ref> [2] </ref>. It does, however, run in time proportional to arithmetic operations with little overhead for data structure manipulation. We use a slightly simplified version of an algorithm from the Yale Sparse Matrix Package [9], which is described in detail by George and Liu [11]. <p> The permutation brings together the "fundamental supernodes" of A, which are full blocks in the Cholesky factor whose structure can be exploited in vectorized or parallel supernodal factorization <ref> [2, 17] </ref>. The postorder permutation can also be used to lay out the vertices for a picture of the elimination tree. The function tspy (A) plots a picture of the elimination tree of A, as shown in Figure 3. 3.4. Matrix division.
Reference: [3] <author> A. Bj orck, </author> <title> A note on scaling in the augmented system methods (unpublished manuscript), </title> <year> 1991. </year>
Reference-contexts: We ignore the symmetry and solve the linear system with a general sparse LU factorization, although a symmetric, indefinite factorization might be twice as fast. A recent note by Bjorck <ref> [3] </ref> analyzes the choice of the parameter ff by bounding the effect of roundoff errors on the error in the computed solution x. The value of ff which minimizes the bound involves two quantities, krk and the smallest singular value of A, which are too expensive to compute.
Reference: [4] <author> T. F. Coleman, A. Edenbrandt, and J. R. Gilbert, </author> <title> Predicting fill for sparse orthogonal factorization, </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 33 (1986), </volume> <pages> pp. 517-532. </pages>
Reference-contexts: Any matrix, whether square or not, has a form called the "Dulmage-Mendelsohn decomposition" <ref> [4, 20] </ref>, which is the same as ordinary block upper triangular form if the matrix is square and nonsingular. The most general form of the decomposition, for arbitrary rectangular A, is [p,q,r,s] = dmperm (A). The first two outputs are permutations that put A (p; q) into block form. <p> The subdiagonal blocks are all zero. The square diagonal blocks have nonzero diagonal elements. All the diagonal blocks are irreducible; for the non-square blocks, this means that they have the "strong Hall property" <ref> [4] </ref>. This block form can be used to solve least squares 15 Fig. 3. The Cholesky factor of a matrix and its elimination tree. problems by a method analogous to block back-substitution; see the references for more details. 3.3.4. Elimination trees.
Reference: [5] <author> J. Dongarra, J. Bunch, C. Moler, and G. Stewart, </author> <title> LINPACK Users Guide, </title> <address> Philadelphia, PA, </address> <year> 1978. </year>
Reference-contexts: The part of Matlab that involves computational linear algebra on dense matrices is based on direct adaptations of subroutines from Linpack and Eispack <ref> [5, 23] </ref>. An m fi n real matrix is stored as a full array of mn floating point numbers. The computational complexity of basic operations such as addition or transposition is proportional to mn. The complexity of more complicated operations such as triangular factorization is proportional to mn 2 . <p> The crucial question of which storage class to choose for a given matrix is the topic of Section 2.5. Even though Matlab is written in C, it follows its Linpack and Fortran predecessors and stores full matrices by columns <ref> [5, 19] </ref>. This organization has been carried over to sparse matrices. A sparse matrix is stored as the concatenation of the sparse vectors representing its columns.
Reference: [6] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 15 (1989), </volume> <pages> pp. 1-14. </pages>
Reference-contexts: The Matlab save and load commands, which save the current workspace or load a saved workspace, have been extended to accept sparse matrices and save them efficiently. We have written a Fortran utility routine that converts a file containing a sparse matrix in the Harwell-Boeing format <ref> [6] </ref> into a file that Matlab can load. 2.5. The results of sparse operations. <p> Effect of permutations on Cholesky factors. This sequence of examples illustrates the effect of reorderings on the computation of the Cholesky factorization of one symmetric test matrix. The matrix is S = W W T where W is the Harwell-Boeing matrix WEST0479 <ref> [6] </ref>, a model due to Westerberg of an eight-stage chemical distillation column. There are four figures. Each figure shows two spy plots, first a particular sym metric permutation of S and then the Cholesky factor of the permuted matrix. The 19 Fig. 4.
Reference: [7] <author> I. S. Duff and J. K. Reid, </author> <title> Some design features of a sparse matrix code, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 5 (1979), </volume> <pages> pp. </pages> <month> 18-35. </month> <title> 23 [8] , The multifrontal solution of indefinite sparse symmetric linear equations, </title> <journal> ACM Trans--actions on Mathematical Software, </journal> <volume> 9 (1983), </volume> <pages> pp. 302-325. </pages>
Reference-contexts: Only the diagonal blocks of the permuted matrix need to be factored, saving fill and arithmetic in the above-diagonal blocks. This strategy is incorporated in some existing Fortran sparse matrix packages, most notably Duff and Reid's code MA28 in the Harwell Subroutine Library <ref> [7] </ref>. Figure 9 is an implementation as a Matlab m-file. This function is a good illustration of the use of permutation vectors. The call [p,q,r] = dmperm (A) returns a row permutation p and a column permutation q to put A in block triangular form.
Reference: [9] <author> S. C. Eisenstat, M. H. Schultz, and A. H. Sherman, </author> <title> Algorithms and data structures for sparse symmetric Gaussian elimination, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 2 (1981), </volume> <pages> pp. 225-237. </pages>
Reference-contexts: Sparse matrices are widely used in scientific computation, especially in large-scale optimization, structural and circuit analysis, computational fluid dynamics, and, generally, the numerical solution of partial differential equations. Several effective Fortran subroutine packages for solving sparse linear systems are available, including Sparspak [11], the Yale Sparse Matrix Package <ref> [9] </ref>, and some of the routines in the Harwell Subroutine Library [25]. Our work was facilitated by our knowledge of the techniques used in the Fortran sparse matrix packages, but we have not directly adapted any of their code. <p> It does, however, run in time proportional to arithmetic operations with little overhead for data structure manipulation. We use a slightly simplified version of an algorithm from the Yale Sparse Matrix Package <ref> [9] </ref>, which is described in detail by George and Liu [11]. We begin with a combinatorial step that determines the number of nonzeros in the Cholesky factor (assuming no exact cancellation) and allocates a large enough block of storage.
Reference: [10] <author> A. George and J. Liu, </author> <title> The evolution of the minimum degree ordering algorithm, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 1-19. </pages>
Reference-contexts: This column ordering is the same as a symmetric minimum degree ordering for the matrix A T A, though we do not actually form A T A to compute it. George and Liu <ref> [10] </ref> survey the extensive development of efficient and effective versions of symmetric minimum degree, most of which is reflected in the symmetric minimum degree codes in Sparspak, YSMP, and the Harwell Subroutine Library. <p> Speelpenning [24] called such a clique representation of a symmetric graph the "generalized element" representation; George and 12 Liu <ref> [10] </ref> call it the "quotient graph model." Ours is the first column minimum degree implementation that we know of whose data structures are based directly on A, and which does not need to spend the time and storage to form the structure of A T A. <p> The idea for such a code is not new, however|George and Liu <ref> [10] </ref> suggest it, and our implementation owes a great deal to discussions between the first author and Esmond Ng and Barry Peyton of Oak Ridge National Laboratories.
Reference: [11] <author> A. George and J. W. H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: This report describes our design and implementation. Sparse matrices are widely used in scientific computation, especially in large-scale optimization, structural and circuit analysis, computational fluid dynamics, and, generally, the numerical solution of partial differential equations. Several effective Fortran subroutine packages for solving sparse linear systems are available, including Sparspak <ref> [11] </ref>, the Yale Sparse Matrix Package [9], and some of the routines in the Harwell Subroutine Library [25]. Our work was facilitated by our knowledge of the techniques used in the Fortran sparse matrix packages, but we have not directly adapted any of their code. <p> Our current implementation of Cholesky factorization emphasizes simplicity and compatibility with the rest of sparse Matlab; thus it does not use some of the more sophisticated techniques such as the compressed index storage scheme <ref> [11, Sec. 5.4.2] </ref>, or supernodal methods to take advantage of the clique structure of the chordal graph of the factor [2]. It does, however, run in time proportional to arithmetic operations with little overhead for data structure manipulation. <p> It does, however, run in time proportional to arithmetic operations with little overhead for data structure manipulation. We use a slightly simplified version of an algorithm from the Yale Sparse Matrix Package [9], which is described in detail by George and Liu <ref> [11] </ref>. We begin with a combinatorial step that determines the number of nonzeros in the Cholesky factor (assuming no exact cancellation) and allocates a large enough block of storage. We then compute the lower triangular factor R T one column at a time. <p> The algorithm first finds a "pseudo-peripheral" vertex of the graph of A, then generates a level structure by breadth-first search and orders the vertices by decreasing distance from the pseudo-peripheral vertex. Our implementation is based closely on the Sparspak implementation as described in the book by George and Liu <ref> [11] </ref>. Profile methods like reverse Cuthill-McKee are not the best choice for most large matrices arising from problems with two or more dimensions, or problems without much geometric structure, because such matrices typically do not have reorderings with low profile.
Reference: [12] <author> J. R. Gilbert, </author> <title> Predicting structure in sparse matrix computations, </title> <type> Tech. Report 86-750, </type> <institution> Cor-nell University, </institution> <year> 1986. </year> <note> To appear in SIAM Journal on Matrix Analysis and Applications. </note>
Reference-contexts: The set of nonzero indices of x corresponds to the set of all vertices of b, plus all vertices that can be reached from vertices of b via directed paths in the graph of A. (This is true even if A is not triangular <ref> [12] </ref>.) Any graph-searching algorithm could be used to identify those vertices and find the nonzero indices of x. A depth-first search has the advantage that a topological order for the list can be generated during the search.
Reference: [13] <author> J. R. Gilbert, C. Lewis, and R. Schreiber, </author> <title> Parallel preordering for sparse matrix factorization. </title> <note> In preparation. </note>
Reference-contexts: The Matlab version of minimum degree uses many of these ideas, as well as some ideas from a parallel symmetric minimum degree algorithm by Gilbert, Lewis, and Schreiber <ref> [13] </ref>. We sketch the algorithm briefly to show how these ideas are expressed in the framework of column minimum degree. The reader who is not interested in all the details can skip to Section 3.3.2.
Reference: [14] <author> J. R. Gilbert, C. Moler, and R. Schreiber, </author> <title> Sparse matrices in Matlab: Design and implementation, </title> <type> Tech. Report CSL 91-4, </type> <institution> Xerox Palo Alto Research Center, </institution> <year> 1991. </year>
Reference-contexts: In a technical report <ref> [14] </ref> we present some experimental evidence that sparse Matlab operations require time proportional to flops and data size in practice. 3.2. Factorizations. The LU and Cholesky factorizations of a sparse matrix yield sparse results. Matlab does not yet have a sparse QR factorization.
Reference: [15] <author> J. R. Gilbert and T. Peierls, </author> <title> Sparse partial pivoting in time proportional to arithmetic operations, </title> <journal> SIAM Journal on Scientific and Statistical Computing, </journal> <volume> 9 (1988), </volume> <pages> pp. 862-874. </pages>
Reference-contexts: Section 3.3 describes a few ways to find such a permutation. The matrix division operators " and / do pivot for sparsity by default; see Section 3.4. We use a version of the GPLU algorithm <ref> [15] </ref> to compute the LU factorization. This computes one column of L and U at a time by solving a sparse triangular system with the already-finished columns of L. Section 3.4.2 describes the sparse triangular solver that does most of the work. <p> Sparse Matlab should do the same, but the current version does not yet implement it. 3.4.2. Sparse triangular systems. The triangular linear system solver, which is also the main step of LU factorization, is based on an algorithm of Gilbert and Peierls <ref> [15] </ref>. When A is triangular and b is a sparse vector, x = Anb is computed in two steps. First, the nonzero structures of A and b are used (as described below) to make a list of the nonzero indices of x.
Reference: [16] <author> J. W. H. Liu, </author> <title> The role of elimination trees in sparse factorization, </title> <journal> SIAM Journal on Matrix Analysis and Applications, </journal> <volume> 11 (1990), </volume> <pages> pp. 134-172. </pages>
Reference-contexts: The Cholesky factor of a matrix and its elimination tree. problems by a method analogous to block back-substitution; see the references for more details. 3.3.4. Elimination trees. The elimination tree [21] of a symmetric positive definite matrix describes the dependences among rows or columns in Cholesky factorization. Liu <ref> [16] </ref> surveys applications of the elimination tree in sparse factorization. The nodes of the tree are the integers 1 through n, representing the rows of the matrix and of its upper triangular Cholesky factor.
Reference: [17] <author> J. W. H. Liu, E. Ng, and B. W. Peyton, </author> <title> On finding supernodes for sparse matrix computations, </title> <type> Tech. Report ORNL/TM-11563, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1990. </year>
Reference-contexts: The permutation brings together the "fundamental supernodes" of A, which are full blocks in the Cholesky factor whose structure can be exploited in vectorized or parallel supernodal factorization <ref> [2, 17] </ref>. The postorder permutation can also be used to lay out the vertices for a picture of the elimination tree. The function tspy (A) plots a picture of the elimination tree of A, as shown in Figure 3. 3.4. Matrix division.
Reference: [18] <author> The MathWorks, </author> <title> Pro-Matlab User's Guide, </title> <address> South Natick, MA, </address> <year> 1990. </year>
Reference-contexts: 1. Introduction. Matlab is an interactive environment and programming language for numeric scientific computation <ref> [18] </ref>. One of its distinguishing features is the use of matrices as the only data type. In Matlab, a matrix is a rectangular array of real or complex numbers.
Reference: [19] <author> C. Moler, </author> <title> Matrix computations with Fortran and paging, </title> <journal> Communications of the ACM, </journal> <volume> 15 (1972), </volume> <pages> pp. 268-270. </pages>
Reference-contexts: The crucial question of which storage class to choose for a given matrix is the topic of Section 2.5. Even though Matlab is written in C, it follows its Linpack and Fortran predecessors and stores full matrices by columns <ref> [5, 19] </ref>. This organization has been carried over to sparse matrices. A sparse matrix is stored as the concatenation of the sparse vectors representing its columns.
Reference: [20] <author> A. Pothen and C.-J. Fan, </author> <title> Computing the block triangular form of a sparse matrix, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 16 (1990), </volume> <pages> pp. 303-324. </pages>
Reference-contexts: Any matrix, whether square or not, has a form called the "Dulmage-Mendelsohn decomposition" <ref> [4, 20] </ref>, which is the same as ordinary block upper triangular form if the matrix is square and nonsingular. The most general form of the decomposition, for arbitrary rectangular A, is [p,q,r,s] = dmperm (A). The first two outputs are permutations that put A (p; q) into block form.
Reference: [21] <author> R. Schreiber, </author> <title> A new implementation of sparse Gaussian elimination, </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> 8 (1982), </volume> <pages> pp. 256-276. </pages>
Reference-contexts: This block form can be used to solve least squares 15 Fig. 3. The Cholesky factor of a matrix and its elimination tree. problems by a method analogous to block back-substitution; see the references for more details. 3.3.4. Elimination trees. The elimination tree <ref> [21] </ref> of a symmetric positive definite matrix describes the dependences among rows or columns in Cholesky factorization. Liu [16] surveys applications of the elimination tree in sparse factorization.
Reference: [22] <author> H. Schwartz, </author> <title> Tridiagonalization of a symmetric band matrix, </title> <journal> Numer. Math., </journal> <volume> 12 (1968), </volume> <pages> pp. 231-241. </pages> <booktitle> Also in [26, </booktitle> <pages> pages 273-283]. </pages>
Reference-contexts: However, we do provide one almost-direct technique for computing all the eigen-values (but not the eigenvectors) of a real symmetric or complex Hermitian sparse matrix. The reverse Cuthill-McKee algorithm is first used to provide a permutation which reduces the bandwidth. Then an algorithm of Schwartz <ref> [22] </ref> provides a sequence of plane rotations which further reduces the bandwidth to tridiagonal. Finally, the symmetric tridiagonal QR algorithm from dense Matlab yields all the eigenvalues. 4. Examples. This section gives the flavor of sparse Matlab by presenting several examples.
Reference: [23] <author> B. Smith, J. Boyle, Y. Ikebe, V. Klema, and C. Moler, </author> <title> Matrix Eigensystem Routines: EISPACK Guide, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <note> second ed., </note> <year> 1970. </year>
Reference-contexts: The part of Matlab that involves computational linear algebra on dense matrices is based on direct adaptations of subroutines from Linpack and Eispack <ref> [5, 23] </ref>. An m fi n real matrix is stored as a full array of mn floating point numbers. The computational complexity of basic operations such as addition or transposition is proportional to mn. The complexity of more complicated operations such as triangular factorization is proportional to mn 2 .
Reference: [24] <author> B. Speelpenning, </author> <title> The generalized element method, </title> <type> Tech. Report UIUCDCS-R-78-946, </type> <institution> University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: Speelpenning <ref> [24] </ref> called such a clique representation of a symmetric graph the "generalized element" representation; George and 12 Liu [10] call it the "quotient graph model." Ours is the first column minimum degree implementation that we know of whose data structures are based directly on A, and which does not need to
Reference: [25] <editor> United Kingdom Atomic Energy Authority, </editor> <title> Harwell subroutine library: A catalogue of subroutines, </title> <type> Tech. Report AERE R 9185, </type> <institution> Harwell Laboratory, Oxfordshire OX11 0RA, Great Britain, </institution> <year> 1988. </year>
Reference-contexts: Several effective Fortran subroutine packages for solving sparse linear systems are available, including Sparspak [11], the Yale Sparse Matrix Package [9], and some of the routines in the Harwell Subroutine Library <ref> [25] </ref>. Our work was facilitated by our knowledge of the techniques used in the Fortran sparse matrix packages, but we have not directly adapted any of their code.
Reference: [26] <author> J. Wilkinson and C. Reinsch, eds., </author> <title> Linear Algebra, vol. 2 of Handbook for Automatic Computation, </title> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1971. </year> <month> 24 </month>
References-found: 25

