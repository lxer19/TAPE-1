URL: file://ftp.cis.ohio-state.edu/pub/tech-report/1994/TR15.ps.gz
Refering-URL: ftp://ftp.cis.ohio-state.edu/pub/tech-report/TRList.html
Root-URL: 
Email: flutz-d,jayasimg@cis.ohio-state.edu  
Title: The Power of Carry-Save Addition  
Author: D. R. Lutz D. N. Jayasimha 
Keyword: Index terms: carry-save adders, computer arithmetic, superpipelined architectures, pipeline hazards, multipliers, counters, frequency dividers.  
Date: March 31, 1994  
Address: Columbus, Ohio 43210  
Affiliation: Department of Computer and Information Science The Ohio State University  
Abstract: A carry-save adder (CSA), or 3-2 adder, is a very fast and cheap adder that does not propagate carry bits. Historically, carry-save addition has been used for a limited set of intermediate calculations, with the most common example being the accumulation of the partial products of a multiplication. We examine carry-save addition from a new perspective: as a binary operation in which one of the operands is a number in carry-save form. From this perspective we develop five new uses for CSAs: (1) as fast adder-comparators for evaluating whether or not X + Y = Z; (2) as the basis of an expanded instruction set that can reduce branch and data hazards and decrease the cycles per instruction (CPI) on superpipelined architectures; (3) as linear-time multipliers for very large integers; (4) as arbitrary-length, constant-time, synchronous, up-down counters; and (5) as extremely fast frequency dividers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Alfred V. Aho, Ravi Sethi, and Jeffrey D. Ullman. </author> <booktitle> Compilers, Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: All of the adds except the last could be done as carry-save adds. * maintaining loop indexes we will show how to do this in an example later in this section. A compiler can detect when variables are used by the standard technique of examining defuse chains <ref> [1] </ref>. We also can determine which kinds of uses require their inputs in c-p form. For example, if the use is a multiplication then the inputs are required to be in c-p form, but if the use is a sum then the inputs could be in either form.
Reference: [2] <author> Peter R. Cappello and Willard L. Miranker. </author> <title> Systolic super summation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37 </volume> <pages> 657-677, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: The frequency divider can handle any frequency in the range from 1 to 2 n . 14 6 Future Directions We are currently investigating new uses for carry-save addition in floating point arithmetic. One interesting possibility is given in <ref> [2] </ref> and [3]. In these papers, Cappello and Miranker present designs for systolic super summers, which are adders that add floating point numbers in fixed point form. We believe that these designs could be simplified by using the carry-save addition concepts in this paper.
Reference: [3] <author> Peter R. Cappello and Willard L. Miranker. </author> <title> Systolic super summation with reduced hardware. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41 </volume> <pages> 339-342, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The frequency divider can handle any frequency in the range from 1 to 2 n . 14 6 Future Directions We are currently investigating new uses for carry-save addition in floating point arithmetic. One interesting possibility is given in [2] and <ref> [3] </ref>. In these papers, Cappello and Miranker present designs for systolic super summers, which are adders that add floating point numbers in fixed point form. We believe that these designs could be simplified by using the carry-save addition concepts in this paper.
Reference: [4] <author> Jordi Cortadella and Jose M. Llaberia. </author> <title> Evaluation of a + b = k conditions without carry propagation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41 </volume> <pages> 1484-1488, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: A four-bit design incorporating the method of theorem 1 is given in figure 2. Cortadello and Llaberia were the first to give a method for evaluating whether X + Y = Z without the need for carry propagation <ref> [4] </ref>. Our design is different from theirs, and has some advantages. First, it is considerably easier to understand and modify. For example, in section 5 we will modify the design to produce a fast programmable frequency divider.
Reference: [5] <author> Barry S. Fagin. </author> <title> Fast addition of large integers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41 </volume> <pages> 1069-1077, </pages> <month> September </month> <year> 1992. </year>
Reference-contexts: Three major differences are apparent. The first, and most striking difference, is the fact that a CSA produces its sum in only two gate delays, regardless of the length of the words being added [8], while the fastest n-bit CPA requires (log n) gate delays <ref> [5] </ref>. For 32 or 64 bit words, the fastest CPA requires around 14 gate delays, giving the CSA roughly a 7-fold speed advantage [7]. For longer words, the speedup is even more dramatic.
Reference: [6] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Palo Alto, California, </address> <year> 1990. </year>
Reference-contexts: Figure 1 shows a four-bit CSA together with its inputs and outputs. The representation of a sum by the ordered pair (C; S) is sometimes called redundant, because there are many values of C and S that produce the same sum <ref> [6] </ref>. In order to avoid confusion 2 with other redundant representations, we will say that (C; S) is the carry-save form (or c-s form) of the sum. <p> We will consider the base machine to be Hennessy and Patterson's DLX pipelined machine <ref> [6] </ref>. DLX has five pipelined stages: instruction fetch, instruction decode, execute, memory, and writeback. The machine cycle in DLX is long enough for the "execute" stage to complete a carry-propagate add. <p> hardware resources to support a given sequence of instructions; data hazards, which occur when an instruction cannot continue until it obtains a result produced by some previous instruction that is still in the pipeline; and control hazards, which occur when the normal flow of control is changed by an instruction <ref> [6] </ref>. Structural hazards can be eliminated by adding sufficient hardware. In this paper we are concerned only with data and control hazards. When an instruction encounters a hazard, the pipeline is stalled for one or more cycles until the hazard is resolved. On superpipelined architectures, hazards are especially problematical. <p> Some measurements on DLX show that conditional branches based on a simple test for equality or inequality with zero account for more than half of all conditional instructions, and approximately 11 percent of all instructions executed (The data used in this paragraph are taken from appendix C in <ref> [6] </ref>). It is likely that most of these tests were preceded by carry-propagate additions or subtactions, many of which could cause a data hazard on a superpipelined machine. Replacing these instructions with their CSA-based equivalents could save two stall cycles on each of these branches.
Reference: [7] <author> F. Hill and G. Peterson. </author> <title> Digital Systems: Hardware Organization and Design. </title> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, New York, third edition, </address> <year> 1987. </year> <month> 16 </month>
Reference-contexts: For 32 or 64 bit words, the fastest CPA requires around 14 gate delays, giving the CSA roughly a 7-fold speed advantage <ref> [7] </ref>. For longer words, the speedup is even more dramatic. The second major difference is that a CSA is a much simpler circuit than a fast CPA, with hardware complexity and area growing only linearly with the size of the input.
Reference: [8] <author> Kai Hwang. </author> <title> Computer Arithmetic: </title> <booktitle> Principles, Architecture, and Design. </booktitle> <publisher> John Wiley and Sons, Inc., </publisher> <address> New York, New York, </address> <year> 1979. </year>
Reference-contexts: Three major differences are apparent. The first, and most striking difference, is the fact that a CSA produces its sum in only two gate delays, regardless of the length of the words being added <ref> [8] </ref>, while the fastest n-bit CPA requires (log n) gate delays [5]. For 32 or 64 bit words, the fastest CPA requires around 14 gate delays, giving the CSA roughly a 7-fold speed advantage [7]. For longer words, the speedup is even more dramatic.
Reference: [9] <author> Kai Hwang. </author> <title> Advanced Computer Architecture: Parallelism, Scalability, Programmability. </title> <publisher> McGraw-Hill, Inc., </publisher> <address> New York, New York, </address> <year> 1993. </year>
Reference-contexts: A superpipelined machine of degree m is a pipelined machine in which the cycle time is only 1=m as long as the cycle time of the base machine, and the pipeline has about m times as many stages <ref> [9, 10] </ref>. Examples of superpipelined machines include the Cray-1 and the Cray Y-MP, which are superpipelined of degree 3. While a carry-propagate add takes one cycle to complete on the base machine, it takes m cycles to complete on the superpipelined machine.
Reference: [10] <author> Norman P. Jouppi and David W. Wall. </author> <title> Available instruction-level parallelism for superscalar and superpipelined machines. </title> <booktitle> Third Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 272-282, </pages> <year> 1989. </year>
Reference-contexts: A superpipelined machine of degree m is a pipelined machine in which the cycle time is only 1=m as long as the cycle time of the base machine, and the pipeline has about m times as many stages <ref> [9, 10] </ref>. Examples of superpipelined machines include the Cray-1 and the Cray Y-MP, which are superpipelined of degree 3. While a carry-propagate add takes one cycle to complete on the base machine, it takes m cycles to complete on the superpipelined machine. <p> We have not yet considered superscalar architectures. What effect would carry-save instructions have on these architectures? Jouppi and Wall showed that superscalar and su-perpipelined machines have roughly the same performance <ref> [10] </ref>. If it turns out that super-pipelined machines can be made significantly faster with the addition of CSA-based instructions, then it is likely that superpipelined machines will outperform superscalar machines.
Reference: [11] <author> W. Kuechlin, D. Lutz, and N. Nevin. </author> <title> Integer multiplication in parsac-2 on stock microprocessors. </title> <booktitle> In Ninth International Symposium on Applied Algebra, Algebraic Algorithms, and Error Correcting Codes, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: For numbers in the 300 to 7000-bit range, the Karatsuba algorithm gives the best results (O (n log 2 3 ) multiplies), while 7,000 or more-bit numbers are best multiplied using an FFT-based algorithm (O (n log n) multiplies) <ref> [11] </ref>. Both of these methods have fairly high overhead, and they would be considerably slower than the low-overhead linear-time multiplication provided by the special-purpose hardware proposed here. For an example of the possible savings, our best hand-tuned algorithm for multiplying 1000-bit numbers on a SPARC SLC (done as part of [11]) <p> <ref> [11] </ref>. Both of these methods have fairly high overhead, and they would be considerably slower than the low-overhead linear-time multiplication provided by the special-purpose hardware proposed here. For an example of the possible savings, our best hand-tuned algorithm for multiplying 1000-bit numbers on a SPARC SLC (done as part of [11]) consumed over 330,000 cycles. Given the same technology, but ignoring I/O, our 1000-bit multiplier could produce the same output in c-s form in 1000 cycles.
Reference: [12] <author> Tso-Kai Liu, Keith R. Hohulin, Lih-Er Shiau, and Saburo Muroga. </author> <title> Optimal one-bit full adders with different types of gates. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 23 </volume> <pages> 63-69, </pages> <month> January </month> <year> 1974. </year>
Reference-contexts: Second, it is easy to separate into two useful functions: carry-save add and compare with -1. The benefits of these functions will be discussed in section 3. Finally, our design may also be easier to implement, because its basic component, the full adder, has been very well studied <ref> [12] </ref>. 5 Our design is much faster than any design employing CPAs to add and perform comparisons. The fastest CPA-based design to perform this test would add X and Y with a CPA and compare each bit of the sum with the corresponding bit of Z, as in figure 3.
Reference: [13] <author> R.L. Rivest, A. Shamir, and L. Adleman. </author> <title> A method for obtaining digital signatures and public-key cryptosystems. </title> <journal> Communications of the ACM, </journal> <volume> 21 </volume> <pages> 120-126, </pages> <month> February </month> <year> 1978. </year>
Reference-contexts: What use can we make of the fact that CSAs are equally fast for large words? One application that uses long words, and in particular long multiplications, is encryption. The RSA encryption algorithm, for example, requires repeated multiplication of numbers that are hundreds of bits in length <ref> [13] </ref>. It is easy to construct an n-bit multiplier that uses a single CSA to multiply in linear time. The design (figure 4) is essentially the same as the multiply-step design used in some RISC architectures, with the accumulation done in c-s form.
Reference: [14] <author> Norman R. Scott. </author> <title> Computer Number Systems and Arithmetic. </title> <publisher> Prentice Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1985. </year>
Reference-contexts: A major disadvantage of most alternate number systems, including the widely-studied residue number systems, is the difficulty of conversion between the alternate system and the two's complement system (for examples see <ref> [14] </ref>). In contrast, conversion between c-s form and c-p form is easy. Numbers in c-s form are converted to c-p form by adding them using a CPA. Numbers in c-p form are converted to c-s form by adding them to (0, 0) using a CSA.
Reference: [15] <author> Gurindar S. Sohi. </author> <title> Instruction issue logic for high-performance, interruptible, multiple functional unit, pipelined computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39 </volume> <pages> 349-359, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We do know that control and data hazards are a big problem. In <ref> [15] </ref>, a simulation of a superpipelined scalar version of the Cray 1 with no structural hazards still showed a CPI of more than 2 on the first 14 Lawrence Livermore Loops (The ideal CPI would be 1).
Reference: [16] <author> J. E. Vuillemin. </author> <title> Constant time arbitrary length synchronous binary counters. </title> <booktitle> In 10th IEEE Symposium on Computer Arithmetic, </booktitle> <pages> pages 180-183, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: The count is retrieved by adding C and S with a CPA. Unlike the usual design, updates occur in constant time, and the slow carry-propagation only takes place when the count is retrieved. In <ref> [16] </ref>, Vuillemin presents the following open question: "is it possible to design a synchronous, arbitrary length, constant time up-down counter"? If we accept the notion that there is no need to maintain the count in c-p form, then figure 5 is such a counter.
Reference: [17] <author> Shlomo Weiss and James E. Smith. </author> <title> A study of scalar compilation techniques for pipelined supercomputers. </title> <booktitle> Proceedings of the Second International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 105-109, </pages> <year> 1987. </year>
Reference-contexts: This means that more than half of the available cycles were spent on stalls because of control and data hazards. Most of these stalls were probably related to loop overhead, because the CPI in a similar simulation was close to 1 when the loops were unrolled eight times <ref> [17] </ref>. Some measurements on DLX show that conditional branches based on a simple test for equality or inequality with zero account for more than half of all conditional instructions, and approximately 11 percent of all instructions executed (The data used in this paragraph are taken from appendix C in [6]).
References-found: 17

