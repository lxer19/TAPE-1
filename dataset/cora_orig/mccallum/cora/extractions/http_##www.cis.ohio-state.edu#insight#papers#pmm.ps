URL: http://www.cis.ohio-state.edu/insight/papers/pmm.ps
Refering-URL: http://www.cis.ohio-state.edu/insight/abs.html
Root-URL: 
Title: The Parallel Multipole Method on the Connection Machine R  
Author: Feng Zhao S. Lennart Johnsson 
Keyword: N -body algorithm, parallel computing, Parallel Multipole Method, hyper cube architecture, grid embedding, binary-reflected Gray code  
Address: 545 Technology Square, Cambridge  245 First Street, Cambridge  
Affiliation: MIT Artificial Intelligence Laboratory  Thinking Machines Corporation  Division of Applied Sciences, Harvard University.  
Date: 12(6):1420-1437, 1991)  
Note: (appeared in SIAM J. on Scientific Statistical Computing,  AMS(MOS) subject classifications: 65C20, 70-08, 70F10, 70F15 Also affiliated with the  
Abstract: This paper reports on a fast implementation of the three-dimensional nonadaptive Parallel Multipole Method (PMM) on the Connection Machine system model CM-2. The data interactions within the decomposition tree are modeled by a hierarchy of three dimensional grids forming a pyramid in which parent nodes have degree eight. The base of the pyramid is embedded in the Connection Machine as a three dimensional grid. The standard grid embedding feature is used. For 10 or more particles per processor the communication time is insignificant. The evaluation of the potential field for a system with 128k particles takes 5 seconds, and a million particle system about 3 minutes. The maximum number of particles that can be represented in 2G bytes of primary storage is ~ 50 million. The execution rate of this implementation of the PMM is at about 1.7 Gflops/sec for a particle-processor-ratio of 10 or greater. A further speed improvement is possible by an improved use of the memory hierarchy associated with each floating-point unit in the system. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Josh Barnes and Piet Hut. </author> <title> A hierarchical o(n log n) force calculation algorithm. </title> <journal> Nature, </journal> <volume> 324 </volume> <pages> 446-449, </pages> <year> 1986. </year>
Reference-contexts: Efficient computation of the gravitational (or Coulombic) potentials and forces exerted on each other by N particles has been of great interest. Recently, a class of fast algorithms known as treelike particle methods has been developed. These methods, notably the Barnes-Hut tree algorithm <ref> [1] </ref> and the Multipole Method [8, 21], compute the potentials (or the forces) by partitioning them into two parts: total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles.
Reference: [2] <author> J. Carrier, L. Greengard, and V. Rokhlin. </author> <title> A fast adaptive multipole algorithm for particle simulations. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 9 </volume> <pages> 669-686, </pages> <year> 1988. </year>
Reference-contexts: The optimal value of h with respect to performance depends upon the relative overheads of different methods. Greengard and Rokhlin <ref> [2, 6] </ref> compared the non-adaptive Multipole Method with the direct evaluation of the Newtonian field on a VAX-8600.
Reference: [3] <author> M.Y. Chan. </author> <title> Dilation-2 embeddings of grids into hypercubes. </title> <booktitle> In 1988 International Conf. on Parallel Processing. </booktitle> <publisher> The Pennsylvania State University Press, </publisher> <year> 1988. </year>
Reference-contexts: The expansion of the embedding is the ratio between the number of nodes of the Boolean cube required for the embedding and the number of nodes in the grid being embedded. Any two dimensional grid can be embedded into a Boolean cube with dilation two and minimum 8 expansion <ref> [3] </ref>. Any three dimensional grid can be embedded with at most dilation seven and minimal expansion [5]. Grids with k dimensions can be embedded with a dilation of at most 4k + 1 and minimal expansion [4].
Reference: [4] <author> M.Y. Chan. </author> <title> The embedding of grids into optimal hypercubes. </title> <type> Technical report, </type> <institution> Computer Science Dept., University of Texas at Dallas, </institution> <year> 1988. </year>
Reference-contexts: Any three dimensional grid can be embedded with at most dilation seven and minimal expansion [5]. Grids with k dimensions can be embedded with a dilation of at most 4k + 1 and minimal expansion <ref> [4] </ref>. Several dilation two embeddings of two and three dimensional grids are given in [11, 12]. In the Connection Machine programming systems data represented by arrays configure the address space as a grid.
Reference: [5] <author> M.Y. Chan. </author> <title> Embeddings of 3-dimensional grids into optimal hypercubes. </title> <type> Technical report, </type> <institution> Computer Science Dept., University of Texas at Dallas, </institution> <year> 1988. </year> <booktitle> Also in the Proceedings of the Fourth Conference on Hypercubes, Concurrent Computers, and Applications, </booktitle> <month> March, </month> <year> 1989. </year>
Reference-contexts: Any two dimensional grid can be embedded into a Boolean cube with dilation two and minimum 8 expansion [3]. Any three dimensional grid can be embedded with at most dilation seven and minimal expansion <ref> [5] </ref>. Grids with k dimensions can be embedded with a dilation of at most 4k + 1 and minimal expansion [4]. Several dilation two embeddings of two and three dimensional grids are given in [11, 12].
Reference: [6] <author> Leslie Greengard. </author> <title> The Rapid Evaluation of Potential Fileds in Particle Systems. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Yale Univ., </institution> <address> New Haven, CT, April 1987. </address> <publisher> Published by MIT Press, </publisher> <year> 1988. </year>
Reference-contexts: These subdomains are the children of the parent domain. The recursive decomposition process continues until some prespecified condition is met. Clearly, there is no reason to further decompose a subdomain containing only one, or no particles. However, as has been shown both analytically <ref> [6] </ref> and in several implementations [21, 7, 9] the direct method is faster than the Multipole Method for particle systems with sufficiently few particles. <p> For the Multipole Method it is advantageous to distinguish among three regions relative to each grid point at each grid level <ref> [8, 6, 21, 19] </ref>: the near-field, the interactive-field, and the far-field. The near-field consists of those subdomains that share a boundary, or an edge, or a corner with the considered subdomain. In two dimensions the near-field consists of eight subdomains, i.e., eight adjacent grid points. <p> The optimal value of h with respect to performance depends upon the relative overheads of different methods. Greengard and Rokhlin <ref> [2, 6] </ref> compared the non-adaptive Multipole Method with the direct evaluation of the Newtonian field on a VAX-8600.
Reference: [7] <author> Leslie Greengard and William D. Gropp. </author> <title> A parallel version of the fast multipole method. In Parallel Processing for Scientific Computing, </title> <editor> G. </editor> <publisher> Rodrigue (ed.), </publisher> <pages> pages 213-222. </pages> <publisher> SIAM, </publisher> <year> 1989. </year>
Reference-contexts: It achieves significant speed-up on parallel machines <ref> [21, 7] </ref>. In [21] we presented a data parallel version of the PMM, and showed that its parallel complexity is O (log N ), asymptotically. <p> These subdomains are the children of the parent domain. The recursive decomposition process continues until some prespecified condition is met. Clearly, there is no reason to further decompose a subdomain containing only one, or no particles. However, as has been shown both analytically [6] and in several implementations <ref> [21, 7, 9] </ref> the direct method is faster than the Multipole Method for particle systems with sufficiently few particles. To mini mize the computational complexity the decomposition of a subdomain should stop at a level where, for instance, using the direct method instead yields a lower computational complexity. <p> It requires O (log N ) time asymptotically for N particles distributed with a fixed number of particles per processor <ref> [7, 21] </ref>. The PMM algorithm performs the computations implied by the expressions (2), (3), (4), (5), and (6). Expression (2) is evaluated concurrently for all leaf nodes. In each such node a multipole expansion due to all particles within the leaf node is formed. <p> The optimum stopping criterion is implementation dependent. The stopping criterion for the subdivision of a domain represented by several floating point processors also has to acknowledge the difference in communication needs between 19 the methods. In the parallel implementation of the two-dimensional Multipole Method re-ported in <ref> [7] </ref> a comparison was made with a parallel implementation of the direct method. The two implementations were made on a shared memory machine, the Encore Multimax.
Reference: [8] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> A fast algorithm for particle simulations. </title> <journal> J. Comp. Phys., </journal> <volume> 73 </volume> <pages> 325-348, </pages> <year> 1987. </year> <month> 21 </month>
Reference-contexts: Efficient computation of the gravitational (or Coulombic) potentials and forces exerted on each other by N particles has been of great interest. Recently, a class of fast algorithms known as treelike particle methods has been developed. These methods, notably the Barnes-Hut tree algorithm [1] and the Multipole Method <ref> [8, 21] </ref>, compute the potentials (or the forces) by partitioning them into two parts: total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles. <p> Greengard and Rokhlin <ref> [8] </ref> have shown that the Multipole Method solves the N -body problem in linear time on sequential machines. The focus of this paper is the implementation of the non-adaptive Multipole Method on the Connection Machine. <p> For the Multipole Method it is advantageous to distinguish among three regions relative to each grid point at each grid level <ref> [8, 6, 21, 19] </ref>: the near-field, the interactive-field, and the far-field. The near-field consists of those subdomains that share a boundary, or an edge, or a corner with the considered subdomain. In two dimensions the near-field consists of eight subdomains, i.e., eight adjacent grid points. <p> union of subdomains labeled n and its interactive-field is the union of those labeled i. 2.2 The algorithm The Multipole Method can be specified in terms of three functions G, and , three translation operators T 1 , T 2 and T 3 , and a set of recursive equations <ref> [8, 21, 19] </ref>. G is 4 the potential function in explicit Newtonian formulation.
Reference: [9] <author> Leslie Greengard and Vladimir Rokhlin. </author> <title> On the efficient implementation of the fast multipole method. </title> <type> Technical Report YALEU/DCS/RR-602, </type> <institution> Dept. of Computer Science, Yale Univ., </institution> <address> New Haven, CT, </address> <month> February </month> <year> 1988. </year>
Reference-contexts: These subdomains are the children of the parent domain. The recursive decomposition process continues until some prespecified condition is met. Clearly, there is no reason to further decompose a subdomain containing only one, or no particles. However, as has been shown both analytically [6] and in several implementations <ref> [21, 7, 9] </ref> the direct method is faster than the Multipole Method for particle systems with sufficiently few particles. To mini mize the computational complexity the decomposition of a subdomain should stop at a level where, for instance, using the direct method instead yields a lower computational complexity. <p> The cross-over point below which the direct method is faster was found to be in the range of 200 - 400 particles for a variety of two dimensional particle distributions including highly non-uniform distributions, where the maximum number of particles per leaf is 30 in the Multipole Method. In <ref> [9] </ref> the non-adaptive algorithm is compared with the direct method for three dimensional problems. For uniform distributions the cross-over point on a VAX-8600 is in the range of 1000 - 2000 particles with an accuracy of 10 6 . <p> In the three dimensional implementation of the Multipole Method reported in [21] the cross-over point is at about 1000 particles. The experiment was carried out on a Symbolics Lisp Machine with the results accurate to 10 4 . In both <ref> [9, 21] </ref> the Multipole Method restricts the number of particles in a leaf to be less than 8. The local direct interaction at the leaf node includes the direct interaction within the same leaf node as well as that with adjacent leaf nodes.
Reference: [10] <author> I. Havel and J. Moravek. </author> <title> B-valuations of graphs. Czech. </title> <journal> Math. J., </journal> <volume> 22 </volume> <pages> 338-351, </pages> <year> 1972. </year>
Reference-contexts: For grids of arbitrary shape the utilization may be as low as ~ 2 d . Any embedding of grids into Boolean cubes preserving adjacency will have this processor utilization <ref> [10] </ref>. In order to increase the processor utilization for grids of arbitrary shape it is necessary to allow some grid edges to be mapped into paths of a length greater than one.
Reference: [11] <author> Ching-Tien Ho and S. Lennart Johnsson. </author> <title> On the embedding of arbitrary meshes in Boolean cubes with expansion two dilation two. </title> <booktitle> In 1987 International Conf. on Parallel Processing, </booktitle> <pages> pages 188-191. </pages> <publisher> IEEE Computer Society, </publisher> <year> 1987. </year>
Reference-contexts: Any three dimensional grid can be embedded with at most dilation seven and minimal expansion [5]. Grids with k dimensions can be embedded with a dilation of at most 4k + 1 and minimal expansion [4]. Several dilation two embeddings of two and three dimensional grids are given in <ref> [11, 12] </ref>. In the Connection Machine programming systems data represented by arrays configure the address space as a grid. Configuring the address space as a grid implies that the array indices are encoded in a binary-reflected Gray code. Each axis is encoded separately. The encoding is transparent to the programmer.
Reference: [12] <author> Ching-Tien Ho and S. Lennart Johnsson. </author> <title> Embedding meshes in Boolean cubes by graph decomposition. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(4) </volume> <pages> 325-339, </pages> <year> 1990. </year>
Reference-contexts: Any three dimensional grid can be embedded with at most dilation seven and minimal expansion [5]. Grids with k dimensions can be embedded with a dilation of at most 4k + 1 and minimal expansion [4]. Several dilation two embeddings of two and three dimensional grids are given in <ref> [11, 12] </ref>. In the Connection Machine programming systems data represented by arrays configure the address space as a grid. Configuring the address space as a grid implies that the array indices are encoded in a binary-reflected Gray code. Each axis is encoded separately. The encoding is transparent to the programmer.
Reference: [13] <author> S. Lennart Johnsson. </author> <title> Odd-even cyclic reduction on ensemble architectures and the solution tridiagonal systems of equations. </title> <type> Technical Report YALE/DCS/RR-339, </type> <institution> Dept. of Computer Science, Yale University, </institution> <month> October </month> <year> 1984. </year>
Reference-contexts: The on-chip and memory fields are encoded in binary code. With a non-adaptive recursive decomposition of the domain it suffices to consider grids with axis lengths being powers of two. For such grids the following two properties <ref> [13, 14, 16] </ref> of the binary-reflected Gray code are important for the embedding of a hierarchy of grids: Hamming (i; i 2 j ) = 2; j &gt; 0, and Hamming (i; i + 3)=1 for i mod 2 = 0. <p> Hence, even though successively coarser grids consist of points with indices differing by increasing powers of two, the points are always in proximity when the grid is mapped to a Boolean cube by a binary-reflected Gray code. This property is easily proven <ref> [13] </ref> and apparent from Figure 4. Communication between adjacent nodes in the finest grid is nearest neighbor communication in the Boolean cube. Communication between adjacent nodes in coarser grids implies communication between processors at distance two in the Boolean cube. <p> Communication between adjacent nodes in the finest grid is nearest neighbor communication in the Boolean cube. Communication between adjacent nodes in coarser grids implies communication between processors at distance two in the Boolean cube. All distance two communications can be arranged such that there is no contention for channels <ref> [13] </ref>. Then, the communication time for bit-serial pipelined communication, which is the communication mode on the Connection Machine system, is dominated by the message length. The path length is an additive term. <p> The path length is an additive term. Subselecting grid points for coarse grids by choosing the grid at level ` to consist of all points such that i mod 2 h` = 0 for each axis causes the grid points to be allocated to different subcubes. In <ref> [13, 16] </ref> an exchange step was introduced to move the selected points into subcubes identified by address bits. After the exchange the subselected points are embedded by a binary-reflected Gray code in a half size subcube.
Reference: [14] <author> S. Lennart Johnsson. </author> <title> Communication efficient basic linear algebra computations on hypercube architectures. </title> <journal> J. Parallel Distributed Comput., </journal> <volume> 4(2) </volume> <pages> 133-172, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: Inter-processor communication can take place either by using the general routing facility, or via embedded grids. The Boolean cube network interconnecting the processor chips contains regular grids of up to twelve dimensions as subgraphs. Address encoding by a binary-reflected Gray code [20] provides a mechanism for grid emulation <ref> [18, 14] </ref>. The dimensionality and shape of the grid can be altered dynamically. The Gray code has the property that adjacent integers are at Hamming distance one, i.e., Hamming (i; i 1)=1. By using this encoding successive integers are mapped into adjacent nodes of a Boolean cube. <p> The on-chip and memory fields are encoded in binary code. With a non-adaptive recursive decomposition of the domain it suffices to consider grids with axis lengths being powers of two. For such grids the following two properties <ref> [13, 14, 16] </ref> of the binary-reflected Gray code are important for the embedding of a hierarchy of grids: Hamming (i; i 2 j ) = 2; j &gt; 0, and Hamming (i; i + 3)=1 for i mod 2 = 0. <p> This scheme converts the binary-reflected Gray code to a binary code <ref> [14, 15] </ref>. This exchange algorithm implicitly and recursively makes use of the fact that Hamming (i; i+3)=1 for i mod 2 = 0. If the interaction between adjacent grid levels consists in a reduction/distribution operation the exchange step need not be performed.
Reference: [15] <author> S. Lennart Johnsson. </author> <title> Optimal Communication in Distributed and Shared Memory Models of Computation on Network Architectures. </title> <booktitle> In VLSI and Parallel Computation, </booktitle> <pages> pages 223-389. </pages> <publisher> Morgan Kaufman, </publisher> <year> 1990. </year>
Reference-contexts: This scheme converts the binary-reflected Gray code to a binary code <ref> [14, 15] </ref>. This exchange algorithm implicitly and recursively makes use of the fact that Hamming (i; i+3)=1 for i mod 2 = 0. If the interaction between adjacent grid levels consists in a reduction/distribution operation the exchange step need not be performed.
Reference: [16] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Optimizing tridiagonal solvers for alternating direction methods on Boolean cube multiprocessors. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 11(3) </volume> <pages> 563-592, </pages> <year> 1990. </year>
Reference-contexts: The on-chip and memory fields are encoded in binary code. With a non-adaptive recursive decomposition of the domain it suffices to consider grids with axis lengths being powers of two. For such grids the following two properties <ref> [13, 14, 16] </ref> of the binary-reflected Gray code are important for the embedding of a hierarchy of grids: Hamming (i; i 2 j ) = 2; j &gt; 0, and Hamming (i; i + 3)=1 for i mod 2 = 0. <p> The path length is an additive term. Subselecting grid points for coarse grids by choosing the grid at level ` to consist of all points such that i mod 2 h` = 0 for each axis causes the grid points to be allocated to different subcubes. In <ref> [13, 16] </ref> an exchange step was introduced to move the selected points into subcubes identified by address bits. After the exchange the subselected points are embedded by a binary-reflected Gray code in a half size subcube. <p> This strategy is used in our implementation of the PMM. It was previously used in <ref> [16, 17] </ref>. 10 4 Implementation 4.1 Overview The first implementation of the PMM on the Connection Machine [21] mapped the nodes at all levels of the decomposition tree to processors by a binary encoding of the node addresses.
Reference: [17] <author> S. Lennart Johnsson and Ching-Tien Ho. </author> <title> Multiple tridiagonal systems, the alternating direction method, and Boolean cube configured multiprocessors. </title> <type> Technical Report YALEU/DCS/RR-532, </type> <institution> Dept. of Computer Science, Yale University, </institution> <address> New Haven, CT, </address> <month> June </month> <year> 1987. </year>
Reference-contexts: This strategy is used in our implementation of the PMM. It was previously used in <ref> [16, 17] </ref>. 10 4 Implementation 4.1 Overview The first implementation of the PMM on the Connection Machine [21] mapped the nodes at all levels of the decomposition tree to processors by a binary encoding of the node addresses.
Reference: [18] <author> S. Lennart Johnsson and Peggy Li. </author> <title> Solutionset for AMA/CS 146. </title> <type> Technical Report 5085:DF:83, </type> <institution> California Institute of Technology, </institution> <month> May </month> <year> 1983. </year>
Reference-contexts: Inter-processor communication can take place either by using the general routing facility, or via embedded grids. The Boolean cube network interconnecting the processor chips contains regular grids of up to twelve dimensions as subgraphs. Address encoding by a binary-reflected Gray code [20] provides a mechanism for grid emulation <ref> [18, 14] </ref>. The dimensionality and shape of the grid can be altered dynamically. The Gray code has the property that adjacent integers are at Hamming distance one, i.e., Hamming (i; i 1)=1. By using this encoding successive integers are mapped into adjacent nodes of a Boolean cube.
Reference: [19] <author> Jacob Katsenelson. </author> <title> Computational structure of the n-body problem. </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 10 </volume> <pages> 787-815, </pages> <year> 1989. </year>
Reference-contexts: For the Multipole Method it is advantageous to distinguish among three regions relative to each grid point at each grid level <ref> [8, 6, 21, 19] </ref>: the near-field, the interactive-field, and the far-field. The near-field consists of those subdomains that share a boundary, or an edge, or a corner with the considered subdomain. In two dimensions the near-field consists of eight subdomains, i.e., eight adjacent grid points. <p> union of subdomains labeled n and its interactive-field is the union of those labeled i. 2.2 The algorithm The Multipole Method can be specified in terms of three functions G, and , three translation operators T 1 , T 2 and T 3 , and a set of recursive equations <ref> [8, 21, 19] </ref>. G is 4 the potential function in explicit Newtonian formulation. <p> The downward-pass through the tree distributes the results of far-field interactions, aligns them properly, and combines them to form the local expansion . The computation of nearfield of (1) performed at leaf nodes reduces near-field interactions. In the recursive formulation by J. Katzenelson <ref> [19] </ref>, the upward-pass consists of the computation Compute h i for all nodes i at the leaf level h (2) ` X i2fchildren (n)g T 1 ( `+1 and the downward-pass consists of the computation ~ ` parent (i) ) (4) ` i + j2finteractivefield (i)g T 2 ( ` The <p> The precomputation makes use of the fact that there exist a subset of kernels from which the coefficients can be computed with a modest effort <ref> [19] </ref>. 4.2 Reduction and distribution functions In the upward-pass (3) of the PMM a reduction operator is applied recursively on a three-dimensional grid data structure. The downward-pass (5) accumulates the data and distributes the results. The reduction operator and the distribution operator are very general.
Reference: [20] <author> E M. Reingold, J. Nievergelt, and N. Deo. </author> <title> Combinatorial Algorithms. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs. NJ, </address> <year> 1977. </year>
Reference-contexts: Inter-processor communication can take place either by using the general routing facility, or via embedded grids. The Boolean cube network interconnecting the processor chips contains regular grids of up to twelve dimensions as subgraphs. Address encoding by a binary-reflected Gray code <ref> [20] </ref> provides a mechanism for grid emulation [18, 14]. The dimensionality and shape of the grid can be altered dynamically. The Gray code has the property that adjacent integers are at Hamming distance one, i.e., Hamming (i; i 1)=1.
Reference: [21] <author> Feng Zhao. </author> <title> An o(n) algorithm for three-dimensional n-body simulations. </title> <type> Technical Report AI-995, </type> <institution> MIT, Artificial Intelligence Laboratory, </institution> <month> October </month> <year> 1987. </year> <month> 22 </month>
Reference-contexts: Efficient computation of the gravitational (or Coulombic) potentials and forces exerted on each other by N particles has been of great interest. Recently, a class of fast algorithms known as treelike particle methods has been developed. These methods, notably the Barnes-Hut tree algorithm [1] and the Multipole Method <ref> [8, 21] </ref>, compute the potentials (or the forces) by partitioning them into two parts: total = nearfield + farfield ; (1) where nearfield is the potential due to nearby particles and farfield is the potential due to faraway particles. <p> It achieves significant speed-up on parallel machines <ref> [21, 7] </ref>. In [21] we presented a data parallel version of the PMM, and showed that its parallel complexity is O (log N ), asymptotically. <p> It achieves significant speed-up on parallel machines [21, 7]. In <ref> [21] </ref> we presented a data parallel version of the PMM, and showed that its parallel complexity is O (log N ), asymptotically. <p> These subdomains are the children of the parent domain. The recursive decomposition process continues until some prespecified condition is met. Clearly, there is no reason to further decompose a subdomain containing only one, or no particles. However, as has been shown both analytically [6] and in several implementations <ref> [21, 7, 9] </ref> the direct method is faster than the Multipole Method for particle systems with sufficiently few particles. To mini mize the computational complexity the decomposition of a subdomain should stop at a level where, for instance, using the direct method instead yields a lower computational complexity. <p> For the Multipole Method it is advantageous to distinguish among three regions relative to each grid point at each grid level <ref> [8, 6, 21, 19] </ref>: the near-field, the interactive-field, and the far-field. The near-field consists of those subdomains that share a boundary, or an edge, or a corner with the considered subdomain. In two dimensions the near-field consists of eight subdomains, i.e., eight adjacent grid points. <p> The number of subdomains in the interactive-field in two dimensions is 27 = 6 2 3 2 , and in three dimensions the interactive field consists of 189 = 6 3 3 3 subdomains. The concepts of near-field and interactive-field are illustrated in two dimensions in Figure 2 <ref> [21] </ref>. <p> union of subdomains labeled n and its interactive-field is the union of those labeled i. 2.2 The algorithm The Multipole Method can be specified in terms of three functions G, and , three translation operators T 1 , T 2 and T 3 , and a set of recursive equations <ref> [8, 21, 19] </ref>. G is 4 the potential function in explicit Newtonian formulation. <p> term in ` i computes the local field due to the particles in the far-field of the parent node by shifting the center of the expansion from the center of the parent domain 1 T 1 , T 2 and T 3 are defined in Lemmas 3.2.1 through 3.2.3 in <ref> [21] </ref>. 5 to that of the current subdomain. The second term converts the far-field expansions to a local expansion. At the end of this recursion it remains to compute the potential field due to particles in the near-field of subdomain i at level h. <p> It requires O (log N ) time asymptotically for N particles distributed with a fixed number of particles per processor <ref> [7, 21] </ref>. The PMM algorithm performs the computations implied by the expressions (2), (3), (4), (5), and (6). Expression (2) is evaluated concurrently for all leaf nodes. In each such node a multipole expansion due to all particles within the leaf node is formed. <p> The sum of the far-field interaction and the near-field interaction gives the answer. The algorithm for PMM on the Connection Machine is given in Figure 3. 6 (1) global-initialization set-up arrays for , , and intermediate results; precompute difference vectors among tree nodes; precompute shifting arrays by theorem-3-2-2 in <ref> [21] </ref> for local-field-reduction; precompute mathematical tables. (2) far-field-reduction init-expansion at leaf nodes forall nodes i at leaf-level h do compute h i by theorem-3-2-1 in [21]; end multigrid-reduction for level ` from h to 1 step -1 do forall nodes i at level ` do compute ` i by lemma-3-2-1 in <p> Figure 3. 6 (1) global-initialization set-up arrays for , , and intermediate results; precompute difference vectors among tree nodes; precompute shifting arrays by theorem-3-2-2 in <ref> [21] </ref> for local-field-reduction; precompute mathematical tables. (2) far-field-reduction init-expansion at leaf nodes forall nodes i at leaf-level h do compute h i by theorem-3-2-1 in [21]; end multigrid-reduction for level ` from h to 1 step -1 do forall nodes i at level ` do compute ` i by lemma-3-2-1 in [21]; reconfigure CM grid; reduce ` i with + op; end (3) local-field-reduction multigrid-distribution for level ` from 1 to h do forall nodes i <p> for local-field-reduction; precompute mathematical tables. (2) far-field-reduction init-expansion at leaf nodes forall nodes i at leaf-level h do compute h i by theorem-3-2-1 in <ref> [21] </ref>; end multigrid-reduction for level ` from h to 1 step -1 do forall nodes i at level ` do compute ` i by lemma-3-2-1 in [21]; reconfigure CM grid; reduce ` i with + op; end (3) local-field-reduction multigrid-distribution for level ` from 1 to h do forall nodes i at level ` do compute P j2finteractivefield (i)g T 2 ( ` j ) by lemma-3-2-2 in [21]; compute ~ ` i by lemma-3-2-3 in [21]; <p> ` do compute ` i by lemma-3-2-1 in <ref> [21] </ref>; reconfigure CM grid; reduce ` i with + op; end (3) local-field-reduction multigrid-distribution for level ` from 1 to h do forall nodes i at level ` do compute P j2finteractivefield (i)g T 2 ( ` j ) by lemma-3-2-2 in [21]; compute ~ ` i by lemma-3-2-3 in [21]; compute ` i ; distribute ` i with identity op; reconfigure CM grid; end (4) local-interaction forall nodes i at leaf-level h do for j 2 fnear f ield (i)g do for each particle in node i do for each particle in <p> <ref> [21] </ref>; reconfigure CM grid; reduce ` i with + op; end (3) local-field-reduction multigrid-distribution for level ` from 1 to h do forall nodes i at level ` do compute P j2finteractivefield (i)g T 2 ( ` j ) by lemma-3-2-2 in [21]; compute ~ ` i by lemma-3-2-3 in [21]; compute ` i ; distribute ` i with identity op; reconfigure CM grid; end (4) local-interaction forall nodes i at leaf-level h do for j 2 fnear f ield (i)g do for each particle in node i do for each particle in node j do compute G j (i); end <p> This strategy is used in our implementation of the PMM. It was previously used in [16, 17]. 10 4 Implementation 4.1 Overview The first implementation of the PMM on the Connection Machine <ref> [21] </ref> mapped the nodes at all levels of the decomposition tree to processors by a binary encoding of the node addresses. The nodes of the decomposition tree were labeled in a breadth first order. At the time of the first Connection Machine implementation, the grid communication mechanism was not available. <p> The leaf node is represented in the field-wise model, i.e., by 32 Connection Machine processors. In our implementation computation-intensive static data are precomputed. For example, the translation operator T 2 requires that first a set of coefficients (b i;j;k <ref> [21] </ref>) be computed before the convolution against the 's contributed by the interactive-field is carried out. For each subdomain this operation is repeated for every member of its interactive-field (in our case, for up to 189 times). This computation accounts for most of the execution time within the tree. <p> The measured performance of this implementation is 1.46 Gflops/sec on a 64k processor Connection Machine, which is very close to the maximum possible within the high level languages at a virtual processor ratio of one. 5 Results The first implementation of the PMM on the Connection Machine <ref> [21] </ref> required 94 seconds on a 8k CM-2 for the evaluation of the potential field due to 16,000 particles. More than half of the total time was spent in communication. <p> For uniform distributions the cross-over point on a VAX-8600 is in the range of 1000 - 2000 particles with an accuracy of 10 6 . In the three dimensional implementation of the Multipole Method reported in <ref> [21] </ref> the cross-over point is at about 1000 particles. The experiment was carried out on a Symbolics Lisp Machine with the results accurate to 10 4 . In both [9, 21] the Multipole Method restricts the number of particles in a leaf to be less than 8. <p> In the three dimensional implementation of the Multipole Method reported in [21] the cross-over point is at about 1000 particles. The experiment was carried out on a Symbolics Lisp Machine with the results accurate to 10 4 . In both <ref> [9, 21] </ref> the Multipole Method restricts the number of particles in a leaf to be less than 8. The local direct interaction at the leaf node includes the direct interaction within the same leaf node as well as that with adjacent leaf nodes. <p> Assigning several processors to a subdomain favors stopping the recursive subdivision at an earlier stage. 6 Summary The goal of the PMM implementation described here was the efficient use of the Connection Machine architecture. For the implementation reported in <ref> [21] </ref> about 60% of the total execution time was due to interprocessor communication. With a non-adaptive recursive partitioning of the domain the data interaction is defined by a pyramid in which each parent node has degree eight in three dimensions.
Reference: [22] <author> Feng Zhao and S. Lennart Johnsson. </author> <title> The parallel multipole method on the Connec--tion Machine. </title> <type> Technical Report CS89-6, </type> <institution> Thinking Machines Corp., </institution> <month> October </month> <year> 1989, </year> <type> Technical Report, </type> <institution> YALEU/DCS/RR-749, Department of Computer Science, Yale University, </institution> <month> October </month> <year> 1989. </year> <month> 23 </month>
References-found: 22

