URL: http://www.cs.helsinki.fi/~tirri/ecml98batch.ps.gz
Refering-URL: http://www.cs.helsinki.fi/~tirri/publications.html
Root-URL: 
Phone: 26,  
Title: Batch Classifications with Discrete Finite Mixtures  
Author: Petri Kontkanen, Petri Myllymaki, Tomi Silander, and Henry Tirri 
Address: P.O.Box  FIN-00014 University of Helsinki, Finland  
Affiliation: Complex Systems Computation Group (CoSCo)  Department of Computer Science  
Date: April 24-28,1998.  
Note: To appear in the 10th European Conference on Machine Learning (ECML-98), Chemnitz, Germany,  
Abstract: In this paper we study batch classification problems where multiple predictions are made simultaneously, in contrast to the standard independent classification case, where the predictions are made independently one at a time. The main contribution of this paper is to demonstrate how the standard EM algorithm for finite mixture models can be modified for the batch classification case. In the empirical part of the paper, the results obtained by the batch classification approach are compared to those obtained by independent predictions.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The standard way to fix a finite mixture model is to estimate the values of the latent clustering variable via the Expectation Maximization (EM) algorithm <ref> [1] </ref> (see, e.g., [5]), and then to choose the maximum a posteriori probability (MAP) parameter values. <p> Unfortunately, determining the MAP parameter values ^ fi D exactly is not possible in practice because of the missing data imposed by the latent variable Y . However, the Expectation Maximization (EM) algorithm <ref> [1] </ref> is an iterative algorithm that can be used for finding an approximation of the MAP model. The EM algorithm can also be understood as an unsupervised clustering algorithm, where the estimated values of the latent variable determine the (probabilistic) clusters.
Reference: 2. <author> W. Emde. </author> <title> Inductive learning of characteristic concept descriptions from small sets of classified examples. </title> <editor> In F. Bergadano and L. De Raedt, editors, </editor> <booktitle> Proceedings of the 7th European Conference on Machine Learning (ECML94), </booktitle> <pages> pages 103-121, </pages> <year> 1994. </year>
Reference-contexts: Therefore it is interesting to investigate the trade-off between the advantage of using the increased information available in the query batch, and the disadvantage of increased complexity in the search process. Similar work has been reported in <ref> [2] </ref>, where the unclassified vectors were used as background knowledge for a conceptual-clustering algorithm. In order to study this problem, we use the probabilistic model family of finite mixtures [3, 7], where the problem domain probability distribution is approximated as a finite, weighted sum of simple component distributions.
Reference: 3. <author> B.S. </author> <title> Everitt and D.J. Hand. Finite Mixture Distributions. </title> <publisher> Chapman and Hall, </publisher> <address> London, </address> <year> 1981. </year>
Reference-contexts: Similar work has been reported in [2], where the unclassified vectors were used as background knowledge for a conceptual-clustering algorithm. In order to study this problem, we use the probabilistic model family of finite mixtures <ref> [3, 7] </ref>, where the problem domain probability distribution is approximated as a finite, weighted sum of simple component distributions. <p> In the discrete variable case, the finite mixture <ref> [3, 7] </ref> distribution for a data instantiation d can be written as P (d) = k=1 P (Y = y k ) i=1 ! where Y denotes a latent clustering random variable, the values of which are not given in the data D, K is the number of possible values of
Reference: 4. <author> P. Kontkanen, P. Myllymaki, T. Silander, H. Tirri, and P. Grunwald. </author> <title> Comparing predictive inference methods for discrete domains. </title> <booktitle> In Proceedings of the Sixth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 311-318, </pages> <address> Ft. Lauderdale, Florida, </address> <month> January </month> <year> 1997. </year> <note> Also: NeuroCOLT Technical Report NC-TR-97-004. </note>
Reference-contexts: This observation seems even more plausible as it is known that for many of the UCI data sets a rather small sample of the actual training data is enough for building a good predictive model (see the discussion in <ref> [4] </ref>). 5 Conclusion We have studied the batch classification problem where multiple predictions can be made simultaneously, instead of performing the classifications independently one at a time.
Reference: 5. <author> P. Kontkanen, P. Myllymaki, and H. Tirri. </author> <title> Constructing Bayesian finite mixture models by the EM algorithm. Technical Report NC-TR-97-003, </title> <booktitle> ESPRIT Working Group 8556: Neural and Computational Learning (NeuroCOLT), </booktitle> <year> 1996. </year>
Reference-contexts: The standard way to fix a finite mixture model is to estimate the values of the latent clustering variable via the Expectation Maximization (EM) algorithm [1] (see, e.g., <ref> [5] </ref>), and then to choose the maximum a posteriori probability (MAP) parameter values. <p> Detailed derivation of these formulas is similar to the derivations used in <ref> [5] </ref>, but technically somewhat involved and omitted here. In the M-step, the parameter values are updated in such a way that the obtained expected posterior is maximized (for the update formulas, see e.g. [5]). 4 Empirical results To validate the batch classification approach described in the previous section, we performed a <p> Detailed derivation of these formulas is similar to the derivations used in <ref> [5] </ref>, but technically somewhat involved and omitted here. In the M-step, the parameter values are updated in such a way that the obtained expected posterior is maximized (for the update formulas, see e.g. [5]). 4 Empirical results To validate the batch classification approach described in the previous section, we performed a series of experiments with a set of public domain classification datasets from the UCI repository 1 .
Reference: 6. <author> H. Tirri, P. Kontkanen, and P. Myllymaki. </author> <title> Probabilistic instance-based learning. </title> <editor> In L. Saitta, editor, </editor> <booktitle> Machine Learning: Proceedings of the Thirteenth International Conference, </booktitle> <pages> pages 507-515. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Description of the datasets used, and the crossvalidated classification results obtained can be found in Table 1. The results are averages over 100 independent crossvalidation runs, and the number of folds used was the same as in <ref> [6] </ref>. The results show that the batch classification approach does not demonstrate significant improvement over independent predictions. The reasons for this are twofold.
Reference: 7. <author> D.M. Titterington, A.F.M. Smith, and U.E. Makov. </author> <title> Statistical Analysis of Finite Mixture Distributions. </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1985. </year>
Reference-contexts: Similar work has been reported in [2], where the unclassified vectors were used as background knowledge for a conceptual-clustering algorithm. In order to study this problem, we use the probabilistic model family of finite mixtures <ref> [3, 7] </ref>, where the problem domain probability distribution is approximated as a finite, weighted sum of simple component distributions. <p> In the discrete variable case, the finite mixture <ref> [3, 7] </ref> distribution for a data instantiation d can be written as P (d) = k=1 P (Y = y k ) i=1 ! where Y denotes a latent clustering random variable, the values of which are not given in the data D, K is the number of possible values of
References-found: 7

