URL: ftp://ftp.cs.umass.edu/pub/techrept/techreport/1992/UM-CS-1992-008.ps
Refering-URL: 
Root-URL: 
Title: Multivariate versus Univariate Decision Trees  
Author: Carla E. Brodley Paul E. Utgoff 
Address: Amherst, Massachusetts 01003 USA  
Affiliation: Department of Computer Science University of Massachusetts  
Abstract: COINS Technical Report 92-8 January 1992 Abstract In this paper we present a new multivariate decision tree algorithm LMDT, which combines linear machines with decision trees. LMDT constructs each test in a decision tree by training a linear machine and then eliminating irrelevant and noisy variables in a controlled manner. To examine LMDT's ability to find good generalizations we present results for a variety of domains. We compare LMDT empirically to a univariate decision tree algorithm and observe that when multivariate tests are the appropriate bias for a given data set, LMDT finds small accurate trees. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: This reduces the time required to search for a set of features by a factor of n; instead of comparing n linear machines, where n is the number of features in the linear machine, LMDT compares two linear machines. CART <ref> (Breiman, et al. 1984) </ref> and PT2 (Utgoff & Brodley, 1990) both perform a SBS search for the best set of features to use as a test in the decision tree. CART searches at each node for the linear discriminant that maximizes the reduction of a discrete impurity measure.
Reference: <author> Brodley, C. E., & Utgoff, P. E. </author> <year> (1992). </year> <title> Multivariate versus univariate decision trees, </title> <type> (Coins Technical Report 92-8), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer and Information Science. </institution>
Reference-contexts: Specifically, rather than search the space of multivariate tests using a fixed bias (like LMDT), such a system would have the capability to focus its search using heuristic measures of the learning process <ref> (Brodley & Utgoff, 1992) </ref>. The problem of bias is not restricted to decision trees.
Reference: <author> Cover, T. M. </author> <year> (1973). </year> <title> Enumerative source coding. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-19, </volume> <pages> 73-77. </pages> <note> Detrano,R., </note> <author> Janosi,A., Steinbrunn,W., Pfisterer, M., Schmid, J., Sandhu, S., Guppy, K., Lee, S., & Froelicher, V. </author> <year> (1989). </year> <title> International application of a new probability algorithm for the diagnosis of coronary artery disese. </title> <journal> American Journal of Cardiology, </journal> <volume> 64, </volume> <pages> 304-310. </pages>
Reference: <author> Duda, R. O., & Fossum, H. </author> <year> (1966). </year> <title> Pattern classification by iteratively determined linear and piecewise linear discriminant functions. </title> <journal> IEEE Transactions on Electronic Computers, </journal> <volume> EC-15, </volume> <pages> 220-232. </pages>
Reference-contexts: One well known method, for training a linear machine, is the absolute error correction rule <ref> (Duda & Fossum, 1966) </ref>, which adjusts W i , where i is the class to which the instance belongs, and W j , where j is the class to which the linear machine incorrectly assigns the instance.
Reference: <author> Duda, R. O., & Hart, P. E. </author> <year> (1973). </year> <title> Pattern classification and scene analysis. </title> <address> New York: </address> <publisher> Wiley & Sons. </publisher>
Reference-contexts: This criterion for when to reduce fi is motivated by the fact that the magnitude of the linear machine increases rapidly during the early training, stabilizing when the decision boundary is near its final location <ref> (Duda & Hart, 1973) </ref>. 2.3 Eliminating Variables In the interest of producing an accurate and understandable tree that does not evaluate unnecessary variables, one wants to eliminate variables that do not contribute to classification accuracy at a node. <p> In the second case the algorithm is avoiding underfitting the data and will eliminate variables until the number of instances is greater than the capacity of a hyperplane <ref> (Duda & Hart, 1973) </ref>. To this end, if the number of unique instances is not twice the dimensionality of each instance, then the linear machine with fewer variables is preferred. <p> A thermal linear machine avoids interaction between different linear functions. In contrast, a least-mean-squares training rule <ref> (Duda & Hart, 1973) </ref> would place a boundary through the middle of the data, thereby obscuring the structure.
Reference: <author> Elias, P. </author> <year> (1975). </year> <title> Universal codeword sets and representations of the integers. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-21, </volume> <pages> 194-203. </pages>
Reference: <author> Frean, M. </author> <year> (1990). </year> <title> Small nets and short paths: Optimising neural computation. </title> <type> Doctoral dissertation, </type> <institution> Center for Cognitive Science, University of Edinburgh. </institution>
Reference: <author> Frey, P. W., & Slate, D. J. </author> <year> (1991). </year> <title> Letter recognition using holland-style adaptive classifiers. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <pages> 161-182. </pages>
Reference-contexts: One class is linearly separable from the other two, and the latter two are not linearly separable from each other. For the Letter Recognition task the objective is to identify a black-and-white rectangular pixel display as one of the 26 capital letters in the English alphabet <ref> (Frey & Slate, 1991) </ref>. In the pixel segmentation domain the task is to learn to segment an image into seven classes. Each instance is the average of a 3x3 grid of pixels represented by 19 low-level, real-valued image features. <p> The data were split randomly for each run, with the same split used for both algorithms. For the letter-recognition task, we used the traditional training set of 16,000 instances and the test set of 4,000 instances <ref> (Frey & Slate, 1991) </ref>.
Reference: <author> Gallant, S. I. </author> <year> (1986). </year> <title> Optimal linear discriminants. </title> <booktitle> Proceedings of the International Conference on Pattern Recognition (pp. </booktitle> <pages> 849-852). </pages> <publisher> IEEE Computer Society Press. </publisher> <address> 13 Hunt, </address> <note> E., </note> <author> Marin, J,, & Stone, P. </author> <year> (1966). </year> <title> Experiments in induction. </title> <publisher> Academic Press Inc.. </publisher>
Reference-contexts: LMDT and PT2 both recalculate the weights after each elimination. However, PT2's training procedure for finding weights can be computationally prohibitive because if one is using the absolute error correction rule without thermal training, in conjunction with the Pocket Algorithm <ref> (Gallant, 1986) </ref>, it is uncertain how long it will take to find the optimal weight vector or even a good weight vector. 3 An Empirical Comparison of LMDT to C4.5 To examine LMDT's ability to find a good generalization of the examples this section provides an empirical evaluation of the LMDT
Reference: <author> Kittler, J. </author> <year> (1986). </year> <title> Feature selection and extraction. </title> <editor> In Young & Fu (Eds.), </editor> <booktitle> Handbook of pattern recognition and image processing. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: To this end, LMDT's dispersion measure computes for each variable the average squared distance between the weights of each pair of classes and then eliminates the variable 5 that has the smallest dispersion. This measure is analogous to the Euclidean interclass distance measure for estimating error <ref> (Kittler, 1986) </ref>. A thermal linear machine has converged when the magnitude of each correction to the linear machine is larger than the amount permitted by the thermal training rule for each instance in the training set. However, one does not need to wait until convergence to begin discarding variables. <p> A criterion function is a figure of merit reflecting the amount of classification information conveyed by a feature <ref> (Kittler, 1986) </ref>. However, instead of selecting the feature to remove by finding which feature causes the minimum decrease of some criterion function, LMDT removes the feature with the lowest weight dispersion.
Reference: <author> Nilsson, N. J. </author> <year> (1965). </year> <title> Learning machines. </title> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1987). </year> <title> Decision trees as probabilistic classifiers. </title> <booktitle> Proceedings of the Fourth International Workshop on Machine Learning (pp. </booktitle> <pages> 31-37). </pages> <address> Irvine, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: To understand under what circumstances the bias of a multivariate tree (and LMDT's search bias for finding such a tree) is more appropriate than the bias of a univariate decision tree we compare LMDT to a univariate decision tree algorithm, C4.5 <ref> (Quinlan, 1987) </ref>, across these tasks. The results of this comparison show that each approach has a selective superiority; for some of the tasks LMDT finds significantly more accurate trees than C4.5 and for others the reverse is true.
Reference: <author> Rissanen, J. </author> <year> (1989). </year> <title> Stochastic complexity in statistical inquiry. </title> <address> New Jersey: </address> <publisher> World Scientific. </publisher>
Reference-contexts: To compare the size of the trees, we use the Minimum Description Length Principle (MDLP) <ref> (Rissanen, 1989) </ref>, which states that the best "hypothesis" to induce from a data set is the one that minimizes the length of the hypothesis plus the length of the data when coded using the hypothesis to predict the data.
Reference: <author> Utgoff, P. E. </author> <year> (1989). </year> <title> Perceptron trees: A case study in hybrid concept representations. </title> <journal> Connection Science, </journal> <volume> 1, </volume> <pages> 377-391. </pages>
Reference-contexts: A solution to this problem would be to determine the appropriate bias dynamically for each test in the tree. The perceptron tree algorithm <ref> (Utgoff, 1989) </ref> is one example of a system that tries to determine the appropriate representational bias for the instances automatically. Specifically, the algorithm first tries to fit a linear threshold unit (LTU) to the space of instances.
Reference: <author> Utgoff, P. E., & Brodley, C. E. </author> <year> (1990). </year> <title> An incremental method for finding multivariate splits for decision trees. </title> <booktitle> Proceedings of the Seventh International Conference on Machine Learning (pp. </booktitle> <pages> 58-65). </pages> <address> Austin, TX: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The linear machine algorithm requires that all variables be numeric and therefore, LMDT encodes symbolic variables to numeric variables using the same method as PT2, ensuring that no order is placed on these variables <ref> (Utgoff & Brodley, 1990) </ref>. The encoded symbolic and numeric variables are normalized automatically at each node. <p> This reduces the time required to search for a set of features by a factor of n; instead of comparing n linear machines, where n is the number of features in the linear machine, LMDT compares two linear machines. CART (Breiman, et al. 1984) and PT2 <ref> (Utgoff & Brodley, 1990) </ref> both perform a SBS search for the best set of features to use as a test in the decision tree. CART searches at each node for the linear discriminant that maximizes the reduction of a discrete impurity measure.
Reference: <author> Weiss, S. M., & Kulikowski, C. S. </author> <year> (1991). </year> <title> Computer systems that learn. </title> <address> Palo Alto: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Various performance measures for each of the tasks are reported in Table 3. Each reported measure is the average of ten runs. To achieve an estimate of the true error rate, for five of the tasks, we performed a ten-fold crossvalidation for each run <ref> (Weiss & Kulikowski, 1991) </ref>. The data were split randomly for each run, with the same split used for both algorithms. For the letter-recognition task, we used the traditional training set of 16,000 instances and the test set of 4,000 instances (Frey & Slate, 1991).
References-found: 17

