URL: http://www.iscs.nus.sg/~plong/papers/comp_func.ps
Refering-URL: 
Root-URL: 
Title: On the Complexity of Function Learning  
Author: PETER AUER PHILIP M. LONG WOLFGANG MAASS GERHARD J. WOEGINGER Editor: Sally A. Goldman 
Keyword: Computational learning theory, on-line learning, mistake-bounded learning, function learning  
Address: Klosterwiesgasse 32/2 A-8010 Graz, Austria  
Affiliation: Institute for Theoretical Computer Science Technische Universitat Graz  
Note: 1-50 c Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Email: pauer@igi.tu-graz.ac.at  plong@igi.tu-graz.ac.at  maass@igi.tu-graz.ac.at  gwoegi@igi.tu-graz.ac.at  
Date: Received May 1, 1991  
Abstract: The majority of results in computational learning theory are concerned with concept learning, i.e. with the special case of function learning for classes of functions with range f0; 1g. Much less is known about the theory of learning functions with a larger range such as IN or IR. In particular relatively few results exist about the general structure of common models for function learning, and there are only very few nontrivial function classes for which positive learning results have been exhibited in any of these models. We introduce in this paper the notion of a binary branching adversary tree for function learning, which allows us to give a somewhat surprising equivalent characterization of the optimal learning cost for learning a class of real-valued functions (in terms of a max-min definition which does not involve any "learning" model). Another general structural result of this paper relates the cost for learning a union of function classes to the learning costs for the individual function classes. Furthermore, we exhibit an efficient learning algorithm for learning convex piecewise linear functions from IR d into IR. Previously, the class of linear functions from IR d into IR was the only class of functions with multi-dimensional domain that was known to be learnable within the rigorous framework of a formal model for on-line learning. Finally we give a sufficient condition for an arbitrary class F of functions from IR into IR that allows us to learn the class of all functions that can be written as the pointwise maximum of k functions from F . This allows us to exhibit a number of further nontrivial classes of functions from IR into IR for which there exist efficient learning algorithms. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Auer, P., and Long, P.M. </author> <year> (1994). </year> <title> Simulating access to hidden information while learning. </title> <booktitle> Proceedings of the 26th Annual ACM Symposium on the Theory of Computing. </booktitle>
Reference-contexts: Remark. The special cases of the Theorems in this section for concepts (i.e. Y := f0; 1g) are due to Littlestone (1988) (Theorem 1), Littlestone and Maass (Maass, 1991) (Theorem 2), and Maass and Turan (1992) (Theorem 3). Using methods from <ref> (Auer and Long, 1994) </ref>, one gets a suboptimal but more general bound than that in Theorem 3, proving that LC (B) d log k log 2k if queries of the form "To which of the subclasses P 1 ; : : : ; P k of F does T belong?" are <p> This provides a contradiction to the definition of ADV sign (F ). Remark. It is obvious that for any F, LC-ARB weak (F ) LC-ARB sign (F) LC-ARB (F): Very recently a new proof technique was introduced in <ref> (Auer and Long, 1994) </ref> which shows that for any F Y X , LC-ARB weak (F ) 1:39jY jd1 + log 2 jY jeLC-ARB (F ). 3.
Reference: <author> Auer, P., Long, P.M., Maass, W., and Woeginger, G.J. </author> <year> (1993). </year> <title> On the complexity of function learning. </title> <booktitle> Proceedings of the Sixth Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 392-401. </pages>
Reference-contexts: This paper provides full proofs and more detailed explanations of the results described in the previously published extended abstract <ref> (Auer, Long, Maass and Woeginger, 1993) </ref>. 2. Adversary Trees for Function Learning We introduce in Definition 2 our new notion of an adversary tree for function learning, which allows us to give in Theorem 1 a completely independent ("dual") definition of the learning complexity of a function class.
Reference: <author> Angluin, D. </author> <year> (1988). </year> <title> Queries and concept learning. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 319-342. </pages>
Reference-contexts: From the theoretical point of view this function learning model is a straightforward generalization of the common model for on-line concept learning in a worst case setting, since concepts may be viewed as functions with range f0; 1g (see (Barzdin and Frievald, 1972), <ref> (Angluin, 1988) </ref>, (Littlestone, 1988), (Maass 3 and Turan, 1992)).
Reference: <author> Barland, I. </author> <year> (1992). </year> <title> Some ideas on learning with directional feedback. </title> <type> Master's thesis, </type> <institution> Computer Science Department, UC Santa Cruz. </institution>
Reference: <author> Berlekamp, E.R. </author> <year> (1968). </year> <title> Block coding for the binary symmetric channel with noiseless, delayless feedback. In Error Correcting Codes. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: <author> Barzdin, J.M., and Frievald, R.V. </author> <year> (1972). </year> <title> On the prediction of general recursive functions. </title> <journal> Soviet Math. Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228. </pages>
Reference-contexts: From the theoretical point of view this function learning model is a straightforward generalization of the common model for on-line concept learning in a worst case setting, since concepts may be viewed as functions with range f0; 1g (see <ref> (Barzdin and Frievald, 1972) </ref>, (Angluin, 1988), (Littlestone, 1988), (Maass 3 and Turan, 1992)).
Reference: <author> Cesa-Bianchi, N., Freund, Y., Helmbold, </author> <title> D.P., and Warmuth, </title> <publisher> M.K. </publisher> <year> (1994). </year> <title> On-line prediction and conversion strategies. In Proceedings of the First Euro-COLT Workshop. </title> <journal> The Institute of Mathematics and its Applications, </journal> <note> to appear. </note>
Reference: <author> Cesa-Bianchi, N., </author> <title> Long, P.M., and Warmuth, </title> <publisher> M.K. </publisher> <year> (1993). </year> <title> Worst-case quadratic loss bounds for a generalization of the Widrow-Hoff rule. </title> <booktitle> In Proceedings of the 6th Annual Workshop on Comput. Learning Theory. </booktitle> <volume> 50 Dawid, </volume> <editor> A. </editor> <year> (1984). </year> <title> Statistical theory: The prequential approach. </title> <journal> Journal of the Royal Statistical Society (Series A), </journal> <pages> pages 278-292. </pages>
Reference: <author> Faber, V., and Mycielski, J. </author> <year> (1991). </year> <title> Applications of learning theorems. </title> <journal> Fundamenta Informaticae, </journal> <volume> 15(2) </volume> <pages> 145-167. </pages>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), <ref> (Faber and Mycielski, 1991) </ref>, (Littlestone and Warmuth, 1991), (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), (Littlestone and Warmuth, 1991), (Lit-tlestone, Long and Warmuth, 1991), <ref> (Faber and Mycielski, 1991) </ref>, (Kimber and Long, 1992)) for functions, and in ((Angluin, 1988), (Littlestone, 1988), (Maass and Turan, 1992)) for concepts.
Reference: <author> Feder, M., Merhav, N., and Gutman, M. </author> <year> (1992). </year> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions of Information Theory, </journal> <volume> 38 </volume> <pages> 1258-1270. </pages>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Littlestone and Warmuth, 1991), (Kimber and Long, 1992), <ref> (Feder, Merhav and Gutman, 1992) </ref>, (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, <p> 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, 1990), (Littlestone and Warmuth, 1991), <ref> (Feder, Merhav and Gutman, 1992) </ref>, (Vovk, 1992)). Nevertheless the only interesting class of functions f : IR ! IR for d 2 that has been shown to be efficiently learnable is the class of linear functions ((Mycielski, 1988), (Littlestone, Long and Warmuth, 1991), (Cesa-Bianchi, Long and Warmuth, 1993)).
Reference: <author> Kimber, D. and Long, P.M. </author> <year> (1992). </year> <title> The learning complexity of smooth functions of a single variable. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory. </booktitle>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Littlestone and Warmuth, 1991), <ref> (Kimber and Long, 1992) </ref>, (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), (Littlestone and Warmuth, 1991), (Lit-tlestone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), <ref> (Kimber and Long, 1992) </ref>) for functions, and in ((Angluin, 1988), (Littlestone, 1988), (Maass and Turan, 1992)) for concepts. <p> Note that in contrast to Section 4 the here considered function class F pmax k may also include non-convex functions, and in contrast to <ref> (Kimber and Long, 1992) </ref> the norm of the derivative of functions in this class need not be bounded. Theorem 15 If F is d-defined with d 1 then LC-ARB (F pmax k ) = O (dk). Remark.
Reference: <author> Kearns, M.J., Schapire, R.E., and Sellie, L.M. </author> <year> (1992). </year> <title> Toward efficient agnostic learning. </title> <booktitle> In Proc. 5th Annu. Workshop on Comput. Learning Theory. </booktitle>
Reference-contexts: Notes 1. Some lower bounds require that jY j 2. The case jY j 1 is trivial. 2. This is in contrast to the case of probabilistic models of learning, where efficient algorithms with good learning performance have been discovered for this function class <ref> (Kearns, Schapire and Sellie, 1992) </ref>.
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318. </pages>
Reference-contexts: If h s (x s ) 6= T (x s ), one says that the learner makes a mistake at trial s <ref> (Littlestone, 1988) </ref>. As before, the loss of the learner at round s is measured by `(h s (x); T (x)), and his goal is to minimize the sum of his losses over all rounds. <p> This variation of the learning model is a bit more plausible from the point of view of applications. It is a straightforward generalization of Littlestone's "mistake bounded" model for concept learning <ref> (Littlestone, 1988) </ref>. However as in the case of concept learning (see (Littlestone, 1988)) one can also show very easily for function learning that this variation leads to the same definition of the learning complexity of a function class. <p> This variation of the learning model is a bit more plausible from the point of view of applications. It is a straightforward generalization of Littlestone's "mistake bounded" model for concept learning <ref> (Littlestone, 1988) </ref>. However as in the case of concept learning (see (Littlestone, 1988)) one can also show very easily for function learning that this variation leads to the same definition of the learning complexity of a function class. Hence we prefer to work with the former version of the learning model, which is somewhat simpler to analyze. <p> From the theoretical point of view this function learning model is a straightforward generalization of the common model for on-line concept learning in a worst case setting, since concepts may be viewed as functions with range f0; 1g (see (Barzdin and Frievald, 1972), (Angluin, 1988), <ref> (Littlestone, 1988) </ref>, (Maass 3 and Turan, 1992)). <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), (Littlestone and Warmuth, 1991), (Lit-tlestone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Kimber and Long, 1992)) for functions, and in ((Angluin, 1988), <ref> (Littlestone, 1988) </ref>, (Maass and Turan, 1992)) for concepts. Starting in Application 2 of this section, we will consider only the discrete loss function ` defined by `(a; y) = 0 if a = y, and `(a; y) = 1 if a 6= y.
Reference: <author> Littlestone, N. </author> <year> (1989). </year> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms PhD thesis, </title> <type> Technical Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz. </institution>
Reference: <author> Littlestone, N., </author> <title> Long, P.M., and Warmuth, </title> <publisher> M.K. </publisher> <year> (1991). </year> <title> On-line learning of linear functions. </title> <booktitle> In Proc. of the 23rd Symposium on Theory of Computing. </booktitle>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), <ref> (Littlestone and Warmuth, 1991) </ref>, (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for <p> and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, 1990), <ref> (Littlestone and Warmuth, 1991) </ref>, (Feder, Merhav and Gutman, 1992), (Vovk, 1992)). <p> Nevertheless the only interesting class of functions f : IR ! IR for d 2 that has been shown to be efficiently learnable is the class of linear functions ((Mycielski, 1988), <ref> (Littlestone, Long and Warmuth, 1991) </ref>, (Cesa-Bianchi, Long and Warmuth, 1993)). Coming up with an efficient algorithm for linear functions with a decent loss bound is trivial, but coming up with optimal or near optimal loss bounded algorithms can be difficult. <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), <ref> (Littlestone and Warmuth, 1991) </ref>, (Lit-tlestone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Kimber and Long, 1992)) for functions, and in ((Angluin, 1988), (Littlestone, 1988), (Maass and Turan, 1992)) for concepts. <p> We obtain closed-form bounds for this case that match to within a constant factor, and bounds which are sometimes tighter that are not closed-form. 3.1. Closed-form bounds To establish the closed-form bound, we have found it useful to generalize some of the Weighted Majority <ref> (Littlestone and Warmuth, 1991) </ref> results to functions with arbitrary ranges. Suppose we have (LC-ARB) algorithms A 1 ; :::; A n for some class F of functions from X to Y . <p> By induction, we have v m+1 (3=4) m v 1 : Solving for m yields the desired result. Many other of the Weighted Majority results <ref> (Littlestone and Warmuth, 1991) </ref> appear to generalize just as easily.
Reference: <author> Littlestone, N., and Warmuth, M.K. </author> <year> (1991). </year> <title> The weighted majority algorithm. </title> <type> Technical Report UCSC-CRL-91-28, </type> <institution> UC Santa Cruz. </institution> <note> A preliminary version appeared in the Proceedings of the 30th Annual IEEE Symposium of the Foundations of Computer Science. </note>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), <ref> (Littlestone and Warmuth, 1991) </ref>, (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for <p> and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, 1990), <ref> (Littlestone and Warmuth, 1991) </ref>, (Feder, Merhav and Gutman, 1992), (Vovk, 1992)). <p> Nevertheless the only interesting class of functions f : IR ! IR for d 2 that has been shown to be efficiently learnable is the class of linear functions ((Mycielski, 1988), <ref> (Littlestone, Long and Warmuth, 1991) </ref>, (Cesa-Bianchi, Long and Warmuth, 1993)). Coming up with an efficient algorithm for linear functions with a decent loss bound is trivial, but coming up with optimal or near optimal loss bounded algorithms can be difficult. <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), <ref> (Littlestone and Warmuth, 1991) </ref>, (Lit-tlestone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Kimber and Long, 1992)) for functions, and in ((Angluin, 1988), (Littlestone, 1988), (Maass and Turan, 1992)) for concepts. <p> We obtain closed-form bounds for this case that match to within a constant factor, and bounds which are sometimes tighter that are not closed-form. 3.1. Closed-form bounds To establish the closed-form bound, we have found it useful to generalize some of the Weighted Majority <ref> (Littlestone and Warmuth, 1991) </ref> results to functions with arbitrary ranges. Suppose we have (LC-ARB) algorithms A 1 ; :::; A n for some class F of functions from X to Y . <p> By induction, we have v m+1 (3=4) m v 1 : Solving for m yields the desired result. Many other of the Weighted Majority results <ref> (Littlestone and Warmuth, 1991) </ref> appear to generalize just as easily.
Reference: <author> Long, P.M., and Warmuth, </author> <title> M.K. (in press). Composite geometric concepts and polynomial predictability. </title> <journal> Inform. Comput.. </journal>
Reference: <author> Maass, W. </author> <year> (1991). </year> <title> On-line learning with an oblivious environment and the power of randomization. </title> <booktitle> Proc. 4th Annu. Workshop on Comput. Learning Theory. </booktitle>
Reference-contexts: an oblivious environment We consider in Theorem 2 an alternative model for function learning, where the learning algorithm may use randomization, and the given sequence of pairs hx; T (x)i is fixed before learning takes place and is therefore oblivious to the actions (and the randomization) of the learner (see <ref> (Maass, 1991) </ref>). For this model we write RLC-OBL ` (F; H) in place of LC ` (F ; H). It is easy to generalize the results from (Maass, 1991) to show that in the case ` is the discrete loss function, RLC-OBL ` (F; F) ln jFj for any finite function <p> hx; T (x)i is fixed before learning takes place and is therefore oblivious to the actions (and the randomization) of the learner (see <ref> (Maass, 1991) </ref>). For this model we write RLC-OBL ` (F; H) in place of LC ` (F ; H). It is easy to generalize the results from (Maass, 1991) to show that in the case ` is the discrete loss function, RLC-OBL ` (F; F) ln jFj for any finite function class F. The following limits the amount randomization can help the learner, even in an oblivious environment. <p> This implies the claimed lower bound for the expected total loss of B, since the latter is equal to the sum of the expected losses of B for the individual rounds of the learning process. It was shown in <ref> (Maass, 1991) </ref> that there exist function classes F (in fact: classes of f0; 1g valued functions) such that RLC-OBL ` (F) = 1 2 ADV ` (F). <p> Remark. The special cases of the Theorems in this section for concepts (i.e. Y := f0; 1g) are due to Littlestone (1988) (Theorem 1), Littlestone and Maass <ref> (Maass, 1991) </ref> (Theorem 2), and Maass and Turan (1992) (Theorem 3).
Reference: <author> Maass, W., and Turan, G. </author> <year> (1992). </year> <title> Lower bound methods and separation results for on-line learning models. </title> <journal> Machine Learning, </journal> <volume> 9 </volume> <pages> 107-145. </pages>
Reference-contexts: From the theoretical point of view this function learning model is a straightforward generalization of the common model for on-line concept learning in a worst case setting, since concepts may be viewed as functions with range f0; 1g (see (Barzdin and Frievald, 1972), (Angluin, 1988), (Littlestone, 1988), <ref> (Maass 3 and Turan, 1992) </ref>). <p> These definitions contain as special cases those that were considered in ((Mycielski, 1988), (Littlestone and Warmuth, 1991), (Lit-tlestone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Kimber and Long, 1992)) for functions, and in ((Angluin, 1988), (Littlestone, 1988), <ref> (Maass and Turan, 1992) </ref>) for concepts. Starting in Application 2 of this section, we will consider only the discrete loss function ` defined by `(a; y) = 0 if a = y, and `(a; y) = 1 if a 6= y. <p> f1; :::; ug to f0; 1g and for each v u and for each f 2 POWER u , let B f;v = fg 2 POWER u : j fx : g (x) 6= f (x)g j vg : The following lemma, implicit in the work of Maass and Turan <ref> (Maass and Turan, 1992, Proposition 6.3) </ref> will prove useful. <p> Theorem 9 If p : IN ! IN is defined as in Lemma 7, then '( p (k) times z -| - 1 o (1))(log 2 p (k) + k): 26 Proof: As observed in <ref> (Maass and Turan, 1992) </ref>, for any f , LC-ARB (B f;k ) = k (predict with f on unseen points).
Reference: <author> Mycielski, J. </author> <year> (1988). </year> <title> A learning algorithm for linear operators. </title> <journal> Proceedings of the American Mathematical Society, </journal> <volume> 103(2) </volume> <pages> 547-550. </pages>
Reference-contexts: In ((Dawid, 1984), <ref> (Mycielski, 1988) </ref> , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Littlestone and Warmuth, 1991), (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and
Reference: <author> Rivest, R.L., Meyer, A.R., Kleitman, D.J., Winklmann, K., and Spencer, J. </author> <year> (1980). </year> <title> Coping woth errors in binary search procedures. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 20 </volume> <pages> 396-404. </pages>
Reference-contexts: Our proof proceeds through a series of lemmas. For the upper bound, we use a technique that was developed for coding theory and extended to address the problem of searching using a comparator that occasionally lies ((Berlekamp, 1968), <ref> (Rivest, Meyer, Kleitman, Winklman and Spencer, 1980) </ref>, (Spencer, 1992)).
Reference: <author> Sauer, N. </author> <year> (1972). </year> <title> On the density of families of sets. </title> <journal> J. Combinatorial Theory (A), </journal> <volume> 13 </volume> <pages> 145-147. </pages>
Reference: <author> Spencer, J. </author> <year> (1992). </year> <title> Ulam's searching game with a fixed number of lies. </title> <journal> Theoretical Computer Science, </journal> <volume> 95(2) </volume> <pages> 307-321. </pages>
Reference-contexts: Our proof proceeds through a series of lemmas. For the upper bound, we use a technique that was developed for coding theory and extended to address the problem of searching using a comparator that occasionally lies ((Berlekamp, 1968), (Rivest, Meyer, Kleitman, Winklman and Spencer, 1980), <ref> (Spencer, 1992) </ref>). Cesa-Bianchi, Freund, Helmbold and Warmuth (1994) showed that the proof of (Spencer, 1992) could be modified to determine the (in some cases only nearly) best bound on the number of mistakes for learning a class of f0; 1g-valued functions, given that an unknown algorithm from a pool fA 1 <p> For the upper bound, we use a technique that was developed for coding theory and extended to address the problem of searching using a comparator that occasionally lies ((Berlekamp, 1968), (Rivest, Meyer, Kleitman, Winklman and Spencer, 1980), <ref> (Spencer, 1992) </ref>). Cesa-Bianchi, Freund, Helmbold and Warmuth (1994) showed that the proof of (Spencer, 1992) could be modified to determine the (in some cases only nearly) best bound on the number of mistakes for learning a class of f0; 1g-valued functions, given that an unknown algorithm from a pool fA 1 ; :::; A p g would make at most k mistakes.
Reference: <author> Uspensky, J.V. </author> <year> (1948). </year> <title> Theory of Equations. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: The d-definedness of these classes (for suitable d 2 IN) follows from the fact that any polynomial with m terms has at most 2m + 1 zeros (by <ref> (Uspensky, 1948, p. 121) </ref>). Functions P k i=1 a i (b i x+c i ) can be rewritten as "rational" functions P k a i 1+C i y b i where C i = e c i and y = e x . <p> Observe that b i is not integer but that the result of <ref> (Uspensky, 1948) </ref> can be generalized to this case. For the class of quotients of sparse polynomials a suitable d is 4kl + 1 when nominator and denominator have at most k and l terms, respectively, and for sums of k sigmoids a suitable d is 4k2 2k1 + 1.
Reference: <author> Vovk, V. </author> <year> (1990). </year> <title> Aggregating strategies. </title> <booktitle> In Proc. 3rd Annu. Workshop on Comput. Learning Theory. </booktitle>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , <ref> (Vovk, 1990) </ref>, (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Littlestone and Warmuth, 1991), (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), (Vovk, 1992), (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds
Reference: <author> Vovk, V. </author> <year> (1992). </year> <title> Universal forecasting algorithms. </title> <journal> Inform. Comput., </journal> <volume> 96(2) </volume> <pages> 245-277. </pages>
Reference-contexts: In ((Dawid, 1984), (Mycielski, 1988) , (Vovk, 1990), (Little-stone, Long and Warmuth, 1991), (Faber and Mycielski, 1991), (Littlestone and Warmuth, 1991), (Kimber and Long, 1992), (Feder, Merhav and Gutman, 1992), <ref> (Vovk, 1992) </ref>, (Cesa-Bianchi, Long and Warmuth, 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, 1990), (Littlestone and Warmuth, 1991), <p> 1993)), the learning complexity of various concrete function classes have been investigated in this function learning model, and cumulative loss bounds in terms of certain properties of F have been proved for general-purpose (but not necessarily computationally efficient) algorithms ((Vovk, 1990), (Littlestone and Warmuth, 1991), (Feder, Merhav and Gutman, 1992), <ref> (Vovk, 1992) </ref>). Nevertheless the only interesting class of functions f : IR ! IR for d 2 that has been shown to be efficiently learnable is the class of linear functions ((Mycielski, 1988), (Littlestone, Long and Warmuth, 1991), (Cesa-Bianchi, Long and Warmuth, 1993)).
References-found: 26

