URL: ftp://ftp.ics.hawaii.edu/pub/tr/ics-tr-97-03.ps.Z
Refering-URL: ftp://ftp.ics.hawaii.edu/pub/tr/INDEX.html
Root-URL: 
Email: email: Chin@Hawaii.Edu  
Title: CHAPTER 9 INTELLIGENT INTERFACES AS AGENTS  
Author: DAVID N. CHIN Joseph W. Sullivan and Sherman W. Tyler 
Date: 127-206.  
Note: 0 This is a reprint of chapter 9 in  (Eds.), Intelligent User Interfaces, Addison-Wesley, Reading, MA, 1991, pp.  
Address: Manoa 2565 The Mall Honolulu, HI 96822  
Affiliation: Department of Information and Computer Sciences University of Hawaii at  
Abstract: An intelligent interface cannot just respond passively to its user's commands and queries. It must be able to take the initiative in order to volunteer information, correct user misconceptions, or reject unethical user requests. To do these things, a system must be an intelligent agent. UCEgo is the intelligent agent component of UC (UNIX Consultant), a natural-language system that helps the user solve problems in using the UNIX operating system. UCEgo provides UC with its own goals and plans. By adopting different goals in different situations, UCEgo creates and executes different plans, enabling it to interact intelligently with the user. UCEgo adopts goals from its themes, adopts subgoals during planning, and adopts metagoals for dealing with goal interactions. It also adopts goals when it notices that the user either lacks necessary knowledge or has incorrect beliefs. In these cases, UCEgo plans to volunteer information or correct the user's misconception, as appropriate. 0 This research was supported in part by the Office of Naval Research, under grant N0014-48-C-0732; the Defense Advanced Research Projects Agency (DoD), under Darpa Order No. 4871, monitored by Space and Naval Warfare Systems Command under Contract N00039-84-C-0089; the National Science Foundation under grant #85-14890; and Hughes Aircraft grant #442427-59868. 
Abstract-found: 1
Intro-found: 1
Reference: [Allen79] <author> Allen, J. F., </author> <year> 1979. </year> <title> A Plan-Based Approach to Speech Act Recognition. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Science, University of Toronto. </institution> <note> Also available as Technical Report 131, </note> <institution> University of Toronto. </institution>
Reference-contexts: Although considerable work has been done in the area of planning, very few planning systems have addressed the problem of how to detect appropriate goals for planning. In almost all planning systems, the high-level goals are provided by the human operators of the planners. An exception is described by <ref> [Allen79] </ref>, whose system simulated a train station ticket agent. It detected goals based on an analysis of obstacles to a user's plans. By addressing these obstacles, the system could volunteer information that the user would need to achieve the user's plan.
Reference: [Austin62] <author> Austin, J. L., </author> <year> 1962. </year> <title> How to Do Things with Words. </title> <publisher> London:Oxford University Press. </publisher>
Reference-contexts: The approach that I take is to view the program as an agent. That is, a consultation system should be viewed as a system that can perform actions. For a natural-language consultation system, acting consists of mostly speech acts <ref> [Austin62, Searle69] </ref>, that is, acting by communicating with the user. Within this paradigm, taking the initiative in a dialogue translates into acting without the guidance of the user. An agent that has this capability of taking the initiative is called an autonomous agent.
Reference: [Carbonell70a] <author> Carbonell, J. R., </author> <year> 1970. </year> <title> Mixed-Initiative Man-Computer Instructional Dialogues. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology. </institution> <note> Also available as Technical Report 1971, </note> <institution> Bolt Beranek and Newman. </institution>
Reference-contexts: Previous efforts in natural-language systems that take the initiative have resulted in programs capable of mixed-initiative dialogues. Among the first of these, the SCHOLAR system <ref> [Carbonell70a, Carbonell70b] </ref> for CAI (Computer-Aided Instruction), could take the initiative to test the user on facts in its knowledge base. It also allowed the user to query the system about facts in its knowledge base. This type of mixed-initiative works only for limited situations, such as mutual quizzing.
Reference: [Carbonell70b] <author> Carbonell, J. R., </author> <year> 1970. </year> <title> Ai in CAI: An artificial-intelligence approach to computer assisted instruction. </title> <journal> IEEE Transactions on Man-Machine-Systems, </journal> <volume> MMS-11 </volume> (4):190-202. 
Reference-contexts: Previous efforts in natural-language systems that take the initiative have resulted in programs capable of mixed-initiative dialogues. Among the first of these, the SCHOLAR system <ref> [Carbonell70a, Carbonell70b] </ref> for CAI (Computer-Aided Instruction), could take the initiative to test the user on facts in its knowledge base. It also allowed the user to query the system about facts in its knowledge base. This type of mixed-initiative works only for limited situations, such as mutual quizzing.
Reference: [Carbonell82] <author> Carbonell, J. G., </author> <year> 1982. </year> <booktitle> Where do goals come from? In Proceedings of the Fourth Annual Conference of the Cognitive Science Society, p. </booktitle> <pages> 191-194, </pages> <address> Ann Arbor, MI. </address>
Reference-contexts: For story understanding, PAM was able to recognize a character's goals when directly stated in the narrative, or when the goal could be inferred from the characters' stated actions based on the assumption that the actions were part of the character's plan. As <ref> [Carbonell82] </ref> points out, none of these systems have systematically addressed the problem of goal detection, which is essential for building intelligent agents. This paper will address the problem of building a natural-language computer consultation system that behaves as an intelligent agent.
Reference: [Chin87] <author> Chin, D. N., </author> <year> 1987. </year> <title> Intelligent Agents as a Basis for Natural Language Interfaces. </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley. </institution> <note> Also available as Technical Report UCB/CSD 88/396, </note> <institution> Computer Science Division, University of California, Berkeley. </institution> <month> 23 </month>
Reference-contexts: UC is able to provide information about how to do things in UNIX, definitions about UNIX or general operating system terminology, and help in debugging problems with using UNIX. A short overview of UC follows. For more details on UC, the reader is referred to <ref> [Chin87] </ref>, [Wilensky86], and [Wilensky84]. In a typical UC session, the user types questions in English to UC, and UC responds to the user in English. <p> In other types of programs, a different focus may lead to other situation classes. In UCEgo, each situation class is represented by a single KODIAK network pattern, which is used as the left-hand-side of an if-detected demon <ref> [Chin87] </ref>. When a piece of KODIAK network matches such a pattern, the network is considered to represent a situation of the class, and the demon adds a new goal (also represented as a piece of KODIAK network) to UC's memory. <p> This is an example of metaplanning [Wilensky83], because UC is planning to create a plan that is used to find another plan. For more details on metaplanning in UCEgo, see <ref> [Chin87] </ref>. 9.8.1. Mutual Inclusion Metagoals are used to deal with both positive and negative interactions between goals. One way in which goals can interact positively is through mutual inclusion [Wilensky83]. This describes situations in which a planner has the same or similar goals for different reasons. <p> This is done because the proposition, cp is a plan for copying files, is already part of the context (it is part of the user's query). Such processing is done by the UCExpress component <ref> [Chin87] </ref>. 9.8.2. Goal Conflict Goals can interact negatively by conflicting. When UCEgo detects a situation in which UC has two goals that conflict with one another, UCEgo adopts the metagoal of resolving the goal conflict. <p> This goal is a background goal <ref> [Chin87] </ref>, which means that UCEgo does not actively attempt to plan for the goal. Rather, the goal is considered only when UCEgo detects a relevant situation, such as in the present example. <p> At this point, UC suspects that the user may have a misconception. In order to see whether the user actually has a misconception or whether UC just lacks information about this particular option of this command, UC checks with the KNOME subcomponent <ref> [Chin87, Chin89] </ref>, which models the limitations of UC's knowledge base using metaknowledge. If KNOME has metaknowledge that UC knows all the options of this command, then UC knows that the user has a misconception.
Reference: [Chin89] <author> Chin, D. N., </author> <year> 1989. </year> <title> Knome: Modeling what the user knows in uc. </title> <editor> In Kobsa, A. and Wahlster, W., editors, </editor> <booktitle> User Models in Dialog Systems, p. </booktitle> <pages> 74-107. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: At this point, UC suspects that the user may have a misconception. In order to see whether the user actually has a misconception or whether UC just lacks information about this particular option of this command, UC checks with the KNOME subcomponent <ref> [Chin87, Chin89] </ref>, which models the limitations of UC's knowledge base using metaknowledge. If KNOME has metaknowledge that UC knows all the options of this command, then UC knows that the user has a misconception.
Reference: [Faletti82] <author> Faletti, J., </author> <year> 1982. </year> <title> PANDORA a program for doing commonsense planning in complex situations. </title> <booktitle> In Proceedings of the Second Annual National Conference on Artificial Intelligence, p. </booktitle> <pages> 185-188, </pages> <address> Pittsburg, PA. </address>
Reference-contexts: Even in terms of volunteering useful information to the user, an analysis of obstacles to a user's plans does not address the problem of when the system should volunteer an alternative plan. The PANDORA planner <ref> [Faletti82] </ref> detected its own goals. It detected goals when actual or projected states conflicted with goals or plans and when certain frames describing situations were activated.
Reference: [Fikes71] <author> Fikes, R. E. and Nilsson, N. J., </author> <year> 1971. </year> <title> STRIPS: a new approach to the, application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 (3-4):189-208. </volume>
Reference-contexts: For example, PAM [Wilensky78] understood stories involving rational agents by analyzing the goals and plans of the characters. Also, TALE-SPIN [Meehan76] used plans and goals to create simple stories with rational agents as characters. In the problem-solving domain, robot-planning programs (e.g., <ref> [Fikes71, Sacerdoti74] </ref>) have shown that planning is a good paradigm for programming robots as rational agents. In the realm of conversation, [Hobbs80] has argued 4 that human conversation fits such a paradigm.
Reference: [Hobbs80] <author> Hobbs, J. R. and Evans, D. A., </author> <year> 1980. </year> <title> Conversation as planned behavior. </title> <journal> Cognitive Science, </journal> <volume> 4 </volume> (4):349-377. 
Reference-contexts: Also, TALE-SPIN [Meehan76] used plans and goals to create simple stories with rational agents as characters. In the problem-solving domain, robot-planning programs (e.g., [Fikes71, Sacerdoti74]) have shown that planning is a good paradigm for programming robots as rational agents. In the realm of conversation, <ref> [Hobbs80] </ref> has argued 4 that human conversation fits such a paradigm. So, a program that contains reasonable plans and goals and whose behavior is determined by those plans and goals can be considered a rational agent.
Reference: [Kaplan83] <author> Kaplan, S. J., </author> <year> 1983. </year> <title> Cooperative responses from a portable natural language database query system. </title> <editor> In Brady, M. and Berwick, R. C., editors, </editor> <booktitle> Computational Models of Discourse. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [Luria85] <author> Luria, M., </author> <year> 1985. </year> <title> Commonsense planning in a consultant system. </title> <booktitle> In Proceedings, 1985 IEEE International Conference on Systems, Man, and Cybernetics, p. </booktitle> <pages> 602-606, </pages> <address> Tuscon, AR. </address>
Reference: [Luria87] <author> Luria, M., </author> <year> 1987. </year> <title> Expressing concern. </title> <booktitle> In Proceedings of the 25th Annual Meeting of the Association for Computational Linguistics, p. </booktitle> <pages> 221-227, </pages> <address> Stanford, CA. </address>
Reference-contexts: This can be contrasted to other preconditions of rmdir such as the need for write permission on the directory containing the directory to be deleted. This precondition is much less frequently violated, so it is need not be mentioned to the user. Such frequency information is stored as concerns <ref> [Luria87] </ref> by the UNIX domain planning component of UC. When the UNIX domain planner creates a plan, it passes along to UCEgo those preconditions that have a high degree of concern. <p> Determine that the user is likely to forget/ignore the information 3. Inform the user if the system believes that the user needs to be reminded of the information Whether a user is likely to forget or ignore information is represented in UC in a fashion similar to concerns <ref> [Luria87] </ref>. Facts that users are likely to forget or ignore are marked as needing reminders. Because UC will provide a reminder-type suggestion regardless of whether the user already knows the information, there is not the need to check for this as there is in other types of suggestions.
Reference: [Luria88] <author> Luria, M., </author> <year> 1988. </year> <title> Knowledge Intensive Planning. </title> <type> Ph.D. thesis, </type> <institution> University of California, Berkeley. </institution>
Reference: [Marburger86] <author> Marburger, H., </author> <year> 1986. </year> <title> A strategy for producing cooperative nl reactions in a database interface. </title> <booktitle> In Proceedings of AIMSA-86, </booktitle> <address> Wana, Bulgaria. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [Mays80] <author> Mays, E., </author> <year> 1980. </year> <title> Failures in natural language systems: Applications to data base query systems. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, p. </booktitle> <pages> 327-330, </pages> <address> Stanford, CA. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [McCoy85] <author> McCoy, K. F., </author> <year> 1985. </year> <title> Correcting Object-Related Misconceptions. </title> <type> Ph.D. thesis, </type> <institution> University of Pennsylvania, Department of Computer and Information Science, Moore School. </institution> <note> Also available as Technical Report MS-CIS-85-57. </note>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception. <p> Currently, UC handles only relational misconceptions, that is, misconceptions in which the user believes a relation holds between two objects when, in fact, such a relation cannot hold or does not happen to hold between those particular objects. UC does not handle object-oriented misconceptions, such as those for which <ref> [McCoy85] </ref> discusses correction strategies. In processing the user's query, UC checks to see whether all relations mentioned by the user in the user's query have a counterpart in UC's knowledge base.
Reference: [McCoy89] <author> McCoy, K. F., </author> <year> 1989. </year> <title> Highlighting a user model to respond to misconceptions. </title> <editor> In Kobsa, A. and Wahlster, W., editors, </editor> <booktitle> User Models in Dialog Systems, p. </booktitle> <pages> 233-254. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [Meehan76] <author> Meehan, J. R., </author> <year> 1976. </year> <title> The Metanovel: Writing Stories by Computer. </title> <type> Ph.D. thesis, </type> <institution> Yale University. Also available as Yale University Computer Science Research Report #74 and through New York: Garland Publishing, </institution> <year> 1980. </year>
Reference-contexts: For example, PAM [Wilensky78] understood stories involving rational agents by analyzing the goals and plans of the characters. Also, TALE-SPIN <ref> [Meehan76] </ref> used plans and goals to create simple stories with rational agents as characters. In the problem-solving domain, robot-planning programs (e.g., [Fikes71, Sacerdoti74]) have shown that planning is a good paradigm for programming robots as rational agents.
Reference: [Norvig83] <author> Norvig, P., </author> <year> 1983. </year> <title> Frame activated inferences in a story understanding program. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, p. </booktitle> <pages> 624-626, </pages> <address> Karlsruhe, West Germany. </address>
Reference-contexts: This representation is in the form of a KODIAK network (a semantic network representation language developed at Berkeley [Wilensky87]). Next, UC's concretion mechanism performs concretion inferences <ref> [Wilensky83, Norvig83] </ref> based on the semantic network. Concretion is the process of inferring more specific interpretations of the user's input than might strictly be correct on a logical basis. Such inferences might be motivated by the context of the utterance or by culturally accepted usage biases.
Reference: [Newell72] <author> Newell, A. and Simon, H. A., </author> <year> 1972. </year> <title> Human Problem Solving. </title> <address> Englewood Clifts, NJ:Prentice-Hall. </address> <month> 24 </month>
Reference-contexts: Much work has been done in AI in the area of planning where the goals of the planner are provided by the operator. For example, <ref> [Newell72] </ref> formulated means-ends analysis as a general strategy for achieving given ends or goals. However, all the robot-planning programs assumed that the goals are given by the users. Likewise, in TALE-SPIN the programmer provided the initial goals of the characters.
Reference: [Quilici89] <author> Quilici, A., </author> <year> 1989. </year> <title> Recognizing and responding to plan-oriented misconceptions. </title> <editor> In Kobsa, A. and Wahlster, W., editors, </editor> <booktitle> User Models in Dialog Systems, p. </booktitle> <pages> 108-132. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [Schank77] <author> Schank, R. C. and Abelson, R. P., </author> <year> 1977. </year> <title> Scripts, Plans, Goals, and Understanding. </title> <address> Hillsdale, </address> <publisher> NJ:Lawrence Erlbaum. </publisher>
Reference-contexts: Also, because PANDORA existed in a self-contained simulated world, it did not address the problem of detecting goals when the system must interact with real users. Schank and Abelson advocate themes as the origin of goals <ref> [Schank77] </ref> . However, themes give rise only to very high-level goals, such as being rich. <p> UC's internal state includes UC's domain knowledge, UC's own goals, and UC's themes <ref> [Schank77] </ref>. The situations that give rise to goals in the UC domain can be divided into five main classes: 1. themes goals, 2. plans - subgoals, 3. goal interactions - metagoals, 4. gaps in the user's knowledge goals, 5. user misconceptions goals.
Reference: [Sacerdoti74] <author> Sacerdoti, E. D., </author> <year> 1974. </year> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5 </volume> (2):115-135,. 
Reference-contexts: For example, PAM [Wilensky78] understood stories involving rational agents by analyzing the goals and plans of the characters. Also, TALE-SPIN [Meehan76] used plans and goals to create simple stories with rational agents as characters. In the problem-solving domain, robot-planning programs (e.g., <ref> [Fikes71, Sacerdoti74] </ref>) have shown that planning is a good paradigm for programming robots as rational agents. In the realm of conversation, [Hobbs80] has argued 4 that human conversation fits such a paradigm.
Reference: [Searle69] <author> Searle, J. R., </author> <year> 1969. </year> <title> Speech Acts; An Essay in the Philosophy of Language. </title> <publisher> Cambridge, England:Cambridge University Press. </publisher>
Reference-contexts: The approach that I take is to view the program as an agent. That is, a consultation system should be viewed as a system that can perform actions. For a natural-language consultation system, acting consists of mostly speech acts <ref> [Austin62, Searle69] </ref>, that is, acting by communicating with the user. Within this paradigm, taking the initiative in a dialogue translates into acting without the guidance of the user. An agent that has this capability of taking the initiative is called an autonomous agent.
Reference: [Wahlster83] <author> Wahlster, W., Marburger, H., Jameson, A., and Busemann, S., </author> <year> 1983. </year> <title> Over-answering yes-no-questions: Extended responses in a nl interface to a vision system. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, p. </booktitle> <pages> 643-646, </pages> <address> Karlsruhe, West Germany. </address>
Reference-contexts: Elaborations Elaborations are given when the user asks a simple yes/no question and the system volunteers more information than a simple yes/no answer. This type of volunteered information has been termed overanswering by <ref> [Wahlster83] </ref>, who implemented overanswering in the HAM-ANS natural-language system. An example of elaboration produced by UC is shown in Figure 9.9. FIGURE 9.9 UC SESSION SHOWING AN ELABORATION IN UC'S RESPONSE TO THE USER # Is rn used to rename files? No, rn is used to read news.
Reference: [Webber83] <author> Webber, B. L. and Mays, E., </author> <year> 1983. </year> <title> Varieties of user misconceptions: Detection and correction. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <volume> volume 2, </volume> <pages> p. 650-652, </pages> <address> Karlsruhe, West Germany. </address>
Reference-contexts: There has been considerable research on user misconceptions, both theoretical work on the types of misconceptions and concrete implementations of systems that can detect and correct some kinds of user misconceptions <ref> [Mays80, Webber83, Kaplan83, McCoy85, McCoy89, Marburger86, Quilici89] </ref>. However, none of these treat the interface as an intelligent agent. UC's agent approach allows greater flexibility in making principled decisions about when UC might wish to correct a user's misconception.
Reference: [Wilensky84] <author> Wilensky, R., Arens, Y., and Chin, D. N., </author> <year> 1984. </year> <title> Talking to unix in english: An overview of uc. </title> <journal> Communications of the ACM, </journal> <volume> 27 </volume> (6):574-593. 
Reference-contexts: UC is able to provide information about how to do things in UNIX, definitions about UNIX or general operating system terminology, and help in debugging problems with using UNIX. A short overview of UC follows. For more details on UC, the reader is referred to [Chin87], [Wilensky86], and <ref> [Wilensky84] </ref>. In a typical UC session, the user types questions in English to UC, and UC responds to the user in English.
Reference: [Wilensky78] <author> Wilensky, R., </author> <year> 1978. </year> <title> Understanding Goal-Based Stories. </title> <type> Ph.D. thesis, </type> <institution> Yale University. </institution> <note> Also available as Yale University Computer Science Research Report #140, </note> <institution> New Haven, CT, </institution> <year> 1978. </year>
Reference-contexts: A rational agent is an agent that behaves rationally. In AI programs, much as in popular psychology, this usually means that a program's behavior is determined by reasonable plans and goals and that the program attributes reasonable plans and goals to other rational agents. For example, PAM <ref> [Wilensky78] </ref> understood stories involving rational agents by analyzing the goals and plans of the characters. Also, TALE-SPIN [Meehan76] used plans and goals to create simple stories with rational agents as characters.
Reference: [Wilensky83] <author> Wilensky, R., </author> <year> 1983. </year> <title> Planning and Understanding: A Computational Approach to Human Reasoning. </title> <address> Reading, MA:Addison-Wesley. </address>
Reference-contexts: For a rational autonomous agent that is based on the planning paradigm, this problem translates to the problem of determining appropriate goals for the planner. Such a process is called goal detection <ref> [Wilensky83] </ref>. After a rational autonomous agent, henceforth called an intelligent agent, has detected appropriate goals, it is up to the planner of the intelligent agent to formulate a plan to satisfy these goals and then carry out the plan. <p> This representation is in the form of a KODIAK network (a semantic network representation language developed at Berkeley [Wilensky87]). Next, UC's concretion mechanism performs concretion inferences <ref> [Wilensky83, Norvig83] </ref> based on the semantic network. Concretion is the process of inferring more specific interpretations of the user's input than might strictly be correct on a logical basis. Such inferences might be motivated by the context of the utterance or by culturally accepted usage biases. <p> By detecting the right goals, UC can respond intelligently to the user. 9.5 GOAL DETECTION The central problem in building an intelligent agent is how to detect appropriate goals for the agent, goals that can then be used to guide the agent's actions. This process is called goal detection <ref> [Wilensky83] </ref>. Although considerable work has been done in the area of planning, very few planning systems have addressed the problem of how to detect appropriate goals for planning. In almost all planning systems, the high-level goals are provided by the human operators of the planners. <p> Any combination of factors in the agent's environment or internal state that leads to a new goal for the agent is called a situation after the terminology of <ref> [Wilensky83] </ref>. This section will classify the kinds of situations that lead UCEgo to detect new goals. Since UC is a computer consultation system, UCEgo's environment is limited to a dialogue with the user on the subject of the UNIX operating system. <p> Another source of goals is the planning process. As an agent plans for goals, the resulting plans may produce subgoals that the agent will need to adopt and plan to satisfy. When an agent has several goals, these goals may interact, giving rise to a metagoal <ref> [Wilensky83] </ref> which is a goal for dealing with the interaction among other goals. Themes, plans, and goal interactions are universal sources of goals in that they are common to all intelligent agents. The other two sources of goals are somewhat more particular to a consulting environment. <p> These include life themes as well as role themes. An example of a life theme is UCEgo's stay-alive theme. This theme gives rise to the recurrent background goals of preserving the UC program and preserving the UNIX system. The stay-alive theme is also an instance of the preservation theme <ref> [Wilensky83] </ref>, because it gives rise to preservation goals. An example of a role theme is UCEgo's consultant role theme. This gives rise to the recurrent goals of helping the user and being polite to the user. <p> In such situations, UCEgo detects a conflict between UC's goal of helping the user (from the consultant role theme) and UC's goal of ACT-ETHICALLY. 9 Such goal interactions are described further in Section. The stay-alive life theme is an instance of the preservation theme <ref> [Wilensky83] </ref>, whence arise preservation goals. This particular preservation theme represents UC's desire to preserve itself. As a result, it leads UC to adopt the goals of preserving the UC program and preserving the UNIX system on which UC runs. <p> Goals can interact either negatively by conflicting or positively by overlapping. When UCEgo detects a situation where goals conflict or overlap, it creates a new goal for dealing with the goal interaction. Goals for dealing with other goals are called metagoals <ref> [Wilensky83] </ref>. Metagoals are not distinguished from other goals in the UC program either by representational differences or by differences in their processing. <p> Also, when UCEgo cannot find a prestored plan to satisfy one of UC's goals, UCEgo adopts the metagoal of knowing a plan for finding out a plan to satisfy this goal. This is an example of metaplanning <ref> [Wilensky83] </ref>, because UC is planning to create a plan that is used to find another plan. For more details on metaplanning in UCEgo, see [Chin87]. 9.8.1. Mutual Inclusion Metagoals are used to deal with both positive and negative interactions between goals. <p> For more details on metaplanning in UCEgo, see [Chin87]. 9.8.1. Mutual Inclusion Metagoals are used to deal with both positive and negative interactions between goals. One way in which goals can interact positively is through mutual inclusion <ref> [Wilensky83] </ref>. This describes situations in which a planner has the same or similar goals for different reasons. In such situations, the planner can merge the goals into a single goal. This saves resources, because the planner no longer has to plan several times nor execute many similar plans.
Reference: [Wilensky86] <author> Wilensky, R., Mayfield, J., Albert, A., Chin, D. N., Cox, C., Luria, M., Martin, J., and Wu, D., </author> <year> 1986. </year> <title> UC a progress report. </title> <type> Technical Report UCB/CSD 87/303, </type> <institution> Computer Science Division, University of California, Berkeley. </institution>
Reference-contexts: UC is able to provide information about how to do things in UNIX, definitions about UNIX or general operating system terminology, and help in debugging problems with using UNIX. A short overview of UC follows. For more details on UC, the reader is referred to [Chin87], <ref> [Wilensky86] </ref>, and [Wilensky84]. In a typical UC session, the user types questions in English to UC, and UC responds to the user in English.
Reference: [Wilensky87] <author> Wilensky, R., </author> <year> 1987. </year> <title> Some problems and proposals for knowledge representation. </title> <type> Technical Report UCB/CSD 87/351, </type> <institution> Computer Science Division, University of California, Berkeley. </institution> <month> 25 </month>
Reference-contexts: This representation is in the form of a KODIAK network (a semantic network representation language developed at Berkeley <ref> [Wilensky87] </ref>). Next, UC's concretion mechanism performs concretion inferences [Wilensky83, Norvig83] based on the semantic network. Concretion is the process of inferring more specific interpretations of the user's input than might strictly be correct on a logical basis.
References-found: 32

