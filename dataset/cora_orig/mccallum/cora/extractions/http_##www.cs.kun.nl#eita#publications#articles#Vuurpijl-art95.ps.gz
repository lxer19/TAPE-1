URL: http://www.cs.kun.nl/eita/publications/articles/Vuurpijl-art95.ps.gz
Refering-URL: http://www.cs.kun.nl/eita/publications/
Root-URL: 
Email: email: louis@cs.kun.nl  
Title: Performance Prediction of large MIMD Systems for Par- allel Neural Network Simulations  
Author: Louis Vuurpijl, Theo Schouten, Jan Vytopil 
Keyword: Performance prediction; Parallel neural network simulations; MIMD transputer systems  
Address: Netherlands,  
Affiliation: Faculty of Mathematics and Informatics, University of Nijmegen, The  
Abstract: In this paper, we present a performance prediction model for indicating the performance range of MIMD parallel processor systems for neural network simulations. The model expresses the total execution time of a simulation as a function of the execution times of a small number of kernel functions, which have to be measured on only one processor and one physical communication link. The functions depend on the type of neural network, its geometry, decomposition and the connection structure of the MIMD machine. Using the model, the execution time, speedup, scalability and efficiency of large MIMD systems can be predicted. The model is validated quantitatively by applying it to two popular neural networks, backpropagation and the Kohonen self-organizing feature map, decomposed on a GCel-512 1 , a 512 transputer system. Measurements are taken from network simulations decomposed via dataset and network decomposition techniques. Agreement of the model with the measurements is within 1%-14%. Estimates are given for the performances that can be expected for the new T9000 transputer systems. The presented method can also be used for other application areas such as image processing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Bader and B. Przywara. </author> <title> T9000 A Preliminary Evaluation of Arithmetic Performance. Brief Note, </title> <institution> Interdisziplinares Zentrum fur Wis-senschaftliches Rechnen, Universitat Heidelberg, </institution> <address> Im Neuenheimer Feld 368, D-6900 Heidelberg, iwr1.iwr.Uni-Heidelberg.de, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: It is interesting to make some predictions for the T9000 transputer, extrapolating the measured parameters on the T805 by dividing them by the increased computing and communication powers of the T9000. Based on the information in [9] and <ref> [1] </ref>, one could roughly say that the T9000 is about 8 times faster than the T805 and has about 6 times higher bandwidth. This means that the number of processors after which the speedup decreases for dataset decomposition is reduced by a factor 3 p (36=64) 0:83.
Reference: [2] <author> M. Berry, G. Cybenko, and J. Larson. </author> <title> Scientific Benchmark Characterizations. </title> <journal> Parallel Computing, </journal> <volume> 17(2) </volume> <pages> 1173-1194, </pages> <year> 1991. </year>
Reference-contexts: The needed benchmarks in our method are restricted to measuring the execution time of a small number of kernel functions on one processor and the time needed to communicate a single information unit between two processors. This approach can be classified as kernel benchmark-ing <ref> [2, 6] </ref>. The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems.
Reference: [3] <author> G.A. Carpenter and S. Grossberg. </author> <title> A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics, and Image Processing, </journal> <volume> 37 </volume> <pages> 54-115, </pages> <year> 1987. </year>
Reference-contexts: The first case holds for networks that require updates of the weights immediately after being inputted with a training sample, such as Hopfield [7] or the ART <ref> [3] </ref> networks. The second case is obvious: if a neural network requires more memory for its connections than there is available on one processor, more processors have to be used. In these cases, the neural network has to be decomposed via network decomposition.
Reference: [4] <author> L-C. Chu and B.W. Wah. </author> <title> Optimal Mapping of Neural Network Learning on Message-Passing Multicomputers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 14 </volume> <pages> 319-339, </pages> <year> 1992. </year>
Reference-contexts: This approach can be classified as kernel benchmark-ing [2, 6]. The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. Chu and Wah <ref> [4] </ref>, Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu [16].
Reference: [5] <author> B.M. Forrest, D. Roweth, N. Stroud, D.J. Wal-lace, and G.V. Wilson. </author> <title> Implementing Neural Network Models on Parallel Computers. </title> <journal> The Computer Journal, </journal> <volume> 30(5) </volume> <pages> 413-419, </pages> <year> 1987. </year>
Reference-contexts: Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu [16]. Several other neural networks like Hopfield [7] networks <ref> [?, 5] </ref> have also been implemented on parallel architectures. Methods for decomposing neural networks on MIMD multi-processor systems can be distinguished in three levels of decomposition, job-level decomposition, dataset decomposition and what we call network decomposition.
Reference: [6] <author> A.J.G. Hey. </author> <title> The Genesis Distributed Memory Benchmarks. </title> <journal> Parallel Computing, </journal> <volume> 17(2) </volume> <pages> 1275-1283, </pages> <year> 1991. </year>
Reference-contexts: This involves that if this approach is taken, the selected benchmarks have to be examined in great detail, which in general puts too much efforts on an application programmer. Furthermore, exist ing benchmarking methods are targeted on vec-tor supercomputers and not on MIMD parallel processor architectures, although the Genesis <ref> [6] </ref> benchmarks have implemented a range of kernel and small application benchmarks on MIMD architectures. Finally, as none of these match the characteristics of distributed neural network simulations, some other method is required that is targeted on neural networks and machines with MIMD architectures. <p> The needed benchmarks in our method are restricted to measuring the execution time of a small number of kernel functions on one processor and the time needed to communicate a single information unit between two processors. This approach can be classified as kernel benchmark-ing <ref> [2, 6] </ref>. The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems.
Reference: [7] <author> J.J. </author> <title> Hopfield. Networks and Physical Systems with Emergent Collective Computational Abilities. </title> <booktitle> In Proc. </booktitle> <institution> Natl. Acad. Sci. </institution> <address> USA 79, </address> <pages> pages 2554-2558, </pages> <year> 1982. </year>
Reference-contexts: Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu [16]. Several other neural networks like Hopfield <ref> [7] </ref> networks [?, 5] have also been implemented on parallel architectures. Methods for decomposing neural networks on MIMD multi-processor systems can be distinguished in three levels of decomposition, job-level decomposition, dataset decomposition and what we call network decomposition. <p> Network decomposition If a neural network model does not accommodate epoch learning or if it does not fit on one processor, dataset decomposition cannot be exploited. The first case holds for networks that require updates of the weights immediately after being inputted with a training sample, such as Hopfield <ref> [7] </ref> or the ART [3] networks. The second case is obvious: if a neural network requires more memory for its connections than there is available on one processor, more processors have to be used. In these cases, the neural network has to be decomposed via network decomposition.
Reference: [8] <author> T. Kohonen. </author> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <note> second edition, </note> <year> 1988. </year>
Reference-contexts: Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen <ref> [8] </ref> networks by for example Obermayer et al [10] and Wu [16]. Several other neural networks like Hopfield [7] networks [?, 5] have also been implemented on parallel architectures.
Reference: [9] <author> Inmos/SGS-Thomson Microelectronics. </author> <title> The T9000 Transputer Product Overview, </title> <year> 1991. </year>
Reference-contexts: It is interesting to make some predictions for the T9000 transputer, extrapolating the measured parameters on the T805 by dividing them by the increased computing and communication powers of the T9000. Based on the information in <ref> [9] </ref> and [1], one could roughly say that the T9000 is about 8 times faster than the T805 and has about 6 times higher bandwidth. This means that the number of processors after which the speedup decreases for dataset decomposition is reduced by a factor 3 p (36=64) 0:83.
Reference: [10] <author> K. Obermayer, H.Heller, H. Ritter, and K. Schulten. </author> <title> Simulation of Self-Organizing Neural Nets: a Comparison between a Transputer Ring and a Connection Machine CM-2. </title> <booktitle> In Proceedings of the Third Conference of NATUG, </booktitle> <address> Sunnyvale, CA, </address> <year> 1990. </year>
Reference-contexts: Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al <ref> [10] </ref> and Wu [16]. Several other neural networks like Hopfield [7] networks [?, 5] have also been implemented on parallel architectures. Methods for decomposing neural networks on MIMD multi-processor systems can be distinguished in three levels of decomposition, job-level decomposition, dataset decomposition and what we call network decomposition.
Reference: [11] <author> D.E. Rumelhart and J.L. McClelland. </author> <title> Parallel Distributed Processing: </title> <journal> Explorations in the Mi-crostructure of Cognition, </journal> <volume> volume 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation <ref> [11] </ref> networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu [16]. Several other neural networks like Hopfield [7] networks [?, 5] have also been implemented on parallel architectures.
Reference: [12] <author> T. Tollenaere and G.A. Orban. </author> <title> Simulating Modular Neural Networks on Message-Passing Multiprocessors. </title> <journal> Parallel Computing, </journal> <volume> 17(1) </volume> <pages> 361-379, </pages> <year> 1991. </year>
Reference-contexts: As only the neurons which lay in the neighbourhood of a winning neuron have to change their weights, only the processors hosting these neurons have to perform step 6. Therefore, we use another decomposition technique called scattered decomposition <ref> [12] </ref>, as has been suggested in [16]. Using this technique, neurons laying close to each other in the map are placed on distinct processors, thus guaranteeing that step 6 is also well balanced. The only communications necessary for the Ko-honen map are contained in steps 1 and 3.
Reference: [13] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Suitability of Transputers for Neural Network Simulations. </title> <editor> In W. Joosen and E. Milgrom, editors, </editor> <booktitle> Parallel Computing: From Theory to Sound Practice, </booktitle> <pages> pages 528-537. </pages> <publisher> IOS Press, </publisher> <year> 1992. </year>
Reference-contexts: Each processor runs a slave process that manages part of the simulation. The master process runs on processor (0,0) in the grid, and controls the slave processes while managing its own part of the simulation. In <ref> [13] </ref> and [14], a detailed description of the different patterns of communication required for such parallel neural network simulations is given. It is explained that the number of communications can be reduced enormously by using only local communications. This can be established via broadcast and gather operations. <p> During the gathering also the accumulation of the weight changes can take place, which can be done in parallel. For these kind of gather-accumulate and broadcast operations, a tree topology is optimal <ref> [13] </ref>. Unfortunately, the GCel does not support the physical configuration of tree topologies. We have tried to use virtual tree topologies by using the Parix MakeTree utility, but the performance of the communication was far worse than using the physical grid topology and communicating locally.
Reference: [14] <author> L.G. </author> <title> Vuurpijl and Th.E. Schouten. Performance of MIMD Execution Platforms for PNNs: </title> <type> How many MCUPS? Technical report, </type> <institution> Department of Real-Time Systems, Faculty of Mathematics and Informatics, University of Nijmegen, </institution> <address> Toer-nooiveld 1, 6525 ED Nijmegen, The Netherlands, </address> <month> August </month> <year> 1993. </year> <note> In progress. </note>
Reference-contexts: For the Kohonen neural network, the calculation times in step 4 also depend on the number of neurons. For a further explanation of the typical operations that make up the calculations, see <ref> [14] </ref>. All subsequent times are expressed in seconds. <p> Each processor runs a slave process that manages part of the simulation. The master process runs on processor (0,0) in the grid, and controls the slave processes while managing its own part of the simulation. In [13] and <ref> [14] </ref>, a detailed description of the different patterns of communication required for such parallel neural network simulations is given. It is explained that the number of communications can be reduced enormously by using only local communications. This can be established via broadcast and gather operations. <p> w (W + H 2) t comm GA (W; H; w) = w (W + H 2) (t comm + t acc ) GAB (W; H; w) = w (W + H 2) (2 t comm + t acc ) Similarly, this time can be modeled for ternary tree topologies <ref> [14] </ref> as: GAB (P; w) = w 3 depth (P ) (2 t comm + t acc ) We have used the SendLink and RecvLink communication library routines of Parix. <p> commback (W; H; L; n; w) can be estimated using (12) and GAB (W; H; n n 0 ): t comm (n n 0 ) ( W + W 1 + 2 (W + H 2) (2 + t acc =t comm ) ) A similar estimation is given in <ref> [14] </ref> for the communication time on ternary tree topologies with depth D: t comm (n n 0 )(3 (D 1) (1 + t acc =t comm ) + 1 In order to determine the calculation time, the times for steps 2, 4 and 6 were analyzed and measured depending on the <p> The following relations are found (see also <ref> [14] </ref>). In step 2, for each two layers l and l+1, the time for computing the new net status amounts to 5:7 (n l n l+1 ) seconds. For all layers, this means 5:7 w seconds. In step 4, two main calculations are made.
Reference: [15] <author> M. Witbrock and M. Zagha. </author> <title> An Implementation of Backpropagation Learning on GF11, a Large SIMD Parallel Computer. </title> <journal> Parallel Computing, </journal> <volume> 14 </volume> <pages> 329-346, </pages> <year> 1990. </year>
Reference-contexts: This approach can be classified as kernel benchmark-ing [2, 6]. The problem of decomposing a given neural network over a parallel processor system with given topology has often been addressed in the literature. Chu and Wah [4], Witbrock and Zagha <ref> [15] </ref> and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu [16].
Reference: [16] <author> C-H. Wu, R.E. Hodges, and C-J. Wang. </author> <title> Par-allelizing the Self-Organizing Feature Map on Multiprocessor Systems. </title> <journal> Parallel Computing, </journal> <volume> 17(1) </volume> <pages> 821-832, </pages> <year> 1991. </year>
Reference-contexts: Chu and Wah [4], Witbrock and Zagha [15] and various other authors have discussed the implementation of backpropagation [11] networks on diverse parallel processor systems. Similar efforts have been reported implementing Kohonen [8] networks by for example Obermayer et al [10] and Wu <ref> [16] </ref>. Several other neural networks like Hopfield [7] networks [?, 5] have also been implemented on parallel architectures. Methods for decomposing neural networks on MIMD multi-processor systems can be distinguished in three levels of decomposition, job-level decomposition, dataset decomposition and what we call network decomposition. <p> As only the neurons which lay in the neighbourhood of a winning neuron have to change their weights, only the processors hosting these neurons have to perform step 6. Therefore, we use another decomposition technique called scattered decomposition [12], as has been suggested in <ref> [16] </ref>. Using this technique, neurons laying close to each other in the map are placed on distinct processors, thus guaranteeing that step 6 is also well balanced. The only communications necessary for the Ko-honen map are contained in steps 1 and 3.
References-found: 16

