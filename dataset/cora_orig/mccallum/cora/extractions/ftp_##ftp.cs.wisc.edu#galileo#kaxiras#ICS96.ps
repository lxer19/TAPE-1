URL: ftp://ftp.cs.wisc.edu/galileo/kaxiras/ICS96.ps
Refering-URL: http://www.cs.wisc.edu/~kaxiras/kaxiras.html
Root-URL: 
Abstract: Programs that make extensive use of widely shared var-iables are expected to achieve modest speedups for non-bus-based cache coherence protocols, particularly as the number of processors sharing the data grows large. Protocols such as the IEEE Scalable Coherent Interface (SCI) are optimized for data that is not widely shared; the GLOW protocol extensions are specifically designed to address this limitation. The GLOW extensions take advantage of physical locality by mapping K-ary logical sharing trees to the network topology. This results in protocol messages that travel shorter distances, experiencing lower latency and consuming less bandwidth. To build the sharing trees, GLOW caches directory information at strategic points in the network, allowing concurrency, and therefore, scalability, of read requests. Scalability in writes is achieved by exploiting the sharing tree to invalidate or update nodes in the sharing tree concurrently. We have defined the GLOW extensions with respect to SCI and we have implemented them in the Wisconsin Wind Tunnel (WWT) parallel discrete event simulator. We studied them on an example topology, the K-ary N-cube, and explored their scalability with four programs for large systems (up to 256 processors). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, M. Horowitz and J. Hennessy, </author> <title> An evaluation of Directory schemes for Cache Coherence. </title> <booktitle> Proceedings of the 15th Interna tional Symposium on Computer Architecture, </booktitle> <pages> pp. 280-289, </pages> <month> June </month> <year> 1988. </year>
Reference-contexts: We also discuss a GLOW implementation for a full map protocol (DIR i X <ref> [1] </ref>). While the version described herein is not fully compatible with SCI, such a compatible version is a proposed component of the IEEE P1596.2 Kiloprocessor Extensions to the SCI standard currently under development. The details of this implementation are reported elsewhere [15].
Reference: [2] <author> J. L. Baer and W. H. Wang, </author> <title> Architectural Choices for Multi-Level Cache Hierarchies. </title> <booktitle> Proceedings 16th International Conference on Parallel Processing, </booktitle> <pages> pp. 258-261, </pages> <year> 1987. </year>
Reference-contexts: All of the sharing tree, other than the leaves, represents a form of hierarchical caching of directory information. In the various implementations of GLOW it is neither necessary to have a copy of the data in the GLOW agents nor is it necessary to maintain multilevel inclusion <ref> [2] </ref> in the hierarchical caching of the directory information. These properties are important because they provide attractive implementation choices for avoiding deadlock and indiscriminate invalidation of descendents in case of replacements.
Reference: [3] <author> R. Bianchini and T. J. LeBlanc, </author> <title> Eager Combining: A Coherency Protocol for Increasing Effective Network and Memory Bandwidth in Shared-Memory Multiprocessors. </title> <booktitle> Proceedings of the 6th Symposium on Parallel and Distributed Processing, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: For the HFMD the authors report a 6% performance improvement over a full-map scheme and for the TD a decrease of 25% in performance compared to a chained directory scheme such as SCI. Eager Combining (EC) <ref> [3] </ref> uses specified nodes as servers for widely shared pages or hot pages. These pages are updated in the server nodes (using eager sharing). Clients request the data from the servers rather than the actual home node.
Reference: [4] <author> John Carter, John Bennett and Willy Zwaenepoel, Munin: </author> <title> Distributed Shared Memory Based on Type-Specific Memory Coherence. </title> <booktitle> Proceedings of the Conference on the Principles and Practices of Par allel Programming, </booktitle> <year> 1990. </year>
Reference-contexts: 1 INTRODUCTION Current trends in research for distributed shared-memory parallel machines, favour the use of specialized protocols for different classes of shared data <ref> [4] </ref>. By classifying data according to its use, a compiler or programmer can exploit data sharing patterns to improve performance. This technique, which is crucial to the success of software-based schemes [19,23], can also be exploited to enhance the performance of hardware-based cache coherence methods.
Reference: [5] <author> David Chaiken, John Kubiatowicz, Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme, </title> <booktitle> Proc. of the 4th Int. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS-IV), </booktitle> <pages> pp. 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: Widely shared data is a class of data that imposes increasingly significant overhead as systems increase in size. Studies have shown that the average degree of sharing (the number of nodes that simultaneously share the same data) in application programs is low <ref> [5] </ref>. This is somewhat misleading for two reasons. First, under most non-bus-based protocols, accessing widely shared data is very expensive and even a small amount of widely shared data can be a serious bottleneck in large systems. <p> In any case, multilevel inclusion can be ignored as long as the memory direc Rings BUS Memory/ DIR Processor/ SCI cache Switch GLOW Interface Card Agent tory has a full map directory (or a LimitLESS directory <ref> [5] </ref>) that can accommodate nodes or agents that do not appear in their proper position in the sharing tree.
Reference: [6] <author> Satish Chandra, James R. Larus, Anne Rogers. </author> <booktitle> Where is Time Spent in Message-Passing and Shared-Memory Programs? Proceedings of the 6th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 61-73, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Details of the shared memory program can be found in <ref> [6] </ref>. A coefficient matrix NN is filled with random numbers and then the linear system is solved using a known vector. The work is divided among the processors by distributing the rows block-wise.
Reference: [7] <author> T.H. Cormen, C.E. Leiserson, R.L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990 </year>
Reference-contexts: The second is to find the transitive closure of a graph, (i.e., a new graph where two vertices are connected if there is a path in the original graph that connects these vertices). For both programs we used dynamic-programming formulations, that are special cases of the Floyd-Warshall <ref> [7] </ref> algorithm. In the All-Pairs Shortest Path program (APSP), an N vertex graph is represented by an NN adjacency matrix. The (i,j) element of this matrix represents the weight or distance between the i and j vertices.
Reference: [8] <author> Kourosh Gharachorloo, Sarita V. Adve, Anoop Gupta, John L. Hen-nessy, and Mark D. Hill, </author> <title> Programming for Different Memory Consistency Models. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 15(4), </volume> <year> 1992. </year>
Reference: [9] <author> J.R. Goodman, Mary K. Vernon, Philip J. Woest, </author> <title> Efficient Synchronization Primitives for Large-Scale Cache Coherent Multiprocessors. </title> <booktitle> Proc. of the 3rd Int. Conf. on Architectural support for Programming Languages and Operating Systems (ASPLOS-III), </booktitle> <month> April </month> <year> 1989. </year>
Reference-contexts: This technique, which is crucial to the success of software-based schemes [19,23], can also be exploited to enhance the performance of hardware-based cache coherence methods. The Scalable Coherent Interface [11], for example, optionally implements the QOLB primitive <ref> [9] </ref> in this way. Several classes of shared data have been identified, including migratory, read-only, etc., [4,26]. Widely shared data is a class of data that imposes increasingly significant overhead as systems increase in size.
Reference: [10] <author> A. Gottlieb, R. Grishman, C.P. Kruskal, K.P. McAuliffe, L. Rudolph, M. Snir, </author> <title> The NYU Ultracomputer Designing a MIMD Shared-Memory Parallel Computer. </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-32, no 2, </volume> <pages> pp. 175-189, </pages> <month> February </month> <year> 1983. </year>
Reference-contexts: The GLOW extensions offer scalable reads and scalable writes to widely shared data. Scalable reads are achieved by caching directory information in the network, a technique inspired by the request combining originally proposed for the NYU Ultracom-puter <ref> [10] </ref>. Because the directory information is more long-lived, however, this technique can be effective even when multiple requests are not generated simultaneously. Scalable writes are achieved by exploiting the topology to invalidate or update sharing nodes in logarithmic time.
Reference: [11] <institution> IEEE Standard for Scalable Coherent Interface (SCI) 1596-1992, </institution> <note> IEEE 1993. </note>
Reference-contexts: By classifying data according to its use, a compiler or programmer can exploit data sharing patterns to improve performance. This technique, which is crucial to the success of software-based schemes [19,23], can also be exploited to enhance the performance of hardware-based cache coherence methods. The Scalable Coherent Interface <ref> [11] </ref>, for example, optionally implements the QOLB primitive [9] in this way. Several classes of shared data have been identified, including migratory, read-only, etc., [4,26]. Widely shared data is a class of data that imposes increasingly significant overhead as systems increase in size.
Reference: [12] <author> Ross E. Johnson, </author> <title> Extending the Scalable Coherent Interface for Large-Scale Shard-Memory Multiprocessors. </title> <type> PhD Thesis, </type> <institution> Univer sity of Wisconsin-Madison, </institution> <year> 1993. </year>
Reference-contexts: Scalable writes are achieved by exploiting the topology to invalidate or update sharing nodes in logarithmic time. Previous examples of sharing tree protocols, such as the STEM Kiloprocessor Extensions to SCI <ref> [12] </ref> and others [3,20,21] have largely ignored locality in the network. In addition, previous work has attempted to treat all shared data in a uniform manner, even though the overhead for such support limits the benefit, or even reduces performance, for data that is not widely shared. <p> In preliminary experimentation using update with the other benchmarks we found that only SPARSE showed improvement over invalidation. 7 RELATED WORK 7.1 STEM The STEM extensions to SCI <ref> [12] </ref> provide a logarithmic-time algorithm to build, maintain and invalidate a binary sharing tree (in contrast to GLOWs K-ary trees) without regard to the topology of the interconnection network. The complexity of the algorithm, however, is high, requiring complex transactions that generate increased traffic.
Reference: [13] <author> Ross E. Johnson, James R.Goodman, </author> <title> Interconnect Topologies with Point-to-Point Rings, </title> <booktitle> Proc. of the International Conference on Par allel Processing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: Scott et al. showed [25] that a ring can accommodate small numbers of high-performance nodes, in the range of four to eight. For larger systems, more complex topologies can be constructed from rings (e.g., K-ary N-cubes, multistage topologies) <ref> [13] </ref>. In these topologies some or all nodes provide low-latency switches to connect more than one ring. SCI defines a distributed, directory-based cache coherence protocol. <p> since the nodes, or other agents it would service, will be serviced by the memory directory in the usual manner specified by the underlying protocol. 3.4 GLOW ON AN EXAMPLE TOPOLOGY GLOW extensions can be implemented on top of a wide range of topologies, including hypercubes; meshes; trees; buttery topologies <ref> [13] </ref> and many others. GLOW can also be used in irregular topologies (e.g., an irregular network of workstations). We chose to implement and study the GLOW extensions on a popular topology we believe is a likely candidate for implementation. It is the K-ary N-cube topology made of rings.
Reference: [14] <author> Alain Kgi, Nagi Aboulenein, Douglas C. Burger, James R. Good-man, </author> <title> Techniques for Reducing Overheads of Shared-Memory Multiprocessing. </title> <booktitle> International Conference on SuperComputing, </booktitle> <month> July </month> <year> 1995. </year>
Reference-contexts: The WWT keeps track of virtual time in processor cycles. The Scalable Coherent Interface has previously been simulated extensively under WWT <ref> [14] </ref> and the GLOW extensions have been applied to this simulation environment. We consistently made conservative choices to establish a lower bound on the performance of GLOW. We have studied the DESTRUCTIVE rollout algorithm, which has lower performance than the LINEARIZING rollout.
Reference: [15] <author> S. Kaxiras, </author> <title> Kiloprocessor Extensions to SCI. </title> <booktitle> Proceedings of the 10th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1996. </year>
Reference-contexts: While the version described herein is not fully compatible with SCI, such a compatible version is a proposed component of the IEEE P1596.2 Kiloprocessor Extensions to the SCI standard currently under development. The details of this implementation are reported elsewhere <ref> [15] </ref>. Memory/Dir (homenode) 3.1 BACKGROUND: SCI The ANSI/IEEE standard 1596 Scalable Coherent Interface represents a robust hardware solution to the challenge of building cache-coherent, shared memory multiprocessor systems. It defines both a network interface and a cache coherence protocol. <p> Note this scheme requires the agent to maintain two pointers: one to the tail for providing the data, and one to the head for adding additional nodes. This scheme is the most compatible with SCI and its details are discussed elsewhere <ref> [15] </ref>. HEAD-TO-TAIL: If a requesting node prepends to a child list, where the old head is waiting for the data, then it assumes responsibility to forward the data. When the agent gets the data it will pass it to the waiting heads of its child lists. <p> Therefore replacements in the agents directory storage can be much faster. The LINEARIZING rollout is used in the SCI compatible version of GLOW and additional details can be found elsewhere <ref> [15] </ref>. Child List Child List 4.3 INVALIDATION Before a node can modify a cache line, it must first become the head of the top-level list connected directly to the memory directory in the home node. <p> There, all the agents in parallel assume a role similar to the head nodes in SCI and invalidate their child lists in the same manner as the SCI protocol <ref> [15] </ref>. 5 SIMULATION METHODOLOGY To evaluate the performance of GLOW we used four Scientific benchmarks. We do not claim that these programs are in any way representative of a real workload. We did not consider programs without widely shared data because such programs would never activate the GLOW extensions. <p> In the case of a miss, The interested reader can find the results of the simulations without network contention and for up to 512 nodes in [16]. Micro-benchmark results for the IEEE P1596.2 GLOW Kiloprocessor extensions, a varia tion of the protocol described herein can also be found in <ref> [15] </ref>. the simulator takes control and takes the appropriate actions defined by the simulated protocol. The WWT keeps track of virtual time in processor cycles. The Scalable Coherent Interface has previously been simulated extensively under WWT [14] and the GLOW extensions have been applied to this simulation environment.
Reference: [16] <author> S. Kaxiras and J. R. Goodman, </author> <title> The GLOW Cache Coherence Extensions for Widely Shared Data. </title> <institution> University of Wisconsin-Madison, C.S. Dept., </institution> <type> Technical Report 1305, </type> <month> March </month> <year> 1996. </year> <month> (ftp.cs.wisc.edu) </month>
Reference-contexts: In the case of a miss, The interested reader can find the results of the simulations without network contention and for up to 512 nodes in <ref> [16] </ref>. Micro-benchmark results for the IEEE P1596.2 GLOW Kiloprocessor extensions, a varia tion of the protocol described herein can also be found in [15]. the simulator takes control and takes the appropriate actions defined by the simulated protocol. The WWT keeps track of virtual time in processor cycles. <p> Finally we assumed infinite queues in the network. For the interested reader a discussion of the network model can be found elsewhere <ref> [16] </ref>. Each GLOW agent is equipped with a 4096-entry directory storage and an optional 256K data storage. The directory storage is rather small: A directory entry (tree tag) for the K-ary 3-cube needs approximately 12 bytes (five 16-bit pointers and state) making the total directory storage (409612 bytes) 48 Kilobytes. <p> SPARSE with a 512512 matrix, as it is depicted in Figure 6, does not exhibit speedup beyond 32 nodes for either SCI or GLOW, though the latter is always faster than SCI for any system size. As reported elsewhere, the benefit of GLOW increases for larger data sets <ref> [16] </ref>. GLOW also benefits more than SCI when upgrading to a 3-dimensional network. Data storage improves GLOW speedups by at most 5%. In Table 2, which follows the same format as Table 1, we present the statistics for SPARSE.
Reference: [17] <author> Leslie Lamport, </author> <title> How to Make a Multiprocessor Computer that Correctly Executes Multiprocess Programs. </title> <journal> IEEE Transactions on Com puters, </journal> <volume> C-28(9):690691, </volume> <month> September </month> <year> 1979. </year>
Reference: [18] <author> Daniel Lenoski et al., </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> Vol. 25 No. 3, </volume> <pages> pp. 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: For larger systems, more complex topologies can be constructed from rings (e.g., K-ary N-cubes, multistage topologies) [13]. In these topologies some or all nodes provide low-latency switches to connect more than one ring. SCI defines a distributed, directory-based cache coherence protocol. Unlike most other directory-based protocols (such as DASH <ref> [18] </ref>) that keep all the directory information in memory, SCI distributes the directory information to the sharing nodes in a doubly-linked sharing list. The sharing list is stored with the cache lines throughout the system.
Reference: [19] <author> Kai Li, Paul Hudak, </author> <title> Memory Coherence in Shared Virtual Memory Systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> Vol. 7, No. 4, </volume> <pages> pp. 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference: [20] <author> Yeong-Chang Maa, Dhiraj K. Pradhan, Dominique Thiebaut, </author> <title> Two Economical Directory Schemes for Large-Scale Cache-Coherent Multiprocessors. </title> <journal> Computer Architecture News, </journal> <volume> Vol 19, No. 5, </volume> <pages> pp. 10-18, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: Simulation [21] shows a 15% speedup over SCI for the GAUSS algorithm in a 16 node system but also a 35% increase in network traffic. This may be a serious drawback for larger systems. The Tree Directory (TD) protocol <ref> [20] </ref> is based on a K-ary tree structure that is maintained in the sharing caches. It behaves as a limited directory in a tree structure. This scheme does not take into account physical locality and it does not provide for scalable reads. <p> It behaves as a limited directory in a tree structure. This scheme does not take into account physical locality and it does not provide for scalable reads. On the other hand, the Hierarchical Full Map Directory also proposed <ref> [20] </ref> is similar to our approach and exhibits network locality. It is based on full-map directories embedded in the network topology. Both TD and HFMD are strictly based on the inclusion property.
Reference: [21] <author> Hfikan Nilsson, Per Stenstrm, </author> <title> The Scalable Tree Protocola Cache Coherence Approach for Large-Scale Multiprocessors. </title> <booktitle> 4th IEEE Symposium on Parallel and Distributed Processing, </booktitle> <pages> pp. 498-506, </pages> <year> 1992. </year>
Reference-contexts: Request combining is used in the network to reduce the bandwidth requirements of the STEM algorithm and possibly capture some network locality. As employed in STEM, request combining does not require information to be stored in the network. 7.2 OTHER WORK The Scalable Tree Protocol <ref> [21] </ref> defines a sharing tree protocol that speeds up the writing of shared data by invalidating the tree in logarithmic time. The notable feature of the protocol is that additions to and deletions (rollouts/replacements) from the tree leave the tree balanced. <p> This is because its structure depends on the timing of the requests, and deletions from the tree (replacements) change its structure without respect to the underlying topology. Furthermore no support has been proposed to speed up reads (e.g. combining). Simulation <ref> [21] </ref> shows a 15% speedup over SCI for the GAUSS algorithm in a 16 node system but also a 35% increase in network traffic. This may be a serious drawback for larger systems.
Reference: [22] <author> Gregory F. Pfister and V. Alan Norton, </author> <title> Hot Spot Contention and Combining in Multistage Interconnection Networks. </title> <booktitle> Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <pages> pp. 790 797, </pages> <month> August 20-23, </month> <year> 1985. </year>
Reference-contexts: Since the GLOW agents intercept multiple requests for a cache line and generate only a new request toward the home node, the same effect as request combining is achieved. The memory directory receives only the requests of a few agents, in effect eliminating hot spots <ref> [22] </ref>. An example of a general K-ary sharing tree that can be formed using GLOW agents is shown in Figure 1.
Reference: [23] <author> Steven K. Reinhardt, James R. Larus, David A. Wood, Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> Proc. of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 325-336, </pages> <month> April </month> <year> 1994. </year>
Reference: [24] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers. </title> <booktitle> Proceedings of the 1993 ACM SIGMETRICS Conference on Measurements and Modeling of Computer Systems, </booktitle> <pages> pp. 4860, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: This range shows, however, that when the network latencies dominate the performance, GLOW becomes increasingly attractive because of the reduced average path length. 5.1 IMPLEMENTATION OF GLOW IN THE WWT The Wisconsin Wind Tunnel <ref> [24] </ref> is a well-established tool for evaluating large-scale parallel systems through the use of massive, detailed simulation. It executes target parallel programs at hardware speeds (without intervention) for the common case when there is a hit in the simulated coherent cache.
Reference: [25] <author> Steven L. Scott, James R. Goodman, Mary K. Vernon, </author> <title> Performance of the SCI Ring. </title> <booktitle> Proc. of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 403-414, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: It defines both a network interface and a cache coherence protocol. The network interface provides a 1GByte/sec ring interconnect and a set of defined transactions. Scott et al. showed <ref> [25] </ref> that a ring can accommodate small numbers of high-performance nodes, in the range of four to eight. For larger systems, more complex topologies can be constructed from rings (e.g., K-ary N-cubes, multistage topologies) [13].
Reference: [26] <author> Wolf-Dietrich Weber and Anoop Gupta, </author> <title> Analysis of Cache Invalidation Patterns in Multiprocessors. </title> <booktitle> Proc. of the 3rd International Conference on Architectural support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 243-256, </pages> <month> April </month> <year> 1989. </year>
References-found: 26

