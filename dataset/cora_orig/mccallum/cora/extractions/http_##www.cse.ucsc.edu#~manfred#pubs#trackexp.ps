URL: http://www.cse.ucsc.edu/~manfred/pubs/trackexp.ps
Refering-URL: http://www.cse.ucsc.edu/~manfred/pubs.html
Root-URL: http://www.cse.ucsc.edu
Title: Machine Learning, NN,  Tracking the Best Expert  
Author: MARK HERBSTER AND MANFRED K. WARMUTH Editors: Gerhard Widmer and Miroslav Kubat 
Keyword: on-line learning, amortized analysis, multiplicative updates, shifting, experts  
Address: Building, Santa Cruz, CA 95064 (USA)  
Affiliation: Department of Computer Science, University of California at Santa Cruz, Applied Sciences  
Note: c 1998 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 1-29  
Email: (markjmanfred)@cs.ucsc.edu  
Date: (1998)  Received September 1997; Revised January 1998  
Abstract: We generalize the recent relative loss bounds for on-line algorithms where the additional loss of the algorithm on the whole sequence of examples over the loss of the best expert is bounded. The generalization allows the sequence to be partitioned into segments, and the goal is to bound the additional loss of the algorithm over the sum of the losses of the best experts for each segment. This is to model situations in which the examples change and different experts are best for certain segments of the sequence of examples. In the single segment case, the additional loss is proportional to log n, where n is the number of experts and the constant of proportionality depends on the loss function. Our algorithms do not produce the best partition; however the loss bound shows that our predictions are close to those of the best partition. When the number of segments is k + 1 and the sequence is of length `, we can bound the additional loss of our algorithm over the best partition by O(k log n + k log(`=k)). For the case when the loss per trial is bounded by one, we obtain an algorithm whose additional loss over the loss of the best partition is independent of the length of the sequence. The additional loss becomes O(k log n + k log(L=k)), where L is the loss of the best partition with k + 1 segments. Our algorithms for tracking the predictions of the best expert are simple adaptations of Vovk's original algorithm for the single best expert case. As in the original algorithms, we keep one weight per expert, and spend O(1) time per weight in each trial. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Auer, P. & Warmuth, M. K. </author> <year> (1998). </year> <title> Tracking the best disjunction. </title> <journal> Machine Learning, </journal> <note> this issue. </note>
Reference-contexts: This guarantees that the ratio of the weight of any expert to the total weight of all the experts may be bounded from below. Different forms of lower bounding the weights have been used by the Wml algorithm and in the companion paper for learning shifting disjunctions <ref> (Auer & Warmuth, 1998) </ref> that appears in this journal issue. The latter two methods have been applied to learning problems where the loss is the discrete 4 M. HERBSTER AND M. K. WARMUTH loss (i.e. counting mistakes). <p> The first method is essentially the same as the one used in the Wml algorithm of (Littlestone & Warmuth, 1994) and a recent alternate developed in <ref> (Auer & Warmuth, 1998) </ref> for learning shifting disjunctions. When the loss is the discrete loss (as in classification problems), then these methods are simple and effective if the algorithm only updates after a mistake occurs (i.e., conservative updates). Our second method, the Variable-share Update, is more sophisticated.
Reference: <author> Blum, A. & Burch, C. </author> <year> (1997). </year> <title> On-line learning and the metrical task system. </title> <booktitle> In Proceedings of the 10th Annual Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference-contexts: Our share updates have been applied experimentally for predicting disk idle times (Helmbold et al., 1996) and for the on-line management of investment portfolios (Singer, 1997). In addition, a reduction has been shown between expert and metrical task systems algorithms <ref> (Blum & Burch, 1997) </ref>. The Share Update has been used successfully in the new domain of metrical task systems. A natural probabilistic interpretation of the Share algorithms has recently been given in (Vovk, 1997).
Reference: <author> Cesa-Bianchi, N., Freund, Y., Haussler, D., Helmbold, D. P., Schapire, R. E., & Warmuth, M. K. </author> <year> (1997). </year> <title> How to use expert advice. </title> <journal> Journal of the ACM, </journal> <volume> 44(3), </volume> <pages> 427-485. </pages>
Reference: <author> Cover, T. & Thomas, J. </author> <year> (1991). </year> <title> Elements of Information Theory. </title> <publisher> Wiley. </publisher>
Reference-contexts: If ff fl is interpreted as the probability that a shift occurs on any of the `1 trials, then the term (`1) [H (ff fl ) + D (ff fl kff)] corresponds to the expected optimal code length (see Chapter 5 of <ref> (Cover & Thomas, 1991) </ref>) if we code the shifts with the estimate ff instead of the true probability ff fl . This bound is thus an example of the close similarity between prediction and coding as brought out by many papers (e.g. (Feder, Merhav & Gutman, 1992)).
Reference: <author> Feder, M., Merhav, N., & Gutman, M. </author> <year> (1992). </year> <title> Universal prediction of individual sequences. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 38, </volume> <pages> 1258-1270. </pages>
Reference-contexts: This bound is thus an example of the close similarity between prediction and coding as brought out by many papers (e.g. <ref> (Feder, Merhav & Gutman, 1992) </ref>). Note that the ff that minimizes the bound of Theorem 1 depends on k and ` which are unknown to the learner. In practice a good choice of ff may be determined experimentally.
Reference: <author> Freund, Y. & Schapire, R. E. </author> <year> (1997). </year> <title> A decision-theoretic generalization of on-line learning and an application to boosting. </title> <journal> Journal of Computer and System Sciences ,55(1), </journal> <pages> 119-139. </pages>
Reference-contexts: Figure 2), the absolute value loss does not have constant parameters, and thus it must be tuned. In practice, the tuning of may be produced by numerical minimization of the upper bounds. However, we use a tuning of produced by Freund and Schapire <ref> (Freund & Schapire, 1997) </ref>. Theorem 4 (Lemma 4 (Freund & Schapire, 1997)) Suppose 0 P ^ P and 0 &lt; Q ^ Q. Let = g ( ^ P = ^ Q), where g (z) = ln (1 + p 2=z); then P + Q q 20 M. <p> In practice, the tuning of may be produced by numerical minimization of the upper bounds. However, we use a tuning of produced by Freund and Schapire <ref> (Freund & Schapire, 1997) </ref>. Theorem 4 (Lemma 4 (Freund & Schapire, 1997)) Suppose 0 P ^ P and 0 &lt; Q ^ Q. Let = g ( ^ P = ^ Q), where g (z) = ln (1 + p 2=z); then P + Q q 20 M. HERBSTER AND M. K. <p> Theoretical techniques exist for the Fixed-share Algorithm for eliminating the need to choose the value of ff ahead of time. One method for tuning parameters (among other things) is the "specialist" framework of <ref> (Freund, Schapire, Singer & Warmuth, 1997) </ref>, even though the bounds produced this way are not always optimal. Another method incorporates a prior distribution on all possible values of ff.
Reference: <author> Freund, Y., Schapire, R. E., Singer, Y., & Warmuth, M. K. </author> <year> (1997). </year> <title> Using and combining predictors that specialize. </title> <booktitle> In Proceedings of the Twentyninth Annual ACM Symposium on Theory of Computing. </booktitle>
Reference-contexts: Figure 2), the absolute value loss does not have constant parameters, and thus it must be tuned. In practice, the tuning of may be produced by numerical minimization of the upper bounds. However, we use a tuning of produced by Freund and Schapire <ref> (Freund & Schapire, 1997) </ref>. Theorem 4 (Lemma 4 (Freund & Schapire, 1997)) Suppose 0 P ^ P and 0 &lt; Q ^ Q. Let = g ( ^ P = ^ Q), where g (z) = ln (1 + p 2=z); then P + Q q 20 M. <p> In practice, the tuning of may be produced by numerical minimization of the upper bounds. However, we use a tuning of produced by Freund and Schapire <ref> (Freund & Schapire, 1997) </ref>. Theorem 4 (Lemma 4 (Freund & Schapire, 1997)) Suppose 0 P ^ P and 0 &lt; Q ^ Q. Let = g ( ^ P = ^ Q), where g (z) = ln (1 + p 2=z); then P + Q q 20 M. HERBSTER AND M. K. <p> Theoretical techniques exist for the Fixed-share Algorithm for eliminating the need to choose the value of ff ahead of time. One method for tuning parameters (among other things) is the "specialist" framework of <ref> (Freund, Schapire, Singer & Warmuth, 1997) </ref>, even though the bounds produced this way are not always optimal. Another method incorporates a prior distribution on all possible values of ff.
Reference: <author> Haussler, D., Kivinen, J., & Warmuth, M. K. </author> <year> (1998). </year> <title> Sequential prediction of individual sequences under general loss functions. </title> <journal> IEEE Transactions on Information Theory. </journal> <note> To appear. </note>
Reference-contexts: This class includes all common loss functions such as the square loss, the relative entropy loss, and the hellinger loss. For this class there are tight bounds on the additional loss <ref> (Haussler et al., 1998) </ref> of the algorithm over the loss of the best expert (i.e., the non-shifting case). <p> A smaller value of c leads to a smaller loss bound (see Lemma 1). The c values for pred Vovk (cf. column two of Figure 2) are optimal for a large class of loss functions <ref> (Haussler et al., 1998) </ref>. The proof of the loss bounds for each of the algorithms is based on the following lemma. <p> To obtain a slightly tighter bound we could also have used the Vee Algorithm for the absolute loss, which is ((2 ln 2 1+e ) 1 ; )-realizable <ref> (Haussler et al., 1998) </ref>. This algorithm takes O (n log n) time to produce its prediction. Both the weighted mean and the Vee prediction allow the outcomes to lie in [0; 1].
Reference: <author> Helmbold, D. P., Kivinen, J., & Warmuth, M. K. </author> <year> (1995). </year> <title> Worst-case loss bounds for sigmoided linear neurons. </title> <booktitle> In Proceedings of the 1995 Neural Information Processing Conference, </booktitle> <pages> (pp. 309-315). </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference: <author> Helmbold, D.P., Long, D.D.E., & Sherrod, B. </author> <year> (1996). </year> <title> A dynamic disk spin-down technique for mobile computing. </title> <booktitle> In Proceedings of the Second Annual ACM International Conference on Mobile Computing and Networking. </booktitle> <address> ACM/IEEE. </address>
Reference-contexts: In a practical application, no worst-case loss bounds may be provable for the given loss function. However, the share updates may still be useful. For an interesting application to the prediction of disk idle time see the work of Helmbold et al. <ref> (Helmbold, Long & Sherrod, 1996) </ref>. The square, relative entropy and hellinger losses are (c; )-realizable for both pred wmean and pred Vovk with ( = 1=c). The values of c (and hence of ) for the two prediction functions are summarized in Figure 2. <p> Our focus in this paper was to track the prediction of the best expert for the same class of loss functions for which the original Static-expert Algorithm of Vovk was developed (Vovk, 1998; Haussler et al., 1998). Our share updates have been applied experimentally for predicting disk idle times <ref> (Helmbold et al., 1996) </ref> and for the on-line management of investment portfolios (Singer, 1997). In addition, a reduction has been shown between expert and metrical task systems algorithms (Blum & Burch, 1997). The Share Update has been used successfully in the new domain of metrical task systems.
Reference: <author> Herbster, M. </author> <year> (1997). </year> <title> Tracking the best expert II. Unpublished Manuscript. TRACKING THE BEST EXPERT 29 Herbster, </title> <editor> M. & Warmuth, M. K. </editor> <year> (1995). </year> <title> Tracking the best expert. </title> <booktitle> In Proceedings of the 12th International Conference on Machine Learning, </booktitle> <pages> (pp. 286-294). </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kivinen, J. & Warmuth, M. K. </author> <year> (1997). </year> <title> Additive versus exponentiated gradient updates for linear prediction. </title> <journal> Information and Computation, </journal> <volume> 132(1), </volume> <pages> 1-64. </pages>
Reference: <author> Littlestone, N. </author> <year> (1988). </year> <title> Learning when irrelevant attributes abound: A new linear-threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> 285-318. </pages>
Reference: <author> Littlestone, N. </author> <year> (1989). </year> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, Technical Report UCSC-CRL-89-11, </type> <institution> University of California Santa Cruz. </institution>
Reference: <author> Littlestone, N. & Warmuth, M. K. </author> <year> (1994). </year> <title> The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108(2), </volume> <pages> 212-261. </pages>
Reference-contexts: For example, if the loss function is the square or relative entropy loss, then c = 1 2 or c = 1, respectively (see Section 2 for definitions of the loss functions). In the paper we consider a modification of the above goal introduced by Littlestone and Warmuth <ref> (Littlestone & Warmuth, 1994) </ref>, in which the sequence of examples is subdivided into k + 1 segments of arbitrary length and distribution. Each segment has an associated expert. The sequence of segments and its associated sequence of experts is called a partition. <p> The essential ingredient for our success in a non-stationary setting, seems to be an algorithm for the stationary setting with a multiplicative weight update whose loss bound grows logarithmically with the dimension of the problem. Besides Vovk's Aggregating Algorithm (Vovk, 1998) and the Weighted Majority Algorithm <ref> (Littlestone & Warmuth, 1994) </ref>, which only use the Loss Update, and are the basis of this work, a number of such algorithms have been developed. <p> However, the algorithms are not limited to these loss functions. The techniques in (Vovk, 1998; Haussler et al., 1998; Warmuth, 1997) can determine the constants c and for a wide class of loss functions. The algorithm is also easy to adapt for classification by using the majority vote <ref> (Littlestone & Warmuth, 1994) </ref> for the prediction function, and counting mistakes for the loss. In a practical application, no worst-case loss bounds may be provable for the given loss function. However, the share updates may still be useful. <p> Thus the tuning is more complex, and for the sake of simplicity we use the weighted mean prediction <ref> (Littlestone & Warmuth, 1994) </ref> in this section. Theorem 3 (Littlestone & Warmuth, 1994) For 2 [0; 1), the absolute loss function L abs (p; q) = jp qj is ( 1 1e ; )-realizable for the prediction function pred wmean (v; x). <p> Thus the tuning is more complex, and for the sake of simplicity we use the weighted mean prediction <ref> (Littlestone & Warmuth, 1994) </ref> in this section. Theorem 3 (Littlestone & Warmuth, 1994) For 2 [0; 1), the absolute loss function L abs (p; q) = jp qj is ( 1 1e ; )-realizable for the prediction function pred wmean (v; x). <p> The Fixed-share Algorithm works well when the loss function can be unbounded, and the Variable-share Algorithm is suitable for the case when the range of the loss lies in [0,1]. The first method is essentially the same as the one used in the Wml algorithm of <ref> (Littlestone & Warmuth, 1994) </ref> and a recent alternate developed in (Auer & Warmuth, 1998) for learning shifting disjunctions. When the loss is the discrete loss (as in classification problems), then these methods are simple and effective if the algorithm only updates after a mistake occurs (i.e., conservative updates). <p> However, if this expert is starting to incur large loss, then it shares weight with the other experts, helping the next best expert to recover its weight from zero. The methods presented here and in <ref> (Littlestone & Warmuth, 1994) </ref> have inspired a number of recent papers. Auer and Warmuth (1998) adapted the Winnow algorithm to learn shifting disjunctions. Comparing against the best shifting disjunction is more complicated than comparing against the best expert.
Reference: <author> Singer, Y. </author> <year> (1997). </year> <title> Towards realistic and competitive portfolio selection algorithms. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: Our share updates have been applied experimentally for predicting disk idle times (Helmbold et al., 1996) and for the on-line management of investment portfolios <ref> (Singer, 1997) </ref>. In addition, a reduction has been shown between expert and metrical task systems algorithms (Blum & Burch, 1997). The Share Update has been used successfully in the new domain of metrical task systems. A natural probabilistic interpretation of the Share algorithms has recently been given in (Vovk, 1997). <p> Theoretical techniques exist for the Fixed-share Algorithm for eliminating the need to choose the value of ff ahead of time. One method for tuning parameters (among other things) is the "specialist" framework of <ref> (Freund, Schapire, Singer & Warmuth, 1997) </ref>, even though the bounds produced this way are not always optimal. Another method incorporates a prior distribution on all possible values of ff.
Reference: <author> Vovk, Y. </author> <year> (1998). </year> <title> A game of prediction with expert advice. </title> <journal> Journal of Computer and System Sciences. </journal> <note> To appear. </note>
Reference-contexts: The "master" algorithm that combines the experts' predictions does not need to know the particular problem domain. It simply keeps one weight per expert, representing the belief in the expert's prediction, and then decreases the weight as a function of the loss of the expert. Previous work of Vovk <ref> (Vovk, 1998) </ref> and others (Littlestone & Warmuth, 1994; Haussler, Kivinen & Warmuth, 1998) has produced an algorithm for which there is an upper bound on the additional loss of the algorithm over the loss of the best expert. <p> The essential ingredient for our success in a non-stationary setting, seems to be an algorithm for the stationary setting with a multiplicative weight update whose loss bound grows logarithmically with the dimension of the problem. Besides Vovk's Aggregating Algorithm <ref> (Vovk, 1998) </ref> and the Weighted Majority Algorithm (Littlestone & Warmuth, 1994), which only use the Loss Update, and are the basis of this work, a number of such algorithms have been developed. <p> In the simplest case, the algorithm predicts with the weighted mean of the experts' predictions, i.e., pred (v; x) = v x. A more sophisticated prediction function introduced by Vovk <ref> (Vovk, 1998) </ref> will be discussed in Section 4. After predicting, the algorithm performs two update steps. The first update is the Loss Update; the second is the Share Update.
Reference: <author> Vovk, V. </author> <year> (1997). </year> <title> Derandomizing stochastic prediction strategies. </title> <booktitle> In Proceedings of the 10th Annual Workshop on Computational Learning Theory. </booktitle> <publisher> ACM Press, </publisher> <address> New York, NY. </address>
Reference-contexts: In addition, a reduction has been shown between expert and metrical task systems algorithms (Blum & Burch, 1997). The Share Update has been used successfully in the new domain of metrical task systems. A natural probabilistic interpretation of the Share algorithms has recently been given in <ref> (Vovk, 1997) </ref>. In any particular application of the Share algorithms, it is necessary to consider how to choose the parameter ff. Theoretical techniques exist for the Fixed-share Algorithm for eliminating the need to choose the value of ff ahead of time. <p> If we replace the assumption that k ^ k by 2 ^ k ^ `, we obtain a bound where the final term c ^ k is replaced by 2c ^ k: 4. Vovk has recently proved a sharper bound for this algorithm <ref> (Vovk, 1997) </ref>: L (S; A) c ln n + c [ + ln 1 ff 1 e + ffe + ln (n 1)]: 5. Unlike Corollary 2 we do not need a lower bound on k. 6.
Reference: <author> Warmuth, M. K. </author> <year> (1997). </year> <title> Predicting with the dot-product in the experts framework. </title> <type> Unpublished Manuscript. </type>
Reference-contexts: Prediction Functions and Proof Techniques We consider two choices of prediction functions. The simplest prediction is the weighted mean <ref> (Warmuth, 1997) </ref>: pred wmean (v; x) = n X v i x i : (8) A more sophisticated prediction function giving slightly better bounds was introduced by Vovk (Vovk, 1998; Haussler et al., 1998). Define L 0 (z) = L (0; z) and L 1 (z) = L (1; z). <p> Theoretical techniques exist for the Fixed-share Algorithm for eliminating the need to choose the value of ff ahead of time. One method for tuning parameters (among other things) is the "specialist" framework of <ref> (Freund, Schapire, Singer & Warmuth, 1997) </ref>, even though the bounds produced this way are not always optimal. Another method incorporates a prior distribution on all possible values of ff.
References-found: 19

