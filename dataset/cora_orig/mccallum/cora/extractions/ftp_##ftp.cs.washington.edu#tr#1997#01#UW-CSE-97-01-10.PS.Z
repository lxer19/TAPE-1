URL: ftp://ftp.cs.washington.edu/tr/1997/01/UW-CSE-97-01-10.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Title: A Generalization and Improvement to PPM's "Blending" 1  
Author: Suzanne Bunton 
Keyword: data compression, universal coding, on-line stochastic model ing, statistical inference, finite-state automata  
Affiliation: Department of Computer Science and Engineering University of Washington  
Pubnum: Technical Report UW-CSE-97-01-10  
Abstract: The best-performing method in the data compression literature for computing probability estimates of sequences on-line using a suffix-tree model is the blending technique used by PPM. Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. We show by decomposition into an inheritance evaluation time and a mixture weighting function that mixtures generalize the techniques used in PPM variants. Doubly controlled experiments with our executable taxonomy of on-line sequence modeling algorithms and the Calgary Corpus demonstrate the impact of varying inheritance evaluation time, mixture weighting function, and including update exclusion. 1 Portions of this paper also appear in Proceedings of the DCC, March 1997 
Abstract-found: 1
Intro-found: 1
Reference: [BCW90] <author> T. C. Bell, J. G. Cleary, and I. H. Witten. </author> <title> Text Compression. Advanced Reference Series. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1990. </year>
Reference-contexts: In <ref> [BCW90] </ref> this approach is called "full blending." However, computing such weighted averages is expensive, and computations cannot be reused between visits to a given set of excited states. Furthermore, there was no published evidence prior to this work that it produces better probability estimates|no published on-line algorithms use it.
Reference: [Bun96] <author> S. Bunton. </author> <title> On-Line Stochastic Processes in Data Compression. </title> <type> PhD thesis, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1996. </year>
Reference-contexts: variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree FSM (see Chapter 2 of <ref> [Bun96] </ref>), where the excited states are those states of the FSM whose associated conditioning context partitions contain the sequence a 1 a 2 a i1 2 A fl . <p> Lastly, let the node-count function count : S ! R be defined as follows: count (s) = X a:count [a;s;u (s)]&gt;0 count (a; s): 2 Chapter 5 of <ref> [Bun96] </ref> explains that the ability to dynamically select update-excluded frequencies or full-update frequencies on a per-state basis is required for correctly combining mixtures, update exclusion (introduced in [Mof90]), and state selection. 1 Given the above definitions, a simple bottom-up procedure for recursively com-puting a mixture that estimates the probability of a <p> The DMC algorithm, which originally used a binary alphabet, adds each new state to its model by "cloning" an eligible parent state. Each clone receives a scaled copy of the parent state's frequency distribution the moment it is added. Since, as we proved in Chapter 4 of <ref> [Bun96] </ref>, the conditioning context relationship among clones and parent states is equivalent to that among suffixes in other suffix-tree models, inherit at state creation corresponds to the numerical aspects of cloning. <p> For example, PPM's blending is a forgetful type of mixture that lazily evaluates its inheritances as novel events occur, while DMC's "cloning" [CH87] produces a mixture that evaluates its inheritances when new states are added, but which also subtracts the inherited frequency from the parent distribution (see Chapter 6 of <ref> [Bun96] </ref> for details). The weighting functions and inheritance evaluation times are independent of each other and of whether inheritances are subtracted from parent distributions; thus mixtures generalize both PPM's blending and the quantitative aspects of DMC's cloning. 12
Reference: [Bun97] <author> S. Bunton. </author> <title> A percolating state selector for suffix-tree context models. </title> <booktitle> In Proceedings Data Compression Conference. </booktitle> <publisher> IEEE Computer Society Press, </publisher> <month> March </month> <year> 1997. </year>
Reference-contexts: a i in a 1 a 2 a n , where ^s i is a state that is specially selected as the starting node for the recursive mixture computation using the states excited by a 1 a 2 a i1 . (State selection is the topic of the companion paper <ref> [Bun97] </ref>.) For the present discussion, assume that ^s i is the maximum-order excited state at time i).
Reference: [CH87] <author> G. V. Cormack and R. N. S. Horspool. </author> <title> Data compression using dynamic Markov mod-elling. </title> <journal> The Computer Journal, </journal> <volume> 30(6) </volume> <pages> 541-550, </pages> <year> 1987. </year>
Reference-contexts: Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. We shall show by decomposition that mixtures generalize the techniques used in DMC variants <ref> [CH87, TR93] </ref>, as well as PPM variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree <p> For example, PPM's blending is a forgetful type of mixture that lazily evaluates its inheritances as novel events occur, while DMC's "cloning" <ref> [CH87] </ref> produces a mixture that evaluates its inheritances when new states are added, but which also subtracts the inherited frequency from the parent distribution (see Chapter 6 of [Bun96] for details).
Reference: [CW84] <author> J. G. Cleary and I. H. Witten. </author> <title> Data compression using adaptive coding and partial string matching. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 32(4) </volume> <pages> 396-402, </pages> <year> 1984. </year>
Reference-contexts: of this paper also appear in Proceedings of the DCC, March 1997 Generalization and Improvement to PPM's "Blending" Suzanne Bunton The University of Washington The best-performing method in the data compression literature for computing probability estimates of sequences on-line using a suffix-tree model is the blending technique used by PPM <ref> [CW84, Mof90] </ref>. Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. <p> That is, count (s) = X a : a is not excluded count [a; s; u (s)] &gt; 0 count (a; s): 7 Unless stated otherwise, we shall assume that exclusions are enabled in all computa-tions described from here on. 4.2 Blending's Missing Term Blending <ref> [CW84] </ref> evaluates the ancestor likelihood P e (ajsuffix (s)) before novel event updates, but at all subsequent occurrences of any string in the set L (s) a.
Reference: [How93] <author> P. G. Howard. </author> <title> The Design and Analysis of Efficient Lossless Data Compression Systems. </title> <type> PhD thesis, </type> <institution> Brown University, </institution> <year> 1993. </year>
Reference-contexts: Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function <ref> [How93] </ref>. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold [Mof90].
Reference: [Mof90] <author> A. Moffat. </author> <title> Implementing the PPM data compression scheme. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 38(11) </volume> <pages> 1917-1921, </pages> <year> 1990. </year>
Reference-contexts: of this paper also appear in Proceedings of the DCC, March 1997 Generalization and Improvement to PPM's "Blending" Suzanne Bunton The University of Washington The best-performing method in the data compression literature for computing probability estimates of sequences on-line using a suffix-tree model is the blending technique used by PPM <ref> [CW84, Mof90] </ref>. Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. <p> count : S ! R be defined as follows: count (s) = X a:count [a;s;u (s)]&gt;0 count (a; s): 2 Chapter 5 of [Bun96] explains that the ability to dynamically select update-excluded frequencies or full-update frequencies on a per-state basis is required for correctly combining mixtures, update exclusion (introduced in <ref> [Mof90] </ref>), and state selection. 1 Given the above definitions, a simple bottom-up procedure for recursively com-puting a mixture that estimates the probability of a given event, a i = a, starting from an excited state s, is P e (ajs; i) = &lt; W (s) count (a;s) count (s) + (1 <p> Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function [How93]. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold <ref> [Mof90] </ref>. However, regardless of whatever merit direct techniques for recency-weighting stored frequencies may have (none has been shown to consistently improve predictions of blended techniques), these approaches each add an additional feature to the model. <p> This is best accomplished with a technique known as exclusion (not to be confused with update exclusion), which was developed for PPM <ref> [Mof90] </ref>. The basic idea is this: when exclusion is enabled for the model, only consider the frequencies of event ajsuffix (s) if the higher-order descendant s is currently excited and event ajs has not occurred before.
Reference: [TR93] <author> J. Teuhola and T. Raita. </author> <title> Application of a finite-state model to text compression. </title> <journal> The Computer Journal, </journal> <volume> 36(7) </volume> <pages> 607-614, </pages> <year> 1993. </year>
Reference-contexts: Blending can be viewed as a bottom-up recursive procedure for computing a mixture, barring one missing term for each level of the recursion, where a mixture is basically a weighted average of several probability estimates. We shall show by decomposition that mixtures generalize the techniques used in DMC variants <ref> [CH87, TR93] </ref>, as well as PPM variants, and thus these techniques, along with other variants of mixtures, are interchangeable. 1 Recursive Mixtures We are concerned with estimating a probability P e (a i ja 1 a 2 a i1 ) using the frequencies stored at the excited states of a suffix-tree
Reference: [WB91] <author> I. H. Witten and T. C. Bell. </author> <title> The zero-frequency problem: Estimating the probabilities of novel events in adaptive text compression. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 37(4) </volume> <pages> 1085-1094, </pages> <year> 1991. </year>
Reference-contexts: Thus the choice of weighting function reduces to a solution to an ancient problem|the "zero-frequency problem," or how to assign a likelihood to an event that has never occurred before|for which it is widely agreed that no principled solution exists, in the absence of a priori knowledge <ref> [WB91] </ref>. Therefore, the merit of any weighting function for a universal model is determined analytically by how the assumptions it imposes interact with other assumptions made in the model, and empirically by its performance on actual data. <p> Several approaches to solving the zero frequency problem, known as "escape" mechanisms, have been used successfully with PPM implementations. Four of the simplest and best-performing escape mechanisms are known in the literature as `A,' `B,' `C,' and `D' <ref> [WB91] </ref>. In this section, we shall show how these simple escape mechanisms correspond to different weighting functions W (s).
Reference: [Wil91] <author> R. N. Williams. </author> <title> Adaptive Data Compression. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> Massachusetts, </address> <year> 1991. </year> <month> 13 </month>
Reference-contexts: For example, a sliding window of input history can be kept, and as sequence symbols that have passed through the buffer pass out of the buffer, the event frequencies originally incremented by these symbols can be decremented <ref> [Wil91] </ref>. Alternatively, at regular intervals, all the frequencies in the model can be scaled by a small constant, which would implement an exponential decay function [How93]. Or, the same process could be carried on locally, on a per-state basis, when the state's total frequency exceeded a threshold [Mof90].
References-found: 10

