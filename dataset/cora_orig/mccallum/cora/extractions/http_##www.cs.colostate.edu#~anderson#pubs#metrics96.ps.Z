URL: http://www.cs.colostate.edu/~anderson/pubs/metrics96.ps.Z
Refering-URL: http://www.cs.colostate.edu/~anderson/pubs/pubs.html
Root-URL: 
Email: anderson@cs.colostate.edu avm@cs.colostate.edu tchen@cs.colostate.edu  
Title: Assessing Neural Networks as Guides for Testing Activities  
Author: Charles Anderson Anneliese von Mayrhauser Tom Chen 
Address: Fort Collins, CO 80523 Fort Collins, CO 80523 Fort Collins, CO  
Affiliation: Colorado State University Colorado State University Colorado State University Computer Science Department Computer Science Department Electrical Eng. Department  
Abstract: As test case automation increases, the volume of tests can become a problem. Further, it may not be immediately obvious whether the test generation tool generates effective test cases. Indeed, it might be useful to have a mechanism that is able to learn, based on past history, which test cases are likely to yield more failures versus those that are not likely to uncover any. We present experimental results on using a neural network for pruning a testcase set while preserving its effectiveness. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Charles W. Anderson, Judy A. Franklin, and Richard S. Sutton. </author> <title> "Learning a Nonlinear Model of a Manufacturing Process Using Multilayer Connectionist Networks," </title> <booktitle> Proc. of the 5th IEEE International Symposium on Intelligent Control, </booktitle> <address> Philadelphia, PA, </address> <month> September </month> <year> 1990, </year> <pages> pp. 404-409. </pages>
Reference-contexts: Then, we trained the network to recognize relationships between test case descriptors and faults. Once trained, the network acts as a fault predictor for new test cases. Related work includes the use of neural networks to predict quality in a manufacturing process <ref> [1] </ref>. The second scenario evaluates how well neural networks predict test coverage of VHDL models 1 .
Reference: [2] <author> DeMillo, R., A.; Offutt, A., J.; </author> <title> "Constraint-Based Automatic Test Data Generation", </title> <journal> IEEE Transactions on Software Engineering SE-17, </journal> <volume> 9(Sept. </volume> <year> 1991), </year> <pages> pp. 900-910. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [7]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [2, 6, 20] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool). <p> Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [18, 19, 5]. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are <ref> [2, 16] </ref>. Prudent test case reduction prunes a test set by eliminating those test cases that are not likely to yield any failures. This paper concentrates on experimental effectiveness analysis and prediction for two test scenarios.
Reference: [3] <author> Laurene Fausett. </author> <title> Fundamentals of Neural Networks. </title> <publisher> Prentice Hall: </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1994. </year>
Reference-contexts: Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. Neural networks have been used as memories, pattern recall devices, pattern classifiers, and general function mapping engines <ref> [3, 8, 21] </ref>. Test effectiveness evaluation concentrates on their use as pattern classifiers. A classifier maps input vectors to output vectors in two phases. The network learns the input-output classification from a set of training vectors. After training, the network acts as a classifier for new vectors. or neuron). <p> A net with too few hidden units cannot learn the mapping to the required accuracy. Too many hidden units allow the net to "memorize" the training data and does not generalize well to new data. Backpropagation is the most popular training algorithm for multilayer neural networks <ref> [3, 8, 21] </ref>. The algorithm initializes the network with a random set of weights, and the network trains from a set of input-output pairs. Each pair requires a two-stage learning algorithm: forward pass and backward pass. <p> First, the error passes from the output layer to the hidden layer updating output weights. Next, each hidden unit calculates an error based on the error from each output unit. The error from the hidden units updates input weights. Details about the Backpropagation algorithm can be found in <ref> [3, 8, 21] </ref>. One training epoch passes when the network sees all input-output pairs in the training set. Training stops when the sum squared error is acceptable or when a predefined number of epochs passes.
Reference: [4] <author> R. Hamlet, R. </author> <title> Taylor; "Partition Testing does not inspire confidence", </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 16, no. 12(Dec. </volume> <year> 1990), </year> <pages> pp. 1402-1411. </pages>
Reference: [5] <author> P. Frankl, S. </author> <title> Weiss; "An experimental comparison of the effectiveness of branch testing and data flow testing", </title> <journal> Transactions on Software Engineering vol. </journal> <volume> 19, no. </volume> <month> 8(August </month> <year> 1993), </year> <pages> pp. 774-787. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [11, 17]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [18, 19, 5] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [2, 16].
Reference: [6] <author> Higashino, T., v. Bochman, G.; </author> <title> "Automatic Analysis and Test Case Derivation for a Restricted Class of LOTOS Expressions with Data Parameters", </title> <journal> IEEE Transactions on Software Engineering SE-20, </journal> <month> 1(January </month> <year> 1994), </year> <pages> pp. 29-42. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [7]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [2, 6, 20] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).
Reference: [7] <author> W. </author> <title> Howden; Functional Testing and Analysis, </title> <publisher> McGraw-Hill, </publisher> <address> New York, NY, </address> <year> 1987. </year>
Reference-contexts: Examples are test generation tools based on symbolic execution that ensure various types of white box testing coverage [12]. This should come as no surprise, since the general test data generation problem is undecidable <ref> [7] </ref>. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data [2, 6, 20], or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).
Reference: [8] <editor> J.L. McClelland, D.E. Rumelhart, </editor> <booktitle> and the PDP Research Group. Parallel Distributed Processing: Exploration in the Microstructure of Cognition, </booktitle> <volume> vol 1. </volume> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Section 4 describes the experiment with the VHDL model. The analysis uses a realistic VHDL design. Section 5 summarizes our conclusions. 2 BACKGROUND ON NEURAL NETWORKS 2.1 CLASSIFIERS Neural network methods were developed to model the neural architecture and computation of the human brain <ref> [8] </ref>. A neural network consists of simple neuron-like processing elements. Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. <p> Processing elements are interconnected by a network of weighted connections that encode network knowledge. Neural networks are highly parallel and exercise distributed control. They emphasize automatic learning. Neural networks have been used as memories, pattern recall devices, pattern classifiers, and general function mapping engines <ref> [3, 8, 21] </ref>. Test effectiveness evaluation concentrates on their use as pattern classifiers. A classifier maps input vectors to output vectors in two phases. The network learns the input-output classification from a set of training vectors. After training, the network acts as a classifier for new vectors. or neuron). <p> A net with too few hidden units cannot learn the mapping to the required accuracy. Too many hidden units allow the net to "memorize" the training data and does not generalize well to new data. Backpropagation is the most popular training algorithm for multilayer neural networks <ref> [3, 8, 21] </ref>. The algorithm initializes the network with a random set of weights, and the network trains from a set of input-output pairs. Each pair requires a two-stage learning algorithm: forward pass and backward pass. <p> First, the error passes from the output layer to the hidden layer updating output weights. Next, each hidden unit calculates an error based on the error from each output unit. The error from the hidden units updates input weights. Details about the Backpropagation algorithm can be found in <ref> [3, 8, 21] </ref>. One training epoch passes when the network sees all input-output pairs in the training set. Training stops when the sum squared error is acceptable or when a predefined number of epochs passes.
Reference: [9] <author> Robert G.D. Steel and James H. Torrie. </author> <title> Principles and Procedures of Statistics: A Biometrical Approach, second edition. </title> <publisher> McGraw-Hill: </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: The minimum value for each input parameter is set to 0.0 and the maximum value is 2 Scaling reduces the side effects of scale differences between parameters. Linear, square root, logarithm, and general data transformation are typical scaling methods <ref> [9] </ref>. set to 1.0. Output parameters are set to 0:1 for a "zero" response and 0:9 for a "one" response. The test data set for neural net training included 180 observations, thirty test cases for each of the six test objectives.
Reference: [10] <author> Maureen Schaffer and Tom Chen. </author> <title> A vlsi architecture for 2d object classification based on tree matching. </title> <booktitle> In Proceedings of the International Conference on Computer Architectures for Machine Perception, </booktitle> <month> September </month> <year> 1995. </year>
Reference-contexts: This result also indicates that we need to add other test case metrics to improve neural net predictions. 4 EXPERIMENT DESIGN - VHDL CODE COVERAGE 4.1 VHDL Model and Branch Coverage Data For the experiment we chose a VHDL model of a VLSI chip used for visual object recognition <ref> [10] </ref>. This model has about 3,700 lines of VHDL code. The code is of the same character and complexity as is found in microprocessors and thus represents a realistic problem. It presents a challenging design validation task, because it consists largely of random logic.
Reference: [11] <author> A. Parrish, S. </author> <title> Zweben; "Analysis and Refinement of Software Test Data Adequacy Properties", </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 17, no. </volume> <month> 6(June </month> <year> 1991), </year> <pages> pp. 565-581. </pages>
Reference-contexts: Simplification is obviously not a problem when test data are effective (reveal faults). This makes it very important to evaluate a test generation method experimentally and/or analytically. Analytical evaluation considers whether the testing criteria meet adequacy axioms <ref> [11, 17] </ref>. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [18, 19, 5].
Reference: [12] <author> Gmeiner, L.; Voges, U.; von Mayrhauser, A.; </author> <title> "SADAT-An Automated Testing Tool", </title> <journal> IEEE Transactions on Software Engineering SE-6, </journal> <month> 3(May </month> <year> 1980), </year> <pages> pp. 286-290. </pages>
Reference-contexts: Examples are test generation tools based on symbolic execution that ensure various types of white box testing coverage <ref> [12] </ref>. This should come as no surprise, since the general test data generation problem is undecidable [7].
Reference: [13] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz, </author> <title> "Testing Applications Using Domain Based Testing and Sleuth," </title> <booktitle> Proceedings of the Fifth International Software Reliability Engineering Symposium, </booktitle> <address> Monterey, </address> <month> November </month> <year> 1994, </year> <pages> pp. 206-215. </pages>
Reference-contexts: This paper concentrates on experimental effectiveness analysis and prediction for two test scenarios. The first scenario analyzes cost-effectiveness predictions by a neural network for system tests generated using Sleuth <ref> [13] </ref> and application domain based testing [15]. We used a neural network as a classifier to learn about the system under test and to predict the fault exposure capability of newly generated test cases. <p> Section 3 introduces the experimental design to evaluate how well neural networks work when used as test effectiveness predictors. As example testing technique we use Domain Based Testing <ref> [13] </ref> and its associated test data generation tool SLEUTH [15]. The results are useful in two ways. First, they point out which of the generated test suites are likely to trigger what types of incidents. <p> The study used the DBT test generation tool Sleuth to generate test data <ref> [15, 13] </ref>. Using test case metrics, a synthetic test oracle evaluated each test case for error classification. The neural net trained on test metric input patterns and mapped them to the test oracle's error classification.
Reference: [14] <author> Anneliese von Mayrhauser, Richard Mraz, Jeff Walls, and Pete Ocken. </author> <title> "Domain Based Testing: Increasing Test Case Reuse," </title> <booktitle> Proc. of the International Conference on Computer Design, </booktitle> <address> Boston, </address> <month> October </month> <year> 1994, </year> <pages> pp. 484-491. </pages>
Reference: [15] <author> Anneliese von Mayrhauser, Jeff Walls, and Richard Mraz. "Sleuth: </author> <title> A Domain Based Testing Tool", </title> <booktitle> Proc. of the International Test Conference, </booktitle> <month> October, </month> <year> 1994. </year> <note> [16] von Mayrhauser, </note> <author> A.; Anderson, Ch.; Mraz, R.; </author> <title> "Using A Neural Network to Predict Test Case Effectiveness", Procs. </title> <booktitle> IEEE Aerospace Applications Conference, </booktitle> <address> Snowmass, CO, </address> <month> Feb. </month> <year> 1995 </year>
Reference-contexts: This paper concentrates on experimental effectiveness analysis and prediction for two test scenarios. The first scenario analyzes cost-effectiveness predictions by a neural network for system tests generated using Sleuth [13] and application domain based testing <ref> [15] </ref>. We used a neural network as a classifier to learn about the system under test and to predict the fault exposure capability of newly generated test cases. We describe attributes of test cases to the neural network as inputs and relate them to resulting faults (neural network outputs). <p> Section 3 introduces the experimental design to evaluate how well neural networks work when used as test effectiveness predictors. As example testing technique we use Domain Based Testing [13] and its associated test data generation tool SLEUTH <ref> [15] </ref>. The results are useful in two ways. First, they point out which of the generated test suites are likely to trigger what types of incidents. <p> The study used the DBT test generation tool Sleuth to generate test data <ref> [15, 13] </ref>. Using test case metrics, a synthetic test oracle evaluated each test case for error classification. The neural net trained on test metric input patterns and mapped them to the test oracle's error classification.
Reference: [17] <author> E. </author> <title> Weyuker; "Axiomatizing Software Test Data Adequacy", </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 12, no. 12(Dec. </volume> <year> 1986), </year> <pages> pp. 1128-1138. </pages>
Reference-contexts: Simplification is obviously not a problem when test data are effective (reveal faults). This makes it very important to evaluate a test generation method experimentally and/or analytically. Analytical evaluation considers whether the testing criteria meet adequacy axioms <ref> [11, 17] </ref>. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include [18, 19, 5].
Reference: [18] <author> E. </author> <title> Weyuker; "The Cost of Data Flow Testing: An Empirical Study", </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 16, no. 2(Feb. </volume> <year> 1990), </year> <pages> pp. 121-128. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [11, 17]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [18, 19, 5] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [2, 16].
Reference: [19] <author> E. </author> <title> Weyuker; "More Experience with Data Flow Testing", </title> <journal> IEEE Transactions on Software Engineering vol. </journal> <volume> 19, no. 9(Sep. </volume> <year> 1993), </year> <pages> pp. 912-919. </pages>
Reference-contexts: Analytical evaluation considers whether the testing criteria meet adequacy axioms [11, 17]. They describe properties any testing criteria should have to be considered adequate. They do not, however, guarantee high failure yields. Thus experimental evaluation of testing techniques is also important. Examples of such evaluations include <ref> [18, 19, 5] </ref>. When the number of tests to be run can become very large, test case reduction becomes desirable. Examples of reducing the number of test cases are [2, 16].
Reference: [20] <author> Weyuker, E., Goradia, T., Singh, A.; </author> <title> "Automatically Generating Test Data from a Boolean Specification", </title> <journal> IEEE Transactions on Software Engineering SE-20, </journal> <month> 5(May </month> <year> 1994), </year> <pages> pp. 353-363. </pages>
Reference-contexts: This should come as no surprise, since the general test data generation problem is undecidable [7]. In practice, many test data generation systems make simplifying assumptions, either as to the power of the language for which they generate test data <ref> [2, 6, 20] </ref>, or as to which information they consider in driving test data generation (e.g., Robert Poston's T tool).

References-found: 19

