URL: http://c.gp.cs.cmu.edu:5103/afs/cs/academic/class/15882-s98/www/dayan-helmholtz.ps.gz
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/academic/class/15882-s98/www/syllabus.html
Root-URL: http://www.cs.cmu.edu
Title: The Helmholtz Machine  
Author: Peter Dayan Geoffrey E Hinton Radford M Neal Richard S Zemel 
Date: 12 th October 1994  
Address: 6 King's College Road PO Box 85800 Toronto, Ontario M5S 1A4 San Diego, CA 92186-5800 Canada USA  
Affiliation: 1 Department of Computer Science 2 CNL University of Toronto The Salk Institute  
Abstract: Discovering the structure inherent in a set of patterns is a fundamental aim of statistical inference or learning. One fruitful approach is to build a parameterised stochastic generative model, independent draws from which are likely to produce the patterns. For all but the simplest generative models, each pattern can be generated in exponentially many ways. It is thus intractable to adjust the parameters to maximize the probability of the observed patterns, We describe a way of finessing this combinatorial explosion by maximising an easily computed lower bound on the probability of the observations. Our method can be viewed as a form of hierarchical self-supervised learning that may relate to the function of bottom-up and top-down cortical processing pathways.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ackley, DH, Hinton, GE & Sejnowski, </author> <title> TJ (1985). A learning algorithm for Boltzmann machines. </title> <journal> Cognitive Science, </journal> <volume> 9, </volume> <pages> 147-169. </pages>
Reference: [2] <author> Barto, AG & Anandan, </author> <title> P (1985). Pattern recognizing stochastic learning automata. </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> 15, </volume> <pages> 360-374. </pages>
Reference: [3] <author> Becker, S & Hinton, </author> <title> GE (1992). A self-organizing neural network that discovers surfaces in random-dot stereograms. </title> <journal> Nature, </journal> <volume> 355, </volume> <pages> 161-163. </pages>
Reference: [4] <author> Carpenter, G & Grossberg, </author> <title> S (1987). A massively parallel architecture for a self-organizing neural pattern recognition machine. Computer Vision, </title> <journal> Graphics and Image Processing, </journal> <volume> 37, </volume> <pages> 54-115. </pages>
Reference: [5] <author> Dayan, P & Zemel, </author> <title> RS (1994). Competition and multiple cause models. Neural Computation, </title> <publisher> in press. </publisher>
Reference: [6] <author> Dempster, AP, Laird, NM & Rubin, </author> <title> DB (1976). Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Proceedings of the Royal Statistical Society, </journal> <pages> 1-38. </pages>
Reference: [7] <author> Grenander, </author> <title> U (1976-1981). Lectures in Pattern Theory I, II and III: Pattern Analysis, Pattern Synthesis and Regular Structures. </title> <publisher> Berlin: Springer-Verlag., Berlin, </publisher> <pages> 1976-1981). </pages>
Reference: [8] <author> Hinton, GE & Sejnowski, </author> <title> TJ (1986). Learning and relearning in Boltzmann machines. </title> <editor> In DE Rumelhart, JL McClelland and the PDP research group, editors, </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. Volume 1: Foundations, </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> 282-317. </pages>
Reference: [9] <author> Hinton, GE & Zemel, </author> <title> RS (1994). Autoencoders, minimum description length and Helmholtz free energy. </title> <editor> In JD Cowan, G Tesauro and J Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 3-10. </pages>
Reference: [10] <author> Jordan, MI & Rumelhart, </author> <title> DE (1992). Forward models: Supervised learning with a distal teacher. </title> <journal> Cognitive Science, </journal> <volume> 16, </volume> <pages> 307-354. </pages>
Reference: [11] <author> Kawato, M, Hayakama, H & Inui, </author> <title> T (1993). A forward-inverse optics model of reciprocal connections between visual cortical areas. </title> <journal> Network, </journal> <volume> 4, </volume> <pages> 415-422. </pages>
Reference: [12] <author> Kullback, </author> <title> S (1959). Information Theory and Statistics. </title> <address> New York: </address> <publisher> Wiley. </publisher>
Reference: [13] <author> MacKay, </author> <title> DM (1956). The epistemological problem for automata. </title> <editor> In CE Shan-non & J McCarthy, editors, </editor> <booktitle> Automata Studies, </booktitle> <address> Princeton, NJ: </address> <publisher> Princeton University Press, </publisher> <pages> 235-251. 11 </pages>
Reference: [14] <author> Mumford, </author> <title> D (1994). Neuronal architectures for pattern-theoretic problems. </title> <editor> In C Koch and J Davis, editors, </editor> <title> Large-Scale Theories of the Cortex. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> 125-152. </pages>
Reference: [15] <author> Neal, </author> <title> RM (1992). Connectionist learning of belief networks. </title> <journal> Artificial Intelligence, </journal> <volume> 56, </volume> <pages> 71-113. </pages>
Reference: [16] <author> Neal, RM & Hinton, </author> <title> GE (1994). A new view of the EM algorithm that justifies incremental and other variants. </title> <note> Submitted to Biometrika. </note>
Reference: [17] <author> Pearl, </author> <title> J (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible Inference. </title> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [18] <author> Pece, </author> <month> AEC </month> <year> (1992). </year> <title> Redundancy reduction of a Gabor representation: a possible computational role for feedback from primary visual cortex to lateral genicu-late nucleus. </title> <editor> In I Aleksander & J Taylor, editors, </editor> <booktitle> Artificial Neural Networks, 2. </booktitle> <address> Amsterdam: </address> <publisher> Elsevier, </publisher> <pages> 865-868. </pages>
Reference: [19] <author> Saund, </author> <title> E (1994a). Unsupervised learning of mixtures of multiple causes in binary data. </title> <editor> In JD Cowan, G Tesauro & J Alspector, editors, </editor> <booktitle> Advances in Neural Information Processing Systems, 6. </booktitle> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <pages> 27-34. </pages>
Reference: [20] <author> Saund, </author> <title> E (1994b). A multiple cause mixture model for unsupervised learning. Neural Computation, </title> <publisher> in press. </publisher>
Reference: [21] <author> Thompson, </author> <month> CJ </month> <year> (1988). </year> <title> Classical Equilibrium Statistical Mechanics. </title> <publisher> Oxford: Clarendon Press. </publisher>
Reference: [22] <author> Ullman, </author> <title> S (1994). Sequence seeking and counterstreams: A model for bidirectional information flow in the cortex. </title> <editor> In C Koch and J Davis, editors, </editor> <title> Large-Scale Theories of the Cortex. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <pages> 257-270. </pages>
Reference: [23] <author> Widrow, B & Stearns, </author> <title> SD (1985). Adaptive Signal Processing. </title> <address> Englewood Cliffs, NJ: </address> <publisher> Prentice-Hall. </publisher>
Reference: [24] <author> Williams, </author> <title> RJ (1992). Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 229-56. </pages>
Reference: [25] <author> Zemel, </author> <title> RS (1994). A Minimum Description Length Framework for Unsupervised Learning. </title> <type> PhD Dissertation, </type> <institution> Computer Science, University of Toronto, Canada. </institution>

References-found: 25

