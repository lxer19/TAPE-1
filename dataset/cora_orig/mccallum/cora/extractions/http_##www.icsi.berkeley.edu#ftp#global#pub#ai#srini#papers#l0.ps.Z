URL: http://www.icsi.berkeley.edu/ftp/global/pub/ai/srini/papers/l0.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/ftp/global/pub/ai/srini/papers/
Root-URL: http://www.icsi.berkeley.edu
Title: L 0 |The First Four Years Abstract A summary of the progress and plans of
Author: J. Feldman G. Lakoff D. Bailey S. Narayanan T. Regier A. Stolcke 
Note: 0 project as of December 1993. To be published in AI Review, Vol. 8, special issue on Integration of Natural Language and Vision Processing, edited by Paul McKevitt.  
Date: December 1993  
Abstract-found: 0
Intro-found: 1
Reference: <author> Berlin, Brent, & Paul Kay, </author> <year> 1969. </year> <title> Basic color terms: Their universality and evolution. </title> <institution> University of California Press, Berkeley. </institution>
Reference-contexts: In this respect it resembles the domain of color, 8 LM LM LM Mixtec siki Mixtec sini German auf German an English onEnglish above English above English on (b)(a) LM another objectively measurable domain which has attracted work in semantics <ref> (Berlin & Kay 1969) </ref>. In addition, space (along with time) has a privileged position as a fundamental conceptual structuring device in language, a position which most other domains, color included, do not share. <p> Decades of work by people from a wide range of disciplines has produced an understanding of this (admittedly very narrow) aspect of cognition that seems as solid and incontestable as any in the sciences <ref> (Berlin & Kay 1969) </ref>. Our claim is that this paradigm can be extended, albeit with enormous effort, to yield a Cognitive Science that will establish a continually growing body of established scientific truth.
Reference: <author> Brugman, Claudia, </author> <year> 1983. </year> <title> The use of body-part terms as locatives in chalcatongo mixtec. in Report No. </title> <booktitle> 4 of the Survey of California and other Indian Languages, </booktitle> <pages> pp. </pages> <month> 235-90. </month> <title> University of California, Berkeley. 2 We might add that the methodological development of the L 0 project is somewhat symptomatic of connectionist research as a whole during recent years. </title> <note> 21 Dempster, </note> <author> A. P., N. M. Laird, & D. B. Rubin. </author> <year> 1977. </year> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, Series B 34.1-38. </journal>
Reference-contexts: Such differences in spatial systems are sometimes quite dramatic, but more often than not they are rather subtle, particularly when one compares closely related languages. Figure 1 provides examples of non-English spatial structuring, from Mixtec <ref> (Brugman 1983) </ref>, a Mexican Indian language, and from German. In Figure 1 (a) we see two spatial configurations which would both be categorized as above in English, but which receive distinct categorizations in Mixtec, which is sensitive to the major axis orientation of objects.
Reference: <author> Feldman, Jerome A. </author> <year> 1989. </year> <title> Neural representation of conceptual knowledge. In Neural connections, mental computation, </title> <editor> ed. by Lynn Nadel et al., </editor> <address> 68-103. Cambridge, Mass.: </address> <publisher> MIT Press. </publisher> ||, <editor> George Lakoff, Andreas Stolcke, & Susan Hollbach Weber. </editor> <year> 1990a. </year> <title> Miniature language acquisition: A touchstone for cognitive science. </title> <booktitle> In Proceedings of the 12th Annual Conference of the Cognitive Science Society, </booktitle> <pages> 686-693, </pages> <publisher> MIT, </publisher> <address> Cambridge, </address> <note> Mass. </note> ||, <author> Susan Hollbach Weber, & Andreas Stolcke. </author> <year> 1990b. </year> <booktitle> A testbed for the miniature language L 0 . In Proceedings of the 5th Rocky Mountain Conference on Artificial Intelligence, </booktitle> <pages> 25-30, </pages> <address> New Mexico State University, Las Cruzes, N.M. </address>
Reference-contexts: Our observations suggest the use of process semantics (Nielsen et al. 1981) to characterize the deep semantics of causal narratives. Of special interest to us is the possibility of modeling such processes in structured connectionist models <ref> (Feldman 1989) </ref>. This work attempts to develop computational models based on a synthesis from process theory, insights from structured connectionist systems and from linguistic research on Cognitive Semantics to model the commonsense understanding of causal narratives.
Reference: <author> Goddard, Nigel, </author> <year> 1992. </year> <title> The perception of articulated motion: Recognizing moving light displays. </title> <institution> University of Rochester dissertation. </institution>
Reference: <author> Goldberg, Adele, & Dan Slobin, </author> <year> 1992. </year> <title> A cross linguistic study of spatial terms. </title> <type> Technical report draft, </type> <institution> International Computer Science Institute. </institution>
Reference: <author> Gull, S. F. </author> <year> 1988. </year> <title> Bayesian inductive inference and maximum entropy. In Maximum entropy and Bayesian methods in science and engineering, volume 1: Foundations, </title> <editor> ed. by G. J. Erickson & C. R. Smith, </editor> <address> 53-74. Dordrecht: </address> <publisher> Kluwer. </publisher>
Reference-contexts: This is just a probabilistic reformulation of our intuition that succinct grammars are to be preferred over profuse ones, other things being equal. The second variant of Occam's Razor, know in the literature as `Occam factors' <ref> (Gull 1988) </ref>, concerns the comparison of models with continuous parameters (e.g., rule probabilities). Here, models with fewer parameters are inherently preferred by the Bayesian approach since the prior probability over the continuous parameter space has to be spread `thinner' if that space has more dimensions.
Reference: <author> Guyon, I., P. Albrecht, Y. LeCun, J. Denker, & W. Hubbard. </author> <year> 1991. </year> <title> Design of a neural network character recognizer for a touch terminal. </title> <journal> Pattern Recognition 24.105-119. </journal>
Reference: <author> Horning, James Jay. </author> <year> 1969. </year> <title> A study of grammatical inference. </title> <type> Technical Report CS 139, </type> <institution> Computer Science Department, Stanford University, Stanford, </institution> <address> Ca. </address>
Reference: <author> Jurafsky, Daniel, Chuck Wooters, Gary Tajchman, Jonathan Segal, Andreas Stolcke, & Nelson Morgan. </author> <year> 1993. </year> <title> The Berkeley Restaurant Project. </title> <type> Technical report, </type> <institution> International Computer Science Institute, Berkeley, CA. </institution> <note> To appear. </note>
Reference-contexts: Going beyond the L 0 project, we have recently started to use the same induction algorithm to infer context-free grammars from real natural language samples for use in a speech understanding system <ref> (Jurafsky et al. 1993) </ref>. Preliminary results there show that the method seems to be working well, allowing us to automatically build grammars that are tightly tailored to specific patterns of language use (which is desirable for speech recognition purposes).
Reference: <author> Keeler, James, David Rumelhart, & Wee-Kheng Leow. </author> <year> 1991. </year> <title> Integrated segmentation and recognition of hand-printed numerals. </title> <type> Technical Report ACT-NN-010-91, </type> <institution> Microelectronics and Computer Technology Corporation. </institution>
Reference: <author> Lakoff, George. </author> <year> 1987. </year> <title> Women, fire, and dangerous things: What categories reveal about the mind. </title> <publisher> University of Chicago Press. </publisher> ||. <year> 1992. </year> <title> What is metaphor? In Advances in connectionist and neural computational theory, Vol. 2: Analogical connections, </title> <editor> ed. by K. J. Holyoak & J. A. Barnden. </editor> <publisher> Ablex. </publisher> <address> LeCun, Yann. </address> <year> 1989. </year> <title> Generalization and network design strategies. </title> <type> Technical Report CRG-TR-89-4, </type> <institution> Connectionist Research Group, University of Toronto. </institution>
Reference-contexts: We postulate therefore, that an adequate representation of the concrete domain is an essential component of modeling the semantics of these narratives. Recent work in Cognitive Semantics provides good evidence for this view. In particular, there are proposals for schema-based semantics, as in the case of Image Schemas <ref> (Lakoff 1987) </ref>. The work in Cognitive Semantics, however, lacks a computational model for such theories. Second, we note that the deep semantics of the causal narratives are dynamic and arise from a continuous interaction between input and memory.
Reference: <author> Mozer, Michael, Richard Zemel, & Marlene Behrmann. </author> <year> 1991. </year> <title> Learning to segment images using dynamic feature binding. </title> <type> Technical Report CU-CS-540-91, </type> <institution> Dept. of Computer Science, University of Colorado at Boulder. </institution> <note> 22 Nenov, </note> <author> Valeriy I. </author> <year> 1991. </year> <title> Perceptually grounded language acquisition: A neu-ral/procedural hybrid model. </title> <type> Technical Report CSD-910083, </type> <institution> Computer Science Department, University of California, </institution> <address> Los Angeles. </address>
Reference: <author> Nielsen, M., G. Plotkin, & G. Winskel. </author> <year> 1981. </year> <title> Petri nets, event structures, and domains, Part I. </title> <booktitle> Theoretical Computer Science 13. </booktitle>
Reference-contexts: Any representation of the semantics should therefore be of a fine granularity, be flexible and adaptive in the way context affects interpretation, and be able to support concurrent activities in the memory storage, retrieval, and indexing process. Our observations suggest the use of process semantics <ref> (Nielsen et al. 1981) </ref> to characterize the deep semantics of causal narratives. Of special interest to us is the possibility of modeling such processes in structured connectionist models (Feldman 1989). <p> The requirements outlined above suggest a net based model of process semantics. The requirements of learnability and connectionist implementation require us to define special modifications to standard concurrency models (such as Petri Nets and Event Logics <ref> (Nielsen et al. 1981) </ref>). * Mapping the abstract social domain structures into the relevant concrete spatial and experiential domain structures. This includes the selection, initialization and execution of specific concrete domain schemas. To this end, we have divided the domain schemas into the following categories: TheWorld Model.
Reference: <author> Omohundro, Stephen M. </author> <year> 1992. </year> <title> Best-first model merging for dynamic learning and recognition. </title> <booktitle> In Advances in Neural Information Processing Systems 4, </booktitle> <publisher> ed. by John E. </publisher>
Reference: <editor> Moody, Steve J. Hanson, & Richard P. Lippman, </editor> <address> 958-965. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Regier, Terry, </author> <year> 1992. </year> <title> The acquisition of lexical semantics for spatial terms: A connectionist model of perceptual categorization. </title> <institution> Computer Science Division, EECS Department, University of California at Berkeley dissertation. </institution> <note> available as Technical Report TR-92-062, </note> <institution> International Computer Science Institute, Berkeley. </institution> ||. <year> 1993. </year> <title> Two predicted universals in the semantics of space. </title> <booktitle> In Proceedings of the Nineteenth Annual Meeting of the Berkeley Linguistics Society. </booktitle> <institution> University of California, Berkeley. ||, & Jerome Feldman, </institution> <year> 1993. </year> <title> Structured connectionist models and spatial concept learning. </title> <editor> In J. Hendler, editor, </editor> <booktitle> Handbook of Neuropsychology, Volume 9 (to appear). </booktitle> <publisher> Elsevier. </publisher>
Reference: <author> Rissanen, Jorma. </author> <year> 1983. </year> <title> A universal prior for integers and estimation by minimum description length. </title> <journal> The Annals of Statistics 11.416-431. </journal>
Reference: <author> Rumelhart, David E., Geoffrey E. Hinton, & Ronald J. Williams. </author> <year> 1986. </year> <title> Learning internal representations by error propagation. In Parallel distributed processing: Explorations in the microstructure of cognition, </title> <editor> ed. by James L. </editor> <publisher> McClelland & David E. </publisher>
Reference-contexts: Once a movie has been shown to the trained network, the nodes corresponding to those lexemes which describe the event shown should be fully activated. The model is trained using the connectionist error back-propagation training algorithm <ref> (Rumelhart et al. 1986) </ref>, while its architecture is informed by the design philosophy of adaptive structured connectionism (Regier & Feldman 1993).
Reference: <editor> Rumelhart, </editor> <address> 318-362. </address> <publisher> MIT Press. </publisher>
Reference: <author> Shieber, Stuart M. </author> <year> 1986. </year> <title> An introduction to unification-based approaches to grammar. Number 4 in CSLI Lecture Note Series. Stanford, Ca.: Center for the Study of Language and Information. </title>
Reference-contexts: The rules that propagate the feature values through the syntactic structure are extension of context-free rules, and similar to those found in many feature-based grammar theories <ref> (Shieber 1986) </ref>.
Reference: <author> Siskind, Jeffrey Mark, </author> <year> 1992. </year> <title> Naive physics, event perception, lexical semantics, and language acquisition. </title> <address> Cambridge, Mass.: </address> <institution> Massachussetts Institute of Technology dissertation. </institution>
Reference: <author> Stolcke, Andreas. </author> <year> 1990. </year> <title> Learning feature-based semantics with simple recurrent networks. </title> <type> Technical Report TR-90-015, </type> <institution> International Computer Science Institute, Berke-ley, CA. ||, & Stephen Omohundro. </institution> <year> 1993. </year> <title> Hidden Markov model induction by Bayesian model merging. </title> <booktitle> In Advances in Neural Information Processing Systems 5, </booktitle> <editor> ed. by Stephen Jose Hanson, Jack D. Cowan, & C. Lee Giles, </editor> <address> 11-18. San Mateo, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The initial L 0 testbed (Feldman et al. 1990b) relied on traditional 20 symbolic AI methods for purposes of rapid prototyping. The grammar learning project started out connectionist <ref> (Stolcke 1990) </ref>, but eventually adopted a probabilistic framework which was thought to provide more versatile representations while still capturing the `soft computation' aspects of the problem. The spatial semantics learning project has remained the most purely connectionists.
Reference: <author> Suppes, Patrick, Lin Liang, & Michael B ottner. </author> <year> 1991. </year> <title> Complexity issues in robotic machine learning of natural language. In Modeling complex phenomena, </title> <editor> ed. by L. Lam & V. Naroditsky. </editor> <address> New York, N.Y.: </address> <publisher> Springer Verlag. </publisher> <address> 23 Talmy, Len. </address> <year> 1985. </year> <title> Lexicalization patterns, semantic structures in lexical form. In Language typology and syntactic description, </title> <editor> ed. by Timothy Shopen. </editor> <publisher> Cambridge: Cambridge University Press. </publisher>
Reference: <author> Tomasello, Michael. </author> <year> 1991. </year> <title> First verbs. </title> <publisher> Cambridge University Press. </publisher>
Reference: <author> Venkataraman, S.T., & T. Iberall. </author> <year> 1990. </year> <title> Dextrous robot hands. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: In switching to the hand-action domain, we must likewise avoid simulating detailed robotics, while still capturing enough of human hand activity so that psycho-linguistic data is pertinent. Taking our cue from simplifications made in the robotics world <ref> (Venkataraman & Iberall 1990) </ref>, we envision simulating a five-fingered hand with one degree of freedom on the various finger joints. We will abstract away the arm, using a primitive to move the hand to any position and orientation.
Reference: <author> Wallace, C. S., & P. R. Freeman. </author> <year> 1987. </year> <title> Estimation and inference by compact coding. </title> <journal> Journal of the Royal Statistical Society, Series B 49.240-265. </journal>
Reference: <author> Weber, Susan Hollbach, & Andreas Stolcke. </author> <year> 1990. </year> <title> L 0 : A testbed for miniature language acquisition. </title> <type> Technical Report TR-90-010, </type> <institution> International Computer Science Institute, Berkeley, </institution> <address> CA. </address>
Reference: <author> Whitehead, Steven, </author> <year> 1992. </year> <title> Reinforcement learning for the adaptive control of perception and action. </title> <institution> University of Rochester dissertation. </institution>
Reference: <author> Whorf, Benjamin Lee. </author> <year> 1956. </year> <title> Language, </title> <booktitle> thought, and reality. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <editor> (ed. John B. Carroll). </editor> <volume> 24 </volume>
References-found: 29

