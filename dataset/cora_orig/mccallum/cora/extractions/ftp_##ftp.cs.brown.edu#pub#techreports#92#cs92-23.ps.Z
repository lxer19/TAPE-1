URL: ftp://ftp.cs.brown.edu/pub/techreports/92/cs92-23.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-92-23.html
Root-URL: http://www.cs.brown.edu/
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> J.A. Abraham, P. Banerjee, C.-Y. Chen, W. K. Fuchs, S.-Y. Kuo, A.L. Narasimha Reddy, </author> <title> "Fault tolerance techniques for systolic arrays", </title> <journal> IEEE Computer, Vol.20, </journal> <volume> No.7, </volume> <pages> pp. 65-76, </pages> <year> 1987. </year>
Reference-contexts: The fault-tolerant issues of the interconnection networks used to integrate processors and memory modules are discussed in [2]. Fault tolerance of systolic arrays | a particular class of parallel machines | has been studied to some extent, and some of the achievements in that area are surveyed in <ref> [1] </ref>. All these areas are extremely important for dependable massively parallel computing. In this work we address the following issues: the reliability and synchronization of parallel processors that can be modeled by PRAMs, and the efficiency of computation on such processors. <p> The arrays are manufactured with spare modules, such that a number of failures can be tolerated by detecting faulty modules and either bypassing them or automatically replacing them with the spare modules. The survey of Abraham et al. <ref> [1] </ref> overviews the algorithm-based fault tolerance in systolic arrays and reconfigurable VLSI-based systolic arrays. Relevant theoretical bounds are given by Kaklamanis et al. in [54]. <p> Here we study arbitrary failures and arbitrary failures and restarts. Recovery: In some models it is reasonable to assume that faulty processors never recover. For example, manufacturing defects may permanently disable some of the systolic array processors, while the array remains functional when equipped with onboard fault-tolerance <ref> [1, 28, 50] </ref>. It is also reasonable for processors to recover at some point and rejoin a computation in progress. Failures may be quantified by the duration of a processor's absence from a computation. We consider both the no-restart and restartable models. <p> 3 failed right after writing 1 into its leaf c [6], and processor 4 failed after calculating c [3] = 2 as the sum of its (c [4] = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c <ref> [1] </ref>: 3 c [4,5,6,7]: 0 1 1 1 the phase. Observe that the root value c [1] = 3 yet the actual number of active processors is 1. 2 It is easy to show that phase W1 will always compute in c [1] an overestimate of the number of processors, which <p> calculating c [3] = 2 as the sum of its (c [4] = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c <ref> [1] </ref>: 3 c [4,5,6,7]: 0 1 1 1 the phase. Observe that the root value c [1] = 3 yet the actual number of active processors is 1. 2 It is easy to show that phase W1 will always compute in c [1] an overestimate of the number of processors, which are surviving at the time of its completion (see Lemma 3.1). <p> look like this after the completion of c <ref> [1] </ref>: 3 c [4,5,6,7]: 0 1 1 1 the phase. Observe that the root value c [1] = 3 yet the actual number of active processors is 1. 2 It is easy to show that phase W1 will always compute in c [1] an overestimate of the number of processors, which are surviving at the time of its completion (see Lemma 3.1). We also need to enumerate the surviving processors. <p> Formally, the nonnegative integer values in a are constrained top down as follows: The root value is a <ref> [1] </ref> = d [1]. For the children of an interior node i (1 i N 1) we have a [2i] d [2i] , a [2i + 1] d [2i + 1] , and a [2i] + a [2i + 1] = a [i] These constraints do not uniquely define a. <p> Formally, the nonnegative integer values in a are constrained top down as follows: The root value is a <ref> [1] </ref> = d [1]. For the children of an interior node i (1 i N 1) we have a [2i] d [2i] , a [2i + 1] d [2i + 1] , and a [2i] + a [2i + 1] = a [i] These constraints do not uniquely define a. <p> Thus, our dynamic top-down traversal (given in detail in Appendix A.4) implements one way of uniquely defining the values of a satisfying these constraints. The constraints on the values of a assure that (i) there are exactly d <ref> [1] </ref> = a [1] number of leaves whose d and a values are 1 | such leaves are called accounted, and no processor will reach these leaves, and (ii) the processors reach leaves with the a values of 0 | such leaves are called unaccounted. <p> Thus, our dynamic top-down traversal (given in detail in Appendix A.4) implements one way of uniquely defining the values of a satisfying these constraints. The constraints on the values of a assure that (i) there are exactly d <ref> [1] </ref> = a [1] number of leaves whose d and a values are 1 | such leaves are called accounted, and no processor will reach these leaves, and (ii) the processors reach leaves with the a values of 0 | such leaves are called unaccounted. <p> This dynamic bottom up traversal is given in Appendix A.3. Phase W4 is a simple variant of phase W1, except for the fact that the path traversed bottom up is dynamically determined. One can easily show that the progress recorded in d <ref> [1] </ref> by phase W4 increases monotonically and it underestimates the actual progress (see Lemma 3.3). This guarantees that the algorithm terminates after at most N iterations, since d [1] 6= N is the guard that controls the main loop. <p> One can easily show that the progress recorded in d <ref> [1] </ref> by phase W4 increases monotonically and it underestimates the actual progress (see Lemma 3.3). This guarantees that the algorithm terminates after at most N iterations, since d [1] 6= N is the guard that controls the main loop. The following example illustrates phase W4, and provides intuition for why the heap a is used in phase W2 and why it is needed by the proof framework presented in the next section. <p> traversal of the progress heap d, processor 4 failed prior to the start of the phase, and processor 3 failed after the first step of the traversal having written 1 into the leaf d [6], then the d heap will look like this after the completion of the phase. d <ref> [1] </ref>: 2 d [4,5,6,7]: 1 1 1 0 Let P 0 = 2 be the number of surviving processors. We see that d [1] = 2 is an underestimate of the actual number, i.e. 3, of visited leaves. If the d heap is used directly in 44 CHAPTER 3. <p> step of the traversal having written 1 into the leaf d [6], then the d heap will look like this after the completion of the phase. d <ref> [1] </ref>: 2 d [4,5,6,7]: 1 1 1 0 Let P 0 = 2 be the number of surviving processors. We see that d [1] = 2 is an underestimate of the actual number, i.e. 3, of visited leaves. If the d heap is used directly in 44 CHAPTER 3. <p> WRITE-ALL ALGORITHMS phase W2 to allocate processors to the unvisited leaves, then the leaf associated with d [7] will be allocated all P 0 surviving processors. On the other hand, by knowing the (overestimate) number of surviving processors P 0 and the (underestimate) of the visited leaves d <ref> [1] </ref>, we would like to prove that the allocation is balanced, and that no leaf is allocated more than dP 0 =(N d [1])e = 1 processors. <p> We first introduce some terminology. Let us consider the i-th iteration of the loop (1 i N ). Note that the first iteration consists only of phases W3 and W4. Define: (1) U i to be the estimated remaining work, the value of N d <ref> [1] </ref> right before the iteration starts, i.e. right after phase W4 of the previous iteration (U 1 is N ); (2) P i to be the real number of surviving processors, right before the iteration starts, i.e. right after phase W4 of the previous iteration (P 1 is P ); (3) <p> is N ); (2) P i to be the real number of surviving processors, right before the iteration starts, i.e. right after phase W4 of the previous iteration (P 1 is P ); (3) R i to be the estimated number of surviving processors, that is the value of c <ref> [1] </ref> right after phase W1 of the i-th iteration (R 1 is P ). The following is shown by straightforward induction on tree c. <p> If no processors reached the node t, then c [t] = 0 s (t). The induction stops at t = 1 where R i = c <ref> [1] </ref> s (1) = P i , and so P i R i . (2) Inequality R i P i+1 : This can be shown using similar induction, but instead of s (t) we define r (t), to be the number of processors that initiated phase W1 in the subtree of <p> of s (t) we define r (t), to be the number of processors that initiated phase W1 in the subtree of the node t and that completed the phase W1 traversal. r (1) is the upper bound for P i+1 , and the induction will show that r (1) c <ref> [1] </ref>. 2 In the dividing done during the dynamic top down traversal in W , we will allocate processors to tasks that either have not been completed, or have been completed, but not yet accounted for at the root d [1]. <p> , and the induction will show that r (1) c <ref> [1] </ref>. 2 In the dividing done during the dynamic top down traversal in W , we will allocate processors to tasks that either have not been completed, or have been completed, but not yet accounted for at the root d [1]. Recall that a leaf of d is accounted if it has value 1 and if the corresponding defined value in the leaf of heap a is also 1 (there are exactly d [1] accounted leaves). <p> have not been completed, or have been completed, but not yet accounted for at the root d <ref> [1] </ref>. Recall that a leaf of d is accounted if it has value 1 and if the corresponding defined value in the leaf of heap a is also 1 (there are exactly d [1] accounted leaves). In Algorithm W , the processors get allocated in a balanced fashion to the unaccounted leaves, i.e. the leaves whose associated (defined and) computed value 46 CHAPTER 3. WRITE-ALL ALGORITHMS in heap a is 0. <p> This can be shown using straightforward induction on the structure of the tree d and the loop-iteration number i. From this, since U i = N d i <ref> [1] </ref> = N a i [1] and U i+1 = N d i+1 [1], we have N U i &lt; N U i+1 which leads to the desired result. 2 3.2. GLOBAL ALLOCATION PARADIGM 47 We now come to the main lemma. <p> This can be shown using straightforward induction on the structure of the tree d and the loop-iteration number i. From this, since U i = N d i <ref> [1] </ref> = N a i [1] and U i+1 = N d i+1 [1], we have N U i &lt; N U i+1 which leads to the desired result. 2 3.2. GLOBAL ALLOCATION PARADIGM 47 We now come to the main lemma. <p> This can be shown using straightforward induction on the structure of the tree d and the loop-iteration number i. From this, since U i = N d i <ref> [1] </ref> = N a i [1] and U i+1 = N d i+1 [1], we have N U i &lt; N U i+1 which leads to the desired result. 2 3.2. GLOBAL ALLOCATION PARADIGM 47 We now come to the main lemma. <p> In both cases, by Lemma 3.2, we have that the (accounted) progress for iteration i is at least the number of surviving processors P i+1 divided by dR i =U i e. This is because each one of these processors returns to the root d <ref> [1] </ref>, reporting some progress, and at most dR i =U i e processors report information about the same leaf. <p> Instead of making a move according to the PID bits (line 09 of Figure 3.3), a processor at an interior progress tree node casts an N -sided die to produce a 70 CHAPTER 3. WRITE-ALL ALGORITHMS random number r in the range <ref> [1; N ] </ref>, reads the values d 1 and d 2 of the left and right children of the progress tree and then: * if d 1 = d 2 = 0, then the processor moves left and right with probabilities 1 2 ; * if d 1 + d 2
Reference: [2] <author> G. B. Adams III, D. P. Agrawal, H. J. Seigel, </author> <title> "A Survey and Comparison of Fault-tolerant Multistage Interconnection Networks", </title> <journal> IEEE Computer, </journal> <volume> 20, 6, </volume> <pages> pp. 14-29, </pages> <year> 1987. </year>
Reference-contexts: The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. The fault-tolerant issues of the interconnection networks used to integrate processors and memory modules are discussed in <ref> [2] </ref>. Fault tolerance of systolic arrays | a particular class of parallel machines | has been studied to some extent, and some of the achievements in that area are surveyed in [1]. All these areas are extremely important for dependable massively parallel computing. <p> Theoretical foundations for such networks are summarized by Pippenger in [83]. The networks are made more reliable by employing redundancy. A survey of fault tolerant interconnection networks is presented by Adams et al. in <ref> [2] </ref>. An interesting interconnection network routing strategy was described by Preparata [85], in which fast routing is achieved by allowing for some messages to be lost and using a redundancy scheme [84, 86] to reconstruct lost information. <p> The area of fault tolerance and efficiency of interconnection networks is extremely important as an enabling technology for fault-tolerant parallel computation. In this thesis, we limit our work to the design of algorithmic techniques that assume that a robust interconnection medium such as those surveyed in <ref> [2] </ref> is available. 12 CHAPTER 1. INTRODUCTION Fault tolerance in special purpose parallel computers Systolic arrays are special purpose parallel computers that lend themselves to being engineered with fault-tolerant features. <p> The fault tolerance of interconnection networks has been the subject of much work in its own turn. The networks are made more reliable by employing redundancy <ref> [2] </ref>. A combining interconnection network that is perfectly suited for implementing synchronous concurrent reads and writes is formally treated in [62] (the combining properties are used in their simplest form only to implement concurrent access to memory). Finally, fail-stop processors are formally presented and justified in [90].
Reference: [3] <author> Y. Afek, B. Awerbuch, E. Gafni, </author> <title> "Applying static network protocols to dynamic networks", </title> <booktitle> in Proc. of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 358-370, </pages> <year> 1987. </year>
Reference-contexts: For example, fault tolerance is the subject of significant current research in the setting of dynamic asynchronous network protocols. Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource monitored [4, 70]. Other developments <ref> [3, 13, 15] </ref> solve the problems of executing distributed algorithms in the presence of dynamic network changes, i.e., dynamic changes of the computation medium. <p> processor level; for recent work on gate granularities see [11, 82, 88]. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by <ref> [3, 4, 13, 15] </ref>). The general problems encountered in fault-tolerant parallel computation and in particular the problems of allocating active processors to tasks have similarities to the problems of resource management in a distributed setting. <p> INTRODUCTION requirement to complete the computation in the presence of faults. Note that unreliable PRAM processor steps must control all available PRAM processor steps. This introduces difficulties that recall the presence of network changes in <ref> [3, 13, 15] </ref>, i.e., dynamic changes of the computation medium. Fault tolerance of particular network architectures is also studied in [38]. However, the distributed computation models, the algorithms, and their analysis are quite different from the parallel setting studied here. <p> If processor 1 failed prior to the start of phase W1, processor 3 failed right after writing 1 into its leaf c [6], and processor 4 failed after calculating c <ref> [3] </ref> = 2 as the sum of its (c [4] = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c [1]: 3 c [4,5,6,7]: 0 1 1 1 the phase. <p> If processor 1 failed prior to the start of phase W1, processor 3 failed right after writing 1 into its leaf c [6], and processor 4 failed after calculating c <ref> [3] </ref> = 2 as the sum of its (c [4] = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c [1]: 3 c [4,5,6,7]: 0 1 1 1 the phase.
Reference: [4] <author> Y. Afek, B. Awerbuch, S. Plotkin, M. Saks, </author> <title> "Local Management of a Global Resource in a Communication Network", </title> <booktitle> Proc. of the 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 347-357, </pages> <year> 1987. </year>
Reference-contexts: For example, fault tolerance is the subject of significant current research in the setting of dynamic asynchronous network protocols. Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource monitored <ref> [4, 70] </ref>. Other developments [3, 13, 15] solve the problems of executing distributed algorithms in the presence of dynamic network changes, i.e., dynamic changes of the computation medium. <p> processor level; for recent work on gate granularities see [11, 82, 88]. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by <ref> [3, 4, 13, 15] </ref>). The general problems encountered in fault-tolerant parallel computation and in particular the problems of allocating active processors to tasks have similarities to the problems of resource management in a distributed setting. <p> Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource controlled. For instance, the algorithms of Lynch et al. [70] (with a probabilistic setting) and of Awerbuch et al. <ref> [4] </ref> (with a deterministic setting) are among the most sophisticated in that area. The problem we address in this thesis is, at an intuitive level, one of controlling resource allocation. <p> If processor 1 failed prior to the start of phase W1, processor 3 failed right after writing 1 into its leaf c [6], and processor 4 failed after calculating c [3] = 2 as the sum of its (c <ref> [4] </ref> = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c [1]: 3 c [4,5,6,7]: 0 1 1 1 the phase.
Reference: [5] <author> A. Aggarwal, A.K. Chandra, M. Snir, </author> <title> On communication latency in PRAM computations, </title> <booktitle> in Proc. of the 30 IEEE FOCS, </booktitle> <pages> pp. 11-22, </pages> <year> 1989. </year>
Reference: [6] <author> G. Almasi and A. Gottlieb, </author> <title> Highly Parallel Computing, </title> <address> Benjamin/Cummins, </address> <year> 1989. </year>
Reference-contexts: Interconnection networks are typically used in multiprocessor systems to provide communication among processors, memory modules and other devices [52]. An encyclopaedic survey of the interconnection networks is given by Al-masi and Gottlieb in <ref> [6, Chapter 8] </ref>. Theoretical foundations for such networks are summarized by Pippenger in [83]. The networks are made more reliable by employing redundancy. A survey of fault tolerant interconnection networks is presented by Adams et al. in [2]. <p> There are 4 processors with PIDs 1, 2, 3, and 4, and the counting tree is represented as the heap c [1::7]. If processor 1 failed prior to the start of phase W1, processor 3 failed right after writing 1 into its leaf c <ref> [6] </ref>, and processor 4 failed after calculating c [3] = 2 as the sum of its (c [4] = 1) and processor 3's (c [3] = 1) contributions, then the heap will look like this after the completion of c [1]: 3 c [4,5,6,7]: 0 1 1 1 the phase. <p> If, during a phase W4 bottom-up traversal of the progress heap d, processor 4 failed prior to the start of the phase, and processor 3 failed after the first step of the traversal having written 1 into the leaf d <ref> [6] </ref>, then the d heap will look like this after the completion of the phase. d [1]: 2 d [4,5,6,7]: 1 1 1 0 Let P 0 = 2 be the number of surviving processors. <p> We use the heap a in phase W2, where the surviving processors compute a <ref> [6] </ref> = a [7] = 0, with each reaching a distinct leaf thus assuring balanced processor allocation. Analysis of algorithm W We now outline the proof of robustness for algorithm W . Lemma 3.1 shows that in each loop iteration, the algorithm computes (over)estimates of the remaining processors.
Reference: [7] <author> R. Anderson, </author> <title> "Parallel Algorithms for Generating Random Permutations on a Shared Memory Machine", </title> <booktitle> Proc. of the 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 95-102, </pages> <year> 1990. </year>
Reference-contexts: We see that d [1] = 2 is an underestimate of the actual number, i.e. 3, of visited leaves. If the d heap is used directly in 44 CHAPTER 3. WRITE-ALL ALGORITHMS phase W2 to allocate processors to the unvisited leaves, then the leaf associated with d <ref> [7] </ref> will be allocated all P 0 surviving processors. <p> We use the heap a in phase W2, where the surviving processors compute a [6] = a <ref> [7] </ref> = 0, with each reaching a distinct leaf thus assuring balanced processor allocation. Analysis of algorithm W We now outline the proof of robustness for algorithm W . Lemma 3.1 shows that in each loop iteration, the algorithm computes (over)estimates of the remaining processors. <p> If a single subtree is not done, the processor moves down appropriately (line 08). For the final case (line 09), the processors move down when neither child is done. 3.3. LOCAL ALLOCATION PARADIGM 59 0 1 2 3 4 5 6 <ref> [7] </ref> fl fl fl fl fl C CO C CW fl C CW C A Q A AK s s This last case is where a non-trivial (italicized) decision is made.
Reference: [8] <author> R. Anderson and H. Woll, </author> <title> "Wait-Free Parallel Algorithms for the Union-Find Problem", </title> <booktitle> Proc. of the 23rd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 370-380, </pages> <year> 1991. </year> <note> 126 BIBLIOGRAPHY </note>
Reference-contexts: General synchronous PRAM simulations are impossible using bounded resources on asynchronous PRAMs. Buss et al. [27] show that some deterministic computations can be performed using subquadratic work, even when arbitrary asynchrony of PRAM processors is allowed. Anderson and Woll <ref> [8] </ref> also showed an efficient randomized solution for Write-All , as well as the existence of Write-All solutions with work O (N 1+" ) for P = N and any " &gt; 0 that can be used with the models we define here. <p> Hashed allocation algorithms can be used in both restartable and non-restartable failure models. Algorithm Y : this is an efficient determinization of a randomized algorithm that was defined by Anderson and Woll in <ref> [8] </ref>. We present this algorithm without analysis. Some experimental work suggests that the algorithm is a very efficient algorithm. <p> the algorithm X coin and X die is better than the worst case work of algorithm X subject to the worst case on-line adversary. 3.4 Hashed Allocation Paradigm The final technique is demonstrated using a new heuristic for determinizing an efficient randomized Write-All solution proposed by Anderson and Woll in <ref> [8] </ref>. 3.4.1 Algorithm Y A family of randomized algorithms for Write-All was presented in [8]. The basic technique in all of these algorithms is abstracted and given as a high level code in Figure 3.9. The basic algorithm in [8] is obtained by randomly choosing the permutation in line 03. <p> of algorithm X subject to the worst case on-line adversary. 3.4 Hashed Allocation Paradigm The final technique is demonstrated using a new heuristic for determinizing an efficient randomized Write-All solution proposed by Anderson and Woll in <ref> [8] </ref>. 3.4.1 Algorithm Y A family of randomized algorithms for Write-All was presented in [8]. The basic technique in all of these algorithms is abstracted and given as a high level code in Figure 3.9. The basic algorithm in [8] is obtained by randomly choosing the permutation in line 03. <p> an efficient randomized Write-All solution proposed by Anderson and Woll in <ref> [8] </ref>. 3.4.1 Algorithm Y A family of randomized algorithms for Write-All was presented in [8]. The basic technique in all of these algorithms is abstracted and given as a high level code in Figure 3.9. The basic algorithm in [8] is obtained by randomly choosing the permutation in line 03. In this case the expected work of the algorithm is O (N log N ), for P = p N (assume N is a square). We propose the following way of determinizing the algorithm of [8]: Given P = p <p> The basic algorithm in <ref> [8] </ref> is obtained by randomly choosing the permutation in line 03. In this case the expected work of the algorithm is O (N log N ), for P = p N (assume N is a square). We propose the following way of determinizing the algorithm of [8]: Given P = p we chose the smallest prime m such that P &lt; m. Primes are sufficiently dense, so that 3.4. <p> This is the same as the expected work using random permutations. We next briefly state the relevant framework, and then state the open problem. In <ref> [8] </ref>, the analysis of the randomized algorithm is based on the definition of a measure, called contention, that evaluates the "overlap" of a permutation representing a processor schedule with respect to a permutation representing a scheduling adversary: 72 CHAPTER 3. <p> on the p N elements of the group 13 for i = 1:: N do p 15 od 16 done [k] := true mark workgroup as finished 17 fi 18 w [P ID] := w [P ID] + 1; advance processor's groups done counter 19 od 20 parend Definition 3.3 <ref> [8] </ref> Given two permutations and ff represented as lists of integers f1; :::; pg, the contention of with respect to ff, denoted C (; ff) is defined as follows: scan ff left-to-right, and for each encountered item, delete that item from , C (; ff) is the number of times the <p> For example, C (h2; 4; 1; 3i; h1; 2; 3; 4i) = 2, and, C (h3; 2; 1i; h2; 1; 3i) = 1. Contentions for a set of permutations is defined as follows: Definition 3.4 <ref> [8] </ref> Given a set of permutation = f 1 ; :::; p g and a permutation ff, C (; ff) is defined as P p Intuitively, contention measures redundant work performed by an algorithm that uses hashed allocation paradigm. The key relevant results shown in [8] are: Lemma 3.19 [8] For <p> defined as follows: Definition 3.4 <ref> [8] </ref> Given a set of permutation = f 1 ; :::; p g and a permutation ff, C (; ff) is defined as P p Intuitively, contention measures redundant work performed by an algorithm that uses hashed allocation paradigm. The key relevant results shown in [8] are: Lemma 3.19 [8] For permutations and ff, C (; ff) is equal to the number of left-to-right maxima in ff 1 ffi , where ffi is the permutation composition. 3.4. HASHED ALLOCATION PARADIGM 73 Lemma 3.20 [8] If in an algorithm Y with p processors, each processor uses a <p> 3.4 <ref> [8] </ref> Given a set of permutation = f 1 ; :::; p g and a permutation ff, C (; ff) is defined as P p Intuitively, contention measures redundant work performed by an algorithm that uses hashed allocation paradigm. The key relevant results shown in [8] are: Lemma 3.19 [8] For permutations and ff, C (; ff) is equal to the number of left-to-right maxima in ff 1 ffi , where ffi is the permutation composition. 3.4. HASHED ALLOCATION PARADIGM 73 Lemma 3.20 [8] If in an algorithm Y with p processors, each processor uses a permutation from = f <p> The key relevant results shown in <ref> [8] </ref> are: Lemma 3.19 [8] For permutations and ff, C (; ff) is equal to the number of left-to-right maxima in ff 1 ffi , where ffi is the permutation composition. 3.4. HASHED ALLOCATION PARADIGM 73 Lemma 3.20 [8] If in an algorithm Y with p processors, each processor uses a permutation from = f 1 ; :::; p g as its schedules, then the worst case work of the algorithm is O (p max ff C (; ff)). <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. <p> is such that t is in t C when the number of processors completing each update cycle of the computation is at least cP for constant c &gt; 0. 2 In the fail-stop restartable model we are going to take advantage of the existential result by Anderson and Woll in <ref> [8] </ref>, who showed that for every " &gt; 0, there exists a deterministic algorithm for P processors that simulates P PRAM instructions with O (P 1+" ) work. This result was developed for the asynchronous model, but it also applies for fail-stop model with restarts. <p> In this analysis, we will be using an algorithm that was described and characterized with the following result by Anderson and Woll: Theorem 6.4 <ref> [8] </ref> There exists a Write-All (H; H; H) solution with H processors that has work O (H 1+" ) for every " &gt; 0. This is an existential result, and we call this algorithm AW . <p> Another open problem is to determine the overhead ratio for algorithm X in the original setting of failures and restarts. Recently, an existence proof for an algorithm achieving O (N 1+* ) work was given in <ref> [8] </ref>.
Reference: [9] <author> S.W. Apgar, </author> <title> "Interactive Animation of Fault-Tolerant Parallel Algorithms," </title> <type> forthcoming Master's thesis, </type> <year> 1992. </year>
Reference-contexts: This improvement also leads to the upper bound that matches the lower bound in [55] under the memory snapshot assumption as we show in this work. A parallel algorithm animation tool was developed by Apgar <ref> [9] </ref> to aid in the analysis of Write-All algorithms using Stasko's [95] TANGO animation system. Our modeling of fault tolerance where a processor is an entity subject to failures has some similarities with the design of "robust" sorting networks using fault-prone 1.3. <p> Experimental analysis: The design of new and the analysis of existing fault-tolerant parallel algorithms can be aided by using experimentation. Algorithm animation [25, 95] has the promise of providing additional insights into algortihms' behavior through visualization. A tool for animating Write-All algorithms was developed by Apgar <ref> [9] </ref> using Stasko's TANGO system [95]. Using the that animation, an observer can monitor the progress of a parallel computation and dynamically inject processor faults and restarts. Concluding remarks It is often claimed that distributed computing systems have the potential advantage of higher reliability over centralized systems.
Reference: [10] <author> J. Aspnes and M. Herlihy, </author> <title> "Wait-Free Data Structures in the Asynchronous PRAM Model", </title> <booktitle> Proc. of the 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 340-349, </pages> <year> 1990. </year>
Reference: [11] <author> S. Assaf and E. Upfal, </author> <title> "Fault Tolerant Sorting Network," </title> <booktitle> in Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 275-284, </pages> <year> 1990. </year>
Reference-contexts: Finally, our work here deals with dynamic patterns of faults; for recent advances on coping with static fault patterns, for example, are addressed by Kaklamanis in [54]. The granularity of faults in our work is at the processor level; for recent work on gate granularities see <ref> [11, 82, 88] </ref>. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by [3, 4, 13, 15]).
Reference: [12] <author> Y. Aumann and Michael Ben-Or, </author> <title> "Asymptotically Optimal PRAM Emulation on Faulty Hypercubes," </title> <booktitle> in Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 440-446, </pages> <year> 1991. </year>
Reference-contexts: Another example is the emulation of PRAMs on faulty hypercubes. See the the recent result of Aumann and Ben-Or on high probability emulation <ref> [12] </ref>. Interesting impossibility results for asynchronous shared memory models are given by Herlihy in [47, 48]. General synchronous PRAM simulations are impossible using bounded resources on asynchronous PRAMs.
Reference: [13] <author> B. Awerbuch, </author> <title> "On the effects of feedback in dynamic network protocols", </title> <booktitle> in Proc. of the 29th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 231-242, </pages> <year> 1988. </year>
Reference-contexts: For example, fault tolerance is the subject of significant current research in the setting of dynamic asynchronous network protocols. Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource monitored [4, 70]. Other developments <ref> [3, 13, 15] </ref> solve the problems of executing distributed algorithms in the presence of dynamic network changes, i.e., dynamic changes of the computation medium. <p> processor level; for recent work on gate granularities see [11, 82, 88]. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by <ref> [3, 4, 13, 15] </ref>). The general problems encountered in fault-tolerant parallel computation and in particular the problems of allocating active processors to tasks have similarities to the problems of resource management in a distributed setting. <p> INTRODUCTION requirement to complete the computation in the presence of faults. Note that unreliable PRAM processor steps must control all available PRAM processor steps. This introduces difficulties that recall the presence of network changes in <ref> [3, 13, 15] </ref>, i.e., dynamic changes of the computation medium. Fault tolerance of particular network architectures is also studied in [38]. However, the distributed computation models, the algorithms, and their analysis are quite different from the parallel setting studied here.
Reference: [14] <author> B. Awerbuch, B. Patt, G. Varghese, </author> <title> "Self-stabilization by Local Checking and Correction," </title> <booktitle> in Proc. of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 258-267, </pages> <year> 1991. </year>
Reference-contexts: In order to describe our technical contributions we must now review the state-of-the-art of the algorithmics of Write-All . For the most recent results in the area of distributed self-stabilizing systems see the works of Awerbuch et al. <ref> [14, 16] </ref>.
Reference: [15] <author> B. Awerbuch, M. Sipser, </author> <title> "Dynamic networks are as fast as static networks", </title> <booktitle> in Proc. of the 29th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 206-219, </pages> <year> 1988. </year>
Reference-contexts: For example, fault tolerance is the subject of significant current research in the setting of dynamic asynchronous network protocols. Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource monitored [4, 70]. Other developments <ref> [3, 13, 15] </ref> solve the problems of executing distributed algorithms in the presence of dynamic network changes, i.e., dynamic changes of the computation medium. <p> One of the important results of the work in the distributed model is that it is possible to take synchronous algorithms or algorithms that are designed for fixed network topologies and "compile" them so that they can be used with asynchronous networks or the networks whose topology changes dynamically <ref> [15] </ref>. Our research begins to yield similar results for certain shared memory parallel models. The potential reliability advantage of distributed computing systems is due to the replication of resources. The resulting redundancy in computation is a trade-off of efficiency (measured in terms of available resources) for fault tolerance. <p> processor level; for recent work on gate granularities see [11, 82, 88]. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by <ref> [3, 4, 13, 15] </ref>). The general problems encountered in fault-tolerant parallel computation and in particular the problems of allocating active processors to tasks have similarities to the problems of resource management in a distributed setting. <p> INTRODUCTION requirement to complete the computation in the presence of faults. Note that unreliable PRAM processor steps must control all available PRAM processor steps. This introduces difficulties that recall the presence of network changes in <ref> [3, 13, 15] </ref>, i.e., dynamic changes of the computation medium. Fault tolerance of particular network architectures is also studied in [38]. However, the distributed computation models, the algorithms, and their analysis are quite different from the parallel setting studied here.
Reference: [16] <author> B. Awerbuch, G. Varghese, </author> <title> "Distributed Program Checking: a Paradigm for Building Self-stabilizing Distributed Protocols," </title> <booktitle> in Proc. of the 33rd IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 258-267, </pages> <year> 1991. </year>
Reference-contexts: In order to describe our technical contributions we must now review the state-of-the-art of the algorithmics of Write-All . For the most recent results in the area of distributed self-stabilizing systems see the works of Awerbuch et al. <ref> [14, 16] </ref>.
Reference: [17] <author> J. Bartlett et al., </author> <title> "Fault Tolerance in Tandem Computer Systems," in The Theory and Practice of Reliable System Design, by D.P. Siewiorek and R.S. </title> <publisher> Swarz, Digital Press, </publisher> <year> 1991. </year>
Reference-contexts: When the number of processors is in the thousands, it is likewise impractical to provide fault tolerance and fault masking at the level that can be achieved by the more expensive, specialized machines with a small number of fast processors such as Tandem <ref> [17] </ref>, Stratus [101] or VAXft [26]. In addition, the software for these systems is typically more complex and thus less reliable than the software of the more conventional uniprocessors. Therefore, it is critical that fault-tolerant versions 1 2 CHAPTER 1.
Reference: [18] <author> K.E. Batcher, </author> <title> Sorting networks and their applications, </title> <booktitle> in Proc. of the AFIPS Spring Joint Comp. Conf., </booktitle> <volume> vol. 32, </volume> <pages> pp. 307-314, </pages> <year> 1968. </year>
Reference-contexts: Therefore, for the algorithms that are dominated by pointer doubling with a cost of fi (N log N ), e.g., Tarjan and Vishkin [96], there is no asymptotic degradation in the absence of failures. This optimization can be used with, for example, the sorting techniques such as Batcher <ref> [18] </ref> to reduce the overall multiplicative cost to O (log N ) in the absence of failures over the O (N log 2 N ) cost associated with sorting networks.
Reference: [19] <author> G. Baudet, </author> <title> Asynchronous iterative methods for multiprocessors, </title> <journal> JACM, </journal> <volume> vol. 25, no. 2, </volume> <pages> pp. 226-244, </pages> <year> 1978. </year> <note> BIBLIOGRAPHY 127 </note>
Reference: [20] <author> P. Beame and J. Hastad, </author> <title> "Optimal bounds for decision problems on the CRCW PRAM," </title> <journal> Journal of the ACM, </journal> <volume> vol. 36, no. 3, </volume> <pages> pp. 643-670, </pages> <year> 1989. </year>
Reference-contexts: Therefore the worst case behavior caused by such adversary will also apply to the cases where the algorithm uses a stronger PRAM model (such as the ideal PRAM of Beame and Hastad <ref> [20] </ref>) with arbitrarily powerful instruction set or the cases where the algorithm makes probabilistic decisions (such as using a coin toss). <p> on a domain D, and x 1 ; . . .; x n 2 D, compute, for each k, (1 k n) the sum L k In order to compute the prefix sums of N values using N processors, at least log N= log log N parallel steps are required <ref> [20, 67] </ref>, and the known algorithms require at least log N steps. Therefore an oblivious simulation of a known prefix algorithm will require simulating at least log N steps. When using P = N processors, the work of such simulation will be O (S w log N ). <p> In lines 13-17 the appropriate subtree sum is added (line 14) at depth h only if the corresponding bit value of the processor P ID is true. 2 Note that because of the lower bounds of Beame and Hastad <ref> [20] </ref> and Li and Yesha [67], at least log N= log log N parallel time and at least N log N= log log N work will be required by P = N processors to compute the prefix sums in the absence of failures.
Reference: [21] <author> G. Bilardi and F. P. Preparata, </author> <title> "Size-Time Complexity of Boolean Networks for Prefix Computation," </title> <journal> Journal of the ACM, </journal> <volume> vol. 36, no. 2, </volume> <pages> pp. 363-382, </pages> <year> 1989. </year>
Reference-contexts: The savings are also possible for the important prefix sums and pointer doubling algorithms. 5.5.1 Parallel prefix We now show how to obtain deterministic improvements in work for the prefix sums algorithm that occurs in solutions of several important problems <ref> [21] </ref>. Efficient parallel algorithms and circuits for computing prefix sums were given by Ladner and Fischer in 100 CHAPTER 5.
Reference: [22] <author> A. Birrell, </author> <title> "An Introduction to Programming with Threads", </title> <type> SRC, </type> <institution> Digital Equip. Corp., </institution> <type> Technical Report 35, </type> <month> January, </month> <year> 1989. </year>
Reference-contexts: Granularity also defines the smallest system components, such that a failure within the component is either completely masked by the component or causes the failure of the entire component. In practice, many parallel programs are implemented using threads packages <ref> [22, 36] </ref>. It is 24 CHAPTER 2. MODELS AND DEFINITIONS also reasonable to study failure granularity at the level of a single thread. <p> Evaluate the feasibility of implementing fault-tolerant algorithms based on differ ent multiprocessing paradigms. 4. In actual multiprocessor practice, threads packages provide a basis for the implementation of a wide variety of parallel paradigms, e.g., <ref> [22, 36] </ref>. The available threads packages typically support shared-memory lightweight processes. How are parallel programs, implemented using threads packages, affected by processor failures? How can the fault-tolerance that can be built into the threads packages themselves. What fault-tolerant programming methodologies, can be designed for the commonly used threads packages. 5.
Reference: [23] <author> G. Birkhoff, S. MacLane, </author> <title> A Survey of Modern Algebra, 4th ed., </title> <publisher> Macmillan, </publisher> <year> 1977. </year>
Reference-contexts: We then construct the multiplication table for the numbers 1; 2; . . .m 1 modulo m. It is not difficult to show that each row of this table is a permutation and that this structure is a group (using the basic group theory facts, e.g., <ref> [23] </ref>). Processor with PID i uses the ith permutation as its schedule.
Reference: [24] <author> B. Bloom, </author> <title> "Constructing two-writer atomic registers," </title> <journal> IEEE Trans. on Computers, </journal> <volume> vol. 37, no. 12, </volume> <pages> pp. 1506-1514, </pages> <year> 1988. </year>
Reference-contexts: This approach is similar to that of Bloom in <ref> [24] </ref>, but it is somewhat simpler due to the fact that we are dealing with the synchronous model. Fault-tolerant algorithms can be automatically transformed using the macro read and write cycles above to versions that only require single bit atomic writes.
Reference: [25] <author> M.H. Brown, </author> <title> "Exploring algorithms using Balsa-II", </title> <journal> IEEE Computer, Vol.21, </journal> <volume> No.5, </volume> <pages> pp. 14-36, </pages> <year> 1988. </year>
Reference-contexts: Experimental analysis: The design of new and the analysis of existing fault-tolerant parallel algorithms can be aided by using experimentation. Algorithm animation <ref> [25, 95] </ref> has the promise of providing additional insights into algortihms' behavior through visualization. A tool for animating Write-All algorithms was developed by Apgar [9] using Stasko's TANGO system [95].
Reference: [26] <author> W. Bruckert, C. Alonso, J. Melvin, </author> <title> "Verification of the First Fault-tolerant VAX System," </title> <journal> Digital Technical Journal, </journal> <volume> vol. 3, no. 1, </volume> <pages> pp. 79-85, </pages> <year> 1991. </year>
Reference-contexts: When the number of processors is in the thousands, it is likewise impractical to provide fault tolerance and fault masking at the level that can be achieved by the more expensive, specialized machines with a small number of fast processors such as Tandem [17], Stratus [101] or VAXft <ref> [26] </ref>. In addition, the software for these systems is typically more complex and thus less reliable than the software of the more conventional uniprocessors. Therefore, it is critical that fault-tolerant versions 1 2 CHAPTER 1. INTRODUCTION of existing or new algorithms be developed, which preserve efficiency under adverse conditions.
Reference: [27] <author> J. Buss, P.C. Kanellakis, P. Ragde, A.A. Shvartsman, </author> <title> "Parallel algorithms with processor failures and delays", </title> <institution> Brown Univ. Tech. Report CS-91-54, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: See the the recent result of Aumann and Ben-Or on high probability emulation [12]. Interesting impossibility results for asynchronous shared memory models are given by Herlihy in [47, 48]. General synchronous PRAM simulations are impossible using bounded resources on asynchronous PRAMs. Buss et al. <ref> [27] </ref> show that some deterministic computations can be performed using subquadratic work, even when arbitrary asynchrony of PRAM processors is allowed. <p> No global synchronization is necessary, and therefore local allocation algorithms can also be used with asynchronous systems (as shown by Buss et al. in <ref> [27] </ref>). We present and analyze a deterministic algorithm, and we present (without analysis) two randomized algorithms based on the deterministic algorithm: Algorithm X: this is a deterministic algorithm that can be used in both the fail-stop and restartable models. <p> The resulting time is then (N log 3 ). Remark 3.6 In algorithm X the processors work independently; they attempt to avoid duplicating already-completed work but do not co-ordinate their actions with other processors. In <ref> [27] </ref>, Buss et al. show that this property allows the algorithm to run on a strongly asynchronous PRAM with the same work and time bounds. <p> OTHER BOUNDS 83 4.3.2 Lower bounds with test-and-set operations Under certain assumptions on the way that memory is accessed in the strongly asynchronous model, a different lower bound is shown by Buss et al. in <ref> [27] </ref>. Assume that, instead of atomic reads and writes, memory is accessed by means of test-and-set operations. <p> That is, memory can only contain zeroes and ones, and a single test-and-set operation on a memory cell sets the value of that cell to 1 and returns the old value of the cell. Theorem 4.9 <ref> [27] </ref> Any strongly asynchronous PRAM algorithm for the Write-All problem which uses test-and-set as an atomic operation requires N + (P log (N=P )) total work, for P 3. <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration.
Reference: [28] <author> M. Chean and J.A.B. Fortes, </author> <title> "A Taxonomy of Reconfiguration Techniques for Fault-Tolerant Processor Arrays," </title> <journal> IEEE Computer, </journal> <volume> vol. 23, no. 1, </volume> <pages> pp. 55-69, </pages> <year> 1990. </year>
Reference-contexts: Here we study arbitrary failures and arbitrary failures and restarts. Recovery: In some models it is reasonable to assume that faulty processors never recover. For example, manufacturing defects may permanently disable some of the systolic array processors, while the array remains functional when equipped with onboard fault-tolerance <ref> [1, 28, 50] </ref>. It is also reasonable for processors to recover at some point and rejoin a computation in progress. Failures may be quantified by the duration of a processor's absence from a computation. We consider both the no-restart and restartable models.
Reference: [29] <author> R. Cole and O. Zajicek, </author> <title> "The APRAM: Incorporating Asynchrony into the PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 170-178, </pages> <year> 1989. </year>
Reference-contexts: For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of <ref> [29, 31, 45, 75, 78] </ref>. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. <p> Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models <ref> [29, 31, 45, 78, 75] </ref>. A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in [75]. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. <p> This is true of all known Write-All solutions [8, 27, 55, 56, 59, 61, 75], and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., <ref> [29] </ref>. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. This can be accomplished by utilizing three identical sets of the heaps. <p> For example, up to now, we assumed it is all 0's, but the algorithms would work even if the N locations were initialized using arbitrary 0's and 1's. A much more important assumption in all previous Write-All solutions (both in this thesis, and by other authors, e.g, <ref> [29, 59, 61, 75] </ref>) was regarding the initial state of additional auxiliary memory used (typically of (P ) size). The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. <p> problem of size N is contamination-tolerant, if it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., [55, 59, 61, 92], or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in <ref> [29] </ref> or the maximum finding algorithm in [75], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [30] <author> S.A. Cook, </author> <title> "An Overview of Computational Complexity," </title> <journal> in Comm. of the ACM, </journal> <volume> vol. 9, </volume> <pages> pp. 400-408, </pages> <year> 1983. </year>
Reference-contexts: For example, the well-known class N C of algorithms characterizes efficiency primarily in terms of (polylogarithmic) time efficiency, even if the computational agent is large (polynomial) relative to the size of a problem <ref> [30, 81] </ref>. To characterize better the efficiency of parallel algorithms, the efficiency measures need to take into account both the parallel time and the size of the computational resource, i.e., parallel work. <p> This is because the algorithms in N C allow for polynomial inefficiency in work. In N C the efficiency is characterized in terms of (polylogarithmic) time, but the computational agent can be large (polynomial) relative to the size of a problem <ref> [30, 81] </ref>. Further critique of the notion that N C class of algorithms is the class of efficient parallel algorithms is given by Kruskal et al. in [63].
Reference: [31] <author> R. Cole and O. Zajicek, </author> <title> "The Expected Advantage of Asynchrony," </title> <booktitle> in Proc. 2nd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 85-94, </pages> <year> 1990. </year>
Reference-contexts: For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of <ref> [29, 31, 45, 75, 78] </ref>. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. <p> Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models <ref> [29, 31, 45, 78, 75] </ref>. A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in [75]. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line.
Reference: [32] <author> T.H. Cormen, C.E. Leiserson, R.L. Rivest, </author> <title> Introduction to Algorithms, </title> <publisher> The MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: 1:: N do 05 if PID [i]th group is not finished 06 then perform sequential work on the p N elements of the group 07 and mark the group as finished 08 fi 10 parend there is at least one prime between P and 2P (e.g, see the discussion in <ref> [32, Sec. 33.8] </ref>), so that the complexity of the algorithms is not distorted when P is not a prime. We then construct the multiplication table for the numbers 1; 2; . . .m 1 modulo m.
Reference: [33] <author> F. Cristian, </author> <title> "Understanding Fault-Tolerant Distributed Systems", </title> <journal> in Communications of the ACM, </journal> <volume> vol. 3, no. 2, </volume> <pages> pp. 56-78, </pages> <year> 1991. </year> <note> 128 BIBLIOGRAPHY </note>
Reference-contexts: INTRODUCTION P ID 1 . . MEM 2 . . O B N N N T O K overviewed in the previous section (for surveys and examples, see <ref> [33, 51, 53] </ref>). We will now cite the particular technologies that are instrumental in providing basic hardware fault tolerance and employ these technologies in a foundation on which the algorithmic and software fault tolerance can be built. Semiconductor memories are the essential components of shared memory parallel systems.
Reference: [34] <author> E. W. Dijkstra, </author> <title> "Self-Stabilizing Systems in Spite of Distributed Control," </title> <journal> Comm. of the ACM, </journal> <volume> Vol. 17, No. 11, </volume> <pages> pp. 643-644, </pages> <year> 1976. </year>
Reference-contexts: Parallel computation in the setting where the shared memory is initially contaminated has some similarities with the notion of a self-stabilizing system introduced by Dijkstra in <ref> [34] </ref>. Paraphrasing [34], a system is self-stabilizing if and only if, regardless of the initial state the system can always make a state transition into another state, and the system is guaranteed to find itself in a legitimate state after a finite number of transitions. <p> Parallel computation in the setting where the shared memory is initially contaminated has some similarities with the notion of a self-stabilizing system introduced by Dijkstra in <ref> [34] </ref>. Paraphrasing [34], a system is self-stabilizing if and only if, regardless of the initial state the system can always make a state transition into another state, and the system is guaranteed to find itself in a legitimate state after a finite number of transitions. <p> Algorithms in this setting have some similarities with the notion of a self-stabilizing system introduced by Dijkstra in <ref> [34] </ref>. Paraphrasing [34], a system is self-stabilizing if and only if, regardless of the initial state the system can always make a state transition into another state, and the system is guaranteed to find itself in a legitimate state after a finite number of transitions. <p> Algorithms in this setting have some similarities with the notion of a self-stabilizing system introduced by Dijkstra in <ref> [34] </ref>. Paraphrasing [34], a system is self-stabilizing if and only if, regardless of the initial state the system can always make a state transition into another state, and the system is guaranteed to find itself in a legitimate state after a finite number of transitions.
Reference: [35] <author> E. W. Dijkstra, </author> <title> The Discipline of Programming, </title> <publisher> Prentice-Hall, </publisher> <year> 1976. </year>
Reference: [36] <author> T. Doeppner, </author> <title> "Threads: A system for the Support of Concurrent Programming," </title> <institution> Computer Science Technical Report CS-87-11, Brown University, </institution> <year> 1987. </year>
Reference-contexts: Granularity also defines the smallest system components, such that a failure within the component is either completely masked by the component or causes the failure of the entire component. In practice, many parallel programs are implemented using threads packages <ref> [22, 36] </ref>. It is 24 CHAPTER 2. MODELS AND DEFINITIONS also reasonable to study failure granularity at the level of a single thread. <p> Evaluate the feasibility of implementing fault-tolerant algorithms based on differ ent multiprocessing paradigms. 4. In actual multiprocessor practice, threads packages provide a basis for the implementation of a wide variety of parallel paradigms, e.g., <ref> [22, 36] </ref>. The available threads packages typically support shared-memory lightweight processes. How are parallel programs, implemented using threads packages, affected by processor failures? How can the fault-tolerance that can be built into the threads packages themselves. What fault-tolerant programming methodologies, can be designed for the commonly used threads packages. 5.
Reference: [37] <author> D. Dolev, C. Dwork, L. Stockmeyer, </author> <title> "On the minimal synchronism needed for distributed consensus", </title> <booktitle> in Proc. of the 24th IEEE FOCS, </booktitle> <pages> pp. 393-402, </pages> <year> 1983. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [41, 69, 79] for surveys of the topic, and <ref> [37, 42, 43] </ref> for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [79, 41, 69] for surveys, and <ref> [42, 43, 37] </ref> for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects. Such processors are modeled as fail-stop processors.
Reference: [38] <author> C. Dwork, D. Peleg, N. Pippenger, E. Upfal, </author> <title> "Fault Tolerance in Networks of Bounded Degree", </title> <booktitle> in Proc. of the 18th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 370-379, </pages> <year> 1986. </year>
Reference-contexts: RELATED WORK 9 switches, as those of Rudolph in [88], and in general with the design of reliable systems from unreliable components, as done by Pippenger in [82] using gates or by Dwork et al. in <ref> [38] </ref> for networks. The notion of robustness that we target in this research differs from that of the sorting network in [88], and in that network a linear number of operations is still critical. Another example is the emulation of PRAMs on faulty hypercubes. <p> Note that unreliable PRAM processor steps must control all available PRAM processor steps. This introduces difficulties that recall the presence of network changes in [3, 13, 15], i.e., dynamic changes of the computation medium. Fault tolerance of particular network architectures is also studied in <ref> [38] </ref>. However, the distributed computation models, the algorithms, and their analysis are quite different from the parallel setting studied here. It is interesting that the concept of a "communication complexity controller" first developed for distributed computing has an analog in parallel computing, i.e., "an algorithmic transformation that guarantees robustness". <p> Our modeling of fault tolerance has some similarities with the design of "robust" sorting networks, as those of Rudolph [88], and in general with the design of reliable systems from unreliable components, as in Pippenger [82] or Dwork et al. <ref> [38] </ref>. The distinguishing characteristic of our approach is the investigation of fault tolerance at the processor granularity as opposed to gate or switch granularities [82] and [88] respectively. Magnitude: Many hardware oriented fault tolerance techniques provide fault masking up to a pre-determined limit.
Reference: [39] <author> P. van Emde Boas, </author> <title> "Machine Models and Simulations," </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 1, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: The PRAM is a convenient abstraction that combines the power of parallelism with the simplicity of a RAM (Random Access Machine) <ref> [39] </ref>, but it has several unrealistic features. The PRAM has the following requirements: 1. Simultaneous access across a significant bandwidth to a shared resource, memory; 2. Global processor synchronization; and 3. Perfectly reliable processors, memory and interconnection between them. 1.1.
Reference: [40] <author> D. Eppstein and Z. Galil, </author> <title> "Parallel Techniques for Combinatorial Computation", </title> <booktitle> Annual Computer Science Review, 3 (1988), </booktitle> <pages> pp. 233-83. </pages>
Reference-contexts: The efficiency and scalability of parallel algorithms have been the subject of research since the seventies. A model of parallel computation known as the Parallel Random Access Machine or PRAM [44] has attracted much attention, and many "efficient" and "optimal" algorithms have been designed for it (the surveys <ref> [40, 58] </ref> contain a wealth of information on the subject). The PRAM is a convenient abstraction that combines the power of parallelism with the simplicity of a RAM (Random Access Machine) [39], but it has several unrealistic features. The PRAM has the following requirements: 1. <p> The PRAM model is used widely in the parallel algorithms research community as a convenient and elegant model, and a wealth of efficient algorithms exist and are continually being developed for this model. The surveys of Eppstein and Galil <ref> [40] </ref> and Karp and Ramachandran [58] cover all of the important variations of the PRAM model, and give most of the fundamental PRAM algorithms. Instead of reiterating the rationale for studying the PRAM model and listing the variations of the PRAM models, 17 18 CHAPTER 2. <p> Instead of reiterating the rationale for studying the PRAM model and listing the variations of the PRAM models, 17 18 CHAPTER 2. MODELS AND DEFINITIONS we refer the reader to the two excellent surveys <ref> [40, 58] </ref> that succinctly address the topic in the respective introductory sections. For the base model in this work, we use the following definition of the PRAM [44]: 1. There are P initial processors with unique identifiers (PID) in the range 1; . . . ; P . <p> In the restartable model, we can make fault-tolerant all of the above, except for the priority PRAM (for detailed 85 86 CHAPTER 5. ALGORITHM SIMULATIONS AND TRANSFORMATIONS surveys of PRAM variations see <ref> [40, 58] </ref>). For the no-restart model, we show that the simulation of PRAM algorithms can be done using optimal work in the presence of arbitrary fail-stop errors. <p> Thus the result holds for EREW, CREW, and weak CRCW models. To extend the simulation to stronger CRCW models such as priority and strong, we use an efficient algorithm transformation technique that preserves algorithms' efficiency to within a logarithmic in the number of processors factor as shown in <ref> [40] </ref> (attributed to folklore): "A parallel computation that can be performed in time t on a P -processor strong CRCW PRAM, can also be performed in time t log P using P EREW processors". <p> This is assured when using algorithm W (or algorithm V ) as the Write-All solution, since the algorithm has this property as we have shown in Section 3.2.3. Property (2) can be assured using auxiliary storage as in the transformation in Eppstein and Galil <ref> [40] </ref>: before writing, processors first write the PID of the simulated processor and then the data, but only if the previously written PID is lower.
Reference: [41] <author> M. J. Fischer, </author> <title> "The consensus problem in unreliable distributed systems (a brief survey)", </title> <institution> Yale Univ. </institution> <type> Tech. Rep., </type> <institution> DCS/RR-273, </institution> <year> 1983. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [41, 69, 79] </ref> for surveys of the topic, and [37, 42, 43] for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> Byzantine and other simpler failures were extensively studied in the context of distributed algorithms for the consensus problem (e.g., Pease et al. [79, 80], also see a survey by Fischer <ref> [41] </ref>). The failure models for parallel computation are constructed from failure definitions along the spectrum of failure models discussed below. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [79, 41, 69] </ref> for surveys, and [42, 43, 37] for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects.
Reference: [42] <author> M. J. Fischer and N. A. Lynch, </author> <title> "A lower bound for the time to assure interactive consistency", </title> <journal> IPL, </journal> <volume> vol. 14., no. 4, </volume> <pages> pp. 183-186, </pages> <year> 1982. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [41, 69, 79] for surveys of the topic, and <ref> [37, 42, 43] </ref> for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [79, 41, 69] for surveys, and <ref> [42, 43, 37] </ref> for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects. Such processors are modeled as fail-stop processors.
Reference: [43] <author> M. J. Fischer, N. A. Lynch, M. S. Paterson, </author> <title> "Impossibility of distributed consensus with one faulty process", </title> <journal> JACM, </journal> <volume> vol. 32, no. 2, </volume> <pages> pp. 374-382, </pages> <year> 1985. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [41, 69, 79] for surveys of the topic, and <ref> [37, 42, 43] </ref> for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see [79, 41, 69] for surveys, and <ref> [42, 43, 37] </ref> for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects. Such processors are modeled as fail-stop processors.
Reference: [44] <author> S. Fortune and J. Wyllie, </author> <title> "Parallelism in Random Access Machines", </title> <booktitle> Proc. the 10th ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 114-118, </pages> <year> 1978. </year>
Reference-contexts: Model and algorithm simplicity are keys to being able to integrate abstract solutions with realizable hardware and software systems. The efficiency and scalability of parallel algorithms have been the subject of research since the seventies. A model of parallel computation known as the Parallel Random Access Machine or PRAM <ref> [44] </ref> has attracted much attention, and many "efficient" and "optimal" algorithms have been designed for it (the surveys [40, 58] contain a wealth of information on the subject). <p> The model of parallel computation that serves as the basis for this work is the synchronous PRAM of Fortune and Wyllie <ref> [44] </ref>, with concurrent reads and concurrent writes (CRCW). The convention for determining which processor or processors succeed, when concurrently writing to shared memory, is immaterial in our algorithms. We investigate fault-prone PRAMs whose processors exhibit fail-stop processor behavior, such as that of Schlichting and Schneider [90]. <p> Specifically, we study algorithms for the systems that can be modeled by the Parallel Random Access Machine (PRAM) of Fortune and Wyllie <ref> [44] </ref>. The PRAM model is used widely in the parallel algorithms research community as a convenient and elegant model, and a wealth of efficient algorithms exist and are continually being developed for this model. <p> MODELS AND DEFINITIONS we refer the reader to the two excellent surveys [40, 58] that succinctly address the topic in the respective introductory sections. For the base model in this work, we use the following definition of the PRAM <ref> [44] </ref>: 1. There are P initial processors with unique identifiers (PID) in the range 1; . . . ; P . Each processor has access to its PID, and the number of processors P . 2. <p> Clear vs. contaminated initial memory: We require that a linear amount of shared memory location be initially clear, i.e., initialized to zero. While this is consistent with 28 CHAPTER 2. MODELS AND DEFINITIONS definitions of PRAM such as <ref> [44] </ref>, it is nevertheless a requirement that fault-tolerant systems ought to be able to do without. We address this issue in Section 6.1 where we develop an efficient procedure that solves the Write-All problem even when the shared memory is contaminated, i.e., contains arbitrary values. <p> The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. In theory, this is a natural, even if unstated assumption, for PRAMs <ref> [44] </ref> and RAMs (cf., Turing Machine auxiliary tapes are initially blank). However, given the definition of Write-All this dependence on clear space raises a legitimate "chicken-or-egg" objection.
Reference: [45] <author> P. Gibbons, </author> <title> "A More Practical PRAM Model," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 158-168, </pages> <year> 1989. </year>
Reference-contexts: For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of <ref> [29, 31, 45, 75, 78] </ref>. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. <p> Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models <ref> [29, 31, 45, 78, 75] </ref>. A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in [75]. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line.
Reference: [46] <author> R. Graham, D. Knuth, O. Patashnik, </author> <title> Concrete Mathematics: A Foundation for Computer Science, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year> <note> BIBLIOGRAPHY 129 </note>
Reference: [47] <author> M. P. Herlihy, </author> <title> "Impossibility and Universality Results for Wait-Free Synchronization", </title> <booktitle> in Proc. of the 7th ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1988. </year>
Reference-contexts: Another example is the emulation of PRAMs on faulty hypercubes. See the the recent result of Aumann and Ben-Or on high probability emulation [12]. Interesting impossibility results for asynchronous shared memory models are given by Herlihy in <ref> [47, 48] </ref>. General synchronous PRAM simulations are impossible using bounded resources on asynchronous PRAMs. Buss et al. [27] show that some deterministic computations can be performed using subquadratic work, even when arbitrary asynchrony of PRAM processors is allowed.
Reference: [48] <author> M. P. Herlihy, </author> <title> "Impossibility Results for Asynchronous PRAM", </title> <booktitle> in Proc. of the Third ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 327-336, </pages> <year> 1991. </year>
Reference-contexts: Another example is the emulation of PRAMs on faulty hypercubes. See the the recent result of Aumann and Ben-Or on high probability emulation [12]. Interesting impossibility results for asynchronous shared memory models are given by Herlihy in <ref> [47, 48] </ref>. General synchronous PRAM simulations are impossible using bounded resources on asynchronous PRAMs. Buss et al. [27] show that some deterministic computations can be performed using subquadratic work, even when arbitrary asynchrony of PRAM processors is allowed.
Reference: [49] <author> S. W. Hornick and F. P. Preparata, </author> <title> "Deterministic P-RAM: Simulation with Constant Redundancy," </title> <booktitle> in Proc. of the 1989 ACM Symposium on Parallel Algorithms and Architectures., </booktitle> <pages> pp. 103-109, </pages> <year> 1989. </year>
Reference-contexts: OVERVIEW AND MOTIVATION 3 The gap between the abstract models of parallel computation and realizable parallel computers is being bridged by current research. For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see <ref> [49, 87, 97] </ref>. Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76].
Reference: [50] <author> R. P. Hughey, </author> <title> Programmable Systolic Array, </title> <type> Ph.D. Dissertation, </type> <institution> Brown University, CS-TR-34, </institution> <month> May, </month> <year> 1991. </year>
Reference-contexts: The survey of Abraham et al. [1] overviews the algorithm-based fault tolerance in systolic arrays and reconfigurable VLSI-based systolic arrays. Relevant theoretical bounds are given by Kaklamanis et al. in [54]. In his thesis, Hughey <ref> [50] </ref> presents a programmable systolic array, and he also describes several techniques used for on-board fault detection along with software techniques that enable the bypassing of certain processing element failures. <p> Here we study arbitrary failures and arbitrary failures and restarts. Recovery: In some models it is reasonable to assume that faulty processors never recover. For example, manufacturing defects may permanently disable some of the systolic array processors, while the array remains functional when equipped with onboard fault-tolerance <ref> [1, 28, 50] </ref>. It is also reasonable for processors to recover at some point and rejoin a computation in progress. Failures may be quantified by the duration of a processor's absence from a computation. We consider both the no-restart and restartable models.
Reference: [51] <author> IEEE Computer, </author> <title> "Fault-Tolerant Computing", </title> <journal> special issue, </journal> <volume> vol. 17, no. 8, </volume> <year> 1984. </year>
Reference-contexts: INTRODUCTION P ID 1 . . MEM 2 . . O B N N N T O K overviewed in the previous section (for surveys and examples, see <ref> [33, 51, 53] </ref>). We will now cite the particular technologies that are instrumental in providing basic hardware fault tolerance and employ these technologies in a foundation on which the algorithmic and software fault tolerance can be built. Semiconductor memories are the essential components of shared memory parallel systems.
Reference: [52] <author> IEEE Computer, </author> <title> "Interconnection Networks", </title> <journal> special issue, </journal> <volume> vol. 20, no. 6, </volume> <year> 1987. </year>
Reference-contexts: Robust interconnection networks Another important subject that has been the target of work is the area of fault-tolerant interconnection networks. Interconnection networks are typically used in multiprocessor systems to provide communication among processors, memory modules and other devices <ref> [52] </ref>. An encyclopaedic survey of the interconnection networks is given by Al-masi and Gottlieb in [6, Chapter 8]. Theoretical foundations for such networks are summarized by Pippenger in [83]. The networks are made more reliable by employing redundancy.
Reference: [53] <author> IEEE Computer, </author> <title> "Fault-Tolerant Systems", </title> <journal> special issue, </journal> <volume> vol. 23, no. 7, </volume> <year> 1990. </year>
Reference-contexts: INTRODUCTION P ID 1 . . MEM 2 . . O B N N N T O K overviewed in the previous section (for surveys and examples, see <ref> [33, 51, 53] </ref>). We will now cite the particular technologies that are instrumental in providing basic hardware fault tolerance and employ these technologies in a foundation on which the algorithmic and software fault tolerance can be built. Semiconductor memories are the essential components of shared memory parallel systems.
Reference: [54] <author> C. Kaklamanis, A. Karlin, F. Leighton, V. Milenkovic, P. Raghavan, S. Rao, C. Thomborson, A. Tsantilas, </author> <title> "Asymptotically Tight Bounds for Computing with Arrays of Processors," </title> <booktitle> in Proc. of the 31st IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 285-296, </pages> <year> 1990. </year>
Reference-contexts: Finally, our work here deals with dynamic patterns of faults; for recent advances on coping with static fault patterns, for example, are addressed by Kaklamanis in <ref> [54] </ref>. <p> The survey of Abraham et al. [1] overviews the algorithm-based fault tolerance in systolic arrays and reconfigurable VLSI-based systolic arrays. Relevant theoretical bounds are given by Kaklamanis et al. in <ref> [54] </ref>. In his thesis, Hughey [50] presents a programmable systolic array, and he also describes several techniques used for on-board fault detection along with software techniques that enable the bypassing of certain processing element failures.
Reference: [55] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms Can Be Made Robust", </title> <journal> Distributed Computing), </journal> <volume> vol. 5, no. 4, </volume> <booktitle> 1992; preliminary version appears in Proc. of the 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 211-222, </pages> <year> 1989. </year>
Reference-contexts: The redundancy provided by concurrent reads and writes is essential to our model, e.g., when the writes are exclusive we show that efficiency and fault tolerance cannot be combined. This thesis includes and extends the study of fault tolerance that was first formalized by Kanellakis and Shvartsman in <ref> [55] </ref>. As it was shown there, it is possible to combine efficiency and fault tolerance in many key PRAM algorithms in the presence of arbitrary dynamic processor errors when processors fail by stopping and do not perform any further actions. <p> However, it is not obvious how to design solutions that are efficient in the presence of failures or asynchrony. The first algorithm for the Write-All problem with poly-logarithmic overhead in work was shown in <ref> [55] </ref>. Using solutions to the Write-All problem, we show that arbitrary PRAM algorithms can be efficiently and deterministically executed on fail-stop PRAMs, whose processors are subject either to arbitrary dynamic patterns of failures, or the dynamic patterns of failures and restarts. <p> It can also be used with restartable fail-stop processor simulations. 1.3 Related Work 1.3.1 Fault-tolerant parallel computation The study of PRAM fault tolerance was initiated by Kanellakis and Shvartsman in <ref> [55] </ref>, where a new complexity measure for fault tolerant PRAM algorithms was defined, where 8 CHAPTER 1. INTRODUCTION the notion of parallel robustness was introduced, and where the Write-All problem was defined. The techniques presented in [55] can readily be employed in making arbitrary PRAM algorithms fault-tolerant. <p> computation The study of PRAM fault tolerance was initiated by Kanellakis and Shvartsman in <ref> [55] </ref>, where a new complexity measure for fault tolerant PRAM algorithms was defined, where 8 CHAPTER 1. INTRODUCTION the notion of parallel robustness was introduced, and where the Write-All problem was defined. The techniques presented in [55] can readily be employed in making arbitrary PRAM algorithms fault-tolerant. The iterated Write-All paradigm was employed (independently) by Kedem et al. in [59] and by Shvartsman in [92] to extend the results of [55] to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). <p> The techniques presented in <ref> [55] </ref> can readily be employed in making arbitrary PRAM algorithms fault-tolerant. The iterated Write-All paradigm was employed (independently) by Kedem et al. in [59] and by Shvartsman in [92] to extend the results of [55] to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. The algorithms analyzed included algorithms from [55] and [75], and a new algorithm based on pointer doubling <p> and by Shvartsman in [92] to extend the results of <ref> [55] </ref> to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. The algorithms analyzed included algorithms from [55] and [75], and a new algorithm based on pointer doubling that has a good expected behavior for the failure model defined. The deterministic execution of PRAM algorithms in [92] is optimal for any adversary when parallel slackness (as in [99]) is exploited to our advantage. <p> Kedem et al. [61] further refined the results in [59] to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. The fail-stop deterministic lower and upper bounds of <ref> [55] </ref> were also improved in [61] by log log N factors. Recently, Kedem et al. [60] further investigated the use of randomization for resilient parallel computation. Martel [71] has improved the analysis of the main algorithm in [55] by a log log N factor. <p> The fail-stop deterministic lower and upper bounds of <ref> [55] </ref> were also improved in [61] by log log N factors. Recently, Kedem et al. [60] further investigated the use of randomization for resilient parallel computation. Martel [71] has improved the analysis of the main algorithm in [55] by a log log N factor. This improvement also leads to the upper bound that matches the lower bound in [55] under the memory snapshot assumption as we show in this work. <p> Recently, Kedem et al. [60] further investigated the use of randomization for resilient parallel computation. Martel [71] has improved the analysis of the main algorithm in <ref> [55] </ref> by a log log N factor. This improvement also leads to the upper bound that matches the lower bound in [55] under the memory snapshot assumption as we show in this work. A parallel algorithm animation tool was developed by Apgar [9] to aid in the analysis of Write-All algorithms using Stasko's [95] TANGO animation system. <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. <p> IMPROVING OBLIVIOUS SIMULATIONS 99 5.5 Improving Oblivious Simulations In addition to serving as the basis for oblivious simulations, any solution for the Write-All problem can also be readily used as a building block for custom transformations of efficient parallel algorithms into robust ones <ref> [55] </ref>. Custom transformations are interesting because in some cases it is possible to improve on the work of the nave oblivious simulation. <p> For example in the models with clear initial memory, a factor of log N= log log N was saved off the pointer doubling simulations <ref> [55] </ref>, and using randomization and off-line adversaries, improvements can be obtained in expected work of other algorithms [72, 75]. <p> SIMPLIFYING MEMORY ASSUMPTIONS Definition 6.1 An algorithm that uses P processors to solve a Write-All problem of size N is contamination-tolerant, if it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., <ref> [55, 59, 61, 92] </ref>, or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in [29] or the maximum finding algorithm in [75], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [56] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Efficient Parallel Algorithms On Restartable Fail-Stop Processors", </title> <booktitle> in Proc. of the 10th ACM Symposium on Principles of Distributed Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration.
Reference: [57] <author> P. C. Kanellakis and A. A. Shvartsman, </author> <title> "Robust Computing with Fail-Stop Processors", </title> <booktitle> in Proceedings of the Second Annual Review and Workshop on Ultrade-pendable Multicomputers, Office of Naval Research, </booktitle> <pages> pp. 55-60, </pages> <year> 1991. </year> <note> 130 BIBLIOGRAPHY </note>
Reference: [58] <author> R. M. Karp and V. Ramachandran, </author> <title> "A Survey of Parallel Algorithms for Shared-Memory Machines", </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 1, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: The efficiency and scalability of parallel algorithms have been the subject of research since the seventies. A model of parallel computation known as the Parallel Random Access Machine or PRAM [44] has attracted much attention, and many "efficient" and "optimal" algorithms have been designed for it (the surveys <ref> [40, 58] </ref> contain a wealth of information on the subject). The PRAM is a convenient abstraction that combines the power of parallelism with the simplicity of a RAM (Random Access Machine) [39], but it has several unrealistic features. The PRAM has the following requirements: 1. <p> The PRAM model is used widely in the parallel algorithms research community as a convenient and elegant model, and a wealth of efficient algorithms exist and are continually being developed for this model. The surveys of Eppstein and Galil [40] and Karp and Ramachandran <ref> [58] </ref> cover all of the important variations of the PRAM model, and give most of the fundamental PRAM algorithms. Instead of reiterating the rationale for studying the PRAM model and listing the variations of the PRAM models, 17 18 CHAPTER 2. <p> Instead of reiterating the rationale for studying the PRAM model and listing the variations of the PRAM models, 17 18 CHAPTER 2. MODELS AND DEFINITIONS we refer the reader to the two excellent surveys <ref> [40, 58] </ref> that succinctly address the topic in the respective introductory sections. For the base model in this work, we use the following definition of the PRAM [44]: 1. There are P initial processors with unique identifiers (PID) in the range 1; . . . ; P . <p> WRITE-ALL LOWER BOUNDS WITH MEMORY SNAPSHOTS PRAM with only a factor of O (log P ) more parallel work <ref> [58] </ref>. We now show that a more severe difference exists between CRCW and CREW PRAMs (and thus also EREW PRAMs) when the processors are subject to failures. <p> In the restartable model, we can make fault-tolerant all of the above, except for the priority PRAM (for detailed 85 86 CHAPTER 5. ALGORITHM SIMULATIONS AND TRANSFORMATIONS surveys of PRAM variations see <ref> [40, 58] </ref>). For the no-restart model, we show that the simulation of PRAM algorithms can be done using optimal work in the presence of arbitrary fail-stop errors. <p> Other processor private memory cells are stored in general purposes registers. Here we will use a formalization of the definition of PRAM such as the one used by Karp and Ramachandran in <ref> [58] </ref>. Informally, PRAM instructions consist of three synchronous cycles: 1. Read cycle: a processor reads a value from a location in shared memory into private memory, 2. Compute cycle: a processor performs a computation using private memory, 3.
Reference: [59] <author> Z. M. Kedem, K. V. Palem, and P. Spirakis, </author> <title> "Efficient Robust Parallel Computations," </title> <booktitle> in Proc. 22nd ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 138-148, </pages> <year> 1990. </year>
Reference-contexts: INTRODUCTION the notion of parallel robustness was introduced, and where the Write-All problem was defined. The techniques presented in [55] can readily be employed in making arbitrary PRAM algorithms fault-tolerant. The iterated Write-All paradigm was employed (independently) by Kedem et al. in <ref> [59] </ref> and by Shvartsman in [92] to extend the results of [55] to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. <p> The iterated Write-All paradigm was employed (independently) by Kedem et al. in <ref> [59] </ref> and by Shvartsman in [92] to extend the results of [55] to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. The algorithms analyzed included algorithms from [55] and [75], and a new algorithm based on pointer doubling that has a good expected behavior for the failure model defined. <p> This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. Other algorithms in this model are given by Martel et al. in [72] and Martel and Subramonian in [73]. Kedem et al. [61] further refined the results in <ref> [59] </ref> to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. The fail-stop deterministic lower and upper bounds of [55] were also improved in [61] by log log N factors. <p> Alternatively, the adversary can be off-line as in Martel et al. [75], in which case all failure decisions are made prior to the start of a computation. Finally, an adversary might be limited probabilistically as in Kedem et al. <ref> [59] </ref>, where the faults are occur with certain probability. Here we deal with the omniscient on-line adversaries. Granularity: Whereas, there is a fair amount of analysis based on types of failures and kinds of adversaries, there has been less attention paid to granularity of failures. <p> In some cases it may be sufficient for a computation to be efficient with high probability, and allow worst case inefficiency. Such an approach was taken in <ref> [59] </ref> using a probabilistically restricted adversary. <p> Randomized algorithms for the Write-All problem are capable of improving on the upper bounds of the deterministic algorithms only when the adversary is off-line as in [75] or when the adversary is limited probabilistically as in <ref> [59, 61] </ref>. A randomized asynchronous coupon clipping (ACC) algorithm for the Write-All problem was analyzed by Martel et al. in [75]. <p> On the other hand, when the adversary is limited as in Martel et al. [75] and Kedem et al. <ref> [59] </ref> by using off-line, oblivious adversaries, or the adversaries that are limited stochastically, better expected work has been reported for CRCW PRAMs. 4.2 Lower Bounds for the Restartable Fail-Stop Model As we have shown in Example 2.4 in Section 2.6, without the update cycle accounting there is a thrashing adversary that <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. <p> 0 ) + (l=P )S w (P; P 0 )] = O [S w (P; P 0 ) + log k N S w (P; P 0 )] 2 A construction for PRAM simulation, similar to the one used in Lemma 5.1 was independently developed by Kedem et al. in <ref> [59] </ref> using the Write-All technique to simulate any common or arbitrary CRCW PRAM algorithm that uses arbitrary shared and no local memory. That construction does not allow any local memory, while we allows up to poly-log local memory. <p> Note that we allow the processors to update the local memory in unit time. This makes our simulation nominally more general because the use of local memory is not mandated but is allowed up to a certain limit, while <ref> [59] </ref> does not allow any local memory. In [59] it is also observed that since P processors can only read P and write P shared memory cells in a single PRAM step, the overhead in shared memory need only be O (P ) when processors have no local memory. <p> Note that we allow the processors to update the local memory in unit time. This makes our simulation nominally more general because the use of local memory is not mandated but is allowed up to a certain limit, while <ref> [59] </ref> does not allow any local memory. In [59] it is also observed that since P processors can only read P and write P shared memory cells in a single PRAM step, the overhead in shared memory need only be O (P ) when processors have no local memory. <p> Using the general simulation techniques, such as <ref> [59, 75, 92] </ref>, if S w (N; P ) is the efficiency of solving a Write-All instance of size N using P processors, then a single N - processor PRAM step can be simulated using P fail-stop processors and work S w (N; P ). <p> For example, up to now, we assumed it is all 0's, but the algorithms would work even if the N locations were initialized using arbitrary 0's and 1's. A much more important assumption in all previous Write-All solutions (both in this thesis, and by other authors, e.g, <ref> [29, 59, 61, 75] </ref>) was regarding the initial state of additional auxiliary memory used (typically of (P ) size). The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. <p> SIMPLIFYING MEMORY ASSUMPTIONS Definition 6.1 An algorithm that uses P processors to solve a Write-All problem of size N is contamination-tolerant, if it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., <ref> [55, 59, 61, 92] </ref>, or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in [29] or the maximum finding algorithm in [75], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [60] <author> Z. M. Kedem, K. V. Palem, M. O. Rabin and A. Raghunathan, </author> <title> "Efficient Program Transformations for Resilient Parallel Computation via Randomization," </title> <booktitle> to appear in Proc 24th ACM Symposium on Theory of Computing, </booktitle> <year> 1992. </year>
Reference-contexts: The fail-stop deterministic lower and upper bounds of [55] were also improved in [61] by log log N factors. Recently, Kedem et al. <ref> [60] </ref> further investigated the use of randomization for resilient parallel computation. Martel [71] has improved the analysis of the main algorithm in [55] by a log log N factor.
Reference: [61] <author> Z. M. Kedem, K. V. Palem, A. Raghunathan, and P. Spirakis, </author> <title> "Combining Tentative and Definite Executions for Dependable Parallel Computing," </title> <booktitle> in Proc 23d ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 381-390, </pages> <year> 1991. </year>
Reference-contexts: This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. Other algorithms in this model are given by Martel et al. in [72] and Martel and Subramonian in [73]. Kedem et al. <ref> [61] </ref> further refined the results in [59] to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. The fail-stop deterministic lower and upper bounds of [55] were also improved in [61] by log log N factors. <p> Kedem et al. <ref> [61] </ref> further refined the results in [59] to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. The fail-stop deterministic lower and upper bounds of [55] were also improved in [61] by log log N factors. Recently, Kedem et al. [60] further investigated the use of randomization for resilient parallel computation. Martel [71] has improved the analysis of the main algorithm in [55] by a log log N factor. <p> Randomized algorithms for the Write-All problem are capable of improving on the upper bounds of the deterministic algorithms only when the adversary is off-line as in [75] or when the adversary is limited probabilistically as in <ref> [59, 61] </ref>. A randomized asynchronous coupon clipping (ACC) algorithm for the Write-All problem was analyzed by Martel et al. in [75]. <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. <p> For example, up to now, we assumed it is all 0's, but the algorithms would work even if the N locations were initialized using arbitrary 0's and 1's. A much more important assumption in all previous Write-All solutions (both in this thesis, and by other authors, e.g, <ref> [29, 59, 61, 75] </ref>) was regarding the initial state of additional auxiliary memory used (typically of (P ) size). The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. <p> SIMPLIFYING MEMORY ASSUMPTIONS Definition 6.1 An algorithm that uses P processors to solve a Write-All problem of size N is contamination-tolerant, if it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., <ref> [55, 59, 61, 92] </ref>, or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in [29] or the maximum finding algorithm in [75], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values. <p> In order to further improve these bounds, the strong assumption of memory snapshots must be removed. There was some progress in this direction. Under different assumptions, an (N log N ) lower bound is shown for failures without restarts in <ref> [61] </ref>. Can these lower bounds be further improved? For the no-restart fail-stop model, the improvements can be modest at best, since only a log N= log log N gap remains between the upper and lower bounds.
Reference: [62] <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> "Efficient Synchronization on Multiprocessors with Shared Memory," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 10, no. 4, </volume> <pages> pp. </pages> <month> 579-601 </month> <year> 1988. </year>
Reference-contexts: The fault tolerance of interconnection networks has been the subject of much work in its own turn. The networks are made more reliable by employing redundancy [2]. A combining interconnection network that is perfectly suited for implementing synchronous concurrent reads and writes is formally treated in <ref> [62] </ref> (the combining properties are used in their simplest form only to implement concurrent access to memory). Finally, fail-stop processors are formally presented and justified in [90]. The abstract model that we are studying can be realized (Figure 1.1) in the following architecture, using the components just cited: 1.
Reference: [63] <author> C. P. Kruskal, L. Rudolph, M. Snir, </author> <title> "A Complexity Theory of Efficient Parallel Algorithms," </title> <booktitle> Theoretical Computer Science 71, </booktitle> <pages> pp. 95-132, </pages> <year> 1990. </year>
Reference-contexts: Such characterizations of parallel algorithm efficiency are defined by Vitter and Simons in [100] and expanded on by Kruskal et al. in <ref> [63] </ref>. 2.3. THE WRITE-ALL PROBLEM 21 2.3 The Write-All Problem In order to deal with failures, it is necessary for the correct processors to detect the failures and reschedule the work of the failed processors. <p> Further critique of the notion that N C class of algorithms is the class of efficient parallel algorithms is given by Kruskal et al. in <ref> [63] </ref>. In the context of fault-tolerant computation we suggest that while a definition of robustness can be made in terms of N C, this definition is not very meaningful in terms of the resulting algorithm efficiency. <p> Such characterization of parallel algorithm efficiency are defined by Vitter and Simons in [100] and expanded on by Kruskal et al. in <ref> [63] </ref>. The efficiency classes defined in [63] are as follows: Let A be a problem such that the (RAM) time complexity of the best known sequential algorithm is T (N ). <p> Such characterization of parallel algorithm efficiency are defined by Vitter and Simons in [100] and expanded on by Kruskal et al. in <ref> [63] </ref>. The efficiency classes defined in [63] are as follows: Let A be a problem such that the (RAM) time complexity of the best known sequential algorithm is T (N ). <p> For example, the algorithms become sequential when only one processor is active. 5.6.1 Fail-stop model without restarts We first examine whether the classes of <ref> [63] </ref> are closed with respect to our fault-tolerant transformations in the fail-stop model. We assume that P = P (N ) processors are used. Also, if P is polynomial in N , then log P = O (log N ). <p> time of algorithm A is t (N ), then the parallel time of execution for ~(A) using at least cP active processors is c 2 t (N ) log 2 N= log log N for some constant c 2 &gt; 0, The resulting closure properties of the classes defined in <ref> [63] </ref> under our fail-stop transformation is summarized in Table 5.1. 5.6. ON PARALLEL COMPLEXITY CLASSES AND FAULT-TOLERANCE 107 5.6.2 Restartable fail-stop model We next examine whether the classes of [63] are closed with respect to our transformations in the restartable fail-stop model. <p> 2 N= log log N for some constant c 2 &gt; 0, The resulting closure properties of the classes defined in <ref> [63] </ref> under our fail-stop transformation is summarized in Table 5.1. 5.6. ON PARALLEL COMPLEXITY CLASSES AND FAULT-TOLERANCE 107 5.6.2 Restartable fail-stop model We next examine whether the classes of [63] are closed with respect to our transformations in the restartable fail-stop model. Again we have log P = O (log N ). <p> The closure properties of the classes of <ref> [63] </ref> under the restartable fail-stop transfor mation is summarized in Table 5.2.
Reference: [64] <author> L. E. Ladner, M. J. Fischer, </author> <title> "Parallel Prefix Computation", </title> <journal> Journal of the ACM, </journal> <volume> vol. 27, no. 4, </volume> <pages> pp. 831-838, </pages> <year> 1980. </year>
Reference-contexts: Efficient parallel algorithms and circuits for computing prefix sums were given by Ladner and Fischer in 100 CHAPTER 5. ALGORITHM SIMULATIONS AND TRANSFORMATIONS <ref> [64] </ref>, where the prefix problem is defined as follows: Given an associative operation on a domain D, and x 1 ; . . .; x n 2 D, compute, for each k, (1 k n) the sum L k In order to compute the prefix sums of N values using N <p> Proof: The prefix summation algorithm that we are going to use as the basis, is an iterative version of the recursive algorithm of <ref> [64] </ref>. The algorithm consists of two stages: (1) first a binary summation tree is computed, (2) then the individual prefix sums are computed from the summation tree obtained in the first stage. Each prefix sum requires no more than logarithmic number of additions.
Reference: [65] <author> L. Lamport, </author> <title> "On Interprocess Communication", </title> <booktitle> Distributed Computing, 1 (1986), </booktitle> <pages> pp. 77-101. </pages>
Reference: [66] <author> L. Lamport and N. Lynch, </author> <title> "Distributed Computing: Models and Methods," </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 2, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: Types of failures, such as byzantine, omission failures, fail-stop failures, etc., have been part of the consensus literature as distributed algorithms were being developed to deal with failures (e.g., as discussed by Lamport and Lynch in the survey <ref> [66] </ref>). In the area of parallel computation, where processors are more tightly coupled as compared 2.4. MODELS OF FAILURE 23 to a distributed environment, failures need to be classified further with the emphasis placed on the more benign fail-stop case.
Reference: [67] <author> M. Li and Y. Yesha, </author> <title> "New Lower Bounds for Parallel Computation," </title> <journal> Journal of the ACM, </journal> <volume> vol. 36, no. 3, </volume> <pages> pp. 671-680, </pages> <year> 1989. </year>
Reference-contexts: on a domain D, and x 1 ; . . .; x n 2 D, compute, for each k, (1 k n) the sum L k In order to compute the prefix sums of N values using N processors, at least log N= log log N parallel steps are required <ref> [20, 67] </ref>, and the known algorithms require at least log N steps. Therefore an oblivious simulation of a known prefix algorithm will require simulating at least log N steps. When using P = N processors, the work of such simulation will be O (S w log N ). <p> In lines 13-17 the appropriate subtree sum is added (line 14) at depth h only if the corresponding bit value of the processor P ID is true. 2 Note that because of the lower bounds of Beame and Hastad [20] and Li and Yesha <ref> [67] </ref>, at least log N= log log N parallel time and at least N log N= log log N work will be required by P = N processors to compute the prefix sums in the absence of failures.
Reference: [68] <author> A. Lopez-Ortiz, </author> <title> "Algorithm X takes work (n log 2 n= log log n) in a synchronous fail-stop (no restart) PRAM", </title> <type> unpublished manuscript, </type> <year> 1992. </year> <note> BIBLIOGRAPHY 131 </note>
Reference-contexts: In the fail-stop model we show and analyse a particular failure scenario (this scenario was refined by Lopez-Ortiz <ref> [68] </ref> to exhibit the known worst fail-stop work for algorithm X). <p> Recently Lopez-Ortiz refined the particular scenario used in the proof above to exhibit the known worst fail-stop work for algorithm X of fi (N log 2 N= log log N ) <ref> [68] </ref>. As the corollary of this result, the upper bound for algorithm X is no better than the upper bound for algorithm W for the fail-stop no-restart model. 3.3.2 Algorithms X coin and X die In this thesis we investigate deterministic solutions and concentrate on the worst case analysis.
Reference: [69] <author> N.A. Lynch, </author> <title> "One Hundred Impossibility Proofs for Distributed Comuting", </title> <booktitle> Proc. of the 8th ACM Symposium on Principles of Distributed Computing, </booktitle> <pages> pp. 1-27, </pages> <year> 1989. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [41, 69, 79] </ref> for surveys of the topic, and [37, 42, 43] for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [79, 41, 69] </ref> for surveys, and [42, 43, 37] for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects.
Reference: [70] <author> N.A. Lynch, N.D. Griffeth, M.J. Fischer, L.J. Guibas, </author> <title> "Probabilistic Analysis of a Network Resource Allocation Algorithm", </title> <journal> Information and Control, </journal> <volume> vol. 68, </volume> <pages> pp. 47-85, </pages> <year> 1986. </year>
Reference-contexts: For example, fault tolerance is the subject of significant current research in the setting of dynamic asynchronous network protocols. Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource monitored <ref> [4, 70] </ref>. Other developments [3, 13, 15] solve the problems of executing distributed algorithms in the presence of dynamic network changes, i.e., dynamic changes of the computation medium. <p> Distributed controllers have been developed for resource allocation in network protocols, where the total number of messages sent is the resource controlled. For instance, the algorithms of Lynch et al. <ref> [70] </ref> (with a probabilistic setting) and of Awerbuch et al. [4] (with a deterministic setting) are among the most sophisticated in that area. The problem we address in this thesis is, at an intuitive level, one of controlling resource allocation. <p> DISCUSSION AND OPEN PROBLEMS 1. Formulate a common framework for merging our results on fault-tolerant parallelism and the existing results on distributed network protocols in the presence of topological changes. 2. What about randomized robust parallel computation? Could a formalization along the lines of <ref> [70] </ref> be used? Another good area here seems to be the average case analysis of fail-stop CREW processors [73]. 3. Evaluate the feasibility of implementing fault-tolerant algorithms based on differ ent multiprocessing paradigms. 4.
Reference: [71] <author> C. </author> <title> Martel, </title> <type> personal communication, </type> <month> March, </month> <year> 1991. </year>
Reference-contexts: The fail-stop deterministic lower and upper bounds of [55] were also improved in [61] by log log N factors. Recently, Kedem et al. [60] further investigated the use of randomization for resilient parallel computation. Martel <ref> [71] </ref> has improved the analysis of the main algorithm in [55] by a log log N factor. This improvement also leads to the upper bound that matches the lower bound in [55] under the memory snapshot assumption as we show in this work. <p> This is done by utilizing a variable per block processor survival coefficient k = 1 1= U b , where U b , is the underestimate of the unvisited leaves in Algorithm W after the completion of block iteration b. It was shown by Martel <ref> [71] </ref> that the worst case performance of algorithm W is no worse than S = fi (N log 2 N= log log N ). We state this result here and give its proof in Appendix A.6. Theorem 3.9 [71] Algorithm W is a robust parallel algorithm for the Write-All problem with <p> It was shown by Martel <ref> [71] </ref> that the worst case performance of algorithm W is no worse than S = fi (N log 2 N= log log N ). We state this result here and give its proof in Appendix A.6. Theorem 3.9 [71] Algorithm W is a robust parallel algorithm for the Write-All problem with S fl = O (N log 2 N= log log N ), where N is the input array size, and the initial number of processors P is between 1 and N . 3.2.2 Algorithm V Algorithm W in <p> Remark 4.1 The lower bound of (N log N log log N ) is the strongest possible bound for the fail-stop model without restarts under the memory snapshot assumption. This can be shown in a straightforward way by adapting the analysis of algorithm W by Martel <ref> [71] </ref> (Theorem 3.4bis in Appendix A.6). According to the analysis, the number of "block steps" of W for P = N is O (N log N= log log N ) and each block-step can be realized at unit cost each, under the memory snapshot assumption.
Reference: [72] <author> C. Martel, A. Park, and R. Subramonian, </author> <title> "Work-optimal Asynchronous Algorithms for Shared Memory Parallel Computers," </title> <note> to appear in SIAM Journal on Computing in 1992. </note>
Reference-contexts: This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. Other algorithms in this model are given by Martel et al. in <ref> [72] </ref> and Martel and Subramonian in [73]. Kedem et al. [61] further refined the results in [59] to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. <p> For example in the models with clear initial memory, a factor of log N= log log N was saved off the pointer doubling simulations [55], and using randomization and off-line adversaries, improvements can be obtained in expected work of other algorithms <ref> [72, 75] </ref>.
Reference: [73] <author> C. Martel and R. Subramonian, </author> <title> "On the Complexity of Certified Write-All Algorithms," </title> <type> Tech. Report CSE-91-24, </type> <institution> Univ. of Calif.-Davis, </institution> <year> 1991. </year>
Reference-contexts: This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. Other algorithms in this model are given by Martel et al. in [72] and Martel and Subramonian in <ref> [73] </ref>. Kedem et al. [61] further refined the results in [59] to produce an approach that leads to constant expected slowdown of PRAM algorithms when the power of the adversary is restricted. <p> Finally, it can be shown by a simple reduction to the Write-One problem that any N -processor CREW solution to the Write-All problem has the worst case of S = (N 2 ). 2 For the CREW PRAMs, Martel and Subramonian show a randomized Write-All algorithm in <ref> [73] </ref> that, when adversary is oblivious and non-adaptive, has expected work of only O (N log N ) using N processors, and expected work (N ) when using N= log N processors. <p> What about randomized robust parallel computation? Could a formalization along the lines of [70] be used? Another good area here seems to be the average case analysis of fail-stop CREW processors <ref> [73] </ref>. 3. Evaluate the feasibility of implementing fault-tolerant algorithms based on differ ent multiprocessing paradigms. 4. In actual multiprocessor practice, threads packages provide a basis for the implementation of a wide variety of parallel paradigms, e.g., [22, 36]. The available threads packages typically support shared-memory lightweight processes.
Reference: [74] <author> C. Martel and R. Subramonian, </author> <title> "Asynchronous PRAM Algorithms for List Ranking and Transitive Closure," </title> <type> Tech. Report CSE-90-12, </type> <institution> Univ. of Calif.-Davis, </institution> <note> revised 1991. </note>
Reference: [75] <author> C. Martel, R. Subramonian, and A. Park, </author> <title> "Asynchronous PRAMs are (Almost) as Good as Synchronous PRAMs," </title> <booktitle> in Proc. 32d IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 590-599, </pages> <year> 1990. </year> <note> Also see Tech. Rep. </note> <institution> CSE-89-6, Univ. of Calif.-Davis, </institution> <year> 1989. </year>
Reference-contexts: For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of <ref> [29, 31, 45, 75, 78] </ref>. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. <p> In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. The algorithms analyzed included algorithms from [55] and <ref> [75] </ref>, and a new algorithm based on pointer doubling that has a good expected behavior for the failure model defined. The deterministic execution of PRAM algorithms in [92] is optimal for any adversary when parallel slackness (as in [99]) is exploited to our advantage. <p> Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models <ref> [29, 31, 45, 78, 75] </ref>. A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in [75]. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. <p> A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in <ref> [75] </ref>. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line. Other algorithms in this model are given by Martel et al. in [72] and Martel and Subramonian in [73]. <p> When the adversary is in addition on-line, it can decide during the computation what processors will fail, as in this work. Alternatively, the adversary can be off-line as in Martel et al. <ref> [75] </ref>, in which case all failure decisions are made prior to the start of a computation. Finally, an adversary might be limited probabilistically as in Kedem et al. [59], where the faults are occur with certain probability. Here we deal with the omniscient on-line adversaries. <p> Randomized algorithms for the Write-All problem are capable of improving on the upper bounds of the deterministic algorithms only when the adversary is off-line as in <ref> [75] </ref> or when the adversary is limited probabilistically as in [59, 61]. A randomized asynchronous coupon clipping (ACC) algorithm for the Write-All problem was analyzed by Martel et al. in [75]. Assuming off-line adversaries, it was shown in [75] that ACC algorithm has expected work O (N ) using P = <p> are capable of improving on the upper bounds of the deterministic algorithms only when the adversary is off-line as in <ref> [75] </ref> or when the adversary is limited probabilistically as in [59, 61]. A randomized asynchronous coupon clipping (ACC) algorithm for the Write-All problem was analyzed by Martel et al. in [75]. Assuming off-line adversaries, it was shown in [75] that ACC algorithm has expected work O (N ) using P = N=(log N log fl N ) processors on inputs of size N . However, when the adversary is on-line then the algorithm becomes inefficient even for simple on-line adversaries. <p> of the deterministic algorithms only when the adversary is off-line as in <ref> [75] </ref> or when the adversary is limited probabilistically as in [59, 61]. A randomized asynchronous coupon clipping (ACC) algorithm for the Write-All problem was analyzed by Martel et al. in [75]. Assuming off-line adversaries, it was shown in [75] that ACC algorithm has expected work O (N ) using P = N=(log N log fl N ) processors on inputs of size N . However, when the adversary is on-line then the algorithm becomes inefficient even for simple on-line adversaries. <p> On the other hand, when the adversary is limited as in Martel et al. <ref> [75] </ref> and Kedem et al. [59] by using off-line, oblivious adversaries, or the adversaries that are limited stochastically, better expected work has been reported for CRCW PRAMs. 4.2 Lower Bounds for the Restartable Fail-Stop Model As we have shown in Example 2.4 in Section 2.6, without the update cycle accounting there <p> Write-All algorithms usually assume that an amount of shared memory proportional to either the number of processors P or the problem size N is available, and that it is initialized to zero (we relax this assumption in Section 6.1). This is true of all known Write-All solutions <ref> [8, 27, 55, 56, 59, 61, 75] </ref>, and this also applies to algorithms that can be adapted to serve as an Write-All solution, e.g., [29]. Furthermore, iterative use of the Write-All technique in a single algorithm requires that the heaps contain zeroes at the start of each iteration. <p> For example in the models with clear initial memory, a factor of log N= log log N was saved off the pointer doubling simulations [55], and using randomization and off-line adversaries, improvements can be obtained in expected work of other algorithms <ref> [72, 75] </ref>. <p> Using the general simulation techniques, such as <ref> [59, 75, 92] </ref>, if S w (N; P ) is the efficiency of solving a Write-All instance of size N using P processors, then a single N - processor PRAM step can be simulated using P fail-stop processors and work S w (N; P ). <p> For example, up to now, we assumed it is all 0's, but the algorithms would work even if the N locations were initialized using arbitrary 0's and 1's. A much more important assumption in all previous Write-All solutions (both in this thesis, and by other authors, e.g, <ref> [29, 59, 61, 75] </ref>) was regarding the initial state of additional auxiliary memory used (typically of (P ) size). The basic assumption has been that: The (P ) auxiliary shared memory is cleared or initialized to some known value. <p> it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., [55, 59, 61, 92], or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in [29] or the maximum finding algorithm in <ref> [75] </ref>, invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [76] <author> R. </author> <title> McEliece, The Theory of Information and Coding, </title> <publisher> Addison-Wesley, </publisher> <year> 1977. </year>
Reference-contexts: Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in <ref> [76] </ref>. The fault-tolerant issues of the interconnection networks used to integrate processors and memory modules are discussed in [2]. Fault tolerance of systolic arrays | a particular class of parallel machines | has been studied to some extent, and some of the achievements in that area are surveyed in [1]. <p> These memory are being routinely manufactured with built-in fault tolerance. The three main techniques used in providing memory fault-tolerance are: (1) Coding: in addition to the bits being stored, this technique utilizes additional (parity) bits in conjunction with various error detecting and/or correcting codes (see McEliece <ref> [76] </ref> for a grand tour). (2) Replication or shadowing: two or more copies of the memory are maintained with either the majority vote being taken, or the faulty units being shut off in a hybrid approach. (3) Reconfiguration: spare memories are used to replace faulty units by reconfiguring memory units.
Reference: [77] <editor> S. Mullender (Editor), </editor> <booktitle> Distributed Systems, </booktitle> <publisher> ACM Press, Frontier Series, </publisher> <year> 1989. </year>
Reference-contexts: The potential reliability advantage of distributed computing systems is due to the replication of resources. The resulting redundancy in computation is a trade-off of efficiency (measured in terms of available resources) for fault tolerance. Mullender, in the 1.2. CONTRIBUTIONS 5 1989 Distributed Systems edition of the ACM's Frontier Series <ref> [77] </ref>, considers independent failures essential for distributed systems, i.e., a failure of a single processing node must not lead to the failure of the entire distributed system. <p> CONTRIBUTIONS 5 1989 Distributed Systems edition of the ACM's Frontier Series [77], considers independent failures essential for distributed systems, i.e., a failure of a single processing node must not lead to the failure of the entire distributed system. He gives the disqualifying disadvantage of multiprocessors with shared memory <ref> [77, page 6] </ref>: What disqualifies multiprocessors is that there is no independent failure: when one processor crashes, the whole system stops working. . . .
Reference: [78] <author> N. Nishimura, </author> <title> "Asynchronous Shared Memory Parallel Computation," </title> <booktitle> in Proc. 3rd ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 76-84, </pages> <year> 1990. </year>
Reference-contexts: For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of <ref> [29, 31, 45, 75, 78] </ref>. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76]. <p> Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models <ref> [29, 31, 45, 78, 75] </ref>. A simple randomized algorithm that serves as a basis for simulating arbitrary PRAM algorithms on an asynchronous PRAM is presented by Martel et al. in [75]. This randomized asynchronous simulation has very good expected performance for the Write-All problem when the adversary is off-line.
Reference: [79] <author> M. Pease, R. Shostak, L. Lamport, </author> <title> "Reaching agreement in the presence of faults", </title> <journal> JACM, </journal> <volume> vol. 27, no. 2, </volume> <pages> pp. 228-234, </pages> <year> 1980. </year>
Reference-contexts: Finally, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [41, 69, 79] </ref> for surveys of the topic, and [37, 42, 43] for lower bounds results). 1.3.3 Technology for fault tolerance Several engineering and technological approaches exist to implementing parallel systems that enable them to operate correctly when they are subjected to certain failures. <p> Significant research in the past decade was devoted to the study of fault-tolerant, distributed computation in the presence of arbitrary (and even malicious) system faults. Byzantine and other simpler failures were extensively studied in the context of distributed algorithms for the consensus problem (e.g., Pease et al. <ref> [79, 80] </ref>, also see a survey by Fischer [41]). The failure models for parallel computation are constructed from failure definitions along the spectrum of failure models discussed below. <p> For example, the synchronous parallel setting with fail-stop processor errors is free from the limitations inherent in the asynchronous environment, or the situations where the processors can perform malicious actions (see <ref> [79, 41, 69] </ref> for surveys, and [42, 43, 37] for lower bounds). Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects.
Reference: [80] <author> M. Pease, R. Shostak, L. Lamport, </author> <title> "The Byzantine Generals Problem", </title> <journal> ACM TOPLAS, </journal> <volume> vol. 4, no. 3, </volume> <pages> pp. 382-401, </pages> <year> 1982. </year> <note> 132 BIBLIOGRAPHY </note>
Reference-contexts: Significant research in the past decade was devoted to the study of fault-tolerant, distributed computation in the presence of arbitrary (and even malicious) system faults. Byzantine and other simpler failures were extensively studied in the context of distributed algorithms for the consensus problem (e.g., Pease et al. <ref> [79, 80] </ref>, also see a survey by Fischer [41]). The failure models for parallel computation are constructed from failure definitions along the spectrum of failure models discussed below.
Reference: [81] <author> N. Pippenger, </author> <title> "On Simultaneous Resource Bounds", </title> <booktitle> in Proc. of 20th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 307-311, </pages> <year> 1979. </year>
Reference-contexts: For example, the well-known class N C of algorithms characterizes efficiency primarily in terms of (polylogarithmic) time efficiency, even if the computational agent is large (polynomial) relative to the size of a problem <ref> [30, 81] </ref>. To characterize better the efficiency of parallel algorithms, the efficiency measures need to take into account both the parallel time and the size of the computational resource, i.e., parallel work. <p> This is because the algorithms in N C allow for polynomial inefficiency in work. In N C the efficiency is characterized in terms of (polylogarithmic) time, but the computational agent can be large (polynomial) relative to the size of a problem <ref> [30, 81] </ref>. Further critique of the notion that N C class of algorithms is the class of efficient parallel algorithms is given by Kruskal et al. in [63].
Reference: [82] <author> N. Pippenger, </author> <title> "On Networks of Noisy Gates", </title> <booktitle> Proc. of 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 30-38, </pages> <year> 1985. </year>
Reference-contexts: RELATED WORK 9 switches, as those of Rudolph in [88], and in general with the design of reliable systems from unreliable components, as done by Pippenger in <ref> [82] </ref> using gates or by Dwork et al. in [38] for networks. The notion of robustness that we target in this research differs from that of the sorting network in [88], and in that network a linear number of operations is still critical. <p> Finally, our work here deals with dynamic patterns of faults; for recent advances on coping with static fault patterns, for example, are addressed by Kaklamanis in [54]. The granularity of faults in our work is at the processor level; for recent work on gate granularities see <ref> [11, 82, 88] </ref>. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by [3, 4, 13, 15]). <p> Our modeling of fault tolerance has some similarities with the design of "robust" sorting networks, as those of Rudolph [88], and in general with the design of reliable systems from unreliable components, as in Pippenger <ref> [82] </ref> or Dwork et al. [38]. The distinguishing characteristic of our approach is the investigation of fault tolerance at the processor granularity as opposed to gate or switch granularities [82] and [88] respectively. Magnitude: Many hardware oriented fault tolerance techniques provide fault masking up to a pre-determined limit. <p> networks, as those of Rudolph [88], and in general with the design of reliable systems from unreliable components, as in Pippenger <ref> [82] </ref> or Dwork et al. [38]. The distinguishing characteristic of our approach is the investigation of fault tolerance at the processor granularity as opposed to gate or switch granularities [82] and [88] respectively. Magnitude: Many hardware oriented fault tolerance techniques provide fault masking up to a pre-determined limit. In a distributed setting, some algorithms can handle processor failures when the number of failures does not exceed a certain fraction of the total number of processors.
Reference: [83] <author> N. </author> <title> Pippenger, </title> <journal> "Communications Networks," </journal> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 1, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: Interconnection networks are typically used in multiprocessor systems to provide communication among processors, memory modules and other devices [52]. An encyclopaedic survey of the interconnection networks is given by Al-masi and Gottlieb in [6, Chapter 8]. Theoretical foundations for such networks are summarized by Pippenger in <ref> [83] </ref>. The networks are made more reliable by employing redundancy. A survey of fault tolerant interconnection networks is presented by Adams et al. in [2].
Reference: [84] <author> F.P. Preparata, </author> <title> "Holographic Dispersal and Recovery of Information," </title> <journal> in IEEE Trans. on Info. Theory, </journal> <volume> vol. 35, no. 5, </volume> <pages> pp. 1123-1124, </pages> <year> 1989. </year>
Reference-contexts: A survey of fault tolerant interconnection networks is presented by Adams et al. in [2]. An interesting interconnection network routing strategy was described by Preparata [85], in which fast routing is achieved by allowing for some messages to be lost and using a redundancy scheme <ref> [84, 86] </ref> to reconstruct lost information. The area of fault tolerance and efficiency of interconnection networks is extremely important as an enabling technology for fault-tolerant parallel computation.
Reference: [85] <author> F.P. Preparata, </author> <title> "Parallel Optical Interconnections and Discrete Holography," </title> <institution> a presentation at Brown. Univ. Industr. </institution> <note> Partners Symp., </note> <month> March, </month> <year> 1991. </year>
Reference-contexts: Theoretical foundations for such networks are summarized by Pippenger in [83]. The networks are made more reliable by employing redundancy. A survey of fault tolerant interconnection networks is presented by Adams et al. in [2]. An interesting interconnection network routing strategy was described by Preparata <ref> [85] </ref>, in which fast routing is achieved by allowing for some messages to be lost and using a redundancy scheme [84, 86] to reconstruct lost information. The area of fault tolerance and efficiency of interconnection networks is extremely important as an enabling technology for fault-tolerant parallel computation.
Reference: [86] <author> M.O. Rabin, </author> <title> "Efficient Dispersal of Information for Security, Load Balancing and Fault Tolerance", </title> <journal> J. of ACM, </journal> <volume> vol. 36, no. 2, </volume> <pages> pp. 335-348, </pages> <year> 1989. </year>
Reference-contexts: A survey of fault tolerant interconnection networks is presented by Adams et al. in [2]. An interesting interconnection network routing strategy was described by Preparata [85], in which fast routing is achieved by allowing for some messages to be lost and using a redundancy scheme <ref> [84, 86] </ref> to reconstruct lost information. The area of fault tolerance and efficiency of interconnection networks is extremely important as an enabling technology for fault-tolerant parallel computation.
Reference: [87] <author> A. Ranade, </author> <title> "How to Emulate Shared Memory", </title> <booktitle> Proc. of 28th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pp. 185-194, </pages> <year> 1987. </year>
Reference-contexts: OVERVIEW AND MOTIVATION 3 The gap between the abstract models of parallel computation and realizable parallel computers is being bridged by current research. For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see <ref> [49, 87, 97] </ref>. Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76].
Reference: [88] <author> L. Rudolph, </author> <title> "A Robust Sorting Network", </title> <journal> IEEE Trans. on Comp., </journal> <volume> vol. 34, no. 4, </volume> <pages> pp. 326-335, </pages> <year> 1985. </year>
Reference-contexts: Our modeling of fault tolerance where a processor is an entity subject to failures has some similarities with the design of "robust" sorting networks using fault-prone 1.3. RELATED WORK 9 switches, as those of Rudolph in <ref> [88] </ref>, and in general with the design of reliable systems from unreliable components, as done by Pippenger in [82] using gates or by Dwork et al. in [38] for networks. The notion of robustness that we target in this research differs from that of the sorting network in [88], and in <p> Rudolph in <ref> [88] </ref>, and in general with the design of reliable systems from unreliable components, as done by Pippenger in [82] using gates or by Dwork et al. in [38] for networks. The notion of robustness that we target in this research differs from that of the sorting network in [88], and in that network a linear number of operations is still critical. Another example is the emulation of PRAMs on faulty hypercubes. See the the recent result of Aumann and Ben-Or on high probability emulation [12]. <p> Finally, our work here deals with dynamic patterns of faults; for recent advances on coping with static fault patterns, for example, are addressed by Kaklamanis in [54]. The granularity of faults in our work is at the processor level; for recent work on gate granularities see <ref> [11, 82, 88] </ref>. 1.3.2 Fault-tolerant distributed computation Adding fault tolerance to algorithms is the subject of significant current research in the qualitatively different setting of dynamic asynchronous network protocols (recent results and an overview of this area is well represented by [3, 4, 13, 15]). <p> It is 24 CHAPTER 2. MODELS AND DEFINITIONS also reasonable to study failure granularity at the level of a single thread. Our modeling of fault tolerance has some similarities with the design of "robust" sorting networks, as those of Rudolph <ref> [88] </ref>, and in general with the design of reliable systems from unreliable components, as in Pippenger [82] or Dwork et al. [38]. The distinguishing characteristic of our approach is the investigation of fault tolerance at the processor granularity as opposed to gate or switch granularities [82] and [88] respectively. <p> those of Rudolph <ref> [88] </ref>, and in general with the design of reliable systems from unreliable components, as in Pippenger [82] or Dwork et al. [38]. The distinguishing characteristic of our approach is the investigation of fault tolerance at the processor granularity as opposed to gate or switch granularities [82] and [88] respectively. Magnitude: Many hardware oriented fault tolerance techniques provide fault masking up to a pre-determined limit. In a distributed setting, some algorithms can handle processor failures when the number of failures does not exceed a certain fraction of the total number of processors.
Reference: [89] <author> D. B. Sarrazin and M. Malek, </author> <title> "Fault-Tolerant Semiconductor Memories", </title> <journal> IEEE Computer, </journal> <volume> vol. 17, no. 8, </volume> <pages> pp. 49-56, </pages> <year> 1984. </year>
Reference-contexts: Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in <ref> [89] </ref>, while the theory of error detecting and correcting codes is reviewed in [76]. The fault-tolerant issues of the interconnection networks used to integrate processors and memory modules are discussed in [2]. <p> The survey of Sarrazin and Malek <ref> [89] </ref> covers these techniques that are used to make memory (cache and main) more reliable without appreciably degrading its performance. Robust interconnection networks Another important subject that has been the target of work is the area of fault-tolerant interconnection networks. <p> Semiconductor memories are the essential components of shared memory parallel systems. Memories are routinely manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance <ref> [89] </ref>. Interconnection networks are typically used in a multiprocessor system to provide communication among processors, memory modules and other devices, e.g., as in the Ultracomputer [91]. The fault tolerance of interconnection networks has been the subject of much work in its own turn.
Reference: [90] <author> R. D. Schlichting and F. B. Schneider, </author> <title> "Fail-Stop Processors: an Approach to Designing Fault-tolerant Computing Systems", </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 1, no. 3, </volume> <pages> pp. 222-238, </pages> <year> 1983. </year>
Reference-contexts: The convention for determining which processor or processors succeed, when concurrently writing to shared memory, is immaterial in our algorithms. We investigate fault-prone PRAMs whose processors exhibit fail-stop processor behavior, such as that of Schlichting and Schneider <ref> [90] </ref>. The only atomicity requirement is the atomic concurrent writing of single bits. The redundancy provided by concurrent reads and writes is essential to our model, e.g., when the writes are exclusive we show that efficiency and fault tolerance cannot be combined. <p> A combining interconnection network that is perfectly suited for implementing synchronous concurrent reads and writes is formally treated in [62] (the combining properties are used in their simplest form only to implement concurrent access to memory). Finally, fail-stop processors are formally presented and justified in <ref> [90] </ref>. The abstract model that we are studying can be realized (Figure 1.1) in the following architecture, using the components just cited: 1. There are P fail-stop processors, each with a unique address and some amount of local memory. Processors are unreliable. 2. There are Q addressable shared memory cells. <p> Using state of the art technology, processing elements are being designed with built-in diagnostics capabilities. Upon detecting failures, such processors can isolate themselves from the rest of the computing environment without harmful effects. Such processors are modeled as fail-stop processors. It was shown by Schlichting and Schneider in <ref> [90] </ref> that using a formal methodology and an appropriate programming language framework, it is possible to construct correct algorithms for fail-stop processors. In this work we in turn show that it is possible to construct efficient and fault-tolerant algorithms for certain classes of parallel fail-stop processors. <p> The two models are: 1. The fail-stop PRAM, where the processors do not restart after a failure, and 2. The restartable fail-stop PRAM, where the processors can restart after a failure. 2.5. NO-RESTART FAIL-STOP CRCW PRAM 25 We study fail-stop processor errors <ref> [90] </ref> that are determined by the worst case omniscient on-line (adaptive) adversary that is not limited as far as the frequency and magnitude of errors are concerned. <p> We only consider fail-stop (no restart) behavior: processors fail by stopping and not performing any further actions. Fail-stop models are reasonable approxima tions of what is desirable and achievable in practice <ref> [90] </ref>. 3. We assume that the shared memory writes of the individual PRAM steps are atomic with respect to failures: failures can occur before or after a shared write of O (log maxfN; P g) bit words, but not during the write. <p> Since the processors lose their context after a failure, they have to read something to regain it. Without at least one update cycle completing, the adversary can force the PRAM to thrash by allowing only these reads to be performed. Similar concerns are discussed in <ref> [90] </ref>. Update cycles as a unit of accounting: In our definition of completed work we only count completed update cycles.
Reference: [91] <author> J. T. Schwartz, </author> <title> "Ultracomputers", </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 484-521, </pages> <year> 1980. </year>
Reference-contexts: Memories are routinely manufactured with built-in fault tolerance using replication and coding techniques without appreciably degrading performance [89]. Interconnection networks are typically used in a multiprocessor system to provide communication among processors, memory modules and other devices, e.g., as in the Ultracomputer <ref> [91] </ref>. The fault tolerance of interconnection networks has been the subject of much work in its own turn. The networks are made more reliable by employing redundancy [2].
Reference: [92] <author> A. A. Shvartsman, </author> <title> "Achieving Optimal CRCW PRAM Fault-Tolerance", </title> <journal> in Information Processing Letters, </journal> <volume> vol. 39, </volume> <pages> pp. 59-66, </pages> <year> 1991. </year>
Reference-contexts: INTRODUCTION the notion of parallel robustness was introduced, and where the Write-All problem was defined. The techniques presented in [55] can readily be employed in making arbitrary PRAM algorithms fault-tolerant. The iterated Write-All paradigm was employed (independently) by Kedem et al. in [59] and by Shvartsman in <ref> [92] </ref> to extend the results of [55] to arbitrary PRAM algorithms (subject to fail-stop errors without restarts). In addition to the general simulation technique, [59] analyzes the expected behavior of several solutions to Write-All using a particular random failure model. <p> The algorithms analyzed included algorithms from [55] and [75], and a new algorithm based on pointer doubling that has a good expected behavior for the failure model defined. The deterministic execution of PRAM algorithms in <ref> [92] </ref> is optimal for any adversary when parallel slackness (as in [99]) is exploited to our advantage. Asynchronous versions of the PRAM is a subject of recent research. <p> Using the general simulation techniques, such as <ref> [59, 75, 92] </ref>, if S w (N; P ) is the efficiency of solving a Write-All instance of size N using P processors, then a single N - processor PRAM step can be simulated using P fail-stop processors and work S w (N; P ). <p> SIMPLIFYING MEMORY ASSUMPTIONS Definition 6.1 An algorithm that uses P processors to solve a Write-All problem of size N is contamination-tolerant, if it is a Write-All (N; P; 0) algorithm. 2 6.1.3 Write-All algorithms using contaminated memory The Write-All algorithms and simulations based on Write-All paradigm, e.g., <ref> [55, 59, 61, 92] </ref>, or the algorithms that can serve as Write-All solution, e.g., the addition algorithm in [29] or the maximum finding algorithm in [75], invariably assume that a linear portion of shared memory is either cleared or is initialized to known values.
Reference: [93] <author> A. A. Shvartsman, </author> <title> "A Group-Theoretic Aspect of a Multi-Processor Scheduling Problem", unpublished manuscript, </title> <booktitle> (presented in the open session of the 16th In-ternat. Symp. on Mathematical Foundations of Computer Science, </booktitle> <address> Poland, </address> <year> 1991). </year> <note> BIBLIOGRAPHY 133 </note>
Reference-contexts: We conjecture that the worst case work of this deterministic algorithm is no worse than the expected work of the randomized algorithm. The open problem below contains an interesting observation of a group-theoretic aspect of a multi-processor scheduling problem <ref> [93] </ref>. An open problem: What is the completed work of algorithm Y with the proposed determinization? We have performed some experimental analysis and all cases it resulted in the the work being is O (N log N ). This is the same as the expected work using random permutations.
Reference: [94] <author> A. A. Shvartsman, </author> <title> "How to Write-All Efficiently Even with Contaminated Memory", </title> <type> Tech. Report CS-92-09, </type> <institution> Brown Univ., </institution> <year> 1992. </year>
Reference: [95] <author> J.T. Stasko, </author> <title> "Tango: A framework and system for algorithms animation", </title> <journal> IEEE Computer, Vol.23, </journal> <volume> No.9, </volume> <pages> pp. 27-39, </pages> <year> 1990. </year>
Reference-contexts: This improvement also leads to the upper bound that matches the lower bound in [55] under the memory snapshot assumption as we show in this work. A parallel algorithm animation tool was developed by Apgar [9] to aid in the analysis of Write-All algorithms using Stasko's <ref> [95] </ref> TANGO animation system. Our modeling of fault tolerance where a processor is an entity subject to failures has some similarities with the design of "robust" sorting networks using fault-prone 1.3. <p> Experimental analysis: The design of new and the analysis of existing fault-tolerant parallel algorithms can be aided by using experimentation. Algorithm animation <ref> [25, 95] </ref> has the promise of providing additional insights into algortihms' behavior through visualization. A tool for animating Write-All algorithms was developed by Apgar [9] using Stasko's TANGO system [95]. <p> Algorithm animation [25, 95] has the promise of providing additional insights into algortihms' behavior through visualization. A tool for animating Write-All algorithms was developed by Apgar [9] using Stasko's TANGO system <ref> [95] </ref>. Using the that animation, an observer can monitor the progress of a parallel computation and dynamically inject processor faults and restarts. Concluding remarks It is often claimed that distributed computing systems have the potential advantage of higher reliability over centralized systems.
Reference: [96] <author> R.E. Tarjan, U. Vishkin, </author> <title> "Finding biconnected components and computing tree functions in logarithmic parallel time", </title> <booktitle> in Proc. of the 25th IEEE FOCS, </booktitle> <pages> pp. 12-22, </pages> <year> 1984. </year>
Reference-contexts: This improvement can be used with several important robust algorithms that are based on pointer doubling: Proposition 5.4 There is a robust parallel algorithm for computing the tree functions of Tarjan and Vishkin <ref> [96] </ref> with S = O (log N S w (N; P )= log log N ), where N is the input tree size and S w (N; P ) is the complexity of algorithm W for the initial number of processors P : 1 P N . <p> This results in O (N log N ) additive overhead. Therefore, for the algorithms that are dominated by pointer doubling with a cost of fi (N log N ), e.g., Tarjan and Vishkin <ref> [96] </ref>, there is no asymptotic degradation in the absence of failures.
Reference: [97] <author> E. Upfal, </author> <title> "An O(log N ) Deterministic Packet Routing Scheme," </title> <booktitle> in Proc. 21st ACM Symposium on Theory of Computing, </booktitle> <pages> pp. 241-250, </pages> <year> 1989. </year>
Reference-contexts: OVERVIEW AND MOTIVATION 3 The gap between the abstract models of parallel computation and realizable parallel computers is being bridged by current research. For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in [98]; for some recent work see <ref> [49, 87, 97] </ref>. Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. The reliability of semiconductor memories has been thoroughly studied, and a survey can be found in [89], while the theory of error detecting and correcting codes is reviewed in [76].
Reference: [98] <author> L. Valiant, </author> <title> "General Purpose Parallel Architectures," </title> <note> in Handbook of Theoretical Computer Science (ed. </note> <editor> J. van Leeuwen), </editor> <volume> vol. 1, </volume> <publisher> North-Holland, </publisher> <year> 1990. </year>
Reference-contexts: OVERVIEW AND MOTIVATION 3 The gap between the abstract models of parallel computation and realizable parallel computers is being bridged by current research. For example, memory access simulation in other architectures is the subject of a large body of literature surveyed in <ref> [98] </ref>; for some recent work see [49, 87, 97]. Computation on asynchronous PRAMs are the subject of [29, 31, 45, 75, 78]. <p> We have also shown that any parallel algorithm can be optimally executed in the presence of arbitrary fail-stop errors using a slightly smaller number of processors by taking advantage of parallel slackness as advocated by Valiant in <ref> [98] </ref>. By optimal execution we mean that the asymptotic efficiency of the source algorithm is not degraded even if arbitrary fail-stop errors are encountered. Given a N -processor PRAM algorithm we simulate it efficiently by a P -processor fail-stop CRCW PRAM algorithm, for P N . <p> Proving robustness is the subject of Section 3.2. The phases of the algorithm are such that reasoning about the failure patterns involves few cases and the algorithm analysis uses recurrences and inequalities. By exploiting parallel slackness as as advocated by Valiant <ref> [98] </ref>, and using a slightly smaller number of processors (1 P N=(log 2 N log log N log N ), where N is the size of the input array) we show that Write-All can be solved optimally with S fl = O (N ).
Reference: [99] <author> L. Valiant, </author> <title> "A Bridging Model for Parallel Computation," </title> <journal> Communications of the ACM, </journal> <volume> vol. 33, no. 8, </volume> <pages> pp. 103-111, </pages> <year> 1990. </year>
Reference-contexts: The algorithms analyzed included algorithms from [55] and [75], and a new algorithm based on pointer doubling that has a good expected behavior for the failure model defined. The deterministic execution of PRAM algorithms in [92] is optimal for any adversary when parallel slackness (as in <ref> [99] </ref>) is exploited to our advantage. Asynchronous versions of the PRAM is a subject of recent research. Various means of relaxing the strict synchronization requirements of the standard PRAM have been used to show that efficient algorithms can be efficiently executed on asynchronous models [29, 31, 45, 78, 75].
Reference: [100] <author> J. S. Vitter, R. A. Simons, </author> <title> "New Classes for Parallel Complexity: A Study of Unification and Other Complete Problems for P," </title> <journal> IEEE Trans. Comput., </journal> <volume> vol. 35, no. 5, </volume> <year> 1986. </year>
Reference-contexts: To characterize better the efficiency of parallel algorithms, the efficiency measures need to take into account both the parallel time and the size of the computational resource, i.e., parallel work. Such characterizations of parallel algorithm efficiency are defined by Vitter and Simons in <ref> [100] </ref> and expanded on by Kruskal et al. in [63]. 2.3. THE WRITE-ALL PROBLEM 21 2.3 The Write-All Problem In order to deal with failures, it is necessary for the correct processors to detect the failures and reschedule the work of the failed processors. <p> To reiterate: in order to characterize better the efficiency of parallel algorithms, the efficiency measures need to take into account both the parallel time and the size of the computational resource, i.e., parallel work. Such characterization of parallel algorithm efficiency are defined by Vitter and Simons in <ref> [100] </ref> and expanded on by Kruskal et al. in [63]. The efficiency classes defined in [63] are as follows: Let A be a problem such that the (RAM) time complexity of the best known sequential algorithm is T (N ).
Reference: [101] <author> S. Webber, </author> <title> "The Stratus Architecture," in The Theory and Practice of Reliable System Design, by D.P. Siewiorek and R.S. </title> <publisher> Swarz, Digital Press, </publisher> <year> 1991. </year>
Reference-contexts: When the number of processors is in the thousands, it is likewise impractical to provide fault tolerance and fault masking at the level that can be achieved by the more expensive, specialized machines with a small number of fast processors such as Tandem [17], Stratus <ref> [101] </ref> or VAXft [26]. In addition, the software for these systems is typically more complex and thus less reliable than the software of the more conventional uniprocessors. Therefore, it is critical that fault-tolerant versions 1 2 CHAPTER 1.
Reference: [102] <author> J. C. Wyllie, </author> <title> The Complexity of Parallel Computation, </title> <type> Ph.D. Thesis, </type> <institution> Cornell University, </institution> <type> TR 79-387, </type> <year> 1979. </year> <note> 134 BIBLIOGRAPHY </note>
Reference-contexts: Such high level notation can be formalized as a programming language that can be compiled using standard compilation techniques and the techniques specific to PRAMs as discussed by Wyllie <ref> [102] </ref>. 2.2 Measures of Efficiency Computation speed-up is one of the central reasons for using parallel computers. In this section, we introduce and discuss a particular way of flexibly factoring fault tolerance into the conventional definition of parallel work. <p> We are going to revisit this technique in more detail in the next two sections. 5.2 A PRAM Interpreter PRAM programs are normally presented as high level programs that can be compiled into assembly level instructions using conventional techniques (see a discussion in Wyl-lie's thesis <ref> [102] </ref>). As is the case with sequential processors, the instructions are stored in memory, the address of an instruction to be executed next is stored in an instruction counter register, and in order to execute an instruction, it is fetched into an instruction buffer.
References-found: 102

