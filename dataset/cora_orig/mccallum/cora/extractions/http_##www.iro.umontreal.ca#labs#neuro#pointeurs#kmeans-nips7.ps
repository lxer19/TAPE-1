URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/kmeans-nips7.ps
Refering-URL: http://www.iro.umontreal.ca/labs/neuro/proc.html
Root-URL: http://www.iro.umontreal.ca
Email: leon@neuristique.fr  bengioy@iro.umontreal.ca  
Title: Convergence Properties of the K-Means Algorithms  
Author: Leon Bottou Neuristique, Yoshua Bengio 
Address: 75010 Paris, France  Montreal, Qc H3C-3J7, Canada  
Affiliation: 28 rue des Petites Ecuries,  Dept. I.R.O. Universite de Montreal  
Abstract: This paper studies the convergence properties of the well known K-Means clustering algorithm. The K-Means algorithm can be described either as a gradient descent algorithm or by slightly extending the mathematics of the EM algorithm to this hard threshold case. We show that the K-Means algorithm actually minimizes the quantization error using the very fast Newton algorithm. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bacceli, F., Cohen, G., and Olsder, G. J. </author> <year> (1992). </year> <title> Synchronization and Linearity. </title> <publisher> Wiley. </publisher>
Reference-contexts: Although K-Means does not fit in a probabilistic framework, this similarity holds for a very deep reason: The semi-ring of probabilies (&lt; + ; +; fi) and the idempotent semi-ring of hard-threshold scores (&lt;; Min; +) share the most significant algebraic properties <ref> (Bacceli, Cohen and Olsder, 1992) </ref>. This assertion completely describes the similarities and the potential differences between soft-threshold and hard-threshold algorithms. A complete discussion however stands outside the scope of this paper. The principle of EM is to introduce additional "hidden" variables whose knowledge would make the optimization problem easier.
Reference: <author> Bengio, Y. </author> <year> (1991). </year> <title> Artificial Neural Networks and their Application to Sequence Recognition. </title> <type> PhD thesis, </type> <institution> McGill University, (Computer Science), Montreal, Qc., Canada. </institution>
Reference-contexts: After going through the first few patterns (depending of the amount of redundancy), online K-Means indeed improves the prototypes as much as a complete batch K-Means epoch. Other researchers have compared batch and online algorithms for neural networks, with similar conclusions <ref> (Bengio, 1991) </ref>. 5 EXPERIMENTS Experiments have been carried out with Fisher's iris data set, which is composed of 150 points in a four dimensional space representing physical measurements on various species of iris flowers.
Reference: <author> Bottou, L. </author> <year> (1991). </year> <institution> Une approche theorique de l'apprentissage connexioniste; applications a la reconnaissance de la parole. </institution> <type> PhD thesis, </type> <institution> Universite de Paris XI. </institution>
Reference-contexts: Convergence proofs for both algorithms <ref> (Bottou, 1991) </ref> exist for decreasing values of the learning rates satisfying the conditions P * 2 t &lt; 1. Following (Kohonen, 1989), we could choose * t = * 0 =t. <p> Although the derivatives are undefined on a few points, these theorems prove that the algorithms almost surely converge to a local minimum because the local variations of the loss function are conveniently bounded (semi-differentiability). Unlike previous results, the above convergence proofs allow for non-linearity, non-differentiability (on a few points) <ref> (Bottou, 1991) </ref>, and replacing learning rates by a positive definite matrix (Drian court, 1994). 4 K-MEANS AS A NEWTON OPTIMIZATION We prove in this section that Batch K-Means (6) applies the Newton algorithm. 4.1 THE HESSIAN OF K-MEANS Let us compute the Hessian H of the K-Means cost function (2).
Reference: <author> Darken, C. and Moody, J. </author> <year> (1991). </year> <title> Note on learning rate schedules for stochastic optimization. </title> <editor> In Lippman, R. P., Moody, R., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 832-838, </pages> <address> Denver, </address> <publisher> CO. Morgan Kaufmann, </publisher> <address> Palo Alto. </address>
Reference: <author> Dempster, A. P., Laird, N. M., and Rubin, D. B. </author> <year> (1977). </year> <title> Maximum-likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38. </pages>
Reference-contexts: A second reason lies in the traditional debate between hard threshold (e.g. K-Means, Viterbi Training) and soft threshold (e.g. Gaussian Mixtures, Baum Welch) algorithms (Nowlan, 1991). Soft threshold algorithms are often preferred because they have an elegant probabilistic framework and a general optimization algorithm named EM (expectation-maximization) <ref> (Dempster, Laird and Rubin, 1977) </ref>. In the case of a gaussian mixture, the EM algorithm has recently been shown to approximate the Newton optimization algorithm (Xu and Jordan, 1994). <p> We insist however on the identity between this derivation and the mathematics of EM (Liporace, 1976) <ref> (Dempster, Laird and Rubin, 1977) </ref>.
Reference: <author> Driancourt, X. </author> <year> (1994). </year> <type> Optimisation par descente de gradient stochastique : : : . PhD thesis, </type> <institution> Universite de Paris XI, </institution> <address> 91405 Orsay cedex, France. </address>
Reference: <author> Kohonen, T. </author> <year> (1989). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, 3 edition. </address>
Reference-contexts: Convergence proofs for both algorithms (Bottou, 1991) exist for decreasing values of the learning rates satisfying the conditions P * 2 t &lt; 1. Following <ref> (Kohonen, 1989) </ref>, we could choose * t = * 0 =t.
Reference: <author> Liporace, L. A. </author> <year> (1976). </year> <title> PTAH on continuous multivariate functions of Markov chains. </title> <type> Technical Report 80193, </type> <institution> Institute for Defense Analysis, Communication Research Department. </institution>
Reference-contexts: We insist however on the identity between this derivation and the mathematics of EM <ref> (Liporace, 1976) </ref> (Dempster, Laird and Rubin, 1977).
Reference: <author> MacQueen, J. </author> <year> (1967). </year> <title> Some methods for classification and analysis of multivariate observations. </title> <booktitle> In Proceedings of the Fifth Berkeley Symposium on Mathematics, Statistics and Probability, </booktitle> <volume> Vol. 1, </volume> <pages> pages 281-296. </pages>
Reference-contexts: We prove however in this paper that there exist a much better choice of learning rates. 3 K-MEANS AS AN EM STYLE ALGORITHM 3.1 EM STYLE ALGORITHM The following derivation of K-Means is similar to the derivation of <ref> (MacQueen, 1967) </ref>. We insist however on the identity between this derivation and the mathematics of EM (Liporace, 1976) (Dempster, Laird and Rubin, 1977).
Reference: <author> Nowlan, S. J. </author> <year> (1991). </year> <title> Soft Competitive Adaptation: Neural Network Learning Algorithms based on Fitting Statistical Mixtures. </title> <institution> CMU-CS-91-126, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA. </address>
Reference-contexts: This motivated us to better understand this convergence speed. A second reason lies in the traditional debate between hard threshold (e.g. K-Means, Viterbi Training) and soft threshold (e.g. Gaussian Mixtures, Baum Welch) algorithms <ref> (Nowlan, 1991) </ref>. Soft threshold algorithms are often preferred because they have an elegant probabilistic framework and a general optimization algorithm named EM (expectation-maximization) (Dempster, Laird and Rubin, 1977).
Reference: <author> Xu, L. and Jordan, M. </author> <year> (1994). </year> <title> Theoretical and experimental studies of convergence properties of the em algorithm for unsupervised learning based on finite mixtures. </title> <booktitle> Presented at the Neural Networks for Computing Conference. </booktitle>
Reference-contexts: Soft threshold algorithms are often preferred because they have an elegant probabilistic framework and a general optimization algorithm named EM (expectation-maximization) (Dempster, Laird and Rubin, 1977). In the case of a gaussian mixture, the EM algorithm has recently been shown to approximate the Newton optimization algorithm <ref> (Xu and Jordan, 1994) </ref>. We prove in this fl also, AT&T Bell Labs, Holmdel, NJ 07733 paper that the corresponding hard threshold algorithm, K-Means, minimizes the quantization error using exactly the Newton algorithm. In the next section, we derive the K-Means algorithm as a gradient descent procedure.
References-found: 11

