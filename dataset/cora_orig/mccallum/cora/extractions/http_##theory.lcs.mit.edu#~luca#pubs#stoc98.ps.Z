URL: http://theory.lcs.mit.edu/~luca/pubs/stoc98.ps.Z
Refering-URL: http://theory.lcs.mit.edu/~luca/papers.html
Root-URL: 
Email: luca@theory.lcs.mit.edu.  
Title: Recycling Queries in PCPs and in Linearity Tests [Extended Abstract]  
Author: LUCA TREVISAN 
Note: and has error probability 1=4, so that its amortized query complexity is 2:5.  
Address: Room NE43-371, 545 Technology Square, Cambridge, MA 02139, USA.  
Affiliation: MIT Laboratory for Computer Science,  
Abstract: We study query-efficient Probabilistically Checkable Proofs (PCPs) and linearity tests. We focus on the number of amortized query bits. A testing algorithm uses q amortized query bits if, for some constant k, it reads qk bits and has error probability at most 2 k . The best known PCP construction for NP in this respect uses 3 amortized query bits [13]; at least one amortized query bit is necessary, unless P = NP [5]. This parameter is a fairly natural one and has applications to proving non-approximability results for constraint satisfaction problems. Furthermore, a PCP characterization of NP with less than 2 amortized query bits implies a separation of the PCP model from the 2-Prover 1-Round model. Our approach is to take an atomic verification procedure and then iterate it several times, saving queries by recycling them between different iterations of the atomic test. We first apply this idea in order to develop query-efficient linearity tests. Linearity testing is a problem closely related to testing the Long Code and making PCP constructions. It is also a significant combinatorial problem still lacking tight characterizations, except for the case of three queries [4]. The best known linearity test uses 3 amortized query bits [4]; a different one achieves 1 amortized free bit (a different parameter related to the Max Clique problem) but uses an unbounded number of amortized query bits [5]. We develop a general analysis technique and a linearity test achieving simultaneously amortized query complexity 1:5 and amortized free bit complexity :5. This test answers an open question raised by Bellare, Goldreich and Sudan. We then show how to adapt a weaker result to the PCP setting, and we obtain a PCP for NP that makes 5 queries 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Arora, C. Lund, R. Motwani, M. Sudan, and M. Szegedy. </author> <title> Proof verification and hardness of approximation problems. </title> <booktitle> In Proc. of the 33rd IEEE FOCS, </booktitle> <pages> pages 1423, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The PCP characterization of NP <ref> [2, 1] </ref>, or the PCP Theorem, is a major achievement of complexity theory and a powerful tool for proving hardness of approximation for optimization problems. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> Note that since the outer verifier given in Raz [19] is essentially the best possible, we only have to care about the inner one, i.e. on the definition of the code and on checking codewords and consistency. THE LONG CODE. A first series of works <ref> [1, 6, 7] </ref> developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in [5, 11, 13].
Reference: [2] <author> S. Arora and S. Safra. </author> <title> Probabilistic checking of proofs; a new characterization of NP. </title> <booktitle> In Proc. of the 33rd IEEE FOCS, </booktitle> <pages> pages 213, </pages> <year> 1992. </year>
Reference-contexts: 1 Introduction The PCP characterization of NP <ref> [2, 1] </ref>, or the PCP Theorem, is a major achievement of complexity theory and a powerful tool for proving hardness of approximation for optimization problems. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in <ref> [2] </ref> and then used e.g. in [1, 6, 7, 5, 11, 12, 13].
Reference: [3] <author> M. Bellare. </author> <title> Proof checking and approximation: Towards tight results. </title> <journal> Sigact News, </journal> <volume> 27(1), </volume> <year> 1996. </year>
Reference-contexts: PCP PARAMETERS. Four main parameters have been considered for their applications to proving hardness of approximation: the number of query bits, the number of free bits and their amortized versions (see also <ref> [3] </ref> for a survey on recent PCP results and for the role of different parameters.) The number of query bits is the number of bits of the proof that are accessed by the verifier.
Reference: [4] <author> M. Bellare, D. Coppersmith, J. Hastad, M. Kiwi, and M. Sudan. </author> <title> Linearity testing over characteristic two. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 42(6):17811795, </volume> <year> 1996. </year>
Reference-contexts: the Long Code: in [5] the linearity testing was a subroutine of the Long Code test; in [13] the analysis of the Long Code is based on a perturbed linearity test and its analysis follows the lines (alas with several complications) of the analysis of the linearity test given in <ref> [4] </ref>. Thus, it appears that techniques and ideas developed to test linearity are, with appropriate extensions, useful in testing the Long Code and eventually getting PCP constructions. This motivates us to get, for starters, a good linearity tester with low amortized query complexity. <p> The linearity test of Blum, Luby and Rubinfeld [8] (henceforth called the BLR test) makes three queries: it picks random a; b 2 f0; 1g n and accepts iff f (a) f (b) = f (a b). The BLR analysis has been improved in <ref> [9, 4, 17] </ref>; in [4] it has been shown that the acceptance probability of this test is at most 1 Dist (f; LIN), that is 1=2 + " when f is at distance 1=2 " from the set of linear functions. <p> The linearity test of Blum, Luby and Rubinfeld [8] (henceforth called the BLR test) makes three queries: it picks random a; b 2 f0; 1g n and accepts iff f (a) f (b) = f (a b). The BLR analysis has been improved in [9, 4, 17]; in <ref> [4] </ref> it has been shown that the acceptance probability of this test is at most 1 Dist (f; LIN), that is 1=2 + " when f is at distance 1=2 " from the set of linear functions. Thus, the BLR test has amortized query complexity essentially 3. <p> We then use Fourier analysis to obtain tight bounds on the error probability of three infinite families of tests. The use of Fourier analysis to study linearity tests was introduced in <ref> [4] </ref>. Hastad elected it to a fine art in his papers on testing the Long Code and on improved PCP constructions [11, 13]. Our results are as follows (k 1 is any fixed positive integer): 1.
Reference: [5] <author> M. Bellare, O. Goldreich, and M. Sudan. </author> <title> Free bits, PCP's and non-approximability towards tight results (5th version). Technical Report TR95-24, </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <year> 1997. </year> <note> Preliminary version in Proc. of FOCS'95. </note>
Reference-contexts: 1 In some cases, including the construction presented in this paper, there is the less restrictive assumption that correct proofs are accepted with probability at least 1 " where " can be made arbitrarily small independently of the other parameters of interest. 2 A more general definition is given in <ref> [5] </ref>. say that a verifier uses q amortized query bits if, for some constant k, it makes qk queries and has error probability at most 2 k ; amortized free bits are defined similarly. The notion of amortization is due to [7, 5]. <p> The notion of amortization is due to <ref> [7, 5] </ref>. <p> Thus, a characterization of NP in terms of a PCP with 1:99 (say) amortized query bits implies a separation between the PCP and the MIP model, a question asked in [6] and <ref> [5] </ref>. The best previous result was a characterization of NP with 3 + " amortized query bits [13]. There cannot be a characterization with 1 amortized query bit unless P = NP [5, 22]. <p> The best previous result was a characterization of NP with 3 + " amortized query bits [13]. There cannot be a characterization with 1 amortized query bit unless P = NP <ref> [5, 22] </ref>. We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. In the most recent constructions <ref> [5, 12, 13] </ref> one starts with an outer protocol given by Raz [19], a 2-Prover 1-Round protocol with perfect completeness, constant answer size and small soundness, and then one tries to improve on the parameter of interest, in our case the number of amortized query bits. <p> THE LONG CODE. A first series of works [1, 6, 7] developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in <ref> [5] </ref> and used in [5, 11, 13]. <p> THE LONG CODE. A first series of works [1, 6, 7] developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in <ref> [5, 11, 13] </ref>. <p> functions f : f0; 1g n ! f0; 1g, and such that A (f ) = f (a) for any function f (we give an alternate equivalent definition in Section 2.) It should be noted that the linearity test is still relevant to the analysis of the Long Code: in <ref> [5] </ref> the linearity testing was a subroutine of the Long Code test; in [13] the analysis of the Long Code is based on a perturbed linearity test and its analysis follows the lines (alas with several complications) of the analysis of the linearity test given in [4]. <p> The error probability of the test is the probability that it accepts a function that is far; clearly this definition depends on what do we mean by far. Bellare, Goldreich and Sudan <ref> [5] </ref> call codeword test the problem where far functions are at distance at least 1=4 from the code. This definition is motivated by the fact that non-far functions have unique decoding. <p> This definition is motivated by the fact that non-far functions have unique decoding. Defining far to be the set of functions at distance at least 1=2 " from the set of linear functions is still good (this is called relaxed codeword testing in <ref> [5] </ref>) since a function that is at distance at most 1=2 " from some linear func 3 In fact, it is a first-order Reed-Muller Code. tion is at this distance from at most 1=4" 2 other linear func-tions, so we can still use bounded distance decoding. <p> Thus, the BLR test has amortized query complexity essentially 3. No better test is known with respect to amortized query complexity. With respect to amortized free bit complexity, in <ref> [5] </ref> there is a test that has amortized free bit complexity essentially 1 and that distinguishes linear functions from functions at distance 1=4 from the set of linear functions. There is a lower bound showing that this result is tight. <p> The amortized query complexity is roughly 1:5 and the amor tized free bit complexity is roughly :5, beating the lower bound of <ref> [5] </ref>. The only tool of Fourier analysis that we use is Parseval's equality. In a sense that can be made formal, 1:5 amortized query bits are a natural limit to the techniques developed in this paper (this is discussed in Section 6.) PCP: OUR RESULTS. <p> LONG n = fl fag : a 2 [n]g. We say that l fag is the Long Code of a. Thus, the Long Code is formed by n codewords of length 2 n . An alternate but equivalent definition was used in <ref> [5] </ref>. We think that our definition makes more explicit the relation between linear functions and the Long Code. We give the definition of PCP parameters and classes. We follow the notation of [6, 5]. <p> An alternate but equivalent definition was used in [5]. We think that our definition makes more explicit the relation between linear functions and the Long Code. We give the definition of PCP parameters and classes. We follow the notation of <ref> [6, 5] </ref>. Definition 2 (Verifier) A verifier V for a language L is a probabilistic polynomial time algorithm that receives an input x and has oracle access to a string P , that is supposed to represent a proof of the statement x 2 L. <p> To this aim, we first extend the framework of proof composition of <ref> [5] </ref>. Our definition of inner verifier and of decoding procedure is general enough to capture (some of) the recent results of Hastad, is fairly compact and conceptually simpler than previous similar definitions. 5.1 The General Framework We first need a definition analogous to that of folding from [5]. <p> proof composition of <ref> [5] </ref>. Our definition of inner verifier and of decoding procedure is general enough to capture (some of) the recent results of Hastad, is fairly compact and conceptually simpler than previous similar definitions. 5.1 The General Framework We first need a definition analogous to that of folding from [5]. Observe that if A = l fag is a Long Code word, then A (x) = A (x) for any x; for any function A : f1; 1g n ! f1; 1g we will define a new function A 0 that satisfies such a property. <p> A proof of Theorem 16 can be found in a preliminary full version of this paper [24]. It differs from a similar proof in <ref> [5] </ref> since, in our definition of inner verifier, we allow randomized decoding procedures (which is easy to deal with) and we do not have a circuit test (this requires some more care). <p> 0 and B 0 we deduce that there exists a ffi 0 (depending only on ffi and on ") such that Pr [D 1 (A 0 ) = (D 2 (B 0 ))] ffi 0 : Theorem 20 For any " &gt; 0, NP = naPCP 1"; 1 4 +" <ref> [log; 5] </ref>. <p> 1 and A (x 2 )B (y)B ((x 2 ffi )ye 2 ) = 1 then accept else reject X ^ A ff 1 ^ B fi 1 x 1 x 2 y e 1 e 2 As observed by Oded Goldreich, one can also derive NP = naPCP 1";1=4 <ref> [log; 5] </ref> by letting the verifier fail uncondition ally with probability 1 (1=4)= 1 and proceed normally otherwise.
Reference: [6] <author> M Bellare, S. Goldwasser, C. Lund, and A. Russell. </author> <title> Efficient probabilistically checkable proofs and applications to approximation. </title> <booktitle> In Proc. of the 25th ACM STOC, </booktitle> <pages> pages 294304, </pages> <year> 1993. </year> <note> See also the errata sheet in Proc of STOC'94. </note>
Reference-contexts: Thus, a characterization of NP in terms of a PCP with 1:99 (say) amortized query bits implies a separation between the PCP and the MIP model, a question asked in <ref> [6] </ref> and [5]. The best previous result was a characterization of NP with 3 + " amortized query bits [13]. There cannot be a characterization with 1 amortized query bit unless P = NP [5, 22]. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> Note that since the outer verifier given in Raz [19] is essentially the best possible, we only have to care about the inner one, i.e. on the definition of the code and on checking codewords and consistency. THE LONG CODE. A first series of works <ref> [1, 6, 7] </ref> developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in [5, 11, 13]. <p> An alternate but equivalent definition was used in [5]. We think that our definition makes more explicit the relation between linear functions and the Long Code. We give the definition of PCP parameters and classes. We follow the notation of <ref> [6, 5] </ref>. Definition 2 (Verifier) A verifier V for a language L is a probabilistic polynomial time algorithm that receives an input x and has oracle access to a string P , that is supposed to represent a proof of the statement x 2 L.
Reference: [7] <author> M. Bellare and M. Sudan. </author> <title> Improved non-approximability results. </title> <booktitle> In Proc. of the 26th ACM STOC, </booktitle> <pages> pages 184193, </pages> <year> 1994. </year>
Reference-contexts: The notion of amortization is due to <ref> [7, 5] </ref>. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> Note that since the outer verifier given in Raz [19] is essentially the best possible, we only have to care about the inner one, i.e. on the definition of the code and on checking codewords and consistency. THE LONG CODE. A first series of works <ref> [1, 6, 7] </ref> developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in [5, 11, 13].
Reference: [8] <author> M. Blum, M. Luby, and R. Rubinfeld. </author> <title> Self-testing/correcting with applications to numerical problems. </title> <booktitle> In Proc. of the 22nd ACM STOC, </booktitle> <pages> pages 7383, </pages> <year> 1990. </year>
Reference-contexts: THE LONG CODE. A first series of works [1, 6, 7] developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld <ref> [8] </ref>. More recently, the Long Code was introduced in [5] and used in [5, 11, 13]. <p> This is the notion of relaxed codeword test that we adopt in this paper. The linearity test of Blum, Luby and Rubinfeld <ref> [8] </ref> (henceforth called the BLR test) makes three queries: it picks random a; b 2 f0; 1g n and accepts iff f (a) f (b) = f (a b).
Reference: [9] <author> D. Coppersmith. </author> <note> Unpublished notes, </note> <year> 1990. </year>
Reference-contexts: The linearity test of Blum, Luby and Rubinfeld [8] (henceforth called the BLR test) makes three queries: it picks random a; b 2 f0; 1g n and accepts iff f (a) f (b) = f (a b). The BLR analysis has been improved in <ref> [9, 4, 17] </ref>; in [4] it has been shown that the acceptance probability of this test is at most 1 Dist (f; LIN), that is 1=2 + " when f is at distance 1=2 " from the set of linear functions.
Reference: [10] <author> N. Creignou. </author> <title> A dichotomy theorem for maximum generalized satisfiability problems. </title> <journal> JCSS, </journal> <volume> 51(3):511522, </volume> <year> 1995. </year>
Reference-contexts: Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13].
Reference: [11] <author> J. Hastad. </author> <title> Testing of the long code and hardness for clique. </title> <booktitle> In Proc. of the 28th ACM STOC, </booktitle> <pages> pages 1119, </pages> <year> 1996. </year>
Reference-contexts: We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> THE LONG CODE. A first series of works [1, 6, 7] developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in <ref> [5, 11, 13] </ref>. <p> The use of Fourier analysis to study linearity tests was introduced in [4]. Hastad elected it to a fine art in his papers on testing the Long Code and on improved PCP constructions <ref> [11, 13] </ref>. Our results are as follows (k 1 is any fixed positive integer): 1. The test associated with a path of length k has accep tance probability at most (1 Dist (f; LIN [ LIN)) k with 2k + 1 query bits and k + 1 free bits.
Reference: [12] <author> J. Hastad. </author> <title> Clique is hard to approximate within n 1" . Technical Report TR97-38, </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <year> 1997. </year> <note> Preliminary version in Proc. of FOCS'96. </note>
Reference-contexts: The amortized free bit complexity parameter is related to the hardness of approximating Max Clique; in this case a tight result is known, due to Hastad <ref> [12] </ref>, showing that NP can be characterized with " amortized free bits for any " &gt; 0. The amortized query complexity parameter is related to the approximability of constraint satisfaction problems described below. A tight result is not known. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. In the most recent constructions <ref> [5, 12, 13] </ref> one starts with an outer protocol given by Raz [19], a 2-Prover 1-Round protocol with perfect completeness, constant answer size and small soundness, and then one tries to improve on the parameter of interest, in our case the number of amortized query bits. <p> In order to do better, it is necessary to consider inner verifiers that look at a larger number of tables, and that try to check some weak form of consistency between the ta bles. This approach was originally used by Hastad <ref> [12] </ref> to obtain his tight result for the Max Clique problem. It remains an open question whether it is possible to use less than 1:5 amortized query bits either for the Linearity Test problem or in PCP constructions.
Reference: [13] <author> J. Hastad. </author> <title> Some optimal inapproximability results. </title> <booktitle> In Proc. of the 29th ACM STOC, </booktitle> <pages> pages 110, </pages> <year> 1997. </year>
Reference-contexts: The notion of amortization is due to [7, 5]. PCPs with small query complexity (and, subject to that, with low error) have applications to prove hardness of approximation for the Max 3SAT problem; a tight result, due to Hastad <ref> [13] </ref> is known in this context (the tightness is due to [14, 26].) Protocols with small free bit complexity (and, subject to that, with low error) imply hardness of approximation results for the Vertex Cover problem; a tight result is not known and is a major open question. <p> This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k <ref> [13] </ref>. A PCP for NP using q amortized query bits implies the NP-hardness of approximation of Max kCSP within 2 bk=qc " for sufficiently large k. <p> Thus, a characterization of NP in terms of a PCP with 1:99 (say) amortized query bits implies a separation between the PCP and the MIP model, a question asked in [6] and [5]. The best previous result was a characterization of NP with 3 + " amortized query bits <ref> [13] </ref>. There cannot be a characterization with 1 amortized query bit unless P = NP [5, 22]. We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. <p> We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in <ref> [1, 6, 7, 5, 11, 12, 13] </ref>. <p> TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. In the most recent constructions <ref> [5, 12, 13] </ref> one starts with an outer protocol given by Raz [19], a 2-Prover 1-Round protocol with perfect completeness, constant answer size and small soundness, and then one tries to improve on the parameter of interest, in our case the number of amortized query bits. <p> THE LONG CODE. A first series of works [1, 6, 7] developed inner verifiers using the Hadamard Code, and the essential testing problem was the linearity test problem, first investigated by Blum, Luby and Rubinfeld [8]. More recently, the Long Code was introduced in [5] and used in <ref> [5, 11, 13] </ref>. <p> (f ) = f (a) for any function f (we give an alternate equivalent definition in Section 2.) It should be noted that the linearity test is still relevant to the analysis of the Long Code: in [5] the linearity testing was a subroutine of the Long Code test; in <ref> [13] </ref> the analysis of the Long Code is based on a perturbed linearity test and its analysis follows the lines (alas with several complications) of the analysis of the linearity test given in [4]. <p> The use of Fourier analysis to study linearity tests was introduced in [4]. Hastad elected it to a fine art in his papers on testing the Long Code and on improved PCP constructions <ref> [11, 13] </ref>. Our results are as follows (k 1 is any fixed positive integer): 1. The test associated with a path of length k has accep tance probability at most (1 Dist (f; LIN [ LIN)) k with 2k + 1 query bits and k + 1 free bits. <p> All the results that we have for the linearity test extend to the analysis of one Long Codeword, using the random perturbation idea of <ref> [13] </ref>. Unfortunately, it is necessary to look at two alleged Long Codewords in order to build an inner verifier, and there is also a consistency test to be implemented. <p> Using Theorem 16, the goal of developing a PCP con-struction reduces to find an inner verifier and a decoding procedure. We will use a decoding procedure from <ref> [13] </ref> and its Fourier analysis. <p> for any A and B such that ^ A ; = ^ B ; = 0, if X j ^ A 2 (fi) j ^ B 2 fi (1 2") jfij ffi ; then Pr [D H 2 (B))] ffi 0 : A proof of Lemma 17 is given in <ref> [13, Lemma 2.2] </ref>. In [24] we present a slight simplification of the proof of [13] (our decoding procedure is simpler.) Our alternative proof is omitted from this extended abstract. 5.2 Our PCP Construction We now define our inner verifier and analyse it. <p> In [24] we present a slight simplification of the proof of <ref> [13] </ref> (our decoding procedure is simpler.) Our alternative proof is omitted from this extended abstract. 5.2 Our PCP Construction We now define our inner verifier and analyse it. It performs two executions of the protocol of Hastad [13]. One query is recycled between the two executions. <p> In [24] we present a slight simplification of the proof of <ref> [13] </ref> (our decoding procedure is simpler.) Our alternative proof is omitted from this extended abstract. 5.2 Our PCP Construction We now define our inner verifier and analyse it. It performs two executions of the protocol of Hastad [13]. One query is recycled between the two executions. The reader can compare our analysis with the analysis of the linearity test asso ciated with a path of length two (or a star with two rays). The inner verifier is described in Figure 3. <p> [Inner " (A; B; ) accepts ] = 1 1 x 1 ;y;e 1 1 x 2 ;y;e 2 1 x 1 ; x 2 A (x 1 )A (x 2 )B ((x 1 ffi )ye 1 )B ((x 2 ffi )ye 2 ) : It has been proved in <ref> [13] </ref> that E A (x i )B (y)B ((x i ffi )ye i ) X ^ A 2 (fi) ^ B 2 fi (1 2") jfij fi fi (1 2") jfij : It remains to evaluate the last term. <p> Let us first estimate the last two terms. Following <ref> [13] </ref> we have E l fi i (e i ) = b2fi i e i For the other terms, everything cancels except when fi 1 = fi 2 = fi and ff 1 = ff 2 = 2 (fi).
Reference: [14] <author> B. Karloff and U. Zwick. </author> <title> A (7=8 ")-approximation algorithm for MAX 3SAT? In Proc. </title> <booktitle> of the 38th IEEE FOCS, </booktitle> <year> 1997. </year>
Reference-contexts: PCPs with small query complexity (and, subject to that, with low error) have applications to prove hardness of approximation for the Max 3SAT problem; a tight result, due to Hastad [13] is known in this context (the tightness is due to <ref> [14, 26] </ref>.) Protocols with small free bit complexity (and, subject to that, with low error) imply hardness of approximation results for the Vertex Cover problem; a tight result is not known and is a major open question.
Reference: [15] <author> S. Khanna, R. Motwani, M. Sudan, and U. Vazirani. </author> <title> On syntactic versus computational views of approximabil-ity. </title> <booktitle> In Proc. of the 35th IEEE FOCS, </booktitle> <pages> pages 819830, </pages> <year> 1994. </year>
Reference-contexts: A tight result is not known. Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in <ref> [15] </ref> and then studied e.g. in [10, 22, 23, 25, 16, 26].
Reference: [16] <author> S. Khanna, M. Sudan, and D.P. Williamson. </author> <title> A complete classification of the approximability of maximization problems derived from boolean constraint satisfaction. </title> <booktitle> In Proc. of the 29th ACM STOC, </booktitle> <pages> pages 1120, </pages> <year> 1997. </year>
Reference-contexts: Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13].
Reference: [17] <author> M. Kiwi. </author> <title> Probabilistically Checkable Proofs and the Testing of Hadamard-like Codes. </title> <type> PhD thesis, </type> <institution> MIT, </institution> <year> 1996. </year>
Reference-contexts: The linearity test of Blum, Luby and Rubinfeld [8] (henceforth called the BLR test) makes three queries: it picks random a; b 2 f0; 1g n and accepts iff f (a) f (b) = f (a b). The BLR analysis has been improved in <ref> [9, 4, 17] </ref>; in [4] it has been shown that the acceptance probability of this test is at most 1 Dist (f; LIN), that is 1=2 + " when f is at distance 1=2 " from the set of linear functions.
Reference: [18] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> Optimization, approximation, and complexity classes. </title> <journal> JCSS, </journal> <volume> 43:425 440, </volume> <year> 1991. </year> <note> Preliminary version in Proc. of STOC'88. </note>
Reference-contexts: A tight result is not known. Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in <ref> [18] </ref>, has been named in [15] and then studied e.g. in [10, 22, 23, 25, 16, 26].
Reference: [19] <author> R. Raz. </author> <title> A parallel repetition theorem. </title> <booktitle> In Proc. of the 27th ACM STOC, </booktitle> <pages> pages 447456, </pages> <year> 1995. </year>
Reference-contexts: TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. In the most recent constructions [5, 12, 13] one starts with an outer protocol given by Raz <ref> [19] </ref>, a 2-Prover 1-Round protocol with perfect completeness, constant answer size and small soundness, and then one tries to improve on the parameter of interest, in our case the number of amortized query bits. <p> Note that since the outer verifier given in Raz <ref> [19] </ref> is essentially the best possible, we only have to care about the inner one, i.e. on the definition of the code and on checking codewords and consistency. THE LONG CODE.
Reference: [20] <author> M. Serna, L. Trevisan, and F. Xhafa. </author> <title> The parallel approximability of non-boolean constraint satisfaction and restricted integer linear programming. </title> <booktitle> In Proc. of the 15th STACS, </booktitle> <year> 1998. </year> <note> To appear. </note>
Reference-contexts: motivation for the study of PCPs with low amortized query complexity is the fact that NP cannot be characterized with 2-Provers 1-Round having amortized query complexity smaller than 2 (we take the query complexity of a 2-Prover 1-Round protocol as the sum of the answer sizes) unless P = NP <ref> [20] </ref>. Thus, a characterization of NP in terms of a PCP with 1:99 (say) amortized query bits implies a separation between the PCP and the MIP model, a question asked in [6] and [5].
Reference: [21] <author> M. Sudan and L. Trevisan. </author> <note> Unpublished Notes, Febru-ary 1998. </note>
Reference-contexts: A significant progress in the first direction has been recently achieved by Sudan and Trevisan <ref> [21] </ref> who adapted our analysis of the Bipartite-Graph Linearity Tester to the PCP setting, obtaining a PCP characterization of NP with 3k + 2 queries, completeness (1 ") and soundness 2 2k , for any k and " (such a verifier uses, asymptotically, essentially 1:5 amortized query bits, and thus shows
Reference: [22] <author> L. Trevisan. </author> <title> Positive linear programming, parallel approximation, </title> <booktitle> and PCP's. In Proc. of the 4th ESA, </booktitle> <pages> pages 6275. </pages> <publisher> LNCS 1136, Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13]. <p> Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in [10, 22, 23, 25, 16, 26]. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 <ref> [22] </ref>; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13]. A PCP for NP using q amortized query bits implies the NP-hardness of approximation of Max kCSP within 2 bk=qc " for sufficiently large k. <p> The best previous result was a characterization of NP with 3 + " amortized query bits [13]. There cannot be a characterization with 1 amortized query bit unless P = NP <ref> [5, 22] </ref>. We believe that 1 + " is the right answer: our goal is to make progress in this direction. TESTING PROOFS. The standard method of getting good PCPs is by composition, a paradigm introduced in [2] and then used e.g. in [1, 6, 7, 5, 11, 12, 13]. <p> We have the following connection with the Max kCSP problem. Theorem 4 ([1]) If NP = naPCP c;s [log; k] then it is NP-hard to approximate Max kCSP within c s " for any " &gt; 0. In <ref> [22] </ref> Theorem 4 was extended to the case of adaptive queries, we will not need such extension in this paper. 3 Graph-Based Tests and Their Acceptance Probability The BLR test picks x 1 and x 2 independently and uniformly at random and tests whether f (x 1 )f (x 2 )
Reference: [23] <author> L. Trevisan. </author> <title> Approximating satisfiable satisfiability problems. </title> <booktitle> In Proc. of the 5th ESA, </booktitle> <pages> pages 472485. </pages> <publisher> LNCS 1284, Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13].
Reference: [24] <author> L. Trevisan. </author> <title> Recycling queries in PCPs and in linearity tests. Technical Report TR98-07, </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <month> January </month> <year> 1998. </year>
Reference-contexts: A proof of Theorem 16 can be found in a preliminary full version of this paper <ref> [24] </ref>. It differs from a similar proof in [5] since, in our definition of inner verifier, we allow randomized decoding procedures (which is easy to deal with) and we do not have a circuit test (this requires some more care). <p> In <ref> [24] </ref> we present a slight simplification of the proof of [13] (our decoding procedure is simpler.) Our alternative proof is omitted from this extended abstract. 5.2 Our PCP Construction We now define our inner verifier and analyse it. It performs two executions of the protocol of Hastad [13].
Reference: [25] <author> L. Trevisan, G.B. Sorkin, M. Sudan, and D.P. Williamson. Gadgets, </author> <title> approximation, and linear programming. </title> <booktitle> In Proc. of the 37th IEEE FOCS, </booktitle> <pages> pages 617626, </pages> <year> 1996. </year>
Reference-contexts: Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13].
Reference: [26] <author> U. Zwick. </author> <title> Approximation algorithms for constraint satisfaction problems involving at most three variables per constraint. </title> <booktitle> In Proc. of the 9th ACM-SIAM SODA, </booktitle> <year> 1998. </year>
Reference-contexts: PCPs with small query complexity (and, subject to that, with low error) have applications to prove hardness of approximation for the Max 3SAT problem; a tight result, due to Hastad [13] is known in this context (the tightness is due to <ref> [14, 26] </ref>.) Protocols with small free bit complexity (and, subject to that, with low error) imply hardness of approximation results for the Vertex Cover problem; a tight result is not known and is a major open question. <p> Max kCSP (for k-ary Constraint Satisfaction Problem) is the generalization of Max kSAT where each clause is allowed to be an arbitrary predicate over (at most) k boolean variables. Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in <ref> [10, 22, 23, 25, 16, 26] </ref>. This problem is known to be 2-approximable for k = 3 [26] and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13]. <p> Max k CSP was implicit in [18], has been named in [15] and then studied e.g. in [10, 22, 23, 25, 16, 26]. This problem is known to be 2-approximable for k = 3 <ref> [26] </ref> and 2 k1 -approximable for k 4 [22]; on the negative side it is NP-hard to approximate within a factor 2 bk=3c " for any k [13].
References-found: 26

