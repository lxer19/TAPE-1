URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/tr.outbox/MIT-LCS-TR-593.ps.gz
Refering-URL: ftp://ftp-pubs.lcs.mit.edu/pub/lcs-pubs/listings/tr500.html
Root-URL: 
Title: Automatic Language Identification Using a Segment-Based Approach  
Author: by Timothy J. Hazen Victor W. Zue 
Degree: 1991 Submitted to the Department of Electrical Engineering and Computer Science in Partial Fulfillment of the Requirements for the Degree of Master of Science at the  c flMassachusetts Institute of Technology, 1993. All rights reserved. Signature of Author  Certified by  Accepted by Frederic R. Morgenthaler Chair, Department Committee on Graduate Students  
Date: September, 1993  May 9, 1995  
Affiliation: S.B., Massachusetts Institute of Technology,  Massachusetts Institute of Technology  Department of Electrical Engineering and Computer Science  Principal Research Scientist Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Deidre Cimarusti and Russell B. Ives. </author> <title> Development of an automatic identification system of spoken languages: Phase I. </title> <booktitle> In Proceedings of the 1982 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 1661-1663. </pages> <publisher> IEEE, </publisher> <year> 1982. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives <ref> [1] </ref>, Ives [13], Foil [6], Goodman et al. [10], Sugiyama [34], Savic et al. [32] and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion.
Reference: [2] <author> Nancy A. Daly and Victor W. Zue. </author> <title> Acoustic, perceptual, and linguistic analyses of intonation contours in human/machine dialogues. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: It has also been observed that some languages use the F0 contour to represent even higher level linguistic information. The F0 contour of the end of an utterances has been observed to differentiate between declarative statements and yes/no questions in languages such as English, French, Italian, and Japanese <ref> [35, 2] </ref>. In some languages such as English, declarative statements are characterized by a falling F0 contour at the end of an utterance while yes/no questions are characterized with a rising contour.
Reference: [3] <author> Nancy A. Daly and Victor W. Zue. </author> <title> Statistical and linguistic analyses of F 0 in read and spontaneous speech. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: The topic-specific utterances were all spontaneous replies to queries while the unconstrained utterances were not limited in any fashion. In fact, the unconstrained utterances contained examples of both spontaneous and read speech, which are known to be different in their prosodic nature <ref> [3] </ref>. 66 67 68 69 4.5 Performance Over Varying Training Set Sizes varied.
Reference: [4] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year>
Reference-contexts: Determining the Set of Phonetic Classes For an approach which utilizes unsupervised training, an automatic method for determining the set of phonetic elements must be used. One simple means for achieving this is to use an unsupervised clustering algorithm. For this thesis, the k-means clustering algorithm is used <ref> [4] </ref>. The algorithm clusters segments extracted from the training data based on similarity of their acoustic feature vectors. The segment-based acoustic feature vector in this case consists of 14 MFCC values averaged over the length of the segment.
Reference: [5] <author> L. Fissore, P. Laface, and G. Micca. </author> <title> Comparison of discrete and continuous HMMs in a CSR task over the telephone. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 253-256. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: The MFCC signal representation was chosen because it has proven to be an effective representation for speech recognition in various different languages including English [25], Italian <ref> [5] </ref> and Japanese [12]. 3.3.2 Voicing Information For this thesis, the voicing information contained in the vector ~ f is extracted from the acoustic signal with the formant program contained in Entropic's ESPS package.
Reference: [6] <author> Jerry T. </author> <title> Foil. Language identification using noisy speech. </title> <booktitle> In Proceedings of the 1986 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 861-864. </pages> <publisher> IEEE, </publisher> <year> 1986. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives [13], Foil <ref> [6] </ref>, Goodman et al. [10], Sugiyama [34], Savic et al. [32] and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion.
Reference: [7] <author> James Glass, David Goodine, Michael Phillips, Shinsuke Sakai, Stephanie Seneff, and Victor Zue. </author> <title> A bilingual voyager system. </title> <booktitle> In Proceedings of the 1993 Euro-pean Conference on Speech Communication and Technology, </booktitle> <month> September </month> <year> 1993. </year>
Reference-contexts: This system, known as the multi-lingual voyager system, is designed to provide travel information for the city of Cambridge [38, 39, 42]. Voyager currently has the capability to understand queries in either English or Japanese <ref> [7] </ref>, and is being ported to French, Italian and German. Within the multi-lingual voyager domain, ALI can be performed as a two step process. The first step is to perform a fast match to provide an ordered list of possible language candidates.
Reference: [8] <author> James R. Glass. </author> <title> Finding Acoustic Regularities in Speech: Applications to Phonetic Recognition. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1988. </year> <month> 91 </month>
Reference-contexts: Because of the tremendous number of possible segmentations which can exist, it is desirable to limit the segmentation search space to a small subset of likely segmentations. One means of accomplishing this search space reduction is to use a hierarchical segmentation algorithm such as the one developed by Glass <ref> [8, 9] </ref>. In Glass's approach, a dendrogram produced from the spectral information of the signal provides a well organized segmentation search space.
Reference: [9] <author> James R. Glass and Victor W. Zue. </author> <title> Multi-level acoustic segmentation of contin-uous speech. </title> <booktitle> In Proceedings of the 1988 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 429-432. </pages> <publisher> IEEE, </publisher> <year> 1988. </year>
Reference-contexts: Because of the tremendous number of possible segmentations which can exist, it is desirable to limit the segmentation search space to a small subset of likely segmentations. One means of accomplishing this search space reduction is to use a hierarchical segmentation algorithm such as the one developed by Glass <ref> [8, 9] </ref>. In Glass's approach, a dendrogram produced from the spectral information of the signal provides a well organized segmentation search space.
Reference: [10] <author> Fred J. Goodman, Alvin F. Martin, and Robert E. Wohlford. </author> <title> Improved automatic language identification in noisy speech. </title> <booktitle> In Proceedings of the 1989 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 528-531. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives [13], Foil [6], Goodman et al. <ref> [10] </ref>, Sugiyama [34], Savic et al. [32] and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion.
Reference: [11] <author> Arthur S. House and Edward P. Neuburg. </author> <title> Toward automatic identification of the language of an utterance. I. Preliminary methodological considerations. </title> <journal> Journal of the Acoustic Society of America, </journal> <volume> 62(3) </volume> <pages> 708-713, </pages> <month> September </month> <year> 1977. </year>
Reference-contexts: This approach was based on the assumption that certain linguistic events occur more frequently in particular languages and the observed statistics of these events can provide for accurate language identification. A similar approach to ALI was proposed by House and Neuburg <ref> [11] </ref>. Like Leonard and Doddington, they believed language identification could be performed by observing the statistics of the linguistic events present in a speech utterance. More specifically, they believed that languages could be identified based upon the sequential constraints of their phonetic elements.
Reference: [12] <author> Ken-ichi Iso and Takao Watanabe. </author> <title> Large vocabulary speech recognition using neural prediction model. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 57-60. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: The MFCC signal representation was chosen because it has proven to be an effective representation for speech recognition in various different languages including English [25], Italian [5] and Japanese <ref> [12] </ref>. 3.3.2 Voicing Information For this thesis, the voicing information contained in the vector ~ f is extracted from the acoustic signal with the formant program contained in Entropic's ESPS package.
Reference: [13] <author> R. B. Ives. </author> <title> A minimal rule AI expert system for real-time classification of natural spoken languages. </title> <booktitle> In Proceedings of the Second Annual Artificial Intelligence and Advanced Computer Technology Conference, </booktitle> <pages> pages 337-340, </pages> <year> 1986. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives <ref> [13] </ref>, Foil [6], Goodman et al. [10], Sugiyama [34], Savic et al. [32] and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion.
Reference: [14] <author> C. Jankowski, A. Kalyanswamy, S. Basson, and J. Spitz. NTIMIT: </author> <title> A phonetically balanced, continuous speech, telephone bandwidth speech database. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 109-112, </pages> <month> April </month> <year> 1990. </year>
Reference-contexts: One alternative database that could be used is the NTIMIT corpus <ref> [14] </ref>. NTIMIT contains the utterances from the TIMIT corpus passed through a telephone network [17, 43, 44]. The use of the NTIMIT corpus for training could cause problems for the phonetic recognizer for two reasons.
Reference: [15] <author> F. Jelinek. </author> <title> Self-organized language modeling for speech recognition. </title> <editor> In Alex Waibel and Kai-Fu Lee, editors, </editor> <booktitle> Readings in Speech Recognition, chapter 8, </booktitle> <pages> pages 450-506. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: of training speakers per language is altered 48 Interpolated n-gram Modeling One means of reconciling the tradeoff between the advantage of increased detail and the disadvantage of limited training data as the number of phonetic classes is increased is to utilize an interpolation approach for combining the different n-gram models <ref> [15] </ref>. The idea behind interpolation is to utilize the strength of larger n-gram models when sufficient training data is available for specific contexts but to rely more heavily on smaller n-gram models when the training data available for a specific context is limited.
Reference: [16] <author> Lori F. Lamel and Jean-Luc Gauvain. </author> <title> Cross-lingual experiments with phone recognition. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech, and Signal Processing. IEEE, </booktitle> <year> 1993. </year>
Reference-contexts: Instead, they devised an approach where various phonetic and prosodic features were extracted from the segments of the phonetically labeled utterance. A neural network which was trained using these features was then used to perform the language identification. Lamel and Gauvain <ref> [16] </ref> used an approach where a phonetic recognition system was trained separately for each language. The training produced language dependent phone and language models for each language.
Reference: [17] <author> Lori F. Lamel, Robert H. Kassel, and Stephanie Seneff. </author> <title> Speech database development: Design and analysis of the acoustic-phonetic corpus. </title> <booktitle> In Proceedings of the DARPA Speech Recognition Workshop, </booktitle> <pages> pages 100-109, </pages> <year> 1986. </year>
Reference-contexts: One alternative database that could be used is the NTIMIT corpus [14]. NTIMIT contains the utterances from the TIMIT corpus passed through a telephone network <ref> [17, 43, 44] </ref>. The use of the NTIMIT corpus for training could cause problems for the phonetic recognizer for two reasons.
Reference: [18] <author> R. G. Leonard. </author> <title> Language recognition test and evaluation. </title> <type> Technical Report RADC-TR-80-83, </type> <institution> Air Force Rome Air Development Center, </institution> <month> March </month> <year> 1980. </year>
Reference-contexts: None of these approaches attempts to model the speech as a sequence of linguistic events. The earliest published research in ALI in this country was performed by Leonard and Doddington <ref> [18, 19, 20, 21] </ref>. They developed an approach where language identification was performed by identifying sound segments or sequences which are particular or common to specific languages.
Reference: [19] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Technical Report RADC-TR-74-200, </type> <institution> Air Force Rome Air Development Center, </institution> <month> August </month> <year> 1974. </year>
Reference-contexts: None of these approaches attempts to model the speech as a sequence of linguistic events. The earliest published research in ALI in this country was performed by Leonard and Doddington <ref> [18, 19, 20, 21] </ref>. They developed an approach where language identification was performed by identifying sound segments or sequences which are particular or common to specific languages.
Reference: [20] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language identification. </title> <type> Techni--cal Report RADC-TR-75-264, </type> <institution> Air Force Rome Air Development Center, </institution> <month> October </month> <year> 1975. </year>
Reference-contexts: None of these approaches attempts to model the speech as a sequence of linguistic events. The earliest published research in ALI in this country was performed by Leonard and Doddington <ref> [18, 19, 20, 21] </ref>. They developed an approach where language identification was performed by identifying sound segments or sequences which are particular or common to specific languages.
Reference: [21] <author> R. G. Leonard and G. R. Doddington. </author> <title> Automatic language discrimination. </title> <type> Technical Report RADC-TR-78-5, </type> <institution> Air Force Rome Air Development Center, </institution> <month> January </month> <year> 1978. </year>
Reference-contexts: None of these approaches attempts to model the speech as a sequence of linguistic events. The earliest published research in ALI in this country was performed by Leonard and Doddington <ref> [18, 19, 20, 21] </ref>. They developed an approach where language identification was performed by identifying sound segments or sequences which are particular or common to specific languages.
Reference: [22] <author> Hong C. Leung, I. Lee Hetherington, and Victor W. Zue. </author> <title> Speech recognition using stochastic explicit-segment modeling. </title> <booktitle> In Proceedings of the Second European Conference on Speech Communication, </booktitle> <year> 1991. </year>
Reference-contexts: One approach to modeling the probability Pr (S j ~ a; ~ f ) is to model the probability of the existence of the boundaries which define the segmentation. This approach is used in segment-based approaches such as the stochastic explicit-segment modeling approach proposed by Leung et al. <ref> [22] </ref>. Because of the tremendous number of possible segmentations which can exist, it is desirable to limit the segmentation search space to a small subset of likely segmentations.
Reference: [23] <author> K. P. Li and T. J. Edwards. </author> <title> Statistical models for automatic language identification. </title> <booktitle> In Proceedings of the 1980 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 884-887. </pages> <publisher> IEEE, </publisher> <year> 1980. </year>
Reference-contexts: However, a few studies that utilize House and Neuburg's basic premise have been conducted. The work of Li and Edwards <ref> [23] </ref> was the first attempt following the general framework proposed by House and Neuburg to be tested on actual speech data. They designed a frame-based classifier which labeled each frame of an utterance with a broad phonetic class.
Reference: [24] <author> John Makhoul, Salim Roucus, and Herbert Gish. </author> <title> Vector quantization in speech coding. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 73(11) </volume> <pages> 1551-1588, </pages> <month> November </month> <year> 1985. </year>
Reference-contexts: Determining the Best Phonetic String When the k-means algorithm is used to find a codebook of phonetic units, phonetic classification is performed using vector quantization (VQ) <ref> [24] </ref>. The use of VQ provides a simple method for modeling the probability Pr (C j S; ~ a; ~ f ) which is used in determining the string ^ C.
Reference: [25] <author> Helen Mei-Ling Meng. </author> <title> The use of distinctive features for automatic speech recognition. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> May </month> <year> 1991. </year>
Reference-contexts: The MFCC signal representation was chosen because it has proven to be an effective representation for speech recognition in various different languages including English <ref> [25] </ref>, Italian [5] and Japanese [12]. 3.3.2 Voicing Information For this thesis, the voicing information contained in the vector ~ f is extracted from the acoustic signal with the formant program contained in Entropic's ESPS package.
Reference: [26] <author> P. Mermelstein and S. Davis. </author> <title> Comparison of parametric representations for monosyllabic word recognition in continuously spoken sentences. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 28(4), </volume> <month> August </month> <year> 1980. </year>
Reference-contexts: set requirements and receiver-operator characteristics are presented in this thesis in Chapter 4. 26 - Preprocessor waveform languageLanguage Identifier Phonetic Recognizer ^ S; ^ C ~ a; ~ f 3.3 Preprocessing 3.3.1 Spectral Representation For this thesis, the acoustic vector ~ a is represented with mel-frequency scale cepstral coefficients (MFCC's) <ref> [26] </ref>. A set of fourteen MFCC's are computed for each utterance with a frame rate of 200 frames per second, a discrete Fourier transform (DFT) size of 256, and a Hamming window of length 25.6 milliseconds. In addition to the MFCC's, fourteen delta MFCC's are also computed.
Reference: [27] <author> Yeshwant K. Muthusamy, Ronald A. Cole, and Murali Gopalakrishnan. </author> <title> A segment-based approach to automatic language identification. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 353-356. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: Their results showed that the use of an imperfect phonetic recog-nizer for determining the string of broad phonetic classes clearly hurt the ability of the language models to perform highly accurate language identification. A study by Muthusamy and Cole <ref> [27, 28] </ref> also utilized the idea of transforming the input speech into a sequence of broad phonetic classes. However, they did not limit the language identification process to simply building language models for the phonetic class sequence.
Reference: [28] <author> Yeshwant K. Muthusamy, Ronald A. Cole, and Beatrice T. Oshika. </author> <title> Automatic segmentation and identification of ten languages using telephone speech. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <year> 1992. </year>
Reference-contexts: It has been observed that humans often have the ability to identify the language of a spoken utterance even when they have no working knowledge of the vocabulary or syntax of that language <ref> [28] </ref>. As will be discussed in Chapter 2, an investigation into the properties of different languages reveals that languages often differ in their phonological and prosodic characteristics. These characteristics are evident in the waveform of a spoken utterance. <p> Their results showed that the use of an imperfect phonetic recog-nizer for determining the string of broad phonetic classes clearly hurt the ability of the language models to perform highly accurate language identification. A study by Muthusamy and Cole <ref> [27, 28] </ref> also utilized the idea of transforming the input speech into a sequence of broad phonetic classes. However, they did not limit the language identification process to simply building language models for the phonetic class sequence. <p> NIST is currently coordinating a series of ALI evaluations utilizing the OGI corpus to compare the approaches of eleven different research efforts. 1 To date, two studies have published preliminary results using the OGI corpus. These studies were conducted by Muthusamy and Cole <ref> [28] </ref> and by Zissman [36].
Reference: [29] <author> Yeshwant K. Muthusamy, Ronald A. Cole, and Beatrice T. Oshika. </author> <title> The OGI multi-language telephone speech corpus. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <pages> pages 1007-1010, </pages> <year> 1992. </year>
Reference-contexts: Nevertheless, a brief summary of the results that have been published is shown in Table 1.1. It should be mentioned that the Muthusamy and Cole system and the Zissman system were both tested on the OGI Multi-Language Telephone Speech Corpus <ref> [29] </ref>. This is the same corpus that was used for the experiments that are presented in this thesis. 1.3 Thesis Overview The ultimate goal of ALI research is to develop language identification methods which are reliable, computationally efficient, and easily portable to new language sets. <p> Additionally, the system design should also consider the constraints placed on the vocabulary and context of the data, as well as the conditions under which the data set was recorded. For this thesis, the ALI system is evaluated using the OGI Multi-Language Telephone Speech Corpus <ref> [29] </ref>. The corpus was collected at the Oregon Graduate Institute (OGI). 1 It contains utterances collected over the phone lines, at an 8 kHz sampling rate, from callers who were native speakers of one of ten different languages.
Reference: [30] <author> Michael Phillips and Victor Zue. </author> <title> Automatic discovery of acoustic measurements for phonetic classification. </title> <booktitle> In Proceedings of the 1992 International Conference on Spoken Language Processing, </booktitle> <month> October </month> <year> 1992. </year> <month> 93 </month>
Reference-contexts: In fact, the acoustic features that are useful for language identification may be quite different from 84 the features that are useful for phonetic recognition. It has been shown that useful segment-based acoustic measurements for phonetic recognition can be discovered in an automatic fashion <ref> [30] </ref>. It may be possible to automatically discover useful segment-based measurements for language identification in a similar fashion. Thus, future work will include attempting to discover more useful segment-based acoustic features.
Reference: [31] <author> Merritt Ruhlen. </author> <title> A Guide to the Languages of the World. </title> <institution> Stanford University, </institution> <year> 1976. </year>
Reference-contexts: factors include the phone set, the phonotactic constraints and the acoustic realizations of particular phones within a language. 1 Because each language uses only a small subset of phones from the set of all possible speech sounds which exist, variances can be observed across the phone sets of different languages <ref> [31] </ref>. Thus, a knowledge of the phones used in particular languages may be enough to help distinguish one language from another. Even if languages contain nearly identical phone sets, the languages may still be distinguishable by the probability distribution of the phones across each language. <p> For fixed stress languages, such as Polish, the stress pattern is dependent only on the number of syllables present in each word. Thus, two words with the same number of syllables will always have the same stress pattern <ref> [31] </ref>. The exact manner in which F0, duration and intensity contribute to the stress of a syllable may also differ from language to language. For example, the timing of rises or falls of the F0 contour in relation to the placement of stressed syllables can vary. <p> This will involve a careful study of the tradeoff between the design's computational efficiency and its language identification accuracy. 86 Appendix A Families of OGI Languages Figure A.1 shows a tree describing the ten languages in the OGI corpus in terms of their linguistic origins <ref> [31] </ref>. It should be noted that the structure of the tree in Figure A.1 is derived from only one of many different hypotheses that linguists have proposed to describe the development of the different languages of the world. <p> The phones are written using the standard International Phonetic Association (IPA) alphabet. The table is created from the language specific phonetic lists compiled by Ruhlen <ref> [31] </ref>. These lists include all of the primary realizations of the phonemes of the language. However, as Ruhlen states, the lists do not always contain context specific allophones.
Reference: [32] <author> Michael Savic, Elena Acosta, and Sunil K. Gupta. </author> <title> An automatic language identification system. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 817-820. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives [13], Foil [6], Goodman et al. [10], Sugiyama [34], Savic et al. <ref> [32] </ref> and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion. Thus, none of these studies used any prior knowledge of the underlying phonetic or prosodic structure of their data. <p> It should be noted that the expression in (2.6) can be easily simplified to form the probabilistic description of a hidden Markov model (HMM) approach. The HMM approach has been widely used for many speech recognition related problems including ALI <ref> [32, 36] </ref>. The HMM approach can be formulated by applying the following assumptions: 1. ~ f is independent of ~ a and C. 2. The frames of ~ a are independent. 3. C is a Markovian sequence.
Reference: [33] <author> B. G. Secrest and G. R. Doddington. </author> <title> An integrated pitch tracking algorithm for speech systems. </title> <booktitle> In Proceedings of the 1983 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 1352-1355. </pages> <publisher> IEEE, </publisher> <year> 1983. </year>
Reference-contexts: The fundamental frequency tracker contained in the formant program is based on an algorithm devised by Secrest and Doddington <ref> [33] </ref>. The frame rate for ~ f is also 200 frames per second. For each frame, a fundamental frequency (F0) and a probability of voicing parameter are estimated. In an attempt to eliminate speaker dependencies a two step transformation is applied to the F0 values.
Reference: [34] <author> Masahide Sugiyama. </author> <title> Automatic language recognition using acoustic features. </title> <booktitle> In Proceedings of the 1991 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 813-816. </pages> <publisher> IEEE, </publisher> <year> 1991. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives [13], Foil [6], Goodman et al. [10], Sugiyama <ref> [34] </ref>, Savic et al. [32] and Zissman [36] are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion.
Reference: [35] <author> Jacqueline Vaissiere. </author> <title> Language-independent prosodic features. </title> <editor> In Anne Cutler and D. Robert Ladd, editors, </editor> <title> Prosody: Models and Measurements, </title> <booktitle> chapter 5, </booktitle> <pages> pages 53-66. </pages> <publisher> Springer-Verlag, </publisher> <year> 1983. </year>
Reference-contexts: In languages that incorporate the concept of word stress, the intensity, duration, and F0 contour of a syllable are all correlated with the inherent stress being placed on that particular syllable <ref> [35] </ref>. Different languages use stress in different manners. For free stress languages, such as English, the stress pattern of words can vary between words with the same number of syllables. <p> For example, the timing of rises or falls of the F0 contour in relation to the placement of stressed syllables can vary. Some languages use a rising F0 at the beginning of a stressed syllable while others use a rising F0 at the end of a stressed syllable <ref> [35] </ref>. It has also been observed that some languages use the F0 contour to represent even higher level linguistic information. <p> It has also been observed that some languages use the F0 contour to represent even higher level linguistic information. The F0 contour of the end of an utterances has been observed to differentiate between declarative statements and yes/no questions in languages such as English, French, Italian, and Japanese <ref> [35, 2] </ref>. In some languages such as English, declarative statements are characterized by a falling F0 contour at the end of an utterance while yes/no questions are characterized with a rising contour. <p> Lengthening of the final vowel in a sentence is a readily observable characteristic of spoken utterances in English, French, German and Italian. However, other languages such as Finnish, Estonian, and Japanese have been observed to contain little to no sentence-final lengthening of vowels <ref> [35] </ref>. 17 2.2 Probabilistic Framework 2.2.1 Maximum A Posteriori Probability Approach General Derivation Before designing any system, it is desirable to develop a strong theoretical framework on which the design can be based. For this thesis the framework will be probabilistic in nature.
Reference: [36] <author> Marc A. Zissman. </author> <title> Automatic language identification using Gaussian mixture and hidden Markov models. </title> <booktitle> In Proceedings of the 1993 International Conference on Acoustics, Speech, and Signal Processing, </booktitle> <pages> pages 399-402. </pages> <publisher> IEEE, </publisher> <year> 1993. </year>
Reference-contexts: These methods use acoustic models to identify the language of a spoken utterance based on the frame by frame statistics of the utterance's acoustic features. The studies by Cimarusti and Ives [1], Ives [13], Foil [6], Goodman et al. [10], Sugiyama [34], Savic et al. [32] and Zissman <ref> [36] </ref> are similar in that each used a frame-based language identification algorithm which was trained on acoustic features of the speech signal in an unsupervised fashion. Thus, none of these studies used any prior knowledge of the underlying phonetic or prosodic structure of their data. <p> It should be noted that the expression in (2.6) can be easily simplified to form the probabilistic description of a hidden Markov model (HMM) approach. The HMM approach has been widely used for many speech recognition related problems including ALI <ref> [32, 36] </ref>. The HMM approach can be formulated by applying the following assumptions: 1. ~ f is independent of ~ a and C. 2. The frames of ~ a are independent. 3. C is a Markovian sequence. <p> NIST is currently coordinating a series of ALI evaluations utilizing the OGI corpus to compare the approaches of eleven different research efforts. 1 To date, two studies have published preliminary results using the OGI corpus. These studies were conducted by Muthusamy and Cole [28] and by Zissman <ref> [36] </ref>.
Reference: [37] <author> Victor Zue, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. </author> <title> Recent progress on the summit system. </title> <booktitle> In Proceedings of the Third DARPA Speech and Natural Language Workshop, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: The phoneme likelihoods are obtained from mixture Gaussian density functions for each phoneme which model segment-based feature vectors. A search algorithm is applied to the segmentation and phoneme search space to find the most likely strings of phonemes. A more detailed description of the summit system is provided in <ref> [37] </ref>, [40] and [41]. For this thesis, summit will be used as the phonetic recognition component of the ALI system. To accomplish this, summit was trained in a fully supervised fashion using the NTIMIT corpus. On NTIMIT, summit achieved a phonetic recognition accuracy of 60.5 %.
Reference: [38] <author> Victor Zue, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. </author> <title> The voyager speech understanding system: A progress report. </title> <booktitle> In Proceedings of the Second DARPA Speech and Natural Language Workshop, </booktitle> <month> October </month> <year> 1989. </year>
Reference-contexts: As a testbed for multi-lingual research, a multi-lingual information retrieval system is currently under development in the Spoken Language 85 Systems group at MIT. This system, known as the multi-lingual voyager system, is designed to provide travel information for the city of Cambridge <ref> [38, 39, 42] </ref>. Voyager currently has the capability to understand queries in either English or Japanese [7], and is being ported to French, Italian and German. Within the multi-lingual voyager domain, ALI can be performed as a two step process.
Reference: [39] <author> Victor Zue, James Glass, David Goodine, Hong Leung, Michael Phillips, Joseph Polifroni, and Stephanie Seneff. </author> <title> The voyager speech understanding system: Preliminary development and evaluation. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <month> April </month> <year> 1990. </year>
Reference-contexts: As a testbed for multi-lingual research, a multi-lingual information retrieval system is currently under development in the Spoken Language 85 Systems group at MIT. This system, known as the multi-lingual voyager system, is designed to provide travel information for the city of Cambridge <ref> [38, 39, 42] </ref>. Voyager currently has the capability to understand queries in either English or Japanese [7], and is being ported to French, Italian and German. Within the multi-lingual voyager domain, ALI can be performed as a two step process.
Reference: [40] <author> Victor Zue, James Glass, David Goodine, Michael Phillips, and Stephanie Sen-eff. </author> <title> The summit speech recognition system: Phonological modeling and lexical access. </title> <booktitle> In Proceedings of the 1990 International Conference on Acoustics, Speech and Signal Processing, </booktitle> <month> April </month> <year> 1990. </year> <month> 94 </month>
Reference-contexts: A search algorithm is applied to the segmentation and phoneme search space to find the most likely strings of phonemes. A more detailed description of the summit system is provided in [37], <ref> [40] </ref> and [41]. For this thesis, summit will be used as the phonetic recognition component of the ALI system. To accomplish this, summit was trained in a fully supervised fashion using the NTIMIT corpus. On NTIMIT, summit achieved a phonetic recognition accuracy of 60.5 %.
Reference: [41] <author> Victor Zue, James Glass, Michael Phillips, and Stephanie Seneff. </author> <title> The MIT summit speech recognition system: A progress report. </title> <booktitle> In Proceedings of the DARPA Speech and Natural Language Workshop, </booktitle> <month> February </month> <year> 1989. </year>
Reference-contexts: A search algorithm is applied to the segmentation and phoneme search space to find the most likely strings of phonemes. A more detailed description of the summit system is provided in [37], [40] and <ref> [41] </ref>. For this thesis, summit will be used as the phonetic recognition component of the ALI system. To accomplish this, summit was trained in a fully supervised fashion using the NTIMIT corpus. On NTIMIT, summit achieved a phonetic recognition accuracy of 60.5 %.
Reference: [42] <author> Victor W. Zue, James R. Glass, Dave Goddeau, David Goodine, Hong C. Leung, Michael K. McCandless, Michael S. Phillips, Joseph Polifroni, Stephanie Seneff, and Dave Whitney. </author> <title> Recent progress on the MIT voyager spoken language system. </title> <booktitle> In Proceedings of the 1990 International Conference on Spoken Language Processing, </booktitle> <month> November </month> <year> 1990. </year>
Reference-contexts: As a testbed for multi-lingual research, a multi-lingual information retrieval system is currently under development in the Spoken Language 85 Systems group at MIT. This system, known as the multi-lingual voyager system, is designed to provide travel information for the city of Cambridge <ref> [38, 39, 42] </ref>. Voyager currently has the capability to understand queries in either English or Japanese [7], and is being ported to French, Italian and German. Within the multi-lingual voyager domain, ALI can be performed as a two step process.
Reference: [43] <author> Victor. W. Zue and Stephanie Seneff. </author> <title> Transcription and alignment of the TIMIT Database. </title> <booktitle> In Proceedings of the Second Meeting on Advanced Man-Machine Interface through Spoken Language, </booktitle> <year> 1988. </year>
Reference-contexts: One alternative database that could be used is the NTIMIT corpus [14]. NTIMIT contains the utterances from the TIMIT corpus passed through a telephone network <ref> [17, 43, 44] </ref>. The use of the NTIMIT corpus for training could cause problems for the phonetic recognizer for two reasons.
Reference: [44] <author> Victor W. Zue, Stephanie Seneff, and James R. Glass. </author> <title> Speech database development at MIT: TIMIT and beyond. </title> <journal> Speech Communication, </journal> <volume> 9(4) </volume> <pages> 351-356, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: One alternative database that could be used is the NTIMIT corpus [14]. NTIMIT contains the utterances from the TIMIT corpus passed through a telephone network <ref> [17, 43, 44] </ref>. The use of the NTIMIT corpus for training could cause problems for the phonetic recognizer for two reasons.
References-found: 44

