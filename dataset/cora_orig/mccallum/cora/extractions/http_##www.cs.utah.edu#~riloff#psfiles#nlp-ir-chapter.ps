URL: http://www.cs.utah.edu/~riloff/psfiles/nlp-ir-chapter.ps
Refering-URL: http://www.cs.utah.edu/~riloff/publications.html
Root-URL: 
Email: friloff,jlorenzg@cs.utah.edu  
Title: EXTRACTION-BASED TEXT CATEGORIZATION: GENERATING DOMAIN-SPECIFIC ROLE RELATIONSHIPS AUTOMATICALLY  
Author: ELLEN RILOFF AND JEFFREY LORENZEN 
Note: In Natural Language Information Retrieval, forthcoming from Kluwer Academic Publishers.  
Address: Salt Lake City, UT 84112  
Affiliation: Department of Computer Science University of Utah  
Abstract: In previous work, we developed several algorithms that use information extraction techniques to achieve high-precision text categorization. The relevancy signatures algorithm classifies texts using extraction patterns, and the augmented relevancy signatures algorithm classifies texts using extraction patterns and semantic features associated with role fillers (Riloff and Lehnert, 1994). These algorithms relied on hand-coded training data, including annotated texts and a semantic dictionary. In this chapter, we describe two advances that significantly improve the practicality of our approach. First, we explain how the extraction patterns can be generated automatically using only preclassified texts as input. Second, we present the word-augmented relevancy signatures algorithm that uses lexical items to represent domain-specific role relationships instead of semantic features. Using these techniques, we can automatically build text categorization systems that benefit from domain-specific natural language processing. Natural language processing and information retrieval evolved as separate research areas. To a large extent, the difference between them revolves around the amount of knowledge that is used. Natural language processing (NLP) techniques rely on syntactic and semantic knowledge that is often manually encoded for a particular topic. Information retrieval (IR) techniques typically use general methods for processing large volumes of text based on statistical word models. These two paradigms reflect a tug-of-war 
Abstract-found: 1
Intro-found: 1
Reference: <author> Borko, H. and Bernick, M. </author> <year> 1963. </year> <title> Automatic Document Classification. </title> <editor> J. </editor> <booktitle> ACM 10(2) </booktitle> <pages> 151-162. </pages>
Reference: <author> Fuhr, N.; Hartmann, S.; Lustig, G.; Schwantner, M.; and Tzeras, </author> <month> Konstadinos </month> <year> 1991. </year> <title> AIR/X A Rule-Based Multistage Indexing System for Large Subject Fields. </title> <booktitle> In Proceedings of RIAO 91. </booktitle> <pages> 606-623. </pages>
Reference: <author> Goodman, M. </author> <year> 1991. </year> <title> Prism: A Case-Based Telex Classifier. </title> <booktitle> In Proceedings of the Second Annual Conference on Innovative Applications of Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher> <pages> 25-37. </pages>
Reference: <author> Hayes, Philip J. and Weinstein, Steven P. </author> <year> 1991. </year> <title> Construe-TIS: A System for Content-Based Indexing of a Database of News Stories. </title> <booktitle> In Proceedings of the Second Annual Conference on Innovative Applications of Artificial Intelligence. </booktitle> <publisher> AAAI Press. </publisher> <pages> 49-64. </pages>
Reference: <author> Hoyle, W. </author> <year> 1973. </year> <title> Automatic Indexing and Generation of Classification Systems by Algorithm. </title> <booktitle> Information Storage and Retrieval 9(4) </booktitle> <pages> 233-242. </pages>
Reference: <author> Huffman, S. </author> <year> 1996. </year> <title> Learning information extraction patterns from examples. </title> <editor> In Wermter, Stefan; Riloff, Ellen; and Scheler, Gabriele, editors 1996, </editor> <title> Connectionist, Statistical, and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer-Verlag, Berlin. </publisher> <pages> 246-260. </pages>
Reference: <author> Kim, J. and Moldovan, D. </author> <year> 1993. </year> <title> Acquisition of Semantic Patterns for Information Extraction from Corpora. </title> <booktitle> In Proceedings of the Ninth IEEE Conference on Artificial Intelligence for Applications, </booktitle> <address> Los Alamitos, CA. </address> <publisher> IEEE Computer Society Press. </publisher> <pages> 171-176. </pages>
Reference: <author> Lehnert, W. </author> <year> 1991. </year> <title> Symbolic/Subsymbolic Sentence Analysis: Exploiting the Best of Two Worlds. </title> <editor> In Barnden, J. and Pollack, J., editors 1991, </editor> <booktitle> Advances in Connectionist and 30 ELLEN RILOFF AND JEFFREY LORENZEN Neural Computation Theory, </booktitle> <volume> Vol. 1. </volume> <publisher> Ablex Publishers, </publisher> <address> Norwood, NJ. </address> <pages> 135-164. </pages>
Reference-contexts: If no extraction patterns apply to a sentence, then nothing will be extracted. Consequently, irrelevant paragraphs and documents can be processed very quickly. 1 The experiments presented in this article were done using a conceptual sentence analyzer called CIRCUS <ref> (Lehnert, 1991) </ref>. CIRCUS uses case frames to extract information, but we will refer to these case frames as extraction patterns for our purposes (it should be noted, however, that CIRCUS' case frames do more than we describe here).
Reference: <author> Maron, M. </author> <year> 1961. </year> <title> Automatic Indexing: An Experimental Inquiry. </title> <editor> J. </editor> <booktitle> ACM 8 </booktitle> <pages> 404-417. </pages> <booktitle> MUC-3 Proceedings, 1991. Proceedings of the Third Message Understanding Conference (MUC-3). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <booktitle> MUC-4 Proceedings, 1992. Proceedings of the Fourth Message Understanding Conference (MUC-4). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <booktitle> MUC-5 Proceedings, 1993. Proceedings of the Fifth Message Understanding Conference (MUC-5). </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA. </address>
Reference: <author> Riloff, E. and Lehnert, W. </author> <year> 1994. </year> <title> Information Extraction as a Basis for High-Precision Text Classification. </title> <journal> ACM Transactions on Information Systems 12(3) </journal> <pages> 296-333. </pages>
Reference-contexts: Both the domain and the task play a significant role in deciding what approach is best. We performed a few experiments to classify texts based on multiple signatures and did not get consistently better results than with the single signature approach <ref> (Riloff, 1994) </ref>. Another reason why the single signature approach is effective is that the signatures capture some linguistic context. <p> Although this definition is arbitrary, it is easy to imagine many domain descriptions that depend on certain types of role fillers. Intuitively, we want to classify a text as relevant if it contains both a relevant key phrase and a relevant role filler. The augmented relevancy signatures algorithm <ref> (Riloff and Lehnert, 1994) </ref> does this by using both relevancy signatures and relevant slot triples. The triples are of the form (event-type slot-type feature) and are generated automatically from each extracted role filler. The feature corresponds to the semantic feature associated with the head noun of the role filler. <p> EXTRACTION-BASED TEXT CATEGORIZATION 11 is very conservative about classifying a text as relevant: a text must have both a key phrase and an associated role filler that were strongly correlated with relevant texts during training. Augmented relevancy signatures performed well in previous experiments <ref> (Riloff and Lehnert, 1994) </ref>, but they depend on both extraction patterns for the domain and a dictionary of words tagged with semantic features. In the following sections, we present new methods for eliminating these knowledge-engineering bottlenecks. <p> The extracted information, in the form of signatures and role fillers, is then analyzed statistically to identify classification terms that are highly correlated with a category. 3. Word-augmented relevancy signatures Augmenting relevancy signatures with semantic features produced much better results than relevancy signatures alone in the MUC-4 terrorism domain <ref> (Riloff and Lehnert, 1994) </ref>. But there was a price to pay. Augmented relevancy signatures need a semantic feature hierarchy and a dictionary of words tagged with semantic features. Consequently, using augmented relevancy signatures in a new domain requires an initial time investment that might not be acceptable for many applications. <p> In a real application, one must choose a set of threshold values in advance and it is often difficult to know what values are best. In previous work, we developed a procedure for identifying good threshold values from the training corpus empirically <ref> (Riloff and Lehnert, 1994) </ref>, but we do not use that procedure here because we want to see what each representation can do in the best case. 11 The MUC-4 definition specifies some additional constraints, for example only terrorist attacks in Latin America are relevant. 18 ELLEN RILOFF AND JEFFREY LORENZEN For
Reference: <author> Riloff, E. </author> <year> 1994. </year> <title> Information Extraction as a Basis for Portable Text Classification Systems. </title> <type> Ph.D. Dissertation, CMPSCI Technical Report 95-04, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst, MA. </address>
Reference-contexts: Both the domain and the task play a significant role in deciding what approach is best. We performed a few experiments to classify texts based on multiple signatures and did not get consistently better results than with the single signature approach <ref> (Riloff, 1994) </ref>. Another reason why the single signature approach is effective is that the signatures capture some linguistic context. <p> Although this definition is arbitrary, it is easy to imagine many domain descriptions that depend on certain types of role fillers. Intuitively, we want to classify a text as relevant if it contains both a relevant key phrase and a relevant role filler. The augmented relevancy signatures algorithm <ref> (Riloff and Lehnert, 1994) </ref> does this by using both relevancy signatures and relevant slot triples. The triples are of the form (event-type slot-type feature) and are generated automatically from each extracted role filler. The feature corresponds to the semantic feature associated with the head noun of the role filler. <p> EXTRACTION-BASED TEXT CATEGORIZATION 11 is very conservative about classifying a text as relevant: a text must have both a key phrase and an associated role filler that were strongly correlated with relevant texts during training. Augmented relevancy signatures performed well in previous experiments <ref> (Riloff and Lehnert, 1994) </ref>, but they depend on both extraction patterns for the domain and a dictionary of words tagged with semantic features. In the following sections, we present new methods for eliminating these knowledge-engineering bottlenecks. <p> The extracted information, in the form of signatures and role fillers, is then analyzed statistically to identify classification terms that are highly correlated with a category. 3. Word-augmented relevancy signatures Augmenting relevancy signatures with semantic features produced much better results than relevancy signatures alone in the MUC-4 terrorism domain <ref> (Riloff and Lehnert, 1994) </ref>. But there was a price to pay. Augmented relevancy signatures need a semantic feature hierarchy and a dictionary of words tagged with semantic features. Consequently, using augmented relevancy signatures in a new domain requires an initial time investment that might not be acceptable for many applications. <p> In a real application, one must choose a set of threshold values in advance and it is often difficult to know what values are best. In previous work, we developed a procedure for identifying good threshold values from the training corpus empirically <ref> (Riloff and Lehnert, 1994) </ref>, but we do not use that procedure here because we want to see what each representation can do in the best case. 11 The MUC-4 definition specifies some additional constraints, for example only terrorist attacks in Latin America are relevant. 18 ELLEN RILOFF AND JEFFREY LORENZEN For
Reference: <author> Riloff, E. </author> <year> 1995. </year> <title> Little Words Can Make a Big Difference for Text Classification. </title> <booktitle> In Proceedings of the 18th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval. </booktitle> <pages> 130-136. </pages>
Reference-contexts: For example, killed by X suggests that X is a perpetrator, while killed with X suggests that X is a weapon. We have shown elsewhere that recognizing different verb forms and prepositions can dramatically improve the accuracy of some classification terms <ref> (Riloff, 1995) </ref>. The linguistic expressions recognized by an extraction pattern can be represented as a signature. A signature is simply an extraction pattern paired with its trigger word. <p> EXTRACTION-BASED TEXT CATEGORIZATION 9 Also, the infinitive form of protest is much more strongly correlated with terrorism than the word protest alone. We have found many examples, in multiple domains, where different verb forms or different prepositions following a verb can dramatically change the effectiveness of classification terms <ref> (Riloff, 1995) </ref>. 1.3. AUGMENTED RELEVANCY SIGNATURES Signatures can be more powerful than individual words because they represent local syntactic context, but a major limitation is that they do not include role fillers.
Reference: <author> Riloff, E. </author> <year> 1996a. </year> <title> An Empirical Study of Automated Dictionary Construction for Information Extraction in Three Domains. </title> <booktitle> Artificial Intelligence 85 </booktitle> <pages> 101-134. </pages>
Reference-contexts: Fortunately, several systems have been developed recently that can generate extraction patterns automatically or semi-automatically from training data (Kim and Moldovan, 1993; Soderland et al., 1995; Huffman, 1996), including our own AutoSlog system <ref> (Riloff, 1996a) </ref>. AutoSlog constructed a dictionary of extraction patterns for the MUC-4 terrorism domain that achieved 98% of the performance of hand-crafted patterns. AutoSlog uses annotated texts for training and a human must manually review the resulting extraction patterns.
Reference: <author> Riloff, E. </author> <year> 1996b. </year> <title> Automatically Generating Extraction Patterns from Untagged Text. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence. </booktitle> <publisher> The AAAI Press/MIT Press. </publisher> <pages> 1044-1049. </pages>
Reference-contexts: Generating an annotated training corpus is both time-consuming and deceptively difficult (determining which words to mark is tricky), and a new training corpus must be annotated for each domain. To further reduce this knowledge-engineering bottleneck, we have developed a successor to AutoSlog, called AutoSlog-TS <ref> (Riloff, 1996b) </ref>, that creates extraction patterns from raw text. As input, AutoSlog-TS needs preclassified texts that have been identified as relevant or irrelevant to the domain. Nothing inside the texts needs to be marked in any way.
Reference: <author> Salton, G., </author> <title> editor 1971. The SMART Retrieval System: Experiments in Automatic Document Processing. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ. </address>
Reference: <author> Soderland, S.; Fisher, D.; Aseltine, J.; and Lehnert, W. </author> <year> 1995. </year> <title> CRYSTAL: Inducing a conceptual dictionary. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence. </booktitle> <pages> 1314-1319. </pages>
Reference: <author> Stanfill, C. and Waltz, D. </author> <year> 1986. </year> <title> Toward Memory-Based Reasoning. </title> <journal> Communications of the ACM 29(12) </journal> <pages> 1213-1228. </pages>
Reference: <author> Turtle, Howard and Croft, W. </author> <title> Bruce 1991. Efficient Probabilistic Inference for Text Retrieval. </title> <booktitle> In Proceedings of RIAO 91. </booktitle> <pages> 644-661. </pages>
References-found: 18

