URL: http://www.ldc.usb.ve/~hector/reports/learning.ps
Refering-URL: http://www.cs.wisc.edu/icml98/schedule.html
Root-URL: 
Email: bonet@usb.ve  hector@usb.ve  
Title: Learning sorting and decision trees with POMDPs  
Author: Blai Bonet Hector Geffner 
Address: Aptdo. 89000, Caracas 1080-A Venezuela  Aptdo. 89000, Caracas 1080-A Venezuela  
Affiliation: Departamento de Computacion Universidad Simon Bolvar  Departamento de Computacion Universidad Simon Bolvar  
Abstract: pomdps are general models of sequential decisions in which both actions and observations can be probabilistic. Many problems of interest can be formulated as pomdps, yet the use of pomdps has been limited by the lack of effective algorithms. Recently this has started to change and a number of problems such as robot navigation and planning are beginning to be formulated and solved as pomdps. The advantage of the pomdp approach is its clean semantics and its ability to produce principled solutions that integrate physical and information gathering actions. In this paper we pursue this approach in the context of two learning tasks: learning to sort a vector of numbers and learning decision trees from data. Both problems are formulated as pomdps and solved by a general pomdp algorithm. The main lessons and results are that 1) the use of suitable heuristics and representations allows for the solution of sorting and classification pomdps of non-trivial sizes, 2) the quality of the resulting solutions are competitive with the best algorithms, and 3) problematic aspects in decision tree learning such as test and mis-classification costs, noisy tests, and missing values are naturally accommodated.
Abstract-found: 1
Intro-found: 1
Reference: <author> Aho, A.; Hopcroft, J.; and Ullman, J. </author> <year> 1983. </year> <title> Data Structures and Algorithms. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: Here we provide solutions for pomdps of size n = 10 that involve more than a million states. Moreover the solutions are good: on average they involve half the number of comparisons and swaps as Quicksort, one of the best sorting algorithms <ref> (Aho, Hopcroft, & Ullman 1983) </ref>. The solution method relies on good heuristic func tions, compact representations of beliefs, and suitable decompositions. The sorting problem is difficult and we use it not to learn about sorting but to learn about pomdps.
Reference: <author> Barto, A.; Bradtke, S.; and Singh, S. </author> <year> 1995. </year> <title> Learning to act using real-time dynamic programming. </title> <booktitle> Artificial Intelligence 72 </booktitle> <pages> 81-138. </pages>
Reference-contexts: Both problems are formulated as pomdps and solved by a general pomdp algorithm (Geffner & Bonet 1998b) based on the ideas of Real Time Dynamic Programming <ref> (Barto, Bradtke, & Singh 1995) </ref>. The choice of the two tasks requires an explanation. Both are sequential decision problems that can be naturally seen as pomdps. Yet the difficulties and insights that result from modeling and solving each problem as a pomdp are different. <p> This has started to change (Cassandra, Kae-bling, & Littman 1995) and here we use a pomdp algorithm introduced in (Geffner & Bonet 1998b) that is based on the ideas of Real Time Dynamic Programming <ref> (Barto, Bradtke, & Singh 1995) </ref>. rtdp-bel is a hill-climbing algorithm that from any state b searches for the goal states b F by performing actions a that lead to new states b o a with probability b a (o) (Figure 1). <p> i These heuristics are not admissible in the sense that they may overestimate the minimum expected cost to the goal, and as a result may prevent the estimates V (b) to approach the optimal values. 3 Yet the admis sible heuristics we have tried were not as informative, 3 See <ref> (Barto, Bradtke, & Singh 1995) </ref> for the relation between admissibility and optimality in rtdp algorithms. Trials for sorting problems of sizes n = 5 and n = 10.
Reference: <author> Bellman, R. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton University Press. </publisher>
Reference-contexts: Finally we summarize the main lessons and ideas (Section 6). 2 BACKGROUND pomdps are a generalization of a model of sequential decision making formulated by Richard Bellman in the 50's called Markov Decision Processes or mdps, in which the state of the environment is assumed known <ref> (Bellman 1957) </ref>. mdps provide the basis for understanding pomdps so we turn to them first. 1 2.1 MDPs The type of mdps that we consider is a generalization of the standard search model used in AI in which actions can have probabilistic effects.
Reference: <author> Bertsekas, D., and Tsitsiklis, J. </author> <year> 1996. </year> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific. </publisher>
Reference: <author> Boutilier, C.; Dean, T.; and Hanks, S. </author> <year> 1995. </year> <title> Planning under uncertainty: structural assumptions and computational leverage. </title> <booktitle> In Proceedings of EWSP-95. </booktitle>
Reference: <author> Breiman, L.; Friedman, J.; Olshen, R.; and Stone, C. </author> <year> 1984. </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth International Group. </publisher>
Reference: <author> Cassandra, A.; Kaebling, L.; and Kurien, J. </author> <year> 1996. </year> <title> Acting under uncertainty: Discrete bayesian model for mobile robot navigation. </title> <booktitle> In Proceedings of IEEE/RSJ International Conference on Intelligent Robot and Systems. </booktitle>
Reference: <author> Cassandra, A.; Kaebling, L.; and Littman, M. </author> <year> 1994. </year> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings AAAI94, </booktitle> <pages> 1023-1028. </pages>
Reference-contexts: The task is to find a mapping from belief states to actions that will take us from the initial belief state b 0 to a final belief state b F at a minimum expected cost. The way actions and observations affect the belief state is given by the equations <ref> (Cassandra, Kaebling, & Littman 1994) </ref>: b a (s) = s 0 2S b a (o) = s2S b o where b a is the belief state that results after doing action a in b, b a (o) is the probability of observing o after doing a in b, and b o
Reference: <author> Cassandra, A.; Kaebling, L.; and Littman, M. </author> <year> 1995. </year> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Proc. of the 12th Int. Conf. on Machine Learning. </booktitle>
Reference-contexts: 1 INTRODUCTION pomdps are general models of sequential decisions in which both actions and observations can be probabilistic (Sondik 1971; Cassandra, Kaebling, & Littman 1994). Many problems of interest can be formulated as pomdps yet the use of pomdps has been limited by the lack of effective algorithms <ref> (Cassandra, Kae-bling, & Littman 1995) </ref>. Recently this has started to change and a number of problems such as robot navigation and planning are beginning to be formulated and solved as pomdps (Cassandra, Kaebling, & Kurien 1996; Geffner & Bonet 1998a). <p> Solving belief mdps is difficult and until recently only very small problems could be solved reasonably well especially when they involved information-gathering actions. This has started to change <ref> (Cassandra, Kae-bling, & Littman 1995) </ref> and here we use a pomdp algorithm introduced in (Geffner & Bonet 1998b) that is based on the ideas of Real Time Dynamic Programming (Barto, Bradtke, & Singh 1995). rtdp-bel is a hill-climbing algorithm that from any state b searches for the goal states b F
Reference: <author> Diettriech, T. </author> <year> 1997. </year> <journal> Machine learning research. Artificial Intelligence Magazine 18(4) </journal> <pages> 97-136. </pages>
Reference: <author> Friedman, J.; Kohavi, R.; and Yun, Y. </author> <year> 1996. </year> <title> Lazy decision trees. </title> <booktitle> In Proceedings AAAI-96, </booktitle> <pages> 717-724. </pages> <publisher> MIT Press. </publisher>
Reference: <author> Geffner, H., and Bonet, B. </author> <year> 1998a. </year> <title> High-level plannig and control with incomplete information using POMDP's. </title> <note> Available at http://www.ldc.usb.ve/~hector. </note>
Reference: <author> Geffner, H., and Bonet, B. </author> <year> 1998b. </year> <title> Solving large POMDPs using real time dynamic programming. </title> <note> Available at http://www.ldc.usb.ve/~hector. </note>
Reference-contexts: In this paper we pursue this approach in the context of two learning tasks: learning to sort a vector of numbers and learning decision trees from data. Both problems are formulated as pomdps and solved by a general pomdp algorithm <ref> (Geffner & Bonet 1998b) </ref> based on the ideas of Real Time Dynamic Programming (Barto, Bradtke, & Singh 1995). The choice of the two tasks requires an explanation. Both are sequential decision problems that can be naturally seen as pomdps. <p> Solving belief mdps is difficult and until recently only very small problems could be solved reasonably well especially when they involved information-gathering actions. This has started to change (Cassandra, Kae-bling, & Littman 1995) and here we use a pomdp algorithm introduced in <ref> (Geffner & Bonet 1998b) </ref> that is based on the ideas of Real Time Dynamic Programming (Barto, Bradtke, & Singh 1995). rtdp-bel is a hill-climbing algorithm that from any state b searches for the goal states b F by performing actions a that lead to new states b o a with probability <p> Then when the value V (b 0 ) of a state b 0 that is not in the table is needed, a new entry with V (b 0 ) set to h (b 0 ) is created. Usually belief states need to be discretized <ref> (Geffner & Bonet 1998b) </ref> 1. Evaluate each action a applicable in b as Q (a; b) = c (a; b) + o2O a ) initializing V (b o a ) to h (b o a ) when b o a not in table 2.
Reference: <author> Korf, R. </author> <year> 1990. </year> <title> Real-time heuristic search. </title> <booktitle> Artificial Intelligence 42 </booktitle> <pages> 189-211. </pages>
Reference-contexts: Initially V (b) is set to h (b), where h is a suitable heuristic function, and every time the state b is visited V (b) is updated to make it consistent with the values V (b 0 ) of its possible successor states b 0 <ref> (Korf 1990) </ref>. In the implementation, the estimates V (b) are stored in a hash table that initially contains an estimate for V (b 0 ) only.
Reference: <author> Mitchell, T. </author> <year> 1997. </year> <title> Machine Learning. </title> <publisher> McGraw-Hill. </publisher>
Reference-contexts: An interesting challenge is the extraction of a concise and generalized representation of the policy that could be applied to problems of any size. 5 DECISION TREES Decision trees are classifiers that map instances into classes by sequentially testing the value of a finite set of attributes <ref> (Mitchell 1997) </ref>.
Reference: <author> Murphy, P. M., and Aha, D. W. </author> <year> 1998. </year> <note> UCI repository of machine learning databases. http://www.ics.uci.edu/learn. </note>
Reference-contexts: All these aspects fit into the pomdp formulation of decision tree learning in a natural way. We evaluate this formulation over a number of datasets from <ref> (Murphy & Aha 1998) </ref>. Our goal is to show that the pomdp approach may be competitive with the standard approaches and potentially more general. <p> It may be possible to calibrate such heuristics to accelerate convergence but we don't know how to do that yet. 5.3 EVALUATION Table 1 compares rtdp-bel with two standard decision tree learning algorithms, ID3 and C4.5 (Quinlan 1990; 1993) over some small datasets obtained from the UCI Repository <ref> (Murphy & Aha 1998) </ref> for two different misclassification costs C. 4 For each dataset, 4 The figures for ID3 and C4.5 were taken from (Fried-man, Kohavi, & Yun 1996). The column named `Test' in the table indicates how the generalization performance of the algorithms was measured.
Reference: <author> Murthy, S. </author> <year> 1998. </year> <title> Automatic construction of decision trees from data: A multidisciplinary survey. </title> <type> Technical report, </type> <institution> Siemens Corporate Research. </institution>
Reference-contexts: The generalization power of decision tree algorithms is measured by the classification error over part of the data that is left aside for testing. Decision tree learning algorithms have been applied to a number of domains <ref> (Murthy 1998) </ref> and a number of variations and extensions have been considered (Diet-triech 1997). 5.1 FORMULATION The problem of learning decision trees can be seen as a sequential decision problem that involves two types of actions: report (i) by which the current instance s is classified in class c i ,
Reference: <author> Puterman, M. </author> <year> 1994. </year> <title> Markov Decision Processes - Discrete Stochastic Dynamic Programming. </title> <publisher> John Wi-ley and Sons, Inc. </publisher>
Reference: <author> Quinlan, J. R. </author> <year> 1990. </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning 1(1). </booktitle>
Reference-contexts: Yet even with this approximation, larger datasets could not be handled as memory tends to explode. The main problem is the lack of an informative heuristic that can guide the search, while leaving a large fraction of the (belief) state space unvisited. Heuristics such as `information gain' <ref> (Quinlan 1990) </ref> are informative but are not calibrated with the expected costs. 5 As a result, they produce a focused 5 That is, information gain is not a good estimate of the expected costs. search for the goal in the first few trials, but then become useless as some of the
Reference: <author> Quinlan, J. R. </author> <year> 1993. </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kauffman. </publisher>
Reference: <author> Russell, S., and Norvig, P. </author> <year> 1994. </year> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice Hall. </publisher>
Reference: <author> Sondik, E. </author> <year> 1971. </year> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> Ph.D. Dissertation, </type> <institution> Stanford University. </institution>
References-found: 22

