URL: http://www.cs.toronto.edu/~ruppert/thesis.ps.gz
Refering-URL: http://www.cs.toronto.edu/~ruppert/index.html
Root-URL: 
Title: Parallel Algorithms for the k Shortest Paths and Related Problems  
Author: Eric Ruppert 
Degree: A thesis submitted in conformity with the requirements for the degree of Master of Science to the  
Note: c flCopyright by Eric Ruppert, 1996  
Address: Toronto  
Affiliation: Graduate Department of Computer Science University of  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> K. Abrahamson, N. Dadoun, D. A. Kirkpatrick, and T. Przytycka. </author> <title> A simple parallel tree contraction algorithm. </title> <booktitle> In Proc. 25th Annual Allerton Conference on Communication, Control and Computing, </booktitle> <volume> volume II, </volume> <pages> pages 624-633, </pages> <year> 1987. </year>
Reference-contexts: Miller and Reif [32] gave the original version of tree contraction. Abrahamson et al. <ref> [1] </ref> and Kosaraju and Delcher [30] independently gave the improved version described below, which performs less work. Here, it is assumed that each internal node has exactly two children. If an internal node in the input tree has only one child, an extra dummy child may be added.
Reference: [2] <author> Paul Beame and Johan Hastad. </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <booktitle> In Proc. 19th ACM Symposium on Theory of Computing, </booktitle> <pages> pages 83-93, </pages> <year> 1987. </year>
Reference-contexts: The straightforward parallel version of this algorithm would run in ( log 2 n log log n ) time, since the summation of the weights of elements less than the median requires ( log n log log n ) parallel steps, if the number of processors is polynomial in n <ref> [2] </ref>, and over the course of the algorithm this contributes 10 ( i=0 log log (n=2 i ) ) = ( log 2 n log log n ) steps to the running time of the algorithm.
Reference: [3] <author> Manuel Blum, Robert W. Floyd, Vaughan Pratt, Ronald L. Rivest, and Robert E. Tarjan. </author> <title> Time bounds for selection. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 7 </volume> <pages> 448-461, </pages> <year> 1973. </year>
Reference-contexts: This was originally shown by Blum et al. <ref> [3] </ref>. Schonhage, Paterson and Pippenger [39] gave a more complicated algorithm which reduced the number of comparisons by a constant factor. The algorithm of Blum et al. was adapted to run in O (log n log log n) time and O (n) work on an EREW PRAM by Vishkin [43].
Reference: [4] <author> A. Borodin and J. E. Hopcroft. </author> <title> Routing, merging and sorting in parallel models of computation. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 30 </volume> <pages> 130-145, </pages> <year> 1985. </year>
Reference-contexts: p = P n i=1 p i processors numbered 1 to p are available, the sums S (k) = P k i=1 p i can be computed and the processors numbered S (k 1) + 1 to S (k) can be assigned to task k. 1.3.2 Merging Borodin and Hopcroft <ref> [4] </ref> describe a CREW PRAM implementation of Valiant's algorithm [42] to merge two sorted arrays X and Y of length n and m that runs in O (log log n) time using n + m processors.
Reference: [5] <author> Gen-Huey Chen and Yung-Chen Hung. </author> <title> Algorithms for the constrained quickest path problem and the enumeration of quickest paths. </title> <journal> Computers and operations research, </journal> <volume> 21(2) </volume> <pages> 113-118, </pages> <year> 1994. </year>
Reference-contexts: If k = ! q n + r log n , this is better than the best known algorithm for the all-pairs version of the problem, due to G.-H. Chen and Y.-C. Hung <ref> [5] </ref>, which runs in O (k 2 mn 2 ) time. <p> Chen and Y.-C. Hung <ref> [5] </ref> O (kmn 2 log k) Single pair k quickest simple paths Improved version of Y. Chen [7] O (krn 3 ) 8 Single pair k quickest simple paths in undirected graph Improved version of Y.
Reference: [6] <author> Y. L. Chen. </author> <title> An algorithm for finding the k quickest paths in a network. </title> <journal> Computers and Operations Research, </journal> <volume> 20(1) </volume> <pages> 59-65, </pages> <year> 1993. </year>
Reference-contexts: If fewer than k s-t paths exist, P should contain all of them. Y. Chen <ref> [6] </ref> gives a sequential algorithm to find the k quickest paths between two vertices that runs in O (m 2 + (m + k)n log n + k 3=2 log k) time.
Reference: [7] <author> Y. L. Chen. </author> <title> Finding the k quickest simple paths in a network. </title> <journal> Information Processing Letters, </journal> <volume> 50(2) </volume> <pages> 89-92, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: is still better when k log k = ! q n + r log n The problem of finding the k quickest simple paths has also been studied (<ref> [7] </ref>, [37]). This version of the problem appears to be significantly harder, and the best previously known algorithm, presented by Y. Chen [7], runs in O (krn 3 + krn log k) time, or O (krn 2 + krn log k) time for an undirected graph. An improved implementation which decreases the running time to O (krn 3 ) (or O (krn 2 ) for undirected graphs) is described here. <p> Chen and Y.-C. Hung [5] O (kmn 2 log k) Single pair k quickest simple paths Improved version of Y. Chen <ref> [7] </ref> O (krn 3 ) 8 Single pair k quickest simple paths in undirected graph Improved version of Y. Chen [7] O (krn 2 ) Table 4.1: Sequential algorithms for quickest path problems for shortest paths in each subgraph G i = (V; E i ), which contains only those edges <p> Chen and Y.-C. Hung [5] O (kmn 2 log k) Single pair k quickest simple paths Improved version of Y. Chen <ref> [7] </ref> O (krn 3 ) 8 Single pair k quickest simple paths in undirected graph Improved version of Y. Chen [7] O (krn 2 ) Table 4.1: Sequential algorithms for quickest path problems for shortest paths in each subgraph G i = (V; E i ), which contains only those edges with capacity at least c i .
Reference: [8] <author> Y. L. Chen and Y. H. Chin. </author> <title> The quickest path problem. </title> <journal> Computers and Operations Research, </journal> <volume> 17(2) </volume> <pages> 153-161, </pages> <year> 1990. </year>
Reference-contexts: It was introduced by Y. Chen and Y. Chin <ref> [8] </ref> to find the quickest way of transmitting data through a computer network. The network is represented as a directed graph, G = (V; E) with n vertices and m edges. <p> For example, when &gt; 6, the quickest path from s to t is s ! u ! v ! t even though the quickest path from s to v is s ! v. 4.1.1 Sequential Algorithms Y. Chen and Y. Chin <ref> [8] </ref> give a sequential algorithm to find the quickest path from vertex s to vertex t in O (m 2 + mn log n) time. <p> To avoid having to perform shortest path computations on such a large graph, the problem will again be broken up by looking 45 Problem Algorithm Running Time 1 Single pair quickest path Y. Chen and Y. Chin <ref> [8] </ref> O (rm + rn log n) 2 All pairs quickest path Repeated application of (1) O (rmn + rn 2 log n) 3 Y.-C. Hung and G.-H.
Reference: [9] <author> R. Cole. </author> <title> An optimally efficient selection algorithm. </title> <journal> Information Processing Letters, </journal> <volume> 26 </volume> <pages> 295-299, </pages> <month> January </month> <year> 1988. </year>
Reference-contexts: Vishkin's algorithm is described in Section 2.1, below, where it is modified to solve the more general problem of weighted selection. Cole <ref> [9] </ref> developed a faster parallel algorithm, which performs O (n) work in O (log n log fl n) time on an EREW 7 PRAM, using an approach that originally appeared in Floyd and Rivest's randomized sequential selection algorithm [15]. <p> The k smallest edges out of each vertex can then be chosen in O (log d log fl d) time and O ( P v2V outdegree (v)) = O (m) work, using Cole's selection algorithm <ref> [9] </ref>. Cole's parallel merge sort [10] can be used to sort the k smallest edges out of each vertex in O (log k) time using nk processors.
Reference: [10] <author> R. Cole. </author> <title> Parallel merge sort. </title> <journal> SIAM Journal on Computing, </journal> <volume> 17(4) </volume> <pages> 770-785, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The fastest known parallel sorting algorithm that performs O (n log n) work is Cole's parallel merge sort <ref> [10] </ref>, which runs in O (log n) time on an n-processor EREW PRAM. The algorithm stores the elements of the unsorted array at the leaves of a complete binary tree. For each internal node, the sorted array of the node's leaf descendants is computed. <p> A detailed description of the algorithm is shown in Figure 2.1. WeightedSelect (X; K) Broadcast n and K to all processors If X has at most p elements Sort the elements of X using Cole's parallel merge sort <ref> [10] </ref> Compute the prefix sums of the sorted array Output the leftmost element whose corresponding prefix sum is at least K Else Divide X into p groups, each with either dn=pe or bn=pc elements Use one processor per group to find the median of each group with a linear time sequential <p> The k smallest edges out of each vertex can then be chosen in O (log d log fl d) time and O ( P v2V outdegree (v)) = O (m) work, using Cole's selection algorithm [9]. Cole's parallel merge sort <ref> [10] </ref> can be used to sort the k smallest edges out of each vertex in O (log k) time using nk processors. Tree contraction is used to compute the array of the k shortest edges whose tails are on the path from each vertex v to the destination t. <p> The k elements of A i v can then be sorted in O (log k) time and O (k log k) work using Cole's parallel merge sort <ref> [10] </ref>. The ith stage of the algorithm that computes the arrays A i v (for all vertices v) therefore uses O (log 2 k log fl k) time and O (nk log k) work.
Reference: [11] <author> R. Cole and U. Vishkin. </author> <title> Approximate and exact parallel scheduling with applications to list, tree and graph problems. </title> <booktitle> In Proc. 27th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 478-491, </pages> <year> 1986. </year> <month> 55 </month>
Reference-contexts: In general, if p n processors are available, this approach can be used to obtain a prefix sum algorithm that runs in O (log p + n=p) time by using p blocks of size n=p. Cole and Vishkin <ref> [11] </ref> describe a slightly faster prefix sum algorithm for O (log n)-bit integers which runs in O (log n= log log n) time and performs O (n) work on an Arbitrary CRCW PRAM. The prefix sum algorithm is often used to allocate processors to tasks.
Reference: [12] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms, chapter 16. </title> <publisher> MIT Press, </publisher> <year> 1990. </year>
Reference-contexts: algorithm, but the amount of work will increase by an amount proportional to the length of the output. 48 4.2 Adapting Dynamic Programming Algorithms to Find the k Best Solutions in Parallel Dynamic programming is a widely used technique for the design of sequential algorithms to solve optimization problems. (See <ref> [12] </ref> for a general discussion of dynamic programming.) A dynamic programming algorithm starts with a recurrence relation that defines the value of the optimal solution for a given problem instance in terms of the values of optimal solutions to smaller problem instances.
Reference: [13] <author> David Eppstein. </author> <title> Finding the k shortest paths. </title> <booktitle> In Proc. 35th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 154-165, </pages> <year> 1994. </year>
Reference-contexts: The problem was discussed as early as 1959 by Hoffman and Pavley [23]. Fox presents an algorithm that can be implemented to run in O (m + kn log n) time [17]. Eppstein's recent sequential algorithm computes the k shortest paths for a given source and destination <ref> [13] </ref> in O (m + n log n + k) time. This algorithm will be described in Section 3.2, below. Other variations of the k shortest path problem have been studied and sequential algorithms have been developed for them. <p> Lemma 3.1 (Eppstein <ref> [13] </ref>) (i) ffi (u; v) 0 for all (u; v) 2 E (iii) For any path p from s to t, weight (p) = dist (s; t) + X ffi (u; v) = dist (s; t) + X (u;v)2sidetracks (p) ffi (u; v) Proof: (i) Since dist (u; t) is the <p> Eppstein's algorithm constructs the trees H (v) to have bounded outdegree, so that the graph G 0 31 also has bounded outdegree. By allowing the trees to share common subtrees, the construction of G 0 can be carried out in O (m + n log n) time. (See <ref> [13] </ref> for details.) The problem of finding the k shortest paths s-t paths in G has now been reduced to a problem of finding the k shortest paths that begin at node s 0 in G 0 . <p> In fact, this technique can be used to compute a variety of properties of the paths without explicitly listing the edges in the path, as observed by Eppstein <ref> [13] </ref>. Suppose each edge in the graph is assigned a value from a semigroup, which has an associative binary multiplication operator defined on it, and the value of a path is defined as the product of the values of the edges along the path. <p> In such cases, the k best solutions to the optimization problem can be obtained by finding the k shortest paths through the graph. In this section, this approach is described, using the Viterbi decoding problem as a concrete example. Other examples are described by Eppstein <ref> [13] </ref>.
Reference: [14] <author> Faith E. Fich. </author> <title> The complexity of computation on the parallel random access machine. </title> <note> In [36], Chapter 20. </note>
Reference-contexts: Listed 4 in decreasing order by power, the PRAM models are Priority CRCW &gt; Arbitrary CRCW &gt; Common CRCW &gt; CREW &gt; EREW; an algorithm that runs on one model will run unchanged on any of the more powerful models. See Fich's survey <ref> [14] </ref> for a detailed account of the relationships between the various PRAM models. There are several measures that can be used to assess the performance of an algorithm.
Reference: [15] <author> Robert W. Floyd and Ronald L. Rivest. </author> <title> Expected time bounds for selection. </title> <journal> Communications of the ACM, </journal> <volume> 18(3) </volume> <pages> 165-172, </pages> <month> March </month> <year> 1975. </year>
Reference-contexts: Cole [9] developed a faster parallel algorithm, which performs O (n) work in O (log n log fl n) time on an EREW 7 PRAM, using an approach that originally appeared in Floyd and Rivest's randomized sequential selection algorithm <ref> [15] </ref>. Cole's algorithm runs in log fl n stages, reducing the number of candidates by computing two approximations to the kth element: one that is larger than the required element and one that is smaller than it. All candidates outside this range are discarded before the following stage.
Reference: [16] <author> G. David Forney, Jr. </author> <title> The Viterbi algorithm. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 61(3) </volume> <pages> 268-278, </pages> <month> March </month> <year> 1973. </year>
Reference-contexts: This problem has applications in communications, and the randomness in the observations is used to model a Markov process whose transitions are being observed over a noisy channel (see <ref> [16] </ref>). The Viterbi decoding problem is to compute the most likely state sequence that could produce the given observations of the system's transitions. <p> Viterbi presented a dynamic programming solution to this problem in 1967 [44]. The Viterbi algorithm runs in O (s 2 T ) time. In 1973, Forney suggested that the algorithm be augmented to output the k most likely state sequences <ref> [16] </ref>. This problem is called the list Viterbi decoding problem, and is of practical importance in a number of areas, including communications and speech 50 recognition (see [16], [40], [41]). <p> In 1973, Forney suggested that the algorithm be augmented to output the k most likely state sequences <ref> [16] </ref>. This problem is called the list Viterbi decoding problem, and is of practical importance in a number of areas, including communications and speech 50 recognition (see [16], [40], [41]). Seshadri and Sundberg describe a list Viterbi algorithm that lists the best k state sequences in O (s 2 T + ksT + k log k) time [40].
Reference: [17] <author> B. L. Fox. </author> <title> Calculating kth shortest paths. </title> <journal> INFOR; Canadian Journal of Operational Research, </journal> <volume> 11(1) </volume> <pages> 66-70, </pages> <year> 1973. </year>
Reference-contexts: The problem was discussed as early as 1959 by Hoffman and Pavley [23]. Fox presents an algorithm that can be implemented to run in O (m + kn log n) time <ref> [17] </ref>. Eppstein's recent sequential algorithm computes the k shortest paths for a given source and destination [13] in O (m + n log n + k) time. This algorithm will be described in Section 3.2, below.
Reference: [18] <author> Greg N. Frederickson. </author> <title> An optimal algorithm for selection in a min-heap. </title> <journal> Information and Computation, </journal> <volume> 104 </volume> <pages> 197-214, </pages> <year> 1993. </year>
Reference-contexts: The latter problem can be solved by Frederickson's O (k) algorithm that selects the k smallest elements in a heap-ordered out-tree F with bounded outdegree <ref> [18] </ref>. The nodes of F represent paths of G 0 that start at s 0 . The root of F represents the trivial path with only one node, s 0 .
Reference: [19] <author> Greg N. Frederickson and Donald B. Johnson. </author> <title> The complexity of selection and ranking in X +Y and matrices with sorted columns. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 24 </volume> <pages> 197-208, </pages> <year> 1982. </year>
Reference-contexts: Frederickson and Johnson describe a sequential solution that runs in O (n) time if k n, and O (n log (k=n)) time otherwise, and they give a matching lower bound for comparison-based algorithms <ref> [19] </ref>. Their approach will be described in Section 2.4, below, where it will be used in the development of a parallel algorithm for the problem. Some similar selection problems, where the input is constrained in some way, have been studied. <p> but would require O (k) work. 2.4 A Parallel Elimination Algorithm for Selection in a Matrix With Sorted Columns This section contains an EREW PRAM implementation of Frederickson and Johnson's sequential algorithm to select the kth smallest element of an m fi n matrix M of integers with sorted columns <ref> [19] </ref>. The sequential algorithm runs in O (n) time if k n, and O (n log k n ) time otherwise. In Section 2.5, the parallel algorithm will be combined with the algorithm of Section 2.3 to improve the running time. <p> All of the sets whose representative is greater than q fl are eliminated. Lemma 2.7 (Frederickson and Johnson <ref> [19] </ref>) When phase 2 of the algorithm is completed, O (k) elements remain active, and the k smallest elements of M are among them. <p> A more precise description of the algorithm appears in Figure 2.6, and the following lemma explains why the algorithm works. Lemma 2.8 (Frederickson and Johnson <ref> [19] </ref>) For each iteration of the loop in phase 3, (i) the number of active elements is reduced from N to at most 15N=16 + 1. (ii) If ~ k N=2 at the beginning of the iteration, only elements smaller than the ~ kth-smallest element are discarded; otherwise only elements larger
Reference: [20] <author> Greg N. Frederickson and Donald B. Johnson. </author> <title> Generalized selection and ranking: Sorted matrices. </title> <journal> SIAM Journal on Computing, </journal> <volume> 13(1) </volume> <pages> 14-30, </pages> <year> 1984. </year>
Reference-contexts: If the vectors X and Y are sorted, then the Cartesian sum matrix is an example of a sorted matrix; the elements of each column and of each row are sorted into non-decreasing order. Frederickson and Johnson <ref> [20] </ref> and Mirzaian and Arjomandi [33] both give O (n) algorithms for selection in an n fi n sorted matrix. A technique similar to the one used by Mirzaian and Arjomandi will be used in Section 2.3 to obtain a parallel algorithm for selection in a matrix with sorted columns. <p> A technique similar to the one used by Mirzaian and Arjomandi will be used in Section 2.3 to obtain a parallel algorithm for selection in a matrix with sorted columns. Sarnath and He [38] use an approach similar to the algorithm described in <ref> [20] </ref> to develop an EREW PRAM algorithm for selection in a sorted matrix that runs in O (log n log log n log fl n) time and performs O (n log log n) work.
Reference: [21] <author> Michael L. Fredman and Robert Endre Tarjan. </author> <title> Fibonacci heaps and their uses in improved network optimization algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 34(3) </volume> <pages> 596-615, </pages> <year> 1987. </year>
Reference-contexts: Dijkstra's algorithm computes the shortest path from a given source vertex to all other vertices in O (m + n log n) time <ref> [21] </ref>. Applying Dijkstra's algorithm n times, once for each source vertex, yields a solution to the shortest path problem between all pairs of vertices in O (nm + n 2 log n) time. <p> Since G 0 has rn + 1 vertices and r (m + 1) edges, the required shortest path in G 0 can be found, using Dijkstra's algorithm, in O (rm + rn log (rn)) = O (rm + rn log n) time <ref> [21] </ref>. Finding the k Quickest Paths The problem of finding the k quickest paths from vertex s to vertex t for the transmission of bits of data has also been studied.
Reference: [22] <author> Y. Han, V. Pan, and J. Reif. </author> <title> Efficient parallel algorithms for computing all pair shortest paths in directed graphs. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pages 353-362, </pages> <year> 1992. </year>
Reference-contexts: A more complicated implementation of the transitive closure computation, by Han, Pan and Reif, runs in O (log 2 n) time on the EREW PRAM while doing only o (n 3 ) work <ref> [22] </ref>. Han et al. also give an O (log n log log n)-time implementation on the CRCW PRAM which also does o (n 3 ) work. The problem of finding the k shortest paths between vertices in a graph has long been studied in sequential models of computation.
Reference: [23] <author> Walter Hoffman and Richard Pavley. </author> <title> A method of solution of the Nth best path problem. </title> <journal> Journal of the ACM, </journal> <volume> 6 </volume> <pages> 506-514, </pages> <year> 1959. </year>
Reference-contexts: The problem of finding the k shortest paths between vertices in a graph has long been studied in sequential models of computation. The problem was discussed as early as 1959 by Hoffman and Pavley <ref> [23] </ref>. Fox presents an algorithm that can be implemented to run in O (m + kn log n) time [17]. Eppstein's recent sequential algorithm computes the k shortest paths for a given source and destination [13] in O (m + n log n + k) time.
Reference: [24] <author> Yung-Chen Hung and Gen-Huey Chen. </author> <title> On the quickest path problem. </title> <booktitle> In Advances in Computing and Information International Conference on Computing and Information, number 497 in Lecture Notes in Computer Science, </booktitle> <pages> pages 44-46, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Kagaris et al. also study a dynamic version of the quickest paths problem. Y.-C. Hung and G.-H. Chen describe an O (mn 2 ) algorithm to find the quickest path between all pairs of vertices <ref> [24] </ref> and also consider distributed algorithms in an asynchronous, message-passing model of computation [25]. The sequential algorithm described here, which is a slightly modified version of Chen and Chin's algorithm, uses a transformation of the graph to reduce the quickest path problem to a shortest path problem.
Reference: [25] <author> Yung-Chen Hung and Gen-Huey Chen. </author> <title> Distributed algorithms for the quickest path problem. </title> <journal> Parallel Computing, </journal> <volume> 18(7) </volume> <pages> 823-834, </pages> <year> 1992. </year> <month> 56 </month>
Reference-contexts: Kagaris et al. also study a dynamic version of the quickest paths problem. Y.-C. Hung and G.-H. Chen describe an O (mn 2 ) algorithm to find the quickest path between all pairs of vertices [24] and also consider distributed algorithms in an asynchronous, message-passing model of computation <ref> [25] </ref>. The sequential algorithm described here, which is a slightly modified version of Chen and Chin's algorithm, uses a transformation of the graph to reduce the quickest path problem to a shortest path problem. <p> Chen and Y. Chin [8] O (rm + rn log n) 2 All pairs quickest path Repeated application of (1) O (rmn + rn 2 log n) 3 Y.-C. Hung and G.-H. Chen <ref> [25] </ref> O (mn 2 ) 4 Single pair k quickest paths Section 4.1.1 O (rm + rn log n + k) 5 All pairs k quickest paths Repeated application of (4) O (rmn + rn 2 log n + n 2 k) 6 Improved version of G.-H. Chen and Y.-C.
Reference: [26] <author> Joseph JaJa. </author> <title> An Introduction to Parallel Algorithms. </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: An out-tree is the same as an in-tree, except that edges are directed from nodes to their children. Unless stated otherwise, trees will be in-trees. 1.2 The PRAM Model The parallel random-access machine (PRAM) is a widely used abstract model for parallel computation (see <ref> [26] </ref> and [36] for a description of the PRAM model and a wide variety of PRAM algorithms). It is a parallel version of the sequential random-access machine (RAM) model. The PRAM consists of a set of processors that have private memories and also have access to a shared memory.
Reference: [27] <author> Donald B. Johnson and Tetsuo Mizoguchi. </author> <title> Selecting the Kth element in X + Y and X 1 + X 2 + : : : + X n . SIAM Journal on Computing, </title> <booktitle> 7(2) </booktitle> <pages> 147-153, </pages> <year> 1978. </year>
Reference-contexts: The problem of selecting the Kth smallest element of X is a special case of this problem, when each weight is one. Johnson and Mizoguchi <ref> [27] </ref> give a sequential algorithm for this problem that runs in O (n) time. The algorithm executes a binary search for x fl . The median of X is found, and the sum s of the weights of the array elements less than the median is computed.
Reference: [28] <author> Dimitrios Kagaris, Grammati E. Pantziou, Spyros Tragoudas, and Christos D. Zaroliagis. </author> <title> On the computation of fast data transmissions in networks with capacities and delays. </title> <booktitle> In 4th Workshop on Algorithms and Data Structures, </booktitle> <pages> pages 291-302, </pages> <year> 1995. </year>
Reference-contexts: If there are only r distinct capacities assigned to the edges of the graph, the running time of their algorithm reduces to O (rm + rn log n). A similar algorithm with the same worst-case running time is described by Rosen, Sun and Xue [37]. Kagaris et al. <ref> [28] </ref> give an algorithm which has the same worst-case running time for the general problem, but runs faster if the graph can be decomposed into a small number of subgraphs (called hammocks) satisfying certain properties. Kagaris et al. also study a dynamic version of the quickest paths problem. Y.-C.
Reference: [29] <author> N. Katoh, T. Ibaraki, and H. </author> <title> Mine. An efficient algorithm for K shortest simple paths. </title> <journal> Networks, </journal> <volume> 12 </volume> <pages> 411-427, </pages> <year> 1982. </year>
Reference-contexts: Other variations of the k shortest path problem have been studied and sequential algorithms have been developed for them. Yen [45] gives an algorithm for the significantly harder problem of finding the k shortest simple paths in O (kn 3 ) time. Katoh, Ibaraki and Mine <ref> [29] </ref> describe an O (kn 2 ) algorithm to find the k shortest simple paths in an undirected graph. <p> This step will ensure that each candidate path with capacity c j appears only in P j . The computation of each set P i runs in O (kn 3 ) time for each G i [45], or O (kn 2 ) time if the graph is undirected <ref> [29] </ref>. Removing candidates from these sets and choosing the best k from the remaining paths using a standard linear-time selection algorithm uses only O (rk) time.
Reference: [30] <author> S. Rao Kosaraju and Arthur L. Delcher. </author> <title> Optimal parallel evaluation of tree-structured computations by raking. </title> <booktitle> In VLSI Algorithms and Architectures, Proc. 3rd Aegean Workshop on Computing, volume 319 of Lecture Notes in Computer Science, </booktitle> <pages> pages 101-110. </pages> <publisher> Springer, </publisher> <year> 1988. </year>
Reference-contexts: Miller and Reif [32] gave the original version of tree contraction. Abrahamson et al. [1] and Kosaraju and Delcher <ref> [30] </ref> independently gave the improved version described below, which performs less work. Here, it is assumed that each internal node has exactly two children. If an internal node in the input tree has only one child, an extra dummy child may be added.
Reference: [31] <author> Richard E. Ladner and Michael J. Fischer. </author> <title> Parallel prefix computation. </title> <journal> Journal of the ACM, </journal> <volume> 27(4) </volume> <pages> 831-838, </pages> <year> 1980. </year>
Reference-contexts: The prefix sum problem can be solved easily using O (log n) time on an n-processor EREW PRAM (Ladner and Fischer, <ref> [31] </ref>). Without loss of generality, suppose the size of the array is a power of two. The elements can be assigned to the leaves of a complete binary tree of height log n y .
Reference: [32] <author> Gary L. Miller and John H. Reif. </author> <title> Parallel tree contraction and its application. </title> <booktitle> In Proc. 26th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 478-489, </pages> <year> 1985. </year>
Reference-contexts: After log fl n stages, the remaining candidates can be sorted to find the required element. 1.3.5 Tree Contraction Parallel tree contraction is a useful technique for designing parallel algorithms that work quickly even on poorly-balanced trees. Miller and Reif <ref> [32] </ref> gave the original version of tree contraction. Abrahamson et al. [1] and Kosaraju and Delcher [30] independently gave the improved version described below, which performs less work. Here, it is assumed that each internal node has exactly two children.
Reference: [33] <author> A. Mirzaian and E. Arjomandi. </author> <title> Selection in X + Y and matrices with sorted rows and columns. </title> <journal> Information Processing Letters, </journal> <volume> 20(1) </volume> <pages> 13-17, </pages> <year> 1985. </year>
Reference-contexts: If the vectors X and Y are sorted, then the Cartesian sum matrix is an example of a sorted matrix; the elements of each column and of each row are sorted into non-decreasing order. Frederickson and Johnson [20] and Mirzaian and Arjomandi <ref> [33] </ref> both give O (n) algorithms for selection in an n fi n sorted matrix. A technique similar to the one used by Mirzaian and Arjomandi will be used in Section 2.3 to obtain a parallel algorithm for selection in a matrix with sorted columns. <p> The algorithm uses a technique similar to Mirzaian and Arjomandi's sequential algorithm for selection in a matrix whose rows and columns are both sorted <ref> [33] </ref>. The algorithm uses a recursive routine, Biselect (M; k 1 ; k 2 ), which selects the k 1 th-smallest element and the k 2 th-largest element of a matrix M with sorted columns. <p> In fact, Chen and Hung's algorithm can be improved to run in O (kmn 2 log k) time by replacing an O (k 2 ) computation of the k smallest elements of a k fi k Cartesian sum matrix (see Section 2.2) by the faster algorithm of Mirzaian and Arjomandi <ref> [33] </ref>, but the algorithm presented here is still better when k log k = ! q n + r log n The problem of finding the k quickest simple paths has also been studied ([7], [37]).
Reference: [34] <author> Richard C. Paige and Clyde P. Kruskal. </author> <title> Parallel algorithms for shortest paths problems. </title> <booktitle> In Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pages 14-20, </pages> <year> 1985. </year>
Reference-contexts: In the PRAM model, the shortest path between each pair of vertices can be found using the well-known method of a min/sum transitive closure computation (see, for example, <ref> [34] </ref>). Suppose the vertex set of the graph is f1; : : : ; ng, and let A be the n fi n matrix of edge weights: A ij = w (i; j).
Reference: [35] <author> Margaret Reid-Miller, Gary L. Miller, and Francesmary Modugno. </author> <title> List ranking and parallel tree contraction. </title> <note> In [36], Chapter 3. </note>
Reference-contexts: Tree contraction can be used to compute the minimum ancestor of each node in a node-weighted tree. This application of tree contraction will be used in Chapter 3, below. 8 Computing Minimum Ancestors Suppose each node in a tree is assigned an integer value. Reid-Miller, Miller and Modugno <ref> [35] </ref> give an algorithm to compute, for each node v, the minimum value assigned to any of v's ancestors. Suppose that the minimum of v's ancestors is to be stored in a variable m v . The computation of m v proceeds in two stages.
Reference: [36] <editor> John H. Reif, editor. </editor> <title> Synthesis of Parallel Algorithms. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, Cali-fornia, </address> <year> 1993. </year>
Reference-contexts: An out-tree is the same as an in-tree, except that edges are directed from nodes to their children. Unless stated otherwise, trees will be in-trees. 1.2 The PRAM Model The parallel random-access machine (PRAM) is a widely used abstract model for parallel computation (see [26] and <ref> [36] </ref> for a description of the PRAM model and a wide variety of PRAM algorithms). It is a parallel version of the sequential random-access machine (RAM) model. The PRAM consists of a set of processors that have private memories and also have access to a shared memory.
Reference: [37] <author> J. B. Rosen, S.-Z. Sun, and G.-L. Xue. </author> <title> Algorithms for the quickest path problem and the enumeration of quickest paths. </title> <journal> Computers and Operations Research, </journal> <volume> 18(6) </volume> <pages> 579-584, </pages> <year> 1991. </year>
Reference-contexts: If there are only r distinct capacities assigned to the edges of the graph, the running time of their algorithm reduces to O (rm + rn log n). A similar algorithm with the same worst-case running time is described by Rosen, Sun and Xue <ref> [37] </ref>. Kagaris et al. [28] give an algorithm which has the same worst-case running time for the general problem, but runs faster if the graph can be decomposed into a small number of subgraphs (called hammocks) satisfying certain properties. <p> k Cartesian sum matrix (see Section 2.2) by the faster algorithm of Mirzaian and Arjomandi [33], but the algorithm presented here is still better when k log k = ! q n + r log n The problem of finding the k quickest simple paths has also been studied ([7], <ref> [37] </ref>). This version of the problem appears to be significantly harder, and the best previously known algorithm, presented by Y. Chen [7], runs in O (krn 3 + krn log k) time, or O (krn 2 + krn log k) time for an undirected graph.
Reference: [38] <author> R. Sarnath and Xin He. </author> <title> On parallel selection and searching in partial orders. </title> <booktitle> In 6th International Parallel Processing Symposium, </booktitle> <pages> pages 108-111, </pages> <year> 1992. </year>
Reference-contexts: A technique similar to the one used by Mirzaian and Arjomandi will be used in Section 2.3 to obtain a parallel algorithm for selection in a matrix with sorted columns. Sarnath and He <ref> [38] </ref> use an approach similar to the algorithm described in [20] to develop an EREW PRAM algorithm for selection in a sorted matrix that runs in O (log n log log n log fl n) time and performs O (n log log n) work. <p> Can efficient algorithms be found for dynamic versions of k-best optimization problems? Chapter 2 contained a parallel algorithm for selection in a matrix with ordered columns. Sarnath and He <ref> [38] </ref> gave a PRAM algorithm for selection in a sorted matrix, and gave lower bounds on the required resources. It would be interesting to develop a similar lower bound for selection in a matrix with ordered columns.
Reference: [39] <author> A. Schonhage, M. Paterson, and N. Pippenger. </author> <title> Finding the median. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 13 </volume> <pages> 184-199, </pages> <month> October </month> <year> 1976. </year>
Reference-contexts: This was originally shown by Blum et al. [3]. Schonhage, Paterson and Pippenger <ref> [39] </ref> gave a more complicated algorithm which reduced the number of comparisons by a constant factor. The algorithm of Blum et al. was adapted to run in O (log n log log n) time and O (n) work on an EREW PRAM by Vishkin [43].
Reference: [40] <author> Nambirajan Seshadri and Carl-Erik W. Sundberg. </author> <title> List Viterbi decoding algorithms with applications. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 42(2/3/4 Part I):313-323, </volume> <year> 1994. </year> <month> 57 </month>
Reference-contexts: In 1973, Forney suggested that the algorithm be augmented to output the k most likely state sequences [16]. This problem is called the list Viterbi decoding problem, and is of practical importance in a number of areas, including communications and speech 50 recognition (see [16], <ref> [40] </ref>, [41]). Seshadri and Sundberg describe a list Viterbi algorithm that lists the best k state sequences in O (s 2 T + ksT + k log k) time [40]. <p> Viterbi decoding problem, and is of practical importance in a number of areas, including communications and speech 50 recognition (see [16], <ref> [40] </ref>, [41]). Seshadri and Sundberg describe a list Viterbi algorithm that lists the best k state sequences in O (s 2 T + ksT + k log k) time [40]. They also present a naive parallel algorithm whose running time is (T ). 4.2.3 The Viterbi Algorithm First, Viterbi's sequential algorithm for the Viterbi decoding problem will be described briefly.
Reference: [41] <author> Frank K. Soong and Eng-Fong Huang. </author> <title> A tree-trellis based fast search for finding the N best sentence hypotheses in continuous speech recognition. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 705-708, </pages> <year> 1991. </year>
Reference-contexts: An algorithm for the k shortest paths problem has been applied in this way to solve the list Viterbi decoding problem and improve the performance of a speech recognition system in some simple settings <ref> [41] </ref>. The list Viterbi decoding problem, and a parallel algorithm that solves it will be described in Section 4.2. There is another important motivation for computing the k best solutions to a problem. <p> In 1973, Forney suggested that the algorithm be augmented to output the k most likely state sequences [16]. This problem is called the list Viterbi decoding problem, and is of practical importance in a number of areas, including communications and speech 50 recognition (see [16], [40], <ref> [41] </ref>). Seshadri and Sundberg describe a list Viterbi algorithm that lists the best k state sequences in O (s 2 T + ksT + k log k) time [40].
Reference: [42] <author> Leslie G. Valiant. </author> <title> Parallelism in comparison problems. </title> <journal> SIAM Journal on Computing, </journal> <volume> 4(3) </volume> <pages> 348-355, </pages> <year> 1975. </year>
Reference-contexts: 1 to p are available, the sums S (k) = P k i=1 p i can be computed and the processors numbered S (k 1) + 1 to S (k) can be assigned to task k. 1.3.2 Merging Borodin and Hopcroft [4] describe a CREW PRAM implementation of Valiant's algorithm <ref> [42] </ref> to merge two sorted arrays X and Y of length n and m that runs in O (log log n) time using n + m processors.
Reference: [43] <author> Uzi Vishkin. </author> <title> An optimal parallel algorithm for selection. </title> <booktitle> In Advances in Computing Research, </booktitle> <volume> volume 4, </volume> <pages> pages 79-86. </pages> <publisher> JAI Press, </publisher> <year> 1987. </year>
Reference-contexts: Schonhage, Paterson and Pippenger [39] gave a more complicated algorithm which reduced the number of comparisons by a constant factor. The algorithm of Blum et al. was adapted to run in O (log n log log n) time and O (n) work on an EREW PRAM by Vishkin <ref> [43] </ref>. Vishkin's algorithm is described in Section 2.1, below, where it is modified to solve the more general problem of weighted selection. <p> A better algorithm can be obtained by modifying Vishkin's selection algorithm <ref> [43] </ref> to solve this more general problem. Vishkin's original algorithm selects the kth smallest element of an unsorted array of n integers in O (log n log log n) time on an EREW PRAM with p = n=(log n log log n) processors.
Reference: [44] <author> Andrew J. </author> <title> Viterbi. Error bounds for convolutional codes and an asymptotically optimum decoding algorithm. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> IT-13(2):260-269, </volume> <month> April </month> <year> 1967. </year>
Reference-contexts: Viterbi presented a dynamic programming solution to this problem in 1967 <ref> [44] </ref>. The Viterbi algorithm runs in O (s 2 T ) time. In 1973, Forney suggested that the algorithm be augmented to output the k most likely state sequences [16]. <p> Let f fl (j; y) denote the maximum value of f (x; y) if the system is required to end in state x T = j. A recurrence relation can be developed for the function f fl . The Viterbi algorithm <ref> [44] </ref> is then obtained by applying the technique of dynamic programming to compute the values of f fl using the recurrence. The optimal value of f (x; y) for the given observation sequence y will be computed as max 1js f fl (j; y).

References-found: 44

