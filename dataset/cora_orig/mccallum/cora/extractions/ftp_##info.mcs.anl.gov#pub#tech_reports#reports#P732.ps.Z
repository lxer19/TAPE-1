URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P732.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts98.htm
Root-URL: http://www.mcs.anl.gov
Email: @mcs.anl.gov  
Title: On Implementing MPI-IO Portably and with High Performance  
Author: Rajeev Thakur William Gropp Ewing Lusk fthakur, gropp, luskg 
Date: October 1998  
Address: Argonne, IL 60439, USA  
Affiliation: Mathematics and Computer Science Division Argonne National Laboratory  
Pubnum: Preprint ANL/MCS-P732-1098  
Abstract: We discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. One way to implement MPI-IO portably is to implement it on top of the basic Unix I/O functions (open, lseek, read, write, and close), which are themselves portable. We argue that this approach has limitations in both functionality and performance. We instead advocate an implementation approach that combines a large portion of portable code and a small portion of code that is optimized separately for different machines and file systems. We have used such an approach to develop a high-performance, portable MPI-IO implementation, called ROMIO. In addition to basic I/O functionality, we consider the issues of supporting other MPI-IO features, such as 64-bit file sizes, noncontiguous accesses, collective I/O, asynchronous I/O, consistency and atomicity semantics, user-supplied hints, shared file pointers, portable data representation, file pre-allocation, and some miscellaneous features. We describe how we implemented each of these features on various machines and file systems. The machines we consider are the HP Exemplar, IBM SP, Intel Paragon, NEC SX-4, SGI Origin2000, and networks of workstations; and the file systems we consider are HP HFS, IBM PIOFS, Intel PFS, NEC SFS, SGI XFS, NFS, and any general Unix file system (UFS). We also present our thoughts on how a file system can be designed to better support MPI-IO. We provide a list of features desired from a file system that would help in implementing MPI-IO correctly and with high performance. fl This work was supported by the Mathematical, Information, and Computational Sciences Division subprogram of the Office of Computational and Technology Research, U.S. Department of Energy, under Contract W-31-109-Eng-38; and by the Scalable I/O Initiative, a multiagency project funded by the Defense Advanced Research Projects Agency (contract number DABT63-94-C-0049), the Department of Energy, the National Aeronautics and Space Administration, and the National Science Foundation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baylor and C. Wu. </author> <title> Parallel I/O Workload Characteristics Using Vesta. </title> <editor> In R. Jain, J. Werth, and J. Browne, editors, </editor> <booktitle> Input/Output in Parallel and Distributed Computer Systems, chapter 7, </booktitle> <pages> pages 167-185. </pages> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [2] <author> P. Cao, E. Felten, A. Karlin, and K. Li. </author> <title> Implementation and Performance of Integrated Application-Controlled File Caching, Prefetching, and Disk Scheduling. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 14(4) </volume> <pages> 311-343, </pages> <month> November </month> <year> 1996. </year>
Reference-contexts: In such cases, the MPI-IO implementation cannot use the nonatomic mode on PFS. 3.7 Hints MPI-IO provides a mechanism for the user to pass hints to the implementation. Hints, such as access-pattern information, can help the implementation optimize file access <ref> [2, 21] </ref>. Hints do not change the semantics of the MPI-IO interface; an implementation may choose to ignore all hints, and the program would still be functionally correct. MPI-IO has some predefined hints for specifying file-striping parameters, access patterns, and so on. An implementation is free to define additional hints. <p> The file system must therefore either detect and automatically adapt to changing access patterns [15, 16] or provide an interface for the user to specify the access pattern or caching/prefetching policy <ref> [2, 21] </ref>. 11 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [3] <author> P. Corbett, J. Prost, C. Demetriou, G. Gibson, E. Reidel, J. Zelenka, Y. Chen, E. Felten, K. Li, J. Hartman, L. Peterson, B. Bershad, A. Wolman, and R. Aydt. </author> <title> Proposal for a Common Parallel File System Programming Interface, </title> <note> Version 1.0. On the World-Wide Web at http://www.cs.arizona.edu/sio/api1.0.ps.gz, September 1996. </note>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [8, 3, 10, 14, 19] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [11] instead of the basic Unix I/O functions.
Reference: [4] <author> P. Crandall, R. Aydt, A. Chien, and D. Reed. </author> <title> Input-Output Characteristics of Scalable Parallel Applications. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [5] <author> J. del Rosario, R. Bordawekar, and A. Choudhary. </author> <title> Improved Parallel I/O via a Two-Phase Run-time Access Strategy. </title> <booktitle> In Proceedings of the Workshop on I/O in Parallel Computer Systems at IPPS '93, </booktitle> <pages> pages 56-70, </pages> <month> April </month> <year> 1993. </year> <note> Also published in Computer Architecture News, </note> <month> 21(5) </month> <pages> 31-38, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The merged request can therefore be serviced efficiently, and such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 13, 24, 29] </ref>. 3 An MPI communicator is used in the open call to specify the participating processes, and the communicator could represent any subset (or all) of the processes of the application. 5 Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs <p> ROMIO's collective-I/O implementation is described in detail in [32], and it is a generalization of the two-phase method originally proposed in <ref> [5] </ref> and extended further in [29]. In two-phase I/O, as the name suggests, I/O is performed in two phases: a communication phase and an I/O phase. The communication phase uses interprocess communication to rearrange data into large chunks. <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 13, 24] </ref>. Our opinion is that collective I/O is a fairly complex optimization that, if done within the file system, could complicate the file system considerably.
Reference: [6] <author> S. Fineberg, P. Wong, B. Nitzberg, and C. Kuszmaul. </author> <title> PMPIO|A Portable Implementation of MPI-IO. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 188-195. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear <ref> [6, 12, 22, 23, 33] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [7] <author> I. Foster, D. Kohr, R. Krishnaiyer, and J. Mogill. </author> <title> Remote I/O: Fast Access to Distant Storage. </title> <booktitle> In Proceedings of the Fifth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 14-25. </pages> <publisher> ACM Press, </publisher> <month> November </month> <year> 1997. </year>
Reference-contexts: Another application of ADIO is for implementing remote I/O. An MPI-IO program running on one machine can access files from remote machines by developing an ADIO implementation that accesses data from an ADIO server running at a remote site. Such an implementation is described in <ref> [7] </ref> and also illustrated in Figure 1. A similar abstract-device interface is used in MPICH [9] for implementing MPI portably. 3 Implementing MPI-IO We describe how we implemented each feature of MPI-IO on various machines and file systems. <p> In such a case, the MPI-IO implementation would need to have servers that implement a virtual shared file system on top of the individual file systems on these machines. Another example is when MPI-IO is used to access files from remote machines, as described in <ref> [7] </ref>. 3.11.2 Operating with Multiple MPI-1 Implementations MPI-IO can be implemented in a way that it can operate with any MPI-1 implementation that also has a few functions defined in the MPI-2 external-interfaces chapter.
Reference: [8] <author> G. Gibson, D. Stodolsky, P. Chang, W. Courtwright II, C. Demetriou, E. Ginting, M. Holland, Q. Ma, L. Neal, R. Patterson, J. Su, R. Youssef, and J. Zelenka. </author> <title> The Scotch Parallel Storage Systems. </title> <booktitle> In Proceedings of 40th IEEE Computer Society International Conference (COMPCON 95), </booktitle> <pages> pages 403-410. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> Spring </month> <year> 1995. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [8, 3, 10, 14, 19] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [11] instead of the basic Unix I/O functions.
Reference: [9] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A High-Performance, Portable Implementation of the MPI Message-Passing Interface Standard. </title> <journal> Parallel Computing, </journal> <volume> 22(6) </volume> <pages> 789-828, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: Such an implementation is described in [7] and also illustrated in Figure 1. A similar abstract-device interface is used in MPICH <ref> [9] </ref> for implementing MPI portably. 3 Implementing MPI-IO We describe how we implemented each feature of MPI-IO on various machines and file systems.
Reference: [10] <author> J. Huber, C. Elford, D. Reed, A. Chien, and D. Blumenthal. </author> <title> PPFS: A High Performance Portable Parallel File System. </title> <booktitle> In Proceedings of the 9th ACM International Conference on Supercomputing, </booktitle> <pages> pages 385-394. </pages> <publisher> ACM Press, </publisher> <month> July </month> <year> 1995. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [8, 3, 10, 14, 19] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [11] instead of the basic Unix I/O functions.
Reference: [11] <author> IEEE/ANSI Std. 1003.1. </author> <title> Portable Operating System Interface (POSIX)-Part 1: System Application Program Interface (API) [C Language], </title> <note> 1996 edition. </note>
Reference-contexts: Many research file systems provide their own APIs [8, 3, 10, 14, 19]. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface <ref> [11] </ref> instead of the basic Unix I/O functions. The POSIX interface is an international standard with greater functionality than basic Unix I/O. For example, POSIX supports asynchronous I/O and list-directed I/O. This approach, however, also has limitations. Although POSIX is a standard, it is not yet widely implemented. <p> Some file systems support the POSIX list-directed I/O function lio listio, which allows users to submit multiple I/O requests at a time. This function also has limitations, because of the way it is defined. For one, it is tied too closely to nonblocking (asynchronous) I/O. The POSIX standard <ref> [11] </ref> says that each of the requests in the list will be submitted as a separate nonblocking request. It is known, however, that issuing too many nonblocking I/O requests at a time can lower performance. <p> MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix [28] or POSIX <ref> [11] </ref>. In Unix and POSIX, after a write function returns, the data is guaranteed to be visible to every other process in the system. MPI-IO guarantees that writes from one process are immediately visible only to those processes in the communicator that was used to open the file.
Reference: [12] <author> T. Jones, R. Mark, J. Martin, J. May, E. Pierce, and L. Stanberry. </author> <title> An MPI-IO interface to HPSS. </title> <booktitle> In Proceedings of the Fifth NASA Goddard Conference on Mass Storage Systems, </booktitle> <pages> pages I:37-50, </pages> <month> September </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear <ref> [6, 12, 22, 23, 33] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. <p> HPSS is different from other file systems in its goals and design features; for example, it supports third-party transfer. A group at Lawrence Livermore National Laboratory has implemented MPI-IO on HPSS, and we refer interested readers to <ref> [12] </ref> for a discussion of issues related to implementing MPI-IO on HPSS. By making MPI-IO available everywhere and also delivering high performance, we expect it will be widely used and popular among application programmers.
Reference: [13] <author> D. Kotz. </author> <title> Disk-directed I/O for MIMD Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 15(1) </volume> <pages> 41-74, </pages> <month> February </month> <year> 1997. </year> <month> 13 </month>
Reference-contexts: The merged request can therefore be serviced efficiently, and such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 13, 24, 29] </ref>. 3 An MPI communicator is used in the open call to specify the participating processes, and the communicator could represent any subset (or all) of the processes of the application. 5 Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 13, 24] </ref>. Our opinion is that collective I/O is a fairly complex optimization that, if done within the file system, could complicate the file system considerably.
Reference: [14] <author> O. Krieger and M. Stumm. </author> <title> HFS: A Performance-Oriented Flexible File System Based on Building--Block Compositions. </title> <booktitle> In Proceedings of Fourth Workshop on Input/Output in Parallel and Distributed Systems, </booktitle> <pages> pages 95-108. </pages> <publisher> ACM Press, </publisher> <month> May </month> <year> 1996. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [8, 3, 10, 14, 19] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [11] instead of the basic Unix I/O functions.
Reference: [15] <author> T. Madhyastha and D. Reed. </author> <title> Intelligent, Adaptive File System Policy Selection. </title> <booktitle> In Proceedings of the Sixth Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 172-179. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications [26]. The file system must therefore either detect and automatically adapt to changing access patterns <ref> [15, 16] </ref> or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 21]. 11 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [16] <author> T. Madhyastha and D. Reed. </author> <title> Exploiting Global Input/Output Access Pattern Classification. </title> <booktitle> In Proceedings of SC97: High Performance Networking and Computing. </booktitle> <publisher> ACM Press, </publisher> <month> November </month> <year> 1997. </year>
Reference-contexts: Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications [26]. The file system must therefore either detect and automatically adapt to changing access patterns <ref> [15, 16] </ref> or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 21]. 11 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [17] <author> Message Passing Interface Forum. </author> <title> MPI-2: Extensions to the Message-Passing Interface. </title> <month> July </month> <year> 1997. </year> <note> On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: To overcome these limitations, the MPI Forum defined a new API for parallel I/O (which we call MPI-IO) as part of the MPI-2 standard <ref> [17] </ref>. MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear [6, 12, 22, 23, 33]. <p> The user can specify noncontiguous locations in the file by creating a file view with MPI's derived datatypes <ref> [17] </ref>. Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications [1, 4, 20, 25, 26, 31]. <p> ROMIO implements nonblocking I/O by using the nonblocking I/O functions of the file system where available. On machines and file systems that do not support nonblocking I/O, ROMIO just calls the corresponding blocking I/O functions. 3.5 Consistency Semantics MPI-IO's consistency semantics (Section 9.6 of <ref> [17] </ref>) define the results users can expect with concurrent file accesses from multiple processes. MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix [28] or POSIX [11].
Reference: [18] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard. </title> <note> Version 1.1, June 1995. On the World-Wide Web at http://www.mpi-forum.org/docs/docs.html. </note>
Reference-contexts: We consider reads and writes in which data is contiguous in both memory and file; noncontiguous accesses are considered in Section 3.2. 3 3.1.1 Open MPI File open is a collective function. One of its arguments is an MPI communicator <ref> [18] </ref> that specifies the group of processes that will call this open function and any other MPI-IO collective function that the user may choose to use on the open file. Most file systems, other than Intel PFS, support only the regular Unix open and do not have collective open functions.
Reference: [19] <author> N. Nieuwejaar and D. Kotz. </author> <title> The Galley Parallel File System. </title> <journal> Parallel Computing, </journal> <volume> 23(4) </volume> <pages> 447-476, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs <ref> [8, 3, 10, 14, 19] </ref>. Implementing MPI-IO on top of Unix I/O functions will not be portable to these file systems. An alternative is to implement MPI-IO on top of the POSIX I/O interface [11] instead of the basic Unix I/O functions.
Reference: [20] <author> N. Nieuwejaar, D. Kotz, A. Purakayastha, C. Ellis, and M. </author> <title> Best. File-Access Characteristics of Parallel Scientific Workloads. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 7(10) </volume> <pages> 1075-1089, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [21] <author> R. Patterson, G. Gibson, E. Ginting, D. Stodolsky, and J. Zelenka. </author> <title> Informed Prefetching and Caching. </title> <booktitle> In Proceedings of the 15th Symposium on Operating System Principles, </booktitle> <pages> pages 79-95. </pages> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: In such cases, the MPI-IO implementation cannot use the nonatomic mode on PFS. 3.7 Hints MPI-IO provides a mechanism for the user to pass hints to the implementation. Hints, such as access-pattern information, can help the implementation optimize file access <ref> [2, 21] </ref>. Hints do not change the semantics of the MPI-IO interface; an implementation may choose to ignore all hints, and the program would still be functionally correct. MPI-IO has some predefined hints for specifying file-striping parameters, access patterns, and so on. An implementation is free to define additional hints. <p> The file system must therefore either detect and automatically adapt to changing access patterns [15, 16] or provide an interface for the user to specify the access pattern or caching/prefetching policy <ref> [2, 21] </ref>. 11 10. File Preallocation. It is easy and inexpensive for a file system to provide a function to preallocate disk space for a file.
Reference: [22] <author> J. Prost. MPI-IO/PIOFS. </author> <note> World-Wide Web page at http://www.research.ibm.com/people/p/prost/sections/mpiio.html, 1996. </note>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear <ref> [6, 12, 22, 23, 33] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [23] <author> D. Sanders, Y. Park, and M. Brodowicz. </author> <title> Implementation and Performance of MPI-IO File Access Using MPI Datatypes. </title> <type> Technical Report UH-CS-96-12, </type> <institution> University of Houston, </institution> <month> November </month> <year> 1996. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear <ref> [6, 12, 22, 23, 33] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance.
Reference: [24] <author> K. Seamons, Y. Chen, P. Jones, J. Jozwiak, and M. Winslett. </author> <title> Server-Directed Collective I/O in Panda. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> ACM Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The merged request can therefore be serviced efficiently, and such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 13, 24, 29] </ref>. 3 An MPI communicator is used in the open call to specify the participating processes, and the communicator could represent any subset (or all) of the processes of the application. 5 Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs <p> Leave Collective I/O to the MPI-IO Implementation. It is not entirely clear whether collective I/O is better if performed in the file system or as a library above the file system. Both techniques have been proposed in the literature <ref> [5, 13, 24] </ref>. Our opinion is that collective I/O is a fairly complex optimization that, if done within the file system, could complicate the file system considerably.
Reference: [25] <author> E. Smirni, R. Aydt, A. Chien, and D. Reed. </author> <title> I/O Requirements of Scientific Applications: An Evolutionary View. </title> <booktitle> In Proceedings of the Fifth IEEE International Symposium on High Performance Distributed Computing, </booktitle> <pages> pages 49-59. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [26] <author> E. Smirni and D. Reed. </author> <title> Lessons from Characterizing the Input/Output Behavior of Parallel Scientific Applications. Performance Evaluation: </title> <journal> An International Journal, </journal> <volume> 33(1) </volume> <pages> 27-44, </pages> <month> June </month> <year> 1998. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file. <p> Variable Caching/Prefetching Policies. Parallel applications exhibit such a wide variation in access patterns that any one caching/prefetching policy is unlikely to perform well for all applications <ref> [26] </ref>. The file system must therefore either detect and automatically adapt to changing access patterns [15, 16] or provide an interface for the user to specify the access pattern or caching/prefetching policy [2, 21]. 11 10. File Preallocation.
Reference: [27] <author> H. Stern. </author> <title> Managing NFS and NIS. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <year> 1991. </year>
Reference-contexts: When using the Network File System (NFS), it is not sufficient to call just the Unix read/write functions. Since NFS performs noncoherent client-side caching by default, file consistency is not guaranteed if multiple processes write to a common file <ref> [27] </ref>. Client-side caching must be disabled by locking the portion of the file being accessed, by using fcntl. A lock and unlock are therefore needed across the read/write call. 4. Many research file systems provide their own APIs [8, 3, 10, 14, 19]. <p> Any other process can access the data only after both the writer and reader call MPI File sync. MPI-IO's consistency semantics are therefore automatically guaranteed on file systems that support Unix consistency semantics. NFS, by default, does not <ref> [27] </ref>. To obtain Unix consistency semantics on NFS, ROMIO uses byte-range locking (fcntl) across the reads and writes in order to turn off the noncoherent client-side caching that NFS otherwise performs. Turning off client-side caching reduces performance considerably, but is, nonetheless, necessary for correctness.
Reference: [28] <author> W. Stevens. </author> <title> Advanced Programming in the UNIX Environment. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1992. </year>
Reference-contexts: We provide a list of features desired from a file system that would help in implementing MPI-IO correctly and with high performance. 2 Achieving Portability and Performance The basic Unix I/O functions (open, lseek, read, write, and close) <ref> [28] </ref> are supported without variation on all machines with a Unix-like operating system. One way to implement MPI-IO portably, therefore, is to implement MPI-IO functions on top of the basic Unix I/O functions. <p> MPI-IO's consistency semantics are actually weaker than the consistency semantics in Unix <ref> [28] </ref> or POSIX [11]. In Unix and POSIX, after a write function returns, the data is guaranteed to be visible to every other process in the system.
Reference: [29] <author> R. Thakur and A. Choudhary. </author> <title> An Extended Two-Phase Method for Accessing Sections of Out-of-Core Arrays. </title> <journal> Scientific Programming, </journal> <volume> 5(4) </volume> <pages> 301-317, </pages> <month> Winter </month> <year> 1996. </year>
Reference-contexts: The merged request can therefore be serviced efficiently, and such optimization is broadly referred to as collective I/O. Collective I/O has been shown to be a very important optimization in parallel I/O and can improve performance significantly <ref> [5, 13, 24, 29] </ref>. 3 An MPI communicator is used in the open call to specify the participating processes, and the communicator could represent any subset (or all) of the processes of the application. 5 Since none of the file systems on which ROMIO is implemented perform collective I/O, ROMIO performs <p> ROMIO's collective-I/O implementation is described in detail in [32], and it is a generalization of the two-phase method originally proposed in [5] and extended further in <ref> [29] </ref>. In two-phase I/O, as the name suggests, I/O is performed in two phases: a communication phase and an I/O phase. The communication phase uses interprocess communication to rearrange data into large chunks. In the I/O phase, processes perform parallel I/O in large chunks and therefore obtain high I/O performance.
Reference: [30] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Abstract-Device Interface for Implementing Portable Parallel-I/O Interfaces. </title> <booktitle> In Proceedings of the 6th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 180-187. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> October </month> <year> 1996. </year> <month> 14 </month>
Reference-contexts: We describe such an architecture, called ADIO, which we use in our MPI-IO implementation, ROMIO [33]. 2.1 Abstract-Device Interface for I/O A key component of ROMIO that enables such a portable MPI-IO implementation is an internal layer called ADIO <ref> [30] </ref>. ADIO, an abstract-device interface for I/O, is a mechanism specifically designed for 2 implementing parallel-I/O APIs portably on multiple file systems. We developed ADIO before MPI--IO became a standard, as a means to implement and experiment with various parallel-I/O APIs that existed at the time. <p> We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS API and the original MPI-IO proposal [35] on PFS, PIOFS, Unix, and NFS file systems. By following such an approach, we achieved portability with very low overhead <ref> [30] </ref>. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO portably (see Figure 1), and this MPI-IO implementation is called ROMIO [33].
Reference: [31] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> An Experimental Evaluation of the Parallel I/O Systems of the IBM SP and Intel Paragon Using a Production Application. </title> <booktitle> In Proceedings of the 3rd International Conference of the Austrian Center for Parallel Computation (ACPC) with Special Emphasis on Parallel Databases and Parallel I/O, </booktitle> <pages> pages 24-35. </pages> <booktitle> Lecture Notes in Computer Science 1127. </booktitle> <publisher> Springer-Verlag, </publisher> <month> September </month> <year> 1996. </year>
Reference-contexts: Noncontiguous locations in memory can be specified by using a derived datatype in the read/write call. The ability of users to specify noncontiguous accesses in a single function call is very important, because noncontiguous accesses are very common in parallel applications <ref> [1, 4, 20, 25, 26, 31] </ref>. Most file systems, however, do not provide functions for noncontiguous I/O. The Unix functions readv/writev are widely supported, but they allow noncontiguity only in memory and not in the file.
Reference: [32] <author> R. Thakur, W. Gropp, and E. Lusk. </author> <title> Data Sieving and Collective I/O in ROMIO. </title> <type> Technical Report ANL/MCS-P723-0898, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> August </month> <year> 1998. </year>
Reference-contexts: Furthermore, POSIX allows the requests in the list to be a mixture of reads and writes. All these specifications make it difficult for the POSIX implementation to optimize I/O for the entire list of requests, for example, by performing data sieving <ref> [32] </ref>. In the absence of proper support for noncontiguous I/O from the file system, one way to implement a noncontiguous MPI-IO request is to access each contiguous portion of the request separately by using the regular contiguous read/write functions of the file system. <p> Such an implementation, however, results in a large number of small requests to the file system, and performance degrades drastically <ref> [32] </ref>. ROMIO instead performs an optimization called data sieving to access noncontiguous data with high performance. The basic idea in data sieving is to make large I/O requests to the file system and extract, in memory, the data that is really needed. Details of this optimization can be found in [32]. <p> <ref> [32] </ref>. ROMIO instead performs an optimization called data sieving to access noncontiguous data with high performance. The basic idea in data sieving is to make large I/O requests to the file system and extract, in memory, the data that is really needed. Details of this optimization can be found in [32]. 3.3 Collective I/O MPI-IO provides collective-I/O functions, which must be called by all processes that together opened the file. 3 This property enables the MPI-IO implementation (or file system) to analyze and merge the requests of different processes. <p> ROMIO's collective-I/O implementation is described in detail in <ref> [32] </ref>, and it is a generalization of the two-phase method originally proposed in [5] and extended further in [29]. In two-phase I/O, as the name suggests, I/O is performed in two phases: a communication phase and an I/O phase. <p> Depending on the machine, data sieving performs either slightly better or much better than Unix-style independent I/O. Collective I/O always performs the best and results in I/O bandwidths of hundreds of megabytes/sec. For detailed performance results, see <ref> [32] </ref>. The top figure shows read bandwidth, and the bottom figure shows write bandwidth. 3.4 Nonblocking (Asynchronous) I/O Many file systems support nonblocking I/O, and one way to implement MPI-IO's nonblocking I/O functions is to use the nonblocking functions of the file system. <p> A file system must be able to deliver high performance to multiple simultaneous users and jobs and, therefore, must be as lightweight as possible. We have demonstrated in ROMIO that collective I/O can be effectively implemented on top of the file system and can deliver good performance <ref> [32] </ref>. We therefore believe that until parallel file systems mature sufficiently, it is better to leave collective I/O to the MPI-IO implementation. 12. No shared file pointers. We believe that the file system should not be burdened with the additional load of supporting shared file pointers.
Reference: [33] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> Users Guide for ROMIO: A High-Performance, Portable MPI-IO Implementation. </title> <type> Technical Report ANL/MCS-TM-234, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, </institution> <month> October </month> <year> 1997. </year>
Reference-contexts: MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance. Implementations of MPI-IO, both portable and machine-specific, are beginning to appear <ref> [6, 12, 22, 23, 33] </ref>. In this paper, we discuss the issues involved in implementing MPI-IO portably on multiple machines and file systems and also achieving high performance. <p> We believe that the only way to implement MPI-IO portably with complete functionality and high performance is to have a mechanism that can utilize the special features and functions of each file system. We describe such an architecture, called ADIO, which we use in our MPI-IO implementation, ROMIO <ref> [33] </ref>. 2.1 Abstract-Device Interface for I/O A key component of ROMIO that enables such a portable MPI-IO implementation is an internal layer called ADIO [30]. ADIO, an abstract-device interface for I/O, is a mechanism specifically designed for 2 implementing parallel-I/O APIs portably on multiple file systems. <p> By following such an approach, we achieved portability with very low overhead [30]. Now that MPI-IO has emerged as the standard, we use ADIO as a mechanism for implementing MPI-IO portably (see Figure 1), and this MPI-IO implementation is called ROMIO <ref> [33] </ref>. ADIOADIO MPI-IO Unix NFS NEC XFSPIOFS IBM Intel PFS HFS Portable Implementation network remote site Implementations File-system-specific called ADIO, and ADIO is optimized separately for different file systems. Another application of ADIO is for implementing remote I/O.
Reference: [34] <author> R. Thakur, E. Lusk, and W. Gropp. </author> <title> I/O in Parallel Applications: The Weakest Link. </title> <journal> International Journal of High Performance Computing Applications, </journal> <note> 1998. To appear. </note>
Reference-contexts: Furthermore, the Unix API is not an appropriate API for parallel I/O; it lacks some of the features necessary to express access patterns common in parallel programs, such as noncontiguous accesses and collective I/O, resulting in poor performance <ref> [34] </ref>. To overcome these limitations, the MPI Forum defined a new API for parallel I/O (which we call MPI-IO) as part of the MPI-2 standard [17]. MPI-IO is a comprehensive API with many features intended specifically for I/O parallelism, portability, and high performance.
Reference: [35] <author> The MPI-IO Committee. </author> <title> MPI-IO: A Parallel File I/O Interface for MPI, </title> <note> Version 0.5. On the World-Wide Web at http://parallel.nas.nasa.gov/MPI-IO, April 1996. </note>
Reference-contexts: ADIO thus separates the machine-dependent and machine-independent aspects involved in implementing an API. The ADIO implementation on a particular file system is optimized for that file system. We used ADIO to implement Intel's PFS API and subsets of IBM's PIOFS API and the original MPI-IO proposal <ref> [35] </ref> on PFS, PIOFS, Unix, and NFS file systems. By following such an approach, we achieved portability with very low overhead [30].
Reference: [36] <author> R. Watson and R. Coyne. </author> <title> The Parallel I/O Architecture of the High-Performance Storage System (HPSS). </title> <booktitle> In Proceedings of the Fourteenth IEEE Symposium on Mass Storage Systems, </booktitle> <pages> pages 27-44. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> September </month> <year> 1995. </year> <month> 15 </month>
Reference-contexts: The discussion in this paper covers numerous file systems|almost all the file systems on commercially available machines. An important storage system that we did not discuss (mainly because ROMIO is not implemented on it) is HPSS <ref> [36] </ref>. HPSS is different from other file systems in its goals and design features; for example, it supports third-party transfer. A group at Lawrence Livermore National Laboratory has implemented MPI-IO on HPSS, and we refer interested readers to [12] for a discussion of issues related to implementing MPI-IO on HPSS.
References-found: 36

