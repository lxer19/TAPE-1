URL: http://www.cs.cmu.edu/~amulet/papers/soundinamulet.ps
Refering-URL: http://c.gp.cs.cmu.edu:5103/afs/cs/project/amulet/www/amulet-papers.html
Root-URL: 
Email: bam@cs.cmu.edu  
Title: Easily Adding Sound Output to Interfaces  
Author: Brad A. Myers and Kenneth A. Strickland 
Keyword: sound, auditory output, earcons, text-to-speech, toolkits, multimodal interfaces, Amulet, Andalusite.  
Web: http://www.cs.cmu.edu/~bam  
Address: Pittsburgh, PA 15213  
Affiliation: Human Computer Interaction Institute School of Computer Science Carnegie Mellon University  
Abstract: Adding sound output to interfaces is a very difficult task with todays toolkits, even though there are many situations in which it would be useful and effective. We have designed an architecture that makes it very easy to add sound output to an interface. Any interaction behavior, animation, or command can be augmented with sounds to occur at the beginning or end, or for the duration. Parameters of the sound, such as the speed or volume can be easily tied to properties of the data using constraints. Two different sound objects are currently supplied: one for playing recorded sounds, and the other for text-to-speech. The text-to-speech sound object can be used to quickly build various kinds of screen readers. Easy-to-use mechanisms give the programmer complete control over interrupting and preempting when multiple sounds are played at the same time. Because sound output can be added to an existing application with as little as a single extra line of code, we expect that this new mechanism will make it easy for researchers and d evelopers to investigate the use of sound in a wide variety of applications. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Beaudouin-Lafon, M. and Gaver, </author> <title> W.W. ENO: Synthesizing Structured Sound Spaces, </title> <booktitle> in Proceedings UIST'94: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1994. </booktitle> <address> Marina del Rey, CA: </address> <pages> pp. 49-57. </pages>
Reference-contexts: Mercator [6] aims to allow blind users to work with graphical interfaces. The first version identified problems with X/11 and the Xt toolkit for integrating sound, which were fixed in later versions of X. An important goal of Andalusite is to make Mercator-like interfaces significantly easier to build. ENO <ref> [1] </ref> concentrated on describing the various acoustic properties of sounds, to make it easier to generate them, as opposed to our system which concentrates on making it easy to integrate the sounds with the rest of the interface, and to control the timing and coordination of sounds with animations and interactions. <p> In the future, we plan to add more sophisticated control for generating sounds dynamically, similar to the techniques provided by ENO <ref> [1] </ref>. Multiple channels of audio are supported, up to the limit available on the hardware. <p> If it is a menu item, it reads the menu item. For graphical objects, a short description is generated using a very simple algorithm based on one property and the type (e.g., blue rectangle). A more sophisticated algorithm for generating descriptions, such as used in ENO <ref> [1] </ref> would be very appropriate and easy to incorporate.
Reference: 2. <author> Blattner, M., Sumikawa, D.A., and Greenberg, R.M., Earcons and Icons: </author> <title> Their Structure and Common Design Principles. </title> <booktitle> Human-Computer Interaction , 1989. </booktitle> <volume> 4 (1): </volume> <pages> pp. 11-44. </pages>
Reference-contexts: The timing of the sounds, the temporal scheduling of sounds to control overlapping, interrupting and sequencing, and the coordination of foreground and background sounds are all easily handled, often with a single line of code. There are many potential benefits for using sound in interfaces <ref> [2, 5, 7] </ref> . Expert game players do better when sound is turned on, showing that it provides strategically critical information [5]. Sound provides an extra channel that can be used to help present complex information [3]. <p> A sound object with ENOs features would be an excellent addition to our system. The Earcons <ref> [2] </ref> work first investigated the different parameters of sounds that can be meaningfully interpreted by people, and then applied these to create different sounds that represented different features of an interface. Using Earcons in palettes was shown to reduce users errors [4]. <p> We will also be creating new kinds of sound objects like playing MIDI files, and more capabilities for composing sounds from the components like pitch, rhythm, timbre, register, dynamics, etc. We would like to create a variety of applications that use Earcons <ref> [2] </ref> and provide a clipsound library that can be used by applications. We would also like to connect the Andalusite sound system with other new features in Amulet, such as the multiuser support [9], to investigate the use of sounds to support collaboration, as recommended by Gaver [8].
Reference: 3. <author> Bly, S. </author> <title> Presenting Information In Sound, </title> <booktitle> in Proceedings Human Factors in Computer Systems. 1982. </booktitle> <address> Gaithersburg, MD: </address> <pages> pp. 371-375. </pages>
Reference-contexts: There are many potential benefits for using sound in interfaces [2, 5, 7] . Expert game players do better when sound is turned on, showing that it provides strategically critical information [5]. Sound provides an extra channel that can be used to help present complex information <ref> [3] </ref>. It provides complementary information to the graphics to provide feedback for what you are doing, for notification, for beeps on errors, or for awareness of what the system and other people are doing. Sound also enables monitoring of background processes while performing a foreground task. <p> For the future, we plan to finish the implementations on the PC and Unix. We want to expand the number and types of applications that use sound. We particularly want to explore using sounds to visualize data, along the lines of Bly <ref> [3] </ref>. We will also be creating new kinds of sound objects like playing MIDI files, and more capabilities for composing sounds from the components like pitch, rhythm, timbre, register, dynamics, etc.
Reference: 4. <author> Brewster, S.A. </author> <title> Using Earcons to Improve the Usability of Tool Palettes, </title> <booktitle> in Adjunct Proceedings SIGCHI'98: Conference Summary: Human Factors in Computing Systems. 1998. </booktitle> <address> Los Angeles, CA: </address> <pages> pp. 297-298. </pages>
Reference-contexts: Sound also enables monitoring of background processes while performing a foreground task. For example, an experiment showed that when sound is used in addition to graphics to show the mode of a palette, users make fewer errors <ref> [4] </ref>. In multiuser applications, sound provides very helpful feedback about what the other users are doing, and can help with awareness [8]. We rely on sound for information in our everyday lives [7]. <p> The Earcons [2] work first investigated the different parameters of sounds that can be meaningfully interpreted by people, and then applied these to create different sounds that represented different features of an interface. Using Earcons in palettes was shown to reduce users errors <ref> [4] </ref>. Andalusite aims to make interfaces using Earcons easier to build. ScriptX [10] is a programming language designed specifically for multimedia. It has built-in primitives like clocks for controlling animations and for playing sounds.
Reference: 5. <author> Buxton, W., </author> <title> Introduction to This Special Issue on Nonspeech Audio . Human-Computer Interaction , 1989. </title> <type> 4 (1): </type> <pages> pp. 1-9. </pages>
Reference-contexts: INTRODUCTION In the introduction to the 1989 special issue of Human-Computer Interaction on nonspeech audio <ref> [5] </ref>, Buxton claims that many of the logistical problems in exploring the use of audio signals have been overcome. Indeed, todays PCs and Macintoshes contain sophisticated support for sound to support multimedia games and the worldwide-web. <p> The timing of the sounds, the temporal scheduling of sounds to control overlapping, interrupting and sequencing, and the coordination of foreground and background sounds are all easily handled, often with a single line of code. There are many potential benefits for using sound in interfaces <ref> [2, 5, 7] </ref> . Expert game players do better when sound is turned on, showing that it provides strategically critical information [5]. Sound provides an extra channel that can be used to help present complex information [3]. <p> There are many potential benefits for using sound in interfaces [2, 5, 7] . Expert game players do better when sound is turned on, showing that it provides strategically critical information <ref> [5] </ref>. Sound provides an extra channel that can be used to help present complex information [3]. It provides complementary information to the graphics to provide feedback for what you are doing, for notification, for beeps on errors, or for awareness of what the system and other people are doing.
Reference: 6. <editor> Edwards, W.K. and Mynatt, E.D. </editor> <title> An Architecture for Transforming Graphical Interfaces, </title> <booktitle> in Proceedings UIST'94: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1994. </booktitle> <address> Marina del Rey, CA: </address> <pages> pp. 39-47. </pages>
Reference-contexts: Even with computers, the whine of the disk drive tells us whether Easily Adding Sound Output to Interfaces - 2 - **Submitted for Publication** they are operating as expected. As a final motivation, sound is crucial for people with visual difficulties <ref> [6] </ref>. Our new sound system is called Andalusite, which is a kind of yellow-green gemstone. Andalusite stands for A mulets N ew D evelopment A ugments the L ook-and-feel U sing S ounds I ncluding T ext-to-speech and E ffects. <p> It uses a context-free grammar for specifying the output which matches the grammar needed for parsing the speech input. This was needed to handle the complex auditory messages composed of generated speech and recorded sounds. Grammars do not match well with direct manipulation user interfaces, however. Mercator <ref> [6] </ref> aims to allow blind users to work with graphical interfaces. The first version identified problems with X/11 and the Xt toolkit for integrating sound, which were fixed in later versions of X. An important goal of Andalusite is to make Mercator-like interfaces significantly easier to build. <p> For example, the following causes a click each time the mouse moves to a different menu item: inter = Am_Menu.Get (Interactor); inter.Set (Interim_Sound,Am_Load_Sound (click.snd)); The generated sounds can be based on semantic properties of the data and interface, and not just the graphical presentation, as required by Mercator <ref> [6] </ref> since all the properties are represented as slots of the data objects, and constraints can be written to compute the properties of the sounds based on the properties of the data objects. We created a mouse-based screen reader Interactor with just a few lines of code (see Figure 4).
Reference: 7. <author> Gaver, </author> <title> W.W., The SonicFinder: An Interface That Uses Auditory Icons. Human-Computer Interaction, </title> <booktitle> 1989. </booktitle> <volume> 4 (1): </volume> <pages> pp. 67-94. </pages>
Reference-contexts: The timing of the sounds, the temporal scheduling of sounds to control overlapping, interrupting and sequencing, and the coordination of foreground and background sounds are all easily handled, often with a single line of code. There are many potential benefits for using sound in interfaces <ref> [2, 5, 7] </ref> . Expert game players do better when sound is turned on, showing that it provides strategically critical information [5]. Sound provides an extra channel that can be used to help present complex information [3]. <p> In multiuser applications, sound provides very helpful feedback about what the other users are doing, and can help with awareness [8]. We rely on sound for information in our everyday lives <ref> [7] </ref>. Even with computers, the whine of the disk drive tells us whether Easily Adding Sound Output to Interfaces - 2 - **Submitted for Publication** they are operating as expected. As a final motivation, sound is crucial for people with visual difficulties [6]. <p> As an example of how constraints are very useful for sound objects, here is a constraint to compute the pitch of a sound (which would be controlled by the Speed slot) based on the size of an object, as described in SonicFinder <ref> [7] </ref>: // define a constraint to get the speed from the size float speed_from_size_constraint (self) - Am_Object ref_obj = Get_Obj_Over (self); int size = ref_obj.Get (Size); if (size &gt; 1000) return 0.5; //slower else if (size &gt; 500) return 1.0; else return 2.0; //faster - // put this constraint into the <p> The typical way that sounds are used in games and systems such as SonicFinder <ref> [7] </ref> is that a sound is played in response to the users action with the mouse and keyboard. For example, clicking on an object might start an animation and a sound, or moving the mouse might trigger sounds based on where the mouse is located. <p> be specified as: Am_Object dancer = Am_Load_Bitmap (Dancing_rabbit.gif) Am_Object dancer_animation = Am_Animation.Create (); Am_Object music = Am_Load_Sound (dancing_music.wav); music.Set (Repeat_Count, Infinity); //play the song during the animation dancer_animation.Set (Interim_Sound, music); //make the image slot of the dancer be animated dancer.Set (Image, dancer_animation); As another example, in Brderbunds KidPix and SonicFinder <ref> [7] </ref> dragging objects makes a scratching noise, and there is a noise like a screech of brakes when the movement stops. SonicFinder has the additional property that the pitch of the sound depends on the size of the object.
Reference: 8. <author> Gaver, W.W., Smith, R.B., and O'Shea, T. </author> <title> Effective Sounds in Complex Systems: </title> <booktitle> The ARKOLA Simulation, in Proceedings SIGCHI'91: Human Factors in Computing Systems. 1991. </booktitle> <address> New Orleans, LA: </address> <pages> pp. 85-90. </pages>
Reference-contexts: For example, an experiment showed that when sound is used in addition to graphics to show the mode of a palette, users make fewer errors [4]. In multiuser applications, sound provides very helpful feedback about what the other users are doing, and can help with awareness <ref> [8] </ref>. We rely on sound for information in our everyday lives [7]. Even with computers, the whine of the disk drive tells us whether Easily Adding Sound Output to Interfaces - 2 - **Submitted for Publication** they are operating as expected. <p> We would also like to connect the Andalusite sound system with other new features in Amulet, such as the multiuser support [9], to investigate the use of sounds to support collaboration, as recommended by Gaver <ref> [8] </ref>. Another interesting future direction is to support more complex compositions of sounds, for example into bars and measures, which would be necessary to create longer and more sophisticated musical pieces.
Reference: 9. <author> Huebner, J. and Myers, </author> <title> B.A. Easily Programminable Shared Objects For Peer-To-Peer Distributed Applications, </title> <note> in Submitted for Publication. </note> <year> 1998. </year>
Reference-contexts: We would like to create a variety of applications that use Earcons [2] and provide a clipsound library that can be used by applications. We would also like to connect the Andalusite sound system with other new features in Amulet, such as the multiuser support <ref> [9] </ref>, to investigate the use of sounds to support collaboration, as recommended by Gaver [8]. Another interesting future direction is to support more complex compositions of sounds, for example into bars and measures, which would be necessary to create longer and more sophisticated musical pieces.
Reference: 10. <author> Kaleida Labs, I., </author> <title> ScriptX Architecture and Components Guide, Version 1.0. </title> <booktitle> 1994, </booktitle> <address> Mountain View, CA: </address>
Reference-contexts: Using Earcons in palettes was shown to reduce users errors [4]. Andalusite aims to make interfaces using Earcons easier to build. ScriptX <ref> [10] </ref> is a programming language designed specifically for multimedia. It has built-in primitives like clocks for controlling animations and for playing sounds.
Reference: 11. <author> Myers, B.A., et al. </author> , <title> The Amulet V3.0 Reference Manual. </title> <institution> Carnegie Mellon University Computer Science Department, CMU-CS-95-166-R2, </institution> <year> 1997, </year>
Reference-contexts: It is a Choice_Interactor that is specified to be always running. It selects any object in any window anywhere on the screen. We also specified that it does not consume any events, but just processes the events and then passes them on to other Interactors (see the Amulet manual <ref> [11] </ref> for a full description of the parameters and capabilities of Interactors). Thus, the screen reader sees every object the mouse moves over, but allows all the usual behaviors to also operate.
Reference: 12. <author> Myers, B.A. and Kosbie, D. </author> <title> Reusable Hierarchical Command Objects, </title> <booktitle> in Proceedings CHI'96: Human Factors in Computing Systems. 1996. </booktitle> <address> Vancouver, BC, Canada: </address> <pages> pp. 260-267. </pages>
Reference-contexts: For example, in Amulet, the interactive behavior of objects can be defined completely independently from their graphical look by attaching Interactor objects to the graphics. Command objects <ref> [12] </ref> encapsulate the complete information about operations and support undo. Animations for objects can be added with a single line of code by attaching an animation constraint [14] to various properties.
Reference: 13. <author> Myers, B.A., et al. </author> , <title> The Amulet Environment: New Models for Effective User Interface Software Development. </title> <journal> IEEE Transactions on Software Engineering, 1997. </journal> <volume> 23 (6): </volume> <pages> pp. 347-365. </pages>
Reference-contexts: Amulet <ref> [13] </ref> is a C++ toolkit that runs on X/11, Windows 95, Windows NT, and the Macintosh. One of the important goals of Amulet is to enable sophisticated features to be provided to end users without requiring much coding by designers. <p> MachineDependent Sound System Andalucite Low-Level Andalucite High-Level Application Programs Low-Level Interface Sound Objects The low-level part of the Andalusite interface provides Sound Objects which contain a number of parameters. Amulet uses a prototype-instance object system <ref> [13] </ref> in which parameters of objects are represented as instance variables which can be local or inherited. The advantage to the programmer is that any parameters that are not relevant or needed can be easily left at their default values. <p> Computing Parameters with Constraints In Amulet, any slot of an object instead of containing a regular value like an integer or a string, can contain a constraint, which is an expression that calculates the value <ref> [13] </ref>. Constraints are automatically reevaluated whenever anything changes that the expression depends on. Constraints can be arbitrary C++ code, and a large library of predefined constraints is available. This makes it very easy to have the values of objects depend on aspects of the applications data. <p> the interim sound would not start, or conversely the mouse might move immediately, and the interim sound would interrupt or play 2 Interactors have many parameters for controlling the behavior such as the specific buttons that start and stop, gridding, the form of feedback, the maximum and minimum sizes, etc. <ref> [13] </ref>. in parallel with the start sound. Therefore, the Next_Sound chain is used to schedule the interim sound for whenever the start sound completes. If there is no start sound, then the interim sound is started immediately when the animation or Interactor starts.
Reference: 14. <author> Myers, B.A., et al. </author> <title> Easily Adding Animations to Interfaces Using Constraints, </title> <booktitle> in Proceedings UIST'96: ACM SIGGRAPH Symposium on User Interface Software and Technology. 1996. </booktitle> <address> Seattle, WA: </address> <pages> pp. 119-128. </pages> <address> http://www.cs.cmu.edu/~amulet. </address>
Reference-contexts: Command objects [12] encapsulate the complete information about operations and support undo. Animations for objects can be added with a single line of code by attaching an animation constraint <ref> [14] </ref> to various properties. The animation constraint detects changes to the value of the slot to which it is attached, and causes the slot to instead take on a series of values interp olated between the original and new values. <p> Amulet introduced the idea of an animation constraint <ref> [14] </ref> which detects changes to the value of the slot to which it is attached. When the value is set, the animation constraint restores the original value, and causes the slot to take on a series of values interp olated between the original and new values. <p> Text-Edit-Interactors call the Interim_Do method on every keystroke. These options for the interim sound can also be useful for animations. In an animation, the Interim_Do method is called every so many clock ticks, with the interval specified as a parameter of the animation <ref> [14] </ref>. This provides an easy way to play a sound at a regular interval, and also supports synchronizing sounds with the interim increments of an animation. <p> Figure 3 shows an example. Boink! it plays the boink sound which is in its Sound_At_Stop slot. Finally, a sound can be attached to a command object . Rather than using a call-back procedure as in other toolkits, Amulet allocates a command object and calls its Do method <ref> [14] </ref>. Amulets commands also provide slots and methods to handle undo, selective undo and repeat, and enabling and disabling the command (graying it out). <p> Other properties of the interface could similarly be signaled by changes in the voice parameters. IMPLEMENTATION The Andalusite sound system was added to Amulet without significant changes to the low-level Amulet architecture. Amulet already had a built-in facility for animations <ref> [14] </ref> that deals with time-based phenomenon. The animations can operate in the background driven by timers. The sound system uses the same low-level timer mechanism. Each timer can have a different timeout that determines when it wants the next tick.
Reference: 15. <author> Stifelman, L. </author> <title> A Tool to Support Speech and NonSpeech Audio Feedback Generation in Audio Interfaces, </title> <booktitle> in Proceedings UIST'95: Eighth Annual Symposium on User Interface Software and Technology. </booktitle> <year> 1995. </year> <pages> pp. 171-179. </pages>
Reference-contexts: RELATED WORK Most research work on auditory user interfaces has concentrated on investigating new uses of sound and on how sounds can be constructed from components. There has been very little prior work on how to integrate sounds with other modalities. Conversational VoiceNotes <ref> [15] </ref> addressed sound-only user interfaces, such as for telephones. It uses a context-free grammar for specifying the output which matches the grammar needed for parsing the speech input. This was needed to handle the complex auditory messages composed of generated speech and recorded sounds.
References-found: 15

