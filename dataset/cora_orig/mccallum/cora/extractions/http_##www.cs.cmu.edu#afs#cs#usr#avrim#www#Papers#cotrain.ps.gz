URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/cotrain.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/usr/avrim/www/Papers/pubs.html
Root-URL: 
Email: avrim+@cs.cmu.edu  mitchell+@cs.cmu.edu  
Title: Combining Labeled and Unlabeled Data with Co-Training  
Author: Avrim Blum Tom Mitchell 
Note: This research was supported in part by the DARPA HPKB program under contract F30602-97-1-0215 and by NSF National Young Investigator grant CCR-9357793.  
Address: Pittsburgh, PA 15213-3891  Pittsburgh, PA 15213-3891  
Affiliation: School of Computer Science Carnegie Mellon University  School of Computer Science Carnegie Mellon University  
Abstract: We consider the problem of using a large unlabeled sample to boost performance of a learning algorithm when only a small set of labeled examples is available. In particular, we consider a problem setting motivated by the task of learning to classify web pages, in which the description of each example can be partitioned into two distinct views. For example, the description of a web page can be partitioned into the words occurring on that page, and the words occurring in hyperlinks that point to that page. We assume that either view of the example would be sufficient for learning if we had enough labeled data, but our goal is to use both views together to allow inexpensive unlabeled data to augment a much smaller set of labeled examples. Specifically, the presence of two distinct views of each example suggests strategies in which two learning algorithms are trained separately on each view, and then each algorithm's predictions on new unlabeled examples are used to enlarge the training set of the other. Our goal in this paper is to provide a PAC-style analysis for this setting, and, more broadly, a PAC-style framework for the general problem of learning from both labeled and unlabeled data. We also provide empirical results on real web-page data indicating that this use of unlabeled examples can lead to significant improvement of hypotheses in practice. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Craven, D. Freitag, A. McCallum, T. Mitchell, K. Nigam, and C.Y. Quek. </author> <title> Learning to extract symbolic knowledge from the world wide web. </title> <type> Technical report, </type> <institution> Carnegie Mellon University, </institution> <month> Jan-uary </month> <year> 1997. </year>
Reference-contexts: One example of this is web-page classification. Suppose that we want a program to electronically visit some web site and download all the web pages of interest to us, such as all the CS faculty member pages, or all the course home pages at some university <ref> [1] </ref>. To train such a system to automatically classify web pages, one would typically rely on hand labeled web pages. These labeled examples are fairly expensive to obtain because they require human effort. <p> Applying Lemma 1, we have the theorem. 6 EXPERIMENTS In order to test the idea of co-training, we applied it to the problem of learning to classify web pages. This particular experiment was motivated by a larger research effort <ref> [1] </ref> to apply machine learning to the problem of extracting information from the world wide web. The data for this experiment 4 consists of 1051 web pages collected from Computer Science department web sites at four universities: Cornell, University of Wash-ington, University of Wisconsin, and University of Texas.
Reference: [2] <author> S. E. Decatur. </author> <title> PAC learning with constant-partition classification noise and applications to decision tree induction. </title> <booktitle> In Proceedings of the Fourteenth International Conference on Machine Learning, </booktitle> <pages> pages 83-91, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: We expect at least one hypothesis to be good since the procedure when ff and fi are known can be viewed as a probability distribution over these m + 1 experiments. The (ff; fi) classification noise model can be thought of as a kind of constant-partition classification noise <ref> [2] </ref>. However, the results in [2] require that each noise rate be less than 1=2. We will need the stronger statement presented here, namely that it suffices to assume only that the sum of ff and fi is less than 1. Proof of Theorem 1. <p> The (ff; fi) classification noise model can be thought of as a kind of constant-partition classification noise <ref> [2] </ref>. However, the results in [2] require that each noise rate be less than 1=2. We will need the stronger statement presented here, namely that it suffices to assume only that the sum of ff and fi is less than 1. Proof of Theorem 1.
Reference: [3] <author> A.P. Dempster, N.M. Laird, and D.B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> 39 </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: We now discuss relations between our setting and other methods that have been used for combining labeled and unlabeled data. One standard approach to learning with missing values (e.g., such as when some of the labels are unknown) is the EM algorithm <ref> [3] </ref>. The EM algorithm is typically analyzed under the assumption that the data is generated according to some simple known parametric model.
Reference: [4] <author> Richard O. Duda and Peter E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: 1 INTRODUCTION In many machine learning settings, unlabeled examples are significantly easier to come by than labeled ones <ref> [4, 15] </ref>. One example of this is web-page classification. Suppose that we want a program to electronically visit some web site and download all the web pages of interest to us, such as all the CS faculty member pages, or all the course home pages at some university [1].
Reference: [5] <author> Z. Ghahramani and M. I. Jordan. </author> <title> Supervised learning from incomplete data via an EM approach. </title> <booktitle> In Advances in Neural Information Processing Systems (NIPS 6). </booktitle> <publisher> Morgan Kauffman, </publisher> <year> 1994. </year>
Reference-contexts: We call this type of bootstrapping co-training, and it has a close connection to bootstrapping from incomplete data in the Expectation-Maximization setting; see, for instance, <ref> [5, 13] </ref>. The question this raises is: is there any reason to believe co-training will help? Our goal is to address this question by developing a PAC-style theoretical framework to better understand the issues involved in this approach.
Reference: [6] <author> S. A. Goldman and M. J. Kearns. </author> <title> On the complexity of teaching. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 50(1) </volume> <pages> 20-31, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: In terms of other PAC-style models, we can think of our setting as somewhat in between the uniform distribution model, in which the distribution is particularly neutral, and teacher models <ref> [6, 8] </ref> in which examples are being supplied by a helpful oracle. 2.1 A BIPARTITE GRAPH REPRESENTATION One way to look at the co-training problem is to view the distribution D as a weighted bipartite graph, which we write as G D (X 1 ; X 2 ), or just G
Reference: [7] <author> A.G. Hauptmann and M.J. Witbrock. Informedia: </author> <title> News-on-demand multimedia information acquisition and retrieval. </title> <editor> In M. Maybury, editor, </editor> <booktitle> Intelligent Multimedia Information Retrieval, </booktitle> <year> 1997. </year>
Reference-contexts: We conjecture that there are many practical learning problems that fit or approximately fit the co-training model. For example, consider the problem of learning to classify segments of television broadcasts <ref> [7, 14] </ref>. We might be interested, say, in learning to identify televised segments containing the US President. Here X 1 could be the set of possible video images, X 2 the set of possible audio signals, and X their cross product.
Reference: [8] <author> J. Jackson and A. Tomkins. </author> <title> A computational model of teaching. </title> <booktitle> In Proceedings of the Fifth Annual Workshop on Computational Learning The ory, </booktitle> <pages> pages 319-326. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: In terms of other PAC-style models, we can think of our setting as somewhat in between the uniform distribution model, in which the distribution is particularly neutral, and teacher models <ref> [6, 8] </ref> in which examples are being supplied by a helpful oracle. 2.1 A BIPARTITE GRAPH REPRESENTATION One way to look at the co-training problem is to view the distribution D as a weighted bipartite graph, which we write as G D (X 1 ; X 2 ), or just G
Reference: [9] <author> D. R. Karger. </author> <title> Random sampling in cut, flow, and network design problems. </title> <booktitle> In Proceedings of the Twenty-Sixth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 648-657, </pages> <month> May </month> <year> 1994. </year>
Reference-contexts: If m t jSj, the above formula is approximately X s j s j m in analogy to Equation 1. In fact, we can use recent results in the study of random graph processes <ref> [9] </ref> to describe quantitatively how 1 To make this more plausible in the context of web pages, think of x 1 as not the document itself but rather some small set of attributes of the document. we expect the components in G S to converge to those of G D as <p> Thus, the expected number of unlabeled samples needed for this to occur is at least 1=ff H . Of course, there are many cuts in H and to have a spanning tree one must include at least one edge from every cut. Nonetheless, Karger <ref> [9] </ref> shows that this is nearly sufficient as well. Specifically, Theorem 2.1 of [9] shows that O ((log N )=ff H ) unlabeled samples are sufficient to ensure that a spanning tree is found with high probability. 2 So, if ff = min H fff H g, then O ((log N <p> Of course, there are many cuts in H and to have a spanning tree one must include at least one edge from every cut. Nonetheless, Karger <ref> [9] </ref> shows that this is nearly sufficient as well. Specifically, Theorem 2.1 of [9] shows that O ((log N )=ff H ) unlabeled samples are sufficient to ensure that a spanning tree is found with high probability. 2 So, if ff = min H fff H g, then O ((log N )=ff) unlabeled samples are sufficient to ensure that the number of connected components
Reference: [10] <author> D. R. Karger. </author> <title> Random sampling in cut, flow, and network design problems. </title> <note> Journal version draft, </note> <year> 1997. </year>
Reference-contexts: In fact, Karger in <ref> [10] </ref> handles this conversion formally. upper bound on the expected number of labeled exam-ples needed to cover all of D.
Reference: [11] <author> M. Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the Twenty-Fifth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392-401, </pages> <year> 1993. </year>
Reference-contexts: Thus, for instance, the conditional independence assumption implies that any concept class learnable in the Statistical Query model <ref> [11] </ref> is learnable from unlabeled data and an initial weakly-useful predictor. Before proving the theorem, it will be convenient to define a variation on the standard classification noise model where the noise rate on positive examples may be different from the noise rate on negative examples.
Reference: [12] <author> D. D. Lewis and M. Ringuette. </author> <title> A comparison of two learning algorithms for text categorization. </title> <booktitle> In Third Annual Symposium on Document Analysis and Information Retrieval, </booktitle> <pages> pages 81-93, </pages> <year> 1994. </year>
Reference-contexts: Classifiers were trained separately for x 1 and for x 2 , using the naive Bayes algorithm. We will refer to these as the page-based and the hyperlink-based classifiers, respectively. This naive Bayes algorithm has been empirically observed to be successful for a variety of text-categorization tasks <ref> [12] </ref>. The co-training algorithm we used is described in Table 1. Given a set L of labeled examples and a set U of unlabeled examples, the algorithm first creates a smaller pool U 0 containing u unlabeled examples. It then iterates the following procedure.
Reference: [13] <author> Joel Ratsaby and Santosh S. Venkatesh. </author> <title> Learning from a mixture of labeled and unlabeled examples with parametric side information. </title> <booktitle> In Proceedings of the 8th Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 412-417. </pages> <publisher> ACM Press, </publisher> <address> New York, NY, </address> <year> 1995. </year>
Reference-contexts: We call this type of bootstrapping co-training, and it has a close connection to bootstrapping from incomplete data in the Expectation-Maximization setting; see, for instance, <ref> [5, 13] </ref>. The question this raises is: is there any reason to believe co-training will help? Our goal is to address this question by developing a PAC-style theoretical framework to better understand the issues involved in this approach.
Reference: [14] <author> M.J. Witbrock and A.G. Hauptmann. </author> <title> Improving acoustic models by watching television. </title> <type> Technical Report CMU-CS-98-110, </type> <institution> Carnegie Mellon University, </institution> <month> March 19 </month> <year> 1998. </year>
Reference-contexts: We conjecture that there are many practical learning problems that fit or approximately fit the co-training model. For example, consider the problem of learning to classify segments of television broadcasts <ref> [7, 14] </ref>. We might be interested, say, in learning to identify televised segments containing the US President. Here X 1 could be the set of possible video images, X 2 the set of possible audio signals, and X their cross product.
Reference: [15] <author> D. Yarowsky. </author> <title> Unsupervised word sense disambiguation rivaling supervised methods. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 189-196, </pages> <year> 1995. </year>
Reference-contexts: 1 INTRODUCTION In many machine learning settings, unlabeled examples are significantly easier to come by than labeled ones <ref> [4, 15] </ref>. One example of this is web-page classification. Suppose that we want a program to electronically visit some web site and download all the web pages of interest to us, such as all the CS faculty member pages, or all the course home pages at some university [1]. <p> Our proposal of a compatibility function between a concept and a probability distribution is an attempt to more broadly consider distributions that do not completely commit to a target function and yet are not completely uncommitted either. A second approach to using unlabeled data, given by Yarowsky <ref> [15] </ref> in the context of the "word sense disambiguation" problem is much closer in spirit to co-training, and can be nicely viewed in our model. The problem Yarowsky considers is the following. Many words have several quite different dictionary definitions. <p> Many words have several quite different dictionary definitions. For instance, "plant" can mean a type of life form or a factory. Given a text document and an instance of the word "plant" in it, the goal of the algorithm is to determine which meaning is intended. Yarowsky <ref> [15] </ref> makes use of unlabeled data via the following observation: within any fixed document, it is highly likely that all instances of a word like "plant" have the same intended meaning, whichever meaning that happens to be.
References-found: 15

