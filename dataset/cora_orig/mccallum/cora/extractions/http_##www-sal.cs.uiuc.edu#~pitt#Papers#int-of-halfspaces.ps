URL: http://www-sal.cs.uiuc.edu/~pitt/Papers/int-of-halfspaces.ps
Refering-URL: http://www-sal.cs.uiuc.edu/~pitt/
Root-URL: http://www.cs.uiuc.edu
Email: (kwek j pitt)@cs.uiuc.edu.  
Title: PAC Learning Intersections of Halfspaces with Membership Queries (Extended Abstract)  
Author: Stephen Kwek and Leonard Pitt 
Note: Supported in part by NSF Grant IRI-9014840, and by NASA grant NAG 1-613. Supported in part by NSF Grant IRI-9014840.  
Address: Urbana, IL 61801  
Affiliation: Computer Science Department University of Illinois  
Abstract:  
Abstract-found: 1
Intro-found: 1
Reference: [A93] <author> P.Auer. </author> <title> On-line Learning of Rectangles in Noisy Environments. </title> <booktitle> In Proc. 6th Annu. Conference on Computational Learning Theory, </booktitle> <pages> pages 253-261, </pages> <year> 1993. </year>
Reference-contexts: Chen and Maass [CM94] presents an efficient on-line mistake bounded algorithm that learns a single box in R d , while Auer <ref> [A93] </ref> investigates the case where there is noise in the mistake bounded model. 4 The Forest Because the class of intersections of s halfspaces has VC-dimension 2 at most 2 (n + 1)s log 3s, by Theorem 2.1 of Blumer et al. [BEHW89], any algorithm that obtains from distribution D a
Reference: [AKMW96] <author> P. Auer, S. Kwek, W. Maass and M. War-muth. </author> <title> Learning of Depth Two Neural Nets with Constant Fan-in at the Hidden Nodes. </title> <booktitle> In Proc. 8th Annu. Conference on Computational Learning Theory, </booktitle> <year> 1996. </year>
Reference-contexts: An algorithm for learn ing convex polytopes in the discretized domain of constant dimension, using stronger queries such as superset or disjointness queries has been given [H94]. Auer et al. <ref> [AKMW96] </ref> presents algorithms for learning depth two neural networks where the hidden nodes are linear threshold gates with constant fan-in, and as a consequence, obtain a noise-tolerant mistake-bounded algorithm for learning (the class of circuits containing) the class of convex polyhedra in fixed dimension.
Reference: [AL88] <author> D. Angluin and P. Laird, </author> <title> Learning from Noisy Examples, </title> <note> Machine Learning (2)1988 343-370. </note>
Reference-contexts: An O (n 6 k) time algorithm for finding one polygon in the class of convex s-gons that minimizes the classification error on a sample of labeled points is given [FK96]. This result implies that convex polygons are learnable in the PAC model with random classification noise <ref> [AL88] </ref> and the "agnostic" PAC model [KSS92] . Learning boxes: There have been a number of papers ([A93, CM94, MT89, MT94]) on learning the simpler class of concepts formed from halfspaces whose bounding hyperplanes are parallel to the coordinate axes.
Reference: [AHHP96] <author> H. Aizenstein, T. Heged-us, L. Hellerstein, and L. Pitt. </author> <title> Complexity Theoretic Hardness Results for Query Learning. To appear, computational complexity. (See also Read-Thrice DNF is Hard to Learn With Membership and Equivalence Queries. </title> <booktitle> Proceedings of the 33rd Annual IEEE symposium on the Foundations of Computer Science, </booktitle> <pages> pages 523-532, </pages> <year> 1992.) </year>
Reference-contexts: When membership queries are allowed, it has been shown that in the boolean domain, exact learning from equivalence and membership queries remains NP-hard if the algorithm is required to find (as ours does) an intersection with the same number of half-spaces s, for any fixed s 3 <ref> [AHHP96, PR94] </ref>. Mostly negative results have been obtained for learning in the case that the number of halfspaces is not held constant, and the learning algorithm is allowed to output a hypothesis containing more halfspaces than the target concept. <p> This is the task found NP-hard when the domain is restricted to the boolean hypercube in the PAC model without membership queries [BR89], and in the more demanding exact learning model with both membership and equivalence queries <ref> [AHHP96, PR94] </ref>. Let S + and S denote the positive and negative examples of S, respectively.
Reference: [B90A] <author> E. Baum. </author> <title> Polynomial Time Algorithm for Learning Neural Nets, </title> <booktitle> In Proc. 3rd Annu. Workshop on Computational Learning Theory, </booktitle> <pages> pages 258-272, </pages> <address> San Mateo, CA, 1990. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [B90B] <author> E. Baum. </author> <title> On Learning a Union of Half Spaces. </title> <journal> Journal of Complexity (6) 1990, </journal> <pages> 67-101. </pages>
Reference-contexts: It follows that if one-way functions exist, then learning convex polyhedra is intractable in this model. Learning in constant dimension: For constant di-mension, Baum <ref> [B90B] </ref> presents an Occam algorithm that learns intersections of s halfspaces in R k , for fixed k, in time O * log 2 1 k+3 . <p> Each negative example is a negative example for (hence is "outside of") at least one of the halfspaces. The problem is a "credit assignment" (actually "blame assignment") problem as discussed in this context by Baum <ref> [B90B] </ref>: If we can figure out for each negative example x, at least one of the s halfspaces that do not contain x, then we can learn each halfspace separately by using any polynomial-time linear programming algorithm to find a hyperplane separating the positive examples from those negative examples that are
Reference: [B90C] <author> E. Baum. </author> <title> A polynomial time algorithm that learns two hidden unit nets. </title> <booktitle> Neural Computation vol. </booktitle> <volume> 2, number 4, </volume> <pages> pp. 510-522, </pages> <year> 1990. </year>
Reference-contexts: Learning two halfspaces: Blum and Rivest [BR89] show that finding an intersection of two halfspaces that are consistent with a sample of labeled points from the boolean domain, if it exists, is NP-Complete. Baum <ref> [B90C] </ref> presents an algorithm that learns intersections of two halfspaces from examples and membership queries, or from examples alone if the distribution obeys a sym metry condition.
Reference: [B91] <author> E. Baum. </author> <title> Neural Net Algorithms that Learn in Polynomial Time from Examples and Queries. </title> <booktitle> In IEEE Transaction on Neural Networks, </booktitle> <volume> 2 </volume> <pages> 5-19, </pages> <year> 1991 </year>
Reference-contexts: Our membership queries are somewhat weaker than the finger probe used in such work, but we demand only approximate, as opposed to exact, identification of the object. We are still investigating relationships between these two types of problems. In a closely related work of Baum <ref> [B91] </ref>, an algorithm is presented that learns the intersection of s halfspaces in n dimensions in time polynomial in s and n, in a nonstandard PAC-inspired learning model designed "to rule out with high confidence pathological configurations with several planes arbitrarily close to one another or large fractions of the measure <p> Learning three or more halfspaces: We have already mentioned a related work of Baum <ref> [B91] </ref> in Section 1 for learning convex polytopes in R n in a PAC inspired model. <p> We believe the techniques here will apply to intersections of other types of objects, which we are currently investigating, together with other related problems. Acknowledgements We thank Herbert Edelsbrunner for helpful conversations regarding this work, and Avrim Blum for pointing out Baum's paper <ref> [B91] </ref>. We also thank Eric Baum for helpful clarifications and the COLT program committee for valuable comments.
Reference: [BCGS95] <author> A. Blum, P. Chalasani, S. Goldman and D. </author> <title> Slonim. Learning with Unreliable Boundary Queries. </title> <booktitle> In Proc. 8th Annu. Conference on Computational Learning Theory, </booktitle> <pages> pages 98-107, </pages> <year> 1995. </year>
Reference-contexts: Baum [B90C] presents an algorithm that learns intersections of two halfspaces from examples and membership queries, or from examples alone if the distribution obeys a sym metry condition. Recently, this result has been extended by <ref> [BCGS95] </ref> to learning intersections of two (not necessarily homogeneous) halfspaces where the membership queries on points that are distance d from the bounding hyperplanes are unreliable and the distribution has weight 0 within d of the boundary.
Reference: [BGM95] <author> N. Bshouty, S. Goldman and H. Mathias. </author> <title> Noise-Tolerant Parallel Learning of Geometric Concepts. </title> <booktitle> In Proc. 8th Annu. Conference on Computational Learning Theory, </booktitle> <pages> pages 345-352, </pages> <year> 1995. </year>
Reference-contexts: Bshouty et al. [BGMST96] give a noise-tolerant PAC algorithm for learning arbitrary boolean functions of s halfspaces in constant dimension. (See also <ref> [BGM95] </ref> for earlier work.) Recently the more difficult related problem of finding a convex polygon (in the plane) of s sides that misclassifies the fewest number of points in a finite sample has been studied [DG95, F95, FK96].
Reference: [BGMST96] <author> N. Bshouty, S. Goldman, D. Mathias, S. Suri, and H. Tamaki. </author> <title> Noise-Tolerant Distribution Free Learning of General Geometric Concepts. </title> <booktitle> In Proceedings of the 28th Annual ACM Symposium on Theory of Computing. </booktitle> <year> 1996. </year>
Reference-contexts: Bshouty et al. <ref> [BGMST96] </ref> give a noise-tolerant PAC algorithm for learning arbitrary boolean functions of s halfspaces in constant dimension. (See also [BGM95] for earlier work.) Recently the more difficult related problem of finding a convex polygon (in the plane) of s sides that misclassifies the fewest number of points in a finite sample
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler and M. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> J. ACM, </journal> <volume> 36 </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Most consider restrictions on either the number of halfspaces, or the dimension. Learning a single halfspace: Learning a single half-space corresponds to training a simple perceptron for which efficient algorithms are known in the PAC model <ref> [BEHW89] </ref> and in the exact model using equivalence queries [MT91]. In both cases the algorithms work with out membership queries. Littlestone [L88] gives an efficient algorithm for learning a single halfspace over the boolean domain in the more demanding "mistake bound model". <p> Learning boxes: There have been a number of papers ([A93, CM94, MT89, MT94]) on learning the simpler class of concepts formed from halfspaces whose bounding hyperplanes are parallel to the coordinate axes. Blumer et al. <ref> [BEHW89] </ref> show that choosing the smallest bounding hyperrectangle covering the set of positive examples in a given sample yields a learning algorithm that is polynomial in both the number of hyperplanes, and the dimension. <p> single box in R d , while Auer [A93] investigates the case where there is noise in the mistake bounded model. 4 The Forest Because the class of intersections of s halfspaces has VC-dimension 2 at most 2 (n + 1)s log 3s, by Theorem 2.1 of Blumer et al. <ref> [BEHW89] </ref>, any algorithm that obtains from distribution D a sample S of size at least m = maxf 4 * log 2 16 (n+1)s log (3s) ffi g, and outputs a consistent intersection of s halfspaces, achieves the PAC-criterion of learning. <p> In order for Theorem 2.1 of <ref> [BEHW89] </ref> (or any of its extensions) to apply, we must not find an intersection of "too many" halfspaces. In particular, we cannot simply find for each negative example a hyperplane separating that negative example from all positives. <p> While the resulting intersection would correctly classify each point in S, the number of hyperplanes could be as large as the number of negative sample points. The generalizations of Theorem 2.1 of <ref> [BEHW89] </ref> (pertaining to "Occam algorithms") may only be applied when the consistent hypothesis produced by the learning algorithm has representational size that is significantly smaller than the sample size|there must be some amount of "compression" of the sample data. <p> Using the midpoint lemma together with a simple induction, it can be shown that Polly will output a collection H of halfspaces whose intersection correctly classifies all points in S. By Theorem 2.1 of <ref> [BEHW89] </ref>, the probability that H has actual error exceeding * on the distribution D from which S was sampled is at most ffi 2 . Thus, the probability that Polly outputs some collection H whose error is at most * within t iterations, is at least 1 ffi.
Reference: [BM91] <author> W. Bultman and W. Maass, </author> <title> Fast identification of Geometric Objects with Membership Queries, </title> <booktitle> In Proc. of 4th Annual ACM Conference on Computational Learning Theory (1991) 337-353. </booktitle>
Reference-contexts: In both cases the algorithms work with out membership queries. Littlestone [L88] gives an efficient algorithm for learning a single halfspace over the boolean domain in the more demanding "mistake bound model". Bultman and Maass <ref> [BM91] </ref> present an algorithm that learns a single halfspace in the discretized domain (1; ; m) 2 using (log m) membership queries in time O ((log m) O (1) ).
Reference: [BR89] <author> A. Blum, R. Rivest, </author> <title> Training a 3-node neural net is NP-complete. </title> <editor> In D.S. Touret-zky, editor, </editor> <booktitle> Advances in Neural Network Information Processing Systems I, </booktitle> <publisher> Morgan Kaufman, </publisher> <pages> pp 494-501, </pages> <year> 1989. </year>
Reference-contexts: Shevchenko [S87] investigates the learning of a single halfspace in (1; ::::; m) n using O ((log m) (n1)d n 2 e+n ) membership queries in poly (log m) time. Learning two halfspaces: Blum and Rivest <ref> [BR89] </ref> show that finding an intersection of two halfspaces that are consistent with a sample of labeled points from the boolean domain, if it exists, is NP-Complete. <p> Polly will actually produce no more hyperplanes than the number in the target. This is the task found NP-hard when the domain is restricted to the boolean hypercube in the PAC model without membership queries <ref> [BR89] </ref>, and in the more demanding exact learning model with both membership and equivalence queries [AHHP96, PR94]. Let S + and S denote the positive and negative examples of S, respectively.
Reference: [CM94] <author> Z. Chen and W. Maass, </author> <title> On-Line Learning of Rectangles and Unions of Rectangles. </title> <journal> Machine Learning, </journal> <volume> 17, </volume> <month> 201-223 </month> <year> (1994). </year>
Reference-contexts: Blumer et al. [BEHW89] show that choosing the smallest bounding hyperrectangle covering the set of positive examples in a given sample yields a learning algorithm that is polynomial in both the number of hyperplanes, and the dimension. Chen and Maass <ref> [CM94] </ref> presents an efficient on-line mistake bounded algorithm that learns a single box in R d , while Auer [A93] investigates the case where there is noise in the mistake bounded model. 4 The Forest Because the class of intersections of s halfspaces has VC-dimension 2 at most 2 (n +
Reference: [DG95] <author> D. Dobkin and D. Gunopulos, </author> <title> Concept Learning with Geometric Hypothesis, </title> <booktitle> Proc. of 8th Annual ACM Conference on Computational Learning Theory 1995, </booktitle> <pages> 329-336. </pages>
Reference-contexts: PAC algorithm for learning arbitrary boolean functions of s halfspaces in constant dimension. (See also [BGM95] for earlier work.) Recently the more difficult related problem of finding a convex polygon (in the plane) of s sides that misclassifies the fewest number of points in a finite sample has been studied <ref> [DG95, F95, FK96] </ref>. An O (n 6 k) time algorithm for finding one polygon in the class of convex s-gons that minimizes the classification error on a sample of labeled points is given [FK96].
Reference: [F95] <author> P. Fischer, </author> <title> More or Less Efficient Agnostic Learning of Convex Polygons, </title> <booktitle> Proc. of 8th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 337-344, </pages> <year> 1995. </year>
Reference-contexts: PAC algorithm for learning arbitrary boolean functions of s halfspaces in constant dimension. (See also [BGM95] for earlier work.) Recently the more difficult related problem of finding a convex polygon (in the plane) of s sides that misclassifies the fewest number of points in a finite sample has been studied <ref> [DG95, F95, FK96] </ref>. An O (n 6 k) time algorithm for finding one polygon in the class of convex s-gons that minimizes the classification error on a sample of labeled points is given [FK96].
Reference: [FK96] <author> P. Fischer and S. Kwek. </author> <title> Minimizing Disagreements for Geometric Regions, Using Dynamic Programming, with Applications in Machine Learning. In Electronic Archive for Computational Learning Theory Technical Report eC-TR-96-004, </title> <year> 1996. </year>
Reference-contexts: PAC algorithm for learning arbitrary boolean functions of s halfspaces in constant dimension. (See also [BGM95] for earlier work.) Recently the more difficult related problem of finding a convex polygon (in the plane) of s sides that misclassifies the fewest number of points in a finite sample has been studied <ref> [DG95, F95, FK96] </ref>. An O (n 6 k) time algorithm for finding one polygon in the class of convex s-gons that minimizes the classification error on a sample of labeled points is given [FK96]. <p> An O (n 6 k) time algorithm for finding one polygon in the class of convex s-gons that minimizes the classification error on a sample of labeled points is given <ref> [FK96] </ref>. This result implies that convex polygons are learnable in the PAC model with random classification noise [AL88] and the "agnostic" PAC model [KSS92] .
Reference: [H94] <author> T. Heged-us, </author> <title> Geometrical Concept Learning and Convex Polytopes, </title> <booktitle> Proc. of 7th Annual ACM Conference on Computational Learning Theory (1994) 228-236. </booktitle>
Reference-contexts: An algorithm for learn ing convex polytopes in the discretized domain of constant dimension, using stronger queries such as superset or disjointness queries has been given <ref> [H94] </ref>. Auer et al. [AKMW96] presents algorithms for learning depth two neural networks where the hidden nodes are linear threshold gates with constant fan-in, and as a consequence, obtain a noise-tolerant mistake-bounded algorithm for learning (the class of circuits containing) the class of convex polyhedra in fixed dimension.
Reference: [KSS92] <author> M. Kearns, E. Schapire and L. Sellie, </author> <title> Toward Efficient Agnostic Learning. </title> <booktitle> Proc. of 5th Annual ACM Conference on Computational Learning Theory (1992) 341-352. </booktitle>
Reference-contexts: This result implies that convex polygons are learnable in the PAC model with random classification noise [AL88] and the "agnostic" PAC model <ref> [KSS92] </ref> . Learning boxes: There have been a number of papers ([A93, CM94, MT89, MT94]) on learning the simpler class of concepts formed from halfspaces whose bounding hyperplanes are parallel to the coordinate axes.
Reference: [L88] <author> N. Littlestone. </author> <title> Learning when irrelevant attributes abound: A new linear threshold algorithm. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 285-318, </pages> <year> 1988. </year>
Reference-contexts: Learning a single halfspace: Learning a single half-space corresponds to training a simple perceptron for which efficient algorithms are known in the PAC model [BEHW89] and in the exact model using equivalence queries [MT91]. In both cases the algorithms work with out membership queries. Littlestone <ref> [L88] </ref> gives an efficient algorithm for learning a single halfspace over the boolean domain in the more demanding "mistake bound model". <p> By using simple prediction preserving reductions it can be shown that PAC learnability of the class of convex polyhedra without membership queries implies PAC-learnability of DNF formulas <ref> [PW90, L88] </ref>, which is one of the more challenging open problems in learning theory.
Reference: [LW91] <author> P. Long and M. Warmuth. </author> <title> Composite Geometric Concepts and Polynomial Predictability. </title> <booktitle> In Proc of the Fourth Workshop on Computational Learning Theory, </booktitle> <pages> pages 167-175. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1991. </year>
Reference-contexts: By using simple prediction preserving reductions it can be shown that PAC learnability of the class of convex polyhedra without membership queries implies PAC-learnability of DNF formulas [PW90, L88], which is one of the more challenging open problems in learning theory. Long and Warmuth <ref> [LW91] </ref> show that learning convex polyhedra in the continuous domain is as hard as learning polynomially-sized circuits, assuming that the chosen representation (whose size dictates a parameter in which the algorithm must be polynomial) is the list of vertices, instead of bounding hyperplanes, of the polytope.
Reference: [MT89] <author> W. Maass and G. Turan. </author> <title> On the Complexity of Learning from Counterexamples. </title> <booktitle> Proceedings of the 30th Annual IEEE symposium on the Foundations of Computer Science, </booktitle> <pages> pages 262-267, </pages> <year> 1989. </year>
Reference: [MT91] <author> W. Maass and G. Turan. </author> <title> How Fast Can a Threshold Gate Learn? in Computa tional Learning Theory and Natural Learn--ing Systems: Constraints and Prospects. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Most consider restrictions on either the number of halfspaces, or the dimension. Learning a single halfspace: Learning a single half-space corresponds to training a simple perceptron for which efficient algorithms are known in the PAC model [BEHW89] and in the exact model using equivalence queries <ref> [MT91] </ref>. In both cases the algorithms work with out membership queries. Littlestone [L88] gives an efficient algorithm for learning a single halfspace over the boolean domain in the more demanding "mistake bound model".
Reference: [MT94] <author> W. Maass and G. Turan. </author> <title> Algorithms and Lower Bounds for on-line Learning of Geometric Concepts. </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <pages> 251-269. </pages>
Reference: [PB90] <author> R. Board and L. Pitt. </author> <title> On the Necessity of Occam Algorithms. </title> <journal> Theoret. Comput. Sci. </journal> <volume> 100, </volume> <pages> pages 157-184, </pages> <year> 1992. </year>
Reference: [PR94] <author> K. Pillaipakkamnatt and V. Raghavan. </author> <title> On the Limits of Proper Learnability of Subclasses of DNF Formula. </title> <booktitle> Machine Learning, </booktitle> <pages> pages 1-29, 1(1994). </pages>
Reference-contexts: When membership queries are allowed, it has been shown that in the boolean domain, exact learning from equivalence and membership queries remains NP-hard if the algorithm is required to find (as ours does) an intersection with the same number of half-spaces s, for any fixed s 3 <ref> [AHHP96, PR94] </ref>. Mostly negative results have been obtained for learning in the case that the number of halfspaces is not held constant, and the learning algorithm is allowed to output a hypothesis containing more halfspaces than the target concept. <p> This is the task found NP-hard when the domain is restricted to the boolean hypercube in the PAC model without membership queries [BR89], and in the more demanding exact learning model with both membership and equivalence queries <ref> [AHHP96, PR94] </ref>. Let S + and S denote the positive and negative examples of S, respectively.
Reference: [PV89] <author> L. Pitt and L. Valiant. </author> <title> Computational Limitations on Learning from Examples. </title> <journal> J. ACM, </journal> <volume> 35: </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference: [PW90] <author> L. Pitt and M. Warmuth. </author> <title> Prediction-preserving reducibility. </title> <journal> Journal of Computer and System Science, </journal> <volume> 41: </volume> <pages> 430-467, </pages> <year> 1990. </year>
Reference-contexts: By using simple prediction preserving reductions it can be shown that PAC learnability of the class of convex polyhedra without membership queries implies PAC-learnability of DNF formulas <ref> [PW90, L88] </ref>, which is one of the more challenging open problems in learning theory.
Reference: [S87] <author> V. Shevchenko, </author> <title> On Deciphering a Threshold Function of Many-Valued Logic, in Combinatorial-Algebraic Methods and their Applications, </title> <institution> Grokii State University 1987, </institution> <note> 155-163 (in Russian). </note>
Reference-contexts: Bultman and Maass [BM91] present an algorithm that learns a single halfspace in the discretized domain (1; ; m) 2 using (log m) membership queries in time O ((log m) O (1) ). Shevchenko <ref> [S87] </ref> investigates the learning of a single halfspace in (1; ::::; m) n using O ((log m) (n1)d n 2 e+n ) membership queries in poly (log m) time.
Reference: [S92] <author> S. Skiena, </author> <title> Interactive Reconstruction via Geometric Probing. </title> <journal> Proc of IEEE, </journal> <volume> vol 80, </volume> <year> 1992, </year> <pages> pp 1364-1382. </pages>
Reference-contexts: Motivated by robotics problems, algorithms have been sought that construct models of an unknown geometric object using different types of "probes" (e.g., the "finger" probe, or the "x-ray" probe) that correspond to sensory input that might be available to a robot <ref> [S92] </ref>. Our membership queries are somewhat weaker than the finger probe used in such work, but we demand only approximate, as opposed to exact, identification of the object. We are still investigating relationships between these two types of problems.
Reference: [V84] <author> L. Valiant, </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM (27)1984 1134-1142. </journal>
Reference-contexts: Then we use linear programming to separate all of these negative sample points from the positive examples. 2 Statement of Results We review the PAC learning model with membership queries, and then state our main theorem. In Valiant's distribution-free, or probably approximately correct (PAC) learning model <ref> [V84] </ref>, the learner's goal is to infer an unknown target concept c chosen from some known concept class C.
References-found: 32

