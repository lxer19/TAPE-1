URL: http://www.cs.toronto.edu/~dt/papers/videre97/videre97.ps.gz
Refering-URL: http://www.cs.toronto.edu/~dt/publications/publications.html
Root-URL: http://www.cs.toronto.edu
Title: Animat Vision: Active Vision in Artificial Animals  
Author: Demetri Terzopoulos and Tamer F. Rabie 
Keyword: Active Vision, Artificial Life, Artificial Animals, Autonomous Agents, Physics-Based Modeling, Biomimetic Vision Systems, Foveation, Visual Stabilization, Color Object Detection, Vision-Guided Navigation.  
Address: 10 King's College Road, Toronto, ON M5S 3G4, Canada  
Affiliation: Department of Computer Science, University of Toronto  
Abstract: We propose and demonstrate a new paradigm for active vision research that draws upon recent advances in the fields of artificial life and computer graphics. A software alternative to the prevailing hardware vision mindset, animat vision prescribes artificial animals, or animats, situated in physics-based virtual worlds as autonomous virtual robots with active perception systems. To be operative in its world, an animat must autonomously control its eyes and muscle-actuated body. Computer vision algorithms continuously analyze the retinal image streams acquired by the animat's eyes, enabling it to locomote purposefully through its world. We describe an initial animat vision implementation within lifelike artificial fishes inhabiting a physics-based, virtual marine world. Emulating the appearance, motion, and behavior of real fishes in their natural habitats, these animats are capable of spatially nonuniform retinal imaging, foveation, retinal image stabilization, color object recognition, and perceptually-guided navigation. These capabilities allow them to pursue moving targets such as other artificial fishes. Animat vision offers a fertile approach to the development, implementation, and evaluation of computational theories that profess sensorimotor competence for animal or robotic situated agents. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Husbands, </author> <title> editor. From Animals to Animats: </title> <booktitle> The 3rd International Conf. on Simulation of Adaptive Behavior, </booktitle> <address> Cambridge, MA, 1994. </address> <publisher> MIT Press. </publisher>
Reference-contexts: 1 Introduction Advances in the emerging field of artificial life (ALife) make possible a fresh approach to computational vision. 1 A major theme in ALife research is the synthesis of artificial animals, or animats <ref> [1] </ref>. Animats, a term coined by Wilson [2], are computational models of real animals situated in their natural habitats. A recent breakthrough in animat research has produced situated virtual agents that realistically emulate animals of nontrivial evolutionary complexity [3].
Reference: [2] <author> S. W. Wilson. </author> <title> The animat path to AI. </title> <editor> In J.-A. Meyer and S. Wilson, editors, </editor> <booktitle> From Animals to Animats, </booktitle> <pages> pages 15-21. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: 1 Introduction Advances in the emerging field of artificial life (ALife) make possible a fresh approach to computational vision. 1 A major theme in ALife research is the synthesis of artificial animals, or animats [1]. Animats, a term coined by Wilson <ref> [2] </ref>, are computational models of real animals situated in their natural habitats. A recent breakthrough in animat research has produced situated virtual agents that realistically emulate animals of nontrivial evolutionary complexity [3]. <p> Artificial fishes are animats of unprecedented sophistication. They are autonomous virtual robots situated in a 3D continuous virtual world governed by physical dynamics. This makes them suitable for grounding active vision systems. By contrast, Wilson's original animat <ref> [2] </ref>, which was proposed for exploring the acquisition of simple behavior rules, is a point marker on a nonphysical 2D grid world that can move between squares containing food or obstacles. Other simple animats include the 2D cockroaches of Beer [19].
Reference: [3] <author> D. Terzopoulos, X. Tu, and R. Grzeszczuk. </author> <title> Artificial fishes: Autonomous locomotion, perception, behavior, and learning in a simulated physical world. </title> <journal> Artificial Life, </journal> <volume> 1(4) </volume> <pages> 327-351, </pages> <year> 1994. </year>
Reference-contexts: Animats, a term coined by Wilson [2], are computational models of real animals situated in their natural habitats. A recent breakthrough in animat research has produced situated virtual agents that realistically emulate animals of nontrivial evolutionary complexity <ref> [3] </ref>. <p> Seaweeds grow from the ocean bed and sway in the current. animat with these essential capabilities has been implemented that emulates animals as complex as teleost fishes in their marine habitats <ref> [3, 6] </ref>. Imagine a virtual marine world inhabited by a variety of realistic fishes (Fig. 1). In the presence of underwater currents, the fishes employ their muscles and fins to swim gracefully around immobile obstacles and among moving aquatic plants and other fishes. <p> Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere <ref> [3, 6, 23] </ref>. This section reviews the animat to a level of detail that suffices to comprehend the remainder of the paper. Each artificial fish is an autonomous agent with a deformable body comprising a graphical display model and a biomechanical model actuated by internal muscles. <p> For example, the artificial fish attends to sensory information about nearby food sources when foraging. The animats in our previous ALife simulations (described in <ref> [3, 6, 23] </ref>) employ a perceptual oracle scheme according to which the artificial fish may satisfy its perceptual needs via direct interrogation of the 3D world model.
Reference: [4] <author> D. Terzopoulos and T.F. Rabie. </author> <title> Animat vision. </title> <booktitle> In Proc. Fifth International Conference on Computer Vision, </booktitle> <pages> pages 801-808, </pages> <address> Cambridge, MA, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: In particular, an 1 For an engaging introduction to the ALife field, see, e.g., S. Levy, Artificial Life (Pantheon, 1992). 2 The animat vision paradigm was introduced in an earlier version of this paper <ref> [4] </ref> and it is further developed in [5]. 2 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. observer.
Reference: [5] <author> T.F. Rabie and D. Terzopoulos. </author> <title> Motion and color analysis for animat perception. </title> <booktitle> In Proc. 13th National Conference on Artificial Intelligence, </booktitle> <pages> pages 1090-1097, </pages> <address> Portland, OR, August 1996. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In particular, an 1 For an engaging introduction to the ALife field, see, e.g., S. Levy, Artificial Life (Pantheon, 1992). 2 The animat vision paradigm was introduced in an earlier version of this paper [4] and it is further developed in <ref> [5] </ref>. 2 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. observer. The 3 reddish fish (center) are engaged in mating behavior, the greenish fish (upper right) is a predator hunting for small prey, the remaining 3 fishes are feeding on plankton (white dots).
Reference: [6] <author> X. Tu and D. Terzopoulos. </author> <title> Artificial fishes: Physics, locomotion, perception, behavior. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, Proc. </booktitle> <address> SIGGRAPH '94 (Orlando, FL), </address> <pages> pages 43-50. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> July </month> <year> 1994. </year>
Reference-contexts: Seaweeds grow from the ocean bed and sway in the current. animat with these essential capabilities has been implemented that emulates animals as complex as teleost fishes in their marine habitats <ref> [3, 6] </ref>. Imagine a virtual marine world inhabited by a variety of realistic fishes (Fig. 1). In the presence of underwater currents, the fishes employ their muscles and fins to swim gracefully around immobile obstacles and among moving aquatic plants and other fishes. <p> Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere <ref> [3, 6, 23] </ref>. This section reviews the animat to a level of detail that suffices to comprehend the remainder of the paper. Each artificial fish is an autonomous agent with a deformable body comprising a graphical display model and a biomechanical model actuated by internal muscles. <p> For example, the artificial fish attends to sensory information about nearby food sources when foraging. The animats in our previous ALife simulations (described in <ref> [3, 6, 23] </ref>) employ a perceptual oracle scheme according to which the artificial fish may satisfy its perceptual needs via direct interrogation of the 3D world model.
Reference: [7] <author> R. D. Fernald. </author> <title> Vision. </title> <editor> In D. H. Evans, editor, </editor> <booktitle> The Physiology of Fishes, chapter 6, </booktitle> <pages> pages 161-189. </pages> <publisher> CRC Press, </publisher> <address> Boca Raton, FL, </address> <year> 1993. </year>
Reference-contexts: In fact, the animat vision paradigm applies to animats that model any animaleven a human beingto the level of fidelity that the artificial fish models a real fish. Indeed, the animat vision system that we develop in this paper makes no attempt to model fish perception <ref> [7] </ref>. Instead, we have found it an interesting and challenging problem to endow the piscine animat with a biomimetic vision system that enables it to be a functional, active observer of its world.
Reference: [8] <author> E.R. Loew and W.N. McFarland, </author> <title> editors. Fish Vision, </title> <booktitle> volume 31(3) of Vision Research, </booktitle> <year> 1991. </year> <note> 23 To appear in Videre: </note> <institution> A Journal of Computer Vision Research, </institution> <address> 1(1), </address> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Fish visual systems have been open to a wider variety of investigations, from biochemistry to ecology, than any other visual system, and many characteristics of vertebrate vision were initially demonstrated using fish (see <ref> [8] </ref>). 5 Animat vision should not be confused with Ballard's animate vision; the latter does not involve artificial animals. 6 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. processors. Artificial fishes are animats of unprecedented sophistication.
Reference: [9] <author> J. J. Gibson. </author> <title> The Ecological Approach to Visual Perception. </title> <publisher> Houghton Mifflin, </publisher> <address> Boston, MA, </address> <year> 1979. </year>
Reference-contexts: Gibson <ref> [9] </ref>, in a sense the grandfather of active vision, stresses in pre-computational terms the importance of modeling the active observer situated in the dynamic environment.
Reference: [10] <author> R. </author> <title> Bajcsy. Active perception. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 76(8) </volume> <pages> 996-1005, </pages> <year> 1988. </year>
Reference-contexts: Gibson [9], in a sense the grandfather of active vision, stresses in pre-computational terms the importance of modeling the active observer situated in the dynamic environment. Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy <ref> [10] </ref> and Ballard [11] under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. [12] and many others (see, e.g., [13, 14, 15]) into the prevalent paradigm that it is today.
Reference: [11] <author> D. Ballard. </author> <title> Animate vision. </title> <journal> Artificial Intelligence, </journal> <volume> 48 </volume> <pages> 57-86, </pages> <year> 1991. </year>
Reference-contexts: Gibson [9], in a sense the grandfather of active vision, stresses in pre-computational terms the importance of modeling the active observer situated in the dynamic environment. Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy [10] and Ballard <ref> [11] </ref> under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. [12] and many others (see, e.g., [13, 14, 15]) into the prevalent paradigm that it is today.
Reference: [12] <author> Y. Aloimonos, A. Bandyopadhyay, and I. Weiss. </author> <title> Active vision. </title> <journal> Int. J. Computer Vision, </journal> <pages> pages 333-356, </pages> <year> 1987. </year>
Reference-contexts: Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy [10] and Ballard [11] under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. <ref> [12] </ref> and many others (see, e.g., [13, 14, 15]) into the prevalent paradigm that it is today. The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg [16].
Reference: [13] <author> D.H. Ballard and C.M. Brown. </author> <booktitle> Principles of animate vision. CVGIP: Image Understanding, </booktitle> <volume> 56(1) </volume> <pages> 3-21, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy [10] and Ballard [11] under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. [12] and many others (see, e.g., <ref> [13, 14, 15] </ref>) into the prevalent paradigm that it is today. The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg [16].
Reference: [14] <author> A. Blake and A. Yuille, </author> <title> editors. Active Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy [10] and Ballard [11] under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. [12] and many others (see, e.g., <ref> [13, 14, 15] </ref>) into the prevalent paradigm that it is today. The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg [16].
Reference: [15] <editor> M.J. Swain and M.A. Stricker (Eds.). </editor> <title> Promising directions in active vision. </title> <journal> Int. J. Computer Vision, </journal> <volume> 11(2) </volume> <pages> 109-126, </pages> <year> 1993. </year>
Reference-contexts: Versions of this paradigm suitable for mainstream computer vision were introduced in the seminal papers of Bajcsy [10] and Ballard [11] under the names of active perception and animate vision, respectively. 5 The active vision approach was further developed by Aloimonos et al. [12] and many others (see, e.g., <ref> [13, 14, 15] </ref>) into the prevalent paradigm that it is today. The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg [16].
Reference: [16] <author> V. </author> <title> Braitenberg. Vehicles, Experiments in Synthetic Psychology. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1984. </year>
Reference-contexts: The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg <ref> [16] </ref>. We believe that they are as appropriate for grounding active vision systems as are the hardware mobots that have come out of the situated robotics work of Brooks and his group [17] and have been an inspiration to numerous other robotics groups (see, e.g., the compilation [18]).
Reference: [17] <author> R.A. Brooks. </author> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47 </volume> <pages> 139-160, </pages> <year> 1991. </year>
Reference-contexts: The artificial animals that we advocate in this paper are active vehicles in the sense of Braitenberg [16]. We believe that they are as appropriate for grounding active vision systems as are the hardware mobots that have come out of the situated robotics work of Brooks and his group <ref> [17] </ref> and have been an inspiration to numerous other robotics groups (see, e.g., the compilation [18]).
Reference: [18] <editor> P. Maes, editor. </editor> <title> Designing Autonomous Agents. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1991. </year>
Reference-contexts: We believe that they are as appropriate for grounding active vision systems as are the hardware mobots that have come out of the situated robotics work of Brooks and his group [17] and have been an inspiration to numerous other robotics groups (see, e.g., the compilation <ref> [18] </ref>).
Reference: [19] <author> R. Beer. </author> <title> Intelligence as Adaptive Behavior. </title> <publisher> Academic press, </publisher> <address> NY, </address> <year> 1990. </year>
Reference-contexts: By contrast, Wilson's original animat [2], which was proposed for exploring the acquisition of simple behavior rules, is a point marker on a nonphysical 2D grid world that can move between squares containing food or obstacles. Other simple animats include the 2D cockroaches of Beer <ref> [19] </ref>. A more sophisticated animat is the kinematic dog described by Blumberg and Galyean [20]. Prior animats make use of perceptual oracles schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat.
Reference: [20] <author> B. M. Blumberg and T. A. Galyean. </author> <title> Multi-level direction of autonomous creatures for real-time virtual environments. </title> <booktitle> In Computer Graphics Proceedings, Annual Conference Series, Proc. </booktitle> <address> SIGGRAPH '95 (Los Angeles, CA), </address> <pages> pages 47-55. </pages> <publisher> ACM SIGGRAPH, </publisher> <month> August </month> <year> 1995. </year>
Reference-contexts: Other simple animats include the 2D cockroaches of Beer [19]. A more sophisticated animat is the kinematic dog described by Blumberg and Galyean <ref> [20] </ref>. Prior animats make use of perceptual oracles schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat. One can also find several instances of oracle vision in the behavioral animation literature [21, 22, 23].
Reference: [21] <author> C. W. Reynolds. </author> <title> Flocks, herds, and schools: A distributed behavioral model. </title> <journal> Computer Graphics, </journal> <volume> 21(4) </volume> <pages> 25-34, </pages> <year> 1987. </year>
Reference-contexts: Prior animats make use of perceptual oracles schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat. One can also find several instances of oracle vision in the behavioral animation literature <ref> [21, 22, 23] </ref>. Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere [3, 6, 23].
Reference: [22] <author> O. Renault, N. Magnenat-Thalmann, and D. Thalmann. </author> <title> A vision-based approach to be-havioural animation. </title> <journal> Visualization and Computer Animation, </journal> <volume> 1 </volume> <pages> 18-21, </pages> <year> 1990. </year>
Reference-contexts: Prior animats make use of perceptual oracles schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat. One can also find several instances of oracle vision in the behavioral animation literature <ref> [21, 22, 23] </ref>. Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere [3, 6, 23].
Reference: [23] <author> X. Tu and D. Terzopoulos. </author> <title> Perceptual modeling for the behavioral animation of fishes. </title> <booktitle> In Proc. 2nd Pacific Conf. on Computer Graphics, </booktitle> <address> Beijing, China, </address> <year> 1994. </year> <note> 24 To appear in Videre: </note> <institution> A Journal of Computer Vision Research, </institution> <address> 1(1), </address> <publisher> MIT Press, </publisher> <year> 1997. </year>
Reference-contexts: Prior animats make use of perceptual oracles schemes for directly interrogating the virtual world models to extract sensory information about the environment as needed by the animat. One can also find several instances of oracle vision in the behavioral animation literature <ref> [21, 22, 23] </ref>. Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere [3, 6, 23]. <p> Unlike oracle vision, for the animat vision approach to make sense, it is absolutely necessary that the animat and its world attain a high standard of visual fidelity. 3 Review of the Fish Animat The artificial fish model is developed elsewhere <ref> [3, 6, 23] </ref>. This section reviews the animat to a level of detail that suffices to comprehend the remainder of the paper. Each artificial fish is an autonomous agent with a deformable body comprising a graphical display model and a biomechanical model actuated by internal muscles. <p> For example, the artificial fish attends to sensory information about nearby food sources when foraging. The animats in our previous ALife simulations (described in <ref> [3, 6, 23] </ref>) employ a perceptual oracle scheme according to which the artificial fish may satisfy its perceptual needs via direct interrogation of the 3D world model.
Reference: [24] <author> D. Terzopoulos, A. Witkin, and M. Kass. </author> <title> Constraints on deformable models: Recovering 3D shape and nonrigid motion. </title> <journal> Artificial Intelligence, </journal> <volume> 36(1) </volume> <pages> 91-123, </pages> <year> 1988. </year>
Reference-contexts: To this end, photographs of real fishes, such as the one shown in Fig. 4 (a), are converted into 3D spline (NURBS) surface body models (Fig. 4 (b)). The digitized photographs are analyzed semi-automatically using deformable models <ref> [24] </ref>, in particular, a snake-mesh tool which is demonstrated in Fig. 4 (d-e) on a different fish image. The snake mesh slides freely over the image and can be manipulated using the mouse.
Reference: [25] <author> M. Swain and D. Ballard. </author> <title> Color indexing. </title> <journal> Int. J. Computer Vision, </journal> <volume> 7 </volume> <pages> 11-32, </pages> <year> 1991. </year>
Reference-contexts: For instance, if the fish is a predator, it would possess models of prey fish. The models are stored as a list of 64 fi 64 color images in the fish's memory. We have adopted into the active vision system the color histogram methods of Swain <ref> [25] </ref>. The fish employs these methods to detect and localize any target that may be imaged in the low resolution periphery of its retinas. <p> a unique color histogram (when the background is subtracted from the object) it can be detected in the periphery by histogram intersection and localized by histogram backprojection. 14 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. 4.3.1 Modified Color Histogram Intersection Method Swain <ref> [25] </ref> developed a technique called color indexing that efficiently identifies objects in a database in the presence of occlusion and over changes in viewpoint. He demonstrated that object color distributions without geometric information can provide a powerful cue for recognition. <p> We developed a more robust intersection measure that is invariant to scale changes. This new method iteratively scales down an initially large model color histogram to the approximate size of the target object appearing in the image. Following Swain's notation in <ref> [25] </ref>, his original histogram intersection measure is H = j=1 min (I j ; M j ) j=1 M j where I is the image color histogram, M is the model color histogram, and n is the number of histogram bins used. <p> A thorough description of this algorithm is available in <ref> [25] </ref>. 16 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. 4.3.3 Saccadic Eye Movements When a target is detected in the visual periphery, the eyes will saccade to the angular offset of the target to bring it within the fovea.
Reference: [26] <author> F. Ennesser and G. Medioni. </author> <title> Finding waldo, or focus of attention using local color information. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition Conf. (CVPR'93), </booktitle> <pages> pages 711-712, </pages> <year> 1993. </year>
Reference-contexts: To overcome this problem, we employ a new intersection measure after scaling the model histogram using (3). Our measure makes use of a weighted histogram intersection method inspired by the local histogram method proposed by Ennesser and Medioni <ref> [26] </ref>. Our measure is H N = j=1 W j min (I j ; M j ) j=1 M j where the weighting histogram W is given by W j = M j =2 l P j .
Reference: [27] <author> P.J. Burt, J.R. Bergen, R. Hingorani, R. Kolczynski, W.A. Lee, A. Leung, J. Lubin, and H. Shvaytser. </author> <title> Object tracking with a moving camera: An application of dynamic motion analysis. </title> <booktitle> Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 2-12, </pages> <address> March 1989. Irvine, CA. </address>
Reference-contexts: The displacement is computed 17 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. as a translational offset in the retinotopic coordinate system by a least squares minimization of the optical flow constraint equation between image frames at times t and t 1 <ref> [27, 28] </ref>.
Reference: [28] <author> M. Irani, B. Rousso, and S. Peleg. </author> <title> Recovery of ego-motion using image stabilization. </title> <booktitle> Proc. IEEE Workshop on Visual Motion, </booktitle> <pages> pages 454-460, </pages> <year> 1994. </year>
Reference-contexts: The displacement is computed 17 To appear in Videre: A Journal of Computer Vision Research, 1 (1), MIT Press, 1997. as a translational offset in the retinotopic coordinate system by a least squares minimization of the optical flow constraint equation between image frames at times t and t 1 <ref> [27, 28] </ref>.
Reference: [29] <author> B. K. P. Horn. </author> <title> Robot Vision. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: The flow constraint equation is given by <ref> [29] </ref> uI x + vI y + I t = 0; (7) where I x , I y , and I t are the derivatives of the image intensity I (x; y; t) = 3 where R, G, and B denote the color component channels. <p> V b Fixation Point Left Eye Right Eye q the range to the target from the gaze angles. Referring to Fig. 7, the range is <ref> [29] </ref> d = b sin ( L R ) cos ( P ) where b is the baseline between the two eyes, and P = 1 2 ( R + L ) is the left/right turn angle.
Reference: [30] <author> D. Coombs and C. Brown. </author> <title> Real-time binocular smooth pursuit. </title> <journal> Int. J. Computer Vision, </journal> <volume> 11(2) </volume> <pages> 147-164, </pages> <year> 1993. </year>
Reference-contexts: When the eyes are verged on a target the vergence angle is V = ( R L ) and its magnitude increases as the fish comes closer to the target <ref> [30] </ref>. 5 Vision-Guided Navigation The artificial fish can employ the direction of gaze of its eyes to effectively navigate its world. In particular, it is natural to use the gaze angles as the eyes are fixated on a target to navigate towards the target.
References-found: 30

