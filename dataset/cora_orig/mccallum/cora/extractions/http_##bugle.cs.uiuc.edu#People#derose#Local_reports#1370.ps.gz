URL: http://bugle.cs.uiuc.edu/People/derose/Local_reports/1370.ps.gz
Refering-URL: http://bugle.cs.uiuc.edu/People/derose/falcon_publications.html
Root-URL: http://www.cs.uiuc.edu
Title: In Proc. DAGS'94 Symposium on Parallel Computing and Problem Solving Environments An environment for the
Author: F. Makedon L. DeRose, K. Gallivan, E. Gallopoulos, B. Marsolf and D. Padua 
Date: July 1994  June 1994  
Address: College,  1308 West Main Street Urbana, Illinois 61801  
Affiliation: Dartmouth  Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Note: ed. pp 11-25,  
Pubnum: CSRD Report No. 1370  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Adams, J., Brainerd, W., Martin, J., Smith, B., and Wagener, J. </author> <title> Fortran 90 Handbook Complete ANSI/ISO Reference. </title> <publisher> McGraw-Hill Book Company, </publisher> <year> 1992. </year>
Reference-contexts: It is possible for the compiler to generate code for any programming language paradigm, with two well-suited choices being a compiled array language, like Fortran 90 <ref> [1] </ref>, and an object-oriented data parallel language, for instance pC++ [5]. In this paper we present our project to develop such an environment.
Reference: [2] <author> Aho, A., Sethi, R., and Ullman, J. </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: These optimizations can be applied at several different places within the environment and can be done either interactively or semi-automatically. However, for best results, the optimizations need to be applied at the most appropriate level. The optimizations to be used include the standard techniques <ref> [2] </ref> as well as other techniques such as context sensitivity, algebraic restructuring, and primitive-set to primitive-set translation. The standard techniques to be used include common subexpression elimination, copy propagation, and dead code elimination. <p> of processors to be used, the size of the cache, and the presence of vector registers. 2.3 Fortran 90 generator After the generation of the AST, the generation of Fortran 90 is straightforward, because most of the problems that normally appear in the code generation phase in a regular compiler <ref> [2] </ref> 7 Vector-Triad Vector-Matrix Product Submatrix Products Do i = 1, n Do i = 1, n Do i = 1, n, k Do j = 1, n End Do C (i:i+k-1,j:j+k-1) = A (i,1:n) * B (j,1:n) B (1:n,j:j+k-1) End Do End Do (e.g., register allocation) do not need to
Reference: [3] <author> Barrett, R., Berry, M., Chan, T., Demmel, J., Donato, J., Dongarra, J., Eijkhout, V., Pozo, R., Romine, C., and van der Vorst, H. </author> <title> Templates for the Solution of Linear Systems: Building Blocks for Iterative Methods. </title> <publisher> SIAM, </publisher> <year> 1993. </year>
Reference-contexts: The code segments used for the testing correspond to the conjugate gradient algorithm (CG) and to the preconditioned CG, using a diagonal preconditioner M , for solving Ax = b. The MATLAB version for the preconditioned algorithm, as found in <ref> [3] </ref>, is presented in Figure 7. For our experiments, three versions of the MATLAB code were tested, namely the non-preconditioned version, the original version, and an optimized version. In the first case, the preconditioner M was removed from the code.
Reference: [4] <author> Bodin, F., Beckman, P., Gannon, D., Gotwals, J., Narayana, S., Srinivas, S., and Winnicka, B. Sage++: </author> <title> An Object-Oriented Toolkit and Class Library for Building Fortran and C++ Restructuring Tools. </title> <booktitle> In OON-SKI'94 Proceedings of the First Annual Object-Oriented Numerics Conference (April 1993), </booktitle> <pages> pp. 122-138. 13 </pages>
Reference-contexts: is side-effect free, a parallelizing compiler will not need to perform interprocedural analysis to decide whether a loop that has a function or subroutine call on its body can be parallelized. 8 2.4 pC++ generator The generation of pC++ from the AST form is performed with the help of Sage++ <ref> [4] </ref>, a class library for building C++ restructuring tools. The main classes supported by the library are projects, files, and statements, which are arranged in a hierarchy, with each project consisting of one or more files and each file consistent of one or more statements.
Reference: [5] <author> Bodin, F., Beckman, P., Gannon, D., Narayana, S., and Yang, S. </author> <title> Distributed pC++: Basic Ideas for an Object Parallel Language. </title> <booktitle> In OON-SKI'93 Proceedings of the First Annual Object-Oriented Numerics Conference (April 1993), </booktitle> <pages> pp. 1-24. </pages>
Reference-contexts: It is possible for the compiler to generate code for any programming language paradigm, with two well-suited choices being a compiled array language, like Fortran 90 [1], and an object-oriented data parallel language, for instance pC++ <ref> [5] </ref>. In this paper we present our project to develop such an environment.
Reference: [6] <author> Budd, T. </author> <title> An APL Compiler. </title> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: In this way, the execution could be made more efficient by eliminating the need for some or all the run-time bookkeeping operations. A study of the effectiveness of this type of approach on APL programs is presented in <ref> [6] </ref>. When the bulk of the computations is done by the high-level array functions, the inefficiency of the interpreter is less of a problem, because these functions are not interpreted and the bookkeeping operations only need to be performed when the function is invoked and/or returns.
Reference: [7] <author> Char, B. W., Geddes, K. O., Gonnet, G. H., Leong, B. L., Monagan, M. B., and Watt, S. M. </author> <title> Maple V Language Reference Manual. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Another technique, that we call algebraic restructuring, uses the algebraic rules defined for the variables, whether they are scalars, vectors, or matrices, to restructure the operations performed on the variables. To perform such manipulations symbolic computation tools, such as Maple <ref> [7] </ref>, can be employed. In some cases applying these rules may be similar to the standard loop-based restructuring strategies already used, such as blocking of matrices, but we also want to be able to handle special matrix classes and more complex operators.
Reference: [8] <author> Chatterjee, S., Gilbert, J., Schreiber, R., and Teng, S.-H. </author> <title> Automatic Array Alignment in Data-Parallel Programs. </title> <booktitle> In Proceedings of the 20th Anual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages (January 1993), </booktitle> <pages> pp. 16-28. </pages>
Reference-contexts: We are also investigating the automatic generation of data partitioning and distribution through the use of directives (other techniques for automatic data partitioning have been described in <ref> [8] </ref>, [15], and [18]). With the use of heuristics, our compiler will select distributions based on its knowledge of the best data partitioning for each built-in function.
Reference: [9] <author> DeRose, L., and Padua, D. </author> <title> An Inference Mechanism for the Compilation of Interactive Array Languages. </title> <booktitle> In WHPC'94 Proceedings of the IEEE/USP International Workshop on High Performance Computing Compilers and Tools for Parallel Processing (Sao Paulo, </booktitle> <address> Brazil, </address> <month> March </month> <year> 1994), </year> <pages> pp. 97-112. </pages>
Reference-contexts: During this process, a mechanism combining static and dynamic inference methods for type, rank, and shape inference is executed <ref> [9] </ref>. One characteristic that makes interactive array languages easy to use is their lack of type 3 4 declarations.
Reference: [10] <author> Gallivan, K., and Marsolf, B. </author> <title> Practical Issues Related to Developing Object-Oriented Numerical Libraries. </title> <booktitle> In OON-SKI'94 Proceedings of the Second Annual Object-Oriented Numerics Conference (April 1994), </booktitle> <pages> pp. 93-106. </pages>
Reference-contexts: Primitive-set to primitive-set translation can also be used to translate the code to the level of numerical operations that will work best for the target machine and application <ref> [10] </ref>. Instead of dealing with the code only at a matrix operation level, this phase is able to convert the algorithms to matrix-vector operations or vector-vector operations. <p> Also, in order to provide load balancing it is necessary to determine how many objects should be grouped into each element of the collection. Additional details about the generation of pC++ code can be found in <ref> [10] </ref>. 3 Current Status At the current time we have a system that can parse MATLAB code into the AST and generate Fortran 90 or pC++ code for most operations and built-in functions 2 .
Reference: [11] <author> Gallivan, K. A., Plemmons, R. J., and Sameh, A. H. </author> <title> Parallel Algorithms for Dense Linear Algebra Computations. </title> <note> SIAM Review 32, </note> <month> 1 (March </month> <year> 1990), </year> <pages> 54-135. </pages>
Reference-contexts: As an example of algebraic restructuring, the associative rule for matrices can be applied to transform a triangular solve with a column sweep, (L 1 3 (L 1 1 fl f )))) ; into the product form <ref> [11] </ref>, (((L 1 3 )(L 1 1 ))f ) ; thereby generating more parallelism at the matrix operation level. In applying the rules, however, one must decide what goal the optimizations should try to achieve: improved serial performance, improved parallelism, or improved numerical stability, and what machine resources are available.
Reference: [12] <author> Gallopoulos, E., Houstis, E., and Rice, J. R. </author> <title> Computer as Thinker/Doer: Problem-Solving Environments for Computational Science. </title> <booktitle> IEEE Computational Science & Engineering 1, 2 (Summer 1994), </booktitle> <pages> 11-23. </pages>
Reference-contexts: One reason is the interactive nature of the language, which facilitates debugging and analysis. A second reason is that interactive array languages are usually contained within problem-solving environments which include easy-to-use facilities for displaying results both graphically and in tabular form <ref> [12] </ref>. Third, in these fl Supported by the CSRD Affiliates under grant from the U.S. National Security Agency. y Supported by the National Science Foundation under Grant No. US NSF CCR-9120105 and by ARPA under a subcontract from the University of Minnesota of Grant No.
Reference: [13] <author> Gilman, L., and Rose, A. </author> <title> APL : An Interactive Approach. </title> <publisher> Wiley, </publisher> <year> 1984. </year>
Reference-contexts: 1 Introduction Interactive array languages such as APL <ref> [13, 21] </ref> and MATLAB [19] are powerful programming tools for the development of numerical programs and libraries. Many computational scientists consider that it is easier to prototype algorithms and applications using array languages instead of conventional languages such as Fortran and C.
Reference: [14] <author> Girkar, M., and Polychronopoulos, C. D. </author> <title> Automatic Extraction of Functional Parallelism from Ordinary Programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 3, </journal> <month> 2 (March </month> <year> 1992). </year>
Reference-contexts: directives for data distribution for solving the linear system using block factorization, as shown in Figure 5, using HPF notation. !HPF$ distribute A (block,*) !HPF$ align with A :: L,U !HPF$ align (:) with A (:,1) :: b,t,y,x Directives can also be used to facilitate the exploitation of functional parallelism <ref> [14] </ref> and loop parallelism. As described before, MATLAB functions are side-effect free; hence, the work executed by a parallelizing compiler to detect functional parallelism can be simplified significantly if this information is given as a directive.
Reference: [15] <author> Gupta, M., and Banerjee, P. </author> <title> Demonstration of Automatic Data Partitioning Techniques for Parallelizing Compilers on Multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems 3, </journal> <month> 2 (March </month> <year> 1992). </year>
Reference-contexts: We are also investigating the automatic generation of data partitioning and distribution through the use of directives (other techniques for automatic data partitioning have been described in [8], <ref> [15] </ref>, and [18]). With the use of heuristics, our compiler will select distributions based on its knowledge of the best data partitioning for each built-in function.
Reference: [16] <author> Harrison, III, W. L., and Padua, D. </author> <title> PARCEL: Project for the Automatic Restructuring and Concurrent Evaluation of Lisp. </title> <booktitle> In Proceedings of 1988 Int'l. Conf. on Supercomputing, </booktitle> <address> St. Malo, France (July 1988), </address> <pages> pp. 527-538. </pages>
Reference-contexts: This approach, however, requires special handling for the case of recursive functions <ref> [16] </ref>. The third approach is to apply interprocedural analysis. This would overcome the code explosion that could occur with inlining and will make it possible to deal with recursive functions. However, it comes at the expense of a more complex implementation.
Reference: [17] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran Language Specification, </title> <month> May </month> <year> 1993. </year> <note> Version 1.0. </note>
Reference-contexts: Secondly, because Fortran 90 is also an array language, it has many features that facilitate the compilation process, especially for vector computations and handling memory management. Thirdly, parallelism and data distribution can be exploited with the migration to High Performance Fortran (HPF) <ref> [17] </ref>, which is a superset of Fortran 90. Moreover, most of today's massively parallel machines have Fortran compilers that are a subset of Fortran 90, containing the array language features and some mechanism for data distribution.
Reference: [18] <author> Hiranandani, S., Kennedy, K., and Tseng, C. </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines. </title> <journal> Communications of the ACM 35, </journal> <month> 8 (August </month> <year> 1992). </year> <month> 14 </month>
Reference-contexts: We are also investigating the automatic generation of data partitioning and distribution through the use of directives (other techniques for automatic data partitioning have been described in [8], [15], and <ref> [18] </ref>). With the use of heuristics, our compiler will select distributions based on its knowledge of the best data partitioning for each built-in function.
Reference: [19] <author> The Math Works, Inc. </author> <title> Matlab, High-Performance Numeric Computation and Visual--ization Software. User's Guide, </title> <year> 1992. </year>
Reference-contexts: 1 Introduction Interactive array languages such as APL [13, 21] and MATLAB <ref> [19] </ref> are powerful programming tools for the development of numerical programs and libraries. Many computational scientists consider that it is easier to prototype algorithms and applications using array languages instead of conventional languages such as Fortran and C.
Reference: [20] <author> Padua, D., Eigenmann, R., Hoeflinger, J., Petersen, P., Tu, P., Weatherford, S., and Faigin, K. </author> <title> Polaris: A New-Generation Parallelizing Compiler for MPP's. </title> <type> Tech. rep., </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Research and Development, </institution> <month> June </month> <year> 1993. </year> <note> CSRD Report No. 1306. </note>
Reference-contexts: These issues, however, are treated by the front-end compiler, during the generation of the AST. Most of the parallelization of the Fortran 90 code will be done by interfacing with Polaris, a parallelizing compiler for massively parallel machines <ref> [20] </ref>. We are also investigating the automatic generation of data partitioning and distribution through the use of directives (other techniques for automatic data partitioning have been described in [8], [15], and [18]).
Reference: [21] <author> Pommier, S. </author> <title> An Introduction to APL. </title> <publisher> Cambridge University Press, </publisher> <address> New York, </address> <year> 1983. </year>
Reference-contexts: 1 Introduction Interactive array languages such as APL <ref> [13, 21] </ref> and MATLAB [19] are powerful programming tools for the development of numerical programs and libraries. Many computational scientists consider that it is easier to prototype algorithms and applications using array languages instead of conventional languages such as Fortran and C.
Reference: [22] <author> Schwartz, J. T. </author> <title> Automatic Data Structure Choice in a Language of a Very High Level. </title> <booktitle> Communications of the ACM 18 (1975), </booktitle> <pages> 722-727. 15 </pages>
Reference-contexts: MATLAB works with only two types for variables, real and complex, which our type inference approach has to support. The solution we adopted is to use a combination of static and dynamic analyses. The static inference uses a type algebra, similar to the one described in <ref> [22] </ref> for SETL. This algebra operates on the type of the MATLAB objects and is implemented with the use of tables for all arithmetic operations. The dynamic analysis is used for all expressions for which type cannot be inferred statically by the compiler.
References-found: 22

