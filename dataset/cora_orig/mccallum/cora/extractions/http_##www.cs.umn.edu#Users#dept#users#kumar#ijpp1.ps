URL: http://www.cs.umn.edu/Users/dept/users/kumar/ijpp1.ps
Refering-URL: http://www.cs.umn.edu/Users/dept/users/kumar/
Root-URL: http://www.cs.umn.edu
Email: Arpanet: kumar@sally.utexas.edu  
Title: Parallel Depth First Search, Part I: Implementation  
Author: V. Nageshwara Rao and Vipin Kumar 
Note: This work was supported by Army Research Office grant DAAG29-84-K-0060 to the Artificial Intelligence Laboratory, and Office of Naval Research Grant N00014-86-K-0763 to the  
Address: Austin, Texas 78712  Austin.  
Affiliation: Department of Computer Sciences, University of Texas at Austin,  computer science department at the University of Texas at  
Abstract: This paper presents a parallel formulation of depth-first search which retains the storage efficiency of sequential depth-first search and can be mapped on to any MIMD architecture. To study its effectiveness it has been implemented to solve the 15-puzzle problem on three commercially available multiprocessors | Sequent Balance 21000, the Intel Hypercube and BBN Butterfly. We have been able to achieve fairly linear speedup on Sequent up to 30 processors ( the maximum configuration available) and on the Intel Hypercube and BBN Butterfly up to 128 processors ( the maximum configurations available). Many researchers considered the ring architecture to be quite suitable for parallel depth-first search. Our experimental results show that hypercube and shared-memory architectures are significantly better. At the heart of our parallel formulation is a dynamic work distribution scheme that divides the work between different processors. The effectiveness of the parallel formulation is strongly influenced by the work distribution scheme and architectural features such as presence/absence of shared memory, the diameter of the network, relative speed of the communication network, etc. In a companion paper[16], we analyze 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. W. Dijkstra, W. H. Seijen, and A. J. M. Van Gasteren. </author> <title> Derivation of a termination detection algorithm for a distributed computation. </title> <journal> Information Processing Letters, </journal> <volume> 16-5:217-219, </volume> <year> 1983. </year>
Reference-contexts: The termination of an iteration can be detected in many ways. On shared-memory architectures (e.g., Sequent, BBN Butterfly), we use a globally shared variable to keep track of the number of idle processors. On distributed memory architectures (e.g., the Intel Hypercube), we use Dijkstra's token termination detection algorithm <ref> [1] </ref>. See [25] for more details. 3.5 Applicability to Depth-First B&B Our parallel formulation is applicable to depth-first B&B with one minor modification. Now we need to keep all the processors informed of the current best solution path.
Reference: [2] <author> Terry Disz, Ewing Lusk, and Ross Overbeek. </author> <title> Experiments with OR-parallel logic programs. </title> <booktitle> In Proceedings of the Fourth International Conference on Logic Programming, </booktitle> <volume> volume 2, </volume> <pages> pages 576-600, </pages> <year> 1987. </year>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive.
Reference: [3] <author> O. I. El-Dessouki and W. H. Huen. </author> <title> Distributed enumeration on network computers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-29:818-825, </volume> <month> September </month> <year> 1980. </year>
Reference-contexts: Fig. 8 gives details of speedup achieved on various architectures. These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search <ref> [4, 32, 21, 3] </ref>. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> The work done by each processor in this scheme can be executed in parallel using our work distribution scheme to give additional speedup. A number of parallel formulations have been proposed for depth-first B&B. One of the first such formulation was proposed by El-Dessoki and Huen <ref> [3] </ref>. Their formulation uses dynamic work sharing much like our formulation. Imai et al [7] proposed another parallel formulation of depth-first B&B. In this scheme, the search tree is maintained as a shared data structure, and different processors remove and expand one node at a time in depth-first fashion.
Reference: [4] <author> Raphael A. Finkel and Udi Manber. </author> <title> DIB a distributed implementation of backtracking. </title> <journal> ACM Transactions of Programming Languages and Systems, </journal> <volume> 9 No. 2 </volume> <pages> 235-256, </pages> <month> April </month> <year> 1987. </year>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> We have been able to achieve linear speedup on Sequent Balance up to 30 processors ( the maximum configuration available) and on the Intel Hypercube and BBN Butterfly up to 128 processors ( the maximum configurations available). Contrary to the expectation of many researchers <ref> [32, 4, 21] </ref>, the performance on the ring architecture is not very good. At the heart of our parallel formulation is a dynamic work distribution scheme that divides the work between different processors. <p> Fig. 8 gives details of speedup achieved on various architectures. These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search <ref> [4, 32, 21, 3] </ref>. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search [4, 32, 21, 3]. Many of these researchers <ref> [4, 32, 21] </ref> have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> If the initial distribution is quite good, then good speedup can be obtained even with the simple work distribution scheme. But good distribution can be difficult to obtain especially for large problems and large number of processors. Finkel and Manber's work <ref> [4] </ref> on distributed backtracking has many similarities to the work reported in this paper. They have experimented with several work distribution strategies for the ring architecture. These strategies are different than the one used in this paper.
Reference: [5] <author> Bogumil Hausman, Andrzej Ciepielewski, and Seif Haridi. </author> <title> OR-parallel PROLOG made efficient on shared memory multiprocessors. </title> <booktitle> In Proceedings of the Fourth Symposium on Logic Programming, </booktitle> <pages> pages 69-79, </pages> <year> 1987. </year>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive.
Reference: [6] <author> Ellis Horowitz and Sartaj Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <address> Rockville, MD, </address> <year> 1978. </year>
Reference-contexts: 1 Introduction Depth-first search (DFS) is a general technique used in Artificial Intelligence for solving a variety of problems in planning, decision making, theorem proving, expert systems, etc. [14, 24]. It is also used under the name of backtracking to solve various combinatorial problems <ref> [6] </ref> and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems [10, 11] and for theorem proving [29]. <p> Also, the work distribution schemes used in our implementation can be used in the parallel implementations of other tree traversal algorithms such as divide-and-conquer <ref> [6] </ref>. Section 2 gives a brief review of sequential depth-first search, the IDA* algorithm, and depth-first branch-and-bound. Section 3 presents a parallel formulation of DFS, and discusses its applicability to IDA* and depth-first branch-and-bound. Section 4 presents performance results of solving 15-puzzle by parallel IDA* on various parallel processors.
Reference: [7] <author> M. Imai, Y. Yoshida, and T. Fukumura. </author> <title> A parallel searching scheme for multiprocessor systems and its application to combinatorial problems. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 416-418, </pages> <year> 1979. </year>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> Superlinear speedup indicates that parallel DFS is able to find a goal node by searching a smaller space than (sequential) DFS. Many other researchers have encountered superlinear speedup <ref> [7, 21] </ref> in parallel depth-first search. <p> A number of parallel formulations have been proposed for depth-first B&B. One of the first such formulation was proposed by El-Dessoki and Huen [3]. Their formulation uses dynamic work sharing much like our formulation. Imai et al <ref> [7] </ref> proposed another parallel formulation of depth-first B&B. In this scheme, the search tree is maintained as a shared data structure, and different processors remove and expand one node at a time in depth-first fashion.
Reference: [8] <author> Virendra K. Janakiram, Dharma P. Agrawal, and Ram Mehrotra. </author> <title> Randomized parallel algorithms for prolog programs and backtracking applications. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 278-281, </pages> <year> 1987. </year>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> Clearly a shared-memory machine or a distributed-memory machine with low diameter like the Intel Hypercube is far superior to a ring for a large number of processors. Janakiram et. al. <ref> [8] </ref> present a parallel formulation in which different processors search the space in different (random) orders. The speedup in this scheme is dependent upon the probability distribution of goal nodes in the search tree. <p> The scheme is in principle similar to executing different iterations of IDA* in parallel with dynamically changing cost bounds. This approach requires very little communication between processors, but the maximum speedup obtained is problem dependent. As with Janakiram's scheme <ref> [8] </ref>, the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search [9, 17, 28, 5, 30, 2].
Reference: [9] <author> S. Kasif, M. Kohli, and J. Minker. </author> <title> PRISM: A parallel inference system for problem solving. </title> <type> Technical report, </type> <institution> Computer Science Department, University of Maryland, College Park, MD, </institution> <year> 1983. </year>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive.
Reference: [10] <author> R. E. Korf. </author> <title> Depth-first iterative-deepening: An optimal admissible tree search. </title> <journal> Artificial Intelligence, </journal> <volume> 27 </volume> <pages> 97-109, </pages> <year> 1985. </year> <month> 25 </month>
Reference-contexts: It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems <ref> [10, 11] </ref> and for theorem proving [29]. A major advantage of the depth-first search strategy is that it requires very little memory. <p> We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. To study its effectiveness we have incorporated it in IDA* (a DFS algorithm with iterative-deepening <ref> [10] </ref>) to solve the 15-puzzle problem [23] on three commercially available multiprocessors Sequent Balance 1 21000, the Intel iPSC 2 Hypercube and BBN Butterfly 3 . We also tested the effectiveness of parallel depth-first search on a ring embedded in the Intel Hypercube. <p> For most other search techniques (such as breadth-first and best-first) the storage requirement is exponential in the length of solution path, whereas for depth-first search, the storage requirement is linear in the depth of the space searched <ref> [14, 10] </ref>. But simple depth-first search has two major drawbacks. 1. If the search space to the left of the first goal node is infinite (or very large) then search would never terminate (or take a very long time). 2. <p> Furthermore, it requires only linear storage. In contrast, A*, the most widely known admissible state-space-search algorithm, requires exponential storage for most practical problems [24]. For a detailed description of IDA* and its properties, the reader is referred to <ref> [10, 11] </ref>. 2.3 Depth-First Branch-and-Bound It is also possible to find an optimal solution path in a finite search space using DFS (even without iterative deepening). <p> The 15-puzzle problem is particularly suited for testing the effectiveness of parallel DFS, as it is possible to create search spaces of different sizes (W) by choosing appropriate initial positions. IDA* is the best known sequential algorithm to find optimal solution paths for the 15-puzzle problem <ref> [10] </ref>. It is significantly faster 9 than simple DFS, as it can use a heuristic function to focus the search (we use the Manhattan distance heuristic [23]). <p> In all the results reported in this paper, we use the third splitting strategy, and keep the cutoff depth between :25 ? depth and :75 ? depth . We ran our algorithm on a number of problem instances given in Korf's paper <ref> [10] </ref>. Each problem was solved using IDA* on one processor, and using parallel IDA* on 9, 6 and 3 processors. As explained earlier (Section 3.3), for the same problem instance, parallel IDA* can expand different number of nodes in the last iteration on different runs.
Reference: [11] <author> Richard Korf. </author> <title> Optimal path finding algorithms. </title> <editor> In L. N. Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems <ref> [10, 11] </ref> and for theorem proving [29]. A major advantage of the depth-first search strategy is that it requires very little memory. <p> Furthermore, it requires only linear storage. In contrast, A*, the most widely known admissible state-space-search algorithm, requires exponential storage for most practical problems [24]. For a detailed description of IDA* and its properties, the reader is referred to <ref> [10, 11] </ref>. 2.3 Depth-First Branch-and-Bound It is also possible to find an optimal solution path in a finite search space using DFS (even without iterative deepening).
Reference: [12] <author> V. Kumar and L. N. Kanal. </author> <title> A general branch-and-bound formulations for understanding and synthesizing and/or tree search procedures. </title> <journal> Artificial Intelligence, </journal> <volume> 21 </volume> <pages> 179-198, </pages> <year> 1983. </year>
Reference-contexts: Although depth-first B&B would usually perform much more work than best-first B&B, it is (like any other depth-first search strategy) highly space efficient. Note that the alpha-beta game tree search algorithm can be viewed as a depth-first B&B algorithm (see <ref> [12, 15] </ref>). 3 Parallel Depth-First Search 3.1 A Parallel Formulation of Depth-First Search) We parallelize DFS by sharing the work to be done among a number of processors. Each processor searches a disjoint part of the search space in a depth-first fashion.
Reference: [13] <author> V. Kumar and L. N. Kanal. </author> <title> Parallel branch-and-bound formulations for and/or tree search. </title> <journal> IEEE Transactions Pattern Analysis and Machine Intelligence, </journal> <volume> PAMI-6:768-778, </volume> <year> 1984. </year>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> Imai's approach more or less follows a left to right scan of the tree and hence is less prone to anomalies than our parallel DFS. Kumar and Kanal <ref> [13] </ref> present a parallel formulation of depth-first B&B in which different processors search the space with different expectations (cost bounds). At any time, at least one processor has the property that if it terminates, it returns an optimal solution path; the other processors conduct a look-ahead search.
Reference: [14] <author> Vipin Kumar. </author> <title> Depth-first search. </title> <editor> In Stuart C. Shapiro, editor, </editor> <booktitle> Encyclopaedia of Artificial Intelligence: </booktitle> <volume> Vol 2, </volume> <pages> pages 1004-1005. </pages> <publisher> John Wiley and Sons, </publisher> <address> New York, NY, </address> <year> 1987. </year> <note> Revised version appears in the second edition of the encyclopedia to be published in 1992. </note>
Reference-contexts: 1 Introduction Depth-first search (DFS) is a general technique used in Artificial Intelligence for solving a variety of problems in planning, decision making, theorem proving, expert systems, etc. <ref> [14, 24] </ref>. It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. <p> Numbers on the right corners of the nodes show the order in which the nodes are expanded. Since deeper nodes are expanded first, the search is called depth-first. A more detailed treatment is provided in <ref> [14] </ref> and [24]. The unit of computation in a search algorithm is the time taken for one node expansion. <p> For most other search techniques (such as breadth-first and best-first) the storage requirement is exponential in the length of solution path, whereas for depth-first search, the storage requirement is linear in the depth of the space searched <ref> [14, 10] </ref>. But simple depth-first search has two major drawbacks. 1. If the search space to the left of the first goal node is infinite (or very large) then search would never terminate (or take a very long time). 2.
Reference: [15] <author> Vipin Kumar, Dana Nau, and L. N. Kanal. </author> <title> General branch-and-bound formulation for and/or graph and game tree search. </title> <editor> In L. N. Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence. </booktitle> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: Although depth-first B&B would usually perform much more work than best-first B&B, it is (like any other depth-first search strategy) highly space efficient. Note that the alpha-beta game tree search algorithm can be viewed as a depth-first B&B algorithm (see <ref> [12, 15] </ref>). 3 Parallel Depth-First Search 3.1 A Parallel Formulation of Depth-First Search) We parallelize DFS by sharing the work to be done among a number of processors. Each processor searches a disjoint part of the search space in a depth-first fashion.
Reference: [16] <author> Vipin Kumar and V. Nageshwara Rao. </author> <title> Parallel depth-first search, part II: Analysis. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 16 </volume> (6):501-519, December 1987. 
Reference-contexts: Our experiments suggest (and theoretical analysis of <ref> [16] </ref> predicts) that on suitable architectures, it is feasible to speed up depth-first search by several orders of magnitude. <p> If the work given out is too small, then the requester would become idle too soon. If the work given out is too large, then the donor will become idle too soon. From the analysis in <ref> [16] </ref>, it is clear that a 1 2 -split leads to an overall high efficiency for the shared-memory and hypercube architectures. <p> Diameter of the network Diameter of the network dictates the amount of communication that needs to be made in order to achieve good load balancing. The effect of diameter is analyzed in <ref> [16] </ref>. Table 1 presents the values of these parameters for the machines under consideration. (U comm and U calc are specific to the implementation of parallel DFS for 15-puzzle 8 .) 8 In our implementations for distributed-memory systems, processors check arrival of requests periodically every 50 node expansions. <p> One reason is that the ratio U comm U calc is much higher for the Intel Hypercube than for BBN Butterfly. Also, the work distribution schemes used for the two architectures are different. In a companion paper <ref> [16] </ref>, we analyze the effect of work distribution schemes, architectural features and problem size on speedup. 4.4 Performance of Parallel Cost-Bounded DFS. Parallel version of IDA* incurs two kinds of overheads | the overhead due to work distribution, and the overhead due to termination detection. <p> Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. The work distribution schemes in these formulations is very similar to the scheme presented in this paper. From our experiments as well as the analysis in <ref> [16] </ref> it is clear that this work distribution scheme is not able to provide good speedup on large rings. The initialization part in Monien's [21] and Wah's [32] scheme is slightly different than the one discussed in this paper. <p> Finkel and Manber's work [4] on distributed backtracking has many similarities to the work reported in this paper. They have experimented with several work distribution strategies for the ring architecture. These strategies are different than the one used in this paper. In a companion paper <ref> [16] </ref>, we analyze the effectiveness of different work distribution strategies, and present a new improved work distribution strategy for the ring architecture. The main thrust of Finkel's work is on developing a package called DIB which allows a variety of applications requiring tree traversal to be implemented on a multicomputer. <p> Average problem size ' 6.68 million nodes 22 While our experiments agree with Monien, Wah and Finkel in that good efficiency is achievable for 16 processor rings, our experimental results show and our analysis in <ref> [16] </ref> predicts that it is unrealistic to achieve the same for rings with larger (64-128) number of processors. Clearly a shared-memory machine or a distributed-memory machine with low diameter like the Intel Hypercube is far superior to a ring for a large number of processors. <p> In our experiments, we are able to achieve a speedup of over 100 on 128 processors on commercial multiprocessors such as BBN Butterfly and the Intel Hypercube. These results (as well as our analysis in <ref> [16] </ref>) clearly show that depth-first search can be speeded up by several orders of magnitude. The sequential DFS running on the CRAY-XMP (which has one of the fastest scalar CPUs) is only about 16 times faster than a single node of the Intel Hypercube. <p> Other researchers [32, 21] considered the ring architecture to be quite suitable for parallel depth-first search. Our experimental results show that hypercube and shared-memory architectures are significantly better. In a companion paper <ref> [16] </ref>, we analyze the effectiveness of different load-balancing schemes and architectures, and also present new improved work distribution schemes for ring and shared-memory architectures.
Reference: [17] <author> K. Kumon, H. Masuzawa, A. Itashiki, K. Satoh, and Y. Sohma. Kabu-wake: </author> <title> A new parallel inference method and its evaluation. </title> <booktitle> In Proceedings of COMPCON 86, </booktitle> <month> March </month> <year> 1986. </year>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive.
Reference: [18] <author> T. H. Lai and Sartaj Sahni. </author> <title> Anomalies in parallel branch and bound algorithms. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 183-190, </pages> <year> 1983. </year>
Reference-contexts: If parallel DFS expands fewer nodes than DFS, then we can observe speedup of greater than N using N processors. This phenomenon (of greater than N speedup on N processors) is referred to as the acceleration anomaly <ref> [18, 20] </ref>. But there can be no detrimental anomaly (i.e., speedup of less than 1 on N processors) in parallel DFS, if we assume that all the processors have roughly equal speed.
Reference: [19] <author> E. L. Lawler and D. Woods. </author> <title> Branch-and-bound methods: A survey. </title> <journal> Operations Research, </journal> <volume> 14, </volume> <year> 1966. </year>
Reference: [20] <author> Guo-Jie Li and Benjamin W. Wah. </author> <title> Coping with anomalies in parallel branch-and-bound algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35, </volume> <month> June </month> <year> 1986. </year>
Reference-contexts: If parallel DFS expands fewer nodes than DFS, then we can observe speedup of greater than N using N processors. This phenomenon (of greater than N speedup on N processors) is referred to as the acceleration anomaly <ref> [18, 20] </ref>. But there can be no detrimental anomaly (i.e., speedup of less than 1 on N processors) in parallel DFS, if we assume that all the processors have roughly equal speed.
Reference: [21] <author> B. Monien and O. Vornberger. </author> <title> The ring machine. </title> <type> Technical report, </type> <institution> University of Paderborn, </institution> <address> FRG, </address> <year> 1985. </year> <note> Also in Computers and Artificial Intelligence, 3(1987). 26 </note>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> We have been able to achieve linear speedup on Sequent Balance up to 30 processors ( the maximum configuration available) and on the Intel Hypercube and BBN Butterfly up to 128 processors ( the maximum configurations available). Contrary to the expectation of many researchers <ref> [32, 4, 21] </ref>, the performance on the ring architecture is not very good. At the heart of our parallel formulation is a dynamic work distribution scheme that divides the work between different processors. <p> Superlinear speedup indicates that parallel DFS is able to find a goal node by searching a smaller space than (sequential) DFS. Many other researchers have encountered superlinear speedup <ref> [7, 21] </ref> in parallel depth-first search. <p> Fig. 8 gives details of speedup achieved on various architectures. These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search <ref> [4, 32, 21, 3] </ref>. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search [4, 32, 21, 3]. Many of these researchers <ref> [4, 32, 21] </ref> have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> Dynamic division of work has been used by many researchers for parallelizing depth-first search [4, 32, 21, 3]. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger <ref> [21] </ref> and Wah and Ma [32] present parallel depth-first search procedures on a ring network. The work distribution schemes in these formulations is very similar to the scheme presented in this paper. <p> From our experiments as well as the analysis in [16] it is clear that this work distribution scheme is not able to provide good speedup on large rings. The initialization part in Monien's <ref> [21] </ref> and Wah's [32] scheme is slightly different than the one discussed in this paper. Before starting parallel search they divide the search space into N parts, and give each part to a processor. <p> The architecture of the multiprocessor and the work distribution algorithm have been found to have significant impact on the performance of the parallel depth-first search algorithm. Other researchers <ref> [32, 21] </ref> considered the ring architecture to be quite suitable for parallel depth-first search. Our experimental results show that hypercube and shared-memory architectures are significantly better.
Reference: [22] <author> Bernard Nadel. </author> <title> Tree search and arc consistency in constraint satisfaction algorithms. </title> <editor> In L. N. Kanal and Vipin Kumar, editors, </editor> <booktitle> Search in Artificial Intelligence, </booktitle> <pages> pages 287-342. </pages> <publisher> Springer-Verlag, </publisher> <address> New York, NY, </address> <year> 1988. </year>
Reference-contexts: 1 Introduction Depth-first search (DFS) is a general technique used in Artificial Intelligence for solving a variety of problems in planning, decision making, theorem proving, expert systems, etc. [14, 24]. It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems <ref> [22] </ref>. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems [10, 11] and for theorem proving [29]. A major advantage of the depth-first search strategy is that it requires very little memory.
Reference: [23] <author> Nils J. Nilsson. </author> <booktitle> Principles of Artificial Intelligence. </booktitle> <publisher> Tioga, </publisher> <address> Palo Alto, CA, </address> <year> 1980. </year>
Reference-contexts: We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. To study its effectiveness we have incorporated it in IDA* (a DFS algorithm with iterative-deepening [10]) to solve the 15-puzzle problem <ref> [23] </ref> on three commercially available multiprocessors Sequent Balance 1 21000, the Intel iPSC 2 Hypercube and BBN Butterfly 3 . We also tested the effectiveness of parallel depth-first search on a ring embedded in the Intel Hypercube. <p> Iterative-Deepening-A* (IDA*)[10] is a variation of depth-first search that takes care of both these drawbacks. 2.2 Iterative-Deepening A*(IDA*) IDA* performs repeated cost-bounded depth-first search (DFS) over the search space. Like other heuristic search procedures (such as the A* algorithm <ref> [23] </ref>), it makes use of two functions h and g. For a node n, g (n) is the cost of reaching n from the initial node, and h (n) is an estimate of the cost of reaching a nearest goal node from n. <p> Since each processor searches the space in a depth-first manner, the (part of) state space to be searched is efficiently represented by a stack. The depth of the stack is the depth of the 4 A search algorithm is admissible if it always finds an optimal solution path <ref> [23] </ref>. 6 node being currently explored; and each level of the stack keeps track of untried alternatives. Each processor maintains its own local stack on which it executes DFS. When the local stack is empty, it takes some of the untried alternatives of another processor's stack. <p> fl U calc + misc: overhead 13 Desired goal configuration A starting configuration 1 2 3 4 9 10 8 15 5 6 7 8 13 14 15 4.2 Experiments for Evaluating Parallel DFS To test the effectiveness of parallel DFS, we have used it to solve the 15-puzzle problem <ref> [23] </ref>. 15-puzzle is a 4x4 square tray containing 15 square tiles. The remaining sixteenth square is uncovered. Each tile has a number on it. A tile that is adjacent to the blank space can be slid into that space. <p> IDA* is the best known sequential algorithm to find optimal solution paths for the 15-puzzle problem [10]. It is significantly faster 9 than simple DFS, as it can use a heuristic function to focus the search (we use the Manhattan distance heuristic <ref> [23] </ref>). We have parallelized IDA* to test the effectiveness of our parallel formulation of depth-first search. 9 Note that due to the use of heuristic function h, the effective branching factor of a search tree in IDA* could be much smaller than the average number of successors of a node.
Reference: [24] <author> Judea Pearl. </author> <title> Heuristics-Intelligent Search Strategies for Computer Problem Solving. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1984. </year>
Reference-contexts: 1 Introduction Depth-first search (DFS) is a general technique used in Artificial Intelligence for solving a variety of problems in planning, decision making, theorem proving, expert systems, etc. <ref> [14, 24] </ref>. It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. <p> Numbers on the right corners of the nodes show the order in which the nodes are expanded. Since deeper nodes are expanded first, the search is called depth-first. A more detailed treatment is provided in [14] and <ref> [24] </ref>. The unit of computation in a search algorithm is the time taken for one node expansion. <p> Iterative-Deepening-A* is an important 5 admissible 4 state-space search algorithm, as it runs in asymptotically optimal time for a wide class of search problems. Furthermore, it requires only linear storage. In contrast, A*, the most widely known admissible state-space-search algorithm, requires exponential storage for most practical problems <ref> [24] </ref>. For a detailed description of IDA* and its properties, the reader is referred to [10, 11]. 2.3 Depth-First Branch-and-Bound It is also possible to find an optimal solution path in a finite search space using DFS (even without iterative deepening).
Reference: [25] <author> V. Nageshwara Rao, V. Kumar, and K. Ramesh. </author> <title> A parallel implementation of iterative-deepening-a*. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI-87), </booktitle> <pages> pages 878-882, </pages> <year> 1987. </year>
Reference-contexts: On shared-memory architectures (e.g., Sequent, BBN Butterfly), we use a globally shared variable to keep track of the number of idle processors. On distributed memory architectures (e.g., the Intel Hypercube), we use Dijkstra's token termination detection algorithm [1]. See <ref> [25] </ref> for more details. 3.5 Applicability to Depth-First B&B Our parallel formulation is applicable to depth-first B&B with one minor modification. Now we need to keep all the processors informed of the current best solution path.
Reference: [26] <author> V. Nageshwara Rao and Vipin Kumar. </author> <title> Superlinear speedup in state-space search. </title> <booktitle> In Proceedings of the 1988 Foundation of Software Technology and Theoretical Computer Science, number 338 in Lecture Notes in Computer Science, </booktitle> <pages> pages 161-174. </pages> <publisher> Springer-Verlag, </publisher> <year> 1988. </year>
Reference-contexts: Superlinear speedup indicates that parallel DFS is able to find a goal node by searching a smaller space than (sequential) DFS. Many other researchers have encountered superlinear speedup [7, 21] in parallel depth-first search. In <ref> [26] </ref>, we present an analysis of the phenomenon of superlinear speedup in depth-first search and show that it is possible to obtain superlinear speedup on average if the search space has certain characteristics. 10 We compute speedup by T he time taken by IDA fl (the best known sequential algorithm f
Reference: [27] <author> Charles L. Seitz. </author> <title> The cosmic cube. </title> <journal> Communications of the ACM, </journal> <volume> 28-1:22-33, </volume> <year> 1985. </year>
Reference-contexts: with the parallelization of simple DFS and IDA*, although most of the discussion is applicable to the parallelization of depth-first B&B as well. 4 Performance of Parallel DFS on various architec tures 4.1 Parallel Architectures used in Experiments We have studied the performance of parallel DFS on shared-memory/common-bus, shared-memory/-switch, hypercube <ref> [27] </ref>, 1-ring and 2-ring architectures .
Reference: [28] <author> Kish Shen and David H.D. Warren. </author> <title> A simulation study of the Argonne model for OR-parallel execution of PROLOG. </title> <booktitle> In Proceedings of the Fourth Symposium on Logic Programming, </booktitle> <pages> pages 54-68, </pages> <address> September 1987. San Francisco, CA. </address>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive. <p> Hence much of the current research in such systems is on developing techniques that allow parts of the stack to be used by many processors <ref> [28] </ref>. 6 Conclusions. We have presented performance results of a parallel formulation of depth-first search on various parallel architectures. Our parallel DFS retains the storage efficiency of sequential DFS, and it can be mapped on to any MIMD architecture.
Reference: [29] <author> M. E. Stickel and W. M. Tyson. </author> <title> An analysis of consecutively bounded depth-first search with applications in automated deduction. </title> <booktitle> In Proceedings of International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1073-1075, </pages> <year> 1985. </year>
Reference-contexts: Execution of a Prolog program can be viewed as depth-first search of a proof tree [31]. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems [10, 11] and for theorem proving <ref> [29] </ref>. A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search [7, 32, 13, 4, 21, 8].
Reference: [30] <author> Peter Tinker. </author> <title> Performance and pragmatics of an OR-parallel logic programming system. </title> <journal> International Journal of Parallel Programming, </journal> ?, <year> 1988. </year>
Reference-contexts: As with Janakiram's scheme [8], the work done by each processor in this scheme can be executed in parallel using our work distribution method to give additional speedup. 23 Most systems for exploiting OR-parallelism in logic programs are essentially implemen-tations of parallel depth-first search <ref> [9, 17, 28, 5, 30, 2] </ref>. These systems also use dynamic work sharing to divide the work evenly among processors. A major problem in such systems is that the size of the stack grows very rapidly for many logic programs, which makes stack splitting rather expansive.
Reference: [31] <author> M. H. van Emden. </author> <title> An interpreting algorithm for prolog programs. </title> <editor> In J.A. Campbell, editor, </editor> <title> Implementations of Prolog. </title> <publisher> Ellis Horwood, </publisher> <address> West Sussex, UK, </address> <year> 1984. </year>
Reference-contexts: It is also used under the name of backtracking to solve various combinatorial problems [6] and constraint satisfaction problems [22]. Execution of a Prolog program can be viewed as depth-first search of a proof tree <ref> [31] </ref>. Iterative-Deepening DFS algorithms are used to solve discrete optimization problems [10, 11] and for theorem proving [29]. A major advantage of the depth-first search strategy is that it requires very little memory.
Reference: [32] <author> Benjamin W. Wah and Y. W. Eva Ma. </author> <title> Manip|a multicomputer architecture for solving combinatorial extremum-search problems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> c-33, </volume> <month> May </month> <year> 1984. </year>
Reference-contexts: A major advantage of the depth-first search strategy is that it requires very little memory. Since many of the problems solved by DFS are highly computation intensive, there has been a great interest in developing parallel versions of depth-first search <ref> [7, 32, 13, 4, 21, 8] </ref>. We have developed a parallel formulation of depth-first search which retains the storage efficiency of DFS. <p> We have been able to achieve linear speedup on Sequent Balance up to 30 processors ( the maximum configuration available) and on the Intel Hypercube and BBN Butterfly up to 128 processors ( the maximum configurations available). Contrary to the expectation of many researchers <ref> [32, 4, 21] </ref>, the performance on the ring architecture is not very good. At the heart of our parallel formulation is a dynamic work distribution scheme that divides the work between different processors. <p> Fig. 8 gives details of speedup achieved on various architectures. These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search <ref> [4, 32, 21, 3] </ref>. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> These speedup figures are somewhat better because there is no overhead of recoordinating processors after every iteration. 5 Related Research. Dynamic division of work has been used by many researchers for parallelizing depth-first search [4, 32, 21, 3]. Many of these researchers <ref> [4, 32, 21] </ref> have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma [32] present parallel depth-first search procedures on a ring network. <p> Dynamic division of work has been used by many researchers for parallelizing depth-first search [4, 32, 21, 3]. Many of these researchers [4, 32, 21] have implemented parallel DFS on the ring architecture and studied its performance for around 16-20 processors. Monien and Vornberger [21] and Wah and Ma <ref> [32] </ref> present parallel depth-first search procedures on a ring network. The work distribution schemes in these formulations is very similar to the scheme presented in this paper. <p> From our experiments as well as the analysis in [16] it is clear that this work distribution scheme is not able to provide good speedup on large rings. The initialization part in Monien's [21] and Wah's <ref> [32] </ref> scheme is slightly different than the one discussed in this paper. Before starting parallel search they divide the search space into N parts, and give each part to a processor. <p> The architecture of the multiprocessor and the work distribution algorithm have been found to have significant impact on the performance of the parallel depth-first search algorithm. Other researchers <ref> [32, 21] </ref> considered the ring architecture to be quite suitable for parallel depth-first search. Our experimental results show that hypercube and shared-memory architectures are significantly better.
References-found: 32

