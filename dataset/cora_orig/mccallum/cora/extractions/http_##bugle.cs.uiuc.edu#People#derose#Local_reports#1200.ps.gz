URL: http://bugle.cs.uiuc.edu/People/derose/Local_reports/1200.ps.gz
Refering-URL: http://bugle.cs.uiuc.edu/People/derose/other_publications.html
Root-URL: http://www.cs.uiuc.edu
Title: Experiments with an Ocean Circulation Model on CEDAR  
Author: L. DeRose, K. Gallivan, E. Gallopoulos 
Address: Urbana, Illinois 61801 U.S.A.  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign  
Abstract: We present the design of the GFDL ocean circulation model as adapted for simulations of the Mediterranean basin for the Cedar multicluster architecture. The model simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. The data and computational mapping strategies and their effect on the design are discussed. The code was parameterized to offer several choices for data partitionings of the computational domain, for placement strategies for the data in the memory hierarchy, and for the number of clusters and processors used in the computational hierarchy of Cedar. The experiments and performance trends are discussed. Using four clusters and 32 processors the code demonstrates significant speedup compared to a single cluster and compared to a single processor. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANDRICH, P., DELECLUSE, P., LEVY, C., AND MADEC, G. </author> <title> A multitasked general circulation model of the ocean. </title> <booktitle> In Proceedings Fourth International Symposium, Cray Research (1988), </booktitle> <pages> pp. 407-428. </pages>
Reference: [2] <author> ANDRICH, P., AND MADEC, G. </author> <title> Performance evaluation for an ocean general circulation model: vectorization and multitasking. </title> <booktitle> In 1988 International Conference on Supercomputing (St. </booktitle> <address> Malo, France, </address> <month> July </month> <year> 1988), </year> <booktitle> ACM, </booktitle> <pages> pp. 295-302. </pages>
Reference-contexts: Nevertheless, Krylov subspace type algorithms are likely to be more efficient for general OCM, as can be testified by the work in <ref> [2] </ref> as well as by the solvers used in the MOM code [22]. The fast convergence of the relaxation step caused the computation of the necessary matrix coefficients to consume an important amount of time relative to the solution step [12].
Reference: [3] <author> BELL, J., AND PATTERSON JR., G. </author> <title> Data organization in large numerical computations. </title> <booktitle> The Journal of Supercomputing 1 (1987), </booktitle> <pages> 105-136. </pages>
Reference-contexts: The explicit nature of the baroclinic phase allows the computation to be applied on sequences of two dimensional (; z) data slabs. This approach was favored in ocean modeling since it only required the storage of two dimensional data in fast memory <ref> [3] </ref>. Since the GFDL-IMGA model was based on the model of [8], computations in the baroclinic phase are performed over both ocean and land areas using masks to distinguish ocean from land cells. The program expects as input initial and boundary conditions, along with topography and wind information.
Reference: [4] <author> BRYAN, K. </author> <title> A numerical method for the study of the circulation of the world ocean. </title> <journal> Journal of Computational Physics 4 (1969), </journal> <pages> 347-376. </pages>
Reference: [5] <editor> Building an advanced climate model: </editor> <title> Program plan for the CHAMMP climate modeling program. </title> <type> Tech. Rep. </type> <institution> DOE/ER-0479T, U.S. Dept. of Energy, </institution> <address> Washington, D.C., </address> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: It is also noted that that the Department of Energy's CHAMMP project <ref> [5] </ref> and the recent release of the Modular Ocean Model from GFDL [22] are expected to contribute to the effort to develop parallel ocean models [19]. Performance studies of the GFDL-IMGA model on the Alliant FX/8 were conducted in [10].
Reference: [6] <author> CHEN, S. C., KUCK, D. J., AND SAMEH, A. H. </author> <title> Practical parallel band triangular system solvers. </title> <journal> ACM Trans. on Mathematical Software 4, </journal> <month> 3 (Sept., </month> <year> 1978), </year> <pages> 270-277. </pages>
Reference: [7] <author> CHERVIN, R. M., AND SEMTNER JR., A. J. </author> <title> An ocean modelling system for supercomputer architectures of the 1990s. </title> <booktitle> In Proceedings of the NATO Advanced Research Workshop on Climate-Ocean Interaction (1988), </booktitle> <editor> M. E. Schlesinger, Ed., </editor> <publisher> Kluwer Academic Publishers., </publisher> <pages> pp. 87-95. </pages>
Reference-contexts: See also <ref> [7] </ref> [25] [26]. Recent attention has focussed toward the exploitation of parallelism.
Reference: [8] <author> COX, M. D. </author> <title> A primitive equation, 3-dimensional model of the ocean. </title> <type> Tech. Rep. 1, </type> <institution> Geophysical Fluid Dynamics Laboratory/NOAA, Princeton University, </institution> <address> Prince-ton, NJ 08542, </address> <month> August </month> <year> 1984. </year>
Reference-contexts: The ocean general circulation model used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) <ref> [8] </ref>, as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediterranean basin geometry [23]. This model, from now on referred to as GFDL-IMGA, simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. <p> Section 5 discusses experiments and 1 results. Finally our conclusions and remarks on future plans are presented in Section 6. 2 Remarks on previous work Major reorganizations of the original model of Bryan and Cox were due to Cox <ref> [8] </ref>, directed toward vector machines with long start-up times, and Semtner [24], for register-to-register vector architectures such as the Cray-1. See also [7] [25] [26]. Recent attention has focussed toward the exploitation of parallelism. <p> Hence, data from three time steps are required during the computation of any grid point, because the leapfrog method uses data from the current and the previous time steps to predict values for the future time step. We refer to <ref> [8] </ref> for the main aspects of the implementation of the code on the Cyber 205 at the Geophysical Fluid Dynamics Laboratory, that was the basic model used in this work. <p> This approach was favored in ocean modeling since it only required the storage of two dimensional data in fast memory [3]. Since the GFDL-IMGA model was based on the model of <ref> [8] </ref>, computations in the baroclinic phase are performed over both ocean and land areas using masks to distinguish ocean from land cells. The program expects as input initial and boundary conditions, along with topography and wind information. It then proceeds as follows.
Reference: [9] <author> CRAY RESEARCH INC. </author> <title> Multitasking User Guide, </title> <month> Jan-uary </month> <year> 1985. </year>
Reference-contexts: Concurrent execution of loops within a single cluster, or across clusters are provided with DOALL and DOACROSS constructs. Three groups of synchronization routines are provided: DOACROSS loop synchronization; Zhu-Yew synchronization primitives (see [29]); and Cray-Style synchronization operations (see <ref> [9] </ref>). 4.2 Design of the parallel code The work described in this paper will focus in the baroclinic phase, the primary motivation being the fast convergence of the SOR algorithm, a peculiarity of the domain and data sets of interest here.
Reference: [10] <author> DE ROSE, L., GALLIVAN, K., AND GALLOPOULOS, E. </author> <title> Trace analysis of the GFDL ocean circulation model: A preliminary study. </title> <type> Tech. Rep. 863, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1989. </year>
Reference-contexts: Performance studies of the GFDL-IMGA model on the Alliant FX/8 were conducted in <ref> [10] </ref>.
Reference: [11] <author> DE ROSE, L., GALLIVAN, K., GALLOPOULOS, E., AND NAVARRA, A. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <booktitle> In Proc. Fifth SIAM Conf. Parallel Processing for Scientific Computing (May 1991), </booktitle> <address> D. C. </address> <note> Sorensen, Ed. To appear. Also CSRD TR-1124. </note>
Reference-contexts: Performance studies of the GFDL-IMGA model on the Alliant FX/8 were conducted in [10]. Preliminary results on the Cedar multiprocessor are described in <ref> [11] </ref>. 3 Description of the model and original code design The mathematical model uses the Navier-Stokes equations with three basic assumptions: Boussinesq approximation, in which density differences are neglected, except in the buoyancy term, hydrostatic assumption, where local acceleration and other terms of equal order are eliminated from the equation of
Reference: [12] <author> DE ROSE, L. A. </author> <title> Parallel ocean circulation modeling on Cedar. </title> <type> Master's thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> December </month> <year> 1991. </year>
Reference-contexts: The fast convergence of the relaxation step caused the computation of the necessary matrix coefficients to consume an important amount of time relative to the solution step <ref> [12] </ref>. Hence the time-independent part of the matrix construction was moved outside the time step loop. We next summarize the steps of the parallelization strategy. 1. <p> These experiments showed that the computa tional bottleneck was the baroclinic phase. 2. Data and control were partitioned to introduce paral-lelization. See <ref> [12] </ref> for further details. Corresponding to the flexibility of Cedar, the partitioning strategies of Section 4.2.1 were investigated. 3. Routine loop parallelization and vectorization within single tasks were handled by the KAP and VAST re structuring compilers. 4.
Reference: [13] <author> EMRATH, P., ANDERSON, M., BARTON, R., AND MC-GRATH, R. </author> <title> The Xylem Operating System. </title> <booktitle> In Proc. 1991 Int'l Conference on Parallel Processing (St. </booktitle> <address> Charles, IL, </address> <month> Aug. </month> <journal> 1991), </journal> <volume> vol. I, </volume> <pages> pp. 67-70. </pages>
Reference-contexts: These switching networks are 2-stage Omega networks built from 8 fi 8 cross-bar switches [18][21]. The Cedar operating system, Xylem, extends the Alliant Concentrix operating system to include multitasking and virtual memory management of the Cedar memory hierarchy (see <ref> [13] </ref>). A Xylem process consists of one or more independently scheduled program segments, called cluster-tasks, that execute asynchronously across the Cedar system, as directed by Xylem system calls.
Reference: [14] <institution> Grand challenges: High performance computing and communications. Office of Science and Technology Policy, </institution> <year> 1991. </year> <title> A report by the committee on Physical, </title> <journal> Mathematical, and Engineering Sciences. </journal>
Reference-contexts: This fact has been recognized with the inclusion of computational ocean circulation as a Grand Challenge of the High Performance Computing and Communications Initiative Program <ref> [14] </ref>. Two major computational difficulties for conducting ocean circulation simulations are the necessary fine spatial resolution orders of magnitude more detailed than what is used in computational simulations today to conduct meaningful forecasts, and the requirement to perform long term simulations for meaningful climatic studies.
Reference: [15] <author> GALLIVAN, K., JALBY, W., TURNER, S., VEIDENBAUM, A., AND WIJSHOFF, H. </author> . <title> Preliminary basic performance analysis of the Cedar multiprocessor memory systems. </title> <booktitle> In Proc. 1991 Int'l Conference on Parallel Processing (Aug. 1991), </booktitle> <volume> vol. I, </volume> <pages> pp. 71-75. </pages>
Reference-contexts: It is also instructive to compare the difference in performance between the GG and GC version with and without the use of prefetch. Basic memory system performance on Cedar <ref> [15] </ref> predicts potential improvement by using the prefetch unit to offset latency may bring the global memory access rate close to the cluster memory access rate. We can confirm this prediction by comparing the results from the GC and GG runs using model P 8 L 16 with 8 CEs. <p> The first is the obvious result that when no prefetching is used there is a significant difference between the versions. The fact that the difference is not as large as one might expect given the basic performance characteristics of the Cedar system, <ref> [15] </ref>, indicates that in both versions there is a nonnegligible amount of cluster and/or register activity. Secondly, consider the data when prefetch-ing is turned on.
Reference: [16] <author> GUZZI, M. D., PADUA, D. A., HOEFLINGER, J. P., AND LAWRIE, D. H. </author> <title> Cedar Fortran and other vector and parallel Fortran dialects. </title> <journal> Journal of Supercomputing (March 1990), </journal> <pages> 37-62. </pages>
Reference-contexts: A Xylem process consists of one or more independently scheduled program segments, called cluster-tasks, that execute asynchronously across the Cedar system, as directed by Xylem system calls. Programs are mostly written in Cedar Fortran <ref> [17, 16] </ref>, a language resembling Fortran 77, with vector constructs such as those proposed for the Fortran 90 standard and has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [17] <author> HOEFLINGER, J. </author> <title> Cedar Fortran programmer's handbook. </title> <type> Tech. rep., </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> October </month> <year> 1991. </year> <note> CSRD Report No. 1157. </note>
Reference-contexts: A Xylem process consists of one or more independently scheduled program segments, called cluster-tasks, that execute asynchronously across the Cedar system, as directed by Xylem system calls. Programs are mostly written in Cedar Fortran <ref> [17, 16] </ref>, a language resembling Fortran 77, with vector constructs such as those proposed for the Fortran 90 standard and has extensions for memory allocation, concurrency control, multitasking and synchronization.
Reference: [18] <author> KONICEK, J., TILTON, T., VEIDENBAUM, A., ZHU, C., DAVIDSON, E., DOWNING, R., HANEY, M., SHARMA, M., YEW, P., FARMWALD, P., KUCK, D., LAVERY, D., LINDSEY, R., POINTER, D., ANDREWS, J., BECK, T., MURPHY, T., TURNER, S., AND WARTER, N. </author> <title> The organization of the Cedar system. </title> <booktitle> In Proc. 1991 Int'l Conference on Parallel Processing (St. </booktitle> <address> Charles, IL, </address> <month> Aug. </month> <journal> 1991), </journal> <volume> vol. I, </volume> <pages> pp. 49-56. </pages>
Reference: [19] <author> MALONE, R. C., CHERVIN, R., SMITH, R., AND DAN-NEVIK, W. P. Minisymposium: </author> <title> Computing climate change: Can we beat nature? In Proc. </title> <publisher> Supercomputing'91. IEEE, </publisher> <address> Albuquerque, New Mexico, </address> <month> Nov. </month> <year> 1991, </year> <pages> pp. 677-679. </pages>
Reference-contexts: It is also noted that that the Department of Energy's CHAMMP project [5] and the recent release of the Modular Ocean Model from GFDL [22] are expected to contribute to the effort to develop parallel ocean models <ref> [19] </ref>. Performance studies of the GFDL-IMGA model on the Alliant FX/8 were conducted in [10]. <p> Several partitioning strategies of grid points of the computational domain to resources can be used. The models discussed in <ref> [19] </ref> demonstrate the importance of the partitioning strategy for the parallel implementation. For Cedar, we consider partitionings to be of two types, one primary, taking into account data partitioning across clusters, the other secondary, specifying the partitioning across vector processors in each cluster.
Reference: [20] <author> MALONY, A. D. </author> <title> High resolution process timing user's manual. </title> <type> Tech. rep., </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomputing Res. & Dev., </institution> <month> June </month> <year> 1987. </year> <note> CSRD Report No. 676. </note>
Reference-contexts: Although memory limitations forced us to use 32 bit precision, the results were in good agreement with those obtained from 64 bit precision. All times reported for Cedar correspond to wall clock time obtained from the high-resolution 6 library routines hrcget and hrcdelta (see <ref> [20] </ref>), in sin-gle user mode. Results on the Alliant FX/8 are CPU times collected in single user mode, using the Alliant library routine etime. All these routines have 10 sec accuracy.
Reference: [21] <author> MCGRATH, R., AND EMRATH, P. </author> <title> Using memory in the Cedar system. </title> <type> Tech. Rep. 655, </type> <institution> Center for Supercomputing Research and Development, University of Illinois at Urbana-Champaign, </institution> <address> Urbana IL 61801, </address> <year> 1987. </year>
Reference: [22] <author> PACANOWSKI, R. C., DIXON, K., AND ROSATI, A. GFDL MOM 1.0. </author> <title> Geophysical Fluid Dynamics Laboratory/NOAA, </title> <month> December </month> <year> 1990. </year>
Reference-contexts: It is also noted that that the Department of Energy's CHAMMP project [5] and the recent release of the Modular Ocean Model from GFDL <ref> [22] </ref> are expected to contribute to the effort to develop parallel ocean models [19]. Performance studies of the GFDL-IMGA model on the Alliant FX/8 were conducted in [10]. <p> Nevertheless, Krylov subspace type algorithms are likely to be more efficient for general OCM, as can be testified by the work in [2] as well as by the solvers used in the MOM code <ref> [22] </ref>. The fast convergence of the relaxation step caused the computation of the necessary matrix coefficients to consume an important amount of time relative to the solution step [12]. Hence the time-independent part of the matrix construction was moved outside the time step loop.
Reference: [23] <author> PINARDI, N., AND NAVARRA, A. </author> <title> A brief review of global Mediterranean wind-driven general circulation experiments. </title> <type> Tech. Rep. 132, </type> <institution> IMGA-CNR, Modena Italy, </institution> <year> 1988. </year>
Reference-contexts: The ocean general circulation model used in this work is based on a basic model of the Geophysical Fluid Dynamics Laboratory (GFDL) [8], as adapted in the Istituto per lo Studio delle Metodologie Geofisiche Ambientali (IMGA-CNR) to the Mediterranean basin geometry <ref> [23] </ref>. This model, from now on referred to as GFDL-IMGA, simulates the basic aspects of large-scale, baroclinic ocean circulation, including treatment of irregular bottom topography. It is used in climate studies and also to study the development of mid-ocean eddies.
Reference: [24] <author> SEMTNER JR., A. J. </author> <title> An oceanic general circulation model with botton topography. </title> <type> Tech. Rep. 9, </type> <institution> UCLA Dept. of Metheorology, </institution> <year> 1974. </year>
Reference-contexts: Finally our conclusions and remarks on future plans are presented in Section 6. 2 Remarks on previous work Major reorganizations of the original model of Bryan and Cox were due to Cox [8], directed toward vector machines with long start-up times, and Semtner <ref> [24] </ref>, for register-to-register vector architectures such as the Cray-1. See also [7] [25] [26]. Recent attention has focussed toward the exploitation of parallelism. <p> See also [7] [25] [26]. Recent attention has focussed toward the exploitation of parallelism. We note for example [1][2] for the Cray 2, with major emphasis on the use of efficient solvers for the two-dimensional mass transport stream function; the award-winning work of [27], on a modified version of <ref> [24] </ref> using microtasking and vector processing, to achieve impressive performance, on a 4-CPU Cray X-MP.
Reference: [25] <author> SEMTNER JR., A. J. </author> <title> Finite-diferrence formulation of a world ocean model. </title> <booktitle> In Proc. NATO Institute of Advanced Physical Oceanographic Numerical Modelling. </booktitle> <year> (1986), </year> <editor> J. J. O'Brien, Ed., D. </editor> <publisher> Reidel Publishing Co., </publisher> <pages> pp. 187-202. </pages>
Reference-contexts: See also [7] <ref> [25] </ref> [26]. Recent attention has focussed toward the exploitation of parallelism.
Reference: [26] <author> SEMTNER JR., A. J. </author> <title> History and methodology of mod-elling the circulation of the world ocean. </title> <booktitle> In Proc. NATO Institute of Advanced Physical Oceanographic Numerical Modelling. </booktitle> <year> (1986), </year> <editor> J. J. O'Brien, Ed., D. </editor> <publisher> Reidel Publishing Co., </publisher> <pages> pp. 23-32. </pages>
Reference-contexts: See also [7] [25] <ref> [26] </ref>. Recent attention has focussed toward the exploitation of parallelism.
Reference: [27] <author> SEMTNER JR., A. J., AND CHERVIN, R. M. </author> <title> A simulation of the global ocean circulation with resolved eddies. </title> <journal> Journal of Geophysical Research 93, </journal> <month> C12 </month> <year> (1988), </year> <pages> 15502-15522. </pages>
Reference-contexts: See also [7] [25] [26]. Recent attention has focussed toward the exploitation of parallelism. We note for example [1][2] for the Cray 2, with major emphasis on the use of efficient solvers for the two-dimensional mass transport stream function; the award-winning work of <ref> [27] </ref>, on a modified version of [24] using microtasking and vector processing, to achieve impressive performance, on a 4-CPU Cray X-MP.
Reference: [28] <author> SMITH, R. D., DUKOWICZ, J. K., AND MALONE, R. C. </author> <title> Massively parallel global ocean modeling. </title> <type> Tech. Rep. </type> <institution> LA-UR-91-2583, Los Alamos National Laboratory, </institution> <address> Los Alamos, New Mexico 87545, </address> <year> 1991. </year>
Reference-contexts: The replacement of the relaxation step for the stream function with a parallelized preconditioned conjugate gradient solution of a pressure equation as described in <ref> [28] </ref> led to an important improvement of the performance on the CM-2 Connection Machine.
Reference: [29] <author> ZHU, C., AND YEW, P. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering SE-13(6) (June 1987), </journal> <pages> 726-739. </pages>
Reference-contexts: Concurrent execution of loops within a single cluster, or across clusters are provided with DOALL and DOACROSS constructs. Three groups of synchronization routines are provided: DOACROSS loop synchronization; Zhu-Yew synchronization primitives (see <ref> [29] </ref>); and Cray-Style synchronization operations (see [9]). 4.2 Design of the parallel code The work described in this paper will focus in the baroclinic phase, the primary motivation being the fast convergence of the SOR algorithm, a peculiarity of the domain and data sets of interest here.
References-found: 29

