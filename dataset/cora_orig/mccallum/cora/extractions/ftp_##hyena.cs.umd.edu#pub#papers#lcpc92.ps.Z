URL: ftp://hyena.cs.umd.edu/pub/papers/lcpc92.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/compilers/compilers-pub-abs.html
Root-URL: 
Email: reinhard@rice.edu ken@rice.edu chk@rice.edu raja@icase.edu jhs@icase.edu  
Title: Compiler Analysis for Irregular Problems in Fortran D  
Author: Reinhard von Hanxleden Ken Kennedy Charles Koelbel Raja Das Joel Saltz 
Abstract: We developed a dataflow framework which provides a basis for rigorously defining strategies to make use of runtime preprocessing methods for distributed memory multiprocessors. In many programs, several loops access the same off-processor memory locations. Our runtime support gives us a mechanism for tracking and reusing copies of off-processor data. A key aspect of our compiler analysis strategy is to determine when it is safe to reuse copies of off-processor data. Another crucial function of the compiler analysis is to identify situations which allow runtime preprocessing overheads to be amortized. This dataflow analysis will make it possible to effectively use the results of interprocedural analysis in our efforts to reduce interprocessor communication and the need for runtime preprocessing. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> T. W. Clark, R. v. Hanxleden, K. Kennedy, C. Koel-bel, and L. R. Scott. </author> <title> Evaluating parallel languages for molecular dynamics computations. </title> <booktitle> In Scalable High Performance Computing Conference, </booktitle> <address> Williamsburg, VA, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The types of irregular loops we are trying to handle are typically found in unstructured mesh explicit and multigrid solvers, molecular dynamics codes, and some sparse iterative linear systems solvers <ref> [1] </ref>. In distributed memory machines, large data arrays need to be partitioned between local memories of processors.
Reference: [2] <author> R. Das, D. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives, </title> <booktitle> AIAA-92-0562. In Proceedings of the 30th Aerospace Sciences Meeting. AIAA, </booktitle> <month> January </month> <year> 1992. </year>
Reference-contexts: Significant work has gone into optimizing the gather, scatter and accumulation communication routines for the iPSC/860. It is not the purpose of this paper to describe the design and implementation of PARTI in great detail; information on this can be found elsewhere <ref> [2] </ref>. Note, however, that the current functionality of the primitives implies that all processors participate in them; this requirement for global coordination can be relaxed when extending the primitives to handle name spaces which are shared only between subsets of processors. <p> The calculation consists of a sequence of loops over edges, boundary faces and nodes of an unstructured mesh. The code was originally developed by Dimitri Mavriplis. The program was ported to the Touchstone Delta using Parti primitives <ref> [2, 3] </ref> and the code was run to simulate a variety of aircraft configurations under a range of test conditions. While the port was carried out by hand, the strategy used to place the PARTI primitives was the same as the strategies that would result from our dataflow framework.
Reference: [3] <author> R. Das, R. Ponnusamy, J. Saltz, and D. Mavriplis. </author> <title> Distributed memory compiler methods for irregular problems | data copy reuse and runtime partitioning. </title> <type> ICASE Report 91-73, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, VA, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: Our runtime support makes it possible to track and reuse off-processor data copies <ref> [3] </ref>. We generate combining communication schedules which combine off-processor data for several indirect references, possibly contained in different loops, and we generate incremental schedules to obtain only those off-processor data which are not already requested by a given set of pre-existing schedules. <p> The calculation consists of a sequence of loops over edges, boundary faces and nodes of an unstructured mesh. The code was originally developed by Dimitri Mavriplis. The program was ported to the Touchstone Delta using Parti primitives <ref> [2, 3] </ref> and the code was run to simulate a variety of aircraft configurations under a range of test conditions. While the port was carried out by hand, the strategy used to place the PARTI primitives was the same as the strategies that would result from our dataflow framework.
Reference: [4] <author> T. Gross and P. Steenkiste. </author> <title> Structured dataflow analysis for arrays and its use in an optimizing compiler. </title> <journal> Software|Practice and Experience, </journal> <volume> 20(2) </volume> <pages> 133-155, </pages> <month> February </month> <year> 1990. </year>
Reference-contexts: For many programs, this can actually be achieved by forward substituting array indices. For example, the code sequence j=ia (i); x (j)=10 would be treated as x (ia (i))=10. Arrays which are never referenced indirectly are assumed to be analyzed using other methods <ref> [4] </ref> prior to this analysis. References with multiple (but bounded) levels of indirection will require more levels of complexity in the dataflow framework; we do not consider potentially unbounded indirection, as is found in linked lists. Let V be the set of arrays which are accessed indirectly. <p> In the following, loop refers to elements of N , i.e., it may denote a pad as well. Future work will present a complete framework in which summary information is built in a bottom-up fashion similar to array kill information <ref> [4] </ref>. Finally, this paper only discusses the case where the summarized loops have no data dependences, except for commutative and associative reductions which are handled specially. 2.2 Array portions Array portions are a central concept to the framework and best introduced by an example.
Reference: [5] <author> S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, and C. Tseng. </author> <title> An overview of the Fortran D programming system. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: The dataflow framework we present here pertains to collections of loops with no loop-carried data dependences except accumulation type dependencies. Such loops are often referred to as data-parallel loops, and are the primary target of the Fortran D compiler <ref> [5] </ref>. The types of irregular loops we are trying to handle are typically found in unstructured mesh explicit and multigrid solvers, molecular dynamics codes, and some sparse iterative linear systems solvers [1]. In distributed memory machines, large data arrays need to be partitioned between local memories of processors.
Reference: [6] <author> S. Horwitz, T. Reps, and D. Binkley. </author> <title> Interprocedu-ral slicing using dependence graphs. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 12(1) </volume> <pages> 26-60, </pages> <month> January </month> <year> 1990. </year> <month> 11 </month>
Reference-contexts: A more general approach could be based on basic blocks combined with slicing <ref> [6] </ref>, which in fact is already part of our current implementation design. Another relatively straightforward extension is to break each communication call up into their matching send/receive pairs and then place these components such as to overlap communication and computation as much as possible.
Reference: [7] <author> J. Kam and J. Ullman. </author> <title> Global data flow analy-sis and iterative algorithms. </title> <journal> Journal of the ACM, </journal> <volume> 23(1) </volume> <pages> 159-171, </pages> <month> January </month> <year> 1976. </year>
Reference-contexts: The framework can be implemented using bit vectors, each bit representing one array portion. The length of these bit vectors is bounded by the number of indirect array references (i.e., it is linear in program size), and all equations given here are rapid <ref> [7] </ref>. Therefore, using bit vectors for the analysis gives us good asymptotic running times. However, for our examples (and probably also in a practical implementation), it seems advantageous to represent the different flow variables as bit matrices.
Reference: [8] <author> J. Knoop, O. Ruthing, and B. Steffen. </author> <title> Lazy code motion. </title> <booktitle> In Proceedings of the ACM SIGPLAN '92 Conference on Program Language Design and Implementation, </booktitle> <address> San Francisco, CA, </address> <month> June </month> <year> 1992. </year>
Reference-contexts: Another strategy would be to limit communication hoisting to cases where we actually reduce the size or number of messages, which can also be achieved in a straightforward manner similar to lazy code motion <ref> [8] </ref>; this might decrease the live ranges of our communication buffer with a possible savings in overall buffer storage requirements, but at the expense of reduced opportunities for hiding communication delays.
Reference: [9] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory machines. </title> <booktitle> In Proceedings of the Second ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Seattle, WA, </address> <month> March </month> <year> 1990. </year>
Reference-contexts: In this case, it is not possible to predict at compile time which data must be prefetched. We treat this lack of information by transforming the original parallel loop into two constructs called inspector and executor <ref> [9, 10] </ref>. During program execution, the inspector examines the data references made by a processor, and calculates which off-processor data need to be fetched and where these data will be stored once they are received. The executor loop then uses the information from the inspector to perform the actual computation.
Reference: [10] <author> R. Mirchandaney, J. Saltz, R. Smith, D. Nicol, and K. Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the Second International Conference on Supercomputing, </booktitle> <address> St. Malo, France, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: In this case, it is not possible to predict at compile time which data must be prefetched. We treat this lack of information by transforming the original parallel loop into two constructs called inspector and executor <ref> [9, 10] </ref>. During program execution, the inspector examines the data references made by a processor, and calculates which off-processor data need to be fetched and where these data will be stored once they are received. The executor loop then uses the information from the inspector to perform the actual computation.
Reference: [11] <author> A. Pothen, H. Simon, and K. Liou. </author> <title> Partitioning sparse matrices with eigenvectors of graphs. </title> <journal> SIAM J. Mat. Anal. Appl., </journal> <volume> 11 </volume> <pages> 430-452, </pages> <year> 1990. </year>
Reference-contexts: For this case, the freestream Mach number is 0.768 and the incidence is 1.16 degrees. We employed the recursive spectral partitioning algorithm to carry out partitioning <ref> [11, 13] </ref>.
Reference: [12] <author> J. Saltz, K. Crowley, R. Mirchandaney, and H. Berryman. </author> <title> Run-time scheduling and execution of loops on message passing machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8(2) </volume> <pages> 303-312, </pages> <year> 1990. </year>
Reference-contexts: These procedures 1. Coordinate interprocessor data movement, 2. Manage the storage of and access to copies of off processor data, and 3. Support a shared name space by building a distributed translation table <ref> [12] </ref> to store the local address and processor number for each distributed ar ray element. 1 This functionality can be used directly to generate in-spector/executor pairs. Each inspector produces a communications schedule, which essentially is a pattern of communication for gathering or scattering data.
Reference: [13] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Perg-amon Press, </publisher> <year> 1991. </year> <month> 12 </month>
Reference-contexts: For this case, the freestream Mach number is 0.768 and the incidence is 1.16 degrees. We employed the recursive spectral partitioning algorithm to carry out partitioning <ref> [11, 13] </ref>.
References-found: 13

