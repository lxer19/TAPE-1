URL: ftp://ftp.cs.umd.edu/pub/papers/papers/3724/3724.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: A Semi-Discrete Matrix Decomposition for Latent Semantic Indexing in Information Retrieval  
Author: Tamara G. Kolda and Dianne P. O'Leary 
Date: December 5, 1996  
Abstract: The vast amount of textual information available today is useless unless it can be effectively and efficiently searched. In information retrieval, we wish to match queries with relevant documents. Documents can be represented by the terms that appear within them, but literal matching of terms does not necessarily retrieve all relevant documents. Latent Semantic Indexing represents documents by approximations and tends to cluster documents on similar topics even if their term profiles are somewhat different. This approximate representation is usually accomplished using a low-rank singular value decomposition (SVD) approximation. In this paper, we use an alternate decomposition, the semi-discrete decomposition (SDD). In our tests, for equal query times, the SDD does as well as the SVD and uses less than one-tenth the storage. Additionally, we show how to update the SDD for a dynamically changing document collection.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael Berry, Theresa Do, Gavin O'Brien, Vijay Krishna, and Sowmini Varadhan. </author> <title> SVDPACKC (Version 1.0) Users' Guide. </title> <type> Technical Report CS-93-194, </type> <institution> Computer Science Department, University of Tennessee, </institution> <address> Knoxville, TN 37996-1301, </address> <year> 1993. </year>
Reference-contexts: All tests were run on a Sparc 20. Our code is in C, with the SVD taken from SVDPACKC <ref> [1] </ref>. 5.1 Methods of Comparison We will compare the SDD- and SVD-based LSI methods using three standard test sets. Each test set comes with a collection of documents, a collection of 9 queries, and relevance judgments for each query.
Reference: [2] <author> Michael W. Berry, Susan T. Dumais, and Gavin W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: The SDD is discussed in Section 4, and computational comparisons with the SVD are given in Section 5. In many information retrieval settings, the document database is constantly being updated. Much work has been done on updating the SVD approximation to the term-document matrix <ref> [2, 9] </ref>, but it can be as expensive as computing the original SVD. <p> The SVD has been used quite effectively for information retrieval, as documented in numerous reports. We recommend the original LSI paper [4], a paper reporting the effectiveness of the LSI approach on the TREC-3 dataset [5], and a more mathematical paper <ref> [2] </ref> for further information on the SVD for LSI. 4 LSI via a Semi-Discrete Decomposition 4.1 Approximating the Term-Document Matrix The SVD produces the best rank-k approximation to a matrix, but generally, even a small SVD approximation requires more storage than the original matrix if the original matrix is sparse.
Reference: [3] <author> M.W. Berry and R.D. Fierro. </author> <title> Low-rank orthogonal decompositions for information retrieval applications. Numerical Linear Algebra with Applications, </title> <booktitle> 1 </booktitle> <pages> 1-27, </pages> <year> 1996. </year>
Reference-contexts: LSI has performed well in both large and small tests; see, for example, Dumais [5, 6]. LSI is described in Section 3. Thus far, only the singular value decomposition and its relatives, the ULV and URV decompositions <ref> [3] </ref>, have been used in LSI. We propose using a very different decomposition, originally developed for image compression by O'Leary and Peleg [10].
Reference: [4] <author> Scott Deerwester, Susan T. Dumais, George W. Furnas, Thomas K. Lan-dauer, and Richard Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41 </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: We will experiment with various choices for ff and re-normalization in Section 5.2. The SVD has been used quite effectively for information retrieval, as documented in numerous reports. We recommend the original LSI paper <ref> [4] </ref>, a paper reporting the effectiveness of the LSI approach on the TREC-3 dataset [5], and a more mathematical paper [2] for further information on the SVD for LSI. 4 LSI via a Semi-Discrete Decomposition 4.1 Approximating the Term-Document Matrix The SVD produces the best rank-k approximation to a matrix, but
Reference: [5] <author> Susan Dumais. </author> <title> Improving the retrieval of infomation from external sources. Behavior Research Methods, Instruments, </title> & <journal> Computers, </journal> <volume> 23 </volume> <pages> 229-236, </pages> <year> 1991. </year> <month> 20 </month>
Reference-contexts: LSI has performed well in both large and small tests; see, for example, Dumais <ref> [5, 6] </ref>. LSI is described in Section 3. Thus far, only the singular value decomposition and its relatives, the ULV and URV decompositions [3], have been used in LSI. We propose using a very different decomposition, originally developed for image compression by O'Leary and Peleg [10]. <p> We will experiment with various choices for ff and re-normalization in Section 5.2. The SVD has been used quite effectively for information retrieval, as documented in numerous reports. We recommend the original LSI paper [4], a paper reporting the effectiveness of the LSI approach on the TREC-3 dataset <ref> [5] </ref>, and a more mathematical paper [2] for further information on the SVD for LSI. 4 LSI via a Semi-Discrete Decomposition 4.1 Approximating the Term-Document Matrix The SVD produces the best rank-k approximation to a matrix, but generally, even a small SVD approximation requires more storage than the original matrix if
Reference: [6] <author> Susan Dumais. </author> <title> Latent sematic indexing (LSI):TREC-3 report. </title> <editor> In D.K. Harman, editor, </editor> <booktitle> Proceedings of the Third Text REtrieval Conference (TREC-3), NIST Special Publication 500-225, </booktitle> <pages> pages 219-230, </pages> <month> April </month> <year> 1995. </year> <note> (http://potomac.ncsl.nist.gov/TREC). </note>
Reference-contexts: LSI has performed well in both large and small tests; see, for example, Dumais <ref> [5, 6] </ref>. LSI is described in Section 3. Thus far, only the singular value decomposition and its relatives, the ULV and URV decompositions [3], have been used in LSI. We propose using a very different decomposition, originally developed for image compression by O'Leary and Peleg [10].
Reference: [7] <author> William B. Frakes and Ricardo Baeza-Yates. </author> <title> Information Retrieval: Data Structures and Algorithms. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1992. </year>
Reference-contexts: We determine a list of key words or terms by 1. creating a list of all words that appear in the documents, 2. removing words void of semantic content such as "of" and "because" (us ing the stop word list of Frakes and Baeza-Yates <ref> [7] </ref>), and 3. further trimming the list by removing words that appear in only one doc ument. The remaining words are the terms, which we number from 1 to m. <p> (log (f kj + 1)) ; i.e., log local weights, no global weights, and column normalization. 3 Symbol Formula for t ij Brief Description Ref. b (f ij ) Binary [11] t f ij Term Frequency [11] c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency <ref> [7, 11] </ref> Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d <p> (f ij ) + :5 f ij Augmented Normalized Term Frequency <ref> [7, 11] </ref> Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] Table 3: Normalization Formulas 4 2.2 Query Creation and Processing A query is represented as an m-vector q =
Reference: [8] <author> Gene H. Golub and Charles F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins Press, </publisher> <address> 2nd edition, </address> <year> 1989. </year>
Reference-contexts: It can be shown that A k is the best rank-k approximation to A in the Frobenius norm and in the Euclidean norm <ref> [8] </ref>. 3.2 Query Processing We can process queries using our approximation for A: s = q T A q T A k k k )( 1ff k ) The scalar ff controls the splitting of the k matrix and has no effect unless we re-normalize the columns of ~ A.
Reference: [9] <author> Gavin O'Brien. </author> <title> Information management tools for updating an SVD-encoded indexing scheme. </title> <type> Master's thesis, </type> <institution> University of Tennessee, </institution> <address> Knoxville, TN 37996-1301, </address> <year> 1994. </year>
Reference-contexts: The SDD is discussed in Section 4, and computational comparisons with the SVD are given in Section 5. In many information retrieval settings, the document database is constantly being updated. Much work has been done on updating the SVD approximation to the term-document matrix <ref> [2, 9] </ref>, but it can be as expensive as computing the original SVD. <p> Thus, the list of terms might also change. In this section, we will focus on the problem of modifying a SDD decomposition when the document collection changes. SVD-updating has been studied by O'Brien <ref> [9] </ref>. He reports that updating the SVD takes almost as much time as re-computing it, but that it requires less memory.
Reference: [10] <author> Dianne P. O'Leary and Shmuel Peleg. </author> <title> Digital image compression by outer product expansion. </title> <journal> IEEE Transactions on Communications, </journal> <volume> 31 </volume> <pages> 441-444, </pages> <year> 1983. </year>
Reference-contexts: LSI is described in Section 3. Thus far, only the singular value decomposition and its relatives, the ULV and URV decompositions [3], have been used in LSI. We propose using a very different decomposition, originally developed for image compression by O'Leary and Peleg <ref> [10] </ref>. In this decomposition, which we call the semi-discrete decomposition (SDD), the matrix is approximated by summing outer products just as in the SVD, but the m-vectors and n-vectors only have entries in the set f1; 0; 1g. <p> 1) k double precision numbers X km numbers from f1; 0; 1g SDD Y kn numbers from f1; 0; 1g 4k + 1 4 k (m + n) D k single precision numbers The SDD approximation is constructed via a greedy algorithm, converging monotonically to A: Algorithm. (O'Leary and Peleg <ref> [10] </ref>) Let k max be the rank of the desired approximation. This value can be determined ahead of time, or we can iterate until we obtain a desired accuracy. Set k = 1. Set the residual matrix A (c) = A. Choose tolerance; we use 0.01.
Reference: [11] <author> Gerald Salton and Chris Buckley. </author> <title> Term weighting approaches in automatic text retrieval. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 24 </volume> <pages> 513-523, </pages> <year> 1988. </year> <month> 21 </month>
Reference-contexts: represent the local, global, and normalization components respectively; for example, using weight lxn specifies that a ij = q k=1 (log (f kj + 1)) ; i.e., log local weights, no global weights, and column normalization. 3 Symbol Formula for t ij Brief Description Ref. b (f ij ) Binary <ref> [11] </ref> t f ij Term Frequency [11] c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency [7, 11] Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre <p> components respectively; for example, using weight lxn specifies that a ij = q k=1 (log (f kj + 1)) ; i.e., log local weights, no global weights, and column normalization. 3 Symbol Formula for t ij Brief Description Ref. b (f ij ) Binary <ref> [11] </ref> t f ij Term Frequency [11] c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency [7, 11] Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! <p> (log (f kj + 1)) ; i.e., log local weights, no global weights, and column normalization. 3 Symbol Formula for t ij Brief Description Ref. b (f ij ) Binary [11] t f ij Term Frequency [11] c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency <ref> [7, 11] </ref> Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d <p> t ij Brief Description Ref. b (f ij ) Binary <ref> [11] </ref> t f ij Term Frequency [11] c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency [7, 11] Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] <p> ij Term Frequency <ref> [11] </ref> c :5 (f ij ) + :5 f ij Augmented Normalized Term Frequency [7, 11] Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] Table 3: Normalization Formulas 4 2.2 Query Creation and Processing A query is <p> (f ij ) + :5 f ij Augmented Normalized Term Frequency <ref> [7, 11] </ref> Table 1: Local Term Weight Formulas Symbol Formula for g i Brief Description Ref. x 1 No change [11] f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] Table 3: Normalization Formulas 4 2.2 Query Creation and Processing A query is represented as an m-vector q = <p> Formula for g i Brief Description Ref. x 1 No change <ref> [11] </ref> f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] Table 3: Normalization Formulas 4 2.2 Query Creation and Processing A query is represented as an m-vector q = [q i ]; where q i represents the weight of term i in the query. <p> <ref> [11] </ref> f log n j (f ij ) Inverse Document Fre quency (IDF) [11] P P ! Probabilistic Inverse [7, 11] Table 2: Global Term Weight Formulas Symbol Formula for d j Brief Description Ref. x 1 No Change [11] n i=1 (g i t ij ) 2 1=2 Normal [11] Table 3: Normalization Formulas 4 2.2 Query Creation and Processing A query is represented as an m-vector q = [q i ]; where q i represents the weight of term i in the query.
References-found: 11

