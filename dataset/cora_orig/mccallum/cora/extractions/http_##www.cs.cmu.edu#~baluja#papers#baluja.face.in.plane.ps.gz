URL: http://www.cs.cmu.edu/~baluja/papers/baluja.face.in.plane.ps.gz
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/baluja/www/techreps.html
Root-URL: 
Email: baluja@jprc.com  
Phone: 2  
Title: Face Detection with In-Plane Rotation:  
Affiliation: School of Computer Science Carnegie Mellon University  
Address: 4616 Henry Street Pittsburgh, PA 15213  Pittsburgh, PA 15213  
Note: Shumeet Baluja 1;2  
Abstract: Early Concepts and Preliminary Results Abstract. This paper presents a method for extending upright, frontal, template-based face detection systems to efficiently handle all in-plane rotations. Detecting rotated faces is a two step procedure. First a "De-Rotation" network is used to process each input window. If there is a face in the window, this network determines its angle of rotation. Based upon this estimated angle of rotation, the window is then rotated to an upright position. Second, a "Detection" network, or multiple detection networks, are used to determine whether the rotated window contained an upright face. The training methods for both networks are presented. Preliminary empirical results are also provided. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> David Beymer, Amnon Shashua, and Tomaso Poggio. </author> <title> Example based image analysis and synthesis. </title> <type> Technical Report A.I. Memo 1431, </type> <institution> MIT, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: Finally, it would be interesting to extend this system to out-of-plane rotations. By using knowledge of the shape and/or symmetry of the face, it should be possible to convert semi-profile views of a face to frontal views. Related work has been explored in <ref> [1, 14] </ref>. Acknowledgements I would like to thank Henry Rowley and Takeo Kanade for collaborating on the original face detection system. I would also like to thank Henry Rowley for pointing out the existence of several face detectors of which I was not aware.
Reference: 2. <author> Gilles Burel and Dominique Carel. </author> <title> Detection and localization of faces on digital images. </title> <journal> Pattern Recognition Letters, </journal> <volume> 15 </volume> <pages> 963-967, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: 1 Introduction This paper presents a general method to extend many template-based frontal, upright, face detection systems to handle in-plane rotations of the face. There have been many template-based face detection systems developed, for example, see <ref> [2, 3, 6-8, 11-13, 15] </ref>. Other systems, such as [5], can also achieve rotation invariance by extracting smaller features of the face and using graph-matching algorithms. In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in [11].
Reference: 3. <author> Antonio J. Colmenarez and Thomas S. Huang. </author> <title> Face detection with information--based maximum discrimination. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 782-787, </pages> <year> 1997. </year>
Reference-contexts: 1 Introduction This paper presents a general method to extend many template-based frontal, upright, face detection systems to handle in-plane rotations of the face. There have been many template-based face detection systems developed, for example, see <ref> [2, 3, 6-8, 11-13, 15] </ref>. Other systems, such as [5], can also achieve rotation invariance by extracting smaller features of the face and using graph-matching algorithms. In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in [11].
Reference: 4. <author> John Hertz, Anders Krogh, and Richard G. Palmer. </author> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Mas-sachusetts, </address> <year> 1991. </year>
Reference-contexts: Apply the preprocessing steps to each of these images. 2. Train a neural network to produce an output of 1 for the face examples, and -1 for the non-face examples. The training algorithm is standard error backpropagation with momentum <ref> [4] </ref>. On the first iteration of this loop, the network's weights are initialized randomly. After the first iteration, we use the weights computed by training in the previous iteration as the starting point. 3. Run the system on an image of scenery which contains no faces. <p> Each set of 25 units covers the entire input without overlap. Fig. 3. Network architecture for the "De-Rotation" network. The network has 100 hidden units, each of which is connected to a 4x4 patch of the input. The network is trained by the standard error backpropagation training method, see <ref> [4] </ref> for details. 3.1 Performance of the De-Rotation Network As described in the previous section, the face detector is trained to handle small variations in rotation of approximately 10 ffi - 10 ffi .
Reference: 5. <author> T. K. Leung, M. C. Burl, and P. Perona. </author> <title> Finding faces in cluttered scenes using random labeled graph matching. </title> <booktitle> In Fifth International Conference on Computer Vision, </booktitle> <pages> pages 637-644, </pages> <address> Cambridge, Massachusetts, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: 1 Introduction This paper presents a general method to extend many template-based frontal, upright, face detection systems to handle in-plane rotations of the face. There have been many template-based face detection systems developed, for example, see [2, 3, 6-8, 11-13, 15]. Other systems, such as <ref> [5] </ref>, can also achieve rotation invariance by extracting smaller features of the face and using graph-matching algorithms. In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in [11].
Reference: 6. <author> Baback Moghaddam and Alex Pentland. </author> <title> Probabilistic visual learning for object detection. </title> <booktitle> In Fifth International Conference on Computer Vision, </booktitle> <pages> pages 786-793, </pages> <address> Cambridge, Massachusetts, June 1995. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference: 7. <author> Edgar Osuna, Robert Freund, and Federico Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 130-136, </pages> <year> 1997. </year>
Reference: 8. <author> Alex Pentland, Baback Moghaddam, and Thad Starner. </author> <title> View-based and modular eigenspaces for face recognition. </title> <booktitle> In Computer Vision and Pattern Recognition, </booktitle> <pages> pages 84-91, </pages> <year> 1994. </year>
Reference: 9. <author> Dean Pomerleau. </author> <title> Neural Network Perception for Mobile Robot Guidance. </title> <type> PhD thesis, </type> <institution> Carnegie Mellon University, </institution> <note> Feburary 1992. Available as CS Technical Report CMU-CS-92-115. </note>
Reference-contexts: There are many possibilities for representing the output of the network. Pomerleau has explored many of these for the design of a neural-network controller for the autonomous navigation of a vehicle <ref> [9] </ref>. In Pomerleau's application, the output of the network was interpreted to be the steering angle of the vehicle to ensure that the vehicle stayed on the road. Several output representations were explored: 1. <p> This representation avoids the imposed discontinuities of the strict 1-of-N encoding for images which are similar, but which have slight differences in rotations. Pomerleau found this representation to work the best of the three <ref> [9] </ref>. For this study, 72 output units were used with the Gaussian output encoding. <p> At runtime, after the forward-propagation pass is done through the network, a Gaussian is fit to the outputs, and the estimated peak of the Gaussian is used to determine the network's estimate of the rotation of the face (similar to the method proposed by <ref> [9] </ref>). This allows a finer granularity than would be possible if only the maximum activation of the output units was used (ie., it is possible to get values other than multiples of 5 ffi with 72 output units); see [9] for details. <p> the rotation of the face (similar to the method proposed by <ref> [9] </ref>). This allows a finer granularity than would be possible if only the maximum activation of the output units was used (ie., it is possible to get values other than multiples of 5 ffi with 72 output units); see [9] for details. Sample input and output pairs are shown in Figure 2. The "De-Rotation" network has a single hidden layer consisting of a total of 100 units. In this hidden layer, there are 4 sets of 25 units.
Reference: 10. <author> Henry A. Rowley, Shumeet Baluja, and Takeo Kanade. </author> <title> Human face detection in visual scenes. </title> <type> Technical Report CMU-CS-95-158R, </type> <institution> Carnegie Mellon University, </institution> <month> November </month> <year> 1995. </year> <note> Also available at http://www.cs.cmu.edu/~har/faces.html. </note>
Reference-contexts: Before giving the details of this system, the next section gives a review of the face detection system described in <ref> [10, 11] </ref>. 2 Frontal, Upright, Face Detection using the Rowley, Baluja & Kanade System [11] This section was largely taken from [11]; it is provided to give the reader the necessary background for the next section. The "Detection" networks used in this study are taken from [11]. <p> Then histogram equalization is performed, which non-linearly maps the intensity values to expand the range of intensities in the window. The histogram is computed for pixels inside an oval region in the window. Fig. 1. Overview of algorithm for frontal, upright, face detection. Taken from <ref> [10] </ref>. To train the neural network used for detection, a large number of face and non-face images are needed. Approximately 1050 face examples were used. <p> As pointed out in <ref> [10] </ref>, training the detector to recognize non-face images is a difficult problem because the space of non-face images is much larger than the space of face images. Collecting a representative set of non-faces is difficult.
Reference: 11. <author> Henry A. Rowley, Shumeet Baluja, and Takeo Kanade. </author> <title> Neural network-based face detection. </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <year> 1998. </year>
Reference-contexts: Other systems, such as [5], can also achieve rotation invariance by extracting smaller features of the face and using graph-matching algorithms. In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in <ref> [11] </ref>. The simplest method for creating a system which is invariant to rotations within the image-plane is to employ an existing frontal, upright, face detection system. Systems such as [11] use a neural-network based filter that receives as input a small, constant-sized window of the image, and generates an output signifying <p> In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in <ref> [11] </ref>. The simplest method for creating a system which is invariant to rotations within the image-plane is to employ an existing frontal, upright, face detection system. Systems such as [11] use a neural-network based filter that receives as input a small, constant-sized window of the image, and generates an output signifying the presence or absence of a face. To detect faces anywhere in the input, the filter is applied at every location in the image. <p> To extend this framework to capture faces which are rotated, the entire image can be repeatedly rotated by small increments and the detection system can be applied to each rotated image. However, this is an extremely computationally expensive procedure. For example, the system reported in <ref> [11] </ref> was invariant to approximately 10 ffi of rotation from upright (both clockwise and counterclockwise). Therefore, the entire detection procedure would need to be applied at least 18 times to each image, with the image rotated in increments of 20 ffi . <p> Before giving the details of this system, the next section gives a review of the face detection system described in <ref> [10, 11] </ref>. 2 Frontal, Upright, Face Detection using the Rowley, Baluja & Kanade System [11] This section was largely taken from [11]; it is provided to give the reader the necessary background for the next section. The "Detection" networks used in this study are taken from [11]. <p> Before giving the details of this system, the next section gives a review of the face detection system described in [10, 11]. 2 Frontal, Upright, Face Detection using the Rowley, Baluja & Kanade System <ref> [11] </ref> This section was largely taken from [11]; it is provided to give the reader the necessary background for the next section. The "Detection" networks used in this study are taken from [11]. <p> Before giving the details of this system, the next section gives a review of the face detection system described in [10, 11]. 2 Frontal, Upright, Face Detection using the Rowley, Baluja & Kanade System <ref> [11] </ref> This section was largely taken from [11]; it is provided to give the reader the necessary background for the next section. The "Detection" networks used in this study are taken from [11]. <p> detection system described in [10, 11]. 2 Frontal, Upright, Face Detection using the Rowley, Baluja & Kanade System <ref> [11] </ref> This section was largely taken from [11]; it is provided to give the reader the necessary background for the next section. The "Detection" networks used in this study are taken from [11]. The detection network operates as a filter that receives as input a 20x20 pixel region of the image, and generates an output ranging from 1 to -1, signifying the presence or absence of a face, respectively. <p> The preprocessing first attempts to equalize the intensity values in across the window. We fit a function which varies linearly across the window to the intensity values in an oval region inside the window, see <ref> [11] </ref>. The linear function will approximate the overall brightness of each part of the window, and can be subtracted from the window to compensate for a variety of lighting conditions. Then histogram equalization is performed, which non-linearly maps the intensity values to expand the range of intensities in the window. <p> Collect sub-images in which the network incorrectly identifies a face (an output activation &gt; 0). 4. Select up to 250 of these sub-images at random, apply the preprocessing steps, and add them into the training set as negative examples. Go to step 2. <ref> [11] </ref> used 120 images of scenery for collecting negative examples in the bootstrap manner described above. Further, several networks were trained in this manner. Various arbitration heuristics were used to combine the results of the multiple networks. More details about the training procedure can be found in [11]. 3 The De-Rotation <p> to step 2. <ref> [11] </ref> used 120 images of scenery for collecting negative examples in the bootstrap manner described above. Further, several networks were trained in this manner. Various arbitration heuristics were used to combine the results of the multiple networks. More details about the training procedure can be found in [11]. 3 The De-Rotation Network The De-Rotation network is trained to output the rotation of a face given in the 20x20 pixel inputs. The training examples consist of a set of images which contain faces each rotated a random amount within the image-plane. <p> The window is preprocessed to account for lighting variation. This step was described earlier in Section 2, and is adapted from [12]. 2. Histogram Equalization is performed on an oval region inside the window <ref> [11, 12] </ref>. 3. The window is passed to the De-Rotation network. A Gaussian is fit to the network's outputs, and the peak is interpreted as an angle between 0 ffi - 360 ffi . 4. The window is rotated by the angle returned in the previous step. <p> The rotated-window is passed to the detection network. If the detection network outputs a positive value, the window is labeled as a face, otherwise the window is labeled as a non-face. As suggested in <ref> [11] </ref>, this process can be repeated for multiple networks. <p> First, we examine the performance of one of the networks from the face detector (from <ref> [11] </ref>) on a set of 4000 upright faces images of size 20x20. These images were synthesized by randomly rotating a set of 1000 face images (about their center points) up to 10 ffi , scaling between 90% and 110%, translating up to half a pixel, and mirroring. <p> As expected, since the network is not trained to handle rotations greater than 10 ffi , the network's performance drops dramatically, to only being able to detect approximately 8% of the images as faces. Table 1. Results of using the Original System Described in <ref> [11] </ref> without any adjustment for rotations. Number Detected Detection Rate Original Face Images 3460/4000 86.5% Rotated Face Images 3181/40000 8.0% Next, we examine the ability of the full system; this system used the De-Rotation network to determine the correct orientation of each input image. <p> At the end of training, this false-positive set is fairly large, and is comprised of a set of images that were difficult for the network to correctly classify as face or non-face. In the face-detector presented in <ref> [11] </ref>, several networks were trained for face detection; each was individually trained by the process described above. Here, we use the false-positive training set of two of the networks trained in [11] to test the accuracy of the detector (note that the network tested here was not trained on this set <p> In the face-detector presented in <ref> [11] </ref>, several networks were trained for face detection; each was individually trained by the process described above. Here, we use the false-positive training set of two of the networks trained in [11] to test the accuracy of the detector (note that the network tested here was not trained on this set of images). <p> Table 3. Comparative Results on Non-Faces Number Mistakenly Detected False Positive Rate Original System from <ref> [11] </ref> 2252/11150 19.6% System with De-Rotation 2570/11150 22.3% Note that the original system falsely detects approximately 2252 out of this set of 11150 faces (a false detection rate of approximately 19.6%). When the system with the De-Rotation network is used, the false detection rate increases, see Table 3. <p> This system is much faster than the alternative of rotating the image multiple times, and applying the detector to each rotated image. However, the speed of this system is not yet up to the same performance as the "fast" version of the system presented in <ref> [11] </ref>. It is suspected that similar techniques which made that system fast can be applied here.
Reference: 12. <author> Kah-Kay Sung. </author> <title> Learning and Example Selection for Object nad Pattern Detection. </title> <type> PhD thesis, </type> <institution> MIT AI Lab, </institution> <month> January </month> <year> 1996. </year> <note> Available as AI Technical Report 1572. </note>
Reference-contexts: For the work presented here, the filter is applied at every pixel position in the image, and the image is scaled down by a factor of 1.2 for each step in the pyramid. The filtering algorithm is shown in Figure 1. First, a preprocessing step, adapted from <ref> [12] </ref>, is applied to a window of the image. The window is then passed through a neural network, which decides whether the window contains a face. The preprocessing first attempts to equalize the intensity values in across the window. <p> Collecting a representative set of non-faces is difficult. Instead of collecting the images before training is started, the images are collected during training, in the following manner, adapted from <ref> [12] </ref>: 3 The reader may question why it is not possible to simply train a network with images which have been rotated by larger amounts, and avoid this rotation problem altogether. Unfortunately, in the experiments conducted, training with larger amounts of rotation adversely affected performance. <p> The following steps are repeated for each 20x20 image which is given to the system: 1. The window is preprocessed to account for lighting variation. This step was described earlier in Section 2, and is adapted from <ref> [12] </ref>. 2. Histogram Equalization is performed on an oval region inside the window [11, 12]. 3. The window is passed to the De-Rotation network. A Gaussian is fit to the network's outputs, and the peak is interpreted as an angle between 0 ffi - 360 ffi . 4. <p> The window is preprocessed to account for lighting variation. This step was described earlier in Section 2, and is adapted from [12]. 2. Histogram Equalization is performed on an oval region inside the window <ref> [11, 12] </ref>. 3. The window is passed to the De-Rotation network. A Gaussian is fit to the network's outputs, and the peak is interpreted as an angle between 0 ffi - 360 ffi . 4. The window is rotated by the angle returned in the previous step.
Reference: 13. <author> R. Vaillant, C. Monrocq, and Y. Le Cun. </author> <title> Original approach for the localisation of objects in images. </title> <booktitle> IEE Proceedings on Vision, Image, and Signal Processing, </booktitle> <volume> 141(4), </volume> <month> August </month> <year> 1994. </year>
Reference: 14. <author> Thomas Vetter, Michael J. Jones, and Tomaso Poggio. </author> <title> A bootstrapping algorithm for learning linear models of object classes. </title> <booktitle> In Computer Vision and Pattern Recogition, </booktitle> <pages> pages 40-46, </pages> <address> San Juan, Puerto Rico, </address> <month> June </month> <year> 1997. </year>
Reference-contexts: Finally, it would be interesting to extend this system to out-of-plane rotations. By using knowledge of the shape and/or symmetry of the face, it should be possible to convert semi-profile views of a face to frontal views. Related work has been explored in <ref> [1, 14] </ref>. Acknowledgements I would like to thank Henry Rowley and Takeo Kanade for collaborating on the original face detection system. I would also like to thank Henry Rowley for pointing out the existence of several face detectors of which I was not aware.
Reference: 15. <author> Gaungzheng Yang and Thomas S. Huang. </author> <title> Human face detection in a complex background. </title> <journal> Pattern Recognition, </journal> <volume> 27(1) </volume> <pages> 53-63, </pages> <year> 1994. </year> <title> This article was processed using the L A T E X macro package with LLNCS style </title>
Reference-contexts: 1 Introduction This paper presents a general method to extend many template-based frontal, upright, face detection systems to handle in-plane rotations of the face. There have been many template-based face detection systems developed, for example, see <ref> [2, 3, 6-8, 11-13, 15] </ref>. Other systems, such as [5], can also achieve rotation invariance by extracting smaller features of the face and using graph-matching algorithms. In this paper, we concentrate on template-based methods, in particular the one presented by Rowley, Baluja & Kanade in [11].
References-found: 15

