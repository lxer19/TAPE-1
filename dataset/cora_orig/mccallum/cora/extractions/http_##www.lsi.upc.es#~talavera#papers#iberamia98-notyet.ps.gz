URL: http://www.lsi.upc.es/~talavera/papers/iberamia98-notyet.ps.gz
Refering-URL: http://www-lsi.upc.es/~talavera/papers.html
Root-URL: 
Email: roure@eupmt.upc.es  talavera@lsi.upc.es  
Phone: 2  
Title: Robust incremental clustering with bad instance orderings: a new strategy  
Author: Josep Roure and Luis Talavera Departament d'Informatica i Gestio 
Keyword: Machine Learning, Data mining, Incremental clustering, Order effects.  
Address: Avda. Puig i Cadafalch 101-111 08303 Mataro, Catalonia, Spain  Campus Nord, Modul C6, Jordi Girona 1-3 08034 Barcelona, Catalonia, Spain  
Affiliation: Escola Universitaria Politecnica de Mataro  Departament de Llenguatges i Sistemes Informatics Universitat Politecnica de Catalunya  
Abstract: It is widely reported in the literature that incremental clustering systems suffer from instance ordering effects and that under some orderings, extremely poor clusterings may be obtained. In this paper we present a new general strategy aimed to mitigate these effects, the Not-Yet strategy which has a general and open formulation and it is not coupled to any particular system. Unlike other proposals, this strategy maintains the incremental nature of learning process. In addition, we propose a classification of strategies to avoid ordering effects which clarifies the benefits and disadvantages we can expect from the proposal made in the paper as well from existing ones. A particular implementation of the Not-Yet strategy is used to conduct several experiments. Results suggest that the strategy improves the clustering quality. We also show that, when combined with other local strategies, the Not-Yet strategy allows the clustering system to get high quality clusterings. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> J. R. Anderson and M. Matessa. </author> <title> Explorations of an incremental, bayesian algorithm for categorization. </title> <journal> Machine Learning, </journal> (9):275-308, 1992. 
Reference-contexts: The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems <ref> [1, 8] </ref>. In addition, we considered an augmented version of this basic procedure by adding the merge and split operators used in COBWEB. Briefly, the merge operator modifies a hierarchy by combining two existing clusters while the split operator breaks existing clusters into smaller ones.
Reference: 2. <author> J. Bejar. Adquisicion automatica de conocimiento en dominios poco estructurados. </author> <type> PhD thesis, </type> <institution> Facultat d'Informatica de Barcelona, UPC, </institution> <year> 1995. </year>
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [2, 5, 7-10] </ref>. We say that incremental clustering algorithms exhibit ordering effects when they may yield different cluster structures when the same instances are presented in different orders. In some cases, they even can produce very poor quality clus-terings. <p> Thereafter, the clustering system may not be able to recover when further instances from other parts of the description space are observed. A typical example of preprocessing are seed selection methods which select 'seed' observations from data growing clusters around them <ref> [2, 11] </ref>. When constructing a cluster structure in an incremental fashion, only two basic operators are needed, one to create a new cluster given an instance and another to incorporate an instance to an existing cluster. Theoretically, using these two operators, any clustering structure could be built. <p> We think that this formulation should help in applying the strategy to any existing algorithm without any major changes. The second related work (from which the Not-Yet name is borrowed) is the application of this strategy to the LINNEO + clustering system <ref> [2, 13] </ref>. This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Although devoted to global methods, we have to mention relevant Fisher's work on iterative optimization of clusterings [5].
Reference: 3. <author> U. Fayyad, G. Piatetsky-Shapiro, and P. Smyth. </author> <title> Knowledge discovery and data mining: towards a unifying framework. </title> <booktitle> In Proceedings of the Second International Conference on Knowledge Discovery and Data Mining, </booktitle> <address> KDD96, Portland, OR, 1996. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: This time will be dependent on the particular application. It is unclear how the proposed procedure scales up to large datasets such as those typically referred to in data mining tasks <ref> [3] </ref>. However, we think that the Not-Yet strategy may be an inexpensive and effective way of avoiding ordering effects since it is unlikely that a whole large dataset would present a bad order.
Reference: 4. <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> (2):139-172, 1987. 
Reference-contexts: These operators can be viewed as providing some sort of backtracking capabilities to the system without having a memory of previous knowledge structures. A classical example of clustering operators are the merge and split operators of COBWEB <ref> [4] </ref>. Finally, we can tackle with ordering effects using after clustering strategies. These strategies are intended to act upon a previously obtained clustering in order to improve it. Iterative optimization algorithms are well suited to this approach. <p> This is a typical model of incremental clustering using a hill climbing strategy which estimates the goodness of applying the available operators and chooses the best option, without reconsider any decision made. Particularly, this model corresponds to the one used in the COBWEB system <ref> [4] </ref>. The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems [1, 8].
Reference: 5. <author> D. H. Fisher. </author> <title> Optimization and simplification of hierarchical clusterings. </title> <journal> Journal of Artificial Intelligence Research, </journal> (4):147-180, 1995. 
Reference-contexts: This way of incorporating single instances into the cluster structure makes incremental systems to be sensitive to instance order, as widely reported in the clustering literature <ref> [2, 5, 7-10] </ref>. We say that incremental clustering algorithms exhibit ordering effects when they may yield different cluster structures when the same instances are presented in different orders. In some cases, they even can produce very poor quality clus-terings. <p> The idea is to arrange the instance order in such a way that favors the system search process to reach the best classification. It is seen that when dissimilar objects are consecutively presented, the resulting classification is much better than when similar objects are presented together <ref> [5, 7] </ref>. This occurs because, in the former case, initial observations are from different areas of the description space leading initial clusters to reflect these areas, while in the later, a skewed cluster structure may evolve. <p> This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Although devoted to global methods, we have to mention relevant Fisher's work on iterative optimization of clusterings <ref> [5] </ref>. This work explores several methods for iteratively improving clustering quality, showing that among these methods some exhibit an optimum performance. But recall that these methods often operate reprocessing the whole dataset and violate the constraints stated for incremental clustering.
Reference: 6. <author> D. H. Fisher and P. Langley. </author> <title> Conceptual clustering and its relation to numerical taxonomy. </title> <editor> In W. A. Gale, editor, </editor> <booktitle> Artificial Intelligence and Statistics. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading,MA, </address> <year> 1986. </year>
Reference-contexts: 1 Introduction Ideally, intelligent agents should possess the ability of adapting their behavior to the environment over time through learning. Thus, learning methods should be able of updating a knowledge base in a continual basis as new experience is gained. Particularly, if an agent performing a clustering task <ref> [6] </ref> should be able of using its learned knowledge to carry out some performance task at any stage of learning, the conceptual scheme should evolve as every new instance is observed without simultaneously processing previous instances. This sort of clustering is often referred to as incremental clustering.
Reference: 7. <author> D. H. Fisher, L. Xu, and N. Zard. </author> <title> Ordering effects in clustering. </title> <booktitle> In Proceedings of the Ninth International Conference on Machine Learning, </booktitle> <pages> pages 163-168, </pages> <year> 1992. </year>
Reference-contexts: The idea is to arrange the instance order in such a way that favors the system search process to reach the best classification. It is seen that when dissimilar objects are consecutively presented, the resulting classification is much better than when similar objects are presented together <ref> [5, 7] </ref>. This occurs because, in the former case, initial observations are from different areas of the description space leading initial clusters to reflect these areas, while in the later, a skewed cluster structure may evolve.
Reference: 8. <author> J. H. Gennari, P. Langley, and D. Fisher. </author> <title> Models of incremental concept formation. </title> <journal> Artificial Intelligence, </journal> (40):11-61, 1989. 
Reference-contexts: The measure of category utility used in this system is also used in the experiments as the CQF. We used a COBWEB-like clustering strategy because it is simple, well-known and it has been applied (or augmented) in several learning systems <ref> [1, 8] </ref>. In addition, we considered an augmented version of this basic procedure by adding the merge and split operators used in COBWEB. Briefly, the merge operator modifies a hierarchy by combining two existing clusters while the split operator breaks existing clusters into smaller ones.
Reference: 9. <author> P. Langley. </author> <title> Order effects in incremental learning. </title> <editor> In P. Reimann and H. Spada, editors, </editor> <title> Learning in humans and machines: Towards an Interdisciplinary Learning Science. </title> <publisher> Pergamon, </publisher> <year> 1995. </year>
Reference-contexts: This sort of clustering is often referred to as incremental clustering. As noted by Langley <ref> [9] </ref>, there can be several interpretations of incremental learning. In the remainder of this paper, we will assume that a clustering method is incremental if inputs one instance at a time, does not reprocess previous instances and maintains a single conceptual structure in memory.
Reference: 10. <author> M. Lebowitz. </author> <title> Deferred commitment in unimem: waiting to learn. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 80-86, </pages> <year> 1988. </year>
Reference-contexts: Lebowitz first introduced the idea of deferred commitment within the framework of his UNIMEM conceptual clustering system <ref> [10] </ref>. Our proposal extends Lebowitz's work by decoupling the buffering strategy from any particular system. Also, we have introduced the ff parameter, that allows to see the original algorithm as a particular case of the new control strategy.
Reference: 11. <author> R. S. Michalski and R. E. Stepp. </author> <title> Learning from observation: Conceptual clustering. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial intelligence approach, </booktitle> <pages> pages 331-363. </pages> <publisher> Morgan Kauffmann, </publisher> <address> San Mateo, CA, </address> <year> 1983. </year>
Reference-contexts: Thereafter, the clustering system may not be able to recover when further instances from other parts of the description space are observed. A typical example of preprocessing are seed selection methods which select 'seed' observations from data growing clusters around them <ref> [2, 11] </ref>. When constructing a cluster structure in an incremental fashion, only two basic operators are needed, one to create a new cluster given an instance and another to incorporate an instance to an existing cluster. Theoretically, using these two operators, any clustering structure could be built.
Reference: 12. <author> P.M. Murphy and D.W. Aha. </author> <title> Repository of machine learning. </title> <institution> University of Cali-fornia at Ivrine. </institution> <note> URL: http://www.ics.uci.edu/mlearn/MLRpositoru.html. </note>
Reference-contexts: Implementation of the Not-Yet control strategy for the experiments. formulation of the Not-Yet strategy, more complex criteria might be used, becoming computational complexity harder. 4 Experiments In order to empirically evaluate the Not-Yet strategy we conducted several experiments using four well-known datasets of the UCI repository <ref> [12] </ref>. Since the clustering task is an unsupervised learning task, we have treated labels just as another attribute. In the experiments we assume a general model of hierarchical incremental clustering using two basic operators, one for creating a new class and another to incorporate an instance to an existing class.
Reference: 13. <author> J. Roure. </author> <title> Study of methods and heuristics to improve the fuzzy classifications of LINNEO + . Master's thesis, </title> <institution> Facultat d'Informatica de Barcelona, UPC, </institution> <year> 1994. </year>
Reference-contexts: We think that this formulation should help in applying the strategy to any existing algorithm without any major changes. The second related work (from which the Not-Yet name is borrowed) is the application of this strategy to the LINNEO + clustering system <ref> [2, 13] </ref>. This work contains the basic ideas proposed here, but again the application is tuned for an specific system and the problems studied are deeply related to a particular clustering strategy. Although devoted to global methods, we have to mention relevant Fisher's work on iterative optimization of clusterings [5].
References-found: 13

