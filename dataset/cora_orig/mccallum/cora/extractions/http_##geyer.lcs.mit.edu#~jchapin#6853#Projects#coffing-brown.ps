URL: http://geyer.lcs.mit.edu/~jchapin/6853/Projects/coffing-brown.ps
Refering-URL: http://geyer.lcs.mit.edu/~jchapin/6853/Projects/index.html
Root-URL: 
Title: A System for Transparent File Compression With Caching Under Linux  
Author: Charles Coffing and Jeremy H. Brown 
Date: November 30, 1997  
Abstract: Transparent file compression systems handle file compression and decompression of files automatically; they are thus transparent to users. The cost of many transparent file system compression schemes, however, is that file accesses require substantial processor time for compression and decompression. One approach to alleviating this problem is to cache recently accessed files (active files) in uncompressed format. This paper examines the design and implementation of a system built on Linux, a free UNIX-like operating system, and e2compr, an existing transparent compression patch for Linux, to provide a framework for experimenting with the exact heuristics underlying such caching. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Remy Card, Theodore Ts'o, and Stephen Tweedie. </author> <title> Design and implementation of the second extended filesystem. </title> <booktitle> In Proceedings to the First Dutch International Symposium on Linux. </booktitle> <address> http://www.redhat.com:8080/HyperNews/get/fs/ext2intro.html. </address>
Reference-contexts: Among other features, the VFS layer provides a buffer pool for managing disk blocks; among other things, the buffer pool takes care of flushing dirty blocks to disk. The standard Linux file system is the Second Extended File System <ref> [1] </ref>, also known as the ext2 file system. Ext2 is very similar to the BSD filesystem [5]. Under ext2, each file is represented by an inode structure. Inodes are preallocated when an ext2 file system is installed on a partition.
Reference: [2] <author> Vincent Cate and Thomas Gross. </author> <title> Combining the concepts of compression and caching for a two-level filesystem. </title> <booktitle> In 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 200-211. </pages> <publisher> ACM, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: DTFS achieves compression of around 2:1, and disk throughput around 85% that of the throughput of a standard UNIX file system. Where DTFS applies continuous compression to all files, the ATTIC <ref> [2] </ref> system makes a distinction between active and inactive files. Active files are those which are likely to be used again soon by the user; others are inactive. The active set on several UNIX systems is empirically measured to be around 10% of all data. <p> A shortcoming of both the RISC/IX and e2compr [6] compression systems is that user intervention is required to decide which files to compress (and, under e2compr, how to compress a file.) DTFS [3] and ATTIC <ref> [2] </ref>, on the other hand, brought total transparency to file compression, including a complete absence of need for user intervention. We wanted to bring that total transparency to our system. Goal 2 Uncompressing an entire file before returning any data imposes unacceptable overhead. <p> This "speculative uncompression" by the migrator costs relatively little disk space, but it may remove a noticeable delay for the user should the rest of the file be needed. These basic heuristics are similar to those implemented by ATTIC <ref> [2] </ref>; however, we have added several additional heuristics to avoid unnecessary compression and uncompression. file ratio file ratio file ratio *.1 0.40 *.fds 0.00 *.osf 0.26 *.3 0.49 *.fdscript 0.34 *.out 0.43 *.a 0.57 *.file 0.20 *.p 0.08 *.ag 0.71 *.flex 0.31 *.pas 0.59 *.aix 0.27 *.fra 0.40 *.pdf 0.24 *.arf <p> These expected ratios allow the migrator to ignore smaller files which will likely not yield a full disk block of savings when compressed. The addition of expected compression ratios is a notable difference from the ATTIC <ref> [2] </ref> migrator. Their migrator assumed a low workload at night, which allowed it time to attempt compression on all files. Only those files which compressed well would be kept in compressed form. <p> The advantage would become more clear on a system with a small or nonexistent cache. 7.4 Migrator Results Several statistics on file access patterns and migra-tor performance were gathered, although due to time constraints long term results are not yet available. Cate and Gross noted in <ref> [2] </ref> that most systems have around ten percent or less of the data accessed per day. On our test system, overall we found about five percent of files were accessed per day, on average. For this paper, we specifically studied a home directory containing approximately 190 megabytes of data.
Reference: [3] <author> Morgan Clark and Stephen Rago. </author> <title> The desktop file system. </title> <booktitle> In Proceedings of the Summer 1994 USENIX Conference, </booktitle> <pages> pages 113-124, </pages> <year> 1994. </year>
Reference-contexts: RISC/IX targets system vendors who are installing large operating systems on small disks; users don't get true transparency, since the compressed executables are not manipulatable by standard debuggers or other binary tools. On the other hand, the Desktop File System (DTFS) <ref> [3] </ref> provides comprehensive transparent file compression for end-users on all files stored on a DTFS volume. <p> A shortcoming of both the RISC/IX and e2compr [6] compression systems is that user intervention is required to decide which files to compress (and, under e2compr, how to compress a file.) DTFS <ref> [3] </ref> and ATTIC [2], on the other hand, brought total transparency to file compression, including a complete absence of need for user intervention. We wanted to bring that total transparency to our system. Goal 2 Uncompressing an entire file before returning any data imposes unacceptable overhead. <p> Goal 4 Different types of files merit different compression policies. The authors of the DTFS <ref> [3] </ref> paper state that in future work, they plan to apply different types of compression algorithms to different classes of data. Similarly, the authors of the RISC/IX paper propose sharing compression tables between sufficiently similar files. <p> Similarly, the authors of the RISC/IX paper propose sharing compression tables between sufficiently similar files. We, too, believe that experiments in heuristically guided transparent file compression could take place along a number of axes. However, for this project we have forgone options such as those proposed in <ref> [3] </ref> or [8], and chosen to focus entirely upon when, how, and to which files a single (de)compression algorithm should be applied.
Reference: [4] <author> Tom R. Halfhill. </author> <title> How safe is data compression? Byte, </title> <booktitle> 19(2) </booktitle> <pages> 56-74, </pages> <year> 1994. </year>
Reference-contexts: In this section, we will briefly review a few of the projects about which something has been written (and in some cases from which we took our inspiration) in order to place our work in context. <ref> [4] </ref> reviews five transparent file compression utilities for MS-DOS, primarily with an eye toward reliability and safety. Each utility inserts a device driver into the system at boot time; the driver simulates a disk drive, but all files stored on it are compressed and uncompressed on-the-fly. <p> In this section we explore each of these goals and the beliefs that gave rise to them. Goal 1 Users should not have to think about compression. A major shortcoming of both the MS-DOS <ref> [4] </ref> and RISC/IX [8] compression systems is that for one reason or another, compression is not truly, totally transparent; some applications cannot deal with some compressed files.
Reference: [5] <author> M. McKusick, W. Joy, S. Le*er, and R. Fabry. </author> <title> A fast file system for unix. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 2(3) </volume> <pages> 181-197, </pages> <month> August </month> <year> 1984. </year>
Reference-contexts: The standard Linux file system is the Second Extended File System [1], also known as the ext2 file system. Ext2 is very similar to the BSD filesystem <ref> [5] </ref>. Under ext2, each file is represented by an inode structure. Inodes are preallocated when an ext2 file system is installed on a partition. The inode for a file contains space to point directly to 12 data blocks, an indirect block, a double indirect block, and a triple indirect block.
Reference: [6] <author> Peter Moulder. </author> <note> E2compr page. Web page, 1997. http://www.netspace.net.au/ re-iter/e2compr.html. </note>
Reference-contexts: For large files, this delay can be several seconds long. As we explain in the next section, the work we report on in this paper draws much of its inspiration from ATTIC. Finally, e2compr <ref> [6] </ref> is a patch applied to Linux [9], a free, UNIX-like operating system, which provides transparent, on-the-fly file compression. A file is compressed in chunks called clusters. Uncompres-sion of individual clusters when read or written is on-demand; compression of uncompressed clusters waits until the file is closed. <p> A major shortcoming of both the MS-DOS [4] and RISC/IX [8] compression systems is that for one reason or another, compression is not truly, totally transparent; some applications cannot deal with some compressed files. A shortcoming of both the RISC/IX and e2compr <ref> [6] </ref> compression systems is that user intervention is required to decide which files to compress (and, under e2compr, how to compress a file.) DTFS [3] and ATTIC [2], on the other hand, brought total transparency to file compression, including a complete absence of need for user intervention.
Reference: [7] <author> Greg Roelofs. </author> <title> Zlib home page. Web page, </title> <publisher> Walnut Creek CD-ROM, </publisher> <year> 1997. </year> <note> http://www.cdrom.com/pub/infozip/zlib/. </note>
Reference-contexts: was made available by a (relatively recent) previous un-compression. e2compr makes no reliability guarantees if a compressed cluster is half-written to disk at the time of a crash, the entire cluster's data could be unreadable due to the inter-block dependencies introduced by compression. e2compr ships with the compression algorithms gzip <ref> [7] </ref>, lzv, and lzrw.
Reference: [8] <author> Mark Taunton. </author> <title> Compressed executables: an exercise in thinking small. </title> <booktitle> In Proceedings of the Summer 1991 USENIX Conference, </booktitle> <pages> pages 385-403, </pages> <year> 1991. </year>
Reference-contexts: While each utility strives for complete transparency, due to MS-DOS's weak protection, in practice many popular applications do not work with compressed volumes. Unlike MS-DOS, all the UNIX systems we review here provide at least basic interoperability. The least transparent system is RISC/IX <ref> [8] </ref>, a UNIX-like kernel which recognizes an executable format in which the data and text segments are compressed in 32kb chunks. When such an executable is run, the RISC/IX kernel uncompresses pages on-demand. <p> In this section we explore each of these goals and the beliefs that gave rise to them. Goal 1 Users should not have to think about compression. A major shortcoming of both the MS-DOS [4] and RISC/IX <ref> [8] </ref> compression systems is that for one reason or another, compression is not truly, totally transparent; some applications cannot deal with some compressed files. <p> Similarly, the authors of the RISC/IX paper propose sharing compression tables between sufficiently similar files. We, too, believe that experiments in heuristically guided transparent file compression could take place along a number of axes. However, for this project we have forgone options such as those proposed in [3] or <ref> [8] </ref>, and chosen to focus entirely upon when, how, and to which files a single (de)compression algorithm should be applied.
Reference: [9] <author> L. </author> <title> Torvalds. Linux kernel implementation. </title> <booktitle> In Proceedings of the AUUG94 Conference, </booktitle> <pages> pages vi+274, 9-14. </pages> <address> AUUG, </address> <month> September </month> <year> 1994. </year> <month> 12 </month>
Reference-contexts: For large files, this delay can be several seconds long. As we explain in the next section, the work we report on in this paper draws much of its inspiration from ATTIC. Finally, e2compr [6] is a patch applied to Linux <ref> [9] </ref>, a free, UNIX-like operating system, which provides transparent, on-the-fly file compression. A file is compressed in chunks called clusters. Uncompres-sion of individual clusters when read or written is on-demand; compression of uncompressed clusters waits until the file is closed.
References-found: 9

