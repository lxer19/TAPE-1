URL: http://www.eecis.udel.edu:80/~kchen/papers/fa97.ps
Refering-URL: http://www.eecis.udel.edu:80/~kchen/research.html
Root-URL: http://www.cis.udel.edu
Email: fkchen, chandrag@cis.udel.edu  
Title: Real-time Facial Animation through Internet  
Author: Kai Chen and Chandra Kambhamettu 
Address: 19716, USA  
Affiliation: Department of Computer and Information Sciences University of Delaware Newark, DE  
Abstract: This paper describes an experiment to transfer parameterized face data across Internet. Face data is transferred using RTP based on UDP. In unicast scenario, rate adaptation is achieved by lost rate feedback. In multicast scenario, we design a method based on receiver-driven layered multicast. For lost and delayed frames, we propose graphics compensation algorithms to improve the quality of animation. Parabola is used for prediction and cubic spline for interpolation. A fast calculation method is also developed for this special problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michael J. Black and Yaser Yacoob. </author> <title> Tracking and recognizing rigid and non-rigid facial motions using local parameteric models of image motion. </title> <booktitle> International Conference on Computer Vision, </booktitle> <pages> pages 374-381, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual [4, 8] and automatic techniques <ref> [13, 10, 6, 1] </ref> that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements.
Reference: [2] <author> J. Bolot and T. Turletti. </author> <title> A rate control mechanism for packet video in the internet. </title> <institution> INRIA B.P.93, </institution> <year> 1993. </year>
Reference-contexts: In this scheme, only one kind of RTCP is needed. To do this, we instantiate an application-defined (APP, PT=204) RTCP packet [18], as shown in Figure 7. Lost rate occupies 4 bytes. Sender performs rate adjustment by the following adaptive algorithm, as discussed in <ref> [2] </ref>. if (lost_rate &gt; lost_tolerant_rate) send_rate = max (send_rate/2,MIN_RATE); else send_rate = min (1.5*sendface,MAX_RATE); This is a feedback control scheme. The lost tolerant rate is chosen by the user, according to the quality requirements. Usually it is a trade-off between lost rate and frame rate.
Reference: [3] <author> J. Bolot, T. Turletti, and I. Wakeman. </author> <title> Scalable feedback control for multicast video distribution in the internet. </title> <institution> INRIA, </institution> <year> 1993. </year>
Reference: [4] <author> P. Ekman and W. V. Friesen. </author> <title> Facial action coding system: A technique for the measurement of facial movement. </title> <publisher> Consulting Psychologists Press, </publisher> <address> Palo Alto, Calif., </address> <year> 1978. </year>
Reference-contexts: There has been tremendous amount of research in both facial analysis and synthesis. Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual <ref> [4, 8] </ref> and automatic techniques [13, 10, 6, 1] that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements.
Reference: [5] <author> Paul Ekman, Thomas S. Huang, and Terrence J. Se-jnowski. </author> <title> Final report to nsf. In NSF Workshop on Facial Expression Understanding, </title> <year> 1992. </year>
Reference-contexts: A complete survey of facial analysis research can be found in the NSF-supported facial gesture workshop'92 report <ref> [5] </ref>. Facial synthesis and animation dates back to twenty years ago when Parke proposed his face model in [14]. Since then, face modeling became a fascinated research field in computer graphics, mostly due to the challenges involved in modeling the complex deformations of face during expressions.
Reference: [6] <author> I. Essa and A. Pentland. </author> <title> Modeling and interactive animation of facial expression using input from video. </title> <booktitle> Proceedings of Computer Animation 1996 Conference, </booktitle> <month> June </month> <year> 1996. </year>
Reference-contexts: Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual [4, 8] and automatic techniques <ref> [13, 10, 6, 1] </ref> that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements. <p> We also investigated two factors that effect the real-time animation at the receiver-end. These factors were then incorporated in a feedback control mechanism. An integrated system for video conferencing using face model has many research issues to it. These studies include, i) real-time extraction of face parameters <ref> [13, 6] </ref>, ii) standardization of face parameter sets [17], iii) real-time rendering of face parameters [16] and iv) Internet transmission of face parameters.
Reference: [7] <author> Van Jacobson. </author> <title> Multimedia conferencing on the internet. </title> <booktitle> SIGCOMM '94 Tutorial, </booktitle> <year> 1994. </year>
Reference-contexts: Our approach exploits the geometric constraints during small deformations to estimate face motion. Face data is transferred using RTP on top of UDP. UDP is a connection-less transport protocol, which is more appropriate than a connection-oriented protocol in real-time data transfer <ref> [7] </ref>. RTP [18] is a generic real time protocol. It depends on explicit time stamp in each packet for timing reconstruction at the receiver side. Currently most Internet packet audio/video applications, such as vic and vat [11], also apply RTP for data transfer.
Reference: [8] <author> S. Kaiser, T. Wehrle, and P. Edwards. </author> <title> Multi-modal emotion measurement in an interactive computer game: A pilot-study. </title> <booktitle> In Proc. of the VIIIth Conference of the International Society for Research on Emotions, </booktitle> <year> 1994. </year>
Reference-contexts: There has been tremendous amount of research in both facial analysis and synthesis. Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual <ref> [4, 8] </ref> and automatic techniques [13, 10, 6, 1] that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements.
Reference: [9] <author> Chandra Kambhamettu, Dmitry B. Goldgof, and Matthew He. </author> <title> Determination of motion parameters and estimation of point correspondences in small non-rigid deformations. </title> <booktitle> Proceedings of IEEE conference on Computer Vision and Pattern Recognition, </booktitle> <pages> pages 943-946, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We are currently performing experiments on fast extraction of parameter sets <ref> [13, 9] </ref>. Our future efforts include, i) identify and transfer hierarchical face parameter sets via multicast, ii) study the behavior of recorded face data and design better prediction and interpolation algorithms, and iii) perform real-time, hierarchical extraction of face parameters.
Reference: [10] <author> Satoshi Kimura Katsuhiro Matsuno, Chil-Woo Lee and Saburo Tsuji. </author> <title> Automatic recognition of human facial expressions. </title> <booktitle> International Conference on Computer Vision, </booktitle> <pages> pages 352-359, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual [4, 8] and automatic techniques <ref> [13, 10, 6, 1] </ref> that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements.
Reference: [11] <author> S. McCanne and V. Jacobson. </author> <title> vic: A flexible framework for packet video. </title> <booktitle> ACM Multimedia '95, </booktitle> <year> 1995. </year>
Reference-contexts: RTP [18] is a generic real time protocol. It depends on explicit time stamp in each packet for timing reconstruction at the receiver side. Currently most Internet packet audio/video applications, such as vic and vat <ref> [11] </ref>, also apply RTP for data transfer. In point-to-point uni-cast, rate-adaption is a feedback scheme. Receiver periodically reports current lost rate by sending RTCP packet to the sender. Sender adjusts sending rate accordingly. Let's look at the receiver in Figure 2. It has two functional modules, network and graphics.
Reference: [12] <author> S. McCanne, V. Jacobson, and M. Vetterli. </author> <title> Receiver-driven layered multicast. </title> <booktitle> ACM SIGCOMM '96, </booktitle> <year> 1996. </year>
Reference-contexts: Problem arises here for rate adaptation. Feedback control is not feasible because i) too many feedback packets (explosion), and ii) no target rate for different receivers <ref> [12] </ref>[3]. We fit our framework into the Receiver-driven Layered Mul-ticast which is proposed in [12]. In this scheme, sender sends face data into different multicast groups (channels). There is no feedback information. Receivers are responsible for choosing a subset of those groups. <p> Usually it is a trade-off between lost rate and frame rate. One can choose to get a quick lossy transfer, or the reverse. In multicast context, there is no feedback control. Each receiver is responsible for choosing a number of multicast channels according to the bandwidth availability <ref> [12] </ref>. With higher network capacity, the receiver can subscribe to more channels and combine those data to get better quality of animation. Receivers should be able to add or drop channels by watching the current lost rate.
Reference: [13] <author> Alex Pentland Nuria Oliver and F. Berard. LAFTER: </author> <title> lips and face real-time tracker. </title> <booktitle> In Proc. of IEEE Conference on Computer Vision and Pattern Recognition (CVPR), </booktitle> <year> 1997. </year>
Reference-contexts: Facial analysis mainly constitutes the development of methods for measuring the facial movements that are generated by the muscle actions. These measurements, in turn, are used in recognizing facial expressions. There have been both manual [4, 8] and automatic techniques <ref> [13, 10, 6, 1] </ref> that were developed for the analysis. While the manual methods are time consuming, requiring more than an hour to measure 1-minute of facial expression, automatic methods are susceptible to color and illumination changes, thus resulting in imprecise measurement of facial movements. <p> We also investigated two factors that effect the real-time animation at the receiver-end. These factors were then incorporated in a feedback control mechanism. An integrated system for video conferencing using face model has many research issues to it. These studies include, i) real-time extraction of face parameters <ref> [13, 6] </ref>, ii) standardization of face parameter sets [17], iii) real-time rendering of face parameters [16] and iv) Internet transmission of face parameters. <p> We are currently performing experiments on fast extraction of parameter sets <ref> [13, 9] </ref>. Our future efforts include, i) identify and transfer hierarchical face parameter sets via multicast, ii) study the behavior of recorded face data and design better prediction and interpolation algorithms, and iii) perform real-time, hierarchical extraction of face parameters.
Reference: [14] <author> F. I. Parke. </author> <title> A parametric model for human faces. </title> <type> Technical report, </type> <institution> University of Utah, </institution> <address> Salt Lake city, Utah, </address> <year> 1972. </year>
Reference-contexts: A complete survey of facial analysis research can be found in the NSF-supported facial gesture workshop'92 report [5]. Facial synthesis and animation dates back to twenty years ago when Parke proposed his face model in <ref> [14] </ref>. Since then, face modeling became a fascinated research field in computer graphics, mostly due to the challenges involved in modeling the complex deformations of face during expressions. Many models have been reported, out of which, parametric models and physically-based models represent the two mainstream modeling approaches [15].
Reference: [15] <author> Frederic I. Parke and Keith Waters. </author> <title> Computer Facial Animation. </title> <publisher> A K PETERS LTD, </publisher> <address> Massachusetts, USA, </address> <year> 1996. </year>
Reference-contexts: Since then, face modeling became a fascinated research field in computer graphics, mostly due to the challenges involved in modeling the complex deformations of face during expressions. Many models have been reported, out of which, parametric models and physically-based models represent the two mainstream modeling approaches <ref> [15] </ref>. Survey of the existing facial animation models can be found in the NSF-supported facial animation workshop'93 report [16]. It is interesting to note that research in facial analysis (computer vision and image understanding) and synthesis (computer graphics and animation) has almost been parallel, with various synergistic models. <p> Table 2: Mapping of k and t 3 System Implementations Our prototype fits into the unicast system framework as shown in Figure 1. It includes a sender and a receiver, which locate in different hosts across Internet. We use Parke's face model <ref> [15] </ref> as the system face model. This framework is however general enough to adapt any other face models. Currently we are using a pseudo-extractor, which generates a sequence of parameters stored in a file.
Reference: [16] <author> Catherine Pelachaud, Norman I. Badler, and Marie-Luce Viaud. </author> <title> Final report to nsf. In NSF Workshop on Facial Animation, </title> <year> 1994. </year>
Reference-contexts: Many models have been reported, out of which, parametric models and physically-based models represent the two mainstream modeling approaches [15]. Survey of the existing facial animation models can be found in the NSF-supported facial animation workshop'93 report <ref> [16] </ref>. It is interesting to note that research in facial analysis (computer vision and image understanding) and synthesis (computer graphics and animation) has almost been parallel, with various synergistic models. However, there has not been much research in the area of facial transmission at the packet-level. <p> These factors were then incorporated in a feedback control mechanism. An integrated system for video conferencing using face model has many research issues to it. These studies include, i) real-time extraction of face parameters [13, 6], ii) standardization of face parameter sets [17], iii) real-time rendering of face parameters <ref> [16] </ref> and iv) Internet transmission of face parameters.
Reference: [17] <author> Eric Petajan. </author> <title> Facial animation coding (unofficial derivative of MPEG-4 standardization, work-in-progress. </title> <type> Technical report, Face & Body Animation Ad Hoc Group, </type> <month> September 10, </month> <year> 1997. </year>
Reference-contexts: These factors were then incorporated in a feedback control mechanism. An integrated system for video conferencing using face model has many research issues to it. These studies include, i) real-time extraction of face parameters [13, 6], ii) standardization of face parameter sets <ref> [17] </ref>, iii) real-time rendering of face parameters [16] and iv) Internet transmission of face parameters.
Reference: [18] <author> H. Schulzrinne, S. Casner, R. Frederick, and V. Ja-cobson. Rtp: </author> <title> A transport protocol for real-time applications. </title> <type> RFC 1889, </type> <year> 1996. </year>
Reference-contexts: Our approach exploits the geometric constraints during small deformations to estimate face motion. Face data is transferred using RTP on top of UDP. UDP is a connection-less transport protocol, which is more appropriate than a connection-oriented protocol in real-time data transfer [7]. RTP <ref> [18] </ref> is a generic real time protocol. It depends on explicit time stamp in each packet for timing reconstruction at the receiver side. Currently most Internet packet audio/video applications, such as vic and vat [11], also apply RTP for data transfer. In point-to-point uni-cast, rate-adaption is a feedback scheme. <p> Parameter Sets This framework is not restricted to any particular face model, as long as the model has a set of parameters describing the facial behavior. 2.3 Network Transfer Face data is transferred using RTP <ref> [18] </ref> on top of UDP. Each RTP packet contains face parameters for one frame. We do this with three reasons in mind. First, to avoid loosing a number of consecutive frames. In such a case, inaccuracy of prediction and interpolation will be accumulated. <p> The overhead will not account for too much bandwidth. parameter set of Parke's face model. A general de PT frame rate (f/s) 126 1/0.01 128 1/0.04 130 1/0.16 132 1/0.64 Table 1: Example of PT and Frame Rate Mapping scription of RTP packet format can be found in <ref> [18] </ref>. Some important fields are payload type, time stamp and sequence number. Payload type (PT) indicates the sending rate. It can also be interpreted as the duration of the current frame. An example of PT and sending rate mapping is shown in Table 1. <p> An example of PT and sending rate mapping is shown in Table 1. Sender and receiver should agree on this mapping. Timestamp (TS) indicates the start time of the frame. It increases with a frequency that equals the maximum frame rate <ref> [18] </ref>. By explicitly keeping time stamps in each RTP packet, receiver can reconstruct the timing sequence of the frames. Sequence number indicates the sending sequence of the frames. Face parameters occupy the data field. Parameters are packed one by one without index. <p> In this example, there are 50 parameters and each parameter occupies 4 bytes. The number of parameters is fixed. In point-to-point unicast, receiver periodically sends RTCP packet to the sender to report the current lost rate. RTCP has a number of functionalities <ref> [18] </ref>. In this scheme, only one kind of RTCP is needed. To do this, we instantiate an application-defined (APP, PT=204) RTCP packet [18], as shown in Figure 7. Lost rate occupies 4 bytes. <p> In point-to-point unicast, receiver periodically sends RTCP packet to the sender to report the current lost rate. RTCP has a number of functionalities <ref> [18] </ref>. In this scheme, only one kind of RTCP is needed. To do this, we instantiate an application-defined (APP, PT=204) RTCP packet [18], as shown in Figure 7. Lost rate occupies 4 bytes. Sender performs rate adjustment by the following adaptive algorithm, as discussed in [2]. if (lost_rate &gt; lost_tolerant_rate) send_rate = max (send_rate/2,MIN_RATE); else send_rate = min (1.5*sendface,MAX_RATE); This is a feedback control scheme.
Reference: [19] <author> Andrew S. Tanenbaum. </author> <title> Computer Networks. </title> <publisher> Prentice Hall PTR, </publisher> <address> New Jersy, USA, </address> <year> 1996. </year>
Reference-contexts: Multicast is sending IP packets from one host to a group of hosts. In multi-party video conference, multicasting data is more efficient. For details please refer to <ref> [19] </ref>. of parameters. The number of parameters that cap-tures the facial behavior depends on the face model used. Therefore, in a video conferencing context, only the face animation parameters need to be transferred to a remote side so as to synthesize the facial movement.
Reference: [20] <author> Z. Tang and J. Zhou. </author> <title> Introduction to CAD. </title> <institution> China Science and Technology Press, Beijing, China, </institution> <year> 1988. </year>
Reference-contexts: P1 and P2 are two old parameters, while P3 is the new one. P4 is the parameter to be interpolated. A cubic spline with free ends is <ref> [20] </ref>: 2 2 1 0 0 1 2 5 4 0 0 0 5 = 4 3 (P 3 P 1 ) 3 The spline curve from P2 to P3 is what we need. P4 is interpolated using this curve.
References-found: 20

