URL: http://www.cs.cmu.edu/~laura/Papers-PS/icassp98_final.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs/user/laura/www/pages/publications.html
Root-URL: 
Email: flaura,riesg@cs.cmu.edu  
Title: AN AUTOMATIC METHOD FOR LEARNING A JAPANESE LEXICON FOR RECOGNITION OF SPONTANEOUS SPEECH  
Author: Laura Mayfield Tomokiyo and Klaus Ries 
Address: Karlsruhe  
Affiliation: Interactive Systems Laboratories Carnegie Mellon University and Universitat  
Abstract: When developing a speech recognition system, one must start by deciding what the units to be recognized should be. This is for the most part a straightforward choice in the case of word-based languages such as English, but becomes an issue even in handling languages with a complex compounding system like German; with an agglutinative language like Japanese, which provides no spaces in written text, the choice is not at all obvious. Once an appropriate unit has been determined, the problem of consistently segmenting transcriptions of training data must be addressed. This paper describes a method for learning a lexicon from a training corpus which contains no word-level segmentation, applied to the problem of building a Japanese speech recognition system. We show not only that one can satisfactorily segment transcribed training data automatically, avoiding human error, but also that our system, when trained with the automatically segmented corpus, showed a significant improvement in recognition performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Geutner, Petra and Rob Malkin and Klaus Ries. </author> <booktitle> The JanusRTk Switchboard/CallHome System Language Modeling. In Proceedings of LVCSR Hub 5 Workshop, </booktitle> <month> May, </month> <year> 1997. </year>
Reference-contexts: The second effect is that the fallback steps a backoff 3 Korean Spontaneous Scheduling Task; SST described more fully in Sec. 5.1 model takes are different when sequences are part of the context. We have observed that we can improve results on Switchboard (SWB) <ref> [1] </ref>: we can get very good perplexity results if we do not extend contexts that already use a trigram context on the word level. We were not, however, able to improve recognition accuracy using this technique. The application of these and similar models to Japanese is future work. 5.
Reference: [2] <author> Ito, Akinori and Masaki Kohda. </author> <title> Language Modeling by String Pattern N-gram for Japanese Speech Recognition. </title> <booktitle> In ICSLP, </booktitle> <year> 1996. </year>
Reference-contexts: Sec. 4 reviews the algorithm and presents some new developments. In Sec. 5, the speech recognition experiments are described, and we conclude with Sec. 6. 2. RELATED WORK Our process is similar to the procedures described by Lauer [4] and Ito and Kohda <ref> [2] </ref>, but we use the more powerful perplexity evaluation criterion, maximizing the predictive power of the m-gram directly. Ries [11] showed that a variation of this measure outperforms classical measures often used to find phrases.
Reference: [3] <author> Kameda, Masayuki. </author> <title> A Portable & Quick Japanese Parser: </title> <booktitle> QJP. In COLING, </booktitle> <address> Copenhagen, </address> <year> 1996. </year>
Reference-contexts: Bunsetsu in our database (described in Sec. 5.1) averaged 10 phones in length. Systems that have used the bunsetsu as a unit of representation include [8] and <ref> [3] </ref>. kimenakereba naranai Word level This is a level of abstraction based not on syntactic principles but rather intended to maximize the usefulness of the segment to a speech recognizer. It is this unit that our baseline system uses [12].
Reference: [4] <author> Lauer, Mark. </author> <title> Corpus Statistics Meet the Noun Compound: Some Empirical Results. </title> <booktitle> In ACL, </booktitle> <year> 1995. </year>
Reference-contexts: Sec. 4 reviews the algorithm and presents some new developments. In Sec. 5, the speech recognition experiments are described, and we conclude with Sec. 6. 2. RELATED WORK Our process is similar to the procedures described by Lauer <ref> [4] </ref> and Ito and Kohda [2], but we use the more powerful perplexity evaluation criterion, maximizing the predictive power of the m-gram directly. Ries [11] showed that a variation of this measure outperforms classical measures often used to find phrases. <p> The estimation of the final model is also fairly simple: we mark the multi-mora in the corpus and use a standard backoff model. Recently, we have tried a number of generalizations to this algorithm. An assumption we originally shared with Lauer <ref> [4] </ref> was that it is not always appropriate to reduce a pair of sequences to a new sequence; it might be better to replace the pair by one of the components of the pair. This idea is reminiscent of the head principle in linguistics.
Reference: [5] <author> Law, Hubert Hin-Cheung and Chorkin Chan. </author> <title> Ergodic Multigram HMM Integrating Word Segmentation and Class Tagging for Chinese Language Modeling. </title> <booktitle> In ICASSP 1996, Vol.1, </booktitle> <pages> pp. 196-199. </pages>
Reference-contexts: Also for Chinese, Law 1 A mora is a unit basically equivalent to a syllable; in most cases one mora corresponds to one character of the Japanese syllabary (kana) and is never more than three phones long. and Chan <ref> [5] </ref> combine a measure similar to ours with a tagging scheme since the basic dictionary consisted of 80,000 words. 3.
Reference: [6] <author> Masataki, Hirokazu and Yoshinori Sagisaka. </author> <title> Variable-order N-gram Generationi by Word-class Splitting and Consecutive Word Grouping. </title> <booktitle> In ICASSP 1996, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 188-191. </pages>
Reference-contexts: Ries [11] showed that a variation of this measure outperforms classical measures often used to find phrases. Masataki and Sagisaka <ref> [6] </ref> describe work on word grouping, although what he describes is critically different in that they are grouping previously defined words into sequences, not defining new words from scratch. Nobesawa presents a method for segmenting strings in [9] which uses a mutual information criterion to identify meaningful strings.
Reference: [7] <author> Mayfield Tomokiyo, Laura and Klaus Ries. </author> <title> What's in a Word: Learning Base Units in Japanese for Speech Recognition. </title> <booktitle> In Proceedings of the ACL Workshop on Natural Language Learning. </booktitle>
Reference-contexts: Hand-processing requires time and human experts, and we experienced significant inconsistency with manual processing. In this paper we present an unsupervised method for simultaneously segmenting a raw (non-segmented) corpus and learning a lexicon for recognition. We build on work introduced in <ref> [7] </ref>, This research was performed at the University of Karlsruhe and at Carnegie Mellon University, Pittsburgh. The authors were supported by project VerbMobil through the German BMBF. We gratefully acknowledge their support. <p> The task of finding sequences is achieved by searching for the two sequences which when joined optimize the perplexity of the bigram model of the training data. We show that we can build a speech recognizer with our original modeling idea <ref> [7] </ref> and improve the accuracy of recognition as scored at the mora level. In Sec. 2 we describe related work; in Sec. 3 we describe the segmentation problem for Japanese. Sec. 4 reviews the algorithm and presents some new developments.
Reference: [8] <author> Morimoto, Tsuyoshi et al. </author> <title> ATR's Speech Translation System: </title> <booktitle> ASURA. In Eurospeech, </booktitle> <year> 1993. </year>
Reference-contexts: Bunsetsu are long enough for accurate recognition, and capture common patterns, but require a dictionary entry for each possible phrase, causing a vocabulary explosion. Bunsetsu in our database (described in Sec. 5.1) averaged 10 phones in length. Systems that have used the bunsetsu as a unit of representation include <ref> [8] </ref> and [3]. kimenakereba naranai Word level This is a level of abstraction based not on syntactic principles but rather intended to maximize the usefulness of the segment to a speech recognizer. It is this unit that our baseline system uses [12].
Reference: [9] <editor> Nobesawa, Shiho et al. </editor> <title> Segmenting Sentences into Linky Strings using D-bigram statistics. </title> <booktitle> In COLING, </booktitle> <address> Copenhagen, </address> <year> 1996. </year>
Reference-contexts: Masataki and Sagisaka [6] describe work on word grouping, although what he describes is critically different in that they are grouping previously defined words into sequences, not defining new words from scratch. Nobesawa presents a method for segmenting strings in <ref> [9] </ref> which uses a mutual information criterion to identify meaningful strings. He evaluates the correctness of the segmentation by cross-referencing with a dictionary, however, and the approach seems to depend to a certain extent on grammar conventions.
Reference: [10] <author> Palmer, David. </author> <title> A Trainable Rule-based Algorithm for Word Segmentation. </title> <booktitle> In ACL, </booktitle> <address> Madrid, </address> <year> 1997, </year> <pages> pp. 321-328. </pages>
Reference-contexts: This work, though, as well as Nobesawa's, is designed for processing Japanese text, and not speech. The problem is not limited to Japanese. Palmer <ref> [10] </ref> proposes a method for transformation-based segmentation of Chinese and Thai. He evaluates this method by comparing the automatically-produced segmentations against the same text segmented by native speakers.
Reference: [11] <author> Ries, Klaus and Finn Dag Bu, and Alex Waibel. </author> <title> Class Phrase Models for Language Modeling. </title> <booktitle> In ICSLP, </booktitle> <address> Philadelphia, </address> <year> 1996. </year>
Reference-contexts: RELATED WORK Our process is similar to the procedures described by Lauer [4] and Ito and Kohda [2], but we use the more powerful perplexity evaluation criterion, maximizing the predictive power of the m-gram directly. Ries <ref> [11] </ref> showed that a variation of this measure outperforms classical measures often used to find phrases. Masataki and Sagisaka [6] describe work on word grouping, although what he describes is critically different in that they are grouping previously defined words into sequences, not defining new words from scratch. <p> This new multi-mora can be used in future joins of multi-moras. To build more and longer sequences we replace all instances of the best pair with a new multi-mora and continue from the beginning. Ries <ref> [11] </ref> shows that the best pair of mora to join can be determined quickly by doing leaving-one-out estimate of the bigram perplexity of all models resulting from a join of any two multi-mora. This is calculated efficiently from a trigram table of the corpus.
Reference: [12] <author> Schultz, Tanja and Detlef Koll. </author> <title> Spontaneously Spoken Japanese Speech Recognition with Janus-3 In EU-ROSPEECH, </title> <type> Rhodes, </type> <year> 1997. </year>
Reference-contexts: It is this unit that our baseline system uses <ref> [12] </ref>. Words are partially hand-picked to have semantic value, be long enough not to cause confusion, and short enough to capture generalizations. <p> Dialogues have been collected for English (ESST), German (GSST), Spanish (SSST), Korean (KSST) and Japanese (JSST). The entire JSST database consists of 800 dialogues. 5.2. LM-level 5.2.1. Test corpora Six language models were created for the scheduling task JSST <ref> [12] </ref>. The models were drawn from six different segmentations of the same corpus, as described below. The mora segmentation task was completely unambiguous given our transcription conventions. Sequences were found using the compounding algorithm described in Sec. 4. C1: Only romanized mora syllables. <p> System Description The systems used in this experiment were derived from a context-dependent system developed using the Janus Recognition toolkit (JRTK) and described in <ref> [12] </ref>. The speech data was re-labeled retrained using the new dictionary and training corpus, the code-books clustered and a second training iteration performed.
Reference: [13] <author> Teller, Virginia and Eleanor Olds Batchelder. </author> <title> A Probabilistic Algorithm for Segmenting Non-Kanji Japanese Strings. </title> <booktitle> In AAAI pp. </booktitle> <pages> 742-747, </pages> <address> Seattle, </address> <year> 1994. </year>
Reference-contexts: Moreover, a breaking-down approach is less suitable for speech recognition applications than a building-up one because the risk of producing out-of-vocabulary strings is higher. (Conversely, with a building-up approach one risks not being able to reproduce all useful sequences.) Teller and Batchelder <ref> [13] </ref> describe another segmentation algorithm which uses extensively knowledge about the type of a character (hiragana/katakana/kanji, etc). This work, though, as well as Nobesawa's, is designed for processing Japanese text, and not speech. The problem is not limited to Japanese.
References-found: 13

