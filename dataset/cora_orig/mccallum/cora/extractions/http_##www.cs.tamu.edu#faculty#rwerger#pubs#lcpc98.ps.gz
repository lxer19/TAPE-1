URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/lcpc98.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Email: http://www.cs.tamu.edu/faculty/rwerger, fdpatel,rwergerg@cs.tamu.edu  
Title: Principles of Speculative Run-time Parallelization  
Author: Devang Patel and Lawrence Rauchwerger 
Address: College Station, TX 77843-3112  
Affiliation: Dept. of Computer Science Texas A&M University  
Abstract: Current parallelizing compilers cannot identify a significant fraction of parallelizable loops because they have complex or statically insufficiently defined access patterns. We advocate a novel framework for the identification of parallel loops. It speculatively executes a loop as a doall and applies a fully parallel data dependence test to check for any unsatisfied data dependencies; if the test fails, then the loop is re-executed serially. We will present the principles of the design and implementation of a compiler that employs both run-time and static techniques to parallelize dynamic applications. Run-time optimizations always represent a tradeoff between a speculated potential benefit and a certain (sure) overhead that must be paid. We will introduce techniques that take advantage of classic compiler methods to reduce the cost of run-time optimization thus tilting the outcome of speculation in favor of significant performance gains. Experimental results from the PERFECT, SPEC and NCSA Benchmark suites show that these techniques yield speedups not obtainable by any other known method. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> J. of Computational Chemistry, </journal> <volume> 4(6), </volume> <year> 1983. </year>
Reference-contexts: Typical examples are complex simulations such as SPICE [16], DYNA-3D [27], GAUSSIAN [14], CHARMM <ref> [1] </ref>. ? Research supported in part by NSF CAREER Award CCR-9734471 and utilized the SGI systems at the NCSA, University of Illinois under grant#ASC980006N. It has become clear that static (compile-time) analysis must be comple-mented by new methods capable of automatically extracting parallelism at run-time [6].
Reference: 2. <author> Santosh Abraham. </author> <type> Private Communication. </type> <institution> Hewlett Packard Laboratories, </institution> <year> 1994. </year>
Reference: 3. <author> Utpal Banerjee. </author> <title> Loop Parallelization. </title> <publisher> Norwell, </publisher> <address> MA: </address> <publisher> Kluwer Publishers, </publisher> <year> 1994. </year>
Reference-contexts: The most effective vehicle for improving multiprocessor performance has been the restructuring compiler [5, 10, 18, 9]. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., <ref> [3, 19] </ref>) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 4. <author> H. Berryman and J. Saltz. </author> <title> A manual for PARTI runtime primitives. </title> <type> Interim Report 90-13, </type> <institution> ICASE, </institution> <year> 1990. </year>
Reference-contexts: If the test phase is performed before the execution of the loop and has no side effects, i.e., it does not modify the state of the original program (shared) variables then this technique is called inspector/executor <ref> [4] </ref>. Its run-time overhead consists only of the time to execute the inspection phase.
Reference: 5. <author> W. Blume, et. al. </author> <title> Advanced Program Restructuring for High-Performance Computers with Polaris. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 78-82, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: At the limit, full parallelism also allows perfect scalability with the number of processors and can be efficiently used to improve memory latency and load balancing. The most effective vehicle for improving multiprocessor performance has been the restructuring compiler <ref> [5, 10, 18, 9] </ref>. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., [3, 19]) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 6. <author> W. Blume and R. Eigenmann. </author> <title> Performance Analysis of Parallelizing Compilers on the Perfect Benchmarks TM Programs. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: It has become clear that static (compile-time) analysis must be comple-mented by new methods capable of automatically extracting parallelism at run-time <ref> [6] </ref>. Run-time techniques can succeed where static compilation fails because they have access to the input data. For example, input dependent or dynamic data distribution, memory accesses guarded by run-time dependent conditions, and subscript expressions can all be analyzed unambiguously at run-time.
Reference: 7. <author> W. Blume et. al. </author> <title> Effective automatic parallelization with polaris. </title> <journal> Int. J. Paral. Prog., </journal> <month> May </month> <year> 1995. </year>
Reference-contexts: These techniques usually rely on a static (compile time) analysis of the memory access pattern (array subscripts in the case of Fortran programs) and on parallelism enabling transformations like privatization, reduction parallelization, induction variable substitution, etc. <ref> [7] </ref>. When static information is insufficient to safely perform an optimizing transformation the classic compiler emits conservative code. Alternatively it might delay the decision to execution time, when sufficient information becomes available.
Reference: 8. <author> W. Blume et al. </author> <title> Polaris: The next generation in parallelizing compilers,. </title> <booktitle> In Proc. of the 7-th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1994. </year>
Reference-contexts: sequential loop and the remaining processors proceed on the more aggressive parallel path. 6 Current Implementation of Run-time Pass in Polaris Based on the previously presented techniques and the early work described in [22] and [11] we have implemented a first version of run-time parallelization in the Polaris compiler infrastructure <ref> [8] </ref>. Here is a very general overview of this 'run-time pass'. Currently, candidate loops for run-time parallelization are marked by a a special directive in the Fortran source code. Alternatively, all loops that Polaris leaves sequential are run-time parallelized.
Reference: 9. <author> K. Cooper et al. </author> <title> The parascope parallel programming environment. </title> <booktitle> Proc. of IEEE, </booktitle> <pages> pp. 84-89, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: At the limit, full parallelism also allows perfect scalability with the number of processors and can be efficiently used to improve memory latency and load balancing. The most effective vehicle for improving multiprocessor performance has been the restructuring compiler <ref> [5, 10, 18, 9] </ref>. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., [3, 19]) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 10. <author> M. Hall et. al. </author> <title> Maximizing multiprocessor performance with the suif compiler. </title> <journal> IEEE Computer, </journal> <volume> 29(12) </volume> <pages> 84-89, </pages> <month> December </month> <year> 1996. </year>
Reference-contexts: At the limit, full parallelism also allows perfect scalability with the number of processors and can be efficiently used to improve memory latency and load balancing. The most effective vehicle for improving multiprocessor performance has been the restructuring compiler <ref> [5, 10, 18, 9] </ref>. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., [3, 19]) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 11. <author> T. Lawrence. </author> <title> Implementation of run time techniques in the polaris fortran restruc-turer. </title> <type> TR 1501, </type> <institution> CSRD, Univ. of Illinois at Urbana-Champaign, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: critical path is never increased, is to fork two processes: one processor executes the original sequential loop and the remaining processors proceed on the more aggressive parallel path. 6 Current Implementation of Run-time Pass in Polaris Based on the previously presented techniques and the early work described in [22] and <ref> [11] </ref> we have implemented a first version of run-time parallelization in the Polaris compiler infrastructure [8]. Here is a very general overview of this 'run-time pass'. Currently, candidate loops for run-time parallelization are marked by a a special directive in the Fortran source code.
Reference: 12. <author> S. Leung and J. Zahorjan. </author> <title> Improving the performance of runtime parallelization. </title> <booktitle> In 4th PPOPP, </booktitle> <pages> pp. 83-91, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Although more powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [12, 25, 28] </ref>. We will present the principles of the design and implementation of a compiling system that employs run-time and classic techniques in tandem to automatically parallelize irregular, dynamic applications.
Reference: 13. <author> Z. Li. </author> <title> Array privatization for parallel execution of loops. </title> <booktitle> In Proceedings of the 19th International Symposium on Computer Architecture, </booktitle> <pages> pp. 313-322, </pages> <year> 1992. </year>
Reference-contexts: A reduction variable is a variable used in one operation of the form x = x exp, where is an associative and commutative operator and x does not occur in exp or anywhere else in the loop. There are known transformations for implementing reductions in parallel <ref> [26, 15, 13] </ref>. marked if the corresponding element in A is both read and written, and is read first, in any iteration. A post-execution analysis, illustrated in Fig. 1 (c), determines whether there were any cross-iteration dependencies between statements referencing A as follows.
Reference: 14. <author> M. J. Frisch et. al. </author> <title> Gaussian 94. Gaussian, </title> <publisher> Inc., </publisher> <address> Pittsburgh PA, </address> <year> 1995. </year>
Reference-contexts: Typical examples are complex simulations such as SPICE [16], DYNA-3D [27], GAUSSIAN <ref> [14] </ref>, CHARMM [1]. ? Research supported in part by NSF CAREER Award CCR-9734471 and utilized the SGI systems at the NCSA, University of Illinois under grant#ASC980006N. It has become clear that static (compile-time) analysis must be comple-mented by new methods capable of automatically extracting parallelism at run-time [6].
Reference: 15. <author> D. E. Maydan, S. P. Amarasinghe, and M. S. Lam. </author> <title> Data dependence and data-flow analysis of arrays. </title> <booktitle> In Proc. 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <month> August </month> <year> 1992. </year>
Reference-contexts: A reduction variable is a variable used in one operation of the form x = x exp, where is an associative and commutative operator and x does not occur in exp or anywhere else in the loop. There are known transformations for implementing reductions in parallel <ref> [26, 15, 13] </ref>. marked if the corresponding element in A is both read and written, and is read first, in any iteration. A post-execution analysis, illustrated in Fig. 1 (c), determines whether there were any cross-iteration dependencies between statements referencing A as follows.
Reference: 16. <author> L. Nagel. </author> <title> SPICE2: A Computer Program to Simulate Semiconductor Circuits. </title> <type> PhD thesis, </type> <institution> University of California, </institution> <month> May </month> <year> 1975. </year>
Reference-contexts: Typical examples are complex simulations such as SPICE <ref> [16] </ref>, DYNA-3D [27], GAUSSIAN [14], CHARMM [1]. ? Research supported in part by NSF CAREER Award CCR-9734471 and utilized the SGI systems at the NCSA, University of Illinois under grant#ASC980006N.
Reference: 17. <author> Y. Paek, J. Hoeflinger, and D. Padua. </author> <title> Simplification of Array Access Patterns for Compiler Optimizat ions. </title> <booktitle> In Proc. of the SIGPLAN 1998 Conf. on Programming Language Design and Implementation, </booktitle> <address> Montreal, Canada, </address> <month> June </month> <year> 1998. </year>
Reference-contexts: While the asymptotic complexity increases the problem size can decrease dramatically (depending on the average length of intervals). In our implementation the compile time detection of these types of semi-irregular access patterns is obtained using recently developed array region analysis techniques <ref> [17] </ref>. It important to mention here the possibility of applying the run-time test somewhat conservatively by always marking whole intervals even if only some of the addresses within the interval have actually been referenced during loop execution.
Reference: 18. <author> C. Polychronopoulos et. al. </author> <title> Parafrase-2: A New Generation Parallelizing Compiler. </title> <booktitle> Proc. of 1989 Int. Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, II:39-48, </address> <month> August </month> <year> 1989. </year>
Reference-contexts: At the limit, full parallelism also allows perfect scalability with the number of processors and can be efficiently used to improve memory latency and load balancing. The most effective vehicle for improving multiprocessor performance has been the restructuring compiler <ref> [5, 10, 18, 9] </ref>. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., [3, 19]) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 19. <author> W. Pugh. </author> <title> A practical algorithm for exact array dependence analysis. </title> <journal> Comm. of the ACM, </journal> <volume> 35(8) </volume> <pages> 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: The most effective vehicle for improving multiprocessor performance has been the restructuring compiler [5, 10, 18, 9]. Compilers have incorporated sophisticated data dependence analysis techniques (e.g., <ref> [3, 19] </ref>) to detect intrinsic parallelism in codes and transform them for parallel execution.
Reference: 20. <author> L. Rauchwerger, N. Amato, and D. Padua. </author> <title> A scalable method for run-time loop parallelization. </title> <journal> IJPP, </journal> <volume> 26(6) </volume> <pages> 537-576, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We now briefly describe a simplified version of the speculative LRPD test (complete details can be found in <ref> [20, 22] </ref>). The LRPD Test. The LRPD test speculatively executes a loop in parallel and tests subsequently if any data dependences could have occurred. If the test fails, the loop is re-executed sequentially.
Reference: 21. <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proc. of the 1994 International Conf. on Supercomputing, </booktitle> <pages> pp. 33-43, </pages> <month> July </month> <year> 1994. </year>
Reference: 22. <author> L. Rauchwerger. </author> <title> Run-time parallelization: A framework for parallel computation. </title> <type> TR UIUCDCS-R-95-1926, </type> <institution> Dept. of Computer Science, University of Illinois, Urbana, IL, </institution> <month> September </month> <year> 1995. </year>
Reference-contexts: We now briefly describe a simplified version of the speculative LRPD test (complete details can be found in <ref> [20, 22] </ref>). The LRPD Test. The LRPD test speculatively executes a loop in parallel and tests subsequently if any data dependences could have occurred. If the test fails, the loop is re-executed sequentially. <p> frequently used variants of the LRPD 2 any returns the "OR" of its vector operand's elements, i.e., any (v (1 : n)) = (v (1) _ v (2) _ : : : _ v (n)). test that we have developed and elaborate on those that have not been pre-sented elsewhere <ref> [24, 22] </ref>. Further refinements and related issues (such as choice of marking data structures) are discussed in Section 5. Processor-wise LRPD test for testing cross-processor instead of cross-iteration dependences, qualifying more parallel loops with less overhead. <p> the program's critical path is never increased, is to fork two processes: one processor executes the original sequential loop and the remaining processors proceed on the more aggressive parallel path. 6 Current Implementation of Run-time Pass in Polaris Based on the previously presented techniques and the early work described in <ref> [22] </ref> and [11] we have implemented a first version of run-time parallelization in the Polaris compiler infrastructure [8]. Here is a very general overview of this 'run-time pass'. Currently, candidate loops for run-time parallelization are marked by a a special directive in the Fortran source code.
Reference: 23. <author> L. Rauchwerger and D. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <booktitle> In Proc. of 9th International Parallel Processing Symposium, </booktitle> <month> April </month> <year> 1995. </year>
Reference: 24. <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Paral-lelization of Loops with Privatization and Reduction Parallelization. </title> <booktitle> In Proc. of the SIGPLAN 1995 Conf. on Programming Language Design and Implementation, </booktitle> <address> La Jolla, CA, </address> <pages> pp. 218-232, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: frequently used variants of the LRPD 2 any returns the "OR" of its vector operand's elements, i.e., any (v (1 : n)) = (v (1) _ v (2) _ : : : _ v (n)). test that we have developed and elaborate on those that have not been pre-sented elsewhere <ref> [24, 22] </ref>. Further refinements and related issues (such as choice of marking data structures) are discussed in Section 5. Processor-wise LRPD test for testing cross-processor instead of cross-iteration dependences, qualifying more parallel loops with less overhead. <p> The value shown is still correct because the hand-parallel version has been statically scheduled and there are no cross-processor dependences. 8 Conclusion While the general LRPD algorithm has been extensively presented in <ref> [24] </ref> and briefly shown here for clarity of the presentation, this paper emphasizes the practical aspects of its application and integration in a compiler. In essence we advocate a very tight connection between static information obtained through classical compiler methods and the run-time system.
Reference: 25. <author> J. Saltz, R. Mirchandaney, and K. Crowley. </author> <title> Run-time parallelization and scheduling of loops. </title> <journal> IEEE Trans. Comput., </journal> <volume> 40(5), </volume> <month> May </month> <year> 1991. </year>
Reference-contexts: Although more powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [12, 25, 28] </ref>. We will present the principles of the design and implementation of a compiling system that employs run-time and classic techniques in tandem to automatically parallelize irregular, dynamic applications. <p> Schedule reuse, inspector decoupling, two-process solution. If the speculatively executed loop is re-executed during the program with the same data access pattern, then the results of the first LRPD test can be reused (this is an instance of schedule reuse <ref> [25] </ref>). If the defining parameters of an inspector loop are available well before the loop will be executed, then the test code can be executed early, perhaps during a portion of the program that does not have enough (or any) parallelism.
Reference: 26. <author> P. Tu and D. Padua. </author> <title> Array privatization for shared and distributed memory machines. </title> <booktitle> In Proc. 2nd Workshop on Languages, Compilers, and Run-Time Environments for Distributed Memory Machines, </booktitle> <month> September </month> <year> 1992. </year>
Reference-contexts: A reduction variable is a variable used in one operation of the form x = x exp, where is an associative and commutative operator and x does not occur in exp or anywhere else in the loop. There are known transformations for implementing reductions in parallel <ref> [26, 15, 13] </ref>. marked if the corresponding element in A is both read and written, and is read first, in any iteration. A post-execution analysis, illustrated in Fig. 1 (c), determines whether there were any cross-iteration dependencies between statements referencing A as follows.
Reference: 27. <author> R. Whirley and B. Engelmann. DYNA3D: </author> <title> A Nonlinear, Explicit, Three-Dimensional Finite Element Code For Solid and Structural Mechanics. </title> <institution> Lawrence Livermore National Laboratory, </institution> <month> Nov., </month> <year> 1993. </year>
Reference-contexts: Typical examples are complex simulations such as SPICE [16], DYNA-3D <ref> [27] </ref>, GAUSSIAN [14], CHARMM [1]. ? Research supported in part by NSF CAREER Award CCR-9734471 and utilized the SGI systems at the NCSA, University of Illinois under grant#ASC980006N. It has become clear that static (compile-time) analysis must be comple-mented by new methods capable of automatically extracting parallelism at run-time [6].
Reference: 28. <author> C. Zhu and P. C. Yew. </author> <title> A scheme to enforce data dependence on large multiprocessor systems. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 13(6) </volume> <pages> 726-739, </pages> <month> June </month> <year> 1987. </year> <title> (a) (b) Fig. 2. Loop TRACK NLFITL DO 300: (a) Timing of test phases, (b) Speedup (a) (b) Fig. 3. Major Loops in TFFT2: (a) Timing of test phases, (b) Speedup (a) (b) Fig. 4. Loop P3M PP DO 100: (a) Timing of test phases, (b) Speedup </title>
Reference-contexts: Although more powerful analysis techniques could remove this last limitation when the index arrays are computed using only statically-known values, nothing can be done at compile-time when the index arrays are a function of the input data <ref> [12, 25, 28] </ref>. We will present the principles of the design and implementation of a compiling system that employs run-time and classic techniques in tandem to automatically parallelize irregular, dynamic applications.
References-found: 28

