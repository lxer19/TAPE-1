URL: ftp://ftp.cs.indiana.edu/pub/techreports/TR387.ps.Z
Refering-URL: http://www.cs.indiana.edu/trindex.html
Root-URL: 
Title: Backtracking and Probing  
Author: Paul Walton Purdom, Jr., 
Date: 92-03942.  
Note: Partial support provided by NSF Grant CCR  
Address: G. Neil Haven, Indiana University  
Affiliation: Indiana University  
Abstract: We analyze two algorithms for solving constraint satisfaction problems. One of these algorithms, Probe Order Backtracking, has an average running time much faster than any previously analyzed algorithm for problems where solutions are common. Probe Order Backtracking uses a probing assignment (a preselected test assignment to unset variables) to help guide the search for a solution to a constraint satisfaction problem. If the problem is not satisfied when the unset variables are temporarily set to the probing assignment, the algorithm selects one of the relations that the probing assignment fails to satisfy and selects an unset variable from that relation. Then at each backtracking step it generates subproblems by setting the selected variable each possible way. It simplifies each subproblem, and tries the same technique on them. For random problems with v variables, t clauses, and probability p that a literal appears in a clause, the average time for Probe Order Backtracking is no more than v n when p (ln t)=v plus lower order terms. The best previous result was p (ln t)=v. When the algorithm is combined with an algorithm of Franco that makes selective use of resolution, the average time for solving random problems is polynomial for all values of p when t O(n 1=3 (v= ln v) 2=3 ). The best previous result was t O(n 1=3 (v= ln v) 1=6 ). Probe Order Backtracking also runs in polynomial average time when p 1=v, compared with the best previous result of p 1=(2v). With Probe Order Backtracking the range of p that leads to more than polynomial time is much smaller than that for previously analyzed algorithms. p
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Cynthia A. Brown and Paul W. Purdom, </author> <title> An Average Time Analysis of Backtracking, </title> <note> SIAM J. Comput. 10 (1981) pp 583-593. </note>
Reference-contexts: Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length <ref> [1, 2, 14, 20] </ref>. This second probability model generates problems that are more difficult to satisfy but perhaps more like the problems encountered in practice.
Reference: 2. <author> Khaled Bugrara and Cynthia Brown, </author> <title> The Average Case Analysis of Some Satisfiability Model Problems, </title> <note> Information Sciences 40 (1986) pp 21-38. </note>
Reference-contexts: Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length <ref> [1, 2, 14, 20] </ref>. This second probability model generates problems that are more difficult to satisfy but perhaps more like the problems encountered in practice.
Reference: 3. <author> Khaled Bugrara and Paul Purdom, </author> <title> Average Time Analysis of Clause Order Backtracking, </title> <note> SIAM J. Comput. 23 (1993) pp 303-317. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. <p> Of the curves that were computed to t = 500, the uppermost is Goldberg's simplified version of the pure literal rule [9, 10], next is Clause Order Backtracking <ref> [3] </ref>, and lowermost is backtracking combined with Goldberg's version of the pure literal rule [19]. Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2. <p> However, Simple Backtracking is unfocused in its variable selection. So long as a problem does not have an empty clause, Simple Backtracking always proceeds by selecting the next splitting variable from a fixed ordering. The Clause Order Backtracking Algorithm <ref> [3] </ref> improves over Simple Backtracking by focusing on the variables in one clause of the problem at a time. This method of searching has the advantage that it performs splitting on just those variables that actually appear in a problem. <p> A problem with i literals has probability p i (1 p) 2tvi . Multiplying the node counts for each i by the probability gives a polynomial in p with integer coefficients <ref> [3] </ref>. We used Maple to solve each recurrence (28, 43, 47, 50, 53) algebraically and verified that the polynomials from the recurrences were identical with the polynomials generated from the corresponding node counts. <p> Details: For eq. <ref> (93) </ref> we have ln y = ln t ln ln v ln (n 1) ln 2 fi 1 t ; ln ( ln y) = ln ln t fi ln ln v ln t fi ln v so, when t increases more rapidly than ln v pv ln t + 2 <p> A better result can be obtained by observing that the average running time for Probe Order Backtracking is no larger than that of Simple Backtracking. (The proof that the average running time of Clause Order Backtracking is no larger than that of Simple Backtracking <ref> [3, Theorem 1] </ref> also applies to Probe Order Backtracking.) We require that A.18 from [22], the bound for Simple Backtracking, be no more than v n .
Reference: 4. <author> Khaled Bugrara, Youfang Pan, and Paul Purdom, </author> <title> Exponential Average Time for the Pure Literal Rule, </title> <note> SIAM J. Comput. 18 (1988) pp 409-418. 44 </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20].
Reference: 5. <author> Michael Buro and Hans Kleine Buning, </author> <title> Report on a SAT Competition, </title> <note> Bulletin of the European Asso ciation for Theoretical Computer Science 49 (1993) pp 143-151. </note>
Reference-contexts: Various greedy approaches where variables are set to satisfy as many clauses as possible should be considered (see [13, 24]). This is particularly important near the upper boundary (1). 3. Probe with several sequences at one time. See <ref> [5, p 151] </ref> for an algorithm that used two sequences. This is helpful along the upper boundary. 4. Carefully select which variable to set. The analysis suggests that this is particularly important along the lower boundary. <p> Variables that appear in lots of relations are more important than those that appear in a few relations. Apparently when the relations are clauses it is helpful to consider the number of clauses containing a particular variable positively and the number containing it negatively <ref> [5] </ref>. It appears that variable selection was a major factor in determining the order of placement of winning entries in a recent SAT competition [5]. 5. <p> Apparently when the relations are clauses it is helpful to consider the number of clauses containing a particular variable positively and the number containing it negatively <ref> [5] </ref>. It appears that variable selection was a major factor in determining the order of placement of winning entries in a recent SAT competition [5]. 5. Use resolution when it does not increase the problem size [8]. 6 Algorithm Statement The precise form of Probe Order Backtracking that is analyzed along with the rules for charging time is given below.
Reference: 6. <author> John Franco, </author> <title> On the Probabilistic Performance of Algorithms for the Satisfiability Problem, </title> <note> Information Processing Letters 18 (1986) pp 103-106. </note>
Reference-contexts: It appears that people who are good at solving puzzles use related ideas all the time. Franco observed that two extremely simple algorithms could quickly solve most problems outside of a small range of p <ref> [6] </ref>. His algorithm for the region of high p did a single probe and gave up if no solution was found. His algorithm for the region of low p looked for an empty clause and gave up if there was none. <p> Simple uses of probing did not seem to lead to a good average time. Probe Order Backtracking was discovered while considering Franco's results <ref> [6] </ref> and considering the measurements of Sosic and Gu [24] for algorithms that concentrate on adjusting values until a solution is found. Both of those algorithms have difficulty with problems that have no solution. <p> We require that the bound on the number of nodes be no more than v n . That is, v n 1 + 2v 1 + (1 p) v p 2 v (v 1) 2 ; <ref> (86) </ref> v n1 1 2 1 p t 9.1 Small t Solving for t in bound (87) gives t lnf1 + [p 2 v (v 1) 1 + p](1 p) v =2g Details: From eq. (87) ln v n1 1 2 1 p Also ln v n1 1 v n :
Reference: 7. <author> John Franco, </author> <title> On the Occurrence of Null Clauses in Random Instances of Satisfiability, </title> <institution> Indiana Univer sity Computer Science Tech. </institution> <note> Report 291 (1989). </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20].
Reference: 8. <author> John Franco, </author> <title> Elimination of Infrequent Variables Improves Average Case Performance of Satisfiability Algorithms, </title> <note> SIAM J. Comput. 20 (1991) pp 1119-1127. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. <p> The ratio of the upper bound to the second lower bound (3) is ln v plus lower order terms. Previously, for small t the best algorithm was a combination of Franco's limited resolution algorithm <ref> [8] </ref> for small p and Iwama's inclusion-exclusion algorithm [12] for large p. When p is unknown, the two algorithms can be run in parallel and stopped as soon as an answer is found. <p> It appears that variable selection was a major factor in determining the order of placement of winning entries in a recent SAT competition [5]. 5. Use resolution when it does not increase the problem size <ref> [8] </ref>. 6 Algorithm Statement The precise form of Probe Order Backtracking that is analyzed along with the rules for charging time is given below. This version of the algorithm is specialized to work on satisfiability problems presented in conjunctive normal form. <p> That is, the leading terms in eq. (94) are bigger than those in eq. (101) only by the amount ln v. When t=v is not large, the relative distance between the two curves increases. 9.6 Intersection with Franco's Analysis Franco gives an algorithm, <ref> [8] </ref>, which makes selective use of resolution. This algorithm has the fastest proven average time for small t so long as p is not too large. <p> algorithm with Iwama's algorithm [12] gives an algorithm that is fast for all p when t O (n 1=3 (v= ln v) 1=6 ): (102) The running time for Franco's algorithm is no more than 3 + v + e te 2p (1+p)v +(ln 2)[8 (pt) 3 v+1] ; (103) <ref> [8, pp 1123-1124] </ref>.
Reference: 9. <author> Allen Goldberg, </author> <title> Average Case Complexity of the Satisfiability Problem, </title> <booktitle> Proc. Fourth Workshop on Automated Deduction (1979) pp 1-6. </booktitle>
Reference-contexts: The vertical axis is the average number of nodes from 1 to over 10 15 with tick marks for each power of 10. Of the curves that were computed to t = 500, the uppermost is Goldberg's simplified version of the pure literal rule <ref> [9, 10] </ref>, next is Clause Order Backtracking [3], and lowermost is backtracking combined with Goldberg's version of the pure literal rule [19]. Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2. <p> One index upper limit. Thus, the number of nodes is bounded by 1 + 2v 1 + (1 p) v p 2 v (v 1) 2 : <ref> (79) </ref> If we take x (v) = 1 (1 p) v (80) p 10 10 10 v = 50 t Fig. 6a. <p> The a that minimizes 1 + a 1 1 + (1 p) v 1 + p 1 + (a 1)pv + 2 is given by eq. (85). 9 Asymptotics Since eqs. (84, 85) are more complex than eq. <ref> (79) </ref> the asymptotic analysis is based on eq. (79). We require that the bound on the number of nodes be no more than v n . <p> The a that minimizes 1 + a 1 1 + (1 p) v 1 + p 1 + (a 1)pv + 2 is given by eq. (85). 9 Asymptotics Since eqs. (84, 85) are more complex than eq. <ref> (79) </ref> the asymptotic analysis is based on eq. (79). We require that the bound on the number of nodes be no more than v n .
Reference: 10. <author> Allen Goldberg, Paul Purdom, and Cynthia Brown, </author> <title> Average Time Analysis of Simplified Davis-Putnum Procedures, </title> <note> Information Processing Letters 15 (1982) pp 72-75. Printer errors corrected in 16 (1983) p 213. </note>
Reference-contexts: The vertical axis is the average number of nodes from 1 to over 10 15 with tick marks for each power of 10. Of the curves that were computed to t = 500, the uppermost is Goldberg's simplified version of the pure literal rule <ref> [9, 10] </ref>, next is Clause Order Backtracking [3], and lowermost is backtracking combined with Goldberg's version of the pure literal rule [19]. Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2.
Reference: 11. <author> G. Neil Haven, </author> <title> unpublished analysis. </title>
Reference-contexts: Probe Order Backtracking. t is the Full Pure Literal Rule [18], next is the Full Pure Literal Rule modified to ignore tautological clauses [18], next is Probe Order Backtracking [this paper], and lowest is Probe Order Backtracking combined with Goldberg's version of the Pure Literal Rule <ref> [11] </ref>. These curves show that there are huge differences in the average number of nodes generated by the various satisfiability algorithms. The average time for the Probe Order Backtracking-type algorithms is by far the best among the analyzed algorithms when t=v is not small.
Reference: 12. <author> Kazuo Iwama, </author> <title> CNF Satisfiability Test by Counting and Polynomial Average Time, </title> <note> SIAM J. Comput. 18 (1989) pp 385-391. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. <p> The bound for large p, bound (1), is much better than that for any previously analyzed algorithm. The best previous result was p ln t ln n (6) for Iwama's inclusion-exclusion algorithm <ref> [12] </ref>. In the region between bounds (1) and (6) Probe Order Backtracking is the fastest algorithm with proven results on its running time. Algorithms that repeatedly adjust variable settings to satisfy as many clauses as possible [24] are even faster on many problems. <p> The ratio of the upper bound to the second lower bound (3) is ln v plus lower order terms. Previously, for small t the best algorithm was a combination of Franco's limited resolution algorithm [8] for small p and Iwama's inclusion-exclusion algorithm <ref> [12] </ref> for large p. When p is unknown, the two algorithms can be run in parallel and stopped as soon as an answer is found. Each algorithm generates no more than than v n nodes (regardless of p) when t = O (n 1=3 (v= ln v) 1=6 ). <p> This algorithm has the fastest proven average time for small t so long as p is not too large. Combining Franco's algorithm with Iwama's algorithm <ref> [12] </ref> gives an algorithm that is fast for all p when t O (n 1=3 (v= ln v) 1=6 ): (102) The running time for Franco's algorithm is no more than 3 + v + e te 2p (1+p)v +(ln 2)[8 (pt) 3 v+1] ; (103) [8, pp 1123-1124].
Reference: 13. <author> K. J. Lieberherr and E. Specker, </author> <title> Complexity of Partial Satisfaction, </title> <journal> J. </journal> <note> ACM 28 (1981) pp 411-421. </note>
Reference-contexts: Carefully choose the probing sequence instead of just setting all variables to a fixed value. Various greedy approaches where variables are set to satisfy as many clauses as possible should be considered (see <ref> [13, 24] </ref>). This is particularly important near the upper boundary (1). 3. Probe with several sequences at one time. See [5, p 151] for an algorithm that used two sequences. This is helpful along the upper boundary. 4. Carefully select which variable to set.
Reference: 14. <author> Henri M. Mejean, Henri Morel, Gerard Reynaud, </author> <title> "A Variational Method for Analysing Unit Clause Search", </title> <note> submitted for publication. </note>
Reference-contexts: Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length <ref> [1, 2, 14, 20] </ref>. This second probability model generates problems that are more difficult to satisfy but perhaps more like the problems encountered in practice.
Reference: 15. <author> Allen Newell and H. A. Simon, </author> <title> "GPS, A Program that Simulates Human Thought", </title> <note> Computers and Thought (1963) pp 279-296, </note> <author> Edward A. </author> <title> Feigenbaum and Julian Feldman eds. </title>
Reference-contexts: This is helpful when designing practical algorithms. The basic idea behind probing is old. The idea resembles that used by Newell and Simon in GPS <ref> [15] </ref>. Just as their program concentrates on differences between its current state and its goal state, Probe Order Backtracking focuses on a set of troublesome relations that are standing in the way of finding a solution.
Reference: 16. <author> Paul W. Purdom, </author> <title> Search Rearrangement Backtracking and Polynomial Average Time, </title> <note> Artificial Intelli gence 21 (1983) pp 117-133. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20].
Reference: 17. <author> Paul W. Purdom, </author> <title> A Survey of Average Time Analyses of Satisfiability Algorithms, </title> <journal> Journal of Informa tion Processing, </journal> <note> 13 (1990) pp 449-455. An earlier version appeared as Random Satisfiability Problems, </note> <editor> Proc. </editor> <booktitle> of the International Workshop on Discrete Algorithms and Complexity, The Institute of Electronics, Information and Communication Engineers, </booktitle> <address> Tokyo (1989) pp 253-259. </address>
Reference-contexts: For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model [3, 4, 7, 8, 12, 16, 18, 21, 22]. Most of these analyses and a few unpublished ones are summarized in <ref> [17] </ref>. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. This second probability model generates problems that are more difficult to satisfy but perhaps more like the problems encountered in practice. <p> The contours do not always extend to the right edge of the figure due to difficulties with floating point overflow (see Fig. 1). This plot show shows that Backtracking with Probing provides no significant improvement over previously analyzed algorithms <ref> [17] </ref>. this case the upper and lower contours join to form horseshoe shaped curves. Note that the region of hard problems is considerably smaller for Probe Order Backtracking than for Backtracking with Probing. Except 2 p 10 10 10 v = 50 t Fig. 1.
Reference: 18. <author> Paul W. Purdom, </author> <title> Average Time for the Full Pure Literal Rule, </title> <journal> Information Sciences, </journal> <note> to appear. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. <p> Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2. Probe Order Backtracking. t is the Full Pure Literal Rule <ref> [18] </ref>, next is the Full Pure Literal Rule modified to ignore tautological clauses [18], next is Probe Order Backtracking [this paper], and lowest is Probe Order Backtracking combined with Goldberg's version of the Pure Literal Rule [11]. <p> Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2. Probe Order Backtracking. t is the Full Pure Literal Rule <ref> [18] </ref>, next is the Full Pure Literal Rule modified to ignore tautological clauses [18], next is Probe Order Backtracking [this paper], and lowest is Probe Order Backtracking combined with Goldberg's version of the Pure Literal Rule [11]. These curves show that there are huge differences in the average number of nodes generated by the various satisfiability algorithms.
Reference: 19. <author> Paul W. Purdom, </author> <title> unpublished analysis. </title>
Reference-contexts: Of the curves that were computed to t = 500, the uppermost is Goldberg's simplified version of the pure literal rule [9, 10], next is Clause Order Backtracking [3], and lowermost is backtracking combined with Goldberg's version of the pure literal rule <ref> [19] </ref>. Of the curves that stop short of t = 500 the highest for large 3 p 10 10 10 v = 50 t Fig. 2.
Reference: 20. <author> Paul W. Purdom and Cynthia A. Brown, </author> <title> An Analysis of Backtracking with Search Rearrangement, </title> <note> SIAM J. Comput. 12 (1983) pp 717-733. </note>
Reference-contexts: Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length <ref> [1, 2, 14, 20] </ref>. This second probability model generates problems that are more difficult to satisfy but perhaps more like the problems encountered in practice.
Reference: 21. <author> Paul W. Purdom and Cynthia A. Brown, </author> <title> The Pure Literal Rule and Polynomial Average Time, </title> <note> SIAM J. Comput. 14 (1985) pp 943-953. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20].
Reference: 22. <author> Paul W. Purdom and Cynthia A. Brown, </author> <title> Polynomial-Average-Time Satisfiability Problems, </title> <note> Information Sciences 41 (1987) pp 23-42. </note>
Reference-contexts: We use v for the number of variables, and t for the number of clauses. For the asymptotic analysis, both p and t are functions of v. Many algorithms have been analyzed with this random clause length model <ref> [3, 4, 7, 8, 12, 16, 18, 21, 22] </ref>. Most of these analyses and a few unpublished ones are summarized in [17]. A few algorithms have also been analyzed with the fixed length model, where random problems consist of random clauses of fixed length [1, 2, 14, 20]. <p> that the average running time for Probe Order Backtracking is no larger than that of Simple Backtracking. (The proof that the average running time of Clause Order Backtracking is no larger than that of Simple Backtracking [3, Theorem 1] also applies to Probe Order Backtracking.) We require that A.18 from <ref> [22] </ref>, the bound for Simple Backtracking, be no more than v n .
Reference: 23. <author> Paul Walton Purdom Jr. and G. Neil Haven, </author> <title> Backtracking and Probing, </title> <institution> Indiana University Computer Science Technical Report No. </institution> <month> 387 </month> <year> (1993). </year>
Reference-contexts: Since the Backtracking with Probing Algorithm does not offer any significant improvement over other previously analyzed algorithms, we restrict our asymptotic analysis to the Probe Order Backtracking Algorithm. The reader who wants more detailed analyses should refer to <ref> [23] </ref>.
Reference: 24. <author> Rok Sosic and Jun Gu, </author> <title> Fast Search Algorithms for the N-Queens Problem, </title> <journal> IEEE Trans. on Systems, Man, </journal> <note> and Cybernetics 21 (1991) pp 1572-1576. 45 </note>
Reference-contexts: In the region between bounds (1) and (6) Probe Order Backtracking is the fastest algorithm with proven results on its running time. Algorithms that repeatedly adjust variable settings to satisfy as many clauses as possible <ref> [24] </ref> are even faster on many problems. Those algorithms, however, have difficulty with problems which have no solution. They have been difficult to correctly analyze, and it is not clear at this time what their average running time is. <p> Simple uses of probing did not seem to lead to a good average time. Probe Order Backtracking was discovered while considering Franco's results [6] and considering the measurements of Sosic and Gu <ref> [24] </ref> for algorithms that concentrate on adjusting values until a solution is found. Both of those algorithms have difficulty with problems that have no solution. Simple Backtracking improves over plain search by noticing when a problem has no solution due to the presence of an empty clause. <p> Carefully choose the probing sequence instead of just setting all variables to a fixed value. Various greedy approaches where variables are set to satisfy as many clauses as possible should be considered (see <ref> [13, 24] </ref>). This is particularly important near the upper boundary (1). 3. Probe with several sequences at one time. See [5, p 151] for an algorithm that used two sequences. This is helpful along the upper boundary. 4. Carefully select which variable to set.
References-found: 24

