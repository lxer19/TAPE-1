URL: http://www.eecs.umich.edu/~btdavis/papers/isca99.ps
Refering-URL: http://www.eecs.umich.edu/~btdavis/papers/
Root-URL: http://www.eecs.umich.edu
Abstract: Copyright 1999 IEEE. Published in the Proceedings of the 26th International Symposium on Computer Architecture, May 2-4, 1999, in Atlanta GA, USA. Personal use of this material is per-mitted. However, permission to repr int/republish this mater ial for advertising or promotional pur poses or for creating ne w collective works for resale or redistr ibution to ser vers or lists , or to reuse any copyrighted component of this work in other works, must be obtained from the IEEE. Contact: Manager, Copyrights and Permissions / IEEE Service Center / 445 Hoes Lane / P.O. Box 1331 / Piscataway, NJ 08855-1331, USA. Telephone: + Intl. 908-562-3966. ABSTRACT In r esponse to the gr owing gap between memory access time and processor speed, DRAM manufactur ers have cr eated se veral ne w DRAM ar chitectures. This paper pr esents a simulation-based performance study of a representative group, each evaluated in a small system organization. These smallsystem or ganizations correspond to workstation-class computer s and use on the or der of 10 DRAM chips. The study co vers Fast Page Mode, Extended Data Out, Synchronous, Enhanced Sync hronous, Synchronous Link, Ramb us, and Direct Ramb us designs. Our simulations r eveal se veral things: (a) current advanced DRAM tec hnologies ar e attac king the memory bandwidth problem but not the latency pr oblem; (b) b us tr ansmis-sion speed will soon become a primary factor limiting memory-system performance; (c) the post-L2 addr ess str eam still contains significant locality, though it varies from application to application; and (d) as we mo ve to wider b uses, row access time becomes muc h more pr ominent, making it important to in vestigate tec hniques to exploit the available locality to decrease access time. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. A. Barroso, et al. </author> <title> Memory system characterization of commercial workloads. </title> <booktitle> Proc. </booktitle> <address> ISCA-25, </address> <month> June </month> <year> 1998, </year> <pages> pp. 314. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [2] <author> D. Bhandarkar and J. Ding. </author> <title> Performance characterization of the Pen-tium Pro processor. </title> <booktitle> Proc. </booktitle> <address> HPCA-3, </address> <month> February </month> <year> 1997, </year> <pages> pp. 288297. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [3] <author> N. Bowman, et al. </author> <title> Evaluation of existing architectures in IRAM systems. </title> <booktitle> Workshop on Mixing Logic and DRAM, </booktitle> <month> June </month> <year> 1997. </year>
Reference-contexts: Current memory busses are adequate for small systems but are likely inadequate for large ones. Embedded DRAM [5, 19, 37] is not a near term solution, as its performance is poor on high-end workloads <ref> [3] </ref>. Faster buses are more likely solutionswit-ness the elimination of the slo w intermediate memory b us in future systems [12].
Reference: [4] <author> D. Burger and T. M. Austin. </author> <title> The SimpleScalar tool set, version 2.0. </title> <type> Tech. Rep. </type> <institution> CS-1342, University of Wisconsin-Madison, </institution> <month> June </month> <year> 1997. </year>
Reference-contexts: While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. To obtain accurate memory-request timing for an aggressi ve out-of-order processor, we integrate our code into the SimpleScalar tool set <ref> [4] </ref>. This paper presents a baseline study of a smallsystem DRAM organization: these are systems with only a handful of DRAM chips (0.11GB). We do not consider lar ge-system DRAM or ganizations with man y gig abytes of storage that are highly interlea ved. <p> because DRDRAM partitions the bus into different components, three transactions can simultaneously utilize the different portions of the DRDRAM interface. 4 EXPERIMENTAL METHODOLOGY For accurate timing of memory requests in a dynamically reordered instruction stream, we integrated our code into SimpleScalar, an execution-driven simulator of an aggressi ve out-of-order processor <ref> [4] </ref>. We calculate the DRAM access time, much of which is o verlapped with instruction e xecution.
Reference: [5] <author> D. Burger, et al. </author> <title> Memory bandwidth limitations of future microprocessors. </title> <booktitle> Proc. </booktitle> <address> ISCA-23, </address> <month> May </month> <year> 1996, </year> <pages> pp. 7889. </pages>
Reference-contexts: The study asks and answers the following questions: What is the effect of improvements in DRAM technology on the memory latency and bandwidth problems? Contemporary techniques for improving processor performance and tolerating memory latency are exacerbating the memory bandwidth problem <ref> [5] </ref>. Our results show that current DRAM architectures are attacking exactly this problem: the most recent technologies (SDRAM, ESDRAM, and Rambus) have reduced the stall time due to limited bandwidth by a factor of three compared to earlier DRAM architectures. However, the memory-latency component of overhead has not improved. <p> However, the latency benefits are limited by bus and DRAM speeds: to get further improvements, one must run the DRAM core and b us at faster speeds. Current memory busses are adequate for small systems but are likely inadequate for large ones. Embedded DRAM <ref> [5, 19, 37] </ref> is not a near term solution, as its performance is poor on high-end workloads [3]. Faster buses are more likely solutionswit-ness the elimination of the slo w intermediate memory b us in future systems [12]. <p> 128-bit memory b us, an eight-way superscalar out-of-order CPU, lockup-free caches, and a smallsystem DRAM organization with ~10 DRAM chips. 2 RELATED WORK Burger, Goodman, and Kagi quantified the effect on memory behavior of high-performance latency-reducing or latency-tolerating techniques such as lockup-free caches, out-of-order e xecution, prefetching, speculative loads, etc. <ref> [5] </ref>. The y concluded that to hide memory latency, these techniques often increase demands on memory bandwidth. The y classify memory stall c ycles into tw o types: those due to lack of a vailable memory bandwidth, and those due purely to latency. <p> F ollowing the methodology in <ref> [5] </ref>, we partition the total application execution time into three components: T P T L and T B which correspond to time spent processing, time spent stalling for memory due to latency, and time spent stalling for memory due to limited bandwidth. <p> Time Column Access Time Data Transfer Time Overlap Data Transfer Time Refresh Time Bus Wait Time caches and out-of-order execution expose memory bandwidth as the bottleneck to impro ving system performance; i.e., common techniques for impro ving CPU performance and tolerating memory latency are e xacerbating the memory bandwidth problem <ref> [5] </ref>. Our results show that contemporary DRAM architectures are attacking exactly that problem.
Reference: [6] <author> M. Charney, P. Coteus, P. Emma, J. Rivers, and J. Rogers. </author> <title> Private communication. </title> <year> 1999. </year>
Reference: [7] <author> R. </author> <title> Crisp. Direct Rambus technology: The new main memory standard. </title> <journal> IEEE Micro, </journal> <volume> vol. 17, no. 6, </volume> <pages> pp. 1828, </pages> <month> November </month> <year> 1997. </year>
Reference-contexts: Their study focuses on the beha vior of latenc y-hiding techniques, while this study focuses on the behavior of different DRAM architectures. Several mark eting studies compare the memory latenc y and bandwidth available from different DRAM architectures <ref> [7, 29, 30] </ref>. This paper builds on these studies by looking at a lar ger assortment of DRAM architectures, measuring DRAM impact on total application performance, decomposing the memory access time into dif fer-ent components, and measuring the hit rates in the row buffers.
Reference: [8] <author> V. Cuppu and B. Jacob. </author> <title> The performance of next-generation DRAM architectures. </title> <type> Tech. Rep. </type> <institution> UMD-SCA-TR-1999-1, University of Maryland Systems and Computer Architecture Group, </institution> <month> March </month> <year> 1999. </year>
Reference-contexts: In the parallel-channel results, it accounts for more than 50%. This suggests that, for some DRAM architectures, bus speed is becoming a critical issue. While current technologies seem balanced, bus speed is likely to become a significant problem v ery quickly for ne xt-generation DRAMs <ref> [8] </ref>. It is interesting to note that the recently announced Alpha 21364 integrates Rambus memory controllers onto the CPU and connects the processor directly to the DRDRAMs with a 400MHz Rambus Channel, thereby eliminating the slow intermediate bus [12].
Reference: [9] <author> Z. Cvetanovic and D. Bhandarkar. </author> <title> Characterization of Alpha AXP performance using TP and SPEC workloads. </title> <booktitle> Proc. </booktitle> <address> ISCA-21, </address> <month> April </month> <year> 1994, </year> <pages> pp. 6070. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [10] <author> ESDRAM. </author> <title> Enhanced SDRAM 1M x 16. Enhanced Memory Systems, </title> <publisher> Inc., </publisher> <address> http://www.edram.com/products/datasheets/16M_esdram0298a.pdf, 1998. </address>
Reference-contexts: This paper presents a simulation-based performance study of a representati ve group, e valuating each in terms of its ef fect on total e xecution time. W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous <ref> [10] </ref>, Synchronous Link [38], Ramb us [31], and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous <ref> [10, 17] </ref>, Synchronous Link [38], Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets.
Reference: [11] <institution> Etch. Memory System Research at the University of Washington. The University of Washington, </institution> <note> http://etch.cs.washington.edu/, 1998. </note>
Reference-contexts: Because SPEC has been criticized as being not representative of real-world applications, we also used Uni versity of Washingtons Etch traces <ref> [11] </ref> to corroborate what we had seen using SPEC on SimpleScalar.
Reference: [12] <author> L. Gwennap. </author> <title> Alpha 21364 to ease memory bottleneck.Microprocessor Report, </title> <journal> vol. </journal> <volume> 12, no. 14, </volume> <pages> pp. 1215, </pages> <month> October </month> <year> 1998. </year>
Reference-contexts: Embedded DRAM [5, 19, 37] is not a near term solution, as its performance is poor on high-end workloads [3]. Faster buses are more likely solutionswit-ness the elimination of the slo w intermediate memory b us in future systems <ref> [12] </ref>. Another solution is to internally bank the memory array into man y small arrays so that each can be accessed v ery quickly, as in the MoSys Multibank DRAM architecture [39]. Second, widening buses will present new optimization opportunities. <p> It is interesting to note that the recently announced Alpha 21364 integrates Rambus memory controllers onto the CPU and connects the processor directly to the DRDRAMs with a 400MHz Rambus Channel, thereby eliminating the slow intermediate bus <ref> [12] </ref>. EDO DRAM does a much better job than FPM DRAM of o ver-lapping column access with data transfer . This is to be e xpected, given the timing diagrams for these architectures. <p> Clearly , we are pushing the limits of todays busses. The Alpha 21364 will solve this problem by ganging together multiple Ramb us Channels connected directly to the CPU, eliminating the 100MHz bus <ref> [12] </ref>. 5.4 Cost-Performance Considerations The organizations are equal in their capacity: all b ut the interleaved examples use eight 64Mbit DRAMs. The FPM3 or ganization uses architectures for the three benchmarks that display the most widely varying behavior. The different DRAM architectures display significantly different access times.
Reference: [13] <author> L. Gwennap. </author> <title> New processor paradigm: </title> <journal> V-IRAM. Microprocessor Report, </journal> <volume> vol. 12, no. 3, </volume> <pages> pp. 1719, </pages> <month> March </month> <year> 1998. </year>
Reference: [14] <author> J. L. Hennessy and D. A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach, 2nd Ed. </title> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1996. </year>
Reference-contexts: This is dif ferent; Rambus refreshes internal banks individually. Because many textbooks describe the refresh operation as a periodic shutting do wn of the DRAM until all ro ws are refreshed (e.g. <ref> [14] </ref>), we also simulated stalling the DRAM once e very 64ms to refresh the entire memory array; thus, e very 64ms, one can potentially delay one or more memory references the time it tak es to refresh the entire memory array .
Reference: [15] <author> S. I. Hong, et al. </author> <title> Access order and effective bandwidth for streams on a Direct Rambus memory. </title> <booktitle> Proc. </booktitle> <address> HPCA-5, </address> <month> January </month> <year> 1999, </year> <pages> pp. 8089. </pages>
Reference: [16] <author> IBM. </author> <title> EDO DRAM 4M x 16 Part No. </title> <address> IBM0165165PT3C. ht-tp://www.chips.ibm.com/products/memory/88H2011/88H2011.pdf, </address> <year> 1998. </year>
Reference-contexts: This paper presents a simulation-based performance study of a representati ve group, e valuating each in terms of its ef fect on total e xecution time. W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out <ref> [16] </ref>, Synchronous [17], Enhanced Synchronous [10], Synchronous Link [38], Ramb us [31], and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out <ref> [16] </ref>, Synchronous [17], Enhanced Synchronous [10, 17], Synchronous Link [38], Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1.
Reference: [17] <author> IBM. </author> <title> SDRAM 1M x 16 x 4 Bank Part No. </title> <address> IBM0364164. ht-tp://www.chips.ibm.com/products/memory/19L3265/19L3265.pdf, </address> <year> 1998. </year>
Reference-contexts: This paper presents a simulation-based performance study of a representati ve group, e valuating each in terms of its ef fect on total e xecution time. W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous <ref> [17] </ref>, Enhanced Synchronous [10], Synchronous Link [38], Ramb us [31], and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous <ref> [17] </ref>, Enhanced Synchronous [10, 17], Synchronous Link [38], Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous <ref> [10, 17] </ref>, Synchronous Link [38], Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets.
Reference: [18] <author> K. Keeton, et al. </author> <title> Performance characterization of a quad Pentium Pro SMP using OLTP workloads. </title> <booktitle> Proc. </booktitle> <address> ISCA-25, </address> <month> June </month> <year> 1998, </year> <pages> pp. 1526. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [19] <author> C. Kozyrakis, et al. </author> <title> Scalable processors in the billion-transistor era: </title> <journal> IRAM. IEEE Computer, </journal> <volume> vol. 30, no. 9, </volume> <pages> pp. 7578, </pages> <month> September </month> <year> 1997. </year>
Reference-contexts: However, the latency benefits are limited by bus and DRAM speeds: to get further improvements, one must run the DRAM core and b us at faster speeds. Current memory busses are adequate for small systems but are likely inadequate for large ones. Embedded DRAM <ref> [5, 19, 37] </ref> is not a near term solution, as its performance is poor on high-end workloads [3]. Faster buses are more likely solutionswit-ness the elimination of the slo w intermediate memory b us in future systems [12].
Reference: [20] <author> D. Kroft. </author> <title> Lockup-free instruction fetch/prefetch cache organization. </title> <booktitle> Proc. </booktitle> <address> ISCA-8, </address> <month> May </month> <year> 1981. </year>
Reference: [21] <author> S. McKee, et al. </author> <title> Design and evaluation of dynamic access ordering hardware. </title> <booktitle> Proc. International Conference on Supercomputing, </booktitle> <month> May </month> <year> 1996. </year>
Reference-contexts: This effect has been seen beforeMcKees work shows that intentionally reordering memory accesses to exploit locality can have an order of magnitude effect on memory-system performance <ref> [21, 22] </ref>. Summary: Coupled with e xtremely wide b uses that hide the effects of limited bandwidth and thus highlight the dif ferences in memory latenc y, the DRAM architectures perform similarly .
Reference: [22] <author> S. A. McKee and W. A. Wulf. </author> <title> Access ordering and memory-conscious cache utilization. </title> <booktitle> Proc. HPCA-1, </booktitle> <month> January </month> <year> 1995, </year> <pages> pp. 253262. </pages>
Reference-contexts: This effect has been seen beforeMcKees work shows that intentionally reordering memory accesses to exploit locality can have an order of magnitude effect on memory-system performance <ref> [21, 22] </ref>. Summary: Coupled with e xtremely wide b uses that hide the effects of limited bandwidth and thus highlight the dif ferences in memory latenc y, the DRAM architectures perform similarly .
Reference: [23] <author> B. Nayfeh, et al. </author> <title> Evaluation of design alternatives for a multiprocessor microprocessor. </title> <booktitle> Proc. </booktitle> <address> ISCA-23, </address> <month> May </month> <year> 1996, </year> <pages> pp. 6777. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [24] <author> B. A. Nayfeh, et al. </author> <title> The impact of shared-cache clustering in small-scale shared-memory multiprocessors. </title> <journal> HPCA-2, Feb. </journal> <volume> 96, </volume> <pages> pp. 7484. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [25] <editor> Y. Nunomura, et al. M32R/Dintegrating DRAM and microprocessor. </editor> <booktitle> IEEE Micro, </booktitle> <volume> vol. 17, no. 6, </volume> <pages> pp. 4048, </pages> <month> Nov. </month> <year> 1997. </year>
Reference: [26] <author> S. Przybylski. </author> <title> New DRAM Technologies: A Comprehensive Analysis of the New Architectures. MicroDesign Resources, </title> <address> Sebastopol CA, </address> <year> 1996. </year>
Reference-contexts: As the standard DRAM interface has become a performance bottleneck, a number of revolutionary proposals <ref> [26] </ref> have been made. <p> Where the refresh or ganization is not specified for an architecture, we simulate a model in which the DRAM allocates bandwidth to either memory references or refresh operations, at the expense of predictability <ref> [26] </ref>. The refresh period for all DRAM parts but Ramb us is 64ms; Ramb us parts ha ve a refresh period of 33ms.
Reference: [27] <author> Rambus. </author> <title> Rambus memory: Enabling technology for PC graphics. </title> <type> Tech. Rep., </type> <institution> Rambus Inc., Mountain View CA, </institution> <month> October </month> <year> 1994. </year>
Reference: [28] <author> Rambus. </author> <title> 64-megabit Rambus DRAM technology directions. </title> <type> Tech. Rep., </type> <institution> Rambus Inc., Mountain View CA, </institution> <month> September </month> <year> 1995. </year>
Reference: [29] <author> Rambus. </author> <title> Comparing RDRAM and SGRAM for 3D applications. </title> <type> Tech. Rep., </type> <institution> Rambus Inc., Mountain View CA, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Their study focuses on the beha vior of latenc y-hiding techniques, while this study focuses on the behavior of different DRAM architectures. Several mark eting studies compare the memory latenc y and bandwidth available from different DRAM architectures <ref> [7, 29, 30] </ref>. This paper builds on these studies by looking at a lar ger assortment of DRAM architectures, measuring DRAM impact on total application performance, decomposing the memory access time into dif fer-ent components, and measuring the hit rates in the row buffers.
Reference: [30] <author> Rambus. </author> <title> Memory latency comparison. </title> <type> Tech. Rep., </type> <institution> Rambus Inc., Mountain View CA, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Their study focuses on the beha vior of latenc y-hiding techniques, while this study focuses on the behavior of different DRAM architectures. Several mark eting studies compare the memory latenc y and bandwidth available from different DRAM architectures <ref> [7, 29, 30] </ref>. This paper builds on these studies by looking at a lar ger assortment of DRAM architectures, measuring DRAM impact on total application performance, decomposing the memory access time into dif fer-ent components, and measuring the hit rates in the row buffers.
Reference: [31] <author> Rambus. </author> <note> 16/18Mbit & 64/72Mbit Concurrent RDRAM Data Sheet. Rambus, http://www.rambus.com/docs/Cnctds.pdf, 1998. </note>
Reference-contexts: W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10], Synchronous Link [38], Ramb us <ref> [31] </ref>, and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. To obtain accurate memory-request timing for an aggressi ve out-of-order processor, we integrate our code into the SimpleScalar tool set [4]. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10, 17], Synchronous Link [38], Rambus <ref> [31] </ref>, and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets.
Reference: [32] <author> Rambus. </author> <note> Direct RDRAM 64/72-Mbit Data Sheet. Rambus, http://www.rambus.com/docs/64dDDS.pdf, 1998. </note>
Reference-contexts: W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10], Synchronous Link [38], Ramb us [31], and Direct Ramb us <ref> [32] </ref>. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. To obtain accurate memory-request timing for an aggressi ve out-of-order processor, we integrate our code into the SimpleScalar tool set [4]. <p> of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10, 17], Synchronous Link [38], Rambus [31], and Direct Rambus <ref> [32] </ref>. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets.
Reference: [33] <author> P. Ranganathan, et al. </author> <title> Performance of database workloads on shared-memory systems with out-of-order processors. </title> <booktitle> Proc. ASPLOS-8, Oc-tober 1998, </booktitle> <pages> pp. 307318. </pages>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [34] <editor> M. Rosenblum, et al. </editor> <booktitle> The impact of architectural trends on operating system performance. In Proc. </booktitle> <address> SOSP-15, </address> <month> December </month> <year> 1995. </year>
Reference-contexts: Finally, there are many studies that measure systemwide performance, including that of the primary memory system <ref> [1, 2, 9, 18, 23, 24, 33, 34] </ref>. Our results resemble theirs, in that we obtain similar figures for the fraction of time spent in the primary memory system.
Reference: [35] <institution> Samsung. </institution> <note> FPM DRAM 4M x 16 Part No. KM416V4100C. Samsung Semiconductor, http://www.usa.samsungsemi.com/products/prod-spec/dramcomp/KM416V40(1)00C.PDF, 1998. </note>
Reference-contexts: This paper presents a simulation-based performance study of a representati ve group, e valuating each in terms of its ef fect on total e xecution time. W e simulate the performance of seven DRAM architectures: F ast P age Mode <ref> [35] </ref>, Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10], Synchronous Link [38], Ramb us [31], and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode <ref> [35] </ref>, Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10, 17], Synchronous Link [38], Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1.
Reference: [36] <author> I. Sase, et al. </author> <title> Multimedia LSI accelerator with embedded DRAM. </title> <journal> IEEE Micro, </journal> <volume> vol. 17, no. 6, </volume> <pages> pp. 4954, </pages> <month> November </month> <year> 1997. </year>
Reference: [37] <author> A. Saulsbury, et al. </author> <title> Missing the memory wall: The case for processor/memory integration. </title> <booktitle> Proc. </booktitle> <address> ISCA-23, </address> <month> May </month> <year> 1996, </year> <pages> pp. 90101. </pages>
Reference-contexts: However, the latency benefits are limited by bus and DRAM speeds: to get further improvements, one must run the DRAM core and b us at faster speeds. Current memory busses are adequate for small systems but are likely inadequate for large ones. Embedded DRAM <ref> [5, 19, 37] </ref> is not a near term solution, as its performance is poor on high-end workloads [3]. Faster buses are more likely solutionswit-ness the elimination of the slo w intermediate memory b us in future systems [12].
Reference: [38] <author> SLDRAM. </author> <title> 4M x 18 SLDRAM Advance Datasheet. </title> <publisher> SLDRAM, Inc., </publisher> <address> http://www.sldram.com/Documents/corp400b.pdf, 1998. </address>
Reference-contexts: W e simulate the performance of seven DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10], Synchronous Link <ref> [38] </ref>, Ramb us [31], and Direct Ramb us [32]. While there are a number of academic proposals for ne w DRAM designs, space limits us to covering only existent commercial parts. <p> This is the portion of T P that is overlapped with memory access. 4.1 DRAM Simulator Overview The DRAM simulator models the internal state of the follo wing DRAM architectures: F ast P age Mode [35], Extended Data Out [16], Synchronous [17], Enhanced Synchronous [10, 17], Synchronous Link <ref> [38] </ref>, Rambus [31], and Direct Rambus [32]. The timing parameters for the different DRAM architectures are given in Table 1. Since we could not find a 64Mbit part specification for ESDRAM, we e xtrapolated based on the most recent SDRAM and ESDRAM datasheets.
Reference: [39] <author> R. Wilson. </author> <title> MoSys tries synthetic SRAM. EE Times Online, </title> <month> July 15, </month> <year> 1997. </year> <note> http://www.eetimes.com/news/98/1017news/tries.html. </note>
Reference-contexts: Another solution is to internally bank the memory array into man y small arrays so that each can be accessed v ery quickly, as in the MoSys Multibank DRAM architecture <ref> [39] </ref>. Second, widening buses will present new optimization opportunities. Each application e xhibits a dif ferent de gree of locality and therefore benefits from page mode to a dif ferent de gree.
References-found: 39

