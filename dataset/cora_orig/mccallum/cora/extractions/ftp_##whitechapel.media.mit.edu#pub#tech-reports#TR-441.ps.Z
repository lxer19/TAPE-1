URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-441.ps.Z
Refering-URL: http://www-white.media.mit.edu/cgi-bin/tr_pagemaker/
Root-URL: http://www.media.mit.edu
Email: fsbasu,sandyg@media.mit.edu  
Title: Three-Dimensional Model of Human Lip Motions Trained from Video  
Author: Sumit Basu and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139 USA  
Affiliation: MIT Media Laboratory,  
Note: A  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 441 Appears: Proceedings of the IEEE Non-Rigid and Articulated Motion Workshop at CVPR'97, San Juan, June 16, 1997 Abstract We present a 3D model of human lips and develop a framework for training it from real data. The model starts off with generic physics specified with the finite element method and "learns" the correct physics through observations. The model's physics allow physically-based regularization between sparse observation points and the resulting set of deformations are used to derive the correct physical modes of the model. Preliminary results showing the model's ability to reconstruct lip shapes from sparse data are shown. The resulting model can be used for both analysis and synthesis.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Adjoudani and C. Benoit. </author> <title> "On the Integration of Auditory and Visual Parameters in an HMM-based ASR". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: level features to form a parametrization of the lip shape: Petajan et al. use several image features to estimate an overal lip contour [13]; Adjoudani et al. relate a small set of observed features (such as lip opening width and height, etc.) to the controls of a polygonal lip model <ref> [1] </ref>. Still others have a trained model of lip variations and attempt to fit the observations to this model. <p> There has been some work done taking information from two known views <ref> [1] </ref>, but this requires the head to remain fairly static. We feel that in order to capture interesting lip data during natural speech and gesture, it will be necessary to robustly track the lips from any pose.
Reference: [2] <author> Ali Azarbayejani and Alex Pentland. </author> <title> Camera self-calibration from one point correspondence. </title> <type> Technical Report 341, </type> <institution> MIT Media Lab Vision and Modeling Group, </institution> <year> 1995. </year> <note> submitted IEEE Symposium on Computer Vision. </note>
Reference-contexts: Methods to continue the training using other forms of input data will be discussed in a later section. The 3D camera calibration algorithm of Azarbayejani and Pentland <ref> [2] </ref> was then used to calibrate the real and virtual cameras using pinhole camera models. Given this calibration, the 3D point location for a given point was estimated by computing the closest point between the projective rays from the camera COP's (centers of projection) corresponding to that point.
Reference: [3] <author> Sumit Basu, Irfan Essa, and Alex Pentland. </author> <title> "Motion Regularization for Model-Based Head Tracking". </title> <booktitle> In Proceedings of 13th Int'l. Conf. on Pattern Recognition, </booktitle> <month> August </month> <year> 1996. </year>
Reference-contexts: In addition, in order to fully train this model, it will be necessary to apply the observations from an arbitrary pose. Prior work has shown that the rigid position of the head can be robustly and accurately tracked <ref> [3] </ref>, so it is feasible that we can apply the observations from any pose to the correct degrees of freedom of the model. As a result, our goal has been to create a model that can cover the full 3D variations of the lips.
Reference: [4] <author> Klaus-Jurgen Bathe. </author> <title> Finite Element Procedures in Engineering Analysis. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: The details of this method are described in many references on finite elements including <ref> [4] </ref> and [16]. At the current time, we are not considering the higher order effects of dynamics (the mass and damping matrices). We hope to incorporate this into our model in the near future. 2.3 Model Specifics For this application, a thin-shell model was chosen. <p> They are thus assembled into the total k e as shown in block-matrix form below: k e = k xy We built the 2D k xy using the formulation as described by Zienkiewicz [16] and Bathe <ref> [4] </ref>. This formulation has the fol lowing stress modes: * = * x fl xy = 4 @x @v @u @x 5 (3) where u and v correspond to displacements along the x and y dimensions of the local coordinate system.
Reference: [5] <author> Christoph Bregler and Stephen M. Omohundro. </author> <title> "Nonlinear Image Interpolation using Manifold Learning". </title> <booktitle> In NIPS 7, </booktitle> <year> 1995. </year>
Reference-contexts: Still others have a trained model of lip variations and attempt to fit the observations to this model. Some of the most interesting work done in this area has been along these lines: Bregler and Omohundro's work, for example <ref> [5] </ref>, models the non-linear subspace of valid lip poses within the image space and can thus be used for both analysis and synthesis. Similarly, Luettin's system learns the subspace of variations for 2D contours surrounding the lips [11].
Reference: [6] <author> Tarcisio Coianiz, Lorenzo Torresani, and Bruno Caprile. </author> <title> "2D Deformable Models for Visual Speech Analysis". </title> <booktitle> In NATO Advanced Study Institute: Speechreading by Man and Machine, </booktitle> <year> 1995. </year>
Reference-contexts: The underlying assumption behind most of these models is that the head will be viewed from only one known pose. As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. <ref> [6] </ref> and Kass et al. [9] model the lips with contours along the outer edge, while Duchnowski et al. [7] feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [7] <author> Paul Duchnowski, Uwe Meier, and Alex Waibel. </author> <title> "See Me, Hear Me: Integrating Automatic Speech Recognition and Lip-Reading". </title> <booktitle> In Int'l Conf. on Spoken Language Processing, </booktitle> <year> 1994. </year>
Reference-contexts: As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. [6] and Kass et al. [9] model the lips with contours along the outer edge, while Duchnowski et al. <ref> [7] </ref> feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [8] <author> Irfan A. Essa. </author> <title> "Analysis, Interpretation, and Synthesis of Facial Expressions". </title> <type> PhD thesis, </type> <institution> MIT Department of Media Arts and Sciences, </institution> <year> 1995. </year>
Reference: [9] <author> Michael Kass, Andrew Witkin, and Demetri Terzopoulous. "Snakes: </author> <title> Active Contour Models". </title> <journal> International Journal of Computer Vision, </journal> <pages> pages 321-331, </pages> <year> 1988. </year>
Reference-contexts: The underlying assumption behind most of these models is that the head will be viewed from only one known pose. As a result, these models are often only two-dimensional. Many are based directly on image features: Coianiz et al. [6] and Kass et al. <ref> [9] </ref> model the lips with contours along the outer edge, while Duchnowski et al. [7] feed the raw pixel intensities into a neural net to classify lip shapes.
Reference: [10] <author> Y. Lee, D. Terzopoulos, and K. Waters. </author> <title> "Realistic Modeling for Facial Animation". </title> <booktitle> In Proceedings of SIGGRAPH, </booktitle> <pages> pages 55-62, </pages> <year> 1995. </year>
Reference-contexts: The other category of lip models are those designed for synthesis and facial animation. These lip models are usually part of a larger facial animation system, and the lips themselves often have a limited repertoire of motions <ref> [10] </ref>. To their credit, these models are mostly in 3D. For many of the models, though, the control parameters are defined by hand. A few are based on the actual physics of the lips: they attempt to model the physical material and musculature in the mouth region [8],[15].
Reference: [11] <author> J. Luettin, N. Thacker, and S. Beet. </author> <title> "Visual Speech Recognition Using Active Shape Models And Hidden Markov Models". </title> <booktitle> In ICASSP96, </booktitle> <pages> pages 817-820. </pages> <booktitle> IEEE Signal Processing Society, </booktitle> <year> 1996. </year>
Reference-contexts: Similarly, Luettin's system learns the subspace of variations for 2D contours surrounding the lips <ref> [11] </ref>. However, in order for these 2D models to be robust, they have to allow for at least small rotations of the head. The changes in the apparent lip shape due to rigid rotations, then, have to be modeled as changes in the actual lip pose.
Reference: [12] <author> John Martin, Alex Pentland, and Ron Kikinis. </author> <title> Shape analysis of brain structures using physical and experimental modes. In CVPR94. </title> <publisher> IEEE Computer Society, </publisher> <year> 1994. </year>
Reference-contexts: We began with the default physics (i.e., fairly uniform stiffness, only adjacent nodes connected) and have now observed how the model actually deforms. This new information can be used to form a new, "learned" K ma trix. Martin et al. <ref> [12] </ref> described the connection between the strain matrix and the covariance of the displacements.
Reference: [13] <author> E.D. Petajan. </author> <title> "Automatic Lipreading to Enhance Speech Recognition". </title> <booktitle> In Proc. IEEE Communications Society Global Telecom. Conf., </booktitle> <month> November </month> <year> 1984. </year>
Reference-contexts: Others use such low level features to form a parametrization of the lip shape: Petajan et al. use several image features to estimate an overal lip contour <ref> [13] </ref>; Adjoudani et al. relate a small set of observed features (such as lip opening width and height, etc.) to the controls of a polygonal lip model [1]. Still others have a trained model of lip variations and attempt to fit the observations to this model.
Reference: [14] <author> Henry Stark and John W. Woods. </author> <title> Probability, Random Processes, and Estimation Theory for Engineers. </title> <publisher> Prentice Hall, </publisher> <year> 1994. </year>
Reference-contexts: Furthermore, noise in the observations makes it unreasonable to estimate even this many modes. We thus take only the 10 observed modes that account for the greatest amount of variance in the input data. These modes are found by performing principal components analysis (PCA) on the sample covariance matrix <ref> [14] </ref>, i.e., taking the eigenvectors and eigenvalues. Finding the eigenvalues and eigenvectors of the expected covari-ance matrix (which is 612 by 612) would take a great deal of computation.
Reference: [15] <author> K. Waters and J. Frisbie. </author> <title> "A Coordinated Muscle Model for Speech Animation". </title> <booktitle> In Graphics Interface, </booktitle> <pages> pages 163-170, </pages> <year> 1995. </year>
Reference-contexts: Some models, as in the work by Frisbie and Waters, have tried to approximate this subspace by modeling key lip positions (visemes) and then interpolating between them <ref> [15] </ref>. However, this limits the accuracy of the resulting lip shapes, since the only shapes learned from data are those for the static viseme poses. We hope to fill the gap in these approaches with a 3D model that can be used for both analysis and synthesis.
Reference: [16] <author> O.C. Zienkiewicz. </author> <title> The Finite Element Method in Structural and Continuum Mechanics. </title> <publisher> McGraw-Hill, </publisher> <year> 1967. </year> <month> 7 </month>
Reference-contexts: The details of this method are described in many references on finite elements including [4] and <ref> [16] </ref>. At the current time, we are not considering the higher order effects of dynamics (the mass and damping matrices). We hope to incorporate this into our model in the near future. 2.3 Model Specifics For this application, a thin-shell model was chosen. <p> We hope to incorporate this into our model in the near future. 2.3 Model Specifics For this application, a thin-shell model was chosen. We constructed the model by beginning with a 2D plane-stress isotropic material formulation <ref> [16] </ref> and adding a strain relationship for the out-of-plane components. For each triangular element, then, the six in-plane degrees of freedom are related with a six-by-six matrix k xy , while the out-of-plane degrees of freedom are related by the three-by-three k z . <p> They are thus assembled into the total k e as shown in block-matrix form below: k e = k xy We built the 2D k xy using the formulation as described by Zienkiewicz <ref> [16] </ref> and Bathe [4]. This formulation has the fol lowing stress modes: * = * x fl xy = 4 @x @v @u @x 5 (3) where u and v correspond to displacements along the x and y dimensions of the local coordinate system.
References-found: 16

