URL: ftp://ftp.cs.bris.ac.uk/pub/users/cgc/ECML98/jelonek.ps.Z
Refering-URL: http://www.cs.bris.ac.uk/~cgc/ECML98-WS/Summary.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: jacek.jelonek@cs.put.poznan.pl, macko@ismael.sc.cs.put.poznan.pl  
Title: 21 Using n 2 classifier in constructive induction  
Author: Jacek Jelonek and Maciej .RPRVL VNL 
Address: Piotrowo 3A, 60-965 Poznan, Poland  
Affiliation: Institute of Computing Science Poznan University of Technology  
Abstract: In this paper, we propose a multi-classification approach for constructive induction. The idea of an improvement of classification accuracy is based on iterative modification of input data space. This process is independently repeated for each pair of n classes. Finally, it gives (n 2 n)/2 input data subspaces of attributes dedicated for optimal discrimination of appropriate pairs of classes. We use genetic algorithms as a constructive induction engine. A final classification is obtained by a weighted majority voting rule, according to n 2 - classifier approach. The computational experiment was performed on medical data set. The obtained results point out the advantage of using a multi-classification model (n 2 classifier) in constructive induction in relation to the analogous single-classifier approach.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Aha, D.W., Kibler E., </author> <title> Albert M.K.: Instance-based learning algorithms. </title> <journal> Machine Learning, </journal> <volume> 6, </volume> <year> (1991) </year> <month> 37-66. </month>
Reference-contexts: Evaluation is usually done by estimating classification accuracy. Feature selection can be treated as the simplest method belonging to constructive induction methodology - regarded as an approach to supporting automatic, problem-oriented transformation of representation space to facilitate learning (Michalski, 1978), (Matheus and Rendell, 1989), <ref> (Wnek and Michalski, 1991) </ref>. Generally, an improvement of accuracy in constructive induction is usually obtained by construction of new features, modification of existing ones and reduction of irrelevant ones. Most methods use a specific technique within one basic computational method. <p> 1989), <ref> (Wnek and Michalski, 1991) </ref>. Generally, an improvement of accuracy in constructive induction is usually obtained by construction of new features, modification of existing ones and reduction of irrelevant ones. Most methods use a specific technique within one basic computational method. Basic methods are classified as data-driven, hypothesis-driven and knowledge-driven (Wnek and Michalski, 1991). In this paper we would like to introduce the idea of using the multi-classification system, n classifier, in a constructive induction framework. The n 2 classifier is composed of (n n)/2 binary base classifiers, (n is a number of classes).
Reference: 2. <author> Aha, D.W., </author> <title> Bankert R.L.: Feature Selection for Case-based Classification of Cloud Types: An Empirical Comparison, </title> <booktitle> Proceedings AAAI-94 Workshop Case-Based Reasoning, </booktitle> <year> (1994), </year> <pages> 106-112. </pages>
Reference: 3. <author> Bloedorn, E., Michalski, R.S, Wnek, J.: </author> <title> Multistrategy Constructive Induction: </title> <booktitle> AQ17-MCI. Proceedings of the 2 nd International Workshop on Multi-Strategy Learning, </booktitle> <address> Harpers Ferry, WV, </address> <year> (1993). </year>
Reference-contexts: In a single-classifier approach, 1-NN is the main classifier. In the n 2 classifier approach, 1-NN is used as a binary classifier which distinguishes between pairs of classes. 1.2.2 The n classifier The n classifier belongs to a group of multiple learning algorithms dedicated to solving multiclass learning problems <ref> (Chan and Stolfo, 1993) </ref>, (Littlestone and Warmuth, 1994). The main principle of the n 2 classifier is the discrimination of each pair of the classes: (i, j), i, j 1..n, ij, by an independent binary classifier C i,j .
Reference: 4. <author> Chan, P.K., Stolfo, S.J.: </author> <title> Experiments on multistrategy learning by metalearning. </title> <booktitle> In Proceedings of the Second International Conference on Information and Knowledge Management, </booktitle> <year> (1993), </year> <pages> 314-323. 29 </pages>
Reference: 5. <author> Friedman, J.H.: </author> <title> Another approach to polychotomous classification, </title> <type> Technical Report, </type> <institution> Stanford University, </institution> <year> (1996). </year>
Reference: 6. <author> Goldberg, D.E.: </author> <title> Genetic Algorithms in Search, Optimization, and Machine Learning, </title> <publisher> Addison-Wesley Publishing Co. </publisher> <year> (1989). </year>
Reference: 7. <author> Hastie, T., Tibshirani R.: </author> <title> Classification by pairwise coupling, </title> <booktitle> Proc. </booktitle> <address> NIPS97. </address>
Reference: 8. <author> Jelonek, J., Stefanowski J.: </author> <title> Using n 2 classifier to solve multiclass learning problems. </title> <type> Technical Report, </type> <institution> Poznan University of Technology, </institution> <year> (1997). </year>
Reference-contexts: Our previous experiments have shown that n 2 classifier based on binary classifiers with inherent capability of reducing the influence of irrelevant features usually yields better classification results than an analogous, single classification model <ref> (Jelonek and Stefanowski, 1997, 1998) </ref>. This observation led us to a hypothesis that, creation of dedicated sets of features (e.g. by constructive induction) for discrimination of each pair of classes by independent classifiers will give higher accuracy than using a single set of features for classification of all classes simultaneously.
Reference: 9. <author> Jelonek, J., Stefanowski J.: </author> <title> Feature subset selection for classification of histological images, </title> <booktitle> Artificial Intelligence in Medicine 9 (1997), </booktitle> <pages> 227-239. </pages>
Reference-contexts: An influence on the accuracy of those remaining may be not significant; sometimes using them may even deteriorate the classification result. In order to solve that problem many feature selection algorithms have been developed (Aha et al, 1994), (John et al, 1994), (Pudil et al, 1994), <ref> (Jelonek and Stefanowski, 1997) </ref>. In general, feature selection can be treated as a search problem. Each state in the search space represents a subset of possible features. <p> The n 2 classifier is composed of (n n)/2 binary base classifiers, (n is a number of classes). Each of them is specialized to discriminate respective pairs of decision classes. A quite similar approach was independently introduced by Friedman (1996) and later extended and modified <ref> (Hastie and Tibshirani, 1997) </ref>, (Mayoraz and Moreira, 1997). Our previous experiments have shown that n 2 classifier based on binary classifiers with inherent capability of reducing the influence of irrelevant features usually yields better classification results than an analogous, single classification model (Jelonek and Stefanowski, 1997, 1998). <p> The n 2 classifier is composed of (n n)/2 binary base classifiers, (n is a number of classes). Each of them is specialized to discriminate respective pairs of decision classes. A quite similar approach was independently introduced by Friedman (1996) and later extended and modified (Hastie and Tibshirani, 1997), <ref> (Mayoraz and Moreira, 1997) </ref>. Our previous experiments have shown that n 2 classifier based on binary classifiers with inherent capability of reducing the influence of irrelevant features usually yields better classification results than an analogous, single classification model (Jelonek and Stefanowski, 1997, 1998). <p> Our previous experiments have shown that n 2 classifier based on binary classifiers with inherent capability of reducing the influence of irrelevant features usually yields better classification results than an analogous, single classification model <ref> (Jelonek and Stefanowski, 1997, 1998) </ref>. This observation led us to a hypothesis that, creation of dedicated sets of features (e.g. by constructive induction) for discrimination of each pair of classes by independent classifiers will give higher accuracy than using a single set of features for classification of all classes simultaneously. <p> Each class is represented by 10 images of 5 patients. To avoid undesirable appearance of the data coming from the same patient in learning and testing data set, we were obliged to perform a 5-fold cross validation technique. In our former researches <ref> (Jelonek and Stefanowski, 1997) </ref>, we havent considered this particular feature of this data set, which resulted in improperly high classification accuracy. Our previous experiments showed that the histogram of intensities of three principal color components (RGB red, green, blue) is a simple feature which gives a relatively high classification results.
Reference: 10. <author> Jelonek, J., Stefanowski J.: </author> <title> Experiments on solving multiclass learning problems by n classifier, </title> <booktitle> Accepted to 10 th European Conference on Machine Learning, </booktitle> <address> Chemnitz, Germany, </address> <year> (1998). </year>
Reference: 11. <author> John, G., Kohavi R., Pfleger K.: </author> <title> Irrelevant features and the subset selection problem, </title> <booktitle> In Proceedings 11 th International Machine Learning Conference, </booktitle> <year> (1994), </year> <pages> 121-129. </pages>
Reference: 12. <author> Littlestone, N., Warmuth, </author> <title> M.K.: The weighted majority algorithm. </title> <journal> Information and Computation, </journal> <volume> 108 (2), </volume> <year> (1994), </year> <month> 212-261,. </month>
Reference: 13. <author> Matheus, Ch.J.: </author> <title> The need for constructive induction, </title> <booktitle> In Proceedings 8 th International Workshop on Machine Learning. </booktitle>
Reference: 14. <author> Matheus, C.J. and Rendell, L.: </author> <title> Constructive Induction on Decision Trees, </title> <booktitle> Proceedings of IJCAI-89, </booktitle> <pages> pp. 645-650, </pages> <address> Detroit, MI, </address> <year> (1989). </year>
Reference: 15. <author> Mayoraz, E., Moreira, M.: </author> <title> On the decomposition of polychotomies into dichotomies, </title> <booktitle> In Proceedings 14 th International Conference on Machine Learning, </booktitle> <year> (1997), </year> <pages> 219-226. </pages>
Reference: 16. <author> Michalewicz, Z.: </author> <title> Genetic Algorithms + Data Structures = Evolution Programs, </title> <publisher> Springer-Verlag, </publisher> <year> (1996). </year>
Reference: 17. <author> Michalski, </author> <title> R.S.: Pattern recognition as knowledge-guided computer induction, </title> <journal> Departament of Computer Science Reports, </journal> <volume> No. </volume> <pages> 927, </pages> <institution> University of Illinois, Urbana, </institution> <month> June </month> <year> 1978. </year>
Reference: 18. <editor> Michalski, R.S., Tecuci, G.: </editor> <title> Machine Learning. A multistrategy approach. </title> <booktitle> Volume IV, </booktitle> <publisher> Morgan Kaufmann (1994). </publisher>
Reference: 19. <author> Pudil, P., Novovicova J., Kittler J., </author> <title> Floating Search methods in feature selection, </title> <journal> Pattern Recognition Letters 15, </journal> <year> (1994) </year> <month> 1119-1125. </month>
Reference-contexts: However, only a part of them is usually relevant. An influence on the accuracy of those remaining may be not significant; sometimes using them may even deteriorate the classification result. In order to solve that problem many feature selection algorithms have been developed <ref> (Aha et al, 1994) </ref>, (John et al, 1994), (Pudil et al, 1994), (Jelonek and Stefanowski, 1997). In general, feature selection can be treated as a search problem. Each state in the search space represents a subset of possible features. Following the typical view of feature selection algorithms (Aha et al, 1994) <p> However, only a part of them is usually relevant. An influence on the accuracy of those remaining may be not significant; sometimes using them may even deteriorate the classification result. In order to solve that problem many feature selection algorithms have been developed (Aha et al, 1994), <ref> (John et al, 1994) </ref>, (Pudil et al, 1994), (Jelonek and Stefanowski, 1997). In general, feature selection can be treated as a search problem. Each state in the search space represents a subset of possible features. <p> An influence on the accuracy of those remaining may be not significant; sometimes using them may even deteriorate the classification result. In order to solve that problem many feature selection algorithms have been developed (Aha et al, 1994), (John et al, 1994), <ref> (Pudil et al, 1994) </ref>, (Jelonek and Stefanowski, 1997). In general, feature selection can be treated as a search problem. Each state in the search space represents a subset of possible features. <p> have been developed <ref> (Aha et al, 1994) </ref>, (John et al, 1994), (Pudil et al, 1994), (Jelonek and Stefanowski, 1997). In general, feature selection can be treated as a search problem. Each state in the search space represents a subset of possible features. Following the typical view of feature selection algorithms (Aha et al, 1994) one can define: search algorithm - which looks through the space of feature subsets; 22 evaluation function used to evaluate examined subsets of features; and classifier - which is constructed basing on final subset of features. <p> These elements can be integrated in two basic ways forming filter or wrapper model <ref> (John et al, 1994) </ref>. In the filter model, features are selected as a preprocessing step before a classifier is used. Features are selected (i.e. filtered) depending on properties of data itself, independently of the learning algorithm used in a classifier. <p> In the n 2 classifier approach, 1-NN is used as a binary classifier which distinguishes between pairs of classes. 1.2.2 The n classifier The n classifier belongs to a group of multiple learning algorithms dedicated to solving multiclass learning problems (Chan and Stolfo, 1993), <ref> (Littlestone and Warmuth, 1994) </ref>. The main principle of the n 2 classifier is the discrimination of each pair of the classes: (i, j), i, j 1..n, ij, by an independent binary classifier C i,j .
Reference: 20. <author> Wnek, J. and Michalski, </author> <title> R.S.: Hypothesis-driven constructive induction in AQ17: a method and experiments, </title> <booktitle> Proceedings of IJCAI-91, Workshop on Evaluating and Changing Representation in Machine Learning, </booktitle> <address> Sydney, Australia, </address> <year> (1991). </year>
References-found: 20

