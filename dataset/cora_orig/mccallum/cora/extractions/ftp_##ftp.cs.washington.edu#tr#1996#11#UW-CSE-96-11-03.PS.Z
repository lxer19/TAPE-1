URL: ftp://ftp.cs.washington.edu/tr/1996/11/UW-CSE-96-11-03.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-date.html
Root-URL: http://www.cs.washington.edu
Title: RaPiD A Configurable Computing Architecture for Compute-Intensive Applications  
Author: Carl Ebeling, Darren C. Cronquist, Paul Franklin and Chris Fisher 
Address: Box 352350 Seattle, WA 98195-2350  
Affiliation: Department of Computer Science and Engineering University of Washington  
Abstract: Technical Report UW-CSE-96-11-03 November, 1996 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Annaratone et al. </author> <title> The warp computer: architecture, implementation, and performance. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1523-38, </volume> <year> 1987. </year>
Reference-contexts: RaPiD is most closely related to the systolic arrays developed in the 1980s in terms of computation style and application domain. But RaPiD fills the gap between static systolic arrays, which are too inflexible, and programmable systolic arrays like Warp and iWarp <ref> [1] </ref>, which more closely resemble multiprocessors with too much control overhead. Space has precluded any discussion of programming RaPiD. Programmability is a major problem for configurable computers, which has limited their widespread use.
Reference: [2] <author> J. M. Arnold, D. A. Buell, D. T. Hoang, D. V. Pryor, N. Shirazi, and M. R. Thistle. </author> <title> The Splash 2 processor and applications. </title> <booktitle> In Proceedings IEEE International Conference on Computer Design: VLSI in Computers and Processors, </booktitle> <pages> pages 482-5. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year>
Reference-contexts: This is done by dynamically constructing a custom architecture from an underlying substrate of configurable circuitry 1 . Although the concept of configurable computing is very attractive, success has been remarkably hard to achieve in practice. Current custom computing machines <ref> [2, 13] </ref> are constructed from field-programmable gate arrays (FP-GAs) [11]. These contain logic blocks which can be configured to compute arbitrary functions, and configurable wiring which can be used to connect the logic blocks as well as registers together into arbitrary circuits.
Reference: [3] <author> G. R. Beck, D. W. L. Yen, and T. L. Anderson. </author> <title> The Cydra 5 minisupercomputer: architecture and implementation. </title> <journal> Journal of Supercomputing, </journal> <volume> 7(1-2):143-80, </volume> <year> 1993. </year>
Reference-contexts: We make use of two different and seemingly competing techniques. First, we use memories that support 6 fast burst mode, that is, high-bandwidth data trans-fer to addresses in the same row. Second, we organize memory into randomly interleaved memory banks <ref> [10, 3] </ref>. Fast burst mode supports mostly-sequential memory accesses. Data can usually be stored in memory so that accesses are mostly-sequential. For example, all the address streams for matrix multiply are in row-major order 3 and therefore mostly-sequential.
Reference: [4] <author> P. Bertin, D. Roncin, and J. Vuillemin. </author> <title> Programmable active memories: a performance assessment. In Parallel Architectures and Their Efficient Use: </title> <booktitle> First Heinz Nixdorf Symposium Proceedings, </booktitle> <pages> pages 119-30. </pages> <publisher> Springer-Verlag, </publisher> <year> 1993. </year>
Reference-contexts: The PAM project has reported some of the best performance for configurable machines. A single PAM P 1 board can perform 2D-DCT at a rate of 1.4 GOPS (an OP is an MAC/subtract/shift) <ref> [4] </ref>. Section 6.2.2 showed that RaPiD achieves 1.6 GOPS. 7 Conclusion and Future Directions RaPiD represents an efficient configurable computing solution for regular computationally-intensive applications. By combining the appropriate amount of static and dynamic control, it achieves substantially reduced control overhead relative to FPGA-based and general-purpose processor architectures.
Reference: [5] <author> E. De Greef, F. Catthoor, and H. De Man. </author> <title> Mapping real-time motion estimation type algorithms to memory efficient, programmable multiprocessor architectures. </title> <journal> Micro-processing & Microprogramming, </journal> <volume> 41(5-6):409-23, </volume> <year> 1995. </year>
Reference-contexts: For comparison, we cite here performance results for a high-performance digital signal processor and one of the highest performance FPGA-based reconfigurable computing machines. De Greef et al. derive a motion estimation algorithm highly optimized for DSP-style architectures <ref> [5] </ref>.
Reference: [6] <author> A. DeHon. </author> <title> Reconfigurable Architectures for General-Purpose Computing. </title> <type> Ph.D. thesis, </type> <institution> Massachusetts Institute of Technology, </institution> <month> August </month> <year> 1996. </year>
Reference-contexts: Unfortunately, the fine-grained circuit structure which makes them so general has a very high cost: Depending on the circuit being constructed, this cost-performance penalty can range from a factor of 20 for random logic to well over 100 for structured circuits like ALUs, multipliers and memory <ref> [6] </ref>. Thus, custom computing based on FPGAs is unlikely to compete on applications that involve heavy arithmetic computation.
Reference: [7] <author> H.T. Kung. </author> <title> Let's design algorithms for VLSI systems. </title> <type> Technical Report CMU-CS-79-151, </type> <institution> Carnegie-Mellon University, </institution> <month> January </month> <year> 1979. </year>
Reference-contexts: RaPiD provides a large number of ALUs, multipliers, registers and memory modules that can be configured into the appropriate pipelined datapath. The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms <ref> [7] </ref>, for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays [9, 8].
Reference: [8] <author> P. Lee and Z. M. Kedem. </author> <title> Synthesizing linear array algorithms from nested FOR loop algorithms. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1578-98, </pages> <year> 1988. </year>
Reference-contexts: The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms [7], for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays <ref> [9, 8] </ref>. RaPiD is not limited to implementing systolic algorithms, however; a pipeline can be constructed which comprises different computations at different stages and at different times.
Reference: [9] <author> D. I. Moldovan and J. A. B. Fortes. </author> <title> Partitioning and mapping algorithms into fixed size systolic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-35(1):1-12, </volume> <year> 1986. </year>
Reference-contexts: The datapaths constructed in RaPiD are linear arrays of functional units communicating in mostly nearest-neighbor fashion. Systolic algorithms [7], for example, map very well into RaPiD datapaths, allowing us to take advantage of the considerable research on compiling computations to systolic arrays <ref> [9, 8] </ref>. RaPiD is not limited to implementing systolic algorithms, however; a pipeline can be constructed which comprises different computations at different stages and at different times.
Reference: [10] <author> B. R. Rau. </author> <title> Pseudo-randomly interleaved memory. </title> <booktitle> In Proceedings of the 18th International Symposium on Computer Architecture, </booktitle> <pages> pages 74-83, </pages> <year> 1991. </year>
Reference-contexts: We make use of two different and seemingly competing techniques. First, we use memories that support 6 fast burst mode, that is, high-bandwidth data trans-fer to addresses in the same row. Second, we organize memory into randomly interleaved memory banks <ref> [10, 3] </ref>. Fast burst mode supports mostly-sequential memory accesses. Data can usually be stored in memory so that accesses are mostly-sequential. For example, all the address streams for matrix multiply are in row-major order 3 and therefore mostly-sequential.
Reference: [11] <author> J. Rose, A. El Gamal, and A Sangiovanni Vincentelli. </author> <title> Architecture of field-programmable gate arrays. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(7) </volume> <pages> 1013-29, </pages> <year> 1993. </year>
Reference-contexts: This is done by dynamically constructing a custom architecture from an underlying substrate of configurable circuitry 1 . Although the concept of configurable computing is very attractive, success has been remarkably hard to achieve in practice. Current custom computing machines [2, 13] are constructed from field-programmable gate arrays (FP-GAs) <ref> [11] </ref>. These contain logic blocks which can be configured to compute arbitrary functions, and configurable wiring which can be used to connect the logic blocks as well as registers together into arbitrary circuits.
Reference: [12] <author> K. A. Vissers et al. </author> <title> Architecture and programming of two generations video signal processors. </title> <journal> Microprocessing & Microprogramming, </journal> <volume> 41(5-6):373-90, </volume> <year> 1995. </year>
Reference-contexts: During each cycle, a read followed by an optional write is performed to the addressed location. This is not used for read-modify-write operations because the latency would be too long, but rather to implement configurable-length pipeline delays similar to the SILOs found in the Philips VSP <ref> [12] </ref>. Input and output data enter and exit RaPiD via I/O streams at each end of the datapath. Each stream contains a FIFO filled with data required or with results produced by the computation.
Reference: [13] <author> J. E. Vuillemin, P. Bertin, D. Roncin, M. Shand, H. H. Touati, and P. Boucard. </author> <title> Programmable active memories: reconfigurable systems come of age. </title> <journal> IEEE Transactions on Very Large Scale Integration (VLSI) Systems, </journal> <volume> 4(1) </volume> <pages> 56-69, </pages> <year> 1996. </year>
Reference-contexts: This is done by dynamically constructing a custom architecture from an underlying substrate of configurable circuitry 1 . Although the concept of configurable computing is very attractive, success has been remarkably hard to achieve in practice. Current custom computing machines <ref> [2, 13] </ref> are constructed from field-programmable gate arrays (FP-GAs) [11]. These contain logic blocks which can be configured to compute arbitrary functions, and configurable wiring which can be used to connect the logic blocks as well as registers together into arbitrary circuits.
Reference: [14] <editor> Wazlowski-M. et al. </editor> <booktitle> PRISM-II compiler and architecture. In Proceedings IEEE Workshop on FPGAs for Custom Computing Machines, </booktitle> <pages> pages 9-16. </pages> <publisher> IEEE Comput. Soc. Press, </publisher> <year> 1993. </year> <month> 12 </month>
Reference-contexts: we are to achieve good performance for computation-intensive applications, we will need new configurable computing architectures that can ef 1 Configurable computing is also used to refer to other architecture styles, e.g. dynamically constructing custom functional units in a general-purpose processor similar to defining custom instructions using writable control store <ref> [14] </ref>. Our research is focused on traditional application-specific architectures. 1 ficiently implement these arithmetic computations. Our response to this challenge is RaPiD, a coarse-grained configurable architecture for constructing deep computational pipelines. RaPiD is aimed at regular, computation-intensive tasks like those found in digital signal processing (DSP).
References-found: 14

