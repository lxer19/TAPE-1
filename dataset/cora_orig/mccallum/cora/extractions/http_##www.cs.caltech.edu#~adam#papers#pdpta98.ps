URL: http://www.cs.caltech.edu/~adam/papers/pdpta98.ps
Refering-URL: http://www.cs.caltech.edu/~adam/papers.html
Root-URL: http://www.cs.caltech.edu
Email: adam@cs.caltech.edu  berna@cs.caltech.edu  
Title: Performance Analysis for Archetypes  
Author: Adam Rifkin Berna L. Massingill 
Keyword: Performance analysis, performance modeling, programming archetypes, design patterns.  
Address: Pasadena, California 91125  Pasadena, California 91125  
Affiliation: Computer Science 256-80 California Institute of Technology  Computer Science 256-80 California Institute of Technology  
Abstract: This document outlines a simple method for benchmarking a parallel communication library and using the results to model the performance of applications developed with that communication library. We use compositional performance analysis (breaking down a parallel program into its modular parts and analyzing them) to model parallel program execution times for different types of programs using communication libraries built with different message-passing schemes running on different architectures. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. M. Chandy. </author> <title> Concurrent program archetypes. </title> <booktitle> In Proceedings of the Scalable Parallel Library Conference, </booktitle> <year> 1994. </year>
Reference-contexts: 1 Introduction A parallel programming archetype <ref> [1, 2] </ref> is an abstraction that captures the common features of a class of problems with similar computational structure and combines them with a par-allelization strategy to produce a pattern of dataflow and communication.
Reference: [2] <author> K. M. Chandy, R. Manohar, B. L. Massingill, and D. I. Meiron. </author> <title> Integrating task and data parallelism with the collective communication archetype. </title> <booktitle> In Proceedings of the International Parallel Processing Symposium IPPS-9, </booktitle> <month> April </month> <year> 1995. </year>
Reference-contexts: 1 Introduction A parallel programming archetype <ref> [1, 2] </ref> is an abstraction that captures the common features of a class of problems with similar computational structure and combines them with a par-allelization strategy to produce a pattern of dataflow and communication.
Reference: [3] <author> I. T. Foster. </author> <title> Designing and Building Parallel Programs. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: The performance model in this paper is based on the ideas of extrapolation from observations, asymptotic analysis, scalability analysis, execution profiles, and data fitting as investigated in <ref> [3] </ref>. The problem of choosing a data partitioning and distribution to achieve optimal performance is NP-complete [4], so we are more interested in user-guided performance evaluation tools for refining parallel applications than in automatic performance prediction (e.g., [5]).
Reference: [4] <author> A. Rifkin. </author> <title> Application development using analytic and experimental performance tuning. </title> <type> Technical Report CS-TR-96-09, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: The performance model in this paper is based on the ideas of extrapolation from observations, asymptotic analysis, scalability analysis, execution profiles, and data fitting as investigated in [3]. The problem of choosing a data partitioning and distribution to achieve optimal performance is NP-complete <ref> [4] </ref>, so we are more interested in user-guided performance evaluation tools for refining parallel applications than in automatic performance prediction (e.g., [5]). <p> We call the methodology of developing the program concurrently with modeling its perfor mance Adapt, for Application Development using Analytic Performance Tuning <ref> [4] </ref>. The Analysis segment of Adapt involves decomposing the given program into a number of subprograms (e.g., initialization, computational loop, and termination) whose running times can be expressed in terms of the benchmark numbers. <p> The finer the grain of decomposition and benchmarking, the more predictive we expect the model equation to be for that program. The basis for this decompositional approach to performance modeling is a structured induction <ref> [4] </ref> on the statements of the program being modeled, assuming implicit barriers between any subprograms. <p> We call the methodology of developing the program while using the performance model with a simulation architecture Adept, for Application Development using Experimental Performance Tuning <ref> [4] </ref>. Like the performance model, these simulations can help guide a programmer in making decisions about data distribution and granularity during the Refinement phase (path (g) in Figure 1). Briefly, setting up a simulation works as follows.
Reference: [5] <author> T. Fahringer. </author> <title> Automatic Performance Prediction of Parallel Programs. </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1996. </year>
Reference-contexts: The problem of choosing a data partitioning and distribution to achieve optimal performance is NP-complete [4], so we are more interested in user-guided performance evaluation tools for refining parallel applications than in automatic performance prediction (e.g., <ref> [5] </ref>). Since our model is intended for use in the context of archetype-based application development, it differs from efforts to do performance measurement for compiler optimization (e.g., [6]) and from efforts to estimate performance statically to automate load balancing (e.g., [7]).
Reference: [6] <author> M. J. Clement and M. J. Quinn. </author> <title> Analytical performance prediction on multicomputers. </title> <booktitle> In Proceedings of Supercomputing 93, </booktitle> <month> November </month> <year> 1993. </year>
Reference-contexts: Since our model is intended for use in the context of archetype-based application development, it differs from efforts to do performance measurement for compiler optimization (e.g., <ref> [6] </ref>) and from efforts to estimate performance statically to automate load balancing (e.g., [7]). Our techniques fit in well with other methodologies for dealing with applications developed for particular architectures (e.g., [8]).
Reference: [7] <author> T. Fahringer. </author> <title> On estimating the useful work distribution of parallel programs under P 3 T : Static performance estimator. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 8(4) </volume> <pages> 261-282, </pages> <year> 1996. </year>
Reference-contexts: Since our model is intended for use in the context of archetype-based application development, it differs from efforts to do performance measurement for compiler optimization (e.g., [6]) and from efforts to estimate performance statically to automate load balancing (e.g., <ref> [7] </ref>). Our techniques fit in well with other methodologies for dealing with applications developed for particular architectures (e.g., [8]). Archetypes frequently represent well-researched patterns or abstractions; for example, the mesh archetype [9] builds on Brinch Hansen's work on parallel cellular automata in the context of multicomputers [10].
Reference: [8] <author> P. Brinch Hansen. </author> <title> Model programs for computational science: A programming methodology for multicomputers. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 5(5) </volume> <pages> 407-423, </pages> <year> 1993. </year>
Reference-contexts: Our techniques fit in well with other methodologies for dealing with applications developed for particular architectures (e.g., <ref> [8] </ref>). Archetypes frequently represent well-researched patterns or abstractions; for example, the mesh archetype [9] builds on Brinch Hansen's work on parallel cellular automata in the context of multicomputers [10].
Reference: [9] <author> B. L. Massingill. </author> <title> The mesh archetype. </title> <type> Technical Report CS-TR-96-25, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Our techniques fit in well with other methodologies for dealing with applications developed for particular architectures (e.g., [8]). Archetypes frequently represent well-researched patterns or abstractions; for example, the mesh archetype <ref> [9] </ref> builds on Brinch Hansen's work on parallel cellular automata in the context of multicomputers [10]. In that paper, the computational complexity of parallel cellular automata is derived and shown to be a sufficiently accurate estimator of the performance of a Laplace's equation solver.
Reference: [10] <author> P. Brinch Hansen. </author> <title> Parallel cellular automata: A model program for computational science. </title> <journal> Con-currency: Practice and Experience, </journal> <volume> 5(5) </volume> <pages> 425-448, </pages> <year> 1993. </year>
Reference-contexts: Our techniques fit in well with other methodologies for dealing with applications developed for particular architectures (e.g., [8]). Archetypes frequently represent well-researched patterns or abstractions; for example, the mesh archetype [9] builds on Brinch Hansen's work on parallel cellular automata in the context of multicomputers <ref> [10] </ref>. In that paper, the computational complexity of parallel cellular automata is derived and shown to be a sufficiently accurate estimator of the performance of a Laplace's equation solver.
Reference: [11] <author> M. I. Cole. </author> <title> Algorithmic Skeletons: Structured Management of Parallel Computation. </title> <publisher> MIT Press, </publisher> <year> 1989. </year>
Reference-contexts: In this paper, we provide an alternative technique for performance estimation using a combination of benchmarking and analysis that is especially suited to applications developed using archetypes. 2.1 Methodology Much work has been done on methods of exploiting patterns in program development (e.g., <ref> [11] </ref>, [12]). Methods of exploiting patterns in program development begin by identifying classes of problems with similar computational structures and creating abstractions that capture the commonality. Combining a problem class's computational structure with a parallelization strategy gives rise to a dataflow pattern and hence a communication structure.
Reference: [12] <author> E. Gamma, R. Helm, R. Johnson, and J. Vlissides. </author> <title> Design Patterns: Elements of Reusable Object-Oriented Software. </title> <publisher> Addison-Wesley, </publisher> <year> 1995. </year>
Reference-contexts: In this paper, we provide an alternative technique for performance estimation using a combination of benchmarking and analysis that is especially suited to applications developed using archetypes. 2.1 Methodology Much work has been done on methods of exploiting patterns in program development (e.g., [11], <ref> [12] </ref>). Methods of exploiting patterns in program development begin by identifying classes of problems with similar computational structures and creating abstractions that capture the commonality. Combining a problem class's computational structure with a parallelization strategy gives rise to a dataflow pattern and hence a communication structure.
Reference: [13] <author> B. L. Massingill. </author> <title> Structured parallel programming. </title> <type> Technical Report CS-TR-98-04, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1998. </year> <type> PhD thesis. </type>
Reference-contexts: Combining a problem class's computational structure with a parallelization strategy gives rise to a dataflow pattern and hence a communication structure. It is this combination of computational structure, parallelization strategy, and the implied pattern of dataflow and communication that is captured by a parallel programming archetype <ref> [13, 14] </ref>. A key question in the development of a parallel application, especially for a multi-computer or a network of computers, is the issue of data decomposition and distribution. Archetypes directly address the question of which data distributions are compatible with a problem's computational structure.
Reference: [14] <author> B. L. Massingill and K. M. Chandy. </author> <title> Parallel program archetypes. </title> <type> Technical Report CS-TR-96-28, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: Combining a problem class's computational structure with a parallelization strategy gives rise to a dataflow pattern and hence a communication structure. It is this combination of computational structure, parallelization strategy, and the implied pattern of dataflow and communication that is captured by a parallel programming archetype <ref> [13, 14] </ref>. A key question in the development of a parallel application, especially for a multi-computer or a network of computers, is the issue of data decomposition and distribution. Archetypes directly address the question of which data distributions are compatible with a problem's computational structure.
Reference: [15] <author> A. Rifkin. </author> <note> Parallel archetypes. In P.B. Gibbons, R.M. </note> <editor> Karp, C.E. Leiserson, and G.M. Papadopou-los, editors, </editor> <booktitle> DIMACS Workshop on Models, Architectures, and Technologies for Parallel Computation, </booktitle> <pages> pages 286-293, </pages> <month> September </month> <year> 1993. </year>
Reference: [16] <author> A. Rifkin and B. L. Massingill. </author> <title> Performance analysis for mesh and mesh-spectral archetype applications. </title> <type> Technical Report CS-TR-96-27, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: The performance model (and simulation strategy) may suggest a decision between archetypes, architectures, or libraries. 2.2 Experimental Method We evaluated our methodology and performance-modeling techniques using a suite of application programs, described in detail in the longer version of this paper <ref> [16] </ref>. We used the following approach to collecting and presenting experimental results. System specifications. <p> That is, "total" time is measured from program initiation to program termination, while "process" time is measured from process initiation to process termination. 3 Application example As noted in the preceding section, our experiments used various combinations of different archetypes, architectures, and languages/libraries. Full results are given in <ref> [16] </ref>; in this section we present one experiment, a simple Poisson solver, as an example of applying our performance model. <p> We obtained values for the computational and communication benchmarks described in the preceding section; see the longer version of this paper <ref> [16] </ref> for details. We then used these values and the equations in the preceding section to derive estimated execution times. We also measured actual execution times for our Poisson solver. with actual execution times. <p> Other experimental results (see <ref> [16] </ref>) confirm that execution times as predicted by our model are often conservative when compared to observed execution times, though in some cases the two are very close. <p> Based on the experiments described in Section 3 and in <ref> [16] </ref>, we believe that we have met that goal. We conclude that the model, though simple, can be of practical value in application development. Future work could investigate (i) a wider range of archetypes and (ii) the applicability of the methodology to shared-memory architectures.
Reference: [17] <author> I. T. Foster and K. M. Chandy. </author> <title> FORTRAN M: A language for modular parallel programming. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 26(1) </volume> <pages> 24-35, </pages> <year> 1995. </year>
Reference-contexts: Our experiments were conducted using various combinations of two different archetypes (the mesh and mesh-spectral), two different architectures (an IBM SP2 at Argonne National Labs using a straight interconnect, and a network of 166 MHz Pentium personal computers connected by 100Mbps ethernet), and two different languages/libraries (Fortran M <ref> [17] </ref> and Fortran with MPI [18]). Environment. Experiments were performed on otherwise unloaded computer nodes, but in an environment in which communications hardware was also supporting other users. Since execution times were consistent across multiple runs, we assume that this sharing of communications hardware did not greatly affect our results.
Reference: [18] <author> M. Snir, S. Otto, S. Huss-Lederman, D. Walker, and J. Dongarra. </author> <title> MPI: The Complete Reference. </title> <publisher> The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: conducted using various combinations of two different archetypes (the mesh and mesh-spectral), two different architectures (an IBM SP2 at Argonne National Labs using a straight interconnect, and a network of 166 MHz Pentium personal computers connected by 100Mbps ethernet), and two different languages/libraries (Fortran M [17] and Fortran with MPI <ref> [18] </ref>). Environment. Experiments were performed on otherwise unloaded computer nodes, but in an environment in which communications hardware was also supporting other users. Since execution times were consistent across multiple runs, we assume that this sharing of communications hardware did not greatly affect our results. Measurement of execution times.
Reference: [19] <author> E. F. Van de Velde. </author> <title> Concurrent Scientific Computing. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Full results are given in [16]; in this section we present one experiment, a simple Poisson solver, as an example of applying our performance model. The application, based on the discussion of the Poisson problem in <ref> [19] </ref>, finds a numerical solution to the equation @x 2 @y 2 = f (x; y) with Dirichlet boundary condition u (x; y) = g (x; y) using discretization and Jacobi iteration; that is, by discretizing the problem domain and applying the following operation to all interior points until convergence is
Reference: [20] <author> G. Davis and B. L. Massingill. </author> <title> The mesh-spectral archetype. </title> <type> Technical Report CS-TR-96-26, </type> <institution> Computer Science Department, California Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: At each step values are copied from ukp1 to uk 1 . Stepsize and convergence tolerance are read in from a file at runtime. Our parallel version of the application is based on the mesh-spectral archetype <ref> [20] </ref>.
References-found: 20

