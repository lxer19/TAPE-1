URL: http://www.cs.iastate.edu/tech-reports/TR92-37.ps
Refering-URL: http://www.cs.iastate.edu/tech-reports/catalog.html
Root-URL: http://www.cs.iastate.edu
Title: The Global Power of Additional Queries to Random Oracles  
Author: TR - Jack H. Lutz and David M. Martin Jr. 
Address: 226 Atanasoff Ames, IA 50011  
Affiliation: Iowa State University of Science and Technology Department of Computer Science  
Date: February 9, 1993  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> V. Arvind, J. Kobler, and M. Mundhenk, </author> <title> On conjunctive, randomized, and nondeterministic reductions to sets of high information content, </title> <booktitle> 1992, Structure Abstracts. </booktitle>
Reference-contexts: 1 Introduction The study of efficient reductions to languages of low information content has yielded interesting results in complexity theory. (See [4, 10] for surveys of such work). Recent work (e.g., <ref> [5, 6, 1] </ref>) indicates that a study of efficient reductions to languages with high information content will be useful as well.
Reference: [2] <author> R. Book and K.-I Ko, </author> <title> On sets truth-table reducible to sparse sets, </title> <journal> SIAM Journal on Computing 17 (1988), </journal> <pages> pp. 903-919. </pages>
Reference-contexts: This is much stronger than the local result that, for all A 2 RAND, P k-tt (A) $ P (k+1)-tt (A) [3]. Note that our main theorem closely parallels the global result by Book and Ko <ref> [2] </ref> that, for every k 1, P k-tt (SPARSE) $ P (k+1)-tt (SPARSE); where SPARSE is the set of all sparse languages, i.e., languages with a polynomial limit on the number of strings of each length. (Sparse languages are well known to have very low information content.) This paper is presented <p> We identify bit positions in a binary string by counting from the left. Therefore, if x 2 f0; 1g fl , then x = x [0::jxj 1] and if x 2 f0; 1g 1 , then x = x [0]x [1]x <ref> [2] </ref> : For a set A f0; 1g fl , define the complement of A to be A c = f0; 1g fl A. Similarly, for a set X f0; 1g 1 , define X c = f0; 1g 1 X.
Reference: [3] <author> R. V. </author> <title> Book, Additional queries and algorithmically random languages, </title> <editor> In K. Ambos-Spies, S. Homer, and U. Schoning, editors, </editor> <booktitle> Complexity Theory. </booktitle> <address> Cambridge University Press, </address> <note> to appear. </note>
Reference-contexts: This is much stronger than the local result that, for all A 2 RAND, P k-tt (A) $ P (k+1)-tt (A) <ref> [3] </ref>.
Reference: [4] <author> R. V. </author> <title> Book, On sets with small information content, </title> <editor> In O. Watanabe, editor, </editor> <booktitle> Kolmogorov Complexity and Computational Complexity, </booktitle> <pages> pp. 21-42. </pages> <publisher> Springer-Verlag, </publisher> <year> 1992. </year> <month> 12 </month>
Reference-contexts: 1 Introduction The study of efficient reductions to languages of low information content has yielded interesting results in complexity theory. (See <ref> [4, 10] </ref> for surveys of such work). Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well.
Reference: [5] <author> R. V. Book and J. H. Lutz, </author> <title> On languages with very high space-bounded Kolmogorov complexity, </title> <note> SIAM Journal on Computing (1993), to appear. </note>
Reference-contexts: 1 Introduction The study of efficient reductions to languages of low information content has yielded interesting results in complexity theory. (See [4, 10] for surveys of such work). Recent work (e.g., <ref> [5, 6, 1] </ref>) indicates that a study of efficient reductions to languages with high information content will be useful as well.
Reference: [6] <author> R. V. Book, J. H. Lutz, and K. Wagner, </author> <title> An observation on probability versus randomness with applications to complexity classes, </title> <note> Mathematical Systems Theory (1993), to appear. </note>
Reference-contexts: 1 Introduction The study of efficient reductions to languages of low information content has yielded interesting results in complexity theory. (See [4, 10] for surveys of such work). Recent work (e.g., <ref> [5, 6, 1] </ref>) indicates that a study of efficient reductions to languages with high information content will be useful as well.
Reference: [7] <author> G. J. Chaitin, </author> <title> A theory of program size formally identical to information theory, </title> <journal> Journal of the Association for Computing Machinery 22 (1975), </journal> <pages> pp. 329-340. </pages>
Reference-contexts: Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well. The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin [11], Schnorr [14], Chaitin <ref> [7] </ref>, Solovay [17], and Shen 0 [15, 16]. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of random languages, the class
Reference: [8] <author> G. J. Chaitin, </author> <title> Incompleteness theorems for random reals, </title> <booktitle> Advances in Applied Mathematics 8 (1987), </booktitle> <pages> pp. 119-146. </pages>
Reference: [9] <author> T. Hagerup and C. Rub, </author> <title> A guided tour of Chernoff bounds, </title> <booktitle> Information Processing Letters 33 (1990), </booktitle> <pages> pp. 305-308. </pages>
Reference: [10] <author> L. A. Hemachandra, M. Ogiwara, and O. Watanabe, </author> <title> How hard are sparse sets?, </title> <booktitle> Proceedings of the Seventh Annual Structure in Complexity Theory Conference, </booktitle> <year> 1992, </year> <pages> pp. 222-238. </pages> <publisher> IEEE Press. </publisher>
Reference-contexts: 1 Introduction The study of efficient reductions to languages of low information content has yielded interesting results in complexity theory. (See <ref> [4, 10] </ref> for surveys of such work). Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well.
Reference: [11] <author> L. A. Levin, </author> <title> On the notion of a random sequence, </title> <journal> Soviet Mathematics Doklady 14 (1973), </journal> <pages> pp. 1413-1416. </pages>
Reference-contexts: Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well. The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin <ref> [11] </ref>, Schnorr [14], Chaitin [7], Solovay [17], and Shen 0 [15, 16]. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of
Reference: [12] <author> J. H. Lutz, </author> <title> Almost everywhere high nonuniform complexity, </title> <journal> Journal of Computer and System Sciences 44 (1992), </journal> <pages> pp. 220-258. </pages>
Reference: [13] <author> P. Martin-Lof, </author> <title> On the definition of random sequences, </title> <booktitle> Information and Control 9 (1966), </booktitle> <pages> pp. 602-619. </pages>
Reference-contexts: Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well. The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof <ref> [13] </ref>, Levin [11], Schnorr [14], Chaitin [7], Solovay [17], and Shen 0 [15, 16]. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be <p> Definition <ref> [13] </ref>.
Reference: [14] <author> C. P. Schnorr, </author> <title> Process complexity and effective random tests, </title> <journal> Journal of Computer and System Sciences 7 (1973), </journal> <pages> pp. 376-388. </pages>
Reference-contexts: Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well. The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin [11], Schnorr <ref> [14] </ref>, Chaitin [7], Solovay [17], and Shen 0 [15, 16]. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of random languages,
Reference: [15] <author> A. Kh. </author> <title> Shen 0 , The frequency approach to the definition of a random sequence, </title> <booktitle> Semiotika i Informatika (1982), </booktitle> <pages> pp. 14-42. </pages>
Reference-contexts: The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin [11], Schnorr [14], Chaitin [7], Solovay [17], and Shen 0 <ref> [15, 16] </ref>. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of random languages, the class of problems that can be solved
Reference: [16] <author> A. Kh. </author> <title> Shen 0 , On relations between different algorithmic definitions of randomness, </title> <journal> Soviet Mathematics Doklady 38 (1989), </journal> <pages> pp. 316-319. </pages>
Reference-contexts: The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin [11], Schnorr [14], Chaitin [7], Solovay [17], and Shen 0 <ref> [15, 16] </ref>. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of random languages, the class of problems that can be solved
Reference: [17] <author> R. M. Solovay, </author> <year> 1975, </year> <note> reported in [8]. </note>
Reference-contexts: Recent work (e.g., [5, 6, 1]) indicates that a study of efficient reductions to languages with high information content will be useful as well. The languages with maximum information content are the algorithmically random languages in the equivalent senses of Martin-Lof [13], Levin [11], Schnorr [14], Chaitin [7], Solovay <ref> [17] </ref>, and Shen 0 [15, 16]. (See Section 3 for a precise definition and basic properties of algorithmic randomness.) In this paper, we show that access to such languages lends computational power in the sense that as we allow more questions to be asked of random languages, the class of problems
Reference: [18] <author> M. van Lambalgen, </author> <title> Random Sequences, </title> <type> PhD thesis, </type> <institution> Department of Mathematics, University of Amsterdam, </institution> <year> 1987. </year> <month> 13 </month>
Reference: [19] <author> M. van Lambalgen, </author> <title> Von Mises' definition of random sequences recon-sidered, </title> <journal> Journal of Symbolic Logic 52 (1987), </journal> <pages> pp. 725-755. 14 </pages>
References-found: 19

