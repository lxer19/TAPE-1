URL: ftp://ftp.cis.upenn.edu/pub/msingh/aistat95.ps.Z
Refering-URL: http://www.cis.upenn.edu/~msingh/frames/papers_list.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: &lt; provan@camis.stanford.edu &gt;  &lt; msingh@gradient.cis.upenn.edu &gt;  
Title: Learning Bayesian Networks Using Feature Selection  
Author: Gregory M. Provan Moninder Singh 
Address: 4984 El Camino Real, Los Altos, CA, 94022  Philadelphia, PA 19104-6389  
Affiliation: Institute for Decision Systems Research  Dept. of Computer and Information Science University of Pennsylvania  
Abstract: This paper introduces a novel enhancement for learning Bayesian networks with a bias for small, high-predictive-accuracy networks. The new approach selects a subset of features which maximizes predictive accuracy prior to the network learning phase. We examine explicitly the effects of two aspects of the algorithm, feature selection and node ordering. Our approach generates networks which are com-putationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all attributes.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.W. Aha and R.L. Bankert. </author> <title> Feature selection for case-based classification of cloud types. </title> <booktitle> In AAAI Workshop on Case-based Reasoning, </booktitle> <pages> pages 106-112, </pages> <address> Seattle, WA, 1994. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Irvine database) have a relatively small number of features, namely the features have been pre-selected for their relevance. It is expected that in such domains feature selection may not make a significant impact. One exception is the study of cloud classification by Aha and Bankert <ref> [1] </ref>, in which a set of 204 attributes were significantly pruned, leading the greatly improved performance. 10 Better understanding of data sets and of domains may lead to a deeper understanding of the role of feature selection, and improved performance from feature selection algorithms.
Reference: [2] <author> H. Amuallim and T.G. Dietterich. </author> <title> Learning with Many Irrelevant Features. </title> <booktitle> In Proc. Conf. of the AAAI, </booktitle> <pages> pages 547-552. </pages> <publisher> AAAI Press, </publisher> <address> Menlo Park, CA, </address> <year> 1991. </year>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the FOCUS algorithm <ref> [2] </ref> the Relief algorithm [14, 15] (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5].
Reference: [3] <author> S.K. Andersen, K.G. Olesen, F.V. Jensen, and F. Jensen. </author> <title> HUGIN|a Shell for Building Belief Universes for Expert Systems. </title> <booktitle> In Proc.IJCAI, </booktitle> <pages> pages 1080-1085, </pages> <year> 1989. </year>
Reference-contexts: The third part of the database, the test data, is then used for determining the predictive accuracy of the network derived from the network construction phase. We performed inference on the networks using the Lauritzen-Spiegelhalter inference algorithm as implemented in the HUGIN <ref> [3] </ref> system. 3 The K2-AS approach trades off the time required to construct a network from the full feature set (as done in K2) with precomputing a feature subset and subsequently constructing a network with this feature subset.
Reference: [4] <author> W.L. Buntine and T. Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 7, </volume> <year> 1992. </year>
Reference-contexts: Instead, the purpose of this paper is not to identify the Bayesian network with the highest predictive accuracy, but to identify a parsimonious model with good predictive accuracy. It is possible to compute multiple models and average over them (e.g. as proposed in <ref> [19, 4] </ref>) to obtain the best predictive accuracy, and we hope to take this approach in future work. In addition, we restrict our attention to Bayesian networks.
Reference: [5] <author> C. Cardie. </author> <title> Using Decision Trees to Improve Case-based Learning. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> pages 25-32. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm [14, 15] (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm <ref> [5] </ref>. Wrapper-based approaches have been studied in [13, 6, 18], among others. 9 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [6] <author> R. Caruana and D. Freitag. </author> <title> Greedy attribute selection. </title> <editor> In W. Cohen and H. Hirsch, editors, </editor> <booktitle> Proc. Machine Learning, </booktitle> <pages> pages 28-36. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Our experimental results verify that this approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [6, 13, 17, 18] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 15% of the features for the gene-splice domain, we obtained a predictive accuracy of 94.8% (as opposed to 96.8% with all features). <p> Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm [14, 15] (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5]. Wrapper-based approaches have been studied in <ref> [13, 6, 18] </ref>, among others. 9 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [7] <author> P. Cheeseman and W. Oldford, </author> <title> editors. Selecting Models from Data: AI and Statistics IV. </title> <publisher> Springer-Verlag, </publisher> <year> 1994. </year>
Reference: [8] <author> G.F. Cooper and E. Herskovits. </author> <title> A Bayesian Method for the Induction of of Probabilistic Networks from Data. </title> <booktitle> In Machine Learning 9, </booktitle> <pages> pages 54-62, </pages> <publisher> Kluwer, </publisher> <year> 1992. </year>
Reference-contexts: 1 INTRODUCTION Bayesian networks are being increasingly recognized as an important representation for probabilistic reasoning. For many domains, the need to specify the probability distributions for a Bayesian network is considerable, and learning these probabilities from data using an algorithm like K2 <ref> [8] </ref> 1 could alleviate such specification difficulties. We describe an extension to the Bayesian network learning approaches introduced in K2. Rather than use all database features (or attributes) for constructing the network, we select a subset of features that maximize the predictive accuracy of the network. <p> In a belief network, a node represents a feature, and the absence of an arc between two nodes denotes the independence of the two nodes given the remaining network structure. The posterior prob 2 We use the nomenclature used in the papers on K2 by Herskovits and Cooper <ref> [12, 8] </ref>. ability of a network given the data, P (B S jD), is proportional to the joint probability, so networks can be ranked according to their joint probabilities.
Reference: [9] <author> A.P. Dawid. </author> <title> Prequential Analysis, Stochastic Complexity and Bayesian Inference. </title> <editor> In J.M. Bernardo, J. Berger, A. Dawid, and A. Smith, editors, </editor> <booktitle> Bayesian Statistics 4, </booktitle> <pages> pages 109-125. </pages> <publisher> Oxford Science Publications, </publisher> <year> 1992. </year>
Reference-contexts: This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL. For example, Dawid discusses the close relation between subset selection and the MDL principle in <ref> [9] </ref>. The computer vision community has studied feature selection for over 20 years [10], and has formed a testing the effect of node orderings sub-community within computational vision called pattern recognition.
Reference: [10] <author> P. Dejviver and J. Kittler. </author> <title> Pattern Recognition: A Statistical Approach. </title> <publisher> Prentice-Hall, </publisher> <year> 1982. </year>
Reference-contexts: This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL. For example, Dawid discusses the close relation between subset selection and the MDL principle in [9]. The computer vision community has studied feature selection for over 20 years <ref> [10] </ref>, and has formed a testing the effect of node orderings sub-community within computational vision called pattern recognition.
Reference: [11] <author> E. Herskovits. </author> <title> Computer-based probabilistic-network construction. </title> <type> Doctoral dissertation, </type> <institution> Medical Information Sciences, Stanford University, Stanford, </institution> <address> CA., </address> <year> 1991 </year>
Reference-contexts: This was particularly true for the network for the chess domain, which is very densely connected. Figures 1 and 2 show the learning curves for the 6 The best results we ever got for the Soybean domain with CB were 86%. Herskovits <ref> [11] </ref>, even with his multiscore algorithm (using multiple networks for inference), got about 86%. As a point of comparison, in the chess endgame domain decision trees are able to obtain 99% accuracy.
Reference: [12] <author> E. Herskovits and G.F. Cooper. KUTATO: </author> <title> An Entropy-Driven System for Construction of Probabilistic Expert Systems from Databases. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 54-62, </pages> <year> 1990. </year>
Reference-contexts: We examine explicitly the fl This work was supported by NSF grant #IRI92-10030, and NLM grant #BLR 3 RO1 LMO5217-02S1. 1 K2 is a Bayesian reformulation of the Kutato learning algorithm <ref> [12] </ref>. effects of two aspects of the algorithm: (a) feature selection, and (b) node ordering. Our experimental results verify that this approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all features. <p> In a belief network, a node represents a feature, and the absence of an arc between two nodes denotes the independence of the two nodes given the remaining network structure. The posterior prob 2 We use the nomenclature used in the papers on K2 by Herskovits and Cooper <ref> [12, 8] </ref>. ability of a network given the data, P (B S jD), is proportional to the joint probability, so networks can be ranked according to their joint probabilities.
Reference: [13] <author> G. John, R. Kohavi, and K. Pfleger. </author> <title> Irrelevant features and the subset selection problem. </title> <editor> In W. Cohen and H. Hirsch, editors, </editor> <booktitle> Proc. Machine Learning, </booktitle> <pages> pages 121-129. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1994. </year>
Reference-contexts: Our experimental results verify that this approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [6, 13, 17, 18] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 15% of the features for the gene-splice domain, we obtained a predictive accuracy of 94.8% (as opposed to 96.8% with all features). <p> We call this approach K2-AS, since it uses the basic K2 algorithm allied with Attribute Selection in the node selection phase. The algorithm we use is what has been described as a wrapper model <ref> [13] </ref>, in that "the feature subset selection algorithms conducts a search for a good subset using the induction algorithm itself as part of the evaluation function" [13, page 124]. Our learning approach consists of two main steps, node selection and network construction. <p> The algorithm we use is what has been described as a wrapper model [13], in that "the feature subset selection algorithms conducts a search for a good subset using the induction algorithm itself as part of the evaluation function" <ref> [13, page 124] </ref>. Our learning approach consists of two main steps, node selection and network construction. In the node selection phase, we choose the set of nodes from which the final network is constructed. <p> Feature selection has received considerable attention in the last few years within the computational learning community, using both filter-based and wrapper-based approaches <ref> [13] </ref>. A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. <p> Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm [14, 15] (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5]. Wrapper-based approaches have been studied in <ref> [13, 6, 18] </ref>, among others. 9 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [14] <author> K. Kira and L. Rendell. </author> <title> A practical approach to feature selection. </title> <booktitle> In Proc. Machine Learning, </booktitle> <pages> pages 249-256, </pages> <address> Aberdeen, Scotland, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm <ref> [14, 15] </ref> (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5].
Reference: [15] <author> K. Kira and L. Rendell. </author> <title> The Feature Selection Problem: Traditional Methods and a New Algorithm. </title> <booktitle> In Proc. AAAI, </booktitle> <pages> pages 129-134, </pages> <address> Min-neapolis, MN, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A filter model filters out less relevant features using an algorithm different from the induction algorithm used for the learning, and a wrapper model uses induction algorithm itself for feature selection. Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm <ref> [14, 15] </ref> (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5].
Reference: [16] <author> I. Kononenko. </author> <title> Estimating attributes: Analysis and extension of relief. </title> <booktitle> In Proc. European Conf. on Machine Learning, </booktitle> <pages> pages 171-182. </pages> <publisher> Springer Verlag, </publisher> <year> 1994. </year>
Reference-contexts: Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm [14, 15] (which Kononenko has extended in <ref> [16] </ref>), and an extended nearest-neighbor algorithm [5]. Wrapper-based approaches have been studied in [13, 6, 18], among others. 9 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [17] <author> P. Langley. </author> <title> Selection of relevant features in machine learning. </title> <editor> In R. Greiner, editor, </editor> <booktitle> Proc. AAAI Fall Symposium on Relevance. </booktitle> <publisher> AAAI Press, </publisher> <year> 1994. </year>
Reference-contexts: Our experimental results verify that this approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [6, 13, 17, 18] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 15% of the features for the gene-splice domain, we obtained a predictive accuracy of 94.8% (as opposed to 96.8% with all features). <p> Many domains studied (for example, the domains described in the University of California, 9 Langley <ref> [17] </ref> presents a thorough review of feature selection approaches studied within the Machine Learning literature. Irvine database) have a relatively small number of features, namely the features have been pre-selected for their relevance. It is expected that in such domains feature selection may not make a significant impact.
Reference: [18] <author> P. Langley and S. Sage. </author> <title> Induction of selective bayesian classifiers. </title> <booktitle> In Proc. Conf. on Uncertainty in AI, </booktitle> <pages> pages 399-406. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1994. </year>
Reference-contexts: Our experimental results verify that this approach generates networks which are computationally simpler to evaluate and which display predictive accuracy comparable to that of Bayesian networks which model all features. Our results, similar to those observed by other studies of feature selection in learning <ref> [6, 13, 17, 18] </ref>, demonstrate that feature selection provides comparable predictive accuracy using smaller networks. For example, by selecting as few as 15% of the features for the gene-splice domain, we obtained a predictive accuracy of 94.8% (as opposed to 96.8% with all features). <p> Three filter-model approaches that have been taken are: the FOCUS algorithm [2] the Relief algorithm [14, 15] (which Kononenko has extended in [16]), and an extended nearest-neighbor algorithm [5]. Wrapper-based approaches have been studied in <ref> [13, 6, 18] </ref>, among others. 9 A growing consensus in this research is that the success of feature selection is strongly correlated to the data itself, as well as to the algorithm employed.
Reference: [19] <author> D Madigan, A. Raftery, J. York, J. Brad-shaw, and R. Almond. </author> <title> Strategies for Graphical Model Selection. </title> <booktitle> In Proc. International Workshop on AI and Statistics, </booktitle> <pages> pages 331-336, </pages> <year> 1993. </year>
Reference-contexts: Instead, the purpose of this paper is not to identify the Bayesian network with the highest predictive accuracy, but to identify a parsimonious model with good predictive accuracy. It is possible to compute multiple models and average over them (e.g. as proposed in <ref> [19, 4] </ref>) to obtain the best predictive accuracy, and we hope to take this approach in future work. In addition, we restrict our attention to Bayesian networks.
Reference: [20] <author> T. Marill and D. Green. </author> <title> On the effectiveness of receptors in recognition systems. </title> <journal> IEEE Trans. on Information Theory, </journal> <volume> 9 </volume> <pages> 11-17, </pages> <year> 1963. </year>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection <ref> [20] </ref>, branch&bound [22], and search algorithms [23, 25]. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[7], and contains a large number of papers on feature selection.
Reference: [21] <author> P.M. Murphy and D.W. Aha. </author> <title> UCI Repository of Machine Learning Databases. Machine-readable data repository, </title> <institution> Dept. of Information and Computer Science, Univ. of California, Irvine. </institution>
Reference-contexts: We tested this method on four databases acquired from the University of California, Irvine Repository of Machine Learning databases <ref> [21] </ref>, namely Michalski's Soybean database, Slate's Letter Recognition database, the Gene-Splicing database due to Towell, Noordewier, and Shavlik, 4 and Shapiro's Chess Endgame database. Table 2: Comparison of predictive accuracy for the basic CB approach and for K2-AS.
Reference: [22] <author> M. Narendra and K. Fukunaga. </author> <title> A branch and bound algorithm for feature subset selection. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-26(9):917-922, </volume> <year> 1977. </year>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [20], branch&bound <ref> [22] </ref>, and search algorithms [23, 25]. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[7], and contains a large number of papers on feature selection.
Reference: [23] <author> W. Siedlecki and J. Sklansky. </author> <title> On automatic feature selection. </title> <journal> Itnl. J. of Pattern Recognition and Artificial Intelligence, </journal> <volume> 2(2) </volume> <pages> 197-220, </pages> <year> 1988. </year>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [20], branch&bound [22], and search algorithms <ref> [23, 25] </ref>. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[7], and contains a large number of papers on feature selection. This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL.
Reference: [24] <author> M. Singh and M. Valtorta. </author> <title> Bayesian Network Structures from Data. </title> <booktitle> In Proc. Conf. Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 259-265, </pages> <publisher> Morgan-Kaufmann Publishers, </publisher> <year> 1993. </year>
Reference-contexts: The second phase computes the network (from the set of features ) which maximizes the predictive accuracy over the test data. The learning algorithm that we use, called CB, is a modified version of K2 <ref> [24] </ref>. Whereas K2 assumes a node ordering, CB uses conditional independence (CI) tests to generate a "good" node ordering, and then uses the K2 algorithm to generate the Bayesian network from the database D using this node ordering. <p> Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, CB is correct in the same sense that K2 is <ref> [24] </ref>. Singh and Valtorta show the importance of deriving a good node ordering [24], given the n! possible node orderings on n features. 3 Feature Selection Algorithm We implemented the Feature Selection Algorithm using the CB algorithm in both the node selection as well as the network construction phase. <p> Since CB uses the K2 algorithm to generate the Bayesian network from a particular ordering, CB is correct in the same sense that K2 is <ref> [24] </ref>. Singh and Valtorta show the importance of deriving a good node ordering [24], given the n! possible node orderings on n features. 3 Feature Selection Algorithm We implemented the Feature Selection Algorithm using the CB algorithm in both the node selection as well as the network construction phase.
Reference: [25] <author> L. Xu, P. Yan, and T. Chang. </author> <title> Best-first strategy for feature selection. </title> <booktitle> In Proc. Ninth International Conf. on Pattern Recognition, </booktitle> <pages> pages 706-708. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1989. </year>
Reference-contexts: In statistics, research on feature selection has focused primarily on selecting a subset of features within linear regression. Techniques developed include sequential backward selection [20], branch&bound [22], and search algorithms <ref> [23, 25] </ref>. A 1993 meeting of the Society of AI and Statistics was dedicated to papers on "Selecting Models from Data"[7], and contains a large number of papers on feature selection. This statistical approach to subset selection shares many principles with other statistical notions of information minimality, like MDL.
References-found: 25

