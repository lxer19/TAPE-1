URL: ftp://synapse.cs.byu.edu/pub/papers/rudolph_95d.ps
Refering-URL: ftp://synapse.cs.byu.edu/pub/papers/details.html
Root-URL: 
Email: e-mail: george@axon.cs.byu.edu, martinez@cs.byu.edu  
Title: Word Perfect Corp. A TRANSFORMATION FOR IMPLEMENTING NEURAL NETWORKS WITH LOCALIST PROPERTIES  
Author: George L. Rudolph Tony R. Martinez 
Keyword: Artificial Neural Networks, Hardware Implementation Design, Dynamic Topologies  
Address: Provo, Utah 84602  
Affiliation: Computer Science Department Brigham Young University  
Note: Intelligent Systems, E. A. Yfantis (ed.), Vol. 1, pp. 637-645, Kluwer Academic Publishers, 1995. This research is funded by grants from Novell Inc. and  
Abstract: Most Artificial Neural Networks (ANNs) have a fixed topology during learning, and typically suffer from a number of shortcomings as a result. Variations of ANNs that use dynamic topologies have shown ability to overcome many of these problems. This paper introduces Location-Independent Transformations (LITs) as a general strategy for implementing feedforward networks that use dynamic topologies. A LIT creates a set of location-independent nodes, where each node computes its part of the network output independent of other nodes, using local information. This type of transformation allows efficient support for adding and deleting nodes dynamically during learning. In particular, this paper presents LITs for the single-layer competitve learning network, and the counterpropagation network, which combines elements of supervised learning with competitive learning. These two networks are localist in the sense that ultimately one node is responsible for each output. LITs for other models are presented in other papers. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Almasi, G., A. Gottlieb. </author> <title> Highly Parallel Computing. </title> <address> Redwood City, CA: </address> <publisher> The Benjamin/Cummings Publishing Company, Inc. </publisher> <year> 1989. </year>
Reference-contexts: Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not Of course, general parallel machines, like the Connection Machine [8] and the CRAY <ref> [1] </ref>, can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation.
Reference: [2] <author> DARPA. </author> <title> Neural Network Study. </title> <publisher> AFCEA International Press, </publisher> <year> 1988. </year>
Reference-contexts: These features allow ANNs to solve various applications not handled well by current conventional computational mechanisms. Application areas include, but are not limited to, problems requiring learning, such as pattern recognition, control and decision systems, speech, and signal analysis <ref> [2] </ref>. Hardware support for ANNs is important for handling large, complex problems in real time. Learning times can exceed tolerable limits for complex applications with conventional computing schemes. Furthermore, hardware is becoming cheaper and easier to design.
Reference: [3] <author> Fahlmann, Scott, C. Lebiere. </author> <booktitle> The Cascade-Correlation Learning Architecture. in Advances in Neural Information Processing 2. </booktitle> <pages> pp. 524-532. </pages> <publisher> Morgan Kaufmann Publishers: </publisher> <address> Los Altos, CA. </address>
Reference-contexts: static topologies typically suffer from the following short - comings: sensitivity to user-supplied parameters local error minima during learning no a priori mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems <ref> [3] </ref>, [9], [11], [13]. A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10].
Reference: [4] <author> Farhat, N., D. Psaltis, A. Prata, and E. Paek. </author> <title> Optical Implementation of the Hopfield Model. </title> <journal> Applied Optics, </journal> <volume> Vol. 24, #10. </volume> <month> pp.1469-1475 . </month> <year> 1985. </year>
Reference: [5] <author> Graf, H., L. Jackel, W. Hubbard. </author> <title> VLSI Implementation of a Neural Network Model. In Artificial Neural Networks: Electronic Implementations, </title> <publisher> Nelson Morgan, Ed. </publisher> <pages> pp. 34-42. </pages> <year> 1990. </year>
Reference: [6] <author> Hammerstrom, D., W. Henry, M. Kuhn. </author> <title> Neurocomputer System for Neural-Network Applications. In Parallel Digital Implementations of Neural Networks . K. </title> <editor> Przytula, V. Prasanna, Eds. </editor> <publisher> Prentice-Hall, Inc. </publisher> <year> 1991. </year>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs <ref> [6] </ref>, [12], [17].
Reference: [7] <author> Hecht-Nielsen, R. </author> <title> Counterpropagation Networks. </title> <journal> Applied Optics, </journal> <volume> Vol. 26, #23. </volume> <pages> pp. 4979 4984. </pages> <month> December, </month> <year> 1987. </year>
Reference-contexts: No other nodes are affected. 3. Transformation Of A Counterpropagation Network The goal of CPN is to learn approximations to input-output pairs presented to the network <ref> [7] </ref>. The original network is presented as a four-layer counterpropagation flow between five sets of nodes (figure 4). <p> In learning, only the winning nodes weights are adjusted, just as in the standard CL model. Hecht-Nielsens original equations allow for different learning constants for the x and y weights, but otherwise the equations for the two vectors are identical <ref> [7] </ref>. Each node receives the input, computes an activation (as above), and sends its activation and class up to its parent. The node with the highest activation is chosen, and its class is broadcast back to the network. The winning node then alters its weights on input and output appropriately.
Reference: [8] <author> Hillis, W. Daniel. </author> <title> The Connection Machine. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press, </publisher> <year> 1985. </year>
Reference-contexts: More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [12], [17]. Although some neurocomputers could potentially support dynamic topologies more directly in hardware, rather than in software, they currently do not Of course, general parallel machines, like the Connection Machine <ref> [8] </ref> and the CRAY [1], can simulate the desired dynamics in software, but these machines are not optimized for neural computation. LIT supports general classes of ANNs and dynamic topologies in an efficient parallel hardware implementation.
Reference: [9] <author> Martinez, T.R., D.M. Campbell. </author> <title> A Self-Adjusting Dynamic Logic Module. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> Vol. 11, #4. </volume> <pages> pp. 303-313. </pages> <year> 1991. </year>
Reference-contexts: topologies typically suffer from the following short - comings: sensitivity to user-supplied parameters local error minima during learning no a priori mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], <ref> [9] </ref>, [11], [13]. A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10]. <p> The deletion of a hidden node in the original network, such as node 5, is accomplished in the transformed network by marking the corresponding node as free. No other nodes are affected. LITs have been developed for backpropagation [15], and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 <ref> [9] </ref>, [14]. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support inherently dynamic ANNs and dynamic variations of many static ANNs.
Reference: [10] <author> Mead, Carver. </author> <title> Analog VLSI and Neural Systems. </title> <publisher> Addison-Wesley Publishing Company, Inc., </publisher> <year> 1989. </year>
Reference-contexts: A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], <ref> [10] </ref>. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [12], [17].
Reference: [11] <author> Odri, </author> <title> S.V., D.P. Petrovacki, G.A. Krstonosic. Evolutional Development of a Multilevel Neural Network. </title> <booktitle> Neural Networks, </booktitle> <volume> Vol. 6, </volume> <pages> #4 . pp. 583-595. </pages> <publisher> Pergamon Press Ltd.: </publisher> <address> New York. </address> <year> 1993. </year>
Reference-contexts: typically suffer from the following short - comings: sensitivity to user-supplied parameters local error minima during learning no a priori mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], [9], <ref> [11] </ref>, [13]. A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10].
Reference: [12] <author> Ramacher, U., W. Raab, J. Anlauf, U. Hachmann, J. Beichter, N. Brls, M. Weiling, E. Schneider, R. Mnner, J. Gl. </author> <title> Multiprocessor and Memory Architecture of the Neurocomputer SYNAPSE-1. </title> <booktitle> Proceedings, World Congress on Neural Networks 1993, </booktitle> <volume> Vol. 4. </volume> <pages> pp. 775-778. </pages> <publisher> INNS Press, </publisher> <year> 1993. </year>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], <ref> [12] </ref>, [17].
Reference: [13] <author> Reilly, D.L., L.N. Cooper, C. Elbaum. </author> <title> Learning Systems Based on Multiple Neural Networks. (Internal paper). Nestor, </title> <publisher> Inc. </publisher> <year> 1988. </year>
Reference-contexts: suffer from the following short - comings: sensitivity to user-supplied parameters local error minima during learning no a priori mechanism for deciding on an effective initial topology (number of nodes, number of layers, etc.) Current research is demonstrating the use of dynamic topologies in overcoming these problems [3], [9], [11], <ref> [13] </ref>. A dynamic topology is one which allows adding and deleting entire nodes and individual weighted connections during learning. Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10].
Reference: [14] <author> Rudolph G., and T.R. Martinez. </author> <title> An Efficient Static Topology for Modeling ASOCS. </title> <booktitle> International Conference on Artificial Neural Networks, </booktitle> <address> Helsinki, Finland. </address> <booktitle> In Artificial Neural Networks, Kohonen et al, </booktitle> <pages> pp. 729-734. </pages> <publisher> North Holland: Elsevier Publishers, </publisher> <year> 1991. </year>
Reference-contexts: The deletion of a hidden node in the original network, such as node 5, is accomplished in the transformed network by marking the corresponding node as free. No other nodes are affected. LITs have been developed for backpropagation [15], and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 [9], <ref> [14] </ref>. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support inherently dynamic ANNs and dynamic variations of many static ANNs.
Reference: [15] <author> Rudolph G., Martinez, T. R. </author> <title> An Efficient Transformation for Implementing Dynamic Backpropagation Networks. </title> <type> Technical Report BYU-CS-94-1. </type>
Reference-contexts: The deletion of a hidden node in the original network, such as node 5, is accomplished in the transformed network by marking the corresponding node as free. No other nodes are affected. LITs have been developed for backpropagation <ref> [15] </ref>, and Adaptive Self-Organizing Concurrent Systems Adaptive Algorithm 2 [9], [14]. Transformations for other important ANNs are also being developed. LITs can potentially support a broad set of ANNs, thus allowing one efficient implementation strategy to support inherently dynamic ANNs and dynamic variations of many static ANNs.
Reference: [16] <editor> Rumelhart, D., J. McClelland, et. al. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition, </booktitle> <volume> Vol. 1. </volume> <publisher> MIT Press, </publisher> <year> 1986 </year>
Reference-contexts: Section 4 is the conclusion. 2. Transformation Of A Competitive Learning Network The goal of the CL model is to spontaneously classify sets of similar inputs to the same class, and sets of different inputs into different classes, according to critical features discovered by the network <ref> [16] </ref>. The original CL has one output node for each output class, and weights from each input node to every output node. <p> The learning equation shows that a winning node will change its input weights to respond more strongly to the current input. The learning equation Dw ij = g (x i / v - w ij ) for the standard CL model <ref> [16] </ref> gives a very non-robust type of learning. Both the zero vector and the vector with all ones will always be put in the class with the lowest number, because in each case, the activations of all nodes is the same.
Reference: [17] <author> Shams, S. </author> <title> Dream MachineA Platform for Efficient Implementation of Neural Networks with Arbitrarily Complex Interconnect Structures. </title> <type> Technical Report CENG 92-23. PhD Dissertation, </type> <institution> USC, </institution> <year> 1992. </year>
Reference-contexts: Early ANN hardware implementations are modelspecific, and are intended to support only static topologies [4-5], [10]. More recent neurocomputer systems have specialized neural hardware, and seek to support more general classes of ANNs [6], [12], <ref> [17] </ref>.
References-found: 17

