URL: http://HTTP.CS.Berkeley.EDU/~christos/dm2.ps
Refering-URL: http://HTTP.CS.Berkeley.EDU/~christos/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Approximation Algorithms for Segmentation Problems (Extended Abstract for STOC 1998)  
Author: Jon Kleinberg Christos Papadimitriou Prabhakar Raghavan 
Date: November 14, 1997  
Abstract: We introduce and study a novel genre of optimization problems, which we call segmentation problems, motivated in part by certain aspects of clustering and data mining. For any classical optimization problem, the corresponding segmentation problem seeks to partition a set of cost vectors into several segments, so that the overall cost is optimized. We focus on two natural and interesting (but MAXSNP-complete) problems in this class, the hypercube segmentation problem and the catalog segmentation problem, and present approximation algorithms for them. We also present a general greedy scheme, which can be specialized to approximate any segmentation problem. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, A. </author> <title> Swami "Mining association rules between sets of items in a large database," </title> <booktitle> 1993 SIGMOD, </booktitle> <pages> pp. 207-216, </pages> <year> 1993. </year>
Reference-contexts: Formalizing what "interesting" means in this context has been an important problem in the data mining literature, which had not been addressed in a satisfactory way (see, e.g., <ref> [1, 17, 14, 16] </ref> for various attempts). Most research in data mining deals with the efficient discovery of patterns for subsequent human evaluation of the degree to which they are "interesting," and not on techniques for automatically evaluating mined patterns, or for automatically focusing on "interesting" patterns.
Reference: [2] <author> S. Arora, D. Karger, M. Karpinski, </author> <title> "Polynomial-time approximation schemes for dense instances of NP-hard problems," </title> <booktitle> Proc. ACM STOC, </booktitle> <year> 1995. </year> <month> 10 </month>
Reference-contexts: In Section 2.1 we give a natural sampling-based approximation scheme for the catalog segmentation problem. Somewhat surprisingly, the algorithm can only be shown to work under a fairly strong density assumption on instances (akin to the work of Arora, Karger and Karpinski <ref> [2] </ref>). Even in this case the analysis is non-trivial, and under slightly weaker assumptions on instances, the algorithm is known not to work. In Section 2.2, we analyze a natural greedy algorithm for the variable catalog segmentation problem, in which there is a fixed cost for each catalog produced.
Reference: [3] <author> O. Berman, M.J. Hodgson, D. Krass, </author> <title> "Flow-interception problems," in Facility Location: A Survey of Applications and Methods, </title> <editor> Z. Drezner, Ed., </editor> <publisher> Springer 1995. </publisher>
Reference-contexts: The maximization of a profit function is a basic NP-complete problem whose approximability appears not to be understood at all; the performance of greedy algorithms for profit functions was raised by Berman, Hodgson, and Krass as an open question <ref> [3] </ref>. In this section, we provide an analysis of the following greedy algorithm for profit functions arising from arbitrary monotone submodular functions. 6 Greedy algorithm for profit functions: At all times, maintain a candidate solution S.
Reference: [4] <author> M. Bern, D. Eppstein, </author> <title> "Approximation algorithms for geometric prolems," in Approximation Algorithms for NP-Hard Problems, </title> <editor> D. Hochbaum, Ed., </editor> <publisher> PWS Publishing, </publisher> <year> 1996. </year>
Reference-contexts: How is the quality of such a partition to be judged? There are numerous general-purpose criteria (minimizing the sum of the radii of the clusters, maximizing their distance, maximizing the weight of the edges cut, optimizing information-theoretic criteria, among many more <ref> [4, 5, 7, 8, 10] </ref>) but very little problem independent guidance on how to select the most appropriate one. But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness.
Reference: [5] <author> J.M. Coggins, </author> <title> "Dissimilarity measures for clustering strings," in Time Warps, String Edits, and Macromolecules: The Theory and Practice of Sequence Comparison, </title> <editor> D. Sankoff and J.S.B. Kruskal, Eds., </editor> <publisher> Addison-Wesley, </publisher> <year> 1983. </year>
Reference-contexts: How is the quality of such a partition to be judged? There are numerous general-purpose criteria (minimizing the sum of the radii of the clusters, maximizing their distance, maximizing the weight of the edges cut, optimizing information-theoretic criteria, among many more <ref> [4, 5, 7, 8, 10] </ref>) but very little problem independent guidance on how to select the most appropriate one. But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness.
Reference: [6] <author> G. Cornuejols, M. Fisher, G. Nemhauser, </author> <title> "Location of Bank Accounts to Optimize Float," </title> <journal> Management Science, </journal> <volume> 23(1977), </volume> <pages> pp. 789-810. </pages>
Reference-contexts: Our analysis is carried out at the more general level of an arbitrary monotone submodular function minus a fixed-cost function; it thus forms a natural parallel to results of Nemhauser, Wolsey, and Fisher <ref> [6, 13] </ref> on the maximization of monotone submodular functions, and addresses an open question raised by Berman, Hodgson, and Krass. In Section 3 we consider the hypercube segmentation problem, defined above. We wish to approximate the k-segmentation of largest possible value. <p> This phrasing of the problem resembles the maximization problem for monotone submodular functions, studied by Cornuejols, Fisher, and Nemhauser <ref> [6] </ref> and Nemhauser, Wolsey, and Fisher [13]; we drew a different but related connection to this work in Section 2.2. However, it turns out that segmentation functions need not be submodular.
Reference: [7] <author> T. Feder, D. Greene, </author> <title> "Optimal algorithms for approximate clustering," </title> <booktitle> Proc. ACM STOC, </booktitle> <year> 1988. </year>
Reference-contexts: How is the quality of such a partition to be judged? There are numerous general-purpose criteria (minimizing the sum of the radii of the clusters, maximizing their distance, maximizing the weight of the edges cut, optimizing information-theoretic criteria, among many more <ref> [4, 5, 7, 8, 10] </ref>) but very little problem independent guidance on how to select the most appropriate one. But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness.
Reference: [8] <author> T. Gonzalez, </author> <title> "Clustering to minimize the maximum inter-cluster distance, </title> <journal> Theoretical Computer Science, </journal> <volume> 38(1985), </volume> <pages> pp. 293-306. </pages>
Reference-contexts: How is the quality of such a partition to be judged? There are numerous general-purpose criteria (minimizing the sum of the radii of the clusters, maximizing their distance, maximizing the weight of the edges cut, optimizing information-theoretic criteria, among many more <ref> [4, 5, 7, 8, 10] </ref>) but very little problem independent guidance on how to select the most appropriate one. But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness.
Reference: [9] <author> A. K. Jain, R. C. Dubes, </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice-Hall, </publisher> <year> 1981. </year>
Reference-contexts: Segmentation problems also relate to clustering, an important, classical, and challenging algorithmic problem area <ref> [9] </ref>, of interest in data mining |which it predates. Suppose that a large set of points in a high-dimensional space must be partitioned into clusters.
Reference: [10] <author> M. Kearns, Y. Mansour, A. Ng, </author> <title> "An information-theoretic analysis of hard and soft assignment methods for clustering," </title> <booktitle> Proc. 13th Conference on Uncertainty in Artificial Intelligence, </booktitle> <year> 1997. </year>
Reference-contexts: How is the quality of such a partition to be judged? There are numerous general-purpose criteria (minimizing the sum of the radii of the clusters, maximizing their distance, maximizing the weight of the edges cut, optimizing information-theoretic criteria, among many more <ref> [4, 5, 7, 8, 10] </ref>) but very little problem independent guidance on how to select the most appropriate one. But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness.
Reference: [11] <author> J. Kleinberg, C. Papadimitriou, P. Raghavan, </author> <title> "A Microeconomic View of Data Mining," </title> <note> submitted to 1998 PODS. </note>
Reference-contexts: Like almost all such problems, it is NP-hard, even in the unit-weight case formulated above. One can define a segmentation problem (and in fact one of several variants) for every classical optimization problem. Segmentation problems are intended to capture certain aspects of the economic basis for data mining <ref> [11] </ref> and clustering; we explain this connection next. The Value of Data Mining Data mining is the application of statistical and machine-learning techniques for extracting interesting patterns from raw data. <p> Most research in data mining deals with the efficient discovery of patterns for subsequent human evaluation of the degree to which they are "interesting," and not on techniques for automatically evaluating mined patterns, or for automatically focusing on "interesting" patterns. We recently proposed in <ref> [11] </ref> a rigorous and algorithmic framework for such evaluation based on the pattern's utility in decision-making. The framework formulated in [11] suggests a number of interesting computational issues, related to sensitivity analysis and clustering; in the present work we study algorithms for one class of such optimization problems | the segmentation <p> We recently proposed in <ref> [11] </ref> a rigorous and algorithmic framework for such evaluation based on the pattern's utility in decision-making. The framework formulated in [11] suggests a number of interesting computational issues, related to sensitivity analysis and clustering; in the present work we study algorithms for one class of such optimization problems | the segmentation problems. 1 Segmentation problems address the problem of the degree of aggregation in the data that an enterprise uses for <p> But suppose that we are told 1 This sum ignores inter-dependencies among the customers i 2 C; but for now we will focus on this first approximation for concreteness. The issue of non-linearities in the data, in a somewhat different context, is addressed in <ref> [11] </ref>. that the dimensions of the space are edges of a graph, and the points are the weights of the edges as perceived by people interested in minimum spanning trees of the graph. <p> In recent approximation algorithms for the discrete facility location problem and its variants, on the other hand (see e.g. [15]), the facilities must typically be sited at demand locations, yielding, immediately, a relatively small space of possible decisions. Complexity In <ref> [11] </ref>, we prove certain negative complexity results about segmentation problems. In particular, we show that many of the most natural versions are NP-complete, even in several cases in which the un-segmented version of the underlying optimization problem is trivially solvable. Theorem 1.1 [11] The segmentation problems corresponding to the following feasible <p> Complexity In <ref> [11] </ref>, we prove certain negative complexity results about segmentation problems. In particular, we show that many of the most natural versions are NP-complete, even in several cases in which the un-segmented version of the underlying optimization problem is trivially solvable. Theorem 1.1 [11] The segmentation problems corresponding to the following feasible sets D are MAXSNP-complete: (1) The d-dimensional unit ball, even with k = 2; (2) the d-dimensional unit L 1 ball; (3) the d-dimensional hypercube, even with k = 2; (4) the r-slice of the d-dimensional hypercube (the catalog segmentation problem), even <p> Theorem 1.2 <ref> [11] </ref> Segmentation problems (2-5) above can be solved in time polynomial in n when the number of dimensions is fixed. Problem 1 (the unit ball) can be solved in time O (n 2 k) in two dimensions, and is MAXSNP-complete in three dimensions.
Reference: [12] <author> B. Liu and W. </author> <title> Hsu "Post-analysis of learned rules," </title> <booktitle> Proc. AAAI, </booktitle> <pages> pp. 828-834, </pages> <year> 1996. </year>
Reference: [13] <author> G. Nemhauser, L. Wolsey, M. Fisher, </author> <title> "An analysis of the approximations for maximizing submodular set functions," </title> <journal> Mathematical Programming, </journal> <volume> 14(1978), </volume> <pages> pp. 265-294. </pages>
Reference-contexts: Our analysis is carried out at the more general level of an arbitrary monotone submodular function minus a fixed-cost function; it thus forms a natural parallel to results of Nemhauser, Wolsey, and Fisher <ref> [6, 13] </ref> on the maximization of monotone submodular functions, and addresses an open question raised by Berman, Hodgson, and Krass. In Section 3 we consider the hypercube segmentation problem, defined above. We wish to approximate the k-segmentation of largest possible value. <p> This phrasing of the problem resembles the maximization problem for monotone submodular functions, studied by Cornuejols, Fisher, and Nemhauser [6] and Nemhauser, Wolsey, and Fisher <ref> [13] </ref>; we drew a different but related connection to this work in Section 2.2. However, it turns out that segmentation functions need not be submodular.
Reference: [14] <author> G. Piatetsky-Schapiro, C. J. </author> <title> Matheus "The interestingness of deviations," </title> <booktitle> KDD 1994, </booktitle> <pages> pp. 25-36, </pages> <year> 1994. </year>
Reference-contexts: Formalizing what "interesting" means in this context has been an important problem in the data mining literature, which had not been addressed in a satisfactory way (see, e.g., <ref> [1, 17, 14, 16] </ref> for various attempts). Most research in data mining deals with the efficient discovery of patterns for subsequent human evaluation of the degree to which they are "interesting," and not on techniques for automatically evaluating mined patterns, or for automatically focusing on "interesting" patterns.
Reference: [15] <author> D. Shmoys, E. Tardos, K. Aardal, </author> <title> "Approximation algorithms for facility location problems," </title> <booktitle> Proc. ACM STOC, </booktitle> <year> 1997. </year>
Reference-contexts: Moreover, our space of possible decisions is typically exponential or infinite, and only implicitly represented. In recent approximation algorithms for the discrete facility location problem and its variants, on the other hand (see e.g. <ref> [15] </ref>), the facilities must typically be sited at demand locations, yielding, immediately, a relatively small space of possible decisions. Complexity In [11], we prove certain negative complexity results about segmentation problems.
Reference: [16] <author> A. Silberschatz and A. </author> <title> Tuzhilin "What makes patterns interesting in knowledge discovery systems ," IEEE Trans. </title> <journal> on Knowledge and Data Eng., </journal> <volume> 8, 6, </volume> <year> 1996. </year>
Reference-contexts: Formalizing what "interesting" means in this context has been an important problem in the data mining literature, which had not been addressed in a satisfactory way (see, e.g., <ref> [1, 17, 14, 16] </ref> for various attempts). Most research in data mining deals with the efficient discovery of patterns for subsequent human evaluation of the degree to which they are "interesting," and not on techniques for automatically evaluating mined patterns, or for automatically focusing on "interesting" patterns.
Reference: [17] <author> P. Smyth, R. M. </author> <title> Goodman "Rule induction using information theory," </title> <booktitle> KDD 1991. </booktitle> <pages> 11 </pages>
Reference-contexts: Formalizing what "interesting" means in this context has been an important problem in the data mining literature, which had not been addressed in a satisfactory way (see, e.g., <ref> [1, 17, 14, 16] </ref> for various attempts). Most research in data mining deals with the efficient discovery of patterns for subsequent human evaluation of the degree to which they are "interesting," and not on techniques for automatically evaluating mined patterns, or for automatically focusing on "interesting" patterns.
References-found: 17

