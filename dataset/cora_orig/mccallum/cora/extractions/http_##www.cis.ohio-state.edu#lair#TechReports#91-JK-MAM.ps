URL: http://www.cis.ohio-state.edu/lair/TechReports/91-JK-MAM.ps
Refering-URL: http://www.cis.ohio-state.edu/lair/Papers/Directories/m-dir.html
Root-URL: 
Email: Email: kolen-j@cis.ohio-state.edu  
Title: Multiassociative Memory  
Author: John F. Kolen Jordan B. Pollack 
Address: Columbus, OH, 43210 Telephone:(614)292-7402  
Affiliation: The Laboratory for AI Research Department of Computer and Information Science The Ohio State University  
Abstract: This paper discusses the problem of how to implement many-to-many, or multi-associative, mapping within connectionist models. Traditional symbolic approaches work through explicit representation of all alternatives via stored links, or implicitly through enumerative algorithms. Classical pattern association models ignore the issue of generating multiple outputs for a single input pattern, and while recent research on recurrent networks is promising, the field has not clearly focused upon multi-associativity as a goal. In this paper, we define multiassociative memory (MM), and several possible variants, and discuss its utility in general cognitive modeling. We extend sequential cascaded networks (Pollack 1987, 1990a) to fit the task, and perform several initial experiments which demonstrate the feasibility of the concept. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Anderson, J. A. </author> <year> (1972). </year> <title> A simple neural network generating an interactive memory. </title> <publisher> Mathematical Biosciences, 14:197220. </publisher>
Reference: <author> Collins, A. M. and Loftus, E. F. </author> <year> (1975). </year> <title> A spreading activation theory of semantic processing. </title> <journal> Psychological Review, 82:240247. </journal>
Reference-contexts: Thus, any possible subset of the range can be captured by this representation. This localist approach has been used to encode possible moves in a tic-tac-toe game (McClelland and Rumelhart, 1986) and the spreading of activation in a semantic network <ref> (Collins and Loftus, 1975) </ref>. Unconstrained representational power, however, is not without its costs. Since individual elements of a range of units can be adequately represented with a binary pattern of length , using units is exponentially more expensive than a sequential retrieval model.
Reference: <author> Elman, J. L. </author> <year> (1988). </year> <title> Finding structure in time. </title> <type> Technical Report CRL 8801, </type> <institution> University of California, </institution> <address> San Diego. </address>
Reference: <author> Grossberg, S. </author> <year> (1987). </year> <title> Competitive learning: From interactive activation to adaptive resonance. </title> <journal> Cognitive Science, 11:2363. </journal>
Reference-contexts: The inversion of a multi-associative memory would remain a multi associative memory. The main difficulty with our initial MM model, and with most recurrent back-propagation sys tems is the reliance on global state information and synchronous update of that information <ref> (- Grossberg, 1987) </ref>. As nature teaches us, it is much cheaper to build asynchronous and local parallel processing, which unfortunately can be quite unstable.
Reference: <author> Hopfield, J. J. </author> <year> (1982). </year> <title> Neural networks and physical systems with emergent collective computational abilities. </title> <booktitle> Proceedings of the National Academy of Sciences USA, </booktitle> <address> 79:25542558. </address>
Reference-contexts: Retrieval or recall of memory is conceived of as the association from a domain set of noisy or partial patterns to a range set of correct patterns. Several connectionist models of associative memory exist, such as linear associators (Kohonen, 1972;Anderson, 1972), Hopfield networks <ref> (Hopfield, 1982) </ref>, and feed-forward networks (Rumelhart, Hinton & Williams, 1986), each assuming a many-to-one mapping from domain to range (Figure 1a). For noisy or partial memory retrieval, or for perceptual categorization, this assumption might prove valid, however, many other tasks exist, each indescribable within associative frameworks.
Reference: <author> Jordan, M. I. </author> <year> (1987). </year> <title> Supervised learning and systems with excess degrees of freedom. </title> <type> Technical Report COINS Technical Report 88-27, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Boston. </address>
Reference: <author> Kohonen, T. </author> <year> (1972). </year> <title> Correlation matrix memories. </title> <journal> IEEE Transactions on Computers, C-21:353359. </journal>
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Reecting on the implementation process and its results suggested possible difficulties with the recurrent network, indicating directions for further work. Many to Many Mappings Associative memories take input patterns, process them, and return output patterns <ref> (Kohonen, 1984) </ref>. One way of describing this operation is as a mapping from input patterns (the domain set) to the generated output patterns (the range set).
Reference: <author> Kolen, J. & Pollack, J. </author> <title> (1990) Back-Propagation is sensitive to Initial Conditions. </title> <booktitle> Complex Systems 4, </booktitle> <pages> 269-280. </pages>
Reference-contexts: Table1 describes network parameters used for the experiments. In each of the experiments, the input and output symbols were converted to random binary patterns of the specified length. Because back propagation is sensitive to the initial weights <ref> (Kolen & Pollack, 1990) </ref>, in different training runs, 1. During an overtraining pass weights are modified, even though the all of the outputs are with tolerance of their de sired values. AAAAAAAAAAAA... BCBCBCBCBCBC...
Reference: <editor> McClelland J., & Rumelhart, D. </editor> <booktitle> (1986) Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <address> Cambridge, </address> <publisher> MIT Press, II, </publisher> <pages> 48-53. </pages>
Reference-contexts: The power set approach involves allocation of one output node for each element in the range (figure 1c). Thus, any possible subset of the range can be captured by this representation. This localist approach has been used to encode possible moves in a tic-tac-toe game <ref> (McClelland and Rumelhart, 1986) </ref> and the spreading of activation in a semantic network (Collins and Loftus, 1975). Unconstrained representational power, however, is not without its costs.
Reference: <author> McCloskey, M. and Glucksberg, S. </author> <year> (1978). </year> <title> Natural categories: Well defined or fuzzy sets? Memory and Cognition, </title> <publisher> 6:462472. </publisher>
Reference-contexts: Consider associating a word with its possible lexical categories or meanings, a chess position to possible next moves, or a category to prototypical members, each input can have a multiplicity of possible outputs. Even categorization itself is known to vary over time in individual subjects <ref> (McCloskey and Glucksberg, 1978) </ref>. In an attempt to address the issue of multiple output responses, this paper discusses the nature of multiassociative memories capable of responding to an input with one from a set of possible outputs.
Reference: <author> Pollack, J. B.(1987). </author> <title> Cascaded back propagation on dynamic connectionist networks. </title> <booktitle> In Proceedings of the Fourth Annual Cognitive Science Conference. </booktitle> <address> Seattle, WA, </address> <pages> 391-404. </pages>
Reference: <author> Pollack, J. B. </author> <title> (1990).Language acquisition via strange automata. </title> <booktitle> In Proceedings of the Twelfth Annual Conference of the Cognitive Science Society. </booktitle> <address> Cambridge, MA, </address> <pages> 678-685. </pages>
Reference-contexts: Table1 describes network parameters used for the experiments. In each of the experiments, the input and output symbols were converted to random binary patterns of the specified length. Because back propagation is sensitive to the initial weights <ref> (Kolen & Pollack, 1990) </ref>, in different training runs, 1. During an overtraining pass weights are modified, even though the all of the outputs are with tolerance of their de sired values. AAAAAAAAAAAA... BCBCBCBCBCBC...
Reference: <author> Pollack, J. B. </author> <year> (1990). </year> <title> Recursive autoassociative memories. </title> <journal> Artificial Intelligence Journal, </journal> <volume> 46, 1, </volume> <pages> 77-105 Pollack, </pages> <editor> J. B. </editor> <title> (In press) The acquisition of dynamical recognizers Machine Learning, </title> <year> 1991. </year>
Reference-contexts: Table1 describes network parameters used for the experiments. In each of the experiments, the input and output symbols were converted to random binary patterns of the specified length. Because back propagation is sensitive to the initial weights <ref> (Kolen & Pollack, 1990) </ref>, in different training runs, 1. During an overtraining pass weights are modified, even though the all of the outputs are with tolerance of their de sired values. AAAAAAAAAAAA... BCBCBCBCBCBC...
Reference: <author> Rumelhart D. Hinton, G., and Williams R. </author> <title> Learning representations by back-propagating errors. </title> <journal> Nature, </journal> <volume> 323, </volume> <pages> 533-536. </pages>

References-found: 15

