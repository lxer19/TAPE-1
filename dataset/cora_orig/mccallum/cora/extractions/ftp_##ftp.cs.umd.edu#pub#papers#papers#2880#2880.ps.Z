URL: ftp://ftp.cs.umd.edu/pub/papers/papers/2880/2880.ps.Z
Refering-URL: http://www.cs.umd.edu/TRs/TR.html
Root-URL: 
Title: Updating URV Decompositions in Parallel  
Author: G. W. Stewart 
Date: April 1992  
Affiliation: University of Maryland College Park Institute for Advanced Computer Studies TR-92-44 Department of Computer Science  
Pubnum: TR-2880  
Abstract: A URV decomposition of a matrix is a factorization of the matrix into the product of a unitary matrix (U), an upper triangular matrix (R), and another unitary matrix (V). In [8] it was shown how to update a URV decomposition in such a way that it reveals the effective rank of the matrix. It was also argued that the updating procedure could be implemented in parallel on a linear array of processors; however, no specific algorithms were given. This paper gives a detailed implementation of the updating procedure. fl This report is available by anonymous ftp from thales.cs.umd.edu in the directory pub/reports. y Department of Computer Science and Institute for Advanced Computer Studies, University of Mary-land, College Park, MD 20742. This work was supported in part by the Air Force Office of Scientific Research under Contract AFOSR-87-0188. Part of it was done while the author was visiting the Istitute for Mathematics and Its Applications, University of Minnesota, Minneapolis, MN 55455. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. H. Golub and C. F. Van Loan. </author> <title> Matrix Computations. </title> <publisher> Johns Hopkins University Press, </publisher> <address> Baltimore, Maryland, 2nd edition, </address> <year> 1989. </year>
Reference-contexts: An example of a rank-revealing URV decomposition is the singular value decomposition <ref> [1] </ref>, in which R and G are diagonal and F is zero. However, the singular value decomposition is expensive to compute and often furnishes more information than is needed to solve the problem at hand. <p> Plane Rotations and Simple Updating 1 Most of the computation in URV updating involves the use of plane rotations to introduce zeros into the matrix R. Since treatments of plane rotations are widely available (e.g., see <ref> [1] </ref>), we will not go into the numerical details here. Instead we will sketch the few basic facts needed to understand our algorithms and introduce some conventions for describing reductions based on plane rotations. rotation. <p> This passing of data is simple enough not to require a precedence diagram, and we give only the code. Fragment 6.1. Distribution of z T . for (j=p; j&gt;I; j--)- temp = z [j]; else fget (NORTH, &temp); fput (SOUTH, temp); - zi = z <ref> [1] </ref>; else fget (NORTH, &zi); Once the components of z T are in place, they are are multiplied into the components of the rows of V , the products being passed down and summed on the way. The precedence diagram is given in Figure 6.1.
Reference: [2] <author> W. B. Gragg and G. W. Stewart. </author> <title> A stable variant of the secant method for solving nonlinear equations. </title> <journal> SIAM Journal on Numerical Analysis, </journal> <volume> 13 </volume> <pages> 880-903, </pages> <year> 1976. </year>
Reference-contexts: Thus, P T RQ is a rank-revealing URV decomposition of R. The determination of the vector w falls under the rubric of condition estimation (for a survey see [3]). Here we give a very simple condition estimator <ref> [2] </ref>. The idea is to solve the system R ^w = b, where the components of b are 1, the sign being chosen to maximize the growth of ^w. The vector w is given by ^w=k ^wk. The following code is adapted from Fragment 3.2. Fragment 7.1.
Reference: [3] <author> N. J. Higham. </author> <title> A survey of condition number estimation for triangular matrices. </title> <journal> SIAM Review, </journal> <volume> 29 </volume> <pages> 575-596, </pages> <year> 1987. </year>
Reference-contexts: Thus, P T RQ is a rank-revealing URV decomposition of R. The determination of the vector w falls under the rubric of condition estimation (for a survey see <ref> [3] </ref>). Here we give a very simple condition estimator [2]. The idea is to solve the system R ^w = b, where the components of b are 1, the sign being chosen to maximize the growth of ^w. The vector w is given by ^w=k ^wk.
Reference: [4] <author> D. P. O'Leary and G. W. Stewart. </author> <title> From determinacy to systaltic arrays. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36:1355-1359, </volume> <year> 1987. </year>
Reference-contexts: The fragments are from programs that have been debugged on a simulator that forks a process for each processor in the array. The communications functions are implemented by manipulating queues in shared memory. It has been shown <ref> [4] </ref> that if a program works on such a simulator it will work on a truly parallel system. 3. Precedence Diagrams and Parallel Code It is not a trivial matter to pass from sequential code for a matrix algorithm to the corresponding quasi-systolic code.
Reference: [5] <author> J. M. Ortega, R. G. Voigt, and C. H. Romine. </author> <title> A Bibliography on Parallel and Vector Numerical Algorithms. </title> <type> Technical Report TM-10998, </type> <institution> Oak Ridge National Laboratory, </institution> <year> 1989. </year>
Reference-contexts: Another purpose is to illustrate a style of programming. Our model of computation will be fine-grain mimd; that is, short messages interleaved with short computations without global control. Fine-grain simd algorithms have an extensive literature, as do coarse-grain mimd algorithms (e.g., see <ref> [5, 6] </ref>). Less attention has been paid to fine-grain mimd algorithms, chiefly because machines to run them have not been widely available. Now that such machines as the iwarp are in production, it is appropriate to illustrate coding techniques with a completely new and fairly complex set of algorithms.
Reference: [6] <author> Y. Robert. </author> <title> The Impact of Vector and Parallel Architectures on the Gaussian Elimination Algorithm. </title> <publisher> Halsted Press, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: Another purpose is to illustrate a style of programming. Our model of computation will be fine-grain mimd; that is, short messages interleaved with short computations without global control. Fine-grain simd algorithms have an extensive literature, as do coarse-grain mimd algorithms (e.g., see <ref> [5, 6] </ref>). Less attention has been paid to fine-grain mimd algorithms, chiefly because machines to run them have not been widely available. Now that such machines as the iwarp are in production, it is appropriate to illustrate coding techniques with a completely new and fairly complex set of algorithms.
Reference: [7] <author> G. W. Stewart. </author> <title> On an Algorithm for Refining a Rank-Revealing URV Decomposition and a Perturbation Theorem for Singular Values. </title> <type> Technical Report CS-TR 2626, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1991. </year> <note> To appear in Linear Algebra and Its Applications.. The Parallel URV Decomposition 23 </note>
Reference-contexts: In the first part, the elements above r kk are reduced to zero by left rotations. This process introduces small elements in the last row, which are then eliminated in the second part by simple updating. For a detailed analysis of this step, see <ref> [7] </ref>. The reduction of the last column of R is shown in Figure 8.1. A precedence diagram is given in Figure 8.2.
Reference: [8] <author> G. W. Stewart. </author> <title> An Updating Algorithm for Subspace Tracking. </title> <type> Technical Report CS-TR 2494, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1990. </year> <note> To appear in IEEE Transactions on Signal Processing. </note>
Reference-contexts: y C C ; 2 The Parallel URV Decomposition where (x T y T ) = z T (V 1 V 2 ), the problem reduces to one of computing a rank-revealing URV factorization of 0 B R F x T y C from the matrix R F ! In <ref> [8] </ref> the author described an algorithm requiring O (p 2 ) time for updating (1.2). The algorithm alternates between two steps: Update: Reduce (1.2) to upper triangular form, preserving as far as possible the small elements of F and G. <p> In Section 3 we show how to use precedence diagrams to pass from sequential to parallel algorithms. The parallel updating algorithm will be described piece by piece in the remaining sections. Although we will give brief sketches of the sequential algorithms, the reader should look to <ref> [8] </ref> for background and details. 2. Architectural Details The way a parallel algorithm is implemented depends on the architecture of the system for which it is intended. In this paper we will work with a linear mimd array that is capable of fine-grained communication. <p> This is a key observations, since the point of the updating algorithm is to keep small elements small. 1 This section draws heavily on a similar section in <ref> [8] </ref>.
Reference: [9] <author> R. J. Vaccaro, </author> <title> editor. SVD and Signal Processing, II, </title> <publisher> Elsevier Science Publishers, </publisher> <address> Amsterdam, </address> <year> 1990. </year>
Reference-contexts: Matrices that are effectively rank degenerate occur in a number of applications, in which it is required to determine the effective rank and to compute an orthonormal basis for its effective null space of the matrix (e.g., see <ref> [9] </ref>). Of course, the notions of "large" and "small", which define the effective rank, depend on the application.
References-found: 9

