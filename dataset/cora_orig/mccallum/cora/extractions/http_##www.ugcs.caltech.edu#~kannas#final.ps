URL: http://www.ugcs.caltech.edu/~kannas/final.ps
Refering-URL: http://www.ugcs.caltech.edu/~kannas/
Root-URL: http://www.cs.caltech.edu
Title: Non-Realtime Voice Compression Using Speech Recognition Techniques categorized into silent, voiced, and unvoiced portions. Different
Author: Kanna Shimizu and Glen A. George 
Note: Speech samples can be  
Affiliation: Electrical Engineering, California Institute of Technology  
Abstract: We present a non-realtime speech compression method which applies fundamental findings of speech recognition research to traditional data compression algorithms such as Vector Quantization (VQ) and the Ziv-Lempel (LZ) adaptive dictionary algorithm. [1]. This system does not aim for speedy processing of the voice sample as many conventional voice coders such as CELP or VSELP [1] do. Instead, it exploits the flexible computation time requirement, and searches the whole sample thoroughly for repetition. Thus, it is effective in compression for what it lacks in speed. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sayood, Khalid: </author> <title> Introduction to Data Compression, </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1996. </year>
Reference-contexts: Since straight VQ is very computationally exhaustive, a variant of it, Tree Vector Quantization, which follows naturally from the Binary Split Algorithm was used. Tree VQ increases the speed of the processing but as a tradeoff, increases the distortion <ref> [1] </ref>. Although not yet implemented, another variant of VQ that would improve results, is the Matrix Vector Quantization Algorithm [2]. This is just a straight extension of the regular VQ but encodes several vectors at a time. This takes advantage of the correlation between consecutive frames.
Reference: [2] <author> L. R. Rabiner, B. Juang: </author> <title> Fundamentals Of Speech Recognition, </title> <publisher> Prentice-Hall, </publisher> <year> 1993. </year>
Reference-contexts: This also serves as a simple compression method. Instead of sending the whole silence segment, a code signifying silence can be sent along with the length of the silence. However, finding the exact point where silence becomes speech, (ie when the speaker starts emitting speech) is a non-trivial problem <ref> [2] </ref>. For example, fricative sounds such as `sss' are hard to distinguish from background noise due to their noise-like quality. These especially become difficulties when words 1 start with these sounds (such as `snake') after a block of silence. To accurately extract silence, a two-stage silence-speech discriminator was developed. <p> The Non-Silence Compression Method After silence is extracted, the speech segments are glued back together to form one silence-less speech sample. They are processed as follows to obtain cep-stral coefficients which are the most robust feature parameters for speech recognition (p115 <ref> [2] </ref>). 1. PreEmphasis The speech signal is passed through a first-order FIR filter to emphasize high and middle frequency components and de-emphasize low frequency components. This is to avoid finite precision effects of the digital signal processing stage. 2. <p> It provides the coefficients for a LTI (linear time-invariant) system which takes in a train of impulse responses for voiced speech portions and a random noise generator for unvoiced speech portions and outputs the reconstructed voice signal <ref> [2] </ref>. Thus, the LPC coefficients can be used to characterize a voice signal. 5. Cepstral Coefficients The cepstral coefficients are traditionally the Inverse Fourier Transform of the Log of the Fourier Transform [5]. They are very effective in characterizing speech units [5]. <p> Cepstral Coefficients The cepstral coefficients are traditionally the Inverse Fourier Transform of the Log of the Fourier Transform [5]. They are very effective in characterizing speech units [5]. A variant of the straight FT cep-stral coefficients is the LPC-derived cepstral coefficients <ref> [2] </ref>. LPC-derived cepstral coefficients better model the formant structure (the long-term frequency shaping of the voice signal) (p67 [8], [2]) plus they take less computational time and output fewer coefficient saving memory space. 6. <p> They are very effective in characterizing speech units [5]. A variant of the straight FT cep-stral coefficients is the LPC-derived cepstral coefficients <ref> [2] </ref>. LPC-derived cepstral coefficients better model the formant structure (the long-term frequency shaping of the voice signal) (p67 [8], [2]) plus they take less computational time and output fewer coefficient saving memory space. 6. <p> It has been shown that the most appropriate, and successful distance measure for cepstral coefficients is the Euclidean measure <ref> [2] </ref>. d = m=1 n ) 2 where the c n 's are the cepstral coefficients of the primed and the unprimed speech segments. Thus, if d is small, we can assume that the two spectral vectors are close enough to be replaced by each other. <p> Vector Quantization We are tempted to use Vector Quantization here to group the cepstral vectors together by their proximity measure. And indeed, as a first step, we used a variant of the generalized Lloyd Algorithm or the K-means clustering algorithm, the Binary Split Algorithm (p126, <ref> [2] </ref>). A VQ codebook dictionary with 1024 cepstral vector entries was created using the first 10240 frames of the speech as training vectors. <p> Tree VQ increases the speed of the processing but as a tradeoff, increases the distortion [1]. Although not yet implemented, another variant of VQ that would improve results, is the Matrix Vector Quantization Algorithm <ref> [2] </ref>. This is just a straight extension of the regular VQ but encodes several vectors at a time. This takes advantage of the correlation between consecutive frames. This method implies that the sequence has memory. <p> This method implies that the sequence has memory. That is true for speech and is exploited by the popular speech recognition technique, the Hidden Markov Model method [9]. A more advanced algorithm which takes advantage of the memory character of speech signals is Trellis Coding <ref> [2] </ref> which can also be explored to improve results. 9.
Reference: [3] <author> L. R. Rabiner, R. W. Schafer: </author> <title> Digital Processing of Speech Signals, </title> <institution> Bell Laboratories, </institution> <year> 1978. </year>
Reference-contexts: Then, in the second stage, both ends of each cut-up speech segments are analyzed to find the exact point of the speech-to-silence and silence-to-speech switches. And silence is appropriately shaved off. Rabiner and Schafer suggest using two parameters to robustly discriminate silence from speech <ref> [3] </ref>. One, average magnitude, is analysis in the time domain, and the other, average zero-crossing rate, is a frequency domain analysis. These two are chosen on the assumption that silence has smaller amplitude and has lower zero-crossing rate than speech.
Reference: [4] <author> L. R. Rabiner and M. R. Sambur: </author> <title> "An Algorithm for Determining the Endpoints of Isolated Utterances," </title> <journal> Bell System Technical Journal, </journal> <volume> Vol. 54, No. 2, p297-315, </volume> <month> February </month> <year> 1975. </year>
Reference-contexts: This is done by identifying a block of `typical' silence and finding the mean and variance of the average magnitude and zero-crossing rate of this representative silence. Then these parameters are manipulated to find appropriate thresholds to pinpoint the endpoints. This method was originally presented by Rabiner and Sambur <ref> [4] </ref>. Now, the question arises of how to define `typical' silence. There are three possible choices. * All the silence frames are used to calculate the parameters * The longest silence block is used * The silence block with the most conservative silence qualities is used. <p> After experimenting with different speech samples, the best choice turned out to be the first two with equally good results. Now, each roughly cut speech blocks can have their ends evaluated with these typical silence parameters. And a finer cut is done to completely shave off silence. See <ref> [4] </ref> for the algorithm. The compression rate achieved by extracting silence greatly varies among speech samples. A 20% reduction in size is approximately the average compression rate realized with the method outlined above.
Reference: [5] <author> Noll, A. M. </author> : <title> "Cepstrum Pitch Determination", </title> <journal> Journal of Acoustic Society America, </journal> <volume> Vol. 41, p293-309, </volume> <month> February </month> <year> 1967. </year>
Reference-contexts: Thus, the LPC coefficients can be used to characterize a voice signal. 5. Cepstral Coefficients The cepstral coefficients are traditionally the Inverse Fourier Transform of the Log of the Fourier Transform <ref> [5] </ref>. They are very effective in characterizing speech units [5]. A variant of the straight FT cep-stral coefficients is the LPC-derived cepstral coefficients [2]. <p> Thus, the LPC coefficients can be used to characterize a voice signal. 5. Cepstral Coefficients The cepstral coefficients are traditionally the Inverse Fourier Transform of the Log of the Fourier Transform <ref> [5] </ref>. They are very effective in characterizing speech units [5]. A variant of the straight FT cep-stral coefficients is the LPC-derived cepstral coefficients [2]. LPC-derived cepstral coefficients better model the formant structure (the long-term frequency shaping of the voice signal) (p67 [8], [2]) plus they take less computational time and output fewer coefficient saving memory space. 6. <p> And a dictionary entry for it is created at both ends. The dictionaries will grown in the following manner. * If the cepstral spectral vector has a pronounced peak, it is determined to be voiced <ref> [5] </ref> and will be categorized into a binary tree. If the value of the location of the peak of the to-be-added entry is less than the parent node's value, the entry is moved to the left child of the parent node.
Reference: [6] <author> Jackson, Leland B. </author> : <title> Signals, Systems, and Transforms, </title> <publisher> Addison-Wesley, </publisher> <year> 1988. </year>
Reference-contexts: Frame Blocking For an 8 kHz sampling rate, the ideal frame size for obtaining short term frequency information, is 30msec frames with 10msec overlap. 3. Windowing Smooth out the abruptly cut ends by applying a Hamming Window. This is to get avoid leakage problems caused by rectangular windowing (p382-383 <ref> [6] </ref>). 2n ); 0 n N 1 Hamming Window 0 5 10 15 20 25 30 35 40 0 0.2 0.4 0.6 0.8 1 Hamming windows are used to de-emphasize the discontinuous points (the edges of the frames) at the ends. 4.
Reference: [7] <author> S. Wang, A. Gersho: </author> <title> "Phonetic Segmentation For Low Rate Speech Coding", Advances in Speech Coding, </title> <publisher> Kluwer Academic Publishers, </publisher> <year> 1991. </year>
Reference: [8] <author> Furui, Sadaoki: </author> <title> Digital Speech Processing, Synthesis, and Recognition, </title> <publisher> Marcel Dekker, </publisher> <year> 1989. </year>
Reference-contexts: They are very effective in characterizing speech units [5]. A variant of the straight FT cep-stral coefficients is the LPC-derived cepstral coefficients [2]. LPC-derived cepstral coefficients better model the formant structure (the long-term frequency shaping of the voice signal) (p67 <ref> [8] </ref>, [2]) plus they take less computational time and output fewer coefficient saving memory space. 6.
Reference: [9] <author> Rabiner, L. R., </author> <title> "A Tutorial on Hidden Markov Models and Selected Applications in Speech Recognition", </title> <journal> Proc. IEEE, </journal> <volume> 77 (2): </volume> <pages> 257-286, </pages> <month> February </month> <year> 1989. </year> <month> 7 </month>
Reference-contexts: This takes advantage of the correlation between consecutive frames. This method implies that the sequence has memory. That is true for speech and is exploited by the popular speech recognition technique, the Hidden Markov Model method <ref> [9] </ref>. A more advanced algorithm which takes advantage of the memory character of speech signals is Trellis Coding [2] which can also be explored to improve results. 9.
References-found: 9

