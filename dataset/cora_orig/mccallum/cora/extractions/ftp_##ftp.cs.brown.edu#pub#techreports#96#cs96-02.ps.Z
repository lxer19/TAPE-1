URL: ftp://ftp.cs.brown.edu/pub/techreports/96/cs96-02.ps.Z
Refering-URL: http://www.cs.brown.edu/publications/techreports/reports/CS-96-02.html
Root-URL: 
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Brill, E. </author> <title> Automatic grammar induction and parsing free text: a transformation-based approach. </title> <booktitle> In Proceedings of the 31st Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1993, </year> <pages> 259-265. </pages>
Reference-contexts: Reduced 84.3 87.9 94.5 2-25 Full 82.0 84.0 90.8 Reduced 81.6 84.7 91.0 2-40 Full 78.8 80.4 87.7 Reduced 78.2 80.7 87.6 10 15 20 25 90 100 4 4 4 fi 2 2 4| The tree-bank grammar fi | The PCFG of [4] 2 | The transformation parser of <ref> [1] </ref> 3 | The PCFG of [7] 5 It seems clear that the tree-bank grammar outperforms the others, partic-ularly when the average sentence length gets higher | i.e., when longer sentences are allowed into the testing corpus, The only data point that matches our current results is one for one of <p> For example, consider the following right-branching bracketing of the sentence "The cat licked several pans." ( (The (cat (licked (several pans)))) .) While the bracketing starting with "cat" is quite absurd, note how many of the bracketings are correct. This tendency has been exploited by Brill's <ref> [1] </ref> "transformational parser," which starts with the right-branching analysis of the sentence and then tries to improve on it. On the other hand, context-free grammars have no preference for right-branching structures.
Reference: 2. <author> Caraballo, S. and Charniak, E. </author> <title> Figures of Merit for Best-First Probabilistic Chart Parsing. </title> <type> Brown Univeristy Technical Report, forthcoming. </type>
Reference-contexts: It is not our purpose here to discuss the benefits of this particular figure of merit (but see <ref> [2] </ref>). Rather we simply want to note the difficulty of obtaining parses, and particularly, high-probability parses, in the face of extreme ambiguity.
Reference: 3. <author> Charniak, E. </author> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1993. </year>
Reference-contexts: p (t j;k+1 ) is the probability of the sequence of terms t j : : : t k and is estimated by a tri-tag model p (t j;k j N i ) is the inside probability of N i j;k and is computed in the normal fashion (see, e.g., <ref> [3] </ref> ) and p (N i j t j1 ) and p (t k j N i ) are estimated by gathering statistics from the training corpus. It is not our purpose here to discuss the benefits of this particular figure of merit (but see [2]).
Reference: 4. <author> Charniak, E. </author> <title> Parsing with context-free grammars and word statistics. </title> <institution> Department of Computer Science, Brown University, </institution> <type> Technical Report CS-95-28, </type> <year> 1995. </year>
Reference-contexts: Recall Accuracy 2-16 Full 85.0 87.7 94.5 Reduced 84.3 87.9 94.5 2-25 Full 82.0 84.0 90.8 Reduced 81.6 84.7 91.0 2-40 Full 78.8 80.4 87.7 Reduced 78.2 80.7 87.6 10 15 20 25 90 100 4 4 4 fi 2 2 4| The tree-bank grammar fi | The PCFG of <ref> [4] </ref> 2 | The transformation parser of [1] 3 | The PCFG of [7] 5 It seems clear that the tree-bank grammar outperforms the others, partic-ularly when the average sentence length gets higher | i.e., when longer sentences are allowed into the testing corpus, The only data point that matches our <p> PCFG of [7] 5 It seems clear that the tree-bank grammar outperforms the others, partic-ularly when the average sentence length gets higher | i.e., when longer sentences are allowed into the testing corpus, The only data point that matches our current results is one for one of our earlier grammars <ref> [4] </ref>, and then only for very short sentences. This is not to say, however, that there are no better grammars/parsers. Magerman [5] reports precision and accuracy figures of 86% for WSJ sentences of length 40 and less.
Reference: 5. <author> Magerman, D. M. </author> <title> Statistical decision-tree models for parsing. </title> <booktitle> In Proceedings of the 33rd Annual Meeting of the Association for Computational Linguistics. </booktitle> <year> 1995, </year> <pages> 276-283. </pages>
Reference-contexts: This is not to say, however, that there are no better grammars/parsers. Magerman <ref> [5] </ref> reports precision and accuracy figures of 86% for WSJ sentences of length 40 and less. The difference is that Magerman's parser uses statistics based upon the actual words of the sentence, while ours, and the others shown in Figure 4 use only the tags of the words. <p> Rather, we need to include lexical items in the information mix upon which we base our statistics. Certainly the 86% precision and recall achieved by Magerman <ref> [5] </ref> supports this contention. On the other hand, [5] abjures grammars altogether, preferring a more complicated (or at least, more unusual) mechanism that, in effect, makes up the rules as it goes along. <p> Rather, we need to include lexical items in the information mix upon which we base our statistics. Certainly the 86% precision and recall achieved by Magerman <ref> [5] </ref> supports this contention. On the other hand, [5] abjures grammars altogether, preferring a more complicated (or at least, more unusual) mechanism that, in effect, makes up the rules as it goes along.
Reference: 6. <author> Marcus, M. P., Santorini, B. and Marcinkiewicz, M. A. </author> <title> Building a large annotated corpus of English: the Penn treebank. </title> <booktitle> Computational Linguistics 19 (1993), </booktitle> <pages> 313-330. </pages>
Reference-contexts: Then the probability assigned to r is given by p (r) = P (1) Originally we used as our set of non-terminals those specified in <ref> [6] </ref>. However, it was found that other non-terminals were used in the tree bank as well. Two of these (ORD and PRT) we added to the grammar, but for the majority we simply ignored any rule in which they occurred.
Reference: 7. <author> Pereira, F. and Schabes, Y. </author> <title> Inside-outside reestimation from partially bracketed corpora. </title> <booktitle> In 27th Annual Meeting of the Association for Computaitonal Linguistics. ACL, </booktitle> <year> 1992, </year> <pages> 128-135. 12 </pages>
Reference-contexts: 82.0 84.0 90.8 Reduced 81.6 84.7 91.0 2-40 Full 78.8 80.4 87.7 Reduced 78.2 80.7 87.6 10 15 20 25 90 100 4 4 4 fi 2 2 4| The tree-bank grammar fi | The PCFG of [4] 2 | The transformation parser of [1] 3 | The PCFG of <ref> [7] </ref> 5 It seems clear that the tree-bank grammar outperforms the others, partic-ularly when the average sentence length gets higher | i.e., when longer sentences are allowed into the testing corpus, The only data point that matches our current results is one for one of our earlier grammars [4], and then
References-found: 7

