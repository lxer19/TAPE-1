URL: http://www.research.digital.com/wrl/people/farkas/papers/paper.isca97.ps.gz
Refering-URL: http://www.research.digital.com/wrl/people/farkas/papers/paper.isca97.html
Root-URL: http://www.research.digital.com
Email: farkas@eecg.toronto.edu  pc@eecg.toronto.edu  jouppi@pa.dec.com  zvonko@eecg.toronto.edu  
Title: Memory-System Design Considerations for Dynamically-Scheduled Processors  
Author: Keith I. Farkas Paul Chow Norman P. Jouppi Zvonko Vranesic 
Address: 10 Kings College Road Toronto, Ontario M5S 3G4 Canada  250 University Avenue Palo Alto, California 94301 USA  
Affiliation: Electrical and Computer Engineering University of Toronto  Digital Equipment Corporation Western Research Lab  
Abstract: In this paper, we identify performance trends and design relationships between the following components of the data memory hierarchy in a dynamically-scheduled processor: the register file, the lockup-free data cache, the stream buffers, and the interface between these components and the lower levels of the memory hierarchy. Similar performance was obtained from all systems having support for fewer than four in-flight misses, irrespective of the register-file size, the issue width of the processor, and the memory bandwidth. While providing support for more than four in-flight misses did increase system performance, the improvement was less than that obtained by increasing the number of registers. The addition of stream buffers to the investigated systems led to a significant performance increase, with the larger increases for systems having less in-flight-miss support, greater memory bandwidth, or more instruction issue capability. The performance of these systems was not significantly affected by the inclusion of traffic filters, dynamic-stride calculators, or the inclusion of the per-load non-unity stride-predictor and the incremental-prefetching techniques, which we introduce. However, the incremental prefetching technique reduces the bandwidth consumed by stream buffers by 50% without a significant impact on performance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Norm Jouppi. </author> <title> Improving Direct Mapped Cache Performance by the Addition of a Small Fully Associative Cache and Prefetch Buffers. </title> <type> Technical Report TN-15, </type> <institution> Digital Equipment Corporation Western Research Lab, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: The memory-system components that we consider are the register file, the lockup-free data cache, and the stream buffers <ref> [1] </ref>, a technique for implementing hardware-based prefetching. We also examine the interface between these components and the lower levels of the memory hierarchy.
Reference: [2] <author> Keith I. Farkas. </author> <title> Memory-system Design Considerations for Dynamically-scheduled Microprocessors. </title> <type> PhD thesis, </type> <institution> Department of Electrical and Computer Engineering, University of Toronto, </institution> <address> Ontario, Canada, </address> <month> January </month> <year> 1997. </year> <note> (URL: http://www.eecg.toronto.edu/farkas/thesis phd.html). </note>
Reference-contexts: A more complete discussion of these components is given in <ref> [2] </ref>. 2.1 Lockup-free Cache When the processor detects a data-cache miss, it must determine whether the cache-block containing the missing data is already being fetched, and it must resolve all pending cache misses when this cache-block is returned from the lower levels of the memory hierarchy.
Reference: [3] <author> David Kroft. </author> <title> Lockup-Free Instruction Fetch/Prefetch Cache Organization. </title> <booktitle> In the Proceedings of the Eighth International Symposium on Computer Architecture, </booktitle> <pages> pages 81-87, </pages> <month> May </month> <year> 1981. </year>
Reference-contexts: This functionality is provided by miss status holding registers (MSHRs) <ref> [3] </ref>, but for dynamically-scheduled processors, additional functionality is required that is not present in statically-scheduled processors. This functionality allows selective cache misses to be suppressed while allowing others to be completed, thereby permitting the cancellation of speculatively executed memory instructions. <p> If a match is detected, the cache miss is referred to as a secondary miss, whereas if no match is detected, the miss is referred to as a primary miss <ref> [3] </ref>. For a primary miss, a free MSHR is allocated, the block address is written, the block valid bit set, and a fetch request for the cache block is issued to the next level in the memory system.
Reference: [4] <author> Keith I. Farkas and Norman P. Jouppi. </author> <title> Complexity/Performance Tradeoffs with Non-Blocking Loads. </title> <booktitle> In the Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 211-222, </pages> <year> 1994. </year>
Reference-contexts: This ability to easily suppress selective cache misses makes an address stack more attractive for dynamically-scheduled processors than the more conventional methods for recording information about primary and secondary misses <ref> [4] </ref>.
Reference: [5] <author> Kenneth C. Yeager. </author> <title> The MIPS R10000 Superscalar Microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 16(2) </volume> <pages> 28-40, </pages> <year> 1996. </year>
Reference-contexts: Although not described in the literature, it is likely that lockup-free caches are implemented in this way by the MIPS R10000 <ref> [5] </ref> using the address queue and address stack, the PA-8000 [6] using the memory buffer and the address-reorder buffer, and in the PowerPC 604 [7] and 620 [8] using the load queue. 2.2 Stream Buffers A stream buffer is a hardware-based prefetching technique that can be used to prefetch and store
Reference: [6] <author> Linley Gwennap. </author> <title> PA-8000 Combines Complexity and Speed. Microprocessor Reports, </title> <address> 8(15):1,6-11, </address> <year> 1994. </year>
Reference-contexts: Although not described in the literature, it is likely that lockup-free caches are implemented in this way by the MIPS R10000 [5] using the address queue and address stack, the PA-8000 <ref> [6] </ref> using the memory buffer and the address-reorder buffer, and in the PowerPC 604 [7] and 620 [8] using the load queue. 2.2 Stream Buffers A stream buffer is a hardware-based prefetching technique that can be used to prefetch and store data that might be required to resolve future data cache
Reference: [7] <author> S. Peter Song, Marvin Denman, and Joe Chang. </author> <title> The PowerPC 604 RISC Microprocessor. </title> <journal> IEEE Micro, </journal> <volume> 14(5) </volume> <pages> 8-17, </pages> <year> 1994. </year>
Reference-contexts: Although not described in the literature, it is likely that lockup-free caches are implemented in this way by the MIPS R10000 [5] using the address queue and address stack, the PA-8000 [6] using the memory buffer and the address-reorder buffer, and in the PowerPC 604 <ref> [7] </ref> and 620 [8] using the load queue. 2.2 Stream Buffers A stream buffer is a hardware-based prefetching technique that can be used to prefetch and store data that might be required to resolve future data cache misses.
Reference: [8] <institution> IBM Microelectronics. </institution> <type> PowerPC 620 RISC Microprocessor Technical Summary, </type> <month> 10 </month> <year> 1994. </year> <title> document number: </title> <publisher> MPR620TSU-01. </publisher>
Reference-contexts: Although not described in the literature, it is likely that lockup-free caches are implemented in this way by the MIPS R10000 [5] using the address queue and address stack, the PA-8000 [6] using the memory buffer and the address-reorder buffer, and in the PowerPC 604 [7] and 620 <ref> [8] </ref> using the load queue. 2.2 Stream Buffers A stream buffer is a hardware-based prefetching technique that can be used to prefetch and store data that might be required to resolve future data cache misses.
Reference: [9] <author> Keith I. Farkas, Norman P. Jouppi, and Paul Chow. </author> <title> How Useful Are Non-blocking Loads, </title> <booktitle> Stream Buffers and Speculative Execution in Multiple Issue Processors? In the Proceedings of the First International Symposium on High Performance Computer Architecture, </booktitle> <pages> pages 78-89, </pages> <year> 1995. </year>
Reference-contexts: In this paper, we extend the model described in <ref> [9] </ref> by including the provision for: (1) a new prefetch strategy, called incremental prefetching, which reduces the memory traffic generated by stream buffers, and (2) a new method for dynamic calculation of strides, called the per-load stride predictor.
Reference: [10] <author> Subbarao Palacharla and R. E. Kessler. </author> <title> Evaluating Stream Buffers as a Secondary Cache Replacement. </title> <booktitle> In the Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 24-33, </pages> <year> 1994. </year>
Reference-contexts: Memory-traffic filtering has been implemented using the allocation filter technique proposed by Palacharla and Kessler <ref> [10] </ref>. This filter prevented a stream buffer from being allocated until the second miss to a stream is detected. On the second miss, a stream buffer was allocated and it began prefetching the block subsequent to the one corresponding to the second miss. <p> This process of fetching the next 2 fi N blocks if one of the last N was used to resolve a cache miss continues until the stream buffer is reallocated. Dynamic-stride prediction has been implemented using a scheme based on the minimum delta scheme proposed by Palacharla and Kessler <ref> [10] </ref>. With this scheme, on a stream-buffer miss, the allocation filter was applied to determine whether a unit-stride should be used. If there was a filter miss, then the minimum signed difference between the miss address and the last N miss addresses was determined.
Reference: [11] <author> John W. Fu, Janak H. Patel, and Bob L. Janssens. </author> <title> Stride Directed Prefetching in Scalar Processors. </title> <booktitle> In the Proceedings of the 25th Annual International Symposium on Microarchitec-ture, </booktitle> <pages> pages 102-110, </pages> <year> 1992. </year>
Reference-contexts: We introduce the per-load stride predictor. It differs from the minimum-delta scheme in that a stride is determined for a load instruction L by considering only the previous miss addresses generated by L. This predictor is based on the scheme proposed by Fu et al. <ref> [11] </ref> for preloading the data cache and is similar to the data prefetching scheme of Chen and Baer [12]. Our predictor uses a fully associative buffer to record the last miss address for N static loads, along with the program-counter address of each load.
Reference: [12] <author> Tien-Fu Chen and Jean-Loup Baer. </author> <title> Reducing Memory Latency via Non-blocking and Prefetching Caches. </title> <booktitle> In the Proceedings of the Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 51-61, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: This predictor is based on the scheme proposed by Fu et al. [11] for preloading the data cache and is similar to the data prefetching scheme of Chen and Baer <ref> [12] </ref>. Our predictor uses a fully associative buffer to record the last miss address for N static loads, along with the program-counter address of each load. Thus, a stride prediction is based only on the past memory behavior of the static load for which the prediction is being made.
Reference: [13] <author> Scott McFarling. </author> <title> Combining Branch Predictors. </title> <institution> Digital Equipment Corporation Western ResearchLab Technical Note TN-36, </institution> <year> 1993. </year>
Reference-contexts: For the eight-way issue processor, there is twice the number of ports. The model implements precise exceptions, and uses a branch prediction scheme proposed by McFarling <ref> [13] </ref> that comprises a bimodal predictor, a global history predictor, and a mechanism to select between them. The prediction scheme is used to predict the direction of conditional branches; all other control flow instructions are assumed to be 100% predictable.
Reference: [14] <author> Amitabh Srivastava and Alan Eustace. </author> <title> Atom: A system for building customized program analysis tools. </title> <booktitle> In the Proceedings of the ACM SIGPLAN `94 Conference on Programming Languages, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: Writing a register or a cache line is assumed to take one cycle. model. 3.2 Simulation Framework This study is based on execution-driven simulations using an object code instrumentation system called ATOM <ref> [14] </ref>, which is available for Alpha AXP workstations. The results presented correspond to simulations of seven benchmarks, six from the SPEC92 suite and the appsp benchmark from the NAS suite. The benchmarks are listed in Table 3 along with some run-time characteristics for the four-way and eight-way issue processors.
Reference: [15] <author> Keith I. Farkas, Paul Chow, Norman P. Jouppi, and Zvonko Vranesic. </author> <title> Memory-system Design Considerations for Dynamically-scheduled Processors. </title> <type> Technical Report 1, </type> <institution> Digital Equipment Corporation Western Research Lab, </institution> <year> 1997. </year> <note> (URL: http://www.research.digital.com/wrl/techreports). </note>
Reference-contexts: Then, these per-benchmark averages are combined using an arithmetic average to obtain the reported (overall) average commit IPC. The commit IPC values for each benchmark are given in <ref> [15] </ref>. cache configuration physical registers ip lk 64 128 f4 m4 f2 m2 f1 m1 0.5 1.5 2.5 3.5 4.5 avg IPC cache configuration physical registers i lkf4 m4 f2 m2 f1 m1 48 96 increasingly aggressive configurations p 0.5 1.5 2.5 avg IPC fetch spacing = 0 fetch spacing = <p> Due to space constraints, this data is not presented, but the following two relationships are nonetheless noted; the corresponding data is presented in <ref> [15] </ref>. First, concerning the number of physical registers and the amount of support for in-flight misses, for systems using the type i lockup-free cache, the performance impact of the stream-buffer implementation is even smaller if the number of physical registers is increased. <p> This analysis has focused on identifying performance 3 The data for these benchmarks is given in <ref> [15] </ref>. trends and design relationships. The components we considered affect the apparent time-cost of servicing cache misses and the tolerance for data-cache misses. The following conclusions can be drawn from the analysis.
References-found: 15

