URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/ps/BCJM94.ps
Refering-URL: http://csg-www.lcs.mit.edu:8001/Users/vivek/sark_pub.html
Root-URL: 
Email: dfb@watson.ibm.com and fchowjh  sarkarg@vnet.ibm.com  
Title: A Compiler Framework for Restructuring Data Declarations to Enhance Cache and TLB Effectiveness  
Author: David F. Bacon Jyh-Herng Chow Dz-ching R. Ju Kalyan Muthukumar Vivek Sarkar royju muthu vivek 
Note: Email addresses:  
Address: 555 Bailey Avenue, San Jose, CA 95141  
Affiliation: Application Development Technology Institute IBM Software Solutions Division  
Abstract: It has been observed that memory access performance can be improved by restructuring data declarations, using simple transformations such as array dimension padding and inter-array padding (array alignment) to reduce the number of misses in the cache and TLB (translation lookaside buffer). These transformations can be applied to both static and dynamic array variables. In this paper, we provide a padding algorithm for selecting appropriate padding amounts, which takes into account various cache and TLB effects collectively within a single framework. In addition to reducing the number of misses, we identify the importance of reducing the impact of cache miss jamming by spreading cache misses more uniformly across loop iterations. We translate undesirable cache and TLB behaviors into a set of constraints on padding amounts and propose a heuristic algorithm of time complexity O(m 2 fi n) to find the padding amounts to satisfy these constraints, where n is the total number of referenced arrays, and m is the total number of distinct references in the considered loop nest. The goal of the padding algorithm is to select padding amounts so that there are no set conflicts and no offset conflicts in the cache and TLB, for a given loop. In practice, this algorithm can efficiently find small padding amounts to satisfy these constraints. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> PF77 Shared Memory Parallelizer and Pre Compiler for the IBM POWER/4 System User's Guide (Preliminary Draft), </institution> <year> 1993. </year> <note> Applied Parallel Research. </note>
Reference-contexts: These two storage transformations have also been implemented in optimizing compilers and preprocessors, e.g. <ref> [1] </ref>. Past solutions have focused on some individual optimization problems in isolation from the overall problem space of selecting array-dimension/inter-array padding to reduce set/offset conflicts in the cache/TLB.
Reference: [2] <author> David Bailey. </author> <title> Unfavorable Strides in Cache Mem ory Systems. </title> <type> Technical Report RNR-92-015, </type> <institution> NASA Ames Research Center, </institution> <address> CA, </address> <year> 1992. </year>
Reference-contexts: They can also significantly improve the effectiveness of subsequent loop transformations. The two primary techniques for data declaration restructuring are array dimension padding (changing the array size) and inter-array padding (repositioning the array starting address). These techniques are not new. For example, dimension padding has been proposed in <ref> [2] </ref> to reduce cache set conflicts for a single array reference in a loop nest, and alignment of arrays to cache line boundaries has been suggested to reduce false sharing [11]. These two storage transformations have also been implemented in optimizing compilers and preprocessors, e.g. [1]. <p> Subsection 5.1 discusses extensions to Algorithm 1 for taking stride efficiency into account. 5.1 Stride Efficiency The issue of stride efficiency has been addressed in past work, e.g. <ref> [2] </ref>. We have extended Algorithm 1 to take stride efficiency into account. <p> Definition 9 (Cache Efficiency Factor of Ref j (X i )) An efficiency factor is defined as Y =(SA), where Y is the number of cache lines that still remain in the cache after SA iterations. We use the function proposed in <ref> [2] </ref> to approximate the cache efficiency factor. Suppose s = Stride (Ref j (X i )), and s=(SW ) is very close to a simple fraction a=b, b S so that D = jbs aSW j is small. <p> An extreme case of a stride with a low efficiency is a large power of two, for which all the accessed cache lines fall into the same cache set. However, non-power-of-two strides can also have poor efficiency [3]. See <ref> [2] </ref> for details. A low efficiency factor for Ref j (X i ) indicates that the cache lines containing the accessed elements of array X i fall into a small number of cache sets.
Reference: [3] <author> Jeanne Ferrante, Vivek Sarkar, and Wendy Thrash. </author> <title> On Estimating and Enhancing Cache Effectiveness. </title> <booktitle> Lecture Notes in Computer Science, (589), 1991. Proceedings of the Fourth International Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, Califor-nia, USA, </address> <month> August </month> <year> 1991. </year> <title> Edited by U. </title> <editor> Banerjee, D. Gelernter, A. Nicolau, D. </editor> <address> Padua. </address>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [13, 5, 3] </ref>). For example, the loop blocking (tiling) transformation enables cached data to be reused multiple times before they are flushed out thus reducing the number of capacity misses and collision misses [6]. Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors [4]. <p> An extreme case of a stride with a low efficiency is a large power of two, for which all the accessed cache lines fall into the same cache set. However, non-power-of-two strides can also have poor efficiency <ref> [3] </ref>. See [2] for details. A low efficiency factor for Ref j (X i ) indicates that the cache lines containing the accessed elements of array X i fall into a small number of cache sets.
Reference: [4] <author> Elana D. Granston. </author> <title> Toward a Compile-Time Methodology for Reducing False Sharing and Communication Traffic in Shared Virtual Memory Systems. </title> <booktitle> In Proc. of Sixth Workshop on Language and Compilers for Parallel Computing, </booktitle> <year> 1993. </year>
Reference-contexts: For example, the loop blocking (tiling) transformation enables cached data to be reused multiple times before they are flushed out thus reducing the number of capacity misses and collision misses [6]. Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors <ref> [4] </ref>. These loop restructuring techniques assume a fixed, known data layout for the program and try to restructure the access pattern in a loop nest to match the given data layout. This paper investigates another dimension of improving cache effectiveness by restructuring data declarations, and thus changing the data layout.
Reference: [5] <author> Manish Gupta and David A. Padua. </author> <title> Effects of Program Parallelization and Stripmining Transformation on Cache Performance in a Multiprocessor. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages I.301-I.304, </pages> <year> 1991. </year>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [13, 5, 3] </ref>). For example, the loop blocking (tiling) transformation enables cached data to be reused multiple times before they are flushed out thus reducing the number of capacity misses and collision misses [6]. Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors [4].
Reference: [6] <author> John L. Hennessy and David A. Patterson. </author> <title> Com puter Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: For example, the loop blocking (tiling) transformation enables cached data to be reused multiple times before they are flushed out thus reducing the number of capacity misses and collision misses <ref> [6] </ref>. Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors [4]. These loop restructuring techniques assume a fixed, known data layout for the program and try to restructure the access pattern in a loop nest to match the given data layout.
Reference: [7] <author> High Performance Fortran Forum. </author> <title> High Perfor mance Fortran. Language Specification Version 0.4, </title> <month> December </month> <year> 1992. </year>
Reference-contexts: Otherwise, the framework can be based to seek a solution that satisfies the most important constraints. We observe that the legality conditions for performing these storage transformations (no sequence association or storage association) are similar to the legality conditions for redistributing arrays in the High Performance Fortran language <ref> [7] </ref>. Assuming a design in which debugging information is generated after data declaration restructuring, these storage transformations have no adverse effect on program debugging. Based on this framework, we present algorithms that solve several problems that were found to have significant impact on cache and TLB effectiveness.
Reference: [8] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The Cache Performance and Optimizations of Blocked Algorithms. </title> <booktitle> In ACM International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 63-74, </pages> <year> 1991. </year>
Reference-contexts: By loop restructuring, cache and TLB performance can be optimized for each individual loop nest, but this approach is confined to the existing data layout. There have been studies that combine these two approaches. Data copying is a technique that has been suggested in <ref> [8, 10] </ref>. Instead of using a fixed array layout in every loop nest, the compiler can allocate specialized temporary arrays for each individual loop nest that will exhibit better cache behavior than the original arrays.
Reference: [9] <author> D. H. Lawrie and C. R. Vora. </author> <title> The prime memory system for array access. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-31(5):435-442, </volume> <month> October </month> <year> 1982. </year>
Reference-contexts: Therefore, by adjusting the cache offsets, cache misses can be distributed. We note that the cache offset conflict problem is similar to the memory bank conflict problem on vector machines. However, our data declaration restructuring framework is different from hardware-oriented solutions, such as the prime memory system <ref> [9] </ref>, which have been proposed in the past to tackle the memory bank conflict problem. Cache efficiency factor: Cache efficiency factor is a measure of cache utilization by examining the spread of referenced cache lines in the entire cache for a single syntactic array reference in a loop nest.
Reference: [10] <author> Olivier Temam, Elana Granston, and William Jalby. </author> <title> To Copy or Not to Copy: A Compile-Time Technique for Assessing When Data Copying Should be Used to Eliminate Cache Conflicts. </title> <booktitle> In Proc. Supercomputing '93, </booktitle> <pages> pages 410-419, </pages> <year> 1993. </year>
Reference-contexts: By loop restructuring, cache and TLB performance can be optimized for each individual loop nest, but this approach is confined to the existing data layout. There have been studies that combine these two approaches. Data copying is a technique that has been suggested in <ref> [8, 10] </ref>. Instead of using a fixed array layout in every loop nest, the compiler can allocate specialized temporary arrays for each individual loop nest that will exhibit better cache behavior than the original arrays.
Reference: [11] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> Shared Data Placement Optimizations to Reduce Multiprocessor Cache Miss Rates. </title> <booktitle> In International Conference on Parallel Processing, </booktitle> <pages> pages II.266-II.270, </pages> <year> 1990. </year>
Reference-contexts: These techniques are not new. For example, dimension padding has been proposed in [2] to reduce cache set conflicts for a single array reference in a loop nest, and alignment of arrays to cache line boundaries has been suggested to reduce false sharing <ref> [11] </ref>. These two storage transformations have also been implemented in optimizing compilers and preprocessors, e.g. [1]. Past solutions have focused on some individual optimization problems in isolation from the overall problem space of selecting array-dimension/inter-array padding to reduce set/offset conflicts in the cache/TLB.
Reference: [12] <author> Steven W. White. POWER2: </author> <title> Architecture and Performance. </title> <booktitle> In Proceedings of COMPCON '94, </booktitle> <pages> pages 384-388, </pages> <year> 1994. </year>
Reference-contexts: For example, on the IBM Power2 <ref> [12] </ref> all memory references are issued by the two fixed-point units. The latency for a cache miss to load the critical word is 8 cycles. A cache miss takes 15 cycles to complete, since the whole line must be loaded, not just the word that was referenced by the load. <p> However, our solution for eliminating cache set conflicts is applicable to physically-addressed caches, if the operating system favors the mapping of consecutive virtual memory pages to consecutive physical memory pages. We use the IBM RISC/6000 model 590 system (built on the Power2 processor architecture <ref> [12] </ref>) as the target architecture in all our examples because the Power2/590 is a state-of-the-art superscalar processor with a high cache bandwidth. However, our results are applicable to other modern cache-based machine architectures as well. <p> Example 1 Consider the Power2 architecture, which contains a 256KB data cache that is 4-way set-associative with 256 sets and 256 bytes per cache line <ref> [12] </ref>. Consider the following loop nest where the six memory locations accessed in the first iteration are mapped to different cache lines of the same cache set. Assume all variables of data type REAL*4 unless stated otherwise.
Reference: [13] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In ACM SIG-PLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <year> 1991. </year>
Reference-contexts: It is also known that cache misses can be reduced, under the constraints of limited cache size and associativity, by improving program locality through program restructuring, in particular loop nest restructuring, since loops often contain most program computation (as in <ref> [13, 5, 3] </ref>). For example, the loop blocking (tiling) transformation enables cached data to be reused multiple times before they are flushed out thus reducing the number of capacity misses and collision misses [6]. Loop blocking has also been used to eliminate false sharing in shared memory multiprocessors [4].
References-found: 13

