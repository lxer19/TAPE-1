URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/1349.ps.gz
Refering-URL: http://www.cs.tamu.edu/faculty/rwerger/pubs/
Root-URL: http://www.cs.tamu.edu
Title: Parallelizing While Loops for Multiprocessor Systems  
Author: Lawrence Rauchwerger and David Padua 
Affiliation: University of Illinois at Urbana-Champaign  
Abstract: Current parallelizing compilers treat while loops and do loops with conditional exits as sequential constructs because their iteration space is unknown. Because these types of loops arise frequently in practice, we have developed techniques that can automatically transform them for parallel execution. We succeed in parallelizing loops involving linked lists traversals something that has not been done before. This is an important problem since linked list traversals arise frequently in loops with irregular access patterns, such as sparse matrix computations. The methods can even be applied to loops whose data dependence relations cannot be analyzed at compile-time. Experimental results on loops from the PERFECT Benchmarks and sparse matrix packages show that these techniques can yield significant speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <institution> Alliant Computer Systems Corporation, </institution> <address> 42 Nagog Park, Acton, MA 01720. </address> <booktitle> FX/Series Architecture Manual, </booktitle> <year> 1986. </year>
Reference-contexts: This iteration must be found so that any iterations that need to be undone can be identified. On computers, such as the Alliant <ref> [1] </ref>, in which iterations are issued in order, the test L [vpn] &gt; i is unnecessary. In order to terminate the parallel loop cleanly before all iterations have been executed, a QUIT operation similar to the one on Alliant computers [1] could be used. <p> On computers, such as the Alliant <ref> [1] </ref>, in which iterations are issued in order, the test L [vpn] &gt; i is unnecessary. In order to terminate the parallel loop cleanly before all iterations have been executed, a QUIT operation similar to the one on Alliant computers [1] could be used. Once a QUIT command is issued by an iteration, all iterations with loop counters less than that of the issuing iteration will be initiated and completed, but no iterations with larger loop counters will be begun. <p> Note that this is is different from operating system monitors that watch such things as network traffic, i/o requests, or paging activity. 8 Experimental Results In this section we present experimental results obtained on a modestly parallel machine with 8 processors (Alliant FX/80 <ref> [1] </ref>) using a Fortran implementation of our methods. Our results scale with the number of processors and data size and they should be extrapolated for MPPs, the actual target of our methods.
Reference: [2] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer. </publisher> <address> Boston, MA., </address> <year> 1988. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [15, 12, 2, 20, 23] </ref>. In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop.
Reference: [3] <author> M. Berry and others. </author> <title> The PERFECT club benchmarks: Effective performance evaluation of supercomputers. </title> <type> TR. 827, Ctr. </type> <institution> for Supercomputing R.&D., Univ. of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1989. </year>
Reference-contexts: Our results scale with the number of processors and data size and they should be extrapolated for MPPs, the actual target of our methods. We considered five while loops that could not be paral-lelized by any compiler available to us; two loops are from the PERFECT Benchmarks <ref> [3] </ref>, two loops are from MA28, a sparse non-symmetric linear solver [5], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver [6, 7]. Our results are summarized in Table 2.
Reference: [4] <author> S. C. Chen, D. J. Kuck, and A. H. Sameh. </author> <title> Practical parallel band triangular solvers. </title> <journal> ACM Trans. on Math. Software, </journal> <volume> 4(1) </volume> <pages> 270-277, </pages> <year> 1978. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of do loops) [22] and associative recurrences <ref> [4, 13, 11] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [5] <author> I. S. Duff. </author> <title> Ma28- a set of fortran subroutines for sparse unsymmetric linear equations. </title> <type> TR. </type> <address> AERE R8730, HMSO, London, </address> <year> 1977. </year>
Reference-contexts: We considered five while loops that could not be paral-lelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse non-symmetric linear solver <ref> [5] </ref>, and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver [6, 7]. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary.
Reference: [6] <author> K. Gallivan, B. Marsolf, and H. Wijshoff. </author> <title> A large-grain parallel sparse system solver. </title> <booktitle> In Proc. 4-th SIAM Conf. on Parallel Proc. for Scient. Comp., </booktitle> <pages> pages 23-28, </pages> <address> Chicago, IL, </address> <year> 1989. </year>
Reference-contexts: five while loops that could not be paral-lelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse non-symmetric linear solver [5], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver <ref> [6, 7] </ref>. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary. Whenever necessary, we performed a simple preventive backup of the variables potentially written in the loop.
Reference: [7] <author> K. A. Gallivan, B. A. Marsolf, and H. A. G. Wijshoff. </author> <title> MCSPARSE: A parallel sparse unsymmetric linear system solver. </title> <type> TR. 1142, </type> <institution> CSRD, Univ. of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: five while loops that could not be paral-lelized by any compiler available to us; two loops are from the PERFECT Benchmarks [3], two loops are from MA28, a sparse non-symmetric linear solver [5], and one loop is extracted from MCSPARSE, a parallel version of a non-symmetric sparse linear systems solver <ref> [6, 7] </ref>. Our results are summarized in Table 2. For each method applied to a loop, we give the speedup that was obtained, and, mention whether backups and time-stamping were necessary. Whenever necessary, we performed a simple preventive backup of the variables potentially written in the loop.
Reference: [8] <author> W. L. Harrison, III. </author> <title> Compiling lisp for evaluation on a tightly coupled multiprocessor. </title> <type> TR. 565, </type> <institution> CSRD, Univ. of Illinois, </institution> <year> 1986. </year>
Reference-contexts: These new parallel constructs could be called while-doall, while-doacross, and while-doany and could prove useful in the parallel programming (manual parallelization) of applications. The methods described here extend previous works <ref> [8, 22] </ref> in that they: 1. can handle remainder variant termination conditions, 2. can test at run-time for cross-iteration data dependences in the remainder, 3. do not require work and storage for saving the values com puted in the recurrence, 4. support both static and dynamic scheduling, and 5. present a <p> We have obtained experimental results on loops from the PERFECT Benchmarks and sparse matrix packages on the Alliant FX/80 which substantiate this conclusion. 2 Parallelizing while Loops While loops have often been treated by parallelizing compilers as intrinsically sequential constructs because their iteration space is unknown <ref> [8] </ref>. A related case which is generally also handled sequentially by compilers is the do loop with a conditional exit. In this paper we propose techniques that can be used to execute such loops in parallel. <p> In [19] the authors have proposed some methods for achieving vector-like performance on multiple issue pipelined machines. They do not try to address the problem for large multiprocessors. Some techniques for solving certain types of recurrences in parallel were proposed by Harrison in <ref> [8] </ref> for Lisp-like languages. His main goal was to parallelize list operations (e.g., traversing linked lists). Generally, his methods assume that the terminator is RI and it is known that there are no cross-iteration dependences in the loop. In the context of his proposed framework ([8]), lists consist of linked chunks <p> In fact, the author mentions that if the chunk sizes become too small, then the result might be an inefficient restructured version of the loop that contains too little parallelism to recover the expense [invested] <ref> [8] </ref>. We note that when the entire list resides in a single chunk (i.e., an array), then this method is equivalent to the one described in Section 3.2 for associative recurrences, i.e., loop distribution together with a parallel prefix computation to evaluate the dispatcher in parallel. <p> When the terminator is RI and it is known that there are no cross-iteration data dependences in the loop, they suggest using the naive form of loop distribution mentioned in Section 3.3 (also implicit in 8 <ref> [8] </ref>), i.e., first a sequential while loop evaluates the dispatcher and stores its values in an array, and then the loop iterations are performed in parallel using this array. For the case of RV termination conditions no methods have been proposed in the past.
Reference: [9] <author> J. Hennessy and D. Patterson. </author> <title> Computer Architecture: A Quantata-tive Approach. </title> <publisher> Morgan Kauffman, </publisher> <address> San Mateo, CA, </address> <year> 1990. </year>
Reference-contexts: One simple method, referred to as General-1, is to serialize the accesses to the next () operation. This technique is equivalent to hardware pipelining which has been well studied in the literature <ref> [9] </ref>. The cost of synchronization may make this scheme unattractive.
Reference: [10] <author> K. Kennedy and K. S. McKinley. </author> <title> Loop distribution with arbitrary control flow. </title> <booktitle> In Supercomputing, </booktitle> <pages> pages 407-416, </pages> <year> 1990. </year>
Reference: [11] <author> C. Kruskal. </author> <title> Efficient parallel algorithms for graph problems. </title> <booktitle> In Proc. of 1990 Int. Conf. on Parallel Processing, </booktitle> <pages> pages 869-876, </pages> <year> 1986. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of do loops) [22] and associative recurrences <ref> [4, 13, 11] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [12] <author> D. J. Kuck and others. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Proceedings of the 8th ACM Symp. on Principles of Programming Languages, </booktitle> <pages> pages 207-218, </pages> <year> 1981. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [15, 12, 2, 20, 23] </ref>. In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop.
Reference: [13] <author> R. Ladner and M. Fisher. </author> <title> Parallel prefix computation. </title> <journal> J. ACM, </journal> <pages> pages 831-838, </pages> <year> 1980. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of do loops) [22] and associative recurrences <ref> [4, 13, 11] </ref> but the evaluation of general recurrences has always been of a sequential nature.
Reference: [14] <author> F. T. Leighton. </author> <title> Introduction to Parallel Algorithms and Architectures: Arrays, Trees, Hypercubes. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference: [15] <author> D. A. Padua and M. J. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Comm. of the ACM, </journal> <volume> 29 </volume> <pages> 1184-1201, </pages> <year> 1986. </year>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [15, 12, 2, 20, 23] </ref>. In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop.
Reference: [16] <author> L. Rauchwerger and D. Padua. </author> <title> The privatizing doall test: A run-time technique for doall loop identification and array privatization. </title> <booktitle> In Proc. of 1994 Int. Conf. on Supercomputing, </booktitle> <pages> pages 33-43, </pages> <year> 1994. </year>
Reference-contexts: In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test <ref> [16] </ref> and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop. These techniques were developed to test at run-time whether a do loop could be executed as a doall. <p> Before discussing how the PD test is used for while loops, we first need to briefly describe the types of operations it performs, and the data structures it uses (see <ref> [16] </ref> for a complete description of the test). The PD test is applied to each shared variable referenced during the loop whose accesses cannot be analyzed at compile-time. For convenience, we discuss the test as applied to a shared array A.
Reference: [17] <author> L. Rauchwerger and D. Padua. </author> <title> The LRPD Test: Speculative Run-Time Parallelization of Loops with Privatization and Reduction Par-allelization. </title> <booktitle> To appear in Proc. of 1995 Conf. on Programming Language Design and Implementation, </booktitle> <year> 1995. </year>
Reference-contexts: In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test <ref> [17] </ref>, for detecting the presence of cross-iteration dependences in a loop. These techniques were developed to test at run-time whether a do loop could be executed as a doall.
Reference: [18] <author> L. Rauchwerger and D. Padua. </author> <title> Parallelizing WHILE Loops for Multiprocessor Systems. </title> <type> TR. 1349, </type> <institution> CSRD, Univ. of Illinois, Urbana, IL, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The techniques are capable of extracting a substantial fraction of available parallelism in a loop. In particular, it can be shown that in the worst case our techniques will extract at least 20-25% of the parallelism inherent in the loop, <ref> [18] </ref> which can amount to significant speedups on massively parallel processors. <p> Although, we do not discuss it here due to lack of space, we note that these techniques can also be used to validate the powerful privatization and reduction parallelization (LRPD test only) transformations in while loops (see <ref> [18] </ref>). Before discussing how the PD test is used for while loops, we first need to briefly describe the types of operations it performs, and the data structures it uses (see [16] for a complete description of the test). <p> In the end this algorithm produces a parallel loop if enough parallelism is available followed by a sequential loop if any <ref> [18] </ref>. We can exploit the availability of dependence graph by scheduling the sequential loops in a doacross fashion. We remark that fusing associative recurrences evaluated by parallel prefix computations must be done carefully if there is data flow between the recurrences. <p> the cost of a failed test will be increased for the resulting loop. 7 Strategies for Applying the Techniques It can be shown that the performance gain (speedup) from our techniques ranges from a minimum of 20 25% of the ideal speedup to nearly 100% of the ideal speedup (see <ref> [18] </ref>). One method to minimize the risk of parallelizing a while loop is to execute the loop sequentially in one processor and execute it in parallel using the rest of the processors. <p> We now briefly mention some methods that can be used to address this problem (more details are provided in <ref> [18] </ref>). One simple way to reduce the additional storage is to strip-mine the loop.
Reference: [19] <author> P. Tirumalai, M. Lee, and M. Schlansker. </author> <title> Parallelization of loops with exits on pipelined architectures. </title> <booktitle> In Supercomputing, </booktitle> <year> 1990. </year>
Reference-contexts: Note that the available parallelism, and therefore our obtained speedup, is strongly dependent on the data input. 9 Related Work We can find in the literature several efforts improve the performance of while loop execution. In <ref> [19] </ref> the authors have proposed some methods for achieving vector-like performance on multiple issue pipelined machines. They do not try to address the problem for large multiprocessors. Some techniques for solving certain types of recurrences in parallel were proposed by Harrison in [8] for Lisp-like languages.
Reference: [20] <author> M. Wolfe. </author> <title> Optimizing Compilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Boston, MA, </address> <year> 1989. </year>
Reference-contexts: To aid our analysis of the dispatching recurrence, it is convenient to extract, at least conceptually, this recurrence from the original while loop by distributing <ref> [20] </ref> the original loop into two do loops with conditional exits: 1. A loop that evaluates the terms of the dispatcher (recurrence) and any termination condition that is strongly connected to the dispatcher. 2. <p> In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [15, 12, 2, 20, 23] </ref>. In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop. <p> The only previous work of which we are aware (except some early work by <ref> [20] </ref>) for parallelizing while loops in languages such as FORTRAN for multiprocessors is due to Wu and Lewis [22]. One method they propose is to pipeline the loop by executing it in doacross fashion, and to enforce any cross-iteration data dependences with explicit synchronization operations.
Reference: [21] <author> M. Wolfe. Doany: </author> <title> Not just another parallel loop. </title> <booktitle> In Proc. 5th Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <volume> vol. 757. </volume> <publisher> Springer-Verlag, </publisher> <year> 1992. </year>
Reference-contexts: In other words the program is designed to be insensitive to the order in which the columns and rows of the matrix are searched for the pivot. Originally, only the row search was parallelized by applying a technique equivalent to a doany construct <ref> [21] </ref>, leaving the traversal of columns in a sequential while loop. We fused the two loops, effectively implementing a new while-doany parallel construct. Through this technique we were able to parallelize the pivot search across the whole matrix.
Reference: [22] <author> Y. Wu and T. Lewis. </author> <title> Parallelizing while loops. </title> <booktitle> In Proc. of 1990 Int. Conf. on Parallel Processing, </booktitle> <volume> vol. II, </volume> <pages> pages 1-8, </pages> <year> 1990. </year>
Reference-contexts: Although the concurrent evaluation of recurrences is in general not possible, some special cases lend themselves to either full or partial parallelization. There are parallel algorithms to solve simple inductions (the case of do loops) <ref> [22] </ref> and associative recurrences [4, 13, 11] but the evaluation of general recurrences has always been of a sequential nature. <p> These new parallel constructs could be called while-doall, while-doacross, and while-doany and could prove useful in the parallel programming (manual parallelization) of applications. The methods described here extend previous works <ref> [8, 22] </ref> in that they: 1. can handle remainder variant termination conditions, 2. can test at run-time for cross-iteration data dependences in the remainder, 3. do not require work and storage for saving the values com puted in the recurrence, 4. support both static and dynamic scheduling, and 5. present a <p> The only previous work of which we are aware (except some early work by [20]) for parallelizing while loops in languages such as FORTRAN for multiprocessors is due to Wu and Lewis <ref> [22] </ref>. One method they propose is to pipeline the loop by executing it in doacross fashion, and to enforce any cross-iteration data dependences with explicit synchronization operations.
Reference: [23] <author> H. Zima. </author> <title> Supercompilers for Parallel and Vector Computers. </title> <publisher> ACM Press, </publisher> <address> New York, NY., </address> <year> 1991. </year> <pages> 9 10 </pages>
Reference-contexts: In order to determine whether or not the execution order of the data accesses affects the semantics of the loop, the data dependence relations between the statements in the loop body must be analyzed <ref> [15, 12, 2, 20, 23] </ref>. In related work, we have proposed run-time techniques, called the Privatizing Doall (PD) test [16] and the more powerful LRPD test [17], for detecting the presence of cross-iteration dependences in a loop.
References-found: 23

