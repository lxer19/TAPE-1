URL: ftp://hyena.cs.umd.edu/pub/papers/universal-mapper.ps.Z
Refering-URL: http://www.cs.umd.edu/projects/hpsl/compilers/compilers-pub-abs.html
Root-URL: 
Title: Runtime Support and Compilation Methods for User-Specified Irregular Data Distributions  
Author: Ravi Ponnusamy yz Joel Saltz Alok Choudhary Yuan-Shin Hwang Geoffrey Fox 
Address: College Park, MD 20742 Syracuse, NY 13244  
Affiliation: UMIACS and Computer Science Dept. Northeast Parallel Architectures Center University of Maryland Syracuse University  
Abstract: This paper describes two new ideas by which a High Performance Fortran compiler can deal with irregular computations effectively. The first mechanism invokes a user specified mapping procedure via a set of proposed compiler directives. The directives allow use of program arrays to describe graph connectivity, spatial location of array elements and computational load. The second mechanism is a conservative method for compiling irregular loops in which dependence arises only due to reduction operations. This mechanism in many cases enables a compiler to recognize that it is possible to reuse previously computed information from inspectors (e.g. communication schedules, loop iteration partitions, information that associates off-processor data copies with on-processor buffer locations). This paper also presents performance results for these mechanisms from a Fortran 90D compiler implementation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Baden. </author> <title> Programming abstractions for dynamically partitioning and coordinating localized scientific calculations running on multiprocessors. </title> <journal> SIAM J. Sci. and Stat. Computation., </journal> <volume> 12(1), </volume> <month> January </month> <year> 1991. </year>
Reference-contexts: The GEOMETRY construct can be viewed as a particular type of value based decomposition. Several researchers have developed programming environments that are targeted toward particular classes of irregular or adaptive problems. Williams [44] describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden <ref> [1] </ref> has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing.
Reference: [2] <author> S.T. Barnard and H. Simon. </author> <title> A fast multilevel implementation of recursive spectral bisection for partitioning unstruc tured problems. </title> <type> Technical Report RNR-92-033, </type> <institution> NAS Systems Division, NASA Ames Research Center, </institution> <month> November </month> <year> 1992. </year>
Reference-contexts: Only a modest effort was made to produce an efficient parallel implementation of the partitioner and it is believed that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner <ref> [2, 18] </ref>. The GeoCoL graph is partitioned into a number of subgraphs equal to the number of processors employed. It should be noted that any parallelized partitioner could be used. The Graph Generation time depicts the time required to generate the GeoCoL graph.
Reference: [3] <author> M.J. Berger and S. H. Bokhari. </author> <title> A partitioning strategy for nonuniform problems on multiprocessors. </title> <journal> IEEE Trans. on Computers, </journal> <volume> C-36(5):570-580, </volume> <month> May </month> <year> 1987. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> For instance, a user might choose a partitioner that is based on coordinates <ref> [3] </ref> to partition data. A coordinate bisection partitioner decomposes data using the spatial location of vertices in the mesh. If the user chooses a graph based partitioner, such as the spectral partitioner [39], the connectivity of the mesh could be used to decompose the data. <p> Since nearby atoms interact, the choice of a BLOCK distribution is likely to result in a large volume of communication. Consider instead a distribution based on the spatial locations of atoms. Figure 3 (b) depicts a distribution of atoms to processors carried out using a coordinate bisection partitioner <ref> [3] </ref>. Figure 3 (b) has a much smaller amounts of surface area between the portions of the molecule associated with each processor compared to that of Figure 3 (a). <p> The following two subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection <ref> [3] </ref> p p Hierarchical Decomposition [10] p p Simulated Annealing [27] p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [39, 3, 44, 17] </ref>. Table 2 lists some of the commonly used heuristics and the types of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> In such cases, each mesh point is associated with a location in space. Each graph vertex can be assigned a set of coordinates that describe its spatial location. These 16 spatial locations can be used to partition data structures <ref> [3, 31] </ref>. Vertices may also be assigned weights to represent estimated computational costs. In order to accurately estimate the computational costs, partitioners need information on how work will be partitioned. <p> To map arrays, two different kinds of parallel partitioners are employed: (1) geometry based partitioners (coordinate bisection <ref> [3] </ref> and inertial bisection [31]), and (2) a connectivity based partitioner (recursive spectral bisection [39]).
Reference: [4] <author> Z. Bozkus, A. Choudhary, G. Fox, T. Haupt, S. Ranka, and M.-Y. Wu. </author> <title> Compiling Fortran 90D/HPF for distributed memory MIMD computers. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 15-26, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: In the Vienna Fortran [45] language definition a user can specify a customized distribution function. The runtime support and compiler transformation strategies described here can also be applied to Vienna Fortran. 3 These ideas have been implemented using the Syracuse Fortran 90D/HPF compiler <ref> [4] </ref>. The following assumptions have been made: 1. irregular accesses are carried out in the context of a single or multiple statement parallel loops. <p> Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [14, 4] </ref> and the Vienna Fortran compiler project [45] at the University of Vienna are two examples. The Jade project at Stanford [24], the DINO project at Colorado [37], Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments.
Reference: [5] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D. J. States, S. Swaminathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculations. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4:187, </volume> <year> 1983. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM <ref> [5] </ref>, AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions. <p> Mesh edges would be partitioned so that 1) good load balance is obtained and 2) computations mostly employ locally stored data. Other unstructured problems have analogous indirectly accessed arrays. For instance, consider the non-bonded force calculation in the molecular dynamics code CHARMM <ref> [5] </ref>. <p> These performance measurements are for a loop over edges from a 3-D unstructured Euler solver [29] for both 10K and 53K mesh points, and for an electrostatic force calculation loop in a molecular dynamics code for a 648 atom water simulation <ref> [5] </ref>. The functionality of these loops is equivalent to the loop L1 in Figure 10. Table 3 presents the performance results of the compiler generated code with and without the schedule reuse technique.
Reference: [6] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an irregular application on a distributed memory multipro cessor. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <month> May </month> <year> 1993. </year> <journal> ACM SIGPLAN Notices, </journal> <volume> Vol. 28, No. </volume> <pages> 7. </pages>
Reference-contexts: The Jade project at Stanford [24], the DINO project at Colorado [37], Kathy Yelick's work <ref> [6] </ref> at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project [23], Marina Chen's work at Yale [26] and the PARTI project [30, 38].
Reference: [7] <author> B. Chapman, P. Mehrotra, and H. Zima. </author> <title> Programming in Vienna Fortran. </title> <journal> Scientific Programming, </journal> <volume> 1(1) </volume> <pages> 31-50, </pages> <month> Fall </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [14], Loveman et al [13], i and Chapman et al <ref> [7, 8] </ref>. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [8] <author> Barbara Chapman, Piyush Mehrotra, and Hans Zima. </author> <title> Programming in Vienna Fortran. </title> <type> Technical Report 92-9, </type> <institution> ICASE, NASA Langley Research Center, </institution> <month> March </month> <year> 1992. </year>
Reference-contexts: Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [14], Loveman et al [13], i and Chapman et al <ref> [7, 8] </ref>. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [9] <author> A. Choudhary, G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, S. Ranka, and J. Saltz. </author> <title> Software support for irregular and loosely synchronous problems. </title> <booktitle> Computing Systems in Engineering, 3(1-4):43-52, 1992. Papers presented at the Symposium on High-Performance Computing for Flight Vehicles, </booktitle> <month> December </month> <year> 1992. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. These problems are called irregular concurrent problems <ref> [9] </ref>. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. <p> The reduction operations in a Forall construct are specified using the Fortran D REDUCE construct. Reduction inside a Forall construct is important for representing a considerable set of scientific computations such as those found in sparse and unstructured problems <ref> [9] </ref>. This representation also preserves explicit parallelism available in the underlying computations. 13 3 Communication Schedule Reuse The cost of carrying out an inspector (phases B, C and D in Figure 8) can be amortized when the information produced by the inspector is computed once and then used repeatedly. <p> Vertex weights can be used as the sole partitioning criterion in problems in which computational costs dominate. Examples of such code include the flame simulation code described in Section 2.1.2 and "embarrassingly parallel problems" <ref> [9] </ref>, where computational cost predominates. A given partitioner can make use of a combination of connectivity, geometrical and weight information.
Reference: [10] <author> T. W. Clark, R. v. Hanxleden, J. A. McCammon, and L. R. Scott. </author> <title> Parallelization strategies for a molecular dynamics program. In Intel Supercomputer University Partners Conference, </title> <address> Timberline Lodge, Mt. Hood, OR, </address> <month> April </month> <year> 1992. </year>
Reference-contexts: The following two subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition <ref> [10] </ref> p p Simulated Annealing [27] p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. <p> For instance, sometimes it is important to take estimated computational costs into account when carrying out coordinate or inertial bisection for problems where computational costs vary greatly from node to node. Other partitioners make use of both geometrical and connectivity information <ref> [10] </ref>. Since the data structure that stores information on which data partitioning is to be based can represent Geometrical, Connectivity and/or Load information, it is called the GeoCoL data structure.
Reference: [11] <author> R. Das, D. J. Mavriplis, J. Saltz, S. Gupta, and R. Ponnusamy. </author> <title> The design and implementation of a parallel unstructured Euler solver using software primitives. </title> <journal> AIAA Journal, </journal> <volume> 32(3) </volume> <pages> 489-496, </pages> <month> March </month> <year> 1994. </year>
Reference-contexts: The project is called CHAOS; the runtime support is called the CHAOS library [33]. The CHAOS library is a superset of the PARTI library <ref> [30, 38, 11] </ref>. Solving concurrent irregular problems on distributed memory machines using CHAOS runtime support involves five major steps (Figure 8). The first three steps in the figure concern mapping data and computations onto processors.
Reference: [12] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing analysis and indirect access to distributed arrays. </title> <booktitle> In Proceedings of the 6th Workshop on Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 152-168. </pages> <publisher> Springer-Verlag, </publisher> <month> August </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3076 and UMIACS-TR-93-42. </note>
Reference-contexts: The compile time analysis needed to reuse inspector communication schedules is touched upon in Das et al <ref> [12] </ref>. This paper proposes a conservative method that in many cases allows reuse of the results from inspectors.
Reference: [13] <author> D. Loveman (Ed.). </author> <title> Draft High Performance Fortran language specification, </title> <note> version 1.0. Technical Report CRPC TR92225, Center for Research on Parallel Computation, </note> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Another example would be to assign consecutively indexed array elements to processors in a round-robin fashion. These two data distribution schemes are often called BLOCK and CYCLIC data distributions <ref> [13] </ref>, respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements [39, 44, 27, 25, 3, 17]. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. <p> Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al [14], Loveman et al <ref> [13] </ref>, i and Chapman et al [7, 8]. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions.
Reference: [14] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <institution> Department of Computer Science Rice COMP TR90-141, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: The paper also presents methods and a prototype implementation that make it possible for compilers to efficiently handle irregular problems coded using a set of language extensions closely related to Fortran D <ref> [14] </ref>, Vienna Fortran [45] and High Performance Fortran (HPF) [19]. The optimizations that must be carried out to solve irregular concurrent problems efficiently on a distributed memory machine include: 1. data partitioning, fl This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821) and ONR (SC292-1-22913). <p> Vienna Fortran, Fortran D and HPF provide a rich set of data decomposition specifications. A definition of such language extensions may be found in Fox et al <ref> [14] </ref>, Loveman et al [13], i and Chapman et al [7, 8]. Fortran D and HPF require that users explicitly define how data is to be distributed. Vienna Fortran allows users to write procedures to generate user defined distributions. <p> Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse <ref> [14, 4] </ref> and the Vienna Fortran compiler project [45] at the University of Vienna are two examples. The Jade project at Stanford [24], the DINO project at Colorado [37], Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments.
Reference: [15] <author> S. Hammond and T. Barth. </author> <title> An optimal massively parallel Euler solver for unstructured grids. </title> <journal> AIAA Journal, </journal> <note> AIAA Paper 91-0441, January 1991. 28 </note>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers <ref> [29, 15] </ref>, molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions.
Reference: [16] <author> R. v. Hanxleden, K. Kennedy, and J. Saltz. </author> <title> Value-based distributions in fortran d | a preliminary report. </title> <type> Technical Report CRPC-TR93365-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> December </month> <year> 1993. </year> <title> submitted to Journal of Programming Languages Special Issue on Compiling and Run-Time Issues for Distributed Address Space Machines. </title>
Reference-contexts: To the best of the authors' knowledge, the implementation described in this paper was the first distributed memory compiler to provide this kind of support. User specified partitioning has recently been implemented in the D System Fortran 77D compiler <ref> [16] </ref>; the CHAOS runtime support described in this paper has been employed in this implementation. In the Vienna Fortran [45] language definition a user can specify a customized distribution function. <p> The GEOMETRY construct is closely related to the geometrical partitioning or value based decomposition directives proposed by von Hanxleden <ref> [16] </ref>. Similarly, a GeoCoL data structure that specifies only vertex weights can be constructed using the keyword LOAD as follows. C$ CONSTRUCT G2 (N, LOAD (weight)) Here, a GeoCoL structure called G2 consists of N vertices with vertex i having LOAD weight (i). <p> Finally, Table 9 summarizes the compiler performance for all the codes and presents a comparison with the hand coded version. For all problems, the performance of the compiler generated code is within 15% of the hand coded version. 7 Related Work Research has been carried out by von Hanxleden <ref> [16] </ref> on compiler-linked partitioners that decompose arrays based on distributed array element values; these are called value based decompositions. The GEOMETRY construct can be viewed as a particular type of value based decomposition. Several researchers have developed programming environments that are targeted toward particular classes of irregular or adaptive problems.
Reference: [17] <author> R. v. Hanxleden and L. R. Scott. </author> <title> Load balancing on message passing architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 312-324, </pages> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [39, 3, 44, 17] </ref>. Table 2 lists some of the commonly used heuristics and the types of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually.
Reference: [18] <author> B. Hendrickson and R. Leland. </author> <title> An improved spectral graph partitioning algorithm for mapping parallel computations. </title> <type> Technical Report SAND 92-1460, </type> <institution> Sandia National Laboratory, </institution> <address> Albuquerque, </address> <month> September </month> <year> 1992. </year>
Reference-contexts: Only a modest effort was made to produce an efficient parallel implementation of the partitioner and it is believed that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner <ref> [2, 18] </ref>. The GeoCoL graph is partitioned into a number of subgraphs equal to the number of processors employed. It should be noted that any parallelized partitioner could be used. The Graph Generation time depicts the time required to generate the GeoCoL graph.
Reference: [19] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification. </title> <journal> Scientific Programming, </journal> <volume> 2(1 </volume> 2):1-170, 1993. 
Reference-contexts: The paper also presents methods and a prototype implementation that make it possible for compilers to efficiently handle irregular problems coded using a set of language extensions closely related to Fortran D [14], Vienna Fortran [45] and High Performance Fortran (HPF) <ref> [19] </ref>. The optimizations that must be carried out to solve irregular concurrent problems efficiently on a distributed memory machine include: 1. data partitioning, fl This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821) and ONR (SC292-1-22913). Also supported by NASA Contract No.
Reference: [20] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. In Compilers and Runtime Software for Scalable Multiprocessors, </title> <editor> J. Saltz and P. Mehrotra Editors, </editor> <address> Amsterdam, The Netherlands, </address> <note> To appear 1991. Elsevier. </note>
Reference-contexts: The Jade project at Stanford [24], the DINO project at Colorado [37], Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments. Runtime compilation methods have been employed in four compiler projects: the Fortran D project <ref> [20] </ref>, the Kali 26 project [23], Marina Chen's work at Yale [26] and the PARTI project [30, 38]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38].
Reference: [21] <author> A. Jameson, T. J. Baker, and N. P. Weatherhill. </author> <title> Calculation of inviscid transonic flow over a complete aircraft. </title> <type> AIAA paper 86-0103, </type> <month> January </month> <year> 1986. </year>
Reference-contexts: In that code, computational costs vary dynamically and cannot be estimated until runtime. 2.1.1 Codes with Indirectly Accessed Arrays The first application code is an unstructured Euler solver used to study the flow of air over an airfoil <ref> [29, 21] </ref>. Complex aerodynamic shapes require high resolution meshes and, consequently, large numbers of mesh points. A mesh vertex is an abstraction represented by Fortran array data structures. Physical values (e.g. velocity, pressure) are associated with each mesh vertex. These values are called flow variables and are stored in arrays.
Reference: [22] <author> B.W. Kernighan and S. Lin. </author> <title> An efficient heuristic procedure for partitioning graphs. </title> <journal> Bell System Technical Journal, </journal> <volume> 49(2) </volume> <pages> 291-307, </pages> <month> February </month> <year> 1970. </year>
Reference-contexts: Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing [27] p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin <ref> [22] </ref> 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. Assume that loop iterations will be partitioned using a user-defined criterion similar to that used for data partitioning. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [39, 22, 27] </ref>. Graph vertices represent array indices; graph edges represent dependencies. Consider the example loop L1 in Figure 10. The graph vertices represent the N elements of arrays x and y.
Reference: [23] <author> C. Koelbel, P. Mehrotra, and J. Van Rosendale. </author> <title> Supporting shared data structures on distributed memory architec tures. </title> <booktitle> In 2nd ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 177-186. </pages> <publisher> ACM, </publisher> <month> March </month> <year> 1990. </year>
Reference-contexts: Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project <ref> [23] </ref>, Marina Chen's work at Yale [26] and the PARTI project [30, 38]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38]. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project <ref> [23] </ref>, Marina Chen's work at Yale [26] and the PARTI project [30, 38]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38]. In earlier work, a strategy was outlined that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops [36].
Reference: [24] <author> Monica Lam, Edward E. Rothberg, and Michael E. Wolf. </author> <title> The cache performance and optimizations of block algorithms. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 63-74. </pages> <publisher> ACM Press, </publisher> <month> April </month> <year> 1991. </year>
Reference-contexts: There are a variety of compiler projects targeting distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [14, 4] and the Vienna Fortran compiler project [45] at the University of Vienna are two examples. The Jade project at Stanford <ref> [24] </ref>, the DINO project at Colorado [37], Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments.
Reference: [25] <author> W. E. Leland. </author> <title> Load-balancing heuristics and process behavior. </title> <booktitle> In Proceedings of Performance 86 and ACM SIG METRICS 86, </booktitle> <pages> pages 54-69, </pages> <year> 1986. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution.
Reference: [26] <author> L. C. Lu and M.C. Chen. </author> <title> Parallelizing loops with indirect array references or pointers. </title> <booktitle> In Proceedings of the Fourth Workshop on Languages and Compilers for Parallel Computing, </booktitle> <address> Santa Clara, CA, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project [23], Marina Chen's work at Yale <ref> [26] </ref> and the PARTI project [30, 38]. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38].
Reference: [27] <author> N. Mansour. </author> <title> Physical optimization algorithms for mapping data to distributed-memory multiprocessors. </title> <type> Technical report, Ph.D. Dissertation, </type> <institution> School of Computer Science,Syracuse University, </institution> <year> 1992. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> The following two subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing <ref> [27] </ref> p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. <p> The following two subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing <ref> [27] </ref> p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. <p> subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing <ref> [27] </ref> p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. Assume that loop iterations will be partitioned using a user-defined criterion similar to that used for data partitioning. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [39, 22, 27] </ref>. Graph vertices represent array indices; graph edges represent dependencies. Consider the example loop L1 in Figure 10. The graph vertices represent the N elements of arrays x and y.
Reference: [28] <author> D. J. Mavriplis. </author> <title> Adaptive mesh generation for viscous flows using delaunay triangulation. </title> <journal> Journal of Computational Physics, </journal> <volume> 90(2) </volume> <pages> 271-291, </pages> <year> 1990. </year>
Reference-contexts: Since meshes are typically associated with physical objects, a spatial location can often be associated with each mesh point. The spatial locations of the mesh points and the connectivity of the vertices are determined by the mesh generation strategy <ref> [42, 28] </ref>. Figure 2 depicts a mesh generated by such a process. This is an unstructured mesh representation of a three dimensional aircraft wing.
Reference: [29] <author> D. J. Mavriplis. </author> <title> Three dimensional unstructured multigrid for the Euler equations, paper 91-1549cp. </title> <booktitle> In AIAA 10th Computational Fluid Dynamics Conference, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: In this class of problems, once runtime information is available, data access patterns are known before each computational phase. These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers <ref> [29, 15] </ref>, molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions. <p> In that code, computational costs vary dynamically and cannot be estimated until runtime. 2.1.1 Codes with Indirectly Accessed Arrays The first application code is an unstructured Euler solver used to study the flow of air over an airfoil <ref> [29, 21] </ref>. Complex aerodynamic shapes require high resolution meshes and, consequently, large numbers of mesh points. A mesh vertex is an abstraction represented by Fortran array data structures. Physical values (e.g. velocity, pressure) are associated with each mesh vertex. These values are called flow variables and are stored in arrays. <p> These performance measurements are for a loop over edges from a 3-D unstructured Euler solver <ref> [29] </ref> for both 10K and 53K mesh points, and for an electrostatic force calculation loop in a molecular dynamics code for a 648 atom water simulation [5]. The functionality of these loops is equivalent to the loop L1 in Figure 10.
Reference: [30] <author> R. Mirchandaney, J. H. Saltz, R. M. Smith, D. M. Nicol, and Kay Crowley. </author> <title> Principles of runtime support for parallel processors. </title> <booktitle> In Proceedings of the 1988 ACM International Conference on Supercomputing, </booktitle> <pages> pages 140-152, </pages> <month> July </month> <year> 1988. </year>
Reference-contexts: During program execution, pre-processing examines the data references of distributed arrays. Each processor pre-computes which data need to be exchanged. The result of this pre-processing is a communication schedule <ref> [30] </ref>. Each processor uses communication schedules to exchange required data before and after executing a loop. The same schedules can be used repeatedly, as long as the data reference patterns remain unchanged. In Figure 1, loop L2 is carried out many times inside loop L1. <p> The project is called CHAOS; the runtime support is called the CHAOS library [33]. The CHAOS library is a superset of the PARTI library <ref> [30, 38, 11] </ref>. Solving concurrent irregular problems on distributed memory machines using CHAOS runtime support involves five major steps (Figure 8). The first three steps in the figure concern mapping data and computations onto processors. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project [23], Marina Chen's work at Yale [26] and the PARTI project <ref> [30, 38] </ref>. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38].
Reference: [31] <author> B. Nour-Omid, A. Raefsky, and G. Lyzenga. </author> <title> Solving finite element equations on concurrent computers. </title> <booktitle> In Proc. of Symposium on Parallel Computations and theis Impact on Mechanics, </booktitle> <address> Boston, </address> <month> December </month> <year> 1987. </year>
Reference-contexts: 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection [39] p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing [27] p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection <ref> [31] </ref> p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations have not yet been assigned to processors. Assume that loop iterations will be partitioned using a user-defined criterion similar to that used for data partitioning. <p> In such cases, each mesh point is associated with a location in space. Each graph vertex can be assigned a set of coordinates that describe its spatial location. These 16 spatial locations can be used to partition data structures <ref> [3, 31] </ref>. Vertices may also be assigned weights to represent estimated computational costs. In order to accurately estimate the computational costs, partitioners need information on how work will be partitioned. <p> To map arrays, two different kinds of parallel partitioners are employed: (1) geometry based partitioners (coordinate bisection [3] and inertial bisection <ref> [31] </ref>), and (2) a connectivity based partitioner (recursive spectral bisection [39]).
Reference: [32] <author> G. Patnaik, K.J. Laskey, K. Kailasanath, E.S. Oran, and T. V. Brun. </author> <title> FLIC A detailed, two-dimensional flame model. </title> <type> NRL Report 6555, </type> <institution> Naval Research Laboratory, </institution> <address> Washington, DC, </address> <month> September </month> <year> 1989. </year>
Reference-contexts: Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes <ref> [32] </ref>. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions.
Reference: [33] <author> R. Ponnusamy. </author> <title> A manual for the CHAOS runtime library. </title> <type> Technical Report TR93-105, </type> <institution> Computer Science Department, University of Maryland, </institution> <month> December </month> <year> 1993. </year> <note> (Available at anonymous ftp site hpsl.cs.umd.edu). </note>
Reference-contexts: The gather on each processor fetches all the necessary x references that reside off-processor. The scatter add call accumulates the off-processor y values. A detailed description of the functionality of these procedures is given in Ponnusamy <ref> [33] </ref>. 2.4 Overview of CHAOS Efficient runtime support has developed to deal with problems that consist of a sequence of clearly demarcated concurrent computational phases. The project is called CHAOS; the runtime support is called the CHAOS library [33]. <p> detailed description of the functionality of these procedures is given in Ponnusamy <ref> [33] </ref>. 2.4 Overview of CHAOS Efficient runtime support has developed to deal with problems that consist of a sequence of clearly demarcated concurrent computational phases. The project is called CHAOS; the runtime support is called the CHAOS library [33]. The CHAOS library is a superset of the PARTI library [30, 38, 11]. Solving concurrent irregular problems on distributed memory machines using CHAOS runtime support involves five major steps (Figure 8). The first three steps in the figure concern mapping data and computations onto processors.
Reference: [34] <author> Ravi Ponnusamy, Yuan-Shin Hwang, Joel Saltz, Alok Choudhary, and Geoffrey Fox. </author> <title> Supporting irregular distributions in FORTRAN 90D/HPF compilers. </title> <institution> Technical Report CS-TR-3268 and UMIACS-TR-94-57, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: The approach described here requires more input from the user and less compiler support. A short version of the techniques described in this paper appeared in a conference proceedings [35]. Support for irregular data distributions in HPF, using intrinsic functions, has been proposed by Ponnusamy et al <ref> [34] </ref>. Recently, support for irregular data distribution has been implemented on the Vienna Fortran Compiler, using CHAOS runtime procedures, in collaboration with this research group. 8 Conclusions This paper has described work that demonstrates two new mechanisms for dealing effectively with irregular computations.
Reference: [35] <author> Ravi Ponnusamy, Joel Saltz, and Alok Choudhary. </author> <title> Runtime-compilation techniques for data partitioning and commu nication schedule reuse. </title> <booktitle> In Proceedings Supercomputing '93, </booktitle> <pages> pages 361-370. </pages> <publisher> IEEE Computer Society Press, </publisher> <month> November </month> <year> 1993. </year> <note> Also available as University of Maryland Technical Report CS-TR-3055 and UMIACS-TR-93-32. 29 </note>
Reference-contexts: The approach described here requires more input from the user and less compiler support. A short version of the techniques described in this paper appeared in a conference proceedings <ref> [35] </ref>. Support for irregular data distributions in HPF, using intrinsic functions, has been proposed by Ponnusamy et al [34].
Reference: [36] <author> Ravi Ponnusamy, Joel Saltz, Charles Koelbel, and Alok Choudhary. </author> <title> A runtime mapping scheme for irregular prob lebms. </title> <booktitle> In Proceedings of the Scalable High Performance Computing Conference (SHPCC-92), </booktitle> <pages> pages 216-219, </pages> <address> Williams-burg, VA, April 1992. </address> <publisher> IEEE Computer Society Press. </publisher>
Reference-contexts: This approach makes an implicit assumption that most (although not necessarily all) computation will be carried out in the processor associated with the variable appearing on the left hand side of each statement this approach is called the almost owner computes rule <ref> [36] </ref>. There are many partitioning heuristics methods available based on physical phenomena and proximity [39, 3, 44, 17]. Table 2 lists some of the commonly used heuristics and the types of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. <p> In earlier work, a strategy was outlined that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops <ref> [36] </ref>. The approach described here requires more input from the user and less compiler support. A short version of the techniques described in this paper appeared in a conference proceedings [35]. Support for irregular data distributions in HPF, using intrinsic functions, has been proposed by Ponnusamy et al [34].
Reference: [37] <author> Matthew Rosing, Robert B. Schnabel, and Robert P. Weaver. </author> <title> The DINO parallel programming language. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13(1) </volume> <pages> 30-42, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: There are a variety of compiler projects targeting distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [14, 4] and the Vienna Fortran compiler project [45] at the University of Vienna are two examples. The Jade project at Stanford [24], the DINO project at Colorado <ref> [37] </ref>, Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments.
Reference: [38] <author> J. Saltz, H. Berryman, and J. Wu. </author> <title> Runtime compilation for multiprocessors. </title> <journal> Concurrency: Practice and Experience, </journal> <volume> 3(6) </volume> <pages> 573-592, </pages> <year> 1991. </year>
Reference-contexts: The project is called CHAOS; the runtime support is called the CHAOS library [33]. The CHAOS library is a superset of the PARTI library <ref> [30, 38, 11] </ref>. Solving concurrent irregular problems on distributed memory machines using CHAOS runtime support involves five major steps (Figure 8). The first three steps in the figure concern mapping data and computations onto processors. <p> Finally, in Phase E, information from the earlier phases is used to carry out the computation and commu nication. CHAOS and PARTI procedures have been used in a variety of applications, including sparse matrix linear solvers, adaptive computational fluid dynamics codes, molecular dynamics codes and a prototype compiler <ref> [38] </ref> aimed at distributed memory multiprocessors. 2.5 Overview of Existing Language Support While these data decomposition directives are presented in the context of Fortran D, the same optimizations and analogous language extensions could be used for a wide range of languages and compilers such as Vienna Fortran, pC++, and HPF. <p> to avoid generating a new GeoCoL graph and carrying out a potentially expensive data repartition when no change has occurred. 4 Coupling Partitioners In irregular problems, it is often desirable to allocate computational work to processors by assigning all computations that involve a given loop iteration to a single processor <ref> [38] </ref>. Consequently, both distributed arrays and loop iterations are partitioned using a two-phase approach (Figure 8). In the first phase, termed the data partitioning phase, distributed arrays are partitioned. In the second phase, called loop iteration partitioning, loop iterations are partitioned using the information from the first phase. <p> Runtime compilation methods have been employed in four compiler projects: the Fortran D project [20], the Kali 26 project [23], Marina Chen's work at Yale [26] and the PARTI project <ref> [30, 38] </ref>. The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays [38]. <p> The Kali compiler was the first compiler to implement inspector/executor type runtime preprocessing [23] and the ARF compiler was the first compiler to support irregularly distributed arrays <ref> [38] </ref>. In earlier work, a strategy was outlined that would make it possible for compilers to generate compiler embedded connectivity based partitioners directly from marked loops [36]. The approach described here requires more input from the user and less compiler support.
Reference: [39] <author> H. Simon. </author> <title> Partitioning of unstructured mesh problems for parallel processing. </title> <booktitle> In Proceedings of the Conference on Parallel Methods on Large Scale Structural Analysis and Physics Applications. </booktitle> <publisher> Pergamon Press, </publisher> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> For instance, a user might choose a partitioner that is based on coordinates [3] to partition data. A coordinate bisection partitioner decomposes data using the spatial location of vertices in the mesh. If the user chooses a graph based partitioner, such as the spectral partitioner <ref> [39] </ref>, the connectivity of the mesh could be used to decompose the data. The next step in parallelizing this application involves assigning equal amounts of work to processors. An unstructured Euler solver consists of a sequence of loops that sweep over a mesh. <p> This appears to be a practical approach, as in many cases the same set of distributed arrays are used by many loops. The following two subsections describe the phases. 15 Table 2: Common Partitioning Heuristics Partitioner Reference Spatial Connectivity Vertex Edge Information Information Weight Weight Spectral Bisection <ref> [39] </ref> p p p Coordinate Bisection [3] p p Hierarchical Decomposition [10] p p Simulated Annealing [27] p p p Neural Network [27] p p p Genetic Algorithms [27] p p p Inertial Bisection [31] p p Kernighan - Lin [22] 4.1 Data Partitioning When distributed arrays are partitioned, loop iterations <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [39, 3, 44, 17] </ref>. Table 2 lists some of the commonly used heuristics and the types of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> Data partitioners can make use of different kinds of program information. Some partitioners operate on data structures that represent undirected graphs <ref> [39, 22, 27] </ref>. Graph vertices represent array indices; graph edges represent dependencies. Consider the example loop L1 in Figure 10. The graph vertices represent the N elements of arrays x and y. <p> To map arrays, two different kinds of parallel partitioners are employed: (1) geometry based partitioners (coordinate bisection [3] and inertial bisection [31]), and (2) a connectivity based partitioner (recursive spectral bisection <ref> [39] </ref>). <p> Generated No Load Balance (Time in Load Comp Total Load Comp Total Total Sec.) Balance Balance 1024x32 16 4.1 19.4 23.5 4.3 19.4 23.7 117 1024x128 32 8.5 34.1 42.9 8.5 34.6 43.4 397 tion the GeoCoL graph data structure using a parallelized version of Simon's single level spectral partitioner <ref> [39] </ref>. Only a modest effort was made to produce an efficient parallel implementation of the partitioner and it is believed that the performance and the execution time of the partitioner can be tremendously improved by using a multilevel version of the partitioner [2, 18].
Reference: [40] <author> W. F. van Gunsteren and H. J. C. Berendsen. Gromos: </author> <title> Groningen molecular simulation software. </title> <type> Technical report, </type> <institution> Laboratory of Physical Chemistry, University of Groningen, </institution> <address> Nijenborgh, The Netherlands, </address> <year> 1988. </year>
Reference-contexts: These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS <ref> [40] </ref>, etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions.
Reference: [41] <author> P. Venkatkrishnan, J. Saltz, and D. Mavriplis. </author> <title> Parallel preconditioned iterative methods for the compressible navier stokes equations. </title> <booktitle> In 12th International Conference on Numerical Methods in Fluid Dynamics, </booktitle> <address> Oxford, England, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM [5], AMBER [43], GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers <ref> [41] </ref>, and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions.
Reference: [42] <author> N. P. Weatherill. </author> <title> The generation of unstructured grids using dirichlet tessalations. </title> <type> Report MAE 1715, </type> <institution> Princeton, </institution> <month> July </month> <year> 1985. </year>
Reference-contexts: Since meshes are typically associated with physical objects, a spatial location can often be associated with each mesh point. The spatial locations of the mesh points and the connectivity of the vertices are determined by the mesh generation strategy <ref> [42, 28] </ref>. Figure 2 depicts a mesh generated by such a process. This is an unstructured mesh representation of a three dimensional aircraft wing.
Reference: [43] <author> P. K. Weiner and P. A. Kollman. </author> <title> Amber:assisted model building with energy refinement. a general program for modeling molecules and their interactions. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 2:287, </volume> <year> 1981. </year>
Reference-contexts: These problems are called irregular concurrent problems [9]. Examples of irregular concurrent problems include adaptive and self-adaptive explicit, multigrid unstructured computational fluid dynamic solvers [29, 15], molecular dynamics codes (CHARMM [5], AMBER <ref> [43] </ref>, GROMOS [40], etc.), diagonal or polynomial preconditioned iterative linear solvers [41], and time dependent flame modeling codes [32]. This paper focuses on the runtime support, the language extensions, and the compiler support required to provide efficient data and work load distributions.
Reference: [44] <author> R. Williams. </author> <title> Performance of dynamic load balancing algorithms for unstructured mesh calculations. </title> <journal> Concurrency, Practice and Experience, </journal> <volume> 3(5) </volume> <pages> 457-482, </pages> <month> February </month> <year> 1991. </year>
Reference-contexts: These two data distribution schemes are often called BLOCK and CYCLIC data distributions [13], respectively. Researchers have developed a variety of heuristic methods to obtain data mappings that are designed to optimize irregular problem communication requirements <ref> [39, 44, 27, 25, 3, 17] </ref>. The distribution produced by these methods typically results in a table that lists a processor assignment for each array element. This kind of distribution is often called an irregular distribution. <p> There are many partitioning heuristics methods available based on physical phenomena and proximity <ref> [39, 3, 44, 17] </ref>. Table 2 lists some of the commonly used heuristics and the types of information they use for partitioning. Most data partitioners make use of undirected connectivity graphs and spatial information. Currently these partitioners must be coupled to user programs manually. <p> The GEOMETRY construct can be viewed as a particular type of value based decomposition. Several researchers have developed programming environments that are targeted toward particular classes of irregular or adaptive problems. Williams <ref> [44] </ref> describes a programming environment (DIME) for calculations with unstructured triangular meshes using distributed memory machines. Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing.
Reference: [45] <author> H. Zima, P. Brezany, B. Chapman, P. Mehrotra, and A. Schwald. </author> <title> Vienna Fortran a language specification. </title> <type> Report ACPC-TR92-4, </type> <institution> Austrian Center for Parallel Computation, University of Vienna, Vienna, Austria, </institution> <year> 1992. </year> <month> 30 </month>
Reference-contexts: The paper also presents methods and a prototype implementation that make it possible for compilers to efficiently handle irregular problems coded using a set of language extensions closely related to Fortran D [14], Vienna Fortran <ref> [45] </ref> and High Performance Fortran (HPF) [19]. The optimizations that must be carried out to solve irregular concurrent problems efficiently on a distributed memory machine include: 1. data partitioning, fl This work was sponsored in part by ARPA (NAG-1-1485), NSF (ASC 9213821) and ONR (SC292-1-22913). <p> User specified partitioning has recently been implemented in the D System Fortran 77D compiler [16]; the CHAOS runtime support described in this paper has been employed in this implementation. In the Vienna Fortran <ref> [45] </ref> language definition a user can specify a customized distribution function. The runtime support and compiler transformation strategies described here can also be applied to Vienna Fortran. 3 These ideas have been implemented using the Syracuse Fortran 90D/HPF compiler [4]. <p> Once the partitioner generates a new distribution, the arrays can be redistributed based on it. A communication schedule is built and used to redistribute the arrays from the default to the new distribution. Vienna Fortran <ref> [45] </ref> provides support for the user to specify a function for distributing data. <p> Baden [1] has developed a programming environment targeted towards particle computations. This programming environment provides facilities that support dynamic load balancing. There are a variety of compiler projects targeting distributed memory multiprocessors: the Fortran D compiler projects at Rice and Syracuse [14, 4] and the Vienna Fortran compiler project <ref> [45] </ref> at the University of Vienna are two examples. The Jade project at Stanford [24], the DINO project at Colorado [37], Kathy Yelick's work [6] at Berkeley, and the CODE project at University of Texas, Austin provide parallel programming environments.
References-found: 45

