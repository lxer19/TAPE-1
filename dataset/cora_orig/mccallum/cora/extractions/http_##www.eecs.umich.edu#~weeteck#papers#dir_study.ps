URL: http://www.eecs.umich.edu/~weeteck/papers/dir_study.ps
Refering-URL: http://www.eecs.umich.edu/~weeteck/pub.html
Root-URL: http://www.eecs.umich.edu
Title: Measuring Memorys Resistance to Operating System Crashes 1 Measuring Memorys Resistance to Operating System Crashes  
Author: Wee Teck Ng Advisor: Prof Peter M. Chen 
Affiliation: Computer Science and Engineering Division Department of Electrical Engineering and Computer Science University of Michigan  
Abstract-found: 0
Intro-found: 1
Reference: [Abbott94] <author> M. Abbott, D. Har, L. Herger, M. Kauffmann, K. Mak, J. Murdock, C. Schulz, T. B. Smith, B. Tremaine, D. Yeh, and L. Wong. </author> <title> Durable Memory RS/6000 System Design. </title> <booktitle> In Proceedings of the 1994 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 414423, </pages> <year> 1994. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures <ref> [Abbott94] </ref>. Finally, several papers have examined the performance advantages and management of reliable memory [Copeland89, Baker92a, Biswas93, Akyurek95]. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. <p> Hardware faults are usually specific and relatively easy to model [Lee93b], and various techniques such as ECC and redundancy have been successfully used to protect against these errors <ref> [Abbott94, Banatre93] </ref>. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error cor rection schemes and corrupt memory. Software errors (like most design aws) are difficult to model and understand.
Reference: [Akyurek95] <author> Sedat Akyurek and Kenneth Salem. </author> <title> Management of partially safe buffers. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(3):394407, </volume> <month> March </month> <year> 1995. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ouster-hout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 198212, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk anyway. A longer delay can decrease disk traffic due to writes, but only at the cost of losing more data.
Reference: [Baker92a] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> Non-Volatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V), </booktitle> <pages> pages 10 22, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: It is hence relatively easy for many simple software errors (such as de-referencing an uninitialized pointer) to accidentally corrupt the contents of memory <ref> [Baker92a] </ref>. The assumption that memory is unreliable hurts system performance, reliability, simplicity, semantics, and cost. Because memory is unreliable, systems that require high reliability, such as databases, write new data through to disk, but this slows performance to that of disks. <p> The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Baker92b] <author> Mary Baker and Mark Sullivan. </author> <title> The Recovery Box: Using Fast Recovery to Provide High Availability in the UNIX Environment. </title> <booktitle> In Proceedings USENIX Summer Conference, </booktitle> <month> June </month> <year> 1992. </year>
Reference-contexts: It also keeps multiple copies of modified pages. The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes [Liskov91]. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface <ref> [Baker92b] </ref>. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums. <p> We use two strategies to detect corruption of the file cache: checksums and a synthetic workload called memTest. 3.1 Checksum Detection of Corruption Our primary method to detect corruption is to maintain a checksum of each memory block in the file cache <ref> [Baker92b] </ref>. We update the checksum in all functions that write the file cache; unintentional changes to file cache buffers will result in an inconsistent checksum.
Reference: [Banatre86] <author> Jean-Pierre Banatre, Michel Banatre, Guy LaPalme, and Florimond Ployette. </author> <title> The design and building of Enchere, a distributed electronic marketing system. </title> <journal> Communications of the ACM, </journal> <volume> 29(1):1929, </volume> <month> January </month> <year> 1986. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre86, Banatre88, Banatre91] </ref>.
Reference: [Banatre88] <author> Michel Banatre, Gilles Muller, and Jean-Pierre Banatre. </author> <title> Ensuring data security and integrity with a fast stable storage. </title> <booktitle> In Proceedings of the 1988 International Conference on Data Engineering, </booktitle> <pages> pages 285293, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre86, Banatre88, Banatre91] </ref>.
Reference: [Banatre91] <author> Michel Banatre, Gilles Muller, Bruno Rochat, and Patrick Sanchez. </author> <title> Design decisions for the FTM: a general purpose fault tolerant machine. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 7178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Banatre, et. al. implement stable transactional memory, which protects memory contents with dual memory banks, a special memory controller, and explicit calls to allow write access to specified memory blocks <ref> [Banatre86, Banatre88, Banatre91] </ref>.
Reference: [Banatre93] <author> Michel Banatre, Pack Heng, Gilles Muller, Nadine Peyrouze, and Bruno Rochat. </author> <title> An experience in the design of a reliable object based system. </title> <booktitle> In Proceedings of the 1993 International Conference on Parallel and Distributed Information Systems, </booktitle> <pages> pages 187190, </pages> <month> January </month> <year> 1993. </year>
Reference-contexts: Hardware faults are usually specific and relatively easy to model [Lee93b], and various techniques such as ECC and redundancy have been successfully used to protect against these errors <ref> [Abbott94, Banatre93] </ref>. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error cor rection schemes and corrupt memory. Software errors (like most design aws) are difficult to model and understand.
Reference: [Barton90] <author> James H. Barton, Edward W. Czeck, Zary Z. Segall, and Daniel P. Siewiorek. </author> <title> Fault injection experiments using FIAT. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 39(4):575582, </volume> <month> April </month> <year> 1990. </year>
Reference-contexts: This implies that permanent fault injection method, like bit ip in program memory image, cannot model the full spectrum of hardware faults. Another tool, FIAT, uses software to eumlate hardware faults. It injects memory bit faults into various code and data segments <ref> [Segall88, Barton90] </ref> of an application program or operating system. The fault injection mechanism is emulated using information from Measuring Memorys Resistance to Operating System Crashes 4 the compiler and loader to corrupt the program memory image. Unlike FERRARI, FIAT cannot inject transient faults. <p> Measuring Memorys Resistance to Operating System Crashes 11 4.1 Random Bit Flips The first category of faults ips randomly chosen bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. To inject faults into the kernel text area, we first determine the kernel text boundary statically using the odump utility, and compiled the boundary into the kernel.
Reference: [Biswas93] <author> Prabuddha Biswas, K. K. Ramakrishnan, Don Towsley, and C. M. Krishna. </author> <title> Performance Analysis of Distributed File Systems with Non-Volatile Caches. </title> <booktitle> In Proceedings of the 1993 International Symposium on High Performance Distributed Computing (HPDC-2), </booktitle> <pages> pages 252262, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [Chapin95] <author> John Chapin, Mendel Rosenblum, Scott Devine, Tirthankar Lahiri, Dan Teodosiu, and Anoop Gupta. Hive: </author> <title> Fault Containment for Shared-Memory Multiprocessors. </title> <booktitle> In Proceedings of the 1995 Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: This section reviews several proposals to protect memory from software and hardware failures. With the exception of <ref> [Chapin95] </ref>, none of these papers attempt to evaluate its proposals with experiments. The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix [Gait90]. Phoenix keeps two versions of an in-memory file system. <p> This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations [Johnson82, Wahbe92]. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor <ref> [Chapin95] </ref>. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. Fault injection test results show that the fault containment scheme can contain the effects of the fault to the faulty processor cell.
Reference: [Chen95] <author> Peter M. Chen, Christopher M. Aycock, Wee Teck Ng, Gurushankar Rajamani, and Rajago-palan Sivaramakrishnan. </author> <title> Rio: Storing Files Reliably in Memory. </title> <type> Technical Report CSE-TR-250-95, </type> <institution> University of Michigan, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: To examine the file cache image in memory, we perform a warm reboot during which all files in memory at the time of the crash are restored to disk <ref> [Chen95] </ref>. We have two other goals in designing the workload. First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. <p> Fortunately, direct corruption from faults such as copy overruns are the easiest type of corruption to protect against. We describe a scheme in <ref> [Chen95] </ref> which uses a combination of VM and code patching to protect against invalid virtual and physical address accesses. The file cache pages are write-protected when not actively being written by the file cache Measuring Memorys Resistance to Operating System Crashes 15 code. <p> For situations where greater protection against operating system crashes is required, we can use a simple, low-overhead software scheme that controls access to file cache buffers using virtual memory protection and code patching proposed in <ref> [Chen95] </ref>. One direction for future work is to redo this study on a different operating system or to perform a similar fault-injection experiment on a database system. We believe these will extend the applicability of our conclusions.
Reference: [Chillarege89] <author> R. Chillarege and N. Bowen. </author> <title> Understanding large system failures - A fault injection experiment. </title> <booktitle> In Proceedings of the 1989 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 356-363, </pages> <year> 1989. </year>
Reference-contexts: We corrupt ten instructions rather than one to increase the probability that a corrupted instruction will be executed. This is a widely-used technique [Kanawati95, Kao93] known as failure acceleration, and is originally proposed by Chillarege and Bowen <ref> [Chillarege89] </ref>. It is used in our fault injection study to decrease the time needed to collect meaningful data. We use a similar approach to inject faults into the kernel heap area.
Reference: [Copeland89] <author> George Copeland, Tom Keller, Ravi Krishnamurthy, and Marc Smith. </author> <title> The Case for Safe RAM. </title> <booktitle> In Proceedings of the Fifteenth International Conference on Very Large Data Bases, </booktitle> <pages> pages 327335, </pages> <month> August </month> <year> 1989. </year>
Reference-contexts: The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94]. Finally, several papers have examined the performance advantages and management of reliable memory <ref> [Copeland89, Baker92a, Biswas93, Akyurek95] </ref>. 3 Experimental Environment and Mechanisms Our experiments were run on DEC Alpha 3000/600 workstations (Table 1) running the Digital Unix V3.0 operating system. Digital Unix is a monolithic kernel derived from Mach 2.5 and OSF/1.
Reference: [DEC95] <author> August 1995. </author> <title> Digital Unix development team, </title> <type> Personal Communication. </type>
Reference-contexts: We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is less concrete. Most people would feel nervous if their system crashed while the sole copy of important data was in memory, even if the power stayed on <ref> [DEC95, Tanenbaum95 page 146, Silberschatz94 page 200] </ref>. As evidence of this view, most systems periodically write file data to disk, and transaction processing applications view transactions as committed only when the changes are made to the disk copy of the database.
Reference: [Dutton92] <author> Todd A. Dutton, Daniel Eiref, Hugh R. Kurth, James J. Reisert, and Robin L. Stewart. </author> <title> The Design of the DEC 3000 AXP Systems, Two High-Performance Workstations. </title> <journal> Digital Technical Journal, </journal> <volume> 4(4):6681, </volume> <year> 1992. </year>
Reference-contexts: the user expect and want to remain intact after a system crash? The user probably does not really want or expect all memory data to survive; after all if the entire state of the machine were preserved, the newly rebooted machine would likely crash Table 1: Specifications of Experimental Platform <ref> [Dutton92] </ref>. machine type DEC 3000 model 600 CPU chip Alpha 21064, 175 MHz SPECint92 114 SPECfp92 165 memory bandwidth 207 MB/s memory capacity 128 MB (512 MB max capacity) system bus Turbochannel system bus bandwidth 100 MB/s Measuring Memorys Resistance to Operating System Crashes 6 again! The abstraction used to distinguish
Reference: [Eich87] <author> Margaret H. Eich. </author> <title> A classification and comparison of main memory database recovery tech Measuring Memorys Resistance to Operating System Crashes 19 niques. </title> <booktitle> In Proceedings of the IEEE International Conference on Data Engineering, </booktitle> <pages> pages 332339, </pages> <month> February </month> <year> 1987. </year>
Reference-contexts: Memorys unreliability also increases system complexity [Rahm92]. Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes <ref> [GM92, Eich87] </ref>. Ideal semantics, such as atomicity for every transaction, are also sacrificed because disk accesses are slow and memory is unreliable. Finally, memorys unreliability forces systems to keep a copy of permanent memory data on disk; this shrinks the available stor age capacity.
Reference: [Gait90] <author> Jason Gait. </author> <title> Phoenix: A Safe In-Memory File System. </title> <journal> Communications of the ACM, </journal> <volume> 33(1):8186, </volume> <month> January </month> <year> 1990. </year>
Reference-contexts: With the exception of [Chapin95], none of these papers attempt to evaluate its proposals with experiments. The only file system we are aware of that attempts to make all permanent files reliable while in memory is Phoenix <ref> [Gait90] </ref>. Phoenix keeps two versions of an in-memory file system. One of these versions is kept write-protected; the other version is unprotected and evolves from the write-protected one via copy-on-write. At periodic checkpoints, the system write-protects the unprotected version and deletes obsolete pages in the original version.
Reference: [GM92] <author> Hector Garcia-Molina and Kenneth Salem. </author> <title> Main Memory Database Systems: An Overview. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 4(6):509516, </volume> <month> December </month> <year> 1992. </year>
Reference-contexts: Memorys unreliability also increases system complexity [Rahm92]. Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes <ref> [GM92, Eich87] </ref>. Ideal semantics, such as atomicity for every transaction, are also sacrificed because disk accesses are slow and memory is unreliable. Finally, memorys unreliability forces systems to keep a copy of permanent memory data on disk; this shrinks the available stor age capacity.
Reference: [Gray90] <author> Jim Gray. </author> <title> A Census of Tandem System Availability between 1985 and 1990. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 39(4), </volume> <month> October </month> <year> 1990. </year>
Reference-contexts: results of our experiments; and Section 7 concludes this report. 2 Related Work We divide the research related to this paper into three areas: field studies, fault injection, and protection schemes. 2.1 Field Studies of System Crashes Studies have shown that software has become the dominant cause of system outages <ref> [Gray90] </ref>. Gray analysed the field data for fault-tolerant Tandem systems, and found that system outage caused by software grew from 33% to 60% from 1985 to 1990. In comparison, system outage caused by hardware decreased from 50% to 10%.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> The Zebra Striped Network File System. </title> <booktitle> In Proceedings of the 1993 Symposium on Operating System Principles, </booktitle> <pages> pages 2943, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds <ref> [Baker91, Hartman93] </ref>, so a large fraction of writes must eventually be written through to disk anyway. A longer delay can decrease disk traffic due to writes, but only at the cost of losing more data.
Reference: [Howard88] <author> John H. Howard, Michael L. Kazar, Sherri G. Menees, David A. Nichols, M. Satyanarayanan, Robert N. Sidebotham, and Michael J. West. </author> <title> Scale and Performance in a Distributed File System. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1):5181, </volume> <month> February </month> <year> 1988. </year>
Reference-contexts: First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. To create a general-purpose workload, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>. Andrew creates and copies a source hierarchy; examines the hierarchy using find, ls, du, grep, and wc; and compiles the source hierarchy.
Reference: [Iyer95] <author> Ravishankar K. Iyer. </author> <title> Experimental Evaluation. </title> <booktitle> In Proceedings of the 1995 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 115132, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: We review some of the most relevant prior work; see <ref> [Iyer95] </ref> for an excellent introduction to the overall area and a summary of much of the past fault injection techniques. The most relevant work to this paper is the FINE fault injector and monitoring environment [Kao93].
Reference: [Johnson82] <author> Mark Scott Johnson. </author> <title> Some Requirements for Architectural Support of Software Debugging. </title> <booktitle> In Proceedings of the 1982 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <pages> pages 140148, </pages> <month> April </month> <year> 1982. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory.
Reference: [Kanawati92] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> a tool for the validation of system dependability properties. </title> <booktitle> In Proceedings of the 1992 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 336344, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: The latter category of faults tend to crash the system immediately. Only 8% of faults propagated to other Unix subsystems. FERRARI also uses software to inject various hardware faults <ref> [Kanawati95, Kanawati92] </ref>. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject permanent or transient faults into user program.
Reference: [Kanawati95] <author> Ghani A. Kanawati, Nasser A. Kanawati, and Jacob A. Abraham. FERRARI: </author> <title> A Flexible Software-Based Fault and Error Injection System. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 44(2):248260, </volume> <month> February </month> <year> 1995. </year>
Reference-contexts: Unfortunately, data of this nature is not recorded (or is not available) from production systems. Other approaches like analytical modeling are not appropriate for very large systems like Unix operating systems <ref> [Kao93, Kanawati95] </ref>. We use software fault injection to induce a wide variety of operating system crashes in our target system (DEC Alphas running Digital Unix) and find that the file cache is almost never corrupted. <p> The latter category of faults tend to crash the system immediately. Only 8% of faults propagated to other Unix subsystems. FERRARI also uses software to inject various hardware faults <ref> [Kanawati95, Kanawati92] </ref>. FERRARI is extremely exible: it can emulate a large number of data, address, and control faults, and it can inject permanent or transient faults into user program. <p> Our primary goal in designing these experiments is to generate a wide variety of system crashes. We use software to emulate both software and hardware faults because software fault injection has proven to be an easy and effective injection mechanism <ref> [Kanawati95] </ref>. The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. <p> Measuring Memorys Resistance to Operating System Crashes 11 4.1 Random Bit Flips The first category of faults ips randomly chosen bits in the kernels address space <ref> [Barton90, Kanawati95] </ref>. We target three areas of the kernels address space: the kernel text, heap, and stack. To inject faults into the kernel text area, we first determine the kernel text boundary statically using the odump utility, and compiled the boundary into the kernel. <p> Faults are injected by corrupting ten random addresses within the kernel text boundary. We corrupt ten instructions rather than one to increase the probability that a corrupted instruction will be executed. This is a widely-used technique <ref> [Kanawati95, Kao93] </ref> known as failure acceleration, and is originally proposed by Chillarege and Bowen [Chillarege89]. It is used in our fault injection study to decrease the time needed to collect meaningful data. We use a similar approach to inject faults into the kernel heap area.
Reference: [Kao93] <author> Wei-Lun Kao, Ravishankar K. Iyer, and Dong Tang. </author> <title> FINE: A Fault Injection and Monitoring Environment for Tracing the UNIX System Behavior under Faults. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 19(11):11051118, </volume> <month> November </month> <year> 1993. </year>
Reference-contexts: Unfortunately, data of this nature is not recorded (or is not available) from production systems. Other approaches like analytical modeling are not appropriate for very large systems like Unix operating systems <ref> [Kao93, Kanawati95] </ref>. We use software fault injection to induce a wide variety of operating system crashes in our target system (DEC Alphas running Digital Unix) and find that the file cache is almost never corrupted. <p> We review some of the most relevant prior work; see [Iyer95] for an excellent introduction to the overall area and a summary of much of the past fault injection techniques. The most relevant work to this paper is the FINE fault injector and monitoring environment <ref> [Kao93] </ref>. FINE uses software to emulate both hardware (memory, CPU and bus) and software (initialization, assignment, condition check and function) bugs. Both types of faults can be dynamically injected into user program or operating system via kernel trap. <p> Faults are injected by corrupting ten random addresses within the kernel text boundary. We corrupt ten instructions rather than one to increase the probability that a corrupted instruction will be executed. This is a widely-used technique <ref> [Kanawati95, Kao93] </ref> known as failure acceleration, and is originally proposed by Chillarege and Bowen [Chillarege89]. It is used in our fault injection study to decrease the time needed to collect meaningful data. We use a similar approach to inject faults into the kernel heap area. <p> These faults are intended to approximate the assembly-level manifestation of real C-level programming errors <ref> [Kao93] </ref>. The low-level fault injection is implemented by modifying relevant instructions within the kernel text. A relevant instruction is selected by sampling random instructions from the kernel text until a matching opcode pattern is found. <p> These are more targeted at specific programming errors than low-level software faults. We inject an initialization fault by deleting (turning into noops) the instructions in the kernel text responsible for initializing a variable at the start of a function <ref> [Kao93, Lee93a] </ref>. The function entry and initialization statements can be identified by the load stack pointer (lda sp, -XX (sp)) and store to stack (stq ra, XX (sp)) instructions, respectively. <p> We observed 80 unique crash error messages and used these error messages to divide the crashes into six categories. The first category is kernel memory fault/unaligned access. These accounted for the largest fraction of crashes (78%), which is consistent with prior results in the field <ref> [Lee93a, Kao93] </ref>. The second largest category is kernel consistency check (11%), followed by user process failure (4%), hardware error (2%), illegal instruction (2%), and unknown (3%). Table 2 shows the distribution of crashes for each type of fault. <p> This is consistent with results from FINE that show most faults do not propagate to other kernel modules <ref> [Kao93] </ref>. Overall, only 1.7% of all crashes corrupted the file cache. Excluding direct corruption from copy overruns, only 0.2% of crashes corrupted the file cache. 6 Discussion In this section, we discuss why the difference between disk and memory reliability is not as great as one might think. <p> 4 initialization fault 76 0 6 pointer corruption 74 0 0 allocation management 103 0 7 copy overrun 176 15 0 total 1022 17 22 Measuring Memorys Resistance to Operating System Crashes 17 memory protection and kernel consistency checks, and errors do not tend to propagate between different kernel modules <ref> [Kao93] </ref>. To illustrate memorys reliability, consider a system that crashes once per month (a pessimistic estimate for production-quality operating systems).
Reference: [Kessler90] <author> Peter B. Kessler. </author> <title> Fast breakpoints: </title> <booktitle> Design and implementation. In Proceedings of the 1990 Conference on Programming Language Design and Implementation (PLDI), </booktitle> <pages> pages 7884, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: However, it does not evaluate the effectiveness of the scheme by measuring the systems fault tolerance in the absence of the Flash firewall. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Lee93a] <author> Inhwan Lee and Ravishankar K. Iyer. </author> <title> Faults, Symptoms, and Software Fault Tolerance in the Tandem GUARDIAN Operating System. </title> <booktitle> In International Symposium on Fault-Tolerant Computing (FTCS), pages 2029, </booktitle> <year> 1993. </year>
Reference-contexts: Lee and Iyer study and classify software failures in Tandems Guardian operating system using memory dumps collected from field software failures <ref> [Lee93a, Lee93b] </ref>. They also identify the effects of software faults on the system, and trace the propagation of the effects to other subsystems. <p> These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from [Sullivan91] and <ref> [Lee93a] </ref>. However, they do not provide specific information about how often system crashes corrupt the permanent data in memory. 2.2 Using Software to Inject Faults Software fault injection is a popular technique for evaluating how prototype systems behave in the presence of hardware and software faults. <p> These are more targeted at specific programming errors than low-level software faults. We inject an initialization fault by deleting (turning into noops) the instructions in the kernel text responsible for initializing a variable at the start of a function <ref> [Kao93, Lee93a] </ref>. The function entry and initialization statements can be identified by the load stack pointer (lda sp, -XX (sp)) and store to stack (stq ra, XX (sp)) instructions, respectively. <p> int i; /* should be int i=0 */ while (i++) /* i is undefined! */ We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91, Lee93a] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. <p> We observed 80 unique crash error messages and used these error messages to divide the crashes into six categories. The first category is kernel memory fault/unaligned access. These accounted for the largest fraction of crashes (78%), which is consistent with prior results in the field <ref> [Lee93a, Kao93] </ref>. The second largest category is kernel consistency check (11%), followed by user process failure (4%), hardware error (2%), illegal instruction (2%), and unknown (3%). Table 2 shows the distribution of crashes for each type of fault.
Reference: [Lee93b] <author> Inhwan Lee, Dong Tang, Ravishankar K. Iyer, and Mei-Chen Hsueh. </author> <title> Measurement-Based Evaluation of Operating System Fault Tolerance. </title> <journal> IEEE Transactions on Reliability, </journal> <volume> 42(2):238249, </volume> <month> June </month> <year> 1993. </year>
Reference-contexts: Lee and Iyer study and classify software failures in Tandems Guardian operating system using memory dumps collected from field software failures <ref> [Lee93a, Lee93b] </ref>. They also identify the effects of software faults on the system, and trace the propagation of the effects to other subsystems. <p> The faults we inject range from low-level hardware faults such as ipping bits in memory to high-level software faults such as memory allocation errors. Hardware faults are usually specific and relatively easy to model <ref> [Lee93b] </ref>, and various techniques such as ECC and redundancy have been successfully used to protect against these errors [Abbott94, Banatre93]. We focus primarily on software faults because: Kernel programming errors are the errors most likely to circumvent hardware error cor rection schemes and corrupt memory.
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <year> 1989. </year>
Reference: [Liskov91] <author> Barbara Liskov, Sanjay Ghemawat, Robert Gruber, Paul Johnson, Liuba Shrira, and Michael Williams. </author> <title> Replication in the Harp File System. </title> <booktitle> In Proceedings of the 1991 Symposium on Operating System Principles, </booktitle> <pages> pages 226238, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: Phoenix does not ensure the reliability of every write; instead, writes are only made permanent at periodic checkpoints. It also keeps multiple copies of modified pages. The Harp file system protects a log of recent modifications by replicating it in volatile, battery-backed memory across several server nodes <ref> [Liskov91] </ref>. The Recovery Box keeps special system state in a region of memory accessed only through a rigid interface [Baker92b]. No attempt is made to prevent other functions from accidentally modifying the recovery box, although the system detects corruption by maintaining checksums.
Reference: [Needham83] <author> R. M. Needham, A. J. Herbert, and J. G. Mitchell. </author> <title> How to Connect Stable Memory to a Computer. Operating System Review, </title> <address> 17(1):16, </address> <month> January </month> <year> 1983. </year>
Reference-contexts: Measuring Memorys Resistance to Operating System Crashes 5 General mechanisms may be used to help protect memory from software faults. <ref> [Needham83] </ref> suggests changing a machines microcode to check certain conditions when writing a memory word; the condition they suggest is that a certain register has been preloaded with the memory words previous content.
Reference: [Ousterhout85] <author> John K. Ousterhout, Herve Da Costa, et al. </author> <title> A Trace-Driven Analysis of the UNIX 4.2 BSD File System. </title> <booktitle> In Proceedings of the 1985 Symposium on Operating System Principles, </booktitle> <pages> pages 1524, </pages> <month> December </month> <year> 1985. </year>
Reference-contexts: as Unix file systems, mitigate the performance loss caused by extra disk writes by only writing new data to disk every 30 seconds or so, but this ensures the loss Directed Study Report Measuring Memorys Resistance to Operating System Crashes 2 of data written within 30 seconds of a crash <ref> [Ousterhout85] </ref>. In addition, 1/3 to 2/3 of newly written data lives longer than 30 seconds [Baker91, Hartman93], so a large fraction of writes must eventually be written through to disk anyway. A longer delay can decrease disk traffic due to writes, but only at the cost of losing more data.
Reference: [Ousterhout90] <author> John K. Ousterhout. </author> <title> Why arent operating systems getting faster as fast as hardware? In Proceedings USENIX Summer Conference, </title> <booktitle> pages 247256, </booktitle> <month> June </month> <year> 1990. </year>
Reference-contexts: First, we want a general-purpose workload that calls many different programs. Second, we want to stress the file system with real programs that expanded the file cache to include most of main memory. To create a general-purpose workload, we run four copies of the Andrew benchmark <ref> [Howard88, Ousterhout90] </ref>. Andrew creates and copies a source hierarchy; examines the hierarchy using find, ls, du, grep, and wc; and compiles the source hierarchy.
Reference: [Rahm92] <author> Erhard Rahm. </author> <title> Performance Evaluation of Extended Storage Architectures for Transaction Measuring Memorys Resistance to Operating System Crashes 20 Processing. </title> <booktitle> In Proceedings of the 1992 ACM SIGMOD International Conference on Management of Data, </booktitle> <pages> pages 308317, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The extreme approach is to use a pure write-back scheme where data is only written to disk when the memory is full. This is only an option for applications where reliability is not an issue, such as compiler-generated temporary files. Memorys unreliability also increases system complexity <ref> [Rahm92] </ref>. Increased disk traffic due to extra write backs forces the use of extra disk optimizations such as disk scheduling, disk reorganization, and group commit. Much of the research in main-memory databases deals with checkpointing and recovering data in case the system crashes [GM92, Eich87].
Reference: [Segall88] <author> Z. Segall, D. Vrsalovic, D. Siewiorek, D. Yaskin, J. Kownacki, J. Barton, R. Dancey, A. Robinson, and T. Lin. </author> <title> FIATFault Injection Based Automated Testing Environment. </title> <booktitle> In 18th Annual International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 102107, </pages> <year> 1988. </year>
Reference-contexts: This implies that permanent fault injection method, like bit ip in program memory image, cannot model the full spectrum of hardware faults. Another tool, FIAT, uses software to eumlate hardware faults. It injects memory bit faults into various code and data segments <ref> [Segall88, Barton90] </ref> of an application program or operating system. The fault injection mechanism is emulated using information from Measuring Memorys Resistance to Operating System Crashes 4 the compiler and loader to corrupt the program memory image. Unlike FERRARI, FIAT cannot inject transient faults.
Reference: [Silberschatz94] <author> Abraham Silberschatz and Peter B. Galvin. </author> <title> Operating System Concepts. </title> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference: [Sites92] <author> Richard L. </author> <title> Sites, editor. Alpha Architecture Reference Manual. </title> <publisher> Digital Press, </publisher> <year> 1992. </year>
Reference-contexts: This is acceptable as each regis try entry is only 32 bytes for a 8 KB file cache page (0.4% overhead). The checksum is implemented by summing every quadwords, avoiding costly unaligned access <ref> [Sites92] </ref>. As our target platform is a uni-processor, we use a simple read/write locking primitive (lock mode 0 in Digital Unix). mmap data poses a greater challenge since the data may not be explicitly unmapped when a process exit. <p> A relevant instruction is selected by sampling random instructions from the kernel text until a matching opcode pattern is found. Each type of low-level faults will have a corresponding opcode pattern, depending on the instruction semantics <ref> [Sites92] </ref>. The first fault in this category is an assignment fault. One type of assignment fault changes the destination register used by an instruction; the other changes a source register.
Reference: [Sullivan91] <author> Mark Sullivan and R. Chillarege. </author> <title> Software Defects and Their Impact on System Availabili-tyA Study of Field Failures in Operating Systems. </title> <booktitle> In Proceedings of the 1991 International Symposium on Fault-Tolerant Computing, </booktitle> <month> June </month> <year> 1991. </year>
Reference-contexts: Sullivan and Chillarege classify software faults in the MVS operating system and DB2 Measuring Memorys Resistance to Operating System Crashes 3 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) <ref> [Sullivan91, Sullivan92] </ref>. Their study found that most overlay errors are due to boundary condition and allocation problems, and not timing or synchronization problems. They also found that most overlays are small, and corrupt data near the data that the programmer meant to update. <p> These studies provide valuable information about failures in production environments; in fact many of the fault types in Section 3 were inspired by the major error categories from <ref> [Sullivan91] </ref> and [Lee93a]. However, they do not provide specific information about how often system crashes corrupt the permanent data in memory. 2.2 Using Software to Inject Faults Software fault injection is a popular technique for evaluating how prototype systems behave in the presence of hardware and software faults. <p> int i; /* should be int i=0 */ while (i++) /* i is undefined! */ We inject pointer corruption by 1) finding a register that is used as a base register of a load or store and 2) deleting the most recent instruction before the load/store that modifies that register <ref> [Sullivan91, Lee93a] </ref>. We do not corrupt the stack pointer register, as this is used to access local variables instead of as a pointer variable. <p> We also inject two of the common, high-level programming errors described by <ref> [Sullivan91] </ref>: allocation management and copy overruns. <p> The length of the overrun was distributed as follows: 50% corrupt one byte; 44% corrupt 2-1024 bytes; 6% corrupt 2-4 KB. This distribution was chosen by starting with the data gathered in <ref> [Sullivan91] </ref> and modifying it somewhat according to our specific platform and experience. bcopy is set to inject this error every 1000-4000 times it is called; this occurs approximately every 15 seconds. 4.4 Crash Latency Most crashes occurred within 15 seconds after the fault was injected for all faults in this paper.
Reference: [Sullivan92] <author> Mark Sullivan and Ram Chillarege. </author> <title> A Comparison of Software Defects in Database Management Systems and Operating Systems. </title> <booktitle> In Proceedings of the 1992 International Symposium on Fault-Tolerant Computing, </booktitle> <pages> pages 475484, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: Sullivan and Chillarege classify software faults in the MVS operating system and DB2 Measuring Memorys Resistance to Operating System Crashes 3 and IMS database systems; in particular, they analyze faults that corrupt program memory (overlays) <ref> [Sullivan91, Sullivan92] </ref>. Their study found that most overlay errors are due to boundary condition and allocation problems, and not timing or synchronization problems. They also found that most overlays are small, and corrupt data near the data that the programmer meant to update.
Reference: [Tanenbaum95] <author> Andrew S. Tanenbaum. </author> <title> Distributed Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1995. </year>
Reference-contexts: I/O devices such as disks and tapes are considered fairly reliable places to store long-term data such as files. However, random-access memory is commonly viewed as an unreliable place to store permanent data (files) because it is perceived to be vulnerable to power outages and operating system crashes <ref> [Tanenbaum95, page 146] </ref>. Memorys vulnerability to power outages is straightforward to understand and fix. A simple solution is to add an uninterruptible power supply to the system. Another solution is to switch to a non-volatile memory technology such as Flash RAM [Wu94].
Reference: [Wahbe92] <author> Robert Wahbe. </author> <title> Efficient Data Breakpoints. </title> <booktitle> In Proceedings of the 1992 International Conference on Architectural Support for Programming Languages and Operating Systems (AS-PLOS), </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: This is similar to modifying the memory controller to enforce protection, as are Johnsons and Wahbes suggestions for various hardware mechanisms to trap the updates of certain memory locations <ref> [Johnson82, Wahbe92] </ref>. Hive uses the Flash firewall to protect memory against wild writes by other processors in a multiprocessor [Chapin95]. Hive preemptively discards pages that are writable by failed processors, an option not available when storing permanent data in memory. <p> However, it does not evaluate the effectiveness of the scheme by measuring the systems fault tolerance in the absence of the Flash firewall. Finally, object code modification has been suggested as a way to provide data breakpoints <ref> [Kessler90, Wahbe92] </ref> and fault isolation between software modules [Wahbe93].
Reference: [Wahbe93] <author> Robert Wahbe, Steven Lucco, Thomas E. Anderson, and Susan L. Graham. </author> <title> Efficient Software-Based Fault Isolation. </title> <booktitle> In Proceedings of the 14th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 203216, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: However, it does not evaluate the effectiveness of the scheme by measuring the systems fault tolerance in the absence of the Flash firewall. Finally, object code modification has been suggested as a way to provide data breakpoints [Kessler90, Wahbe92] and fault isolation between software modules <ref> [Wahbe93] </ref>.
Reference: [Wu94] <author> Michael Wu and Willy Zwaenepoel. eNVy: </author> <title> A Non-Volatile, Main Memory Storage System. </title> <booktitle> In Proceedings of the 1994 International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS), </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Memorys vulnerability to power outages is straightforward to understand and fix. A simple solution is to add an uninterruptible power supply to the system. Another solution is to switch to a non-volatile memory technology such as Flash RAM <ref> [Wu94] </ref>. We do not consider power outages further in this paper. Memorys vulnerability to OS crashes is less concrete. <p> Other projects seek to improve the reliability of memory against hardware faults such as power outages and board failures. eNVy implements a memory board based on ash RAM, which is non-volatile <ref> [Wu94] </ref>. eNVy uses copy-on-write, page remapping, and a small, battery-backed, SRAM buffer to hide ash RAMs slow writes and bulk erases. The Durable Memory RS/6000 uses batteries, replicated processors, memory ECC, and alternate paths to tolerate a wide variety of hardware failures [Abbott94].
References-found: 46

