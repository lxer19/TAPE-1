URL: http://www.cs.virginia.edu/~gjf2a/research/papers/thesis.ps
Refering-URL: http://www.cs.virginia.edu/~gjf2a/why.html
Root-URL: http://www.cs.virginia.edu
Title: Using Genetic Programming to Evolve Board Evaluation Functions  
Author: Gabriel J. Ferrer 
Degree: A Thesis Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree of Master of Science Computer Science by  
Date: August 1996  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Peter J. Angeline and Jordan B. Pollack. </author> <title> Competitive environments evolve better solutions for complex tasks. </title> <booktitle> In Proceedings of the Fifth International Conference on Genetic Algorithms, </booktitle> <pages> pages 264-270, </pages> <year> 1993. </year>
Reference-contexts: A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] [15] [22] [19] <ref> [1] </ref>. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> Maintaining this property avoids the potential complication of introducing individuals into the population who do not have a well-defined fitness, i.e., who do not play the game legally. For example, infeasible tic-tac-toe players can be created by Angeline and Pollack's system <ref> [1] </ref>. In their representation, if a player never invokes the function which causes a move to occur, the player will never move and the opponent will get bonus moves. Such individuals are not, strictly speaking, playing legal tic-tac-toe and can be considered infeasible as a result. <p> Such individuals are not, strictly speaking, playing legal tic-tac-toe and can be considered infeasible as a result. The fitness operator determines fitness using a competition scheme modeled on a sports tournament, similar to that used for the game of tic-tac-toe by Angeline and Pollack <ref> [1] </ref>. The individuals in the population are paired off arbitrarily and then play some number of games against each other. The individual who wins the most games is declared the winner and progresses to the next round of the tournament. The losers at each level all have the same fitness. <p> The use of tables facilitates accessing large amounts of precomputed useful strategic information quickly enough to be useful in play. D Other Methods for Evolving Game Players A number of researchers have previously applied evolutionary approaches to the problem of playing a boardgame. Angeline and Pollack <ref> [1] </ref> used genetic programming to evolve computer programs which played the game tic-tac-toe. This project did not, strictly speaking, evolve board evaluation functions, since the programs also were responsible for "physically" moving pieces on the board.
Reference: [2] <author> J. E. Baker. </author> <title> Reducing bias and inefficiency in the selection algorithm. </title> <booktitle> In Proceedings of an International Conference on Genetic Algorithms and Their Applications, </booktitle> <year> 1985. </year>
Reference-contexts: A population of individuals is evolved in the following manner. Selection for both reproduction and crossover is done in a rank-proportionate manner <ref> [2] </ref>. One-eighth of the new population is reproduced from the original population at each new generation, with the remainder of the new population being generated via crossover. These proportions are based on those used by Koza in his first book [13].
Reference: [3] <author> Avron Barr and Edward A. Feigenbaum. </author> <booktitle> The Handbook of Artificial Intelligence, </booktitle> <volume> Volume 1. </volume> <publisher> William Kaufmann, Inc., </publisher> <year> 1981. </year>
Reference-contexts: An overview of the design issues involved with creating effective board evaluation functions has been given by Hans Berliner, a noted backgammon player and programmer [4]. A good overview of heuristics for playing board games can be found in Barr and Feigenbaum's Handbook of Artificial Intelligence <ref> [3] </ref>. The international chess master David Levy has also written several books on this subject. His book Computer Gamesmanship [16] gives a good overview of the issues involved in developing computer game players and discusses specifics of implementing computer game players for a large number of games.
Reference: [4] <author> Hans Berliner. </author> <title> On the construction of evaluation functions for large domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 53-55, </pages> <year> 1979. </year>
Reference-contexts: A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include <ref> [4] </ref> [20] [18] [15] [16]). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. <p> The formulation given in this work also has the advantage (over a Samuel-like formulation) of allowing the evolution of individuals, i.e., board evaluation functions, that are nonlinear in nature, thus enlarging the range of possible solutions [15] <ref> [4] </ref>. Our formulation for these functions allows the strategy encoded in each board evaluation function to compete against others for "survival" in a tournament format. The performance of each board evaluation function in the tournament will be quantified as an integer and be called the "fitness" of the strategy. <p> An overview of the design issues involved with creating effective board evaluation functions has been given by Hans Berliner, a noted backgammon player and programmer <ref> [4] </ref>. A good overview of heuristics for playing board games can be found in Barr and Feigenbaum's Handbook of Artificial Intelligence [3]. The international chess master David Levy has also written several books on this subject.
Reference: [5] <author> Jonathan Cerf. </author> <title> Machine vs. </title> <journal> machine. Othello Quarterly, </journal> <volume> 2(4) </volume> <pages> 12-16, </pages> <year> 1980. </year>
Reference-contexts: Iago was victorious in the Santa Cruz Open Machine Othello Tournament [8] without losing a single game against an international field of computer Othello programs. Jonathan Cerf, who was at the time the current Othello world champion, stated in a review of the tournament <ref> [5] </ref> that "In my opinion the top programs from Santa Cruz are now equal (if not superior) to the best human players." In that same article, he goes on to state: I understand Paul Rosenbloom is interested in arranging a match [for Iago] against me.
Reference: [6] <author> R. Duda and P. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: Bill's evaluation function is composed of terms representing edge stability, current mobility, potential mobility, and the "sequence penalty". As described above, computation of all of these features is done by table-based algorithms for the sake of speed. To combine these four features, Bayesian learning <ref> [6] </ref> is used to combine the features C.2. Computing Othello Board Evaluation Functions 59 "optimally" into a quadratic polynomial. This technique is often used in pattern recognition to classify an image based on features extracted from it.
Reference: [7] <author> Gabriel J. Ferrer and Worthy N. Martin. </author> <title> Using genetic programming to evolve board evaluation functions. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 747-752, </pages> <address> Perth, Australia, </address> <month> November </month> <year> 1995. </year> <note> http://www.cs.virginia.edu/~gjf2a/work/papers/senet.ps. 66 Bibliography67 </note>
Reference-contexts: This uniquely specifies a legal move from each position. The Rosin/Belew representation described above has the advantage of being fixed in size, thus avoiding memory consumption problems sometimes experienced using genetic programming <ref> [7] </ref>. However, using computer programs as the representation allows for much greater scalability, in that a program doesn't have to internally represent every possible board position. It is difficult to imagine using the Rosin/Belew representation for a game such as chess, for instance.
Reference: [8] <author> P. W. Frey. </author> <title> The Santa Cruz open Othello tournament for computers. </title> <journal> BYTE, </journal> <volume> 6(7) </volume> <pages> 26-37, </pages> <year> 1981. </year>
Reference-contexts: C.3. Conclusion 60 C.3 Conclusion Bill and Iago have both performed impressively. Iago was victorious in the Santa Cruz Open Machine Othello Tournament <ref> [8] </ref> without losing a single game against an international field of computer Othello programs.
Reference: [9] <author> J. Grefenstette. </author> <title> Incorporating problem specific knowledge into genetic algorithms. </title> <editor> In L. Davis, editor, </editor> <booktitle> Genetic Algorithms and Simulated Annealing, </booktitle> <year> 1987. </year>
Reference-contexts: This approach of population seeding can result in the evolution of better fit individuals in a shorter period of time when compared to an initial population consisting entirely of random individuals <ref> [9] </ref>. This work shows that a fairly straightforward application of genetic programming results in the evolution of board evaluation functions that can play strategy games with an appreciable level of skill. <p> The rest of our experiments contained a small 4.2. Results 24 number of seed individuals in the initial population. We wanted to see how population seeding would work in the context of genetic programming, because it has been used successfully before with more standard genetic algorithms <ref> [9] </ref>. Our expectation was that incorporating some of our knowledge about the games into the initial population would improve the results of the search process. Incorporating hand-crafted players which encode useful strategies for game play can provide promising directions for where the search might proceed.
Reference: [10] <author> John H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <year> 1975. </year>
Reference-contexts: The work in this area is discussed in more detail in Chapter 3. 2.2 Genetic Algorithms What is now called the genetic algorithm was first described by John Holland in 1975 in his book Adaptation in Natural and Artificial Systems <ref> [10] </ref>. The genetic algorithm was inspired by the process of evolution in the natural world, wherein a population of individuals competes for survival, and the individuals which are the most fit survive and propagate themselves.
Reference: [11] <author> Timothy Kendall. </author> <title> Passing Through the Netherworld. </title> <publisher> Kirk Game Company, </publisher> <year> 1978. </year>
Reference-contexts: The player then gets to select the sequence in which the moves in the constructed set are executed. (A summary of the rules can be found in Appendix A, while the detailed rules can be found in <ref> [11] </ref>.) The board evaluation function then selects the most desirable move of those available from the present board position. Because of the probabilistic nature of the game, a scheme to search beyond one ply is too computationally expensive to be practical, as is also the case in backgammon [26]. <p> For this implementation of Senet, the rules were based upon the rules developed by Timothy Kendall for a version of Senet he published as a boardgame <ref> [11] </ref>. Here is a summary of Kendall's rules, paraphrased directly from his rule booklet: Set-up The 7 spools and the 7 cones are set up alternating with each other in the first 14 squares of the board.
Reference: [12] <author> John R. Koza. </author> <title> Hierarchical genetic algorithms operating on populations of computer programs. </title> <booktitle> In Proceedings of the 11th Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 768-774, </pages> <year> 1989. </year>
Reference-contexts: This description is only one version of the genetic algorithm, and an enormous number of variations on this basic scheme can be found in the literature. 2.3 Genetic Programming The concept of genetic programming was first formulated by John R. Koza in 1989 <ref> [12] </ref> and further developed in his two books on the subject [13] [14]. The basic outline of the genetic programming paradigm bears a strong resemblance to that given above for the "standard" genetic algorithm. The key difference lies in how individuals are encoded.
Reference: [13] <author> John R. Koza. </author> <title> Genetic Programming: On the Programming of Computers by Means of Natural Selection. </title> <publisher> MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: This thesis examines using the genetic programming paradigm <ref> [13] </ref> to evolve board evaluation functions for game playing programs. A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] [20] [18] [15] [16]). <p> The genetic algorithm was inspired by the process of evolution in the natural world, wherein a population of individuals competes for survival, and the individuals which are the most fit survive and propagate themselves. The genetic algorithm constitutes an artificial analog to this process in the following manner <ref> [13] </ref>. (Many variations on the scheme described below have also been used.) Individuals are represented as fixed-length chromosonal strings. Each piece of the chromosonal string encodes some attribute of a potential solution to a problem that the creator of the genetic algorithm is attempting to solve. <p> This is the first child. The two remaining pieces are joined to form the second child. This version of the crossover process is known as one-point crossover <ref> [13] </ref>. Other crossover operators also exist. Once the new population has been created, a relatively small number of individuals are mutated. A mutation consists of randomly changing one of the elements of the chromosonal 2.3. Genetic Programming 8 string representing an individual. <p> Koza in 1989 [12] and further developed in his two books on the subject <ref> [13] </ref> [14]. The basic outline of the genetic programming paradigm bears a strong resemblance to that given above for the "standard" genetic algorithm. The key difference lies in how individuals are encoded. Rather than encoding individuals as chromosonal fixed-length strings, they are instead encoded as computer programs. <p> A node is selected in a uniformly random manner from the tree of each parent as the crossover point. The subtrees rooted at the crossover points of each parent are exchanged, generating two children. This is the standard genetic programming crossover operator as described by Koza <ref> [13] </ref>. 3.1. Problem Formulation 15 The reproduction operator copies selected individuals from the old population into the new population. For each individual reproduced, there is a one in eight chance that that individual will mutate. <p> One-eighth of the new population is reproduced from the original population at each new generation, with the remainder of the new population being generated via crossover. These proportions are based on those used by Koza in his first book <ref> [13] </ref>. In that work, his experiments generated 10% of each new population by reproduction and the remaining 90% by crossover. <p> The limits used are a maximum tree height of six for a newly created individual, and a maximum tree height of seventeen for an individual which has been created through crossover or mutation. These same limits have been used by Koza <ref> [13] </ref>. We experimented briefly with removing height limits from the process to determine whether the presence of height limits negatively impacted the ability of individuals to evolve. No significant difference was observed between the fitness of individuals evolved with and without the standard height limits. 3.1.
Reference: [14] <author> John R. Koza. </author> <title> Genetic Programming II: Automatic Discovery of Reusable Programs. </title> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Koza in 1989 [12] and further developed in his two books on the subject [13] <ref> [14] </ref>. The basic outline of the genetic programming paradigm bears a strong resemblance to that given above for the "standard" genetic algorithm. The key difference lies in how individuals are encoded. Rather than encoding individuals as chromosonal fixed-length strings, they are instead encoded as computer programs.
Reference: [15] <author> Kai-Fu Lee and Sanjoy Mahajan. </author> <title> The development of a world-class othello program. </title> <journal> Aritificial Intelligence, </journal> <volume> 43 </volume> <pages> 21-36, </pages> <year> 1990. </year>
Reference-contexts: A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] [20] [18] <ref> [15] </ref> [16]). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. <p> A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] <ref> [15] </ref> [22] [19] [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> The formulation given in this work also has the advantage (over a Samuel-like formulation) of allowing the evolution of individuals, i.e., board evaluation functions, that are nonlinear in nature, thus enlarging the range of possible solutions <ref> [15] </ref> [4]. Our formulation for these functions allows the strategy encoded in each board evaluation function to compete against others for "survival" in a tournament format. The performance of each board evaluation function in the tournament will be quantified as an integer and be called the "fitness" of the strategy. <p> Using this algorithm, from random initial weights, the neural net learned to play backgammon at a strong intermediate level, solely through this self-play mechanism. Lee and Mahajan <ref> [15] </ref> used Bayesian learning to optimize the nonlinear evaluation polynomial which was used by their Othello playing program, Bill. Bayesian learning was used to decide whether a particular position represents a win or a loss based on features extracted from the board. <p> C Computerized Othello Playing This appendix is based on the work described in two major papers discussing computerized Othello players, namely Rosenbloom's program Iago [18] and the program of Lee and Mahajan, Bill <ref> [15] </ref>. C.1 Basic Othello Strategy C.1.1 Trivial Othello Strategies The most obvious strategy is to try to greedily accomplish the top-level Othello goal of having more pieces than one's opponent. This seemingly intuitive strategy works pretty badly in practice. <p> Basic Othello Strategy 54 C.1.2 Mobility and Stability Certain strategic considerations have been shown to be of immense value when utilized in computer Othello players. Two of the most studied strategic considerations are mobility and stability [18] <ref> [15] </ref>. For example, a failing encountered by both of the above strategies is that they tend to sacrifice mobility for the sake of more pieces or possessing a certain location. <p> The alternative preferred by Rosenbloom is to retain the simple count of the total number of legal moves and add a second measure which eliminates moves which result in immediate surrender of a corner. In Bill <ref> [15] </ref>, the computation of current mobility incorporates several additional criteria for determining which moves are "acceptable" or "good". <p> The combination of these measures gives Iago a pretty good idea of the potential mobility of the board configuration being examined. In Bill <ref> [15] </ref>, potential mobility is computed in a manner similar to how current mobility is computed. Tables are generated for each of the 38 lines, with bonuses given for each enemy internal disc which is next to an empty space.
Reference: [16] <author> David Levy. Computer Gamesmanship. Simon and Schuster, Inc., </author> <year> 1983. </year>
Reference-contexts: A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] [20] [18] [15] <ref> [16] </ref>). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. <p> A good overview of heuristics for playing board games can be found in Barr and Feigenbaum's Handbook of Artificial Intelligence [3]. The international chess master David Levy has also written several books on this subject. His book Computer Gamesmanship <ref> [16] </ref> gives a good overview of the issues involved in developing computer game players and discusses specifics of implementing computer game players for a large number of games.
Reference: [17] <author> David E. Moriarty and Risto Miikkulainen. </author> <title> Discovering complex othello strategies through evolutionary neural networks. </title> <journal> Connection Science, </journal> <volume> 7(3) </volume> <pages> 195-209, </pages> <year> 1995. </year>
Reference-contexts: A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] <ref> [17] </ref> [15] [22] [19] [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> Moriarty and Miikkulainen used an approach based on the artificial evolution of neural networks to develop a board evaluation function for Othello <ref> [17] </ref>. The networks were afforded no search mechanism, and consequently were forced to rely solely on pattern recognition of the current board configuration to achieve good play. Fitness was evaluated by playing each individual against a standard opponent.
Reference: [18] <author> Paul S. Rosenbloom. </author> <title> A world-championship-level Othello program. </title> <journal> Artificial Intelligence, </journal> <volume> 19 </volume> <pages> 279-320, </pages> <year> 1982. </year> <month> Bibliography68 </month>
Reference-contexts: A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] [20] <ref> [18] </ref> [15] [16]). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. <p> Normally, this occurs when the board has been completely filled with pieces, although this is not necessarily the case. C Computerized Othello Playing This appendix is based on the work described in two major papers discussing computerized Othello players, namely Rosenbloom's program Iago <ref> [18] </ref> and the program of Lee and Mahajan, Bill [15]. C.1 Basic Othello Strategy C.1.1 Trivial Othello Strategies The most obvious strategy is to try to greedily accomplish the top-level Othello goal of having more pieces than one's opponent. This seemingly intuitive strategy works pretty badly in practice. <p> Basic Othello Strategy 54 C.1.2 Mobility and Stability Certain strategic considerations have been shown to be of immense value when utilized in computer Othello players. Two of the most studied strategic considerations are mobility and stability <ref> [18] </ref> [15]. For example, a failing encountered by both of the above strategies is that they tend to sacrifice mobility for the sake of more pieces or possessing a certain location. <p> This is a problem in that having seven possible moves all of which yield a corner to the opponent is considered "better" by an algorithm not making this distinction than having three moves available with less disastrous consequences. Rosenbloom <ref> [18] </ref> discusses three possible approaches for dealing with this problem. One approach is to search the game tree below each move. This is essentially just adding another search level and if we assume that the search is already as deep as possible, this approach is necessarily too expensive. <p> Both move counting and move penalty are calculated by a series of table lookups. A board is represented for this purpose as 38 numbers, where each number is an index into a table that represents a horizontal, vertical, or diagonal line on the board. C.2.1.2 Potential Mobility In Iago <ref> [18] </ref>, the key idea for computing potential mobility is to find a string of the opponent's discs with an empty square at one end.
Reference: [19] <author> Christopher D. Rosin and Richard K. Belew. </author> <title> Methods for competitive co-evoltion: Finding opponents worth beating. </title> <booktitle> In Proceedings of the 6th International Conference on Genetic Algorithms, </booktitle> <pages> pages 373-380, </pages> <institution> University of Pittsburgh, </institution> <month> July </month> <year> 1995. </year>
Reference-contexts: A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] [15] [22] <ref> [19] </ref> [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> This allowed the possibility that a player would fail to move a piece on its turn, which adds the potentially undesirable possibility of infeasible individuals who don't actually move on every turn, thus not playing a legal game. Rosin and Belew <ref> [19] </ref> used a coevolving method for creating tic-tac-toe players. One population was the host, and the other the parasite. Fitness for the host population is evaluated by having each member of the host population play against some number of individuals from the parasite population.
Reference: [20] <author> Arthur Samuel. </author> <title> Some studies in machine learning using the game of checkers. </title> <journal> IBM Journal of Research and Development 3, </journal> <pages> pages 210-229, </pages> <year> 1959. </year>
Reference-contexts: A fairly extensive literature on the subject of developing good computer strategy game players in general and developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] <ref> [20] </ref> [18] [15] [16]). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. The notion of such adaptive systems can be found as early as Samuel's landmark checkers playing program [20] that was able to automatically adjust parameters of its evaluation function as it played in order to <p> developing good board evaluation functions in particular testifies to the difficulty of doing so (examples include [4] <ref> [20] </ref> [18] [15] [16]). Consequently, designing adaptive systems capable of automatically generating such functions is appealing. The notion of such adaptive systems can be found as early as Samuel's landmark checkers playing program [20] that was able to automatically adjust parameters of its evaluation function as it played in order to improve its performance. A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] [15] [22] [19] [1]. <p> of such adaptive systems can be found as early as Samuel's landmark checkers playing program <ref> [20] </ref> that was able to automatically adjust parameters of its evaluation function as it played in order to improve its performance. A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] [15] [22] [19] [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> Arthur Samuel first implemented a checker program on the IBM 701 in 1952, recoded it for the IBM 704 in 1954, and demonstrated it on television in 1956. His 1959 paper on the subject <ref> [20] </ref> was one of the first attempts to devise a credible computer game player for a strategy game, and one of the first to incorporate some form of machine learning. Samuel isolated several features of the checkers game that he considered to be useful information for deciding where to move.
Reference: [21] <author> C. E. Shannon. </author> <title> Programming a computer for playing chess. </title> <journal> Philosophical Magazine, </journal> <volume> 41 </volume> <pages> 256-275, </pages> <month> March </month> <year> 1950. </year>
Reference-contexts: As early as 1950, C. E. Shannon speculated on the possibility of using a computer to play chess, a revolutionary idea at the time <ref> [21] </ref>. He was the first to suggest (in print) the use of a board evaluation function in conjunction with a search mechanism. Arthur Samuel first implemented a checker program on the IBM 701 in 1952, recoded it for the IBM 704 in 1954, and demonstrated it on television in 1956.
Reference: [22] <author> Chuen-Tsai Sun, Ting-Hong Liao, Jing-Yi Lu, and Fu-May Zheng. </author> <title> Genetic algorithm learning in game playing with multiple coaches. </title> <booktitle> In Proceedings of the 1994 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 239-243, </pages> <year> 1994. </year>
Reference-contexts: A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] [26] [17] [15] <ref> [22] </ref> [19] [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> Another more conventional genetic algorithm representation is that used by Sun and Wu [23] for the game of Othello. They evolved the numerical coefficients of a linear board evaluation function. The terms of the function are features of the current state of the game. In a previous paper <ref> [22] </ref> they mention that the features used are a board position measurement, current mobility, stability, and two versions of potential mobility. This problem formulation means that the evaluation function will necessarily be a linear polynomial, which limits the flexibility of possible evaluation functions that can be evolved by this method.
Reference: [23] <author> Chuen-Tsai Sun and Ming-Da Wu. </author> <title> Self-adaptive genetic algorithm learning in game playing. </title> <booktitle> In Proceedings of the 1995 IEEE Conference on Evolutionary Computation, </booktitle> <pages> pages 814-818, </pages> <address> Perth, Australia, </address> <month> November </month> <year> 1995. </year>
Reference-contexts: It is difficult to imagine using the Rosin/Belew representation for a game such as chess, for instance. Another more conventional genetic algorithm representation is that used by Sun and Wu <ref> [23] </ref> for the game of Othello. They evolved the numerical coefficients of a linear board evaluation function. The terms of the function are features of the current state of the game.
Reference: [24] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: Among the papers which focus on the application of machine learning concepts to the development of board evaluation functions, one of the best known is the work of Gerald Tesauro on backgammon [26]. He used the T D () function developed by Sutton <ref> [24] </ref> for the training of multi-layer perceptrons to develop a board evaluation function for backgammon that learned strategy for the game by playing against itself hundreds of thousands of times. Each of these games has the perceptron, i.e., a neural network, select moves for both sides at each step. <p> A more rigorously mathematical description of the workings of this algorithm can be found in the papers of Sutton <ref> [24] </ref> and Tesauro [26]. Using this algorithm, from random initial weights, the neural net learned to play backgammon at a strong intermediate level, solely through this self-play mechanism. Lee and Mahajan [15] used Bayesian learning to optimize the nonlinear evaluation polynomial which was used by their Othello playing program, Bill.
Reference: [25] <author> Astro Teller. </author> <title> Turing completeness in the language of genetic programming with indexed memory. </title> <booktitle> In IEEE World Congress on Computational Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: It has even been shown that inclusion of primitives to read and write memory in some fashion can make genetic programming Turing Complete, enabling the generation of any conventional computer program <ref> [25] </ref>. What programs can actually be evolved in practice remains an open problem.
Reference: [26] <author> Gerald Tesauro. </author> <title> Temproal difference learning and TD-Gammon. </title> <journal> Communications of the ACM, </journal> <volume> 38(3) </volume> <pages> 58-68, </pages> <month> March </month> <year> 1995. </year>
Reference-contexts: A number of other researchers have likewise attempted to develop self-improving board evaluation functions [20] <ref> [26] </ref> [17] [15] [22] [19] [1]. Genetic programming gives us an adaptive framework that can automatically specify possible board evaluation functions using a natural representation to evolve players using a 1 2 survival-of-the-fittest mechanism. <p> Among the papers which focus on the application of machine learning concepts to the development of board evaluation functions, one of the best known is the work of Gerald Tesauro on backgammon <ref> [26] </ref>. He used the T D () function developed by Sutton [24] for the training of multi-layer perceptrons to develop a board evaluation function for backgammon that learned strategy for the game by playing against itself hundreds of thousands of times. <p> A more rigorously mathematical description of the workings of this algorithm can be found in the papers of Sutton [24] and Tesauro <ref> [26] </ref>. Using this algorithm, from random initial weights, the neural net learned to play backgammon at a strong intermediate level, solely through this self-play mechanism. Lee and Mahajan [15] used Bayesian learning to optimize the nonlinear evaluation polynomial which was used by their Othello playing program, Bill. <p> Because of the probabilistic nature of the game, a scheme to search beyond one ply is too computationally expensive to be practical, as is also the case in backgammon <ref> [26] </ref>. Consequently, we limit our application of the board evaluation function to the moves which can be made immediately. Each match in the Senet tournament is a best-of-three games match, with the winner of two or three games being declared the overall winner.
References-found: 26

