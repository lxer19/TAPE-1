URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers/parswec.ps
Refering-URL: http://www.cs.berkeley.edu/Research/Projects/parallel/castle/multipol/papers.html
Root-URL: http://www.cs.berkeley.edu
Title: Parallel Timing Simulation on a Distributed Memory Multiprocessor  
Author: Chih-Po Wen Katherine A. Yelick 
Address: Berkeley, CA 94720  
Affiliation: Computer Science Division University of California,  
Abstract: We present a parallel timing simulator, PARSWEC, that exploits speculative parallelism and runs on a distributed memory multiprocessor. It is based on an event-driven timing simulator called SWEC. Our approach uses optimistic scheduling to take advantage of the latency of digital signals. Using data from trace-driven analysis, we demonstrate that optimistic scheduling exploits more parallelism than conservative scheduling for circuits with feedback signal paths. We then describe the PARSWEC implementation and discuss several design trade-offs. Speedups over SWEC on large circuits are as high as 55 on a 64-node CM5 multiprocessor. These results indicate the feasibility of using distributed memory multiprocessors for large-scale circuit simulation. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Marek-Sadowska S. Lin, E. Kuh. Swec: </author> <title> A stepwise equivalent conductance simulator for cmos vlsi circuits. </title> <booktitle> In Proc. of European Design Automation conference, </booktitle> <month> February </month> <year> 1991. </year>
Reference-contexts: 1 Introduction We present a parallel timing simulator, PARSWEC, developed for distributed memory multiprocessors. PARSWEC is a parallelization of SWEC <ref> [1] </ref>, an event-driven timing simulator. SWEC employs a stepwise linear waveform and device model, in which subcir-cuits are evaluated by solving a linear system of node voltages. The evaluation of a subcircuit is triggered by events generated by the state changes of the subcircuit and its fanins.
Reference: [2] <author> R. Saleh et al. </author> <title> Parallel circuit simulation on supercomputers. </title> <journal> Proc. of the IEEE, </journal> <volume> 77(2), </volume> <month> December </month> <year> 1989. </year>
Reference-contexts: The rate of state change determines the time step size to be used for a given subcircuit at a particular time point. These features take advantage of the latency and multirate properties in most digital circuits <ref> [2] </ref>. The latency property states that most digital signals change infrequently; the multirate property states that different parts of a circuit produce signals at different speeds.
Reference: [3] <author> L. Snyder M. Bailey. </author> <title> An empirical study of on-chip parallelism. </title> <booktitle> In Proc. of 25th Design Automation Conference, </booktitle> <year> 1988. </year>
Reference-contexts: Optimistic scheduling exploits the parallelism determined by the actual signal flows at runtime, rather than limiting parallelism to the static interconnection of the circuit. A previous study shows the lack of static parallelism for several CMOS circuits <ref> [3] </ref>. We demonstrate that for sequential circuits, available run-time parallelism is much higher than static parallelism. This justifies our choice of optimistic scheduling|although it requires more memory and potentially more total computation than conservative scheduling, it leads to better overall performance on a large scale multiprocessor. <p> large amount of memory for storing history information and incurs overhead from rollbacks, but as we show in Section 4, it yields more parallelism than a conservative approach for timing simulation of sequential circuits. 4 Measuring Available Parallelism Bailey and Snyder performed a study of available parallelism in digital circuits <ref> [3] </ref> to explain the limitations in parallelizing SPICE-like circuit simulators. They measured the real time parallelism, the average number of transistors switching at the same time, in six circuits.
Reference: [4] <author> D.R. Jefferson. </author> <title> Virtual time. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 7(3), </volume> <month> July </month> <year> 1985. </year>
Reference-contexts: In this paper we present the following: * trace-driven simulation results that demonstrate the parallelism advantage of optimistic scheduling for sequential CMOS circuits; * a parallel timing simulation algorithm based on the Timewarp idea of Jefferson <ref> [4] </ref>, which was pre viously used for other simulation domains; * an implementation for a scalable (distributed memory) multiprocessor, which is highly tuned for locality to maximize performance; * analysis of the design trade-offs and performance costs in our implementation. 2 Problem Statement The first step in a timing simulation is <p> Memory management in PARSWEC is crucial, because there is no virtual memory on the CM5, and subcircuit states, message queues, and event histories can be large. Subcircuit states are reclaimed by fossil collection. To avoid message buffer overflow, Jeffer-son proposed a flow control protocol that runs along with Timewarp <ref> [4] </ref>. Instead, we place restrictions on scheduling and use fixed history sizes. Namely, we require that all event messages be acknowledged, and that a Schedule operation for a region r is started only after the previous event messages from r have been processed.
Reference: [5] <author> J. Misra K.M. Chandy. </author> <title> Asynchronous distributed simulation via a sequence of parallel computations. </title> <journal> Communications of the ACM, </journal> <volume> 24(11), </volume> <month> April </month> <year> 1981. </year>
Reference-contexts: Processes communicate by sending events, which are time-stamped messages containing state values. Two techniques for parallel discrete event simulation correspond to two different scheduling policies. The Chandy-Misra algorithm uses conservative scheduling <ref> [5] </ref>, and the Time-warp algorithm uses optimistic scheduling. In the Chandy-Misra algorithm, each process keeps a logical clock to denote its progress. Progress is conveyed to other processes via the timestamps of events.
Reference: [6] <author> J. Pal Singh et al. </author> <title> Splash: Stanford parallel application for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1), </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: Deadlocks can occur if there are cyclic dependencies in between processes, so deadlock prevention or detection is needed. Both of these require global information and therefore incur communication and synchronization overhead in a distributed environment. In logic simulation, for example, the Splash study shows that deadlocks severely limit parallelism <ref> [6] </ref>. The Timewarp algorithm relies on the rarity of events. Processes may compute ahead even if more events for earlier times are forthcoming. If events do come, the results are discarded and evaluation resumed from a previous time point.
Reference: [7] <author> B. Ackland E. DeBenedicts. </author> <title> Circuit simulation on a hypercube. </title> <booktitle> In Distributed Simulation Conference, </booktitle> <year> 1988. </year>
Reference-contexts: Notice that even with the overhead of optimistic paralleliza-tion, the actual speedup achieved with PLA is greater than the theoretical speedup achievable by the conservative method. The peak speedups of our simulator are far greater than those reported for similar timing simulators in <ref> [7, 8] </ref>. The speedup for SIMD is particularly encouraging, and it shows the feasibility of using large distributed memory multiprocessors to perform large simulations. The speedup is usually below the theoretical maximum, due to the overhead of data management, scheduling, and communication that arise in practice. <p> Previous work on parallel timing simulation includes XPSIM [8], an asynchronous algorithm, and CEMU <ref> [7] </ref>, which has both a synchronous and an asynchronous version. CEMU results indicate that the synchronous algorithm is more efficient than the asynchronous one; the synchronous implementation showed speedups up to 20 on a 64-processor hypercube, whereas the asynchronous version showed speedups under 10.
Reference: [8] <author> T. Lee. </author> <title> Parallel circuit simulation: A case study in parallelizing programs. </title> <type> Technical Report Master thesis, </type> <institution> Computer Science Division, University of Califor-nia, Berkeley, </institution> <year> 1991. </year>
Reference-contexts: Notice that even with the overhead of optimistic paralleliza-tion, the actual speedup achieved with PLA is greater than the theoretical speedup achievable by the conservative method. The peak speedups of our simulator are far greater than those reported for similar timing simulators in <ref> [7, 8] </ref>. The speedup for SIMD is particularly encouraging, and it shows the feasibility of using large distributed memory multiprocessors to perform large simulations. The speedup is usually below the theoretical maximum, due to the overhead of data management, scheduling, and communication that arise in practice. <p> Previous work on parallel timing simulation includes XPSIM <ref> [8] </ref>, an asynchronous algorithm, and CEMU [7], which has both a synchronous and an asynchronous version. CEMU results indicate that the synchronous algorithm is more efficient than the asynchronous one; the synchronous implementation showed speedups up to 20 on a 64-processor hypercube, whereas the asynchronous version showed speedups under 10.
Reference: [9] <editor> Jr. et al. J.V. Briner. </editor> <booktitle> Breaking the barrier of parallel simulation of digital systems. In Proc. Design Automation Conference, </booktitle> <year> 1991. </year>
Reference-contexts: The adverse effect of increasing history size for PLA probably comes from the rollback overhead due to excessive speculation, plus the increased cost for managing longer histories. 7 Previous Work The Timewarp simulation technique has been applied to logic and mixed-level simulation <ref> [9] </ref>, but we are not aware of any Timewarp timing simulator at the level of precision of SWEC. Previous work on parallel timing simulation includes XPSIM [8], an asynchronous algorithm, and CEMU [7], which has both a synchronous and an asynchronous version.
References-found: 9

