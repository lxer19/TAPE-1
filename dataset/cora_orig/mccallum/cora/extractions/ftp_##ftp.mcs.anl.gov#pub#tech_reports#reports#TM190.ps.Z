URL: ftp://ftp.mcs.anl.gov/pub/tech_reports/reports/TM190.ps.Z
Refering-URL: http://www.mcs.anl.gov/~toonen/
Root-URL: http://www.mcs.anl.gov
Title: Load-Balancing Algorithms for the Parallel Community Climate Model  
Author: by Ian T. Foster and Brian R. Toonen 
Note: This work was supported by the Atmospheric and Climate Research Division, Office of Energy Research, Office of Health and Environmental Research, U.S. Department of Energy, under Contract W-31-109-Eng-38.  
Date: January 1995  
Web: ANL/MCS-TM-190  
Address: 9700 South Cass Avenue Argonne, IL 60439  
Affiliation: ARGONNE NATIONAL LABORATORY  Mathematics and Computer Science Division  
Pubnum: Technical Memorandum No. 190  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Bath, L., Olson, J., and Rosinski, J. </author> <title> User's Guide to NCAR CCM2. </title> <institution> National Center for Atmospheric Research, Boulder, Colorado. </institution> <year> 1992. </year>
Reference-contexts: This proportion is expected to increase as other components of the model are optimized. Three types of physics time step can be distinguished within PCCM2: partial radiation, full radiation, and no radiation <ref> [1] </ref>. Partial radiation time steps occur every hour (which, in the current implementation, is every third time step). During these time steps, the shortwave radiation calculations are performed. A full radiation time step additionally computes the absorptivity and emissivity of longwave radiation.
Reference: [2] <author> Drake, J., Walker, D., and Worley, P. </author> <title> Parallelizing the Spectral Transform Method - Part II, </title> <institution> ORNL/TM-11855. Oak Ridge National Laboratory, Oak Ridge, Tennessee. </institution> <year> 1991. </year>
Reference-contexts: For the purposes of this discussion, we restrict N glat , N glon , P lat , and P lon to powers of two. The following restrictions also apply: P lat (N glat =2) and P lon (N glon =4). The resulting structure is illustrated in Figure 1 <ref> [2] </ref>.
Reference: [3] <author> Drake, J., Foster, I., Hack, J., Michalakes, J., Semeraro, B., Toonen, B., Williamson, D., and Worley, P. PCCM2: </author> <title> A GCM Adapted for Scalable Parallel Computers. </title> <booktitle> Proc. AMS Annual Meeting, AMS. </booktitle> <year> 1994. </year>
Reference-contexts: 2 PCCM2 While much of the work reported in this paper is independent of any particular climate model, our implementation work and empirical studies have been performed in the context of PCCM2, a parallel implementation of the Community Climate Model (CCM2) developed by the National Center for Atmospheric Research (NCAR) <ref> [3] </ref>. Hence, we provide a brief introduction to the structure of this model. <p> The parallel implementation uses domain decomposition techniques to decompose these data structures, and associated computation, in the two horizontal dimensions <ref> [3] </ref>. Some of these data structures are used only by dynamics or only by physics; others are shared by the two components. At each time step, a subset of these data structures is passed between the two components of the model, which are executed one after the other.
Reference: [4] <author> Fox, G., Johnson, M., Lyzenga, G., Otto, S., Salmon, J., and Walker D. </author> <title> Solving Problems on Concurrent Processors. </title> <publisher> Prentice-Hall. </publisher> <year> 1988. </year> <month> 20 </month>
Reference-contexts: Contemporary examples of this architecture include the Intel Paragon, Thinking Machines CM5, IBM SP, and CRAY T3D. Science and engineering applications can often be adapted for execution on scalable parallel computers by using a technique called domain decomposition <ref> [4] </ref>. This works as follows. First, principal program data structures are decomposed into disjoint subdo-mains of approximately equal size. Then, each subdomain is mapped together with its associated computation to a different processor. Finally, communication is introduced to move data between subdomains when this is required for computation.
Reference: [5] <author> Michalakes, J. </author> <title> Analysis of Workload and Load Balancing Issues in the NCAR Com--munity Climate Model, </title> <institution> ANL/MCS-TM-144. Argonne National Laboratory, Argonne, Illinois, </institution> <year> 1991. </year>
Reference-contexts: Furthermore, cycling between the three types of time steps introduces still another form of load variation. Other load imbalances encountered in PCCM2 physics include land/sea imbalances, variations caused by weather patterns (e.g., convection over the Indian subcontinent during the monsoon), and the seasonal cycle <ref> [5] </ref>.
Reference: [6] <author> Michalakes, J., and Stevens, R. </author> <title> Analysis of Computational Load Distribution in the NCAR Community Climate Model. Computer Hardware, Advanced Mathematics and Model Physics Pilot Project Final Report, </title> <booktitle> DOE/ER-0541T, </booktitle> <pages> pages 19-24. </pages> <institution> U.S. Department of Energy. </institution> <year> 1992. </year> <month> 21 </month>
Reference-contexts: The remaining time steps are referred to as "no-radiation steps," since no solar radiation computations are performed. A study of computational load distribution within CCM2 reveals that the most significant source of load imbalance is the diurnal cycle <ref> [6] </ref>. <p> The load imbalance introduced by the diurnal cycle is temporal as well as spatial <ref> [6] </ref>. The earth's rotation about its axis causes the area impacted by solar radiation to continuously shift westward.
References-found: 6

