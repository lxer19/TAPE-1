URL: http://www.cs.ubc.ca/spider/cebly/Papers/_download_/avg-reward.ps
Refering-URL: http://www.cs.ubc.ca/spider/cebly/papers.html
Root-URL: 
Email: cebly@cs.ubc.ca  marty@markov.commerce.ubc.ca  
Title: Process-Oriented Planning and Average-Reward Optimality  
Author: Craig Boutilier Martin L. Puterman 
Web: http://www.cs.ubc.ca/spider/cebly/craig.html  http://acme.commerce.ubc.ca/puterman/puterman.html  
Address: Vancouver, BC V6T 1Z4, CANADA  Vancouver, BC V6T 1Z2, CANADA  
Affiliation: Department of Computer Science University of British Columbia  Faculty of Commerce University of British Columbia  
Date: August, 1995  
Note: To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal,  
Abstract: We argue that many AI planning problems should be viewed as process-oriented, where the aim is to produce a policy or behavior strategy with no termination condition in mind, as opposed to goal-oriented. The full power of Markov decision models, adopted recently for AI planning, becomes apparent with process-oriented problems. The question of appropriate optimality criteria becomes more critical in this case; we argue that average-reward optimality is most suitable. While construction of average-optimal policies involves a number of subtleties and computational difficulties, certain aspects of the problem can be solved using compact action representations such as Bayes nets. In particular, we provide an algorithm that identifies the structure of the Markov process underlying a planning problem a crucial element of constructing average optimal policies without explicit enumeration of the problem state space.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bellman, R. E. </author> <year> 1957. </year> <title> Dynamic Programming. </title> <publisher> Princeton U. Press. </publisher>
Reference: <author> Boutilier, C. and Dearden, R. </author> <year> 1994. </year> <title> Using abstractions for decision-theoretic planning with time constraints. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1016-1022, Seattle. </address>
Reference: <author> Boutilier, C., Dearden, R., and Goldszmidt, M. </author> <year> 1995. </year> <title> Exploiting structure in policy construction. </title> <booktitle> IJCAI-95. This volume. </booktitle>
Reference-contexts: We also have four boolean variables denoting whether there is mail in the user's box (M ), an outstanding coffee request by the user (CR), the robot has 1 For example, software agents, as commonly conceived, often have this flavor. 2 In <ref> (Boutilier and Puterman 1995) </ref> we give a full description of this problem, and further details of our algorithms. To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 mail (HRM), or the robot has coffee (HRC). This gives rise to a problem with 400 states. <p> On the other hand, associating rewards with state transitions (e.g., a transition to a good state from a bad one) has its own difficulties. We discuss these issues in detail in <ref> (Boutilier and Puterman 1995) </ref>. For this problem, and many in which there are separate objectives to be balanced, a useful reward model is one where penalties are associated with states in which objectives are unsatisfied. <p> For the problems we consider, optimal stationary policies always exist. 6 We discuss stopping criteria in Section 4; see (Puterman 1994). <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref> for a more detailed discussion of this representation). PuM, describing the effect of PuM independent of any event occurrences. The tables for the postaction variables describe the effects of the action. <p> Thus, the identification of the underlying chain structure of an MDP becomes an important computational tool for constructing average optimal policies. We note that the techniques of <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref> can be applied in this setting, allowing value iteration to work on groups of states instead of com 8 In the full paper we discuss weakly communicating MDPs, which share nice features with communicating MDPs. 9 The span of a function V on S is defined <p> By creating cycles whenever possible, the problem representation tends to stay compact. Structured Fox-Landi Algorithm: We give a high-level sketch of the SFL algorithm (Figure 3), and describe its application to our example (Figure 2). We defer a detailed description to <ref> (Boutilier and Puterman 1995) </ref> along with more formal definitions and a proof of correctness. The example here blurs a number of steps in the algorithm for conciseness. We begin by choosing the initial s-state in Figure 2 (a). <p> If our aim is to simply categorize an MDP as communicating or not, the algorithm can be terminated as soon as any transient states (or multiple recurrent classes) are identified. If identified as communicating, a simple algorithm like value iteration, or related methods based on structured representations <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref>, can be used to determine the average optimal policy. If the algorithm discovers more than one recurrent class then the MDP is multichain (i.e., general). If a single recurrent class is discovered together with transient states, then it may be weakly communicating or multichain. <p> The second level of FL provides new recurrent classes for which optimal gain (in the sub-problem) is constant. These can be pieced together with the previously classified states 11 The precise meaning of the graph and its construction are described in <ref> (Boutilier and Puterman 1995) </ref>. to determine a new policy: if the gain in the subproblem is greater, these states adopt actions that keep them from the earlier states.
Reference: <author> Boutilier, C. and Puterman, M. L. </author> <year> 1995. </year> <title> Communicating Structure and Average Optimal Policies. </title> <type> Tech. report, </type> <institution> Univ. British Columbia, </institution> <address> Vancouver. </address> <publisher> (Forthcoming). </publisher>
Reference-contexts: We also have four boolean variables denoting whether there is mail in the user's box (M ), an outstanding coffee request by the user (CR), the robot has 1 For example, software agents, as commonly conceived, often have this flavor. 2 In <ref> (Boutilier and Puterman 1995) </ref> we give a full description of this problem, and further details of our algorithms. To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 mail (HRM), or the robot has coffee (HRC). This gives rise to a problem with 400 states. <p> On the other hand, associating rewards with state transitions (e.g., a transition to a good state from a bad one) has its own difficulties. We discuss these issues in detail in <ref> (Boutilier and Puterman 1995) </ref>. For this problem, and many in which there are separate objectives to be balanced, a useful reward model is one where penalties are associated with states in which objectives are unsatisfied. <p> For the problems we consider, optimal stationary policies always exist. 6 We discuss stopping criteria in Section 4; see (Puterman 1994). <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref> for a more detailed discussion of this representation). PuM, describing the effect of PuM independent of any event occurrences. The tables for the postaction variables describe the effects of the action. <p> Thus, the identification of the underlying chain structure of an MDP becomes an important computational tool for constructing average optimal policies. We note that the techniques of <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref> can be applied in this setting, allowing value iteration to work on groups of states instead of com 8 In the full paper we discuss weakly communicating MDPs, which share nice features with communicating MDPs. 9 The span of a function V on S is defined <p> By creating cycles whenever possible, the problem representation tends to stay compact. Structured Fox-Landi Algorithm: We give a high-level sketch of the SFL algorithm (Figure 3), and describe its application to our example (Figure 2). We defer a detailed description to <ref> (Boutilier and Puterman 1995) </ref> along with more formal definitions and a proof of correctness. The example here blurs a number of steps in the algorithm for conciseness. We begin by choosing the initial s-state in Figure 2 (a). <p> If our aim is to simply categorize an MDP as communicating or not, the algorithm can be terminated as soon as any transient states (or multiple recurrent classes) are identified. If identified as communicating, a simple algorithm like value iteration, or related methods based on structured representations <ref> (Boutilier, Dearden and Goldszmidt 1995) </ref>, can be used to determine the average optimal policy. If the algorithm discovers more than one recurrent class then the MDP is multichain (i.e., general). If a single recurrent class is discovered together with transient states, then it may be weakly communicating or multichain. <p> The second level of FL provides new recurrent classes for which optimal gain (in the sub-problem) is constant. These can be pieced together with the previously classified states 11 The precise meaning of the graph and its construction are described in <ref> (Boutilier and Puterman 1995) </ref>. to determine a new policy: if the gain in the subproblem is greater, these states adopt actions that keep them from the earlier states.
Reference: <author> Dean, T., Kaelbling, L. P., Kirman, J., and Nicholson, A. </author> <year> 1993. </year> <title> Planning with deadlines in stochastic domains. </title> <booktitle> AAAI-93, </booktitle> <address> pp.574-579, Washington, D.C. </address>
Reference-contexts: Such problems have received the bulk of the attention from the planning community, even when uncertainty is involved (though a relaxed definition of success may be used (Kushm-erick, Hanks and Weld 1994)). In decision-theoretic settings, goal-based approaches are also common, with utilities used often to discriminate feasible plans <ref> (Dean et al. 1993) </ref>. A process-oriented problem is one in which there does not (necessarily) exist a goal state of the type described above. More specifically, there may be no state (or goal) such that the agent should stop acting once that state is reached (or the goal is true).
Reference: <author> Dean, T. and Kanazawa, K. </author> <year> 1989. </year> <title> A model for reasoning about persistence and causation. </title> <journal> Comp. Intel., </journal> <volume> 5(3) </volume> <pages> 142-150. </pages>
Reference-contexts: Recently, a number of action representations such as STRIPS and influence diagrams have been applied to the problem of representing stochastic actions and MDPs generally (Kushmerick, Hanks and Weld 1994; Boutilier and Dearden 1994; Tatman and Shachter 1990). We adopt the two-slice temporal Bayes network <ref> (Dean and Kanazawa 1989) </ref>.
Reference: <author> Dean, T. and Wellman, M. </author> <year> 1991. </year> <title> Planning and Control. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: In particular, there has been much interest in decision-theoretic planning (DTP) <ref> (Dean and Wellman 1991) </ref>. The theory of Markov decision processes (MDPs) has found considerable popularity recently both as a conceptual and computational model for DTP (Dean et al. 1993; Boutilier and Dearden 1994).
Reference: <author> Fox, B. L. and Landi, D. M. </author> <year> 1968. </year> <title> An algorithm for identifying the ergodic subchains and transient states of a stochastic matrix. </title> <journal> Comm. of the ACM, </journal> <volume> 2 </volume> <pages> 619-621. </pages>
Reference-contexts: The second aim and key technical contribution of this paper is the development of an algorithm that determines this structure using a compact representation of the MDP's dynamics. Unlike existing algorithms for structure classification <ref> (Fox and Landi 1968) </ref>, our algorithm exploits the problem representation to avoid enumeration and traversal of the underlying state space. This is an important feature because the planning state space grows exponentially with the number of variables or features present. <p> The classification algorithm we use has the added advantage that it can be used to apply value iteration (piecewise) to general MDPs (as we sketch below). An efficient algorithm for classifying Markov chains known as the Fox-Landi algorithm (FL) <ref> (Fox and Landi 1968) </ref> can be extended to the classification of MDPs by considering the reachability matrix for the MDP.
Reference: <author> Howard, R. A. </author> <year> 1971. </year> <title> Dynamic Probabilistic Systems. </title> <publisher> Wiley. </publisher>
Reference-contexts: If exogenous events can cause these goals to become false, then such a plan proceeds indefinitely. MDPs are excellent models for such process-oriented problems: techniques such as policy iteration <ref> (Howard 1971) </ref> can be used to derive optimal plans for infinite horizon problems of this type under uncertainty. Unfortunately, the emphasis in recent work using MDPs for DTP has been on goal-oriented problems (Dean et al. 1993; Boutilier and Dearden 1994), albeit conditional and decision-theoretic. <p> The expected value (under this measure) of a fixed policy at any given state s can be shown to satisfy <ref> (Howard 1971) </ref>: V (s) = R (s) + fi t2S The value of at any initial state s can be computed by solving this system of linear equations. A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 .
Reference: <author> Kushmerick, N., Hanks, S., and Weld, D. </author> <year> 1994. </year> <title> An algorithm for probabilistic least-commitment planning. </title> <booktitle> AAAI-94, </booktitle> <address> pp.1073-1078, Seattle. </address>
Reference: <author> Puterman, M. L. </author> <year> 1994. </year> <title> Markov Decision Processes: Discrete Stochastic Dynamic Programming. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: For the problems we consider, optimal stationary policies always exist. 6 We discuss stopping criteria in Section 4; see <ref> (Puterman 1994) </ref>. (Boutilier, Dearden and Goldszmidt 1995) for a more detailed discussion of this representation). PuM, describing the effect of PuM independent of any event occurrences. The tables for the postaction variables describe the effects of the action. <p> An MDP is communicating 7 We assume this limit exists. This may not be the case if the MDP admits policies that are periodic; in this case, the definition may use a slightly more robust Cesaro limit <ref> (Puterman 1994) </ref>. if for any pair of states s; t, there is some policy under which s can reach t. <p> Note also that setting fi = 1 is not problematic; relative value iteration can be used if undiscounted values get too large. See <ref> (Puterman 1994) </ref> for these details. To appear, Proc. Fourteenth Inter. Conf. on AI (IJCAI-95),Montreal, August, 1995 puting over an explicitly enumerated state space, if it can be factored (e.g., using a Bayes net).
Reference: <author> Puterman, M. L. and Shin, M. </author> <year> 1978. </year> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137. </pages>
Reference-contexts: A policy is optimal if V (s) V 0 (s) for all s 2 S and policies 0 . Techniques for constructing optimal policies for discounted problems have been well-studied. While algorithms such as modified policy iteration <ref> (Puterman and Shin 1978) </ref> are often used in practice, an especially simple algorithm is value iteration, based on Bellman's (1957) principle of optimality. We discuss value iteration because it can, under certain conditions, be used directly for average-reward problems as we describe below.
Reference: <author> Ross, K. W. and Varadarajan, R. </author> <year> 1991. </year> <title> Multichain Markov decision processes with a sample-path constraint: A decomposition approach. </title> <journal> Math. of Op. Res., </journal> <volume> 16(1) </volume> <pages> 195-207. </pages>
Reference: <author> Singh, S. P. </author> <year> 1994. </year> <title> Reinforcement learning algorithms for average-payoff markovian decision processes. AAAI-94, </title> <publisher> pp.700-705. </publisher>
Reference-contexts: This last issue, the design of appropriate optimality criteria, has been paid little attention in DTP. MDPs have been used for planning and reinforcement learning quite extensively, and most models measure the goodness of policies using discounted total reward (one exception is <ref> (Singh 1994) </ref>). However, little thought seems to have been given to this choice of optimality measure or to good discounting rates. In fact, for many ongoing processes it seems that the correct (or most useful) measure of a policy is the average reward it accrues per unit time.
Reference: <author> Tatman, J. A. and Shachter, R. D. </author> <year> 1990. </year> <title> Dynamic programming and influence diagrams. </title> <journal> IEEE Trans. Sys., Man and Cyber., </journal> <volume> 20(2) </volume> <pages> 365-379. </pages>
References-found: 15

