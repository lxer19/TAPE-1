URL: http://www.icsi.berkeley.edu/ftp/global/pub/speech/papers/ncomp92-fact.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/papers.html
Root-URL: http://www.icsi.berkeley.edu
Title: FACTORING NETWORKS BY A STATISTICAL METHOD  
Author: Nelson Morgan and Herve Bourlard 
Address: Berkeley, CA 94704, USA Lernout Hauspie Speechproducts, Ieper, B-8900, BELGIUM  
Affiliation: International Computer Science Institute,  
Abstract: We show that it is possible to factor a multi-layered classification network with a large output layer into a number of smaller networks, where the product of the sizes of the output layers equals the size of the original output layer. No assumptions of statistical independence are required. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bourlard, H., & Wellekens, C.J., </author> <year> 1990, </year> <title> "Links Between Markov Models and Multilayer Perceptrons", </title> <journal> IEEE Trans. on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 12, no. 12, </volume> <pages> pp. </pages> <note> 1167-1178 Morgan, </note> <author> N., Bourlard, H., Wooters, C., Kohn, P., & Cohen, M., </author> <year> 1991, </year> <title> "Phonetic Context in Hybrid HMM/MLP Continuous Speech Recognition", </title> <booktitle> Proc. of Eurospeech'91, </booktitle> <address> Genova, </address> <publisher> pp.109-112 Morgan, </publisher> <editor> N., Bourlard, H., </editor> <title> "Generalization and Parameter Estimation in Feedforward Nets: Some Experiments", </title> <booktitle> 1990. Advances in Neural Information Processing Systems II, </booktitle> <publisher> Morgan Kauf-mann 3 </publisher>
Reference-contexts: and show an efficient implementation of the resulting networks. 2 MLPs TO ESTIMATE FRAMEWISE PROBABILITIES Earlier work has shown that the outputs of a Multi-Layer Perceptron (MLP) trained in classification mode with a Least-Mean-Square or relative entropy criterion can be interpreted as posterior probabilities of each class given the input <ref> (Bourlard and Wellekens, 1990) </ref>. <p> However, parameter reduction is exactly the aim of the proposed approach, both to reduce computation and to improve generalization. As it was done for p (s ` jx n ) (Bourlard and Wellekens, 1990; Morgan et al, 1991), it will be necessary to find (e.g. by using cross-validation techniques <ref> (Morgan and Bourlard, 1990) </ref>) the number of hidden units (and hence the number of parameters) leading to the best estimate of p (r j js ` ; x n ). The desired probabilities can in principle be estimated without any statistical assumptions (e.g., independence).
References-found: 1

