URL: ftp://ftp.cs.washington.edu/tr/1996/07/UW-CSE-96-07-02.PS.Z
Refering-URL: http://www.cs.washington.edu/research/tr/tr-by-title.html
Root-URL: 
Email: fbaer,xqing@cs.washington.edu  
Title: On the Use and Performance of Explicit Communication Primitives in Cache-coherent Multiprocessor Systems  
Author: Xiaohan Qin and Jean-Loup Baer 
Keyword: communication primitives, software-controlled cache coherence, performance evalua tion, shared-memory multiprocessors  
Date: July 16, 1996  
Address: Box 352360  Seattle, Wa 98195-2350  
Affiliation: Department of Computer Science and Engineering,  University of Washington,  
Abstract: Recent developments in shared-memory multiprocessor systems advocate using off-the-shelf hardware to provide basic communication mechanisms and using software to implement cache coherence policies. The exposure of communication mechanisms to software opens many oppor tunities for enhancing application performance. In this paper we propose a set of communication primitives that are absent from pure cache coherent schemes. The communication primitives, implemented on a communication co-processor, introduce a flavor of message passing and permit protocol optimization, without sacrificing the simplicity of the shared memory systems. To assess the overhead of the software implementation of the primitives and protocols, we compare, via simulation, the execution of three programs from the SPLASH-2 suite on four environments: a PRAM model, a hardware cache coherence scheme, a software scheme imple menting only the basic cache coherence protocol, and an optimized software solution supporting the additional communication primitives and running with applications annotated with those primitives. With the parameters we chose for the communication processor, the overall memory system overhead of the basic software scheme is at least 50% higher than that of the hardware implementation. With the adequate insertion of the communication primitives, the optimized software solution has a performance comparable to that of the hardware scheme. These results show that the trend towards software-controlled cache coherence is justified since there is no loss in performance and the software solution is more flexible and scalable. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Agarwal, D. Kranz B.-H. Lim, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> a processor architecture for multiprocessing. </title> <booktitle> In Proceedings of 17th International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <year> 1990. </year>
Reference-contexts: In an optimized design, the 5 E O Y Interconnection Network . . . . . Network Interface . . Comp Processor & Cache Protocol Processor & Cache Processor/Cache Interface overhead of interrupt handling and context switch can be minimized to just a few instructions <ref> [1] </ref>. Finally, in our architectural model, we let the private references bypass the communication pro cessor to avoid unnecessary processing overhead. To that effect, the compute processor is directly connected to the memory module. In our current model, both private and shared read misses block the compute processor.
Reference: [2] <author> D. H. Bailey. </author> <title> FFT in external or hierarchical memory. </title> <journal> Journal of Supercomputing, </journal> <volume> 4(1) </volume> <pages> 23-35, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: We then present the results of the simulation of the four architectures described in Section 4 and assess the effectiveness of the software implementation using the communication primitives. 5.1 FFT The application The SPLASH-2 FFT algorithm is optimized for distributed or hierarchical memory systems <ref> [2] </ref>. The input data of FFT consists of a p p n matrix of complex numbers. The major data structures are the input matrix A, its transpose B, and another matrix of same dimension for the "roots of unity".
Reference: [3] <author> D. H. Bailey et al. </author> <title> The NAS parallel benchmarks. </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> 5(3) </volume> <pages> 63-73, </pages> <month> Fall </month> <year> 1991. </year>
Reference-contexts: In that case, the performance of the optimized software is as good as that of the hardware implementation which, itself, is within 20% to 45% of the PRAM lower bound. 5.3 RADIX Sort The application RADIX sort is part of the NAS parallel benchmark <ref> [3] </ref>. It sorts k-bit integers by examining r bits (r k) of the keys per iteration. In the parallel implementation, each processor is assigned an equal fraction of the keys. An iteration consists of three phases.
Reference: [4] <author> V. Bala et al. </author> <title> The IBM external user interface for scalable parallel systems. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 445-462, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Since our interest was mainly on performance related issues, we have concentrated on the latter, imposing a global coherence strategy for prefetching and bulk data transfers. The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces <ref> [4, 24] </ref>, prefetching [23, 6, 14] and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [5] <author> L. M. Censier and P. Feautrier. </author> <title> A new solution to coherence problems in multicache systems. </title> <journal> IEE Transactions on Computers, </journal> <volume> c-27(12):1112-1118, </volume> <month> Dec. </month> <year> 1978. </year>
Reference-contexts: the communication primitives with respect to a hardware implementation, we will isolate the contributions of each component and compare the performance of the applications, via simulation, in four experimental environments. * Case 1: A machine with a perfect memory system (PRAM model). * Case 2: A hardware-based cache-coherent (full directory <ref> [5] </ref>) system. * Case 3: A system that uses a communication processor/node with the communication pro cessor implementing the coherence protocol (software implementation). * Case 4: A system as in case 3, with, in addition, the communication processor being able to handle the user-based communication primitives (optimized software implementation).
Reference: [6] <author> T.-F. Chen and J.-L. Baer. </author> <title> A performance study of software and hardware data prefetching schemes. </title> <booktitle> In Proceedings the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 223-32, </pages> <year> 1994. </year>
Reference-contexts: The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching <ref> [23, 6, 14] </ref> and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [7] <author> T. M. Chilimbi and J. R. Larus. Cachier: </author> <title> A tool for automatically inserting cico annotations. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 89-98, </pages> <year> 1994. </year>
Reference-contexts: A separation of the coherence policy and of the communication primitives to support it requires a set of communication and memory-system mechanisms that can be used at run-time to implement application-specific coherence policies. The usage of such primitives can be directed by the user or the compiler <ref> [7] </ref>, or can be detected by some monitoring device. The implementation of these primitives requires some programmable network interface [22]. In this paper, we propose a set of communication primitives that will allow the user to take advantage of features common in message-passing systems.
Reference: [8] <author> A. L. Cox and R. J. Fowler. </author> <title> Adaptive cache coherency for detecting migratory shared data. </title> <booktitle> In Proceedings of 20th International Symposium on Computer Architecture, </booktitle> <pages> pages 98-108, </pages> <year> 1993. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level [12, 13] or in incremental fashion [17], can be dictated by the compiler [25, 10], or can be the result of hardware monitoring <ref> [8, 29] </ref>.
Reference: [9] <editor> D. E. Culler et al. </editor> <booktitle> Parallel programming in Split-C. In Proceedings Supercomputing '93, </booktitle> <pages> pages 262-73, </pages> <year> 1993. </year>
Reference-contexts: The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching [23, 6, 14] and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C <ref> [9] </ref>, and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [10] <author> R. Cytron, S. Karlovsky, and K. P. McAuliffe. </author> <title> Automatic management of programmable caches. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 229-238, </pages> <year> 1988. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level [12, 13] or in incremental fashion [17], can be dictated by the compiler <ref> [25, 10] </ref>, or can be the result of hardware monitoring [8, 29].
Reference: [11] <author> S. Eggers and R. Katz. </author> <title> A characterization of sharing in parallel programs and its application to coherency protocol evaluation. </title> <booktitle> In Proc. of 15th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 373-382, </pages> <year> 1988. </year>
Reference-contexts: Execution time in the hardware implementation is more than double that of the PRAM when we include a realistic memory latency and a fast but inflexible coherence protocol. Third, as already noticed in other studies <ref> [11] </ref>, infinite caches can be worse than large finite caches because while there is less data traffic there can be more coherence traffic or contention for data in a single cache.
Reference: [12] <author> B. Falsafi et al. </author> <title> Application-specific protocols for user-level shared memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 380-9, </pages> <year> 1994. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level <ref> [12, 13] </ref> or in incremental fashion [17], can be dictated by the compiler [25, 10], or can be the result of hardware monitoring [8, 29].
Reference: [13] <author> M. I. Frank and M. K. Vernon. </author> <title> A hybrid shared messory/message passing parallel machine. </title> <booktitle> In Proceedings of International Conference on Parallel Processing, </booktitle> <pages> pages 232-236, </pages> <year> 1993. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level <ref> [12, 13] </ref> or in incremental fashion [17], can be dictated by the compiler [25, 10], or can be the result of hardware monitoring [8, 29].
Reference: [14] <author> E. Gornish, E. Granston, and A. Veidenbaum. </author> <title> Compiler-directed data prefetching in multiprocessor with memory hierarchies. </title> <booktitle> In Proceedings of International Conference on Supercomputing 1990, </booktitle> <pages> pages 354-368, </pages> <year> 1990. </year> <month> 22 </month>
Reference-contexts: The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching <ref> [23, 6, 14] </ref> and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [15] <author> J. Heinlein, K. Gharachorloo, S. Dresser, and A. Gupta. </author> <title> Integration of message passing and shared memory in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 38-50, </pages> <year> 1994. </year>
Reference-contexts: Combining message passing with shared memory to overcome some of the inefficiencies of cache coherence mechanisms was first proposed in the context of the Alewife project [20, 21] and further elaborated in Flash <ref> [15] </ref>. A number of important issues have been raised and discussed, e.g., user-level messaging and protection, and coherence strategy for bulk data transferring. Since our interest was mainly on performance related issues, we have concentrated on the latter, imposing a global coherence strategy for prefetching and bulk data transfers.
Reference: [16] <author> M. Heinrich et al. </author> <title> The performance impact of flexibility in the Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 274-285, </pages> <year> 1994. </year>
Reference-contexts: The design of our communication processor is closely related to that of the Flash project <ref> [22, 16] </ref>. Flash is a tightly coupled CC-NUMA system that uses a programmable processor (MAGIC) and software (running on MAGIC) to maintain cache coherence. The MAGIC chip is highly optimized. It includes special hardware to assist message receiving, scheduling, dispatching, and directing outgoing messages to proper destination interface units.
Reference: [17] <author> M. Hill, J. Larus, S. Reinhardt, and D. Wood. </author> <title> Cooperative shared memory: software and hardware for scalable multiprocessors. </title> <booktitle> In Proc. of 5th Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 262-273, </pages> <year> 1992. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level [12, 13] or in incremental fashion <ref> [17] </ref>, can be dictated by the compiler [25, 10], or can be the result of hardware monitoring [8, 29].
Reference: [18] <institution> Intel Corporation. Intel Paragon(tm) Supercomputer Product Brochure. </institution> <note> http://www.ssd.intel.com/paragon.html#system. </note>
Reference-contexts: The use of dedicated communication hardware can also be found in tightly-coupled message-passing systems such as the Intel Paragon <ref> [18] </ref> and has been proposed for networks of workstations [27]. Combining message passing with shared memory to overcome some of the inefficiencies of cache coherence mechanisms was first proposed in the context of the Alewife project [20, 21] and further elaborated in Flash [15].
Reference: [19] <institution> Kendall Square Research Corporation. </institution> <type> KSR1 technical summary, </type> <year> 1992. </year>
Reference-contexts: The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching [23, 6, 14] and poststore <ref> [19] </ref> commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [20] <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B.H. Lim. </author> <title> Integrating message-passing and shared-memory: early experience. </title> <booktitle> In Proceedings of 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <year> 1993. </year>
Reference-contexts: Combining message passing with shared memory to overcome some of the inefficiencies of cache coherence mechanisms was first proposed in the context of the Alewife project <ref> [20, 21] </ref> and further elaborated in Flash [15]. A number of important issues have been raised and discussed, e.g., user-level messaging and protection, and coherence strategy for bulk data transferring.
Reference: [21] <author> J. Kubiatowicz and A. Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of 7th ACM International Conference on Supercomputing, </booktitle> <year> 1993. </year>
Reference-contexts: Combining message passing with shared memory to overcome some of the inefficiencies of cache coherence mechanisms was first proposed in the context of the Alewife project <ref> [20, 21] </ref> and further elaborated in Flash [15]. A number of important issues have been raised and discussed, e.g., user-level messaging and protection, and coherence strategy for bulk data transferring.
Reference: [22] <author> J. Kuskin et al. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <year> 1994. </year>
Reference-contexts: The usage of such primitives can be directed by the user or the compiler [7], or can be detected by some monitoring device. The implementation of these primitives requires some programmable network interface <ref> [22] </ref>. In this paper, we propose a set of communication primitives that will allow the user to take advantage of features common in message-passing systems. <p> The function of the communication processor is to handle network and memory related transactions. The communication processor integrates a processor/cache interface (PI), a network interface (NI), and a programmable protocol processor with instruction and data caches. In contrast with the MAGIC processor in the Flash system <ref> [22] </ref>, which uses hardware to schedule and dispatch messages to the protocol processor, in our model both tasks are performed in software. The motivation for this choice is that a software implementation adds flexibility. <p> The design of our communication processor is closely related to that of the Flash project <ref> [22, 16] </ref>. Flash is a tightly coupled CC-NUMA system that uses a programmable processor (MAGIC) and software (running on MAGIC) to maintain cache coherence. The MAGIC chip is highly optimized. It includes special hardware to assist message receiving, scheduling, dispatching, and directing outgoing messages to proper destination interface units.
Reference: [23] <author> D. Lenoski et al. </author> <title> The Standford DASH multiprocessor. </title> <journal> IEEE Transactions on Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching <ref> [23, 6, 14] </ref> and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [24] <author> P. Pierce. </author> <title> The NX message passing interface. </title> <journal> Parallel Computing, </journal> <volume> 20(4) </volume> <pages> 463-480, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Since our interest was mainly on performance related issues, we have concentrated on the latter, imposing a global coherence strategy for prefetching and bulk data transfers. The communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces <ref> [4, 24] </ref>, prefetching [23, 6, 14] and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms [26]. The common idea is the overlap of communication with computation.
Reference: [25] <author> D. K. Poulsend and P.-C. Yew. </author> <title> Integrating fine-grained message passing in cache coherent shared memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 33(2) </volume> <pages> 172-188, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level [12, 13] or in incremental fashion [17], can be dictated by the compiler <ref> [25, 10] </ref>, or can be the result of hardware monitoring [8, 29].
Reference: [26] <author> U. Ramachandran, G. Shah, A. Sivasubramaniam, A. Singla, and I Yanasak. </author> <title> Architectural mechanisms for explicit communication in shared memory multiprocessors. </title> <booktitle> In Proceedings of Supercomputing '95, </booktitle> <year> 1995. </year>
Reference-contexts: communication primitives that instruct the system to perform efficient data transfers resemble the asynchronous send/receive operations in message passing interfaces [4, 24], prefetching [23, 6, 14] and poststore [19] commands, non-blocking (bulk) read (get) and write (put) operations in the split-phase assignment statement of Split-C [9], and explicit communication mechanisms <ref> [26] </ref>. The common idea is the overlap of communication with computation.
Reference: [27] <author> S. K. Reinhardt, R. W. Pfile, and D. A. Wood. </author> <title> Decoupled hardware support for distributed shared memory. </title> <booktitle> In Proc. of 24th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 34-43, </pages> <year> 1996. </year>
Reference-contexts: The use of dedicated communication hardware can also be found in tightly-coupled message-passing systems such as the Intel Paragon [18] and has been proposed for networks of workstations <ref> [27] </ref>. Combining message passing with shared memory to overcome some of the inefficiencies of cache coherence mechanisms was first proposed in the context of the Alewife project [20, 21] and further elaborated in Flash [15].
Reference: [28] <author> A. Saulsbury, F. Pong, and A. Nowatzyk. </author> <title> Missing the memory wall: The case for processor/memory integration. </title> <booktitle> In Proc. 23rd Int. Symp. on Computer Architecture, </booktitle> <pages> pages 90-101, </pages> <year> 1996. </year>
Reference-contexts: This speed advantage could be negated by the competition for off-chip bandwidth and overall complexity of design. The second case would be more in the philosophy of intelligent memory, i.e., the integration of processor-like functions in DRAMs <ref> [28] </ref>. Another possible study is to see how the communication processor could be shared in cluster-like environments. In a cluster-like architecture, tight-coupling between the communication processor and several compute processors could become too complex. A looser coupling, i.e., another design, might be more appropriate.
Reference: [29] <author> P. Strenstrom, M. Brorsson, and L. Sandberg. </author> <title> An adaptive cache coherence protocol optimized for migratory sharing. </title> <booktitle> In Proc. of 20th Int. Symp. on Computer Architecture, </booktitle> <pages> pages 109-118, </pages> <year> 1993. </year>
Reference-contexts: Tailoring the coherence protocol to the application can be done in several ways. Protocol modifications can be specified by the user at a coarse grain level [12, 13] or in incremental fashion [17], can be dictated by the compiler [25, 10], or can be the result of hardware monitoring <ref> [8, 29] </ref>.
Reference: [30] <author> J. E. Veenstra and R. J. Fowler. MINT: </author> <title> a front end for efficient simulation of shared-memory multiprocessors. </title> <booktitle> In Proceedings of the Second International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems, </booktitle> <pages> pages 201-7, </pages> <year> 1994. </year>
Reference-contexts: The cache line size was set at 32 bytes. 8 4.2 Simulation Parameters We use Mint <ref> [30] </ref> as our simulation tool since our interest is primarily in the performance aspects of the memory system. Mint is a software package that emulates multiprocessing execution environments and generates memory reference events which drive a memory system simulator.
Reference: [31] <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. K. Schauser. </author> <title> Active messages: a mechanism for intergrated communication and computation. </title> <booktitle> In Proceedings of 19th International Symposium on Computer Architecture, </booktitle> <pages> pages 256-66, </pages> <year> 1992. </year>
Reference-contexts: The interrupt handler dispatches the message to an appropriate message handler based on the message type (see below). The message handler either executes the message directly or moves the message from PI or NI to the software message queues, just as in Active Messages <ref> [31] </ref>. However, on a conventional processor, an active message handler still results in too high an overhead for a communication processor maintaining cache coherency at the granularity of a cache line.
Reference: [32] <author> S. C. Woo, M. Ohara, E. Torrie, J. P. Singh, and A. Gupta. </author> <title> The SPLASH-2 programs: Characterization and methodological considerations. </title> <booktitle> In Proceedings of 22nd International Symposium on Computer Architecture, </booktitle> <pages> pages 24-36, </pages> <year> 1995. </year>
Reference-contexts: This is in contrast with cache miss requests that need to be performed to completion. 4 Experimental Methodology 4.1 Applications and Experiments For our experiments, we selected three kernel applications, FFT, LU factorization, and RADIX sort from the SPLASH-2 benchmark suite <ref> [32, 33] </ref>. These applications have been coded with a CC-NUMA system in mind, thus they already have some communication optimizations embedded in them. They also exhibit coarse grain regular communication patterns that can be exploited by the proposed communication primitives.
Reference: [33] <author> S. C. Woo, J. P. Singh, and J. L. Hennessy. </author> <title> The performance advantages of integrating block data transfer in cache-coherent multiprocessors. </title> <booktitle> In Proceedings of 6th International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-VI), </booktitle> <pages> pages 219-229, </pages> <year> 1994. </year> <month> 24 </month>
Reference-contexts: This is in contrast with cache miss requests that need to be performed to completion. 4 Experimental Methodology 4.1 Applications and Experiments For our experiments, we selected three kernel applications, FFT, LU factorization, and RADIX sort from the SPLASH-2 benchmark suite <ref> [32, 33] </ref>. These applications have been coded with a CC-NUMA system in mind, thus they already have some communication optimizations embedded in them. They also exhibit coarse grain regular communication patterns that can be exploited by the proposed communication primitives. <p> lines in the correct state in advance yields execution times for the software optimization comparable, in fact even slightly lower, to those of the hardware implementation. 5.2 LU Factorization The application The SPLASH-2 parallel implementation of the LU factorization of a dense matrix has been optimized to exploit data locality <ref> [33] </ref>. Nonetheless, the serial sections of the application produce a fair amount of load imbalance. The input matrix is divided into submatrices, or blocks, which are assigned to processors in a 2D scatter decomposition fashion.
References-found: 33

