URL: http://www.cs.wisc.edu/~jai/papers/mining.ps
Refering-URL: http://www.cs.wisc.edu/~jai/pubs.html
Root-URL: 
Email: shan@cs.umass.edu  nagendra@cs.umass.edu  agupta@mit.edu  
Title: Temporal Data Mining  
Author: J. Shanmugasundaram M. V. Nagendra Prasad A. Gupta 
Note: Appeared as Profit Working Paper: PROFIT-97-31, "PROFIT" Research Initiative,  
Address: Amherst, MA 01003  Amherst, MA 01003  Cambridge, MA 02139  USA.  
Affiliation: Department of Computer Science University of Massachusetts  Department of Computer Science University of Massachusetts  Sloan School of Management Massachusetts Institute of Technology  Sloan School of Management, Massachusetts Institute of Technology, Cambridge,  
Abstract: Finding patterns in historical data is an important problem in many domains. In this paper, we concentrate on the problem of estimating the future sales of products using past sales data. We use recurrent neural networks as the tool to predict future sales because of (a) its power to generalize trends and (b) its ability to store relevant information about past sales. We first describe the implementation of a distributed recurrent neural network using the real time recurrent learning algorithm. We then describe the validation of this implementation by providing results of tests with well known examples from the literature. The description and analysis of the predictions made on real-world data are then presented. Possible explanations for some of the limitations, based on the predictions of noisy mathematical functions, are also given. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Barto, </author> <title> A.G., "Adaptive Neural Networks for Learning Control: Some Computational Experiments," </title> <booktitle> Proceedings from the IEEE Workshop on Intelligent Control, </booktitle> <year> 1985. </year>
Reference-contexts: This figure is similar for slow moving drugs. We see that the error increases with the value of x. The reason for this is twofold: (a) As the value of x increases, even slight differences in the output are magnified when the scale is adjusted to the <ref> [0; 1] </ref> range (b) The networks do not make high amplitude predictions for this application (reasons will be given in later sections).
Reference: [2] <author> Barto, A.G., Jordan, </author> <title> M.L., "Gradient Following Without Back-Propagation in Layered Networks," </title> <booktitle> Proceedings of the IEEE First Annual International Conference on Neural Networks, </booktitle> <month> June </month> <year> 1987. </year>
Reference-contexts: However, the trends in sales are predicted accurately, in spite of the influence of these exogenous variables. During the course of this project, we also investigated alternatives to the recurrent neural network architecture. Barto et. al. <ref> [2] </ref> use Associative Reward Penalty (ARP) units instead of standard logistic units in a neural network. A modified architecture could use these units in place of logistic units in a neural network and use eligibility traces so that the units remember the past and use it to predict the future.
Reference: [3] <author> Barto, A.G., Sutton, R.S., Anderson, C.W., </author> <title> "Neuronlike Adaptive Elements That Can Solve Difficult Learning Control Problems," </title> <journal> IEEE Transactions on Systems, Man and Cybernetics, </journal> <volume> Vol. SMC-13, No. 5, </volume> <month> Oct. </month> <year> 1983. </year>
Reference: [4] <author> Elman, J.L., </author> <title> "Finding Structure in Time," </title> <journal> Cognitive Science, </journal> <volume> vol. 14, </volume> <pages> pp. 179-211, </pages> <year> 1990. </year>
Reference-contexts: No other input was given because no information about other variables was provided by Medicorp. We used an implicit representation of time <ref> [4] </ref>, i.e., time was not an explicit parameter which was input to the recurrent neural network but was implicitly represented as the training period of the network.
Reference: [5] <author> Hertz, J.A., Krogh, A.S., Palmer, R.G., </author> <title> "Introduction to the Theory of Neural Computation," </title> <booktitle> Santa Fe Institute Studies in the Sciences of Complexity, Lect. Notes vol. I, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: The results in [7] indicate that the predictive performance of recurrent neural networks and time delay networks do not differ greatly. We chose the recurrent neural network architecture because the length of the delay in time-delay networks has to be set in advance <ref> [5] </ref> and because recurrent neural networks are more general than time delay networks.
Reference: [6] <author> McClelland, J.L., Rumelhart, D.E., </author> <title> "Explorations in Parallel and Distributed Processing," </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1988. </year>
Reference: [7] <author> Mozer, </author> <title> M.C., "Neural Net Architectures for Temporal Sequence Processing," Predicting the future and understanding the past (Eds. </title> <editor> A. Weigend and N. Ger-shenfeld), </editor> <publisher> Addison-Wesley, </publisher> <year> 1993. </year>
Reference-contexts: For example, the results of the Santa Fe competition on time series prediction [9] suggest that the performance of neural networks is better than that of other techniques for predicting the future trends in stock prices. A paper authored by Mozer <ref> [7] </ref>, which explains the details of the neural network architectures used in that competition, served as a starting point for the exploration of different neural network architectures. 2 The problem of predicting future sales, as with other time series prediction problems, requires the network to maintain some sort of state 3 <p> Of the neural network architectures with state, we decided to choose from either recurrent neural networks or time-delay neural networks because they seemed to be the most well studied, with a large body of work describing how to set parameters etc. The results in <ref> [7] </ref> indicate that the predictive performance of recurrent neural networks and time delay networks do not differ greatly. <p> This hypothesis is consistent with that presented in <ref> [7] </ref>.
Reference: [8] <author> Rumelhart, D.E., McClelland, J.L., </author> <title> "Parallel Distributed Processing : Explorations in the Microstructure of Cognition," </title> <publisher> MIT Press, </publisher> <address> Cambridge, </address> <year> 1986. </year>
Reference-contexts: specified to filter out high frequency disturbances on the error surface that the network traverses during the learning phase. 4.3 Validation of the Implementation We tested the implementation of the recurrent neural network by making it learn (a) Exclusive Or with Delay [12] and (b) to be a Shift Register <ref> [8] </ref>.
Reference: [9] <author> Weigend, </author> <title> A.S., Gershenfeld, N.A., "Time Series Prediction: Forecasting the Future and Understanding the Past," </title> <booktitle> A Proceedings Volume in the Santa Fe Institute Studies in the Sciences of Complexity, </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1994. </year>
Reference-contexts: After a survey of the time series prediction literature we found that neural networks performed at least as well as other techniques in a majority of cases. For example, the results of the Santa Fe competition on time series prediction <ref> [9] </ref> suggest that the performance of neural networks is better than that of other techniques for predicting the future trends in stock prices.
Reference: [10] <author> Weigend, A.S., Huberman, B.A., Rumelhart, D.E., </author> <title> "Predicting the Future: A Connectionist Approach," </title> <journal> International Journal of Neural Systems, </journal> <volume> vol. 1, </volume> <pages> pp. 193-209, </pages> <year> 1990. </year>
Reference-contexts: The sum of the squared errors of the predicted sales for the first half of 1996 was measured after each training epoch. When this error started rising, it meant that the neural network was starting to over-fit with respect to the training data <ref> [10] </ref>. This technique proved to be very effective for most of the predictions. This was because the minimum sum of the squared errors on the test data set far exceeded the sum of the squared errors on the test data set when the network "converged".
Reference: [11] <author> Williams, R.J, Peng, J., </author> <title> "An Efficient Gradient-Based Algorithm for On-Line Training of Recurrent Network Trajectories," </title> <journal> Neural Computation, </journal> <volume> vol. 2, </volume> <pages> pp. 490-501, </pages> <year> 1990. </year>
Reference-contexts: An advantage of this algorithm is that it can be run on-line without waiting for the sequence to be complete. Since we wanted to have a general implementation which could be used for many time series prediction tasks, we chose to use the Real-Time Recurrent Learning <ref> [12, 11] </ref> (RTRL) algorithm.
Reference: [12] <author> Williams, R.J., Zipser, D., </author> <title> "Experimental Analysis of the Real-time Recurrent Learning Algorithm," </title> <journal> Connection Science, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 87-111, </pages> <year> 1989. </year> <month> 38 </month>
Reference-contexts: An advantage of this algorithm is that it can be run on-line without waiting for the sequence to be complete. Since we wanted to have a general implementation which could be used for many time series prediction tasks, we chose to use the Real-Time Recurrent Learning <ref> [12, 11] </ref> (RTRL) algorithm. <p> be varied and a momentum term can be specified to filter out high frequency disturbances on the error surface that the network traverses during the learning phase. 4.3 Validation of the Implementation We tested the implementation of the recurrent neural network by making it learn (a) Exclusive Or with Delay <ref> [12] </ref> and (b) to be a Shift Register [8]. <p> We also tried training a 5 node recurrent neural network with a 3 time delay Exclusive Or pattern and this function too was effectively learnt. The results which we obtained are consistent with those presented in <ref> [12] </ref>. A recurrent neural network with three nodes was also trained to be a shift register with two time delays between input and output.
References-found: 12

