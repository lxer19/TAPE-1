URL: http://www.iscs.nus.sg/~liuh/tai97.ps
Refering-URL: 
Root-URL: 
Email: fmanoranj,liuh,yaojung@iscs.nus.sg  
Title: Dimensionality Reduction for Unsupervised Data  
Author: M Dash, H Liu, J Yao 
Keyword: Data Preprocessing, Unsupervised Data, Entropy.  
Address: Ridge, Singapore 119260  
Affiliation: Department of Information Systems and Computer Science National University of Singapore Kent  
Abstract: Dimensionality reduction is an important problem for efficient handling of large databases. Many feature selection methods can serve the purpose for supervised data in which each record is attached with a calss label. Little work has been done for dimensionality reduction for unsupervised data in which class information is not available. Principal Component Analysis (PCA) is often used. However, PCA creates new features or principal components which are functions of original features. It is difficult to obtain intuitive understanding of the data using the new features only. In this paper we are concerned with the problem of determining and choosing the important original features for unsupervised data. Our method is based on the observation that removing an irrelevant feature from the feature set may not change the underlying concept of the data, but not so otherwise. We propose an entropy measure for ranking features, and conduct extensive experiments to show that our method is able to find the important features without class informtion by testing the ranked features using different classifiers, and it compares well with a similar method (Relief) that requires class information. This work can be used for dimensionality reduction in many applications in which class labels are not readily known. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. L. Chiu. </author> <title> Method and software for extracting fuzzy classification rules by subtractive clustering. </title> <booktitle> In Proceedings of North American Fuzzy Information Processing Society Conf. </booktitle> <address> (NAFIPS'96), </address> <month> June </month> <year> 1996. </year>
Reference-contexts: The important variables for Iris and Chemical Plant are based on the results of other researchers. For Iris data, Chiu <ref> [1] </ref> and Liu and Setiono [8] conclude that v 3 (petal-length) and v 4 (petal-width) are the most important variables. Both Chemical Plant and Non-linear data sets are taken from Yasukawa and Sugeno's paper [9]. For Chemical Plant data, they conclude that the first 3 variables are important.
Reference: [2] <author> P. A. Devijver and J. Kittler. </author> <title> Pattern Recognition : A Statistical Approach. </title> <publisher> Prentice Hall, </publisher> <year> 1982. </year>
Reference-contexts: For mixed data (i.e., both numeric and nominal variables), we can discretize numeric values first before applying our measure [8]. 3 Algorithm to Find Important Variables We use a Sequential Backward Selection algorithm <ref> [2] </ref> to determine the relative importance of variables for Unsupervised Data (SUD). In the algorithm M is the number of variables originally present, and D is the data set.
Reference: [3] <author> Johan Diedrich Fast. </author> <title> Entropy : the significance of the concept of entropy and its applications in science and technology, chapter 2: The Statistical Significance of the Entropy Concept. </title> <type> Eindhoven : Philips Technical Library, </type> <year> 1962. </year>
Reference-contexts: There is a need to have an entropy measure that evaluates the distinctness among the underlying clusters given a subset of variables. We say the data has orderly configurations if it has distinct clusters, and has disorderly or chaotic configurations otherwise. From entropy theory <ref> [3] </ref>, we know that entropy (or probability) is less for orderly configurations, and more for disorderly configurations for the simple reason that there are few orderly configurations compared to disorderly configurations.
Reference: [4] <author> I. T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer-Verlag, </publisher> <year> 1986. </year>
Reference-contexts: Although original features are selected based on some evaluation function, these methods require class labels. Hence, feature selection methods largely fail for unsupervised data. Feature extraction methods create new features which are uncorrelated and retain as much variation as possible in the database ([15], <ref> [4] </ref>). One commonly known method is principal component analysis (PCA).
Reference: [5] <author> K. Kira and L. A. Rendell. </author> <title> The feature selection problem: Traditional methods and a new algorithm. </title> <booktitle> In Proceedings of Ninth National Conference on AI, </booktitle> <year> 1992. </year>
Reference-contexts: This can be achieved in various ways, for example, by feature selection or feature extraction. There are a number of feature selection methods that determine the relative importance of the features before selecting a subset of important ones. Examples are Relief <ref> [5, 7] </ref>, POE+ACC [10]. A typical feature selection method tries to choose a subset of features from the original set that ideally is necessary and sufficient to describe the target concept [5]. Target concepts are denoted by class labels. <p> Examples are Relief [5, 7], POE+ACC [10]. A typical feature selection method tries to choose a subset of features from the original set that ideally is necessary and sufficient to describe the target concept <ref> [5] </ref>. Target concepts are denoted by class labels. Data with class labels are called supervised data; data without class labels unsupervised. Although original features are selected based on some evaluation function, these methods require class labels. Hence, feature selection methods largely fail for unsupervised data. <p> It also shows that the performance of the proposed algorithm without using class labels is very close to and sometimes better than that of Relief-F [7] (a generalized version of Relief <ref> [5] </ref> for multiple class labels) which ranks the original features using the class labels The paper concludes in Section 5. 1 2 An Entropy Measure for Continuous and Nominal Data Types In this section we introduce an entropy measure for determining the relative importance of variables.
Reference: [6] <author> George J. Klir and Tina A. Folger. </author> <title> Fuzzy Sets, Uncertainty, and Information, chapter Ch. 5: Uncertainty and Information. </title> <publisher> Prentice-Hall International Editions, </publisher> <year> 1988. </year>
Reference-contexts: For two instances, the entropy measure is E = S log 2 S (1 S) log 2 (1 S) which assumes the maximum value of 1.0 for S = 0:5, and the minimum value of 0.0 for S = 0:0 and S = 1:0 <ref> [6] </ref>. For a data set of N instances the entropy measure is given as: E = i=1 j=1 where S ij is the similarity value, normalized to [0,1], between the instances x i and x j .
Reference: [7] <author> I. Kononenko. </author> <title> Estimating attributes : Analysis and extension of RELI EF. </title> <booktitle> In Proceedings of European Conference on Machine Learn ing, </booktitle> <pages> pages 171-182, </pages> <year> 1994. </year>
Reference-contexts: This can be achieved in various ways, for example, by feature selection or feature extraction. There are a number of feature selection methods that determine the relative importance of the features before selecting a subset of important ones. Examples are Relief <ref> [5, 7] </ref>, POE+ACC [10]. A typical feature selection method tries to choose a subset of features from the original set that ideally is necessary and sufficient to describe the target concept [5]. Target concepts are denoted by class labels. <p> The experimental study in Section 4 shows that the proposed algorithm is able to find the important features for databases with known important features. It also shows that the performance of the proposed algorithm without using class labels is very close to and sometimes better than that of Relief-F <ref> [7] </ref> (a generalized version of Relief [5] for multiple class labels) which ranks the original features using the class labels The paper concludes in Section 5. 1 2 An Entropy Measure for Continuous and Nominal Data Types In this section we introduce an entropy measure for determining the relative importance of <p> Method: for part (1) we test SUD over a number of data sets having continuous or nominal variables whose important variables are known. For part (2) we select Relief-F <ref> [7] </ref>, and (a) run SUD (without using class variable) and Relief-F (using class variable) over a total of 15 data sets having class variables (class variables are required to run a classifier) to find variables ordered by their importance, and (b) compare SUD and Relief-F by performing 10-fold cross-validation of C4.5
Reference: [8] <author> H. Liu and R. Setiono. Chi2: </author> <title> Feature selection and discretization of numeric attributes. </title> <booktitle> In Proceedings of the 7th IEEE International Conference on Tools with Artificial Intelligence(TAT'95), </booktitle> <pages> pages 388-391, </pages> <month> November </month> <year> 1995. </year>
Reference-contexts: For mixed data (i.e., both numeric and nominal variables), we can discretize numeric values first before applying our measure <ref> [8] </ref>. 3 Algorithm to Find Important Variables We use a Sequential Backward Selection algorithm [2] to determine the relative importance of variables for Unsupervised Data (SUD). In the algorithm M is the number of variables originally present, and D is the data set. <p> The important variables for Iris and Chemical Plant are based on the results of other researchers. For Iris data, Chiu [1] and Liu and Setiono <ref> [8] </ref> conclude that v 3 (petal-length) and v 4 (petal-width) are the most important variables. Both Chemical Plant and Non-linear data sets are taken from Yasukawa and Sugeno's paper [9]. For Chemical Plant data, they conclude that the first 3 variables are important.
Reference: [9] <author> T. Yasukawa M. Sugeno. </author> <title> A fuzzy-logic-based approach to qualitative modeling. </title> <journal> In IEEE Transactions on Fuzzy system Vol.1, </journal> <volume> No.1, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: For Iris data, Chiu [1] and Liu and Setiono [8] conclude that v 3 (petal-length) and v 4 (petal-width) are the most important variables. Both Chemical Plant and Non-linear data sets are taken from Yasukawa and Sugeno's paper <ref> [9] </ref>. For Chemical Plant data, they conclude that the first 3 variables are important.
Reference: [10] <author> A. N. Mucciardi and E. E. Gose. </author> <title> A comparison of seven techniques for choosing subsets of pattern recognition. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-20:1023-1031, </volume> <month> September </month> <year> 1971. </year>
Reference-contexts: This can be achieved in various ways, for example, by feature selection or feature extraction. There are a number of feature selection methods that determine the relative importance of the features before selecting a subset of important ones. Examples are Relief [5, 7], POE+ACC <ref> [10] </ref>. A typical feature selection method tries to choose a subset of features from the original set that ideally is necessary and sufficient to describe the target concept [5]. Target concepts are denoted by class labels. Data with class labels are called supervised data; data without class labels unsupervised.
Reference: [11] <author> Sreerama K. Murthy, Simon Kasif, and Steven Salzberg. </author> <title> A system for induction of oblique decision trees. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2 </volume> <pages> 1-32, </pages> <year> 1994. </year>
Reference-contexts: run SUD (without using class variable) and Relief-F (using class variable) over a total of 15 data sets having class variables (class variables are required to run a classifier) to find variables ordered by their importance, and (b) compare SUD and Relief-F by performing 10-fold cross-validation of C4.5 and OC1 <ref> [11] </ref> using the subset consisting of the d most important variables, d varies from 1 to M . Data Sets: a summary of 17 data sets is given in Table 1.
Reference: [12] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <booktitle> In Machine Learning, </booktitle> <pages> pages 81-106. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: This measure is applicable to both nominal and continuous data types, and does not need class information to evaluate the variables unlike some other entropy measures, for example, information gain measure in ID3 <ref> [12] </ref>. (a) v1-v3-v4 dimension (b) v3-v4-v1 dimension (c) v1-v3 dimension (d) v3-v4 dimension 2.1 Physical Significance Consider an instance in a dataset, which is described by a set of variables.
Reference: [13] <author> J.R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The automated selection of d is, 4 Vote Zoo however, more complicated. We investigated this issue through experiments on a number of data sets with class labels. We ran SUD by removing class labels, and determined the importance levels of all variables. Then we performed 10-fold cross-validation of C4.5 <ref> [13] </ref> using the subset consisting of the d most important variables, d varies from 1 to M , to obtain average error rates. Two results are shown in Figure 2 and more in Figure 3.
Reference: [14] <author> R. Uthurusamy. </author> <title> From data mining to knoweldge discovery: Current challenges and future directions. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining, </booktitle> <pages> pages 561-569. </pages> <publisher> AAAI Press / The MIT Press, </publisher> <year> 1996. </year>
Reference-contexts: Data is normally collected for organizational or book-keeping purposes, and not particaularly for data mining. For example, transactional data does not contain any class information, and it could have a huge number of features. It is often desirable to preprocess the data before applying a knowledge discovery tool <ref> [14] </ref>. Dimensionality reduction without creating any new feature offers an effective solution to decreasing the data size. PCA or feature selection methods cannot help much for this type of dimensionality reduction due to the reasons given above.
Reference: [15] <author> N. Wyse, R. Dubes, and A.K. Jain. </author> <title> A critical evaluation of intrinsic dimensionality algorithms. In E.S. </title> <editor> Gelsema and Kanal L.N., editors, </editor> <booktitle> Pattern Recognition in Practice, </booktitle> <pages> pages 415-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <year> 1980. </year> <month> 10 </month>
References-found: 15

