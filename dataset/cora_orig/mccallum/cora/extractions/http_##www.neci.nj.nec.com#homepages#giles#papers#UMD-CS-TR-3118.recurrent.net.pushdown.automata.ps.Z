URL: http://www.neci.nj.nec.com/homepages/giles/papers/UMD-CS-TR-3118.recurrent.net.pushdown.automata.ps.Z
Refering-URL: http://www.neci.nj.nec.com/homepages/giles/papers/
Root-URL: http://www.neci.nj.nec.com
Email: and  
Phone: 3  
Title: The Neural Network Pushdown Automaton: Model, Stack and Learning Simulations deterministic Palindrome the extracted PDA
Author: G.Z. Sun a,b C.L. Giles b,c H.H. Chen a,b and Y.C. Lee a,b 
Note: language, 1 n 0 n and the  PDAs can be shown to be identical or equivalent to the PDAs of the source grammars which were used to generate the training strings.  
Address: College Park, MD 20742  4 Independence Way, Princeton, NJ 08540  
Date: August 20, 1993 Revised January, 1995  
Affiliation: UNIVERSITY OF MARYLAND TR  a Laboratory For Plasma Research, b Institute for Advanced Computer Studies University of Maryland,  NEC Research Institute  
Pubnum: NOs. UMIACS-TR-93-77 CS-TR-3118  
Abstract: In order for neural networks to learn complex languages or grammars, they must have sufficient computational power or resources to recognize or generate such languages. Though many approaches to effectively utilizing the computational power of neural networks have been discussed, an obvious one is to couple a recurrent neural network with an external stack memory - in effect creating a neural network pushdown automata (NNPDA). This NNPDA generalizes the concept of a recurrent network so that the network becomes a more complex computing structure. This paper discusses in detail a NNPDA - its construction, how it can be trained and how useful symbolic information can be extracted from the trained network. To effectively couple the external stack to the neural network, an optimization method is developed which uses an error function that connects the learning of the state automaton of the neural network to the learning of the operation of the external stack: push, pop, and no-operation. To minimize the error function using gradient descent learning, an analog stack is designed such that the action and storage of information in the stack are continuous. One interpretation of a continuous stack is the probabilistic storage of and action on data. After training on sample strings of an unknown source grammar, a quantization procedure extracts from the analog stack and neural network a discrete pushdown automata (PDA). Simulations show that in learning deterministic context-free grammars - the balanced parenthesis 
Abstract-found: 1
Intro-found: 1
References-found: 0

