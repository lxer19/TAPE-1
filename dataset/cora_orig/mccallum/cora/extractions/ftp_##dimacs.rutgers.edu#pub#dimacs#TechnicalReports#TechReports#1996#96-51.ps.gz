URL: ftp://dimacs.rutgers.edu/pub/dimacs/TechnicalReports/TechReports/1996/96-51.ps.gz
Refering-URL: http://paul.rutgers.edu/~loewenst/cdna.html
Root-URL: 
Email: Email: loewenst@paul.rutgers.edu  Email: pny@research.nj.nec.com  
Phone: 08540,  
Title: Lower Entropy Estimates for Natural DNA Sequences  
Author: by David M. Loewenstern Peter N. Yianilos 
Address: New Brunswick, New Jersey 08903  4 Independence Way, Princeton, NJ  Princeton, New Jersey 08544  
Affiliation: Department of Computer Science Rutgers University  NEC Research Institute,  and, Department of Computer Science, Princeton University,  
Note: Significantly  DIMACS is a partnership of Rutgers University, Princeton University, AT&T Research, Bellcore, and Bell Laboratories. DIMACS is an NSF Science and Technology Center, funded under contract STC-91-19999; and also receives support from the New Jersey Commission on Science and Technology.  
Abstract: DIMACS Technical Report 96-51 December 1996 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum, </author> <title> An inequality and associated maximization technique in statistical esti-matation of probabilistic functions of Markov processes, Inequalities, </title> <booktitle> 3 (1972), </booktitle> <pages> pp. 1-8. </pages>
Reference-contexts: Other parameters include our selection of window sizes to consider, and several explained below having to do with the learning algorithm. The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [2, 3, 1, 20] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [8, 21] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [2] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: Other parameters include our selection of window sizes to consider, and several explained below having to do with the learning algorithm. The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [2, 3, 1, 20] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [8, 21] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [3] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss, </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains, </title> <journal> Ann. Math Stat., </journal> <volume> 41 (1970), </volume> <pages> pp. 164-171. </pages>
Reference-contexts: Other parameters include our selection of window sizes to consider, and several explained below having to do with the learning algorithm. The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [2, 3, 1, 20] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [8, 21] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [4] <author> T. C. Bell, J. G. Cleary, and I. H. Witten, </author> <title> Text Compression, </title> <publisher> Prentice Hall, </publisher> <year> 1990. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [7, 4] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> 1:90 bit result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding (See their original paper [23] and the discussion of the considerable work that followed in <ref> [4] </ref>) may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length. <p> This problem has led to the development of variable length context language models <ref> [4] </ref> which simply stated, use long contexts when enough earlier observations exists, and otherwise use shorter ones. <p> In [18] conventional fixed multigram entropy estimates are reported for several sequences. Grum-bach and Tahi in [12] exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see <ref> [4] </ref>). The results of this paper substantially improve on Grumbach and Tahi's results for many sequences. <p> For example, Shannon's work estimated the entropy of English text to be roughly 1:3 bits per character (see discussion in <ref> [4] </ref>). Random text over a 27 letter alphabet (A-Z and space) corresponds to 4:8 bits. Today's statistical models, with no specific knowledge of English syntax or semantics, can achieve roughly 2 bits.
Reference: [5] <author> L. Cardon and G. Stormo, </author> <title> Expectation maximization algorithm for identifying protein-binding sites with variable lengths from unaligned DNA fragments, </title> <journal> JMB, </journal> <volume> 223 (1992), </volume> <pages> pp. 159-170. </pages>
Reference-contexts: That is, we model so that we can better understand. Practically, statistical modeling of DNA has proven to be somewhat effective in several problem areas, including the automatic identification of coding regions <ref> [16, 6, 5, 10, 15] </ref>, and in classifying unknown sequences into one of a discrete number of classes. The success of such methods ultimately rests on the strength of the models they employ.
Reference: [6] <author> C. Cosmi, V. Cuomo, M. Ragosta, and M. Macchiato, </author> <title> Characterization of nu-cleotidic sequences using maximum entropy techniques, </title> <journal> J. Theor. Biol, </journal> <year> (1990), </year> <pages> pp. 423-432. - 27 </pages> - 
Reference-contexts: That is, we model so that we can better understand. Practically, statistical modeling of DNA has proven to be somewhat effective in several problem areas, including the automatic identification of coding regions <ref> [16, 6, 5, 10, 15] </ref>, and in classifying unknown sequences into one of a discrete number of classes. The success of such methods ultimately rests on the strength of the models they employ.
Reference: [7] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: Assuming each character (nucleotide) is drawn uniformly at random from the alphabet, and that all positions in the string are independent, we know from elementary information theory <ref> [7, 4] </ref> that an optimal code will devote 2 bits to representing each character. This is the maximum entropy case. <p> The entropy rate of a stochastic process fX t g is defined as: lim 1 H (X 1 ; X 2 ; : : : ; X t ) (1) where this limit need not in general exist, and H denotes the information theoretic entropy function (see <ref> [7] </ref>.) Given full knowledge of the process, and the limit's existence, the entropy rate is a well-defined attribute of the process. But we know very little of the process underlying the generation of natural DNA, and can merely observe the outcome, i.e. the nucleotide sequence.
Reference: [8] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. </pages>
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [2, 3, 1, 20], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [8, 21] </ref> of essentially the same algorithm and underlying information theoretic inequality. We denote Pr (w = i) by w i , and Pr (h = ijf = j; w = k) by h ijj;k . Before learning begins they are initialized in our implementation to the flat (uniform) distribution.
Reference: [9] <author> B. Dujon, D. Alexandraki, B. Andre, W. Ansorge, and V. B. et al., </author> <title> Complete DNA sequence of yeast chromosome XI, </title> <booktitle> Nature, </booktitle> <month> 369 </month> <year> (1994). </year>
Reference-contexts: In this case of panmtpacga, nearly all of the redundancy is explained by the unigraph statistics H (1) = 1:88. The observed relationship between %(C + G) and gene density <ref> [9] </ref> in yeast is reflected in the discrepancy in H (1) between the coding and non-coding region of Yeast chromosome III. Observe that the H (6) estimate is rarely better than H (4), and in some cases is markedly worse (a consequence of limited sequence length).
Reference: [10] <author> M. Farach, M. Noordewier, S. Savari, L. Shepp, A. Wyner, and J. Ziv, </author> <title> On the entropy of DNA: Algorithms and measurements based on memory and rapid convergence, </title> <booktitle> in Proceedings of the Sixth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1994. </year>
Reference-contexts: That is, we model so that we can better understand. Practically, statistical modeling of DNA has proven to be somewhat effective in several problem areas, including the automatic identification of coding regions <ref> [16, 6, 5, 10, 15] </ref>, and in classifying unknown sequences into one of a discrete number of classes. The success of such methods ultimately rests on the strength of the models they employ. <p> In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [18, 22, 10] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [11] <author> L. L. Gatlin, </author> <title> Information Theory and the Living System, </title> <publisher> Columbia University Press, </publisher> <address> New York, </address> <year> 1972. </year>
Reference: [12] <author> S. Grumbach and F. Tahi, </author> <title> A new challenge for compression algorithms: genetic sequences, </title> <booktitle> Information Processing & Management, 30 (1994), </booktitle> <pages> pp. 875-886. </pages>
Reference-contexts: The most effective metric looks not only for matches in the forward direction, but also for matches that are reversed and complemented. By "complemented" we mean that A; G are interchanged along with C; T . This technique was introduced by Grumbach and Tahi <ref> [12] </ref> and in almost all cases slightly improves the rate of compression. <p> In recent years there has been increasing interest in entropy estimates for DNA. In [18] conventional fixed multigram entropy estimates are reported for several sequences. Grum-bach and Tahi in <ref> [12] </ref> exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see [4]). The results of this paper substantially improve on Grumbach and Tahi's results for many sequences. <p> The resulting sequence contains 484; 483 bases and is referred to as our nonredundant data set. 5 Experimental Results Our model's performance on the sequences described in section 4 is summarized in table 2. In some cases our results may be compared directly with estimates from <ref> [12] </ref>, which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of [18] and are in agreement. We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). <p> We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). The H (1); H (4); H (6) columns contain conventional multigram entropy estimates. The cdna column reports our model's cross-validation entropy estimates. Compressive estimates from the biocompress-2 program of <ref> [12] </ref> are contained in the following column. Our model's - 13 - Algorithm Unix biocom- cdna sequence length compress H (1) H (4) H (6) cdna press-2 [12] compress Mammals 14 Sequences 1029151 2.19 1.98 1.91 1.91 1.67 Coding only 46457 2.19 1.99 1.94 1.94 1.80 Noncoding only 982694 2.11 1.98 <p> The cdna column reports our model's cross-validation entropy estimates. Compressive estimates from the biocompress-2 program of <ref> [12] </ref> are contained in the following column. Our model's - 13 - Algorithm Unix biocom- cdna sequence length compress H (1) H (4) H (6) cdna press-2 [12] compress Mammals 14 Sequences 1029151 2.19 1.98 1.91 1.91 1.67 Coding only 46457 2.19 1.99 1.94 1.94 1.80 Noncoding only 982694 2.11 1.98 1.92 1.90 1.74 humdystrop 38770 2.23 1.95 1.91 1.95 1.91 1.93 1.93 humghcsa 66495 2.19 2.00 1.92 1.86 0.54 1.31 0.95 humhbb 73308 2.20 1.97 1.91 1.92
Reference: [13] <author> H. Herzel, </author> <title> Complexity of symbol sequences, </title> <journal> Syst. Anal. Modl. Simul., </journal> <volume> 5 (1988), </volume> <pages> pp. 435-444. </pages>
Reference-contexts: Our formalization of this idea in section 3 is one of the main contributions of this paper. That natural DNA includes near repeats is well known, and in <ref> [13] </ref> the statistics of their occurrence are discussed. We measure nearness using ordinary Hamming distance, i.e. the number of positions at which two equal length strings disagree.
Reference: [14] <author> H. Herzel, W. Ebeling, and A. Schmitt, </author> <title> Entropies of biosequences: The role of repeats, </title> <journal> Physical Review E, </journal> <volume> 50 (1994), </volume> <pages> pp. 5061-5071. </pages>
Reference-contexts: It should be noted that H (1) entropy estimates include the known effect of %(C + G)[11] on entropy estimation, and biocompress-2 includes the also known effect of long exact repeats and exact complement repeats <ref> [14] </ref>. cdna's generally superior performance indicates that DNA possesses more structure which may be exploited. Several notions of distance were evaluated and the best performance resulted from considering both Hamming distance to reversed and complemented targets, as well as standard Hamming distance, then selecting the minimum.
Reference: [15] <author> A. Krogh, I. Mian, and D. Haussler, </author> <title> A hidden Markov model that finds genes in Escheria Coli DNA, </title> <journal> Nucleic Acids Research supplement, </journal> <volume> 22 (1994), </volume> <pages> pp. 4768-4778. </pages>
Reference-contexts: That is, we model so that we can better understand. Practically, statistical modeling of DNA has proven to be somewhat effective in several problem areas, including the automatic identification of coding regions <ref> [16, 6, 5, 10, 15] </ref>, and in classifying unknown sequences into one of a discrete number of classes. The success of such methods ultimately rests on the strength of the models they employ.
Reference: [16] <author> G. Lauc, I. Ili c, and H. Heffer-Lauc, </author> <title> Entropies of coding and noncoding sequences of DNA and proteins, </title> <journal> Biophysical Chemistry, </journal> <year> (1992), </year> <pages> pp. 7-11. </pages>
Reference-contexts: That is, we model so that we can better understand. Practically, statistical modeling of DNA has proven to be somewhat effective in several problem areas, including the automatic identification of coding regions <ref> [16, 6, 5, 10, 15] </ref>, and in classifying unknown sequences into one of a discrete number of classes. The success of such methods ultimately rests on the strength of the models they employ.
Reference: [17] <author> D. Loewenstern, H. Hirsh, P. N. Yianilos, and M. Noordewier, </author> <title> DNA sequence classification using compression-based induction, </title> <type> Tech. Rep. TR 95-087, </type> <institution> DI-MACS, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: The success of such methods ultimately rests on the strength of the models they employ. In our own earlier work <ref> [17] </ref> we were encouraged by experiments that suggested that models based on the recent past could provide a somewhat effective solution to several classification problems. Further work involving incremental improvements to the model and general scheme failed to yield much better performance.
Reference: [18] <author> R. Mantegna, S. Buldyrev, A. Goldberger, S. Havlin, C.-K. Peng, M. Si-mons, and H. Stanley, </author> <title> Linguistic features of noncoding DNA sequences, </title> <journal> Physical Review Letters, </journal> <volume> 73 (1993), </volume> <pages> pp. 3169-3172. </pages>
Reference-contexts: A logical next step taken by several investigators, focuses instead on higher order entropy estimates arising from measurements of the frequencies of longer sequences. For natural languages (e.g. English) this step typically leads to significantly lower entropy estimates. But the best resulting estimate for humretblas is roughly 1:90 bits <ref> [18] </ref>, still not impressively different from our 2-bit random starting point. This may be something of a surprise, since such models reflect such known DNA structure as %(C + G) composition and CG suppression. But these known effects have little impact on the entropy of DNA sequences as a whole. <p> Section 6 concludes with a discussion that suggests why the high entropy levels observed for coding regions may not be so surprising after all. In recent years there has been increasing interest in entropy estimates for DNA. In <ref> [18] </ref> conventional fixed multigram entropy estimates are reported for several sequences. Grum-bach and Tahi in [12] exploit reverse-complements and long matches to achieve improved compression rates. The method is one of pointer substitution and resembles LZ78 (see [4]). <p> In some cases our results may be compared directly with estimates from [12], which are included in the table. Our values for H (4) (the 4-symbol entropy) may be compared with the redundancy estimates of <ref> [18] </ref> and are in agreement. We have grouped our results by general type (i.e. mammalian, prokaryote, etc.). The H (1); H (4); H (6) columns contain conventional multigram entropy estimates. The cdna column reports our model's cross-validation entropy estimates. <p> In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [18, 22, 10] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [19] <author> M. Noordewier, </author> <month> April </month> <year> 1996. </year> <title> Private Communication. </title>
Reference-contexts: To gather a larger body of coding regions, we obtained a data set of 490 complete human genes. This data set was screened to remove any partial genes, pseudogenes, mutants, copies, or variants of the same gene <ref> [19] </ref>. The resulting sequence contains 484; 483 bases and is referred to as our nonredundant data set. 5 Experimental Results Our model's performance on the sequences described in section 4 is summarized in table 2.
Reference: [20] <author> A. B. Poritz, </author> <title> Hidden Markov models: a guided tour, </title> <booktitle> in Proc. </booktitle> <address> ICASSP-88, </address> <year> 1988, </year> <pages> pp. 7-13. </pages>
Reference-contexts: Other parameters include our selection of window sizes to consider, and several explained below having to do with the learning algorithm. The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models <ref> [2, 3, 1, 20] </ref>, and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery [8, 21] of essentially the same algorithm and underlying information theoretic inequality.
Reference: [21] <author> R. A. Redner and H. F. Walker, </author> <title> Mixture densities, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review, </journal> <volume> 26 (1984), </volume> <pages> pp. 195-239. - 28 </pages> - 
Reference-contexts: The learning task before us is to estimate these parameters by examining the training set. Our algorithm is an application of the Baum-Welch algorithm for Hidden Markov Models [2, 3, 1, 20], and may also be viewed as an instance of Expectation Maximization (EM), a later rediscovery <ref> [8, 21] </ref> of essentially the same algorithm and underlying information theoretic inequality. We denote Pr (w = i) by w i , and Pr (h = ijf = j; w = k) by h ijj;k . Before learning begins they are initialized in our implementation to the flat (uniform) distribution.
Reference: [22] <author> P. Salamon and A. K. Konopka, </author> <title> A maximum entropy principle for the distribution of local complexity in naturally occurring nucleotide sequences, </title> <journal> Computers Chem., </journal> <volume> 16 (1992), </volume> <pages> pp. 117-124. </pages>
Reference-contexts: In fact, it has been observed by several authors that coding regions are less compressible than non-coding regions (e.g., <ref> [18, 22, 10] </ref>). So it is clear that two sequences that code for the same polypeptide may nevertheless have large Hamming distance.
Reference: [23] <author> J. Ziv and A. Lempel, </author> <title> A universal algorithm for sequential data compression, </title> <journal> IEEE Transactions on Information Theory, </journal> <month> IT-23 </month> <year> (1977). </year>
Reference-contexts: The 1:90 bit result is then even more surprising since it implies that knowledge of the immediate past reduces the entropy estimate by a mere 0:05 bits. 1 Data compression techniques such as Lempel-Ziv (LZ) coding (See their original paper <ref> [23] </ref> and the discussion of the considerable work that followed in [4]) may be viewed as entropy estimators, with LZ corresponding to a model that predicts based on a historical context of variable length.
References-found: 23

