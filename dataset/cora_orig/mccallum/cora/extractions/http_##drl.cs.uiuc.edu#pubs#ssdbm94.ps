URL: http://drl.cs.uiuc.edu/pubs/ssdbm94.ps
Refering-URL: http://drl.cs.uiuc.edu/pubs/ssdbm94.html
Root-URL: http://www.cs.uiuc.edu
Email: fseamons,winslettg@cs.uiuc.edu  
Title: Physical Schemas for Large Multidimensional Arrays in Scientific Computing Applications  
Author: Kent E. Seamons and Marianne Winslett 
Address: Urbana, Illinois 61801  
Affiliation: Computer Science Department University of Illinois  
Abstract: We describe physical schemas for storing multidimensional arrays on disk. We have developed an i/o library supporting these schemas that provides an abstract interface shielding scientific application developers from physical storage details. Our library has resulted in simplified programming and improved i/o performance in the applications we have studied. 
Abstract-found: 1
Intro-found: 1
Reference: [Bell87] <author> J. L. Bell, G. S. Patterson, Jr., </author> <title> Data Organization in Large Numerical Computations, </title> <journal> The Journal of Supercomputing, </journal> <volume> Volume 1, Number 1, </volume> <year> 1987. </year>
Reference-contexts: 1 Motivation Scientific applications often center around computation on large arrays <ref> [Bell87] </ref>. Some of the goals in traditional database systems research are relevant to the needs for persistent array storage in scientific computing applications. Traditional database systems focus on providing persistent storage that is efficient and easy-to-use.
Reference: [Bordawekar93] <author> Rajesh Bordawekar, Juan Miguel del Rosario, and Alok Choudary, </author> <title> Design and Evaluation of Primitives for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 452-461, </pages> <year> 1993. </year>
Reference-contexts: J. Wat-son Research Center [Corbett93]. It provides a user-definable view of parallel files that are applicable to array data. In our work, we utilize a higher-level semantic interface than they provide, and expect our library will be layered above file systems like Vesta. <ref> [Bordawekar93] </ref> describes run-time primitives to support a two-phase access strategy for conducting parallel i/o. Such a facility is useful, although it is not needed so far in the write-intensive applications we studied. [Galbreath93] reports on experiences with parallel applications at Argonne National Laboratory.
Reference: [Brezany92] <author> Peter Brezany, Michael Gerndt, Piyush Mehrotra, Hans Zima, </author> <title> Concurrent File Operations in a High Performance FORTRAN, </title> <booktitle> Proceedings of Supercomputing '92, </booktitle> <pages> pages 230-238, </pages> <year> 1992. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decom-position across the processors that is appropriate for the computation.
Reference: [DeWitt92] <author> David DeWitt, Jeffrey Naughton, Dono-van Schneider, S. Seshadri, </author> <title> Practical Skew Handling in Parallel Joins, </title> <booktitle> Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: HDF will very soon provide support for chunked physical schemas, with our experiments to guide the development. Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins <ref> [DeWitt92] </ref> and range searching [Li92]. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase [Interbase90], Orion [Xidak91], and Stratum [Stratum89].
Reference: [Corbett93] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, Sandra Johnson Baylor, </author> <title> Parallel Access to Files in the Vesta File System, </title> <booktitle> Proceedings of Supercomputing '93, </booktitle> <pages> pages 472-481, </pages> <year> 1993. </year>
Reference-contexts: The Vesta Parallel File System is designed to run on the Vulcan multicomputer at the IBM T. J. Wat-son Research Center <ref> [Corbett93] </ref>. It provides a user-definable view of parallel files that are applicable to array data.
Reference: [Crockett89] <author> Thomas W. Crockett, </author> <title> File Concepts for Parallel I/O, </title> <booktitle> Proceedings of Supercomputing '89, </booktitle> <pages> pages 574-579, </pages> <year> 1989. </year>
Reference: [Fox90] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kremer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Tech Report TR 90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> Dec. </month> <year> 1990. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decom-position across the processors that is appropriate for the computation.
Reference: [French91] <author> James C. French, Terrence W. Pratt, and Mriganka Das, </author> <title> Performance Measurement of a Parallel Input/Output System for the Intel iPSC/2 Hypercube, </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <address> San Diego, CA, </address> <month> May 21-24, </month> <year> 1991, </year> <pages> pages 178-187. </pages>
Reference: [Globus93] <author> Al Globus, </author> <title> C++ Class Library Data Management for Scientific Visualization, </title> <type> Technical Report RNR-93-006, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, March 3, </address> <year> 1993. </year>
Reference-contexts: Under the first approach, the entire array from each time step is read into memory. The second approach uses memory mapped files and the virtual memory page faulting features of the operating system to read into memory only those pages that the application actually needs <ref> [Globus93] </ref>. This technique is similar to the approach supported by object-oriented database systems that utilize the capabilities of virtual memory to assist in accessing persistent data [Lamb91]. We have constructed libraries that support both types of i/o.
Reference: [Galbreath93] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> Applications-Driven Parallel I/O, </title> <booktitle> Proceeding of Supercomputing '93, </booktitle> <pages> pages 462-471, </pages> <year> 1993. </year>
Reference-contexts: This is a common approach for implementing checkpoints <ref> [Galbreath93] </ref>. The advantages of this approach are the simple coding that it requires, and its reasonable performance. The first disadvantage of this approach is the large numbers of files that are created for checkpoints when many processors and/or arrays are involved in the computation. <p> Such a facility is useful, although it is not needed so far in the write-intensive applications we studied. <ref> [Galbreath93] </ref> reports on experiences with parallel applications at Argonne National Laboratory. Like ours, and in contrast to many of the efforts discussed above, their work emphasizes the value of abstractions. The interfaces we propose are at a more abstract level than those of [Galbreath93]. <p> far in the write-intensive applications we studied. <ref> [Galbreath93] </ref> reports on experiences with parallel applications at Argonne National Laboratory. Like ours, and in contrast to many of the efforts discussed above, their work emphasizes the value of abstractions. The interfaces we propose are at a more abstract level than those of [Galbreath93]. We decouple the notions of file and array and do not require an array to be stored in a single file. We also emphasize the need to make files self-describing. 5 Conclusions and future work We have introduced physical schemas for storing multidimensional arrays on secondary storage.
Reference: [Hiranandani92] <author> S. Hiranandani, K. Kennedy, and C. Tseng, </author> <title> Compiling Fortran D for MIMD Distributed-Memory Machines, </title> <journal> Communications of the ACM, </journal> <volume> Volume 35, Number 8, </volume> <month> August </month> <year> 1992. </year>
Reference-contexts: Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decom-position across the processors that is appropriate for the computation.
Reference: [HDF94] <institution> NCSA HDF Reference Manual, </institution> <note> Version 3.3, </note> <institution> National Center for Supercomputing Applications, University of Illinois, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using a traditional order, whether using file format systems <ref> [HDF94, NetCDF91] </ref>, database management systems [Interbase90, Stratum89, Xidak91], or hand-coded persistence. Traditional array ordering on disk is ideal for applications that need to access an entire persistent array that fits in main memory, a single array element, or contiguous elements along the fastest varying dimension. <p> The most useful physical schemas that we identify are being incorporated into the HDF file format system <ref> [HDF94] </ref> developed by our colleagues at the University of Illinois. In the longer run, our approaches may find their way into other data management packages and DBMSes as well. <p> Until fairly recently, scientists had only flat files for storing data. Now several file format systems in widespread use support storing multidimensional array data. Hierarchical Data Format (HDF) <ref> [HDF94] </ref> is a self-describing, machine independent file format system developed at NCSA that supports storing multiple objects in a file. HDF was designated by NASA as the preferred file format for EOSDIS version 0.
Reference: [HPF93] <author> High Performance Fortran Forum, </author> <title> High Performance Fortran Language Specification Version 1.0, </title> <type> Technical Report CRPC-TR92225, CRPC, </type> <institution> Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: In the longer run, our approaches may find their way into other data management packages and DBMSes as well. We also expect that writers of compilers, for languages such as High Performance Fortran (HPF) <ref> [HPF93] </ref>, for massively parallel machines will find our library routines useful, as will any effort to develop an automated `out of core' compiler. 2 Physical schemas for multidimen sional arrays To address the problems faced by applications that do not perform well with traditionally ordered arrays on disk, we have been <p> Current research efforts in compiler technology for massively parallel machines are addressing the issue of distributing large multidimensional arrays across multiple processors. In High Performance Fortran and related languages <ref> [HPF93, Fox90, Hiranandani92, Brezany92] </ref>, the user can provide hints to the compiler, with the ALIGN and DISTRIBUTE commands, that are used to create a data layout and problem decom-position across the processors that is appropriate for the computation.
Reference: [Interbase90] <institution> Interbase Data Definition Guide, Inter-base Software Corporation, </institution> <year> 1990. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using a traditional order, whether using file format systems [HDF94, NetCDF91], database management systems <ref> [Interbase90, Stratum89, Xidak91] </ref>, or hand-coded persistence. Traditional array ordering on disk is ideal for applications that need to access an entire persistent array that fits in main memory, a single array element, or contiguous elements along the fastest varying dimension. <p> Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase <ref> [Interbase90] </ref>, Orion [Xidak91], and Stratum [Stratum89]. Like the file format systems described earlier, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
Reference: [Lamb91] <author> C. Lamb, G. Landis, J. Orenstein, and D. Weinreb, </author> <title> The ObjectStore Database System, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 34, No. 10, </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: This technique is similar to the approach supported by object-oriented database systems that utilize the capabilities of virtual memory to assist in accessing persistent data <ref> [Lamb91] </ref>. We have constructed libraries that support both types of i/o. In the first case, an entire chunked array can be read from disk to a traditionally ordered array in memory.
Reference: [Li92] <author> Jianzhong Li, Jaideep Srivastava, Doron Rotem, CMD: </author> <title> A Multidimensional Declus-tering Method for Parallel Database Systems, </title> <booktitle> Proceedings of the 18th VLDB Conference, </booktitle> <year> 1992. </year>
Reference-contexts: HDF will very soon provide support for chunked physical schemas, with our experiments to guide the development. Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins [DeWitt92] and range searching <ref> [Li92] </ref>. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase [Interbase90], Orion [Xidak91], and Stratum [Stratum89].
Reference: [Maier93] <author> David Maier and Bennet Vance, </author> <title> A Call to Order, </title> <booktitle> Proceedings of the Twelfth ACM Symposium on Principles of Database Systems, </booktitle> <address> Washington D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Although the needs of scientific applications coincide with those of traditional database systems in this regard, today's commercial database management systems are not immediately applicable to scientific computing with large arrays, because they lack many characteristics and facilities needed in a computation-intensive environment <ref> [Maier93] </ref>. <p> Database researchers have studied the use of parallelism in the context of traditional DBMS operations, such as joins [DeWitt92] and range searching [Li92]. Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk <ref> [Maier93] </ref>. A few commercial DBMSes support multidimensional arrays, including Interbase [Interbase90], Orion [Xidak91], and Stratum [Stratum89]. Like the file format systems described earlier, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
Reference: [NetCDF91] <author> NetCDF User's Guide, </author> <note> Version 2.0, </note> <institution> Unidata Program Center, University Corporation for Atmospheric Research, </institution> <month> October </month> <year> 1991. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using a traditional order, whether using file format systems <ref> [HDF94, NetCDF91] </ref>, database management systems [Interbase90, Stratum89, Xidak91], or hand-coded persistence. Traditional array ordering on disk is ideal for applications that need to access an entire persistent array that fits in main memory, a single array element, or contiguous elements along the fastest varying dimension. <p> Hierarchical Data Format (HDF) [HDF94] is a self-describing, machine independent file format system developed at NCSA that supports storing multiple objects in a file. HDF was designated by NASA as the preferred file format for EOSDIS version 0. NetCDF <ref> [NetCDF91] </ref> is an i/o library that is also used to store scientific information in self-describing, machine independent files. HDF was recently enhanced to support the NetCDF interface. Both HDF and NetCDF organize fixed size arrays using traditional array ordering on disk. They both support unlimited sized arrays along one dimension.
Reference: [Nitzberg92] <author> Bill Nitzberg, </author> <title> Performance of the iPSC/860 Concurrent File System, </title> <type> Technical Report RND-92-020, </type> <institution> NASA Ames Research Center, Moffett Field, </institution> <address> CA, </address> <month> December </month> <year> 1992. </year>
Reference-contexts: Our experiments revealed a performance characteristic of the iPSC/860. Optimum performance was obtained when taking full advantage of the 10 Mb of total cache available from the 10 i/o processors [Seamons94]. <ref> [Nitzberg92] </ref> provides complete hardware details of the iPSC/860 at NAS and additional experimental results that illustrate the significance of the cache size. In our i/o library for iPSC/860 checkpoints, the best physical schema varies depending on the size of the arrays.
Reference: [Pierce93] <author> Paul Pierce, </author> <title> A Concurrent File System for a Highly Parallel Mass Storage Subsystem, </title> <booktitle> Proceedings of the 4th Conference on Hypercube Concurrent Computers and Applications, </booktitle> <address> Monterey, </address> <month> March </month> <year> 1989. </year>
Reference: [Ryan93] <author> J. S. Ryan and S. K. Weeratunga, </author> <title> Parallel Computation of 3-D Navier Stokes Flow-fields for Supersonic Vehicles, AIAA Paper 93-0064, </title> <booktitle> 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <address> Reno, NV. </address> <year> 1993. </year>
Reference-contexts: The flow solver runs at the NAS facility on the Intel iPSC/860, a distributed memory multiprocessor computer with a single RAID disk system. <ref> [Ryan93] </ref> contains algorithmic details of the flow solver. As a starting point for creating chunked arrays on disk, one approach is to allow the in-memory chunking strategy employed by the flow solver to determine the size and shape of the chunks.
Reference: [Sarawagi94] <author> Sunita Sarawagi and Michael Stone-braker, </author> <title> Efficient Organization of Large Multidimensional Arrays, </title> <booktitle> Proceedings of the 10th International Conference on Data Engineering, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: There is research in progress to enhance the POSTGRES DBMS to support multidimensional arrays and store multidimensional arrays as chunks <ref> [Sarawagi94] </ref>. The POSTGRES work focuses on read-only applications that can take advantage of the query capabilities of a DBMS, while our work to date has examined write-intensive applications on massively parallel machines and scientific visualization applications.
Reference: [Seamons94] <author> K. E. Seamons and M. Winslett, </author> <title> An Efficient Abstract Interface for Multidimensional Array I/O, </title> <booktitle> Proceedings of Supercomputing '94, </booktitle> <address> Washington D.C., </address> <month> November, </month> <year> 1994. </year>
Reference-contexts: In this section we describe the functionality and interface of our i/o library (accessible on-line at URL http://bunny.cs.uiuc.edu/CADR/WinslettGroup/arrays.html) which incorporates the physical schemas described earlier. We also comment on the general performance characteristics we have observed testing our i/o library; specific performance numbers are reported in <ref> [Seamons94] </ref>. 3.1 Checkpoint and restart A flow solver periodically conducts a checkpoint to save the current state of a computation in order to resume that computation in case of a system failure. <p> Our experiments revealed a performance characteristic of the iPSC/860. Optimum performance was obtained when taking full advantage of the 10 Mb of total cache available from the 10 i/o processors <ref> [Seamons94] </ref>. [Nitzberg92] provides complete hardware details of the iPSC/860 at NAS and additional experimental results that illustrate the significance of the cache size. In our i/o library for iPSC/860 checkpoints, the best physical schema varies depending on the size of the arrays. <p> We modified the flow solver to output time step data using interleaved and non-interleaved chunking schemas. Our experiments show that moving to any sort of chunked schema gives approximately a 10-fold performance improvement in the time step i/o routines of the flow solver on the Intel iPSC/860 <ref> [Seamons94] </ref>. Performance for time step output data is more critical than for checkpoints because time step data are output more frequently. <p> Our experience modifying this tool confirms that applications with the proper abstract interfaces for data management can be easily extended to support new alternatives with little impact to the application code. <ref> [Seamons94] </ref> describes some preliminary performance studies we conducted with sample data sets at NAS to determine the number of disk pages accessed during a run of the particle tracer using the memory mapped approach.
Reference: [Stratum89] <institution> Stratum Technical Reference Manual, Scientific and Engineering Software, Austin, Texas, </institution> <month> October </month> <year> 1989. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using a traditional order, whether using file format systems [HDF94, NetCDF91], database management systems <ref> [Interbase90, Stratum89, Xidak91] </ref>, or hand-coded persistence. Traditional array ordering on disk is ideal for applications that need to access an entire persistent array that fits in main memory, a single array element, or contiguous elements along the fastest varying dimension. <p> Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase [Interbase90], Orion [Xidak91], and Stratum <ref> [Stratum89] </ref>. Like the file format systems described earlier, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
Reference: [Xidak91] <institution> Overview of Orion, </institution> <note> Version 2 Release 3(4), </note> <institution> XIDAK Inc., Palo Alto, California, </institution> <year> 1991. </year>
Reference-contexts: Our experience with scientific applications shows that multidimensional arrays are generally stored on disk using a traditional order, whether using file format systems [HDF94, NetCDF91], database management systems <ref> [Interbase90, Stratum89, Xidak91] </ref>, or hand-coded persistence. Traditional array ordering on disk is ideal for applications that need to access an entire persistent array that fits in main memory, a single array element, or contiguous elements along the fastest varying dimension. <p> Very little work from the database community has examined advanced techniques for storing multidimensional arrays on disk [Maier93]. A few commercial DBMSes support multidimensional arrays, including Interbase [Interbase90], Orion <ref> [Xidak91] </ref>, and Stratum [Stratum89]. Like the file format systems described earlier, these systems organize arrays in traditional order on disk and provide an interface that supports reading an entire array or subarray into memory from disk.
References-found: 25

