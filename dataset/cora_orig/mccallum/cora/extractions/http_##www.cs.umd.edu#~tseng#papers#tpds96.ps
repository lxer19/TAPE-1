URL: http://www.cs.umd.edu/~tseng/papers/tpds96.ps
Refering-URL: http://www.cs.umd.edu/~tseng/papers.html
Root-URL: 
Email: torrie@cs.stanford.edu mrm@ee.princeton.edu tseng@cs.umd.edu mhall@isi.edu  
Title: Characterizing the Memory Behavior of Compiler-Parallelized Applications  
Author: Evan Torrie Margaret Martonosi Chau-Wen Tseng Mary W. Hall 
Address: 4676 Admiralty Way Stanford, CA 94305-4070 Princeton, NJ 08544-5263 College Park, MD 20742 Marina del Rey, CA 90292  
Affiliation: Computer Systems Lab. Dept. of Electrical Eng. Dept. of Computer Science USC Information Sciences Inst. Stanford University Princeton University University of Maryland  
Abstract: Compiler-parallelized applications are increasing in importance as moderate-scale multiprocessors become common. This paper evaluates how features of advanced memory systems (e.g., longer cache lines) impact memory system behavior for applications amenable to compiler par-allelization. Using full-sized input data sets and applications taken from standard benchmark suites, we measure statistics such as speedups, synchronization and load imbalance, causes of cache misses, cache line utilization, data traffic and memory costs. This exploration allows us to draw several conclusions. First, we find that larger granularity parallelism often correlates with good memory system behavior, good overall performance, and high speedup in these applications. Second, we show that when long (512 byte) cache lines are used, many of these applications suffer from false sharing and low cache line utilization. Third, we identify some of the common artifacts in compiler-parallelized codes that can lead to false sharing or other types of poor memory system performance, and we suggest methods for improving them. Overall, this study offers both an important snapshot of the behavior of applications compiled by current parallelizing compilers, as well as an increased understanding of the interplay between cache line size, program granularity, and memory performance in moderate-scale multiprocessors. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Anderson and M. Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proc. SIGPLAN '93 Conf. on Programming Language Design and Implementation, </booktitle> <address> Albuquerque, NM, </address> <month> June </month> <year> 1993. </year> <month> 26 </month>
Reference-contexts: Finally, we note that there is much active research on compiler techniques to improve memory performance. Heuristics are being developed to reduce true sharing by improved co-location of data and computation <ref> [1, 3] </ref> and eliminate false sharing by better compiler management of large coherence units [9].
Reference: [2] <author> W. Blume and R. Eigenmann. </author> <title> Performance analysis of parallelizing compilers on the Perfect Bench--marks programs. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 3(6) </volume> <pages> 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Blume and Eigenmann <ref> [2] </ref> analyzed the performance of commercial parallelizing compilers on the PERFECT benchmarks, concluding that they detected only limited amounts of parallelism. The SUIF compiler incorporates many of the analyses they deemed vital; as a result, it enjoys much better success in extracting parallelism.
Reference: [3] <author> W. Bolosky and M. Scott. </author> <title> False sharing and its effect on shared memory performance. </title> <booktitle> In Proceedings of the USENIX Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <address> San Diego, CA, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: They also demonstrate that the overall miss rate in a multiprocessor can increase as the cache line size increases, whereas it tends to go down in uniprocessors. Bolosky and Scott <ref> [3] </ref> developed the cost component method to measure false sharing and applied it to four computation kernels. More recently, Dubois et al. [5] introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. <p> Finally, we note that there is much active research on compiler techniques to improve memory performance. Heuristics are being developed to reduce true sharing by improved co-location of data and computation <ref> [1, 3] </ref> and eliminate false sharing by better compiler management of large coherence units [9].
Reference: [4] <author> Helen Davis, Stephen R. Goldschmidt, and John Hennessy. </author> <title> Multiprocessor Simulation and Tracing Using Tango. </title> <booktitle> In Proc. International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1991. </year>
Reference-contexts: run-time system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH [16], SGI Challenge, and KSR-1 multiprocessors. 3 Methodology For these experiments, we used an extended version of the MemSpy simulator [18, 19] and the TangoLite simulation and tracing system <ref> [4, 8] </ref>. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. Because of this multiplexing, simulation statistics will vary depending on how often one switches between simulated threads.
Reference: [5] <author> Michel Dubois, Jonas Skeppstedt, Livio Ricciulli, et al. </author> <title> The Detection and Elimination of Useless Misses in Multiprocessors. </title> <booktitle> In Proc. 20th Intl. Symp. on Computer Architecture, </booktitle> <pages> pages 88-97, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: MemSpy supports monitoring cold, replacement, and invalidation cache misses on a procedure and data item basis. For our study we have further broken down the category of invalidation misses into true sharing and false sharing misses using the scheme described by Dubois et al. <ref> [5] </ref>. In this definition, a true sharing miss occurs if: during a lifetime of the line in the cache, the processor accesses a word written by a different processor since the last true, cold or replacement miss by the same processor to the same cache line. <p> Bolosky and Scott [3] developed the cost component method to measure false sharing and applied it to four computation kernels. More recently, Dubois et al. <ref> [5] </ref> introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. Torrellas et al. [23] measured false and true sharing and the number of bytes used per cache line.
Reference: [6] <author> S. J. Eggers and T. E. Jeremiassen. </author> <title> Eliminating false sharing. </title> <booktitle> In Proc. 1991 Int'l Conf. on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1991. </year>
Reference-contexts: They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. In comparison, the SUIF applications in this study have excellent spatial locality and are limited mostly by false sharing. Both Torrellas et al. [23] and Eggers and Jeremiassen <ref> [6] </ref> suggest program transformations to eliminate false sharing in hand-parallelized programs.
Reference: [7] <author> Susan J. Eggers and Randy H. Katz. </author> <title> The Effect of Sharing on the Cache and Bus Performance of Parallel Programs. </title> <booktitle> In Third Intl. Conf. on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [7, 10, 16] </ref>. Because of the increasing disparity between processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs [22, 26]. <p> Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned. Finally, longer cache lines may lead to increased data traffic, causing memory contention. Previous research has shown false sharing to be a problem for hand-parallelized applications <ref> [7] </ref>. Our study attempts to evaluate the effect of longer cache lines on applications amenable to compiler parallelization. By varying the cache line size, we are in effect measuring the spatial locality of compiler-parallelized applications. <p> While our work draws on a significant body of related work in understanding multiprocessor memory behavior, we outline below the most directly relevant studies. Eggers and Katz <ref> [7] </ref> did important early work characterizing application caching behavior of hand-parallelized programs in bus-based multiprocessors. For their applications, they show that the majority of cache misses in a bus-based multiprocessor are due to sharing misses.
Reference: [8] <author> Stephen R. Goldschmidt. </author> <title> Simulation of Multiprocessors, Speed and Accuracy. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: run-time system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH [16], SGI Challenge, and KSR-1 multiprocessors. 3 Methodology For these experiments, we used an extended version of the MemSpy simulator [18, 19] and the TangoLite simulation and tracing system <ref> [4, 8] </ref>. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. Because of this multiplexing, simulation statistics will vary depending on how often one switches between simulated threads.
Reference: [9] <author> E. Granston and H. Wishoff. </author> <title> Managing pages in shared virtual memory systems: Getting the compiler into the game. </title> <booktitle> In Proc. 1993 ACM Int'l. Conf. on Supercomputing, </booktitle> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: Finally, we note that there is much active research on compiler techniques to improve memory performance. Heuristics are being developed to reduce true sharing by improved co-location of data and computation [1, 3] and eliminate false sharing by better compiler management of large coherence units <ref> [9] </ref>. Our study helps to point out areas of poor program memory behavior deserving of additional research. 9 Conclusions In this paper, we demonstrate that good memory system behavior is vital to achieving reasonable speedups on moderate-scale multiprocessors.
Reference: [10] <author> Anoop Gupta and Wolf-Dietrich Weber. </author> <title> Cache Invalidation Patterns in Shared-Memory Multipro-cessos. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 41(7) </volume> <pages> 794-810, </pages> <month> July </month> <year> 1992. </year>
Reference-contexts: However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [7, 10, 16] </ref>. Because of the increasing disparity between processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs [22, 26].
Reference: [11] <author> M. W. Hall, B. R. Murphy, and S. P. Amarasinghe. </author> <title> Interprocedural parallelization analysis: A case study. </title> <booktitle> In Proc. Seventh SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <address> San Francisco, </address> <month> February </month> <year> 1995. </year>
Reference-contexts: At 16 processors, speedups for SUIF applications 5 range from 0.6 to 16 with an average speedup of 7.6. These simulated speedups correspond well with actual speedups observed for these programs on the Stanford DASH and SGI Challenge multiprocessors <ref> [11, 25] </ref>. Clearly then, memory overhead is a primary factor in the less than linear speedups displayed by most of these applications. Ideally we would like to understand what aspects of these applications are not amenable to features of advanced memory systems when executing in parallel. <p> The combination of these techniques enables the compiler to parallelize outer loops containing over a thousand lines of code in some cases. A more detailed discussion of the implementation of these techniques can be found in <ref> [11, 12] </ref>. To illustrate how these techniques can impact memory behavior, consider the performance of two SUIF applications, appbt and appsp.
Reference: [12] <author> M.W. Hall, S.P. Amarasinghe, B.S. Murphy, S. Liao, and M. Lam. </author> <title> Interprocedural parallelization analysis. </title> <booktitle> In Proceedings of Supercomputing '95. </booktitle> <publisher> IEEE Press, </publisher> <month> December </month> <year> 1995. </year>
Reference-contexts: The combination of these techniques enables the compiler to parallelize outer loops containing over a thousand lines of code in some cases. A more detailed discussion of the implementation of these techniques can be found in <ref> [11, 12] </ref>. To illustrate how these techniques can impact memory behavior, consider the performance of two SUIF applications, appbt and appsp.
Reference: [13] <author> T. Jeremiassen and S. Eggers. </author> <title> Reducing false sharing on shared memory multiprocessors through compile time data transformations. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: Both Torrellas et al. [23] and Eggers and Jeremiassen [6] suggest program transformations to eliminate false sharing in hand-parallelized programs. The latter have implemented their transformations in a compiler, and used them to eliminate false sharing in the SPLASH benchmarks by padding lock variables <ref> [13] </ref>. (In our SPLASH programs lock variables have also been padded to eliminate false sharing.) Only a handful of researchers have looked at the behavior of compiler-parallelized applications.
Reference: [14] <author> J. Kuskin et al. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proc. of the 21st Int'l Symp. on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <address> Chicago, IL, </address> <month> April </month> <year> 1994. </year>
Reference-contexts: We include additional results on an advanced memory system that more closely resembles an aggressive next-generation multiprocessor. It has a directory-based cache-coherent non-uniform memory access memory system with a high speed interconnect <ref> [14, 16] </ref>. Each processor has a single-level, least-recently-used cache whose size, associativity, and line size we vary. The penalty for a cache miss is dependent on the line size; Table 1 shows the cache miss penalties calculated for these machine parameters on a 16 processor system, assuming no contention.
Reference: [15] <author> R. L. Lee. </author> <title> The Effectiveness of Caches and Data Prefetch Buffers in Large-Scale Shared Memory Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, </institution> <month> May </month> <year> 1987. </year>
Reference-contexts: In uniprocessors, cache miss rates behave predictably with increasing line size, decreasing at first, eventually increasing as cache conflicts start to dominate. Unfortunately, miss rates are not so predictable for multiprocessor caches <ref> [15, 23] </ref>. Longer cache lines may prove problematic for parallel codes for several reasons. First, false sharing may cause cache misses on logically separate data placed on the same cache line. Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned.
Reference: [16] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The Directory-Based Protocol for the DASH Multiprocessor. </title> <booktitle> In Proc. 17th Annual Int'l Symp. on Computer Architecture, </booktitle> <month> May </month> <year> 1990. </year>
Reference-contexts: However, relatively little is known about their characteristics. In particular, memory system behavior has been shown to have a significant impact on the performance of scalable multiprocessors <ref> [7, 10, 16] </ref>. Because of the increasing disparity between processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs [22, 26]. <p> SUIF programs rely on a run-time system built from ANL macros for thread creation, barriers, and locks. The run-time system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH <ref> [16] </ref>, SGI Challenge, and KSR-1 multiprocessors. 3 Methodology For these experiments, we used an extended version of the MemSpy simulator [18, 19] and the TangoLite simulation and tracing system [4, 8]. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. <p> We include additional results on an advanced memory system that more closely resembles an aggressive next-generation multiprocessor. It has a directory-based cache-coherent non-uniform memory access memory system with a high speed interconnect <ref> [14, 16] </ref>. Each processor has a single-level, least-recently-used cache whose size, associativity, and line size we vary. The penalty for a cache miss is dependent on the line size; Table 1 shows the cache miss penalties calculated for these machine parameters on a 16 processor system, assuming no contention.
Reference: [17] <author> D. J. Lilja. </author> <title> The Impact of Parallel Loop Scheduling Strategies on Prefetching in a Shared-Memory Multiprocessor. </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 5(6) </volume> <pages> 573-584, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: They determined that parallelism overhead consumed 10-25% of program execution time and memory contention overhead was over 10%. Our study focused on a more advanced memory system and compiler; we also determine causes of poor memory behavior. Lilja <ref> [17] </ref> examines the impact 25 of prefetching in conjunction with loop scheduling strategies that schedule blocks of consecutive iterations to execute on each processor. Finally, we note that there is much active research on compiler techniques to improve memory performance.
Reference: [18] <author> Margaret Martonosi, Anoop Gupta, and Thomas Anderson. MemSpy: </author> <title> Analyzing Memory System Bottlenecks in Programs. </title> <booktitle> In Proc. ACM SIGMETRICS Conf. on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 1-12, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: The run-time system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH [16], SGI Challenge, and KSR-1 multiprocessors. 3 Methodology For these experiments, we used an extended version of the MemSpy simulator <ref> [18, 19] </ref> and the TangoLite simulation and tracing system [4, 8]. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. Because of this multiplexing, simulation statistics will vary depending on how often one switches between simulated threads.
Reference: [19] <author> Margaret R. Martonosi. </author> <title> Analyzing and Tuning Memory Performance in Sequential and Parallel Programs. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <month> December </month> <year> 1993. </year> <note> Also Stanford CSL Technical Report CSL-TR-94-602. </note>
Reference-contexts: The run-time system has been tuned to eliminate false sharing and minimize true sharing; it has been ported to the Stanford DASH [16], SGI Challenge, and KSR-1 multiprocessors. 3 Methodology For these experiments, we used an extended version of the MemSpy simulator <ref> [18, 19] </ref> and the TangoLite simulation and tracing system [4, 8]. TangoLite allows simulation of parallel programs by multiplexing their execution on a uniprocessor workstation. Because of this multiplexing, simulation statistics will vary depending on how often one switches between simulated threads.
Reference: [20] <author> C. Natarajan, S. Sharma, and R. Iyer. </author> <title> Measurement-based characterization of global memory and network contention, operating system and parallelization overheads: Case study on a shared-memory multiprocessor. </title> <booktitle> In Proc. of the 21st Int'l Symp. on Computer Architecture, </booktitle> <address> Chicago, IL, </address> <month> May </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: The SUIF compiler incorporates many of the analyses they deemed vital; as a result, it enjoys much better success in extracting parallelism. More recently, Natarajan et al. <ref> [20] </ref> measured operating system, parallelism, and memory contention overhead for five PERFECT applications on the Cedar multiprocessor. They determined that parallelism overhead consumed 10-25% of program execution time and memory contention overhead was over 10%.
Reference: [21] <author> S. K. Reinhardt, J. R. Larus, and D. A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proc. 21st Annual Int'l. Symp. on Computer Architecture, </booktitle> <pages> pages 325-337, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Overall, distinguishing utilization behavior can be important, because the bimodal behavior 13 suggests that special optimizations for each type of behavior may be possible. In systems allowing flexible protocols (such as Tempest <ref> [21] </ref>), one could specialize handling for each type of data. Essentially, the protocol could implement smaller coherence units for the previously-invalidated data, while maintaining coherence units equal to the cache line size for the previously replaced data.
Reference: [22] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Because of the increasing disparity between processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs <ref> [22, 26] </ref>. In this paper, we examine the memory system behavior of a new class of applications|those amenable to compiler parallelization. Our goal is to evaluate how these programs are impacted by advanced memory systems.
Reference: [23] <author> Josep Torrellas, Monica S. Lam, and John L. Hennessy. </author> <title> False Sharing and Spatial Locality in Multiprocessor Caches. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 43(6) </volume> <pages> 651-63, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: In uniprocessors, cache miss rates behave predictably with increasing line size, decreasing at first, eventually increasing as cache conflicts start to dominate. Unfortunately, miss rates are not so predictable for multiprocessor caches <ref> [15, 23] </ref>. Longer cache lines may prove problematic for parallel codes for several reasons. First, false sharing may cause cache misses on logically separate data placed on the same cache line. Second, applications may exhibit less spatial locality when executing in parallel, depending on how computation is partitioned. <p> This metric is a good indicator of spatial locality, because it shows how effectively the program is making use of longer cache lines. (This metric is similar to one that Torrellas et al. used in <ref> [23] </ref>, where they studied the number of words touched in a cache line.) 11 12 bars indicate the overall cache line utilization for each application across a range of cache line sizes. <p> More recently, Dubois et al. [5] introduced a definition of false sharing and used it to measure four hand-parallelized applications. We use their definition for our study. Torrellas et al. <ref> [23] </ref> measured false and true sharing and the number of bytes used per cache line. They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. <p> They find poor spatial locality has a greater impact than false sharing in determining the overall miss rate of their applications. In comparison, the SUIF applications in this study have excellent spatial locality and are limited mostly by false sharing. Both Torrellas et al. <ref> [23] </ref> and Eggers and Jeremiassen [6] suggest program transformations to eliminate false sharing in hand-parallelized programs.
Reference: [24] <author> E. Torrie, C-W Tseng, M. Martonosi, and M. W. Hall. </author> <title> Evaluating the impact of advanced memory systems on compiler-parallelized codes. </title> <booktitle> In Proc. Int'l. Conf. on Parallel Architectures and Compilation Techniques (PACT), </booktitle> <pages> pages 204-213, </pages> <address> Limassol, Cyprus, </address> <month> June </month> <year> 1995. </year>
Reference-contexts: Our goal is to evaluate how these programs are impacted by advanced memory systems. This paper makes several contributions: * We provide a detailed examination of the memory behavior of compiler-parallelized applications. Extending on previous work in <ref> [24] </ref>, we characterize the application behavior on both PRAM (parallel random access memory) and more realistic memory models. We use an extensive collection of real applications taken from well-known benchmark suites such as SPEC, NAS, and PERFECT. <p> In comparison to our earlier study of compiler-parallelized codes <ref> [24] </ref>, the machine model in this work includes a realistic simulation of write buffering, and more detailed simulation of contention. In particular, we model contention for the local memory bus and memory ports. <p> spec quantum physics 8 3 x16 grid (4.22) 240 98 0.05 swm256 spec shallow water model 256 2 grid (3.73) 44 100 5.34 tomcatv spec mesh generation 256 2 grid (3.72) 44 100 4.88 Table 2: Characteristics of scientific applications in study. these codes to a set of hand-parallelized applications <ref> [24] </ref>.) We define parallel coverage as the percentage of sequential program execution time spent inside parallel regions. Using pixie to instrument each program, we found parallel coverage of our programs by the SUIF compiler was between 94 and 100%. <p> Thus, while many compilers have so far striven for coarse-grain parallelism in order to reduce synchronization overhead, we provide compelling evidence of its importance for memory performance as well. We can compare our results for SUIF applications to similar numbers collected previously for SPLASH <ref> [24] </ref>. Unfortunately, we also find that, overall, even coarse-grain SUIF applications for these problem sizes have relatively high memory system costs when compared to hand-tuned SPLASH applications. For the baseline memory system, SPLASH benchmarks tended to have lower MCPI and miss rates than the set of SUIF applications.
Reference: [25] <author> R. Wilson et al. </author> <title> SUIF: An infrastructure for research on parallelizing and optimizing compilers. </title> <journal> ACM SIGPLAN Notices, </journal> <volume> 29(12) </volume> <pages> 31-37, </pages> <month> December </month> <year> 1994. </year>
Reference-contexts: In the following sections, we describe the compiler, simulation methodology, and applications used in our experiments. We present our measurements for these programs, then examine the behavior of the compiler in greater detail before concluding. 2 The SUIF Parallelizing Compiler For our study we used the SUIF parallelizing compiler <ref> [25] </ref> to generate parallel versions of our applications. SUIF takes as input sequential Fortran or C programs, producing as output parallel C programs that execute according to a master-worker model. For each program, SUIF 2 performed identical set of analyses and optimizations. <p> At 16 processors, speedups for SUIF applications 5 range from 0.6 to 16 with an average speedup of 7.6. These simulated speedups correspond well with actual speedups observed for these programs on the Stanford DASH and SGI Challenge multiprocessors <ref> [11, 25] </ref>. Clearly then, memory overhead is a primary factor in the less than linear speedups displayed by most of these applications. Ideally we would like to understand what aspects of these applications are not amenable to features of advanced memory systems when executing in parallel.
Reference: [26] <author> S. Woo, M. Ohara, E. Torrie, J.P. Singh, and A. Gupta. </author> <title> Methodological considerations and characterization of the SPLASH-2 parallel application suite. </title> <booktitle> In Proc. of the 22st Int'l Symp. on Computer Architecture, </booktitle> <address> Santa Margherita Ligure, Italy, </address> <month> June </month> <year> 1995. </year> <month> 28 </month>
Reference-contexts: Because of the increasing disparity between processor and memory speeds, memory systems have been evolving towards longer cache lines in order to hide memory latency. Researchers have studied how this trend affects carefully tuned hand-parallelized programs <ref> [22, 26] </ref>. In this paper, we examine the memory system behavior of a new class of applications|those amenable to compiler parallelization. Our goal is to evaluate how these programs are impacted by advanced memory systems.
References-found: 26

