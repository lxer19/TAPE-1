URL: http://www-db.stanford.edu/~sergey/near.ps
Refering-URL: http://www-db.stanford.edu/~sergey/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Near Neighbor Search in Large Metric Spaces  
Author: Sergey Brin 
Keyword: near neighbor, metric space, approximate queries, data mining, Dirichlet domains, Voronoi regions.  
Date: February 27, 1995  
Affiliation: Department of Computer Science Stanford University  
Abstract: Given user data, one often wants to find approximate matches in a large database. A good example of such a task is finding images similar to a given image in a large collection of images. We focus on the important and technically difficult case where each data element is high dimensional, or more generally, is represented by a point in a large metric space 1 and distance calculations are computationally expensive. In this paper we introduce a data structure to solve this problem called a GNAT Geometric Near-neighbor Access Tree. It is based on the philosophy that the data structure should act as a hierarchical geometrical model of the data as opposed to a simple decomposition of the data which doesn't use its intrinsic geometry. In experiments, we find that GNAT's outperform previous data structures in a number of applications. 
Abstract-found: 1
Intro-found: 1
Reference: [BFR + 93] <author> E. Bugnion, S. Fei, T. Roos, P. Widmayer, and F. </author> <title> Widmer. A spatial index for approximate multiple string matching. </title> <booktitle> In Proc. First South American Workshop on String Processing, </booktitle> <address> Belo Horizonte, Brazil, </address> <month> September </month> <year> 1993. </year>
Reference-contexts: However, it has a weakness in that it requires two computations at every node and is limited to a branching factor of two. A variation of gh-trees was implemented at ETH Zurich <ref> [BFR + 93] </ref> as monotonous bisector trees (MBT's) to deal specifically with text. However, nothing in the method would have prevented them from dealing with arbitrary metric spaces. The key difference between MBT's and gh-trees is that MBT's only select one new point at each new node.
Reference: [BK73] <author> W. A. Burkhard and R. M. Keller. </author> <title> Some approaches to best-match file searching. </title> <journal> Communications of the ACM, </journal> <volume> 16(4), </volume> <month> April </month> <year> 1973. </year>
Reference-contexts: They do this by reusing the point they are associated with in the parent node. As a result, MBT's overcome the first weakness but the branching factor remains a problem. The most relevant works, however, are also the oldest. Burkhard and Keller suggested several data structures (and algorithms) <ref> [BK73] </ref> for approximate search. The first is very similar to vp-trees except that it requires a finite number of discrete distance values. Essentially, for every vantage point, a separate branch is allocated for every possible distance value. This method, however, suffers from the same asymmetry problem as the vp-trees.
Reference: [FN75] <author> K. Fukunaga and P. M. Narendra. </author> <title> A branch and bound algorithm for computing k-nearest neighbors. </title> <journal> IEEE Trans. Comput., </journal> <volume> C-24:750-753, </volume> <year> 1975. </year>
Reference-contexts: K. Fukunaga and P. Narendra worked out a very similar scheme, which requires more than just a metric space, to create a tree structure with an arbitrary branching factor in 1975 <ref> [FN75] </ref> as follows.
Reference: [FS82] <author> C.D. Feustel and L. G. Shapiro. </author> <title> The nearest neighbor problem in an abstract metric space. </title> <journal> Pattern Recognition Letters, </journal> <month> De-cember </month> <year> 1982. </year>
Reference-contexts: This is the case as long as the database size is fairly small compared to the range of the search <ref> [FS82] </ref> or if preprocessing is not allowed and only arbitrary precom-puted distances are given [SW90]. The other category of solutions are hierarchical and typically have an O (log n) query time given a sufficiently small range (typically too small to be practical).
Reference: [HKR93] <author> Huttenlocher, Klanderman, and Ruck-lidge. </author> <title> Comparing images using the haus Page 10 dorff distance. </title> <journal> IEEE Transactions on Pat--tern Analysis and Machine Intelligence, </journal> <volume> 15, </volume> <year> 1993. </year>
Reference-contexts: Genetics Finding similar DNA or protein sequences in one of a number of large genetics databases. Speaker Recognition Finding similar vocal patterns (e.g., under Fourier transforms) from a database of vocal patterns. Image Recognition Finding images similar (using the Hausdorff metric <ref> [HKR93] </ref>) to a given one from a large image library.
Reference: [SW90] <author> Dennis Shasha and Tsong-Li Wang. </author> <title> New techniques for best-match retrieval. </title> <journal> ACM Transactions on Information Systems., </journal> <volume> 8(2):140, </volume> <year> 1990. </year>
Reference-contexts: This is the case as long as the database size is fairly small compared to the range of the search [FS82] or if preprocessing is not allowed and only arbitrary precom-puted distances are given <ref> [SW90] </ref>. The other category of solutions are hierarchical and typically have an O (log n) query time given a sufficiently small range (typically too small to be practical). They are of the following form: The space is broken up hierarchically.
Reference: [Uhl91] <author> Uhlmann. </author> <title> Satisfying general proximity / similarity queries with metric trees. </title> <journal> Information Processing Letters, </journal> <volume> 40, </volume> <year> 1991. </year>
Reference-contexts: Based on these distances, the points are separated into two or several different branches. For each branch, the structure is constructed recursively. J. K. Uhlmann outlined the foundation for two different methods, generally described as metric trees <ref> [Uhl91] </ref>. One of these methods, subsequently called vp-trees 3 , was implemented by P. N. Yiani-los [Yia93]. The basic construction of a vp-tree is to break the space up using spherical cuts.
Reference: [Ukk92] <author> Ukkonen. </author> <title> Approximate string matching with q-grams and maximal matches. </title> <journal> Theoretical Computer Science, </journal> <volume> 92, </volume> <year> 1992. </year>
Reference-contexts: Another important research direction is to begin to use approximate distance metrics. For example, in order to compute near neighbors in text using the edit distance (an expensive computation), we can first use the q-gram distance <ref> [Ukk92] </ref> (a relatively fast computation) to narrow the search quickly and then apply proper edit distance to complete the search. The key is that q-gram distance is a lower bound for edit distance.
Reference: [Yia93] <author> Yianilos. </author> <title> Data structures and algorithms for nearest neighbor search in general metric spaces. </title> <booktitle> In ACM-SIAM Symposium on Discrete Algorithms (A Conference on Theoretical and Experimental Analysis of Discrete Algorithms), </booktitle> <year> 1993. </year> <pages> Page 11 </pages>
Reference-contexts: For each branch, the structure is constructed recursively. J. K. Uhlmann outlined the foundation for two different methods, generally described as metric trees [Uhl91]. One of these methods, subsequently called vp-trees 3 , was implemented by P. N. Yiani-los <ref> [Yia93] </ref>. The basic construction of a vp-tree is to break the space up using spherical cuts. To build it, pick a point in the data set (this is called the vantage point, hence the name vp-tree). <p> Page 7 sampling technique to chose vantage points since we could not be sure that we would do it identically to <ref> [Yia93] </ref>. However, some limited tests with sampling indicated that savings were in the 10% range for images and were negligible for text and random vectors.
References-found: 9

