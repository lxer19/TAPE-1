URL: http://polaris.cs.uiuc.edu/reports/1403.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Email: Email: gaohu@dso008.sch.ge.com,  Email: larson.john@epamail.epa.gov,  
Phone: Tel: (518) 385-0872  Tel: (517) 894-7600  
Title: A Year's Profile of Academic Supercomputer Users Using the CRAY Hardware Performance Monitor  
Author: Hui Gao John L. Larson 
Note: 1 Current address:  2 Corresponding Author, Current address: National Environmental Supercomputing Center, 135  
Date: February 22, 1995  
Address: 465 CSRL, 1308 W. Main St. Urbana, IL 61801  465 CSRL, 1308 W. Main St. Urbana, IL 61801  Drive West, Albany, NY 12205,  Avenue, Bay City, MI 48708,  
Affiliation: Center for Supercomputing Research and Development  Center for Supercomputing Research and Development  Keane, Inc., 17 Computer  Washington  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> M. Berry and et.al. </author> <title> The Perfect Club Benchmarks: Effective Performance Evaluation of Supercomputers. </title> <journal> The International Journal of Supercomputer Applications, </journal> <volume> 3(3) </volume> <pages> 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: In this approach a benchmark set represents a workload well if it performs the same functions as the workload, e.g. a real workload performing payroll activities should be represented by payroll programs. This application area coverage approach is followed by many current benchmarks, e.g. Perfect <ref> [1] </ref>. * Performance Oriented Approach. In this approach a benchmark set represents a workload well if it causes the system to exhibit the same performance characteristics as the workload, e.g. Mflops, average vector length.
Reference: [2] <author> M. W. Berry. </author> <title> Scientific Workload Characterization By Loop-Based Analyses. </title> <type> Technical Report CS-91-140, </type> <institution> University of Tennessee, Knoxville, TN, </institution> <month> August </month> <year> 1991. </year>
Reference-contexts: These studies include: * Bradley [5] - 5 codes from the Perfect Benchmarks * Berry <ref> [2] </ref> - 9 codes from the Perfect Benchmarks * Berry [3] most of the current, popular benchmark sets The above studies in benchmark characterization complement our work in workload characterization.
Reference: [3] <author> Mike Berry, George Cybenko, and John Larson. </author> <title> Scientific Benchmarks Characterization. </title> <journal> Parallel Computing, </journal> 17(10&11):1173-1194, December 1991. 
Reference-contexts: These studies include: * Bradley [5] - 5 codes from the Perfect Benchmarks * Berry [2] - 9 codes from the Perfect Benchmarks * Berry <ref> [3] </ref> most of the current, popular benchmark sets The above studies in benchmark characterization complement our work in workload characterization.
Reference: [4] <author> D. Bradley, G. Cybenko, H. Gao, J. Larson, F. Ahmad, J. Golab, and M. Straka. </author> <title> Supercomputer Workload Decomposition and Analysis. </title> <booktitle> In Proceedings of the ACM International Conference on Supercomputing, </booktitle> <pages> pages 458-467, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Both types 3 of characterization studies are necessary to collect the data required for comparing and measuring the representa- tiveness of benchmarks and workloads. * Bradley <ref> [4] </ref> - 11 codes from large time-consuming users at NCSA * Delic [9] - 24 codes from the users at the Ohio Supercomputer Center These two studies in workload characterization use the method of workload sampling by taking "typical" programs from the workload.
Reference: [5] <author> David K. Bradley and John L. Larson. </author> <title> A Parallelism-Based Analytic Approach to Performance Evaluation Using Application Programs. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 81(8) </volume> <pages> 1126-1135, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Other studies have been made using the HPM (primarily on the CRAY X-MP) for program and workload analysis in an effort to gain a more detailed understanding of the performance characteristics of benchmarks, user programs, and workloads. These studies include: * Bradley <ref> [5] </ref> - 5 codes from the Perfect Benchmarks * Berry [2] - 9 codes from the Perfect Benchmarks * Berry [3] most of the current, popular benchmark sets The above studies in benchmark characterization complement our work in workload characterization. <p> Nevertheless, the HPM capabilities could be extended to provide a clearer picture and better understanding of the performance we seek to measure and characterize. The performance of a computer system can be expressed as the product of 3 terms <ref> [5] </ref>. P erf ormance = (clock rate) fl parallelism = work The clock rate of a computer is available from the vendor. The HPM facilities on the Cray machines is quite useful and accurate in providing a measure of the work performed in the execution of a program. <p> An improved HPM facility would have the capability to measure and record the parallelism profile. Enhanced counters would be able to count the clock periods in which up to n work operations occurred simultaneously. In our companion project <ref> [5] </ref>, we use parallelism profiles to compare and distinguish candidate benchmark programs.
Reference: [6] <author> Ingrid Bucher and Joanne Martin. </author> <title> Methodology for Characterizing a Scientific Workload. </title> <type> Technical Report LA-UR-82-1702, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1982. </year>
Reference: [7] <author> Melvyn Ciment, William Scherlis, Stephen Griffin, Charles Brownstein, and et al, </author> <title> editors. Grand Challenges 1993: High Performance Computing and Communications. </title> <booktitle> National Science Foundation, </booktitle> <year> 1993. </year>
Reference-contexts: MPP, in the HPCC program <ref> [7] </ref>. * to serve as a target specification for a benchmark set that analytically represents this workload * to provide vendors with feedback on how their machines are used and how to make better ones Our own plans for future work include a deeper analysis of the execution characteristics of the
Reference: [8] <author> Cray Research, Inc. </author> <title> UNICOS Performance Utilities Reference Manual, </title> <address> SR-2040 - 6.0 edition, </address> <year> 1991. </year>
Reference: [9] <author> George Delic. </author> <title> Performance Analysis of a 24 Code Sample on Cray X/Y-MP Systems at the Ohio Supercomputer Center. </title> <editor> In Danny C. Sorenson, editor, </editor> <booktitle> Proceedings of the Fifth SIAM Conference on Parallel Processing for Scientific Applications, </booktitle> <pages> pages 530-535, </pages> <address> Philadelphia, </address> <year> 1991. </year> <note> SIAM. </note>
Reference-contexts: Both types 3 of characterization studies are necessary to collect the data required for comparing and measuring the representa- tiveness of benchmarks and workloads. * Bradley [4] - 11 codes from large time-consuming users at NCSA * Delic <ref> [9] </ref> - 24 codes from the users at the Ohio Supercomputer Center These two studies in workload characterization use the method of workload sampling by taking "typical" programs from the workload. The representativeness of the programs to the workload is not quantified.
Reference: [10] <author> J. J. Dongarra, H. W. Meuer, and E. Strohmaier, </author> <title> editors. </title> <type> TOP500 Report: </type> <year> 1993, </year> <pages> pages 70, 105-107. </pages> <institution> University of Mannheim, Germany, </institution> <year> 1994. </year>
Reference-contexts: 1) how important are academic sites?, and 2) how similar are academic sites to other types of sites, or to supercomputing in general? Table 1 shows that the academic setting is the largest sector as measured in 1993 by the number of top 500 supercomputer systems installed in the world <ref> [10] </ref>. Also, as the 1993 top 500 report documents, Cray Research machines comprised 40% and CRAY Y-MP's comprised 25% of the top 500 machines in the world.
Reference: [11] <author> Domineco Ferrari. </author> <title> Workload Characterization and Selection in Computer Performance Measurement. </title> <journal> Computer, </journal> <volume> 5(4) </volume> <pages> 18-24, </pages> <month> July-August </month> <year> 1972. </year>
Reference: [12] <author> Steven L. </author> <title> Gaede. Tools for Research in Computer Workload Characterization and Modeling. </title> <editor> In D. Ferrari and M. Spadoni, editors, </editor> <booktitle> Experimental Computer Performance and Evaluation, </booktitle> <pages> pages 235-247. </pages> <publisher> North-Holland, </publisher> <year> 1981. </year>
Reference-contexts: To address these problems at least three approaches to workload characterization and benchmark construction have been proposed <ref> [12] </ref>: 1 * Resource Consumption Approach. In this approach a benchmark set represents a workload well if it consumes resources at the same rate as the real workload, e.g. CPU time used, memory space, and duration of I/O. * Functional Approach.
Reference: [13] <author> U. Grenander and R. F. Tsau. </author> <title> Quantitative methods for evaluating computer system performance: a review and proposals. </title> <booktitle> In Proceedings of the Conference on Statistical Methods for the Evaluation of Computer Systems Performance. </booktitle> <institution> Brown University, </institution> <month> November </month> <year> 1971. </year>
Reference-contexts: some basic problems in performance evaluation that are still as true today as they were 20 years ago [19]: Grenander and Tsao (1971) wrote: "We believe that no real significant advance in the evaluation of systems can be expected until some breakthrough is made in the characterization of the workload <ref> [13] </ref>." Ferrari (1972) states: "The lack of satisfactory workload characterization techniques is one of the main reasons for the primitive state of this important branch of computer engineering" (i.e., performance evaluation)[11].
Reference: [14] <author> John Larson and Bob Lutz. </author> <title> PERFTRACE User's Guide. </title> <type> Technical report, Cray Research internal technical report, </type> <month> August </month> <year> 1985. </year> <note> available from the authors. </note>
Reference: [15] <author> John L. Larson. </author> <title> Collecting and Interpreting HPM Performance Data on the CRAY Y-MP. </title> <journal> NCSA Datalink, </journal> <volume> 5(5) </volume> <pages> 14-24, </pages> <month> November-December </month> <year> 1991. </year>
Reference-contexts: The statistics accumulated in a system file that may be post-processed, summarized, and sorted in various ways to produce reports about activities that occurred during the recording period <ref> [15] </ref>.
Reference: [16] <author> Doru Marcusiu. </author> <title> Performance Data Now Automatic on CRAY Y-MP. </title> <journal> NCSA Datalink, </journal> <volume> 5(4) </volume> <pages> 5-6, </pages> <month> September-October </month> <year> 1991. </year>
Reference-contexts: This change caused UNICOS to write HPM data to a system file at the termination of each user process or job <ref> [16] </ref>. Jobs that were not linked after June 1, 1991 are not recorded. This includes some third party software which was installed in a binary form. A filter was employed to avoid the recording of small processes taking less than one second, e.g. user commands such as 'ls'.
Reference: [17] <author> Joanne Martin, Ingrid Bucher, and Tony Warnock. </author> <title> Workload Characterization for Vector Computers: Tools and Techniques. </title> <type> Technical Report LA-UR-83-305, </type> <institution> Los Alamos National Laboratory, </institution> <address> Los Alamos, NM, </address> <year> 1983. </year> <month> 26 </month>
Reference: [18] <author> Harry Nelson. </author> <title> Using the Performance Monitors on the X-MP/48. </title> <institution> LLNL Tentacle, </institution> <year> 1985. </year>
Reference-contexts: The representativeness of the programs to the workload is not quantified. This shortcoming is present in most benchmarks sets developed and used today. * Nelson <ref> [18] </ref> - 30 hours at the Lawrence Livermore National Laboratory * Sato [20] - 8 days at the National Center for Atmospheric Research * Williams [24][23] - 7 weekends at 2 government sites * West [22] - 35 days at the Ontario Centre for Large Scale Computation.
Reference: [19] <author> Saul Rosen. </author> <booktitle> Lectures on the Measurement and Evaluation of the Performance of Computing Systems. SIAM, 1976. Regional Conference Series in Applied Mathematics. </booktitle>
Reference-contexts: We believe that we must address some basic problems in performance evaluation that are still as true today as they were 20 years ago <ref> [19] </ref>: Grenander and Tsao (1971) wrote: "We believe that no real significant advance in the evaluation of systems can be expected until some breakthrough is made in the characterization of the workload [13]." Ferrari (1972) states: "The lack of satisfactory workload characterization techniques is one of the main reasons for the
Reference: [20] <author> Dick Sato. </author> <title> SCD Conducts Performance and Benchmarking Tests. </title> <journal> SCD Computing News, </journal> <volume> 9(4) </volume> <pages> 3-5, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: The representativeness of the programs to the workload is not quantified. This shortcoming is present in most benchmarks sets developed and used today. * Nelson [18] - 30 hours at the Lawrence Livermore National Laboratory * Sato <ref> [20] </ref> - 8 days at the National Center for Atmospheric Research * Williams [24][23] - 7 weekends at 2 government sites * West [22] - 35 days at the Ontario Centre for Large Scale Computation.
Reference: [21] <author> Scientific Supercomputing Subcommittee. </author> <title> NSF Supercomputer Centers Study. </title> <type> Technical report, </type> <institution> IEEE Computer Society, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: We are motivated by a current lack of detailed understanding (quantification) about how computers are used. There is a need to know more about the use of computers than the number of hours consumed in different application areas <ref> [21] </ref>. The limited performance information available about how computers are used makes it difficult to construct benchmarks that are representative of the workloads at various sites or of supercomputers in general, if workloads are not known or difficult to understand in a quantifiable way.
Reference: [22] <author> Edmond West. </author> <title> X-MP System Wide Performance Analysis. </title> <journal> The CLSC News, </journal> <volume> 3(1) </volume> <pages> 15-17, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: This shortcoming is present in most benchmarks sets developed and used today. * Nelson [18] - 30 hours at the Lawrence Livermore National Laboratory * Sato [20] - 8 days at the National Center for Atmospheric Research * Williams [24][23] - 7 weekends at 2 government sites * West <ref> [22] </ref> - 35 days at the Ontario Centre for Large Scale Computation. These workload characterization studies use the method of HPM recording for the entire workload for various periods of time. The longer the recording time, the more program samples were taken.
Reference: [23] <author> Elizabeth Williams, C. Thomas Myers, and Rebecca Koskela. </author> <title> The Characterization of Two Scientific Workloads Using the CRAY X-MP Hardware Performance Monitor. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 142-152, </pages> <month> November </month> <year> 1990. </year>
Reference: [24] <author> Elizabeth A. Williams. </author> <title> Measurement of Two Scientific Workloads Using the CRAY X-MP Performance Monitor. </title> <type> Technical Report SRC-TR-88-020, </type> <institution> Supercomputing Research Center, </institution> <month> November </month> <year> 1988. </year> <month> 27 </month>
References-found: 24

