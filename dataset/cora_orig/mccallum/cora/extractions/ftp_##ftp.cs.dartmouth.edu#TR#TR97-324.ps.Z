URL: ftp://ftp.cs.dartmouth.edu/TR/TR97-324.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR97-324/
Root-URL: http://www.cs.dartmouth.edu
Email: fjaa,katya,rusg@cs.dartmouth.edu  
Phone: phone: (603) 646 1691 fax: (603) 646 1672  
Title: Computing Dense Clusters On-line for Information Organization  
Author: Javed Aslam Katya Pelekhov Daniela Rus 
Address: Hanover, NH 03755  
Affiliation: Department of Computer Science Dartmouth College  
Abstract: We present and analyze the off-line star algorithm for clustering static information systems and the online star algorithm for clustering dynamic information systems. These algorithms partition a document collection into a number of clusters that is naturally induced by the collection. We show a lower bound on the accuracy of the clusters produced by these algorithms. We use the random graph model to show that both star algorithms produce correct clusters in time fi(V + E). Finally, we provide data from extensive experiments. 
Abstract-found: 1
Intro-found: 1
Reference: [AB84] <author> M. Aldenderfer and R. Blashfield, </author> <title> Cluster Analysis, </title> <type> Sage, Beverly Hills, </type> <year> 1984. </year>
Reference-contexts: There has been extensive research on clustering and applications to many domains <ref> [HS86, AB84] </ref>. For a good overview see [JD88]. For a good overview of using clustering in information retrieval see [Wil88].
Reference: [APR97] <author> J. Aslam, K. Pelekhov, and D. Rus, Generat--ing, </author> <title> visualizing, and evaluating high-accuracy clusters for information organization, </title> <type> Technical Report PCS-TR97-319, </type> <institution> Department of Computer Science, Dartmouth, </institution> <year> 1997. </year>
Reference-contexts: Since in dynamic information systems the the number of topics is not known a priori, a fixed number of clusters 1 can't generate a natural partition for the information. Our work on clustering presented in this paper and in <ref> [APR97] </ref> provides positive evidence for the cluster hypothesis. We propose an off-line algorithm for clustering static information and an on-line version of this algorithm for clustering dynamic information. These two algorithms compute clusters induced by the natural topic structure of the space. <p> Since the number of clusters produced by our algorithms is given by the underlying topic structure in the information system, our clusters are dense and accurate. Our work extends previous results [HP96] that support using clustering for browsing applications and presents positive evidence for the cluster hypothesis. In <ref> [APR97] </ref>, we argue that by using a clustering algorithm that guarantees the cluster quality through separation of dissimilar documents and aggregation of similar documents, clustering is beneficial for information retrieval tasks that require high precision and high recall.
Reference: [Bol95] <author> B. Bollobas, </author> <title> Random Graphs, </title> <publisher> Academic Press, </publisher> <address> London, </address> <year> 1995. </year>
Reference-contexts: We further under took a study to determine the expected similarity be tween two satellite vertices. 2.2 The random graph model The model we use for computing the expected similarity error in a star is the random graph model <ref> [Bol95] </ref>. A random graph G n;p is an undirected graph with n vertices, where each of its possible edges is inserted randomly and independently with probability p. <p> The mean of this distribution is = (n 1)p and its variance is = p (n 1)p (1 p). Note that while the the degrees of the vertices do exhibit some dependence, for practical purposes they can be considered independent <ref> [Bol95] </ref>. This means that on average, each star covers (n 1)p + 1 np vertices 4 . Since the np vertices covered by each star are randomly chosen, there will be some overlap between the star covers. Each new star leaves uncovered a (1p) fraction of the previously uncovered vertices.
Reference: [Can93] <author> F. </author> <title> Can, Incremental clustering for dynamic information processing, </title> <journal> in ACM Transactions on Information Systems, </journal> <volume> no. 11, pp143-164, </volume> <year> 1993. </year>
Reference: [CCFM97] <author> M. Charikar, C. Chekuri, T. Feder, and R. Motwani, </author> <title> Incremental clustering and dynamic information retrieval, </title> <booktitle> in Proceedings of the 29 th Symposium on Theory of Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Scatter/Gather uses fractionation to compute nearest-neighbor clusters. In a recent STOC paper, Charika et al. <ref> [CCFM97] </ref> consider a dynamic clustering algorithm to partition a collection of text documents into a fixed number of clusters. Since in dynamic information systems the the number of topics is not known a priori, a fixed number of clusters 1 can't generate a natural partition for the information. <p> We propose an off-line algorithm for clustering static information and an on-line version of this algorithm for clustering dynamic information. These two algorithms compute clusters induced by the natural topic structure of the space. Thus, this work is different than <ref> [CKP93, CCFM97] </ref> in that we do not impose the constraint to use a fixed number of clusters. As a result, we can guarantee a lower bound on the topic similarity between the documents in each cluster.
Reference: [Cro80] <author> W. B. Croft. </author> <title> A model of cluster searching based on classification. </title> <journal> Information Systems, </journal> <volume> 5 </volume> <pages> 189-195, </pages> <year> 1980. </year>
Reference-contexts: Efforts have been made to find whether the cluster hypothesis is valid. Voorhees [Voo85] discusses a way of evaluating whether the cluster hypothesis holds and shows negative results. Croft <ref> [Cro80] </ref> describes a method for bottom-up cluster search that could be shown to outperform a full ranking system for the Cranfield collection. The single link method [Cro77] does not provide any guarantees for the topic similarity within a cluster.
Reference: [Cro77] <author> W. B. Croft. </author> <title> Clustering large files of documents using the single-link method. </title> <journal> Journal of the American Society for Information Science, </journal> <volume> pp189-195, </volume> <month> November </month> <year> 1977. </year>
Reference-contexts: Voorhees [Voo85] discusses a way of evaluating whether the cluster hypothesis holds and shows negative results. Croft [Cro80] describes a method for bottom-up cluster search that could be shown to outperform a full ranking system for the Cranfield collection. The single link method <ref> [Cro77] </ref> does not provide any guarantees for the topic similarity within a cluster. In [JR71] Jardine and van Rijsber-gen show some evidence that search results could be improved by clustering.
Reference: [CKP93] <author> D. Cutting, D. Karger, and J. Peder-sen. </author> <title> Constant interaction-time scatter/gather browsing of very large document collections. </title> <booktitle> In Proceedings of the 16 th SIGIR, </booktitle> <year> 1993. </year>
Reference-contexts: The single link method [Cro77] does not provide any guarantees for the topic similarity within a cluster. In [JR71] Jardine and van Rijsber-gen show some evidence that search results could be improved by clustering. Hearst and Pedersen [HP96] re-examine the cluster hypothesis by focusing on the Scatter/Gather system <ref> [CKP93] </ref> and conclude that it holds for browsing tasks. Systems like Scatter/Gather [CKP93] provide a mechanism for user-driven organization of data in a fixed number of clusters, but the users need to be in the loop and the computed clusters do not have accuracy guarantees. <p> In [JR71] Jardine and van Rijsber-gen show some evidence that search results could be improved by clustering. Hearst and Pedersen [HP96] re-examine the cluster hypothesis by focusing on the Scatter/Gather system <ref> [CKP93] </ref> and conclude that it holds for browsing tasks. Systems like Scatter/Gather [CKP93] provide a mechanism for user-driven organization of data in a fixed number of clusters, but the users need to be in the loop and the computed clusters do not have accuracy guarantees. Scatter/Gather uses fractionation to compute nearest-neighbor clusters. <p> We propose an off-line algorithm for clustering static information and an on-line version of this algorithm for clustering dynamic information. These two algorithms compute clusters induced by the natural topic structure of the space. Thus, this work is different than <ref> [CKP93, CCFM97] </ref> in that we do not impose the constraint to use a fixed number of clusters. As a result, we can guarantee a lower bound on the topic similarity between the documents in each cluster.
Reference: [FG88] <author> T. Feder and D. Greene, </author> <title> Optimal algorithms for approximate clustering, </title> <booktitle> in Proceedings of the 20 th Symposium on Theory of Computing, </booktitle> <pages> pp 434-444, </pages> <year> 1988. </year>
Reference: [HP96] <author> M. Hearst and J. Pedersen. </author> <title> Reexamining the cluster hypothesis: </title> <booktitle> Scatter/Gather on Retrieval Results. In Proceedings of the 19 th SIGIR, </booktitle> <year> 1996. </year>
Reference-contexts: The single link method [Cro77] does not provide any guarantees for the topic similarity within a cluster. In [JR71] Jardine and van Rijsber-gen show some evidence that search results could be improved by clustering. Hearst and Pedersen <ref> [HP96] </ref> re-examine the cluster hypothesis by focusing on the Scatter/Gather system [CKP93] and conclude that it holds for browsing tasks. <p> Since the number of clusters produced by our algorithms is given by the underlying topic structure in the information system, our clusters are dense and accurate. Our work extends previous results <ref> [HP96] </ref> that support using clustering for browsing applications and presents positive evidence for the cluster hypothesis.
Reference: [HS86] <author> D. Hochbaum and D. Shmoys, </author> <title> A unified approach to approximation algorithms for bottleneck problems, </title> <journal> Journal of the ACM, </journal> <volume> no. 33, pp533-550, </volume> <year> 1986. </year>
Reference-contexts: There has been extensive research on clustering and applications to many domains <ref> [HS86, AB84] </ref>. For a good overview see [JD88]. For a good overview of using clustering in information retrieval see [Wil88].
Reference: [JD88] <author> A. Jain and R. Dubes. </author> <title> Algorithms for Clustering Data, </title> <publisher> Prentice Hall 1988. </publisher>
Reference-contexts: There has been extensive research on clustering and applications to many domains [HS86, AB84]. For a good overview see <ref> [JD88] </ref>. For a good overview of using clustering in information retrieval see [Wil88]. The use of clustering in information retrieval was mostly driven by the cluster hypothesis [Rij79] which states that relevant documents tend to be more closely related to each other than to non-relevant documents.
Reference: [JR71] <author> N. Jardine and C.J. van Rijsbergen. </author> <title> The use of hierarchical clustering in information retrieval, </title> <booktitle> 7 </booktitle> <pages> 217-240, </pages> <year> 1971. </year>
Reference-contexts: Croft [Cro80] describes a method for bottom-up cluster search that could be shown to outperform a full ranking system for the Cranfield collection. The single link method [Cro77] does not provide any guarantees for the topic similarity within a cluster. In <ref> [JR71] </ref> Jardine and van Rijsber-gen show some evidence that search results could be improved by clustering. Hearst and Pedersen [HP96] re-examine the cluster hypothesis by focusing on the Scatter/Gather system [CKP93] and conclude that it holds for browsing tasks.
Reference: [KP93] <author> G. Kortsarz and D. Peleg. </author> <title> On choosing a dense subgraph. </title> <booktitle> In Proceedings of the 34th Annual Symposium on Foundations of Computer Science (FOCS), </booktitle> <year> 1993. </year>
Reference-contexts: To compute accurate clusters, we formalize clustering as covering graphs by cliques. Covering by cliques is NP-complete, and thus intractable for large document collections. Recent graph-theoretic results have shown that the problem cannot even be approximated in polynomial time [LY94, Zuc93]. Recent results for covering graphs by dense subgraphs <ref> [KP93] </ref> are encouraging. We used a cover by dense subgraphs that are star-shaped and that can be computed off-line for static data, or on-line for dynamic data. We show that the off-line and on-line algorithms produce correct clusters efficiently.
Reference: [LY94] <author> C. Lund and M. Yannakakis. </author> <title> On the hardness of approximating minimization problems. </title> <journal> Journal of the ACM 41, </journal> <pages> 960-981, </pages> <year> 1994. </year>
Reference-contexts: To compute accurate clusters, we formalize clustering as covering graphs by cliques. Covering by cliques is NP-complete, and thus intractable for large document collections. Recent graph-theoretic results have shown that the problem cannot even be approximated in polynomial time <ref> [LY94, Zuc93] </ref>. Recent results for covering graphs by dense subgraphs [KP93] are encouraging. We used a cover by dense subgraphs that are star-shaped and that can be computed off-line for static data, or on-line for dynamic data. We show that the off-line and on-line algorithms produce correct clusters efficiently. <p> In our information retrieval application this is a desirable feature as documents can have multiple subthemes. Unfortunately, this approach is computationally intractable. For real corpora, similarity graphs can be very large. The clique cover problem is NP-complete, and it does not admit polynomial-time approximation algorithms <ref> [LY94, Zuc93] </ref>. While we cannot perform a clique cover nor even approximate such a cover, we can instead cover our graph by dense subgraphs. What we lose in intra-cluster similarity guarantees, we gain in computational efficiency.
Reference: [Rij79] <author> C.J. van Rijsbergen. </author> <title> Information Retrieval. </title> <publisher> Butterworths, </publisher> <address> London, </address> <year> 1979. </year>
Reference-contexts: There has been extensive research on clustering and applications to many domains [HS86, AB84]. For a good overview see [JD88]. For a good overview of using clustering in information retrieval see [Wil88]. The use of clustering in information retrieval was mostly driven by the cluster hypothesis <ref> [Rij79] </ref> which states that relevant documents tend to be more closely related to each other than to non-relevant documents. Efforts have been made to find whether the cluster hypothesis is valid. Voorhees [Voo85] discusses a way of evaluating whether the cluster hypothesis holds and shows negative results.
Reference: [Sal89] <author> G. Salton. </author> <title> Automatic Text Processing: the transformation, analysis, and retrieval of information by computer, </title> <publisher> Addison-Wesley, </publisher> <year> 1989. </year>
Reference-contexts: As a result, we can guarantee a lower bound on the topic similarity between the documents in each cluster. The model for topic similarity is the standard vector space model used in the information retrieval community <ref> [Sal89] </ref> which is explained in more detail in this paper in Section 2. To compute accurate clusters, we formalize clustering as covering graphs by cliques. Covering by cliques is NP-complete, and thus intractable for large document collections. <p> We measure the similarity between two documents by using the cosine metric in the vector space model of the Smart information retrieval system <ref> [Sal91, Sal89] </ref>. The vector space model for textual information aggregates statistics on the occurrence of words in documents. The premise of the vector space model is that two documents are similar if they use the same words. <p> Precision-recall are the standard measurements for the performance of an information retrieval algorithm <ref> [Sal89] </ref>. Acknowledgements This work is supported in part by the Navy under contract ONR N00014-95-1-1204. Special thanks to Philip Klein for stimulating discussions in the initial phase of this work.
Reference: [Sal91] <author> G. Salton. </author> <title> The Smart document retrieval project. </title> <booktitle> In Proceedings of the Fourteenth Annual International ACM/SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 356-358. </pages>
Reference-contexts: In such dynamic systems users need to locate information fast. Users often don't know what they need until they need it. In dynamic, time-pressured situations such as emergency relief for weather disasters, access to the latest information needs to happen fast. Current information systems such as Inquery [Tur90], Smart <ref> [Sal91] </ref>, or Alta Vista provide some simple automation by computing ranked (sorted) lists of documents, but it is ineffective for users to scan a list of hundreds of document titles. <p> We measure the similarity between two documents by using the cosine metric in the vector space model of the Smart information retrieval system <ref> [Sal91, Sal89] </ref>. The vector space model for textual information aggregates statistics on the occurrence of words in documents. The premise of the vector space model is that two documents are similar if they use the same words.
Reference: [Tur90] <author> H. </author> <title> Turtle. Inference networks for document retrieval. </title> <type> PhD thesis. </type> <institution> University of Mas-sachusetts, Amherst, </institution> <year> 1990. </year>
Reference-contexts: In such dynamic systems users need to locate information fast. Users often don't know what they need until they need it. In dynamic, time-pressured situations such as emergency relief for weather disasters, access to the latest information needs to happen fast. Current information systems such as Inquery <ref> [Tur90] </ref>, Smart [Sal91], or Alta Vista provide some simple automation by computing ranked (sorted) lists of documents, but it is ineffective for users to scan a list of hundreds of document titles.
Reference: [Voo85] <author> E. Voorhees. </author> <title> The cluster hypothesis revisited. </title> <booktitle> In Proceedings of the 8 th SIGIR, </booktitle> <pages> pp 95-104, </pages> <year> 1985. </year>
Reference-contexts: The use of clustering in information retrieval was mostly driven by the cluster hypothesis [Rij79] which states that relevant documents tend to be more closely related to each other than to non-relevant documents. Efforts have been made to find whether the cluster hypothesis is valid. Voorhees <ref> [Voo85] </ref> discusses a way of evaluating whether the cluster hypothesis holds and shows negative results. Croft [Cro80] describes a method for bottom-up cluster search that could be shown to outperform a full ranking system for the Cranfield collection.
Reference: [Wil88] <author> P. Willett. </author> <title> Recent trends in hierarchical document clustering: A critical review. </title> <booktitle> Information Processing and Management, </booktitle> <address> 24:(5):577-597, </address> <year> 1988. </year>
Reference-contexts: There has been extensive research on clustering and applications to many domains [HS86, AB84]. For a good overview see [JD88]. For a good overview of using clustering in information retrieval see <ref> [Wil88] </ref>. The use of clustering in information retrieval was mostly driven by the cluster hypothesis [Rij79] which states that relevant documents tend to be more closely related to each other than to non-relevant documents. Efforts have been made to find whether the cluster hypothesis is valid.
Reference: [Wor71] <author> S. Worona. </author> <title> Query clustering in a large document space. </title> <editor> In Ed. G. Salton, </editor> <booktitle> The SMART Retrieval System, </booktitle> <pages> pp 298-310. </pages> <publisher> Prentice-Hall, </publisher> <year> 1971. </year>

References-found: 22

