URL: http://www.cs.wisc.edu/~dewitt/papers/paralleldb/pdis91.ps
Refering-URL: http://www.cs.wisc.edu/~dewitt/paralleldb.html
Root-URL: 
Title: A Performance Study of Three High Availability Data Replication Strategies  
Author: Hui-I Hsiao David J. DeWitt 
Note: This research was partially supported by the Defense Advanced Research Projects Agency under contract N00039-86-C-0578, by the National Science Foundation under grants DCR-8512862, by a Digital Equipment Corporation External Research Grant, and by a research grant from the Tandem Computer Corporation. This work was done while the author was in the  
Address: Yorktown Heights, NY 10598  Madison, WI 53706  
Affiliation: IBM T. J. Watson Research Center  Computer Sciences Department University of Wisconsin  University of Wisconsin.  
Abstract-found: 0
Intro-found: 1
Reference: [Anon85] <editor> Anon et. al, </editor> <title> "A Measure of Transaction Processing Power," </title> <type> TR# 85.1, </type> <institution> Tandem Computer, Cupertino, </institution> <address> CA, </address> <year> 1985. </year>
Reference: [Bitt88] <author> Bitton, D. and J. Gray, </author> <title> "Disk Shadowing," </title> <booktitle> Proceedings of the 14th International Conference on Very Large Data Base, </booktitle> <address> Los Angeles, </address> <month> August </month> <year> 1988. </year>
Reference-contexts: When one copy fails, the other copy can continue to be used and, unless both copies fail simultaneously, the failure will be transparent to users of the system and no interruption of service will occur. Examples of this mechanism include mirrored disks <ref> [Katz78, Bitt88] </ref>, interleaved declustering [Tera85], the inverted file strategy [Cope88], and chained declustering [Hsia90a]. In the second approach, the data, along with the redundant error detection/correction information (usually parity bytes), is spread across an array of disk drives. <p> The results of our simulation experiments are presented and analyzed in Section 4. Our conclusions and future research directions are contained in Section 5. 2. Existing High Availability Strategies In this section, we briefly describe the three data replication schemes studied in this paper: mirrored disks <ref> [Borr81, Bitt88] </ref>, interleaved declustering [Tera85, Cope89], and chained declustering [Hsia90a]. Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1. <p> An elevator disk scheduling discipline is used except in the case of mirrored disks. In this case, a FIFO (with the shortest seek time) scheduling discipline is used <ref> [Bitt88, Gray88] </ref>. <p> In this case, a FIFO (with the shortest seek time) scheduling discipline is used [Bitt88, Gray88]. The total time required to complete a disk access is DiskAccessTime =SeekTime +RotationalLatency+SettleTime+TransferTime The Seek Time for seeking across n tracks is modeled by the formula <ref> [Bitt88] </ref>: Seek Time (n) = SeekFactor * dd n The Rotational Latency is modeled by a random function which returns uniformly distributed values in the range of MinLatency to MaxLatency. SettleTime models the disk head settle time after a disk arm movement. <p> In the Tandem Non-Stop SQL system, and our model of this architecture, the disk with the shortest seek time is assigned to serve a disk read request. As a result, the expected seek distance for random reads is one-sixth of the tracks <ref> [Bitt88, Gray88] </ref>. <p> While no extra CPU cycles are required with the MD mechanism, the write to the mirrored pair ends up synchronizing both disk arms and the average seek distance becomes 0.47n, where n is the number of cylinders <ref> [Bitt88] </ref>. This is 0.14n higher than the average seek distance of a single disk. Henceforth, we shall refer to this effect as the Synchronizing Write Overhead. <p> Consequently, no extra CPU cycles are needed for updating the backup copy. However, with mirrored disks each write operation ends up synchronizing the read/write heads of both disks in the mirrored pair <ref> [Bitt88] </ref>. Therefore, the disk service time for a write becomes the maximum service 20 time of two writes.
Reference: [Bitt89] <author> Bitton, D., </author> <title> "Arm Scheduling in Shadowed Disks," COMPCON, </title> <publisher> IEEE Press, </publisher> <month> March </month> <year> 1989. </year>
Reference-contexts: R i represents the i-th horizontal fragment of the first copy of R and r i stands for the mirror image of R i . As shown in identical, causing the two disk arms to become synchronized on writes <ref> [Bitt89] </ref>. sc 0.30 medium 8 3 1 8 3 9 file GRN/fig.tan.d1 When a disk in a mirrored pair fails, the remaining disk can assume the workload of the failed drive and, unless both disks fail simultaneously, data will always be available. <p> If most I/Os are reads, losing a drive may result in doubling the average I/O time because only one disk arm is available. On the other hand, if most I/Os are write operations, the impact of a failure may be minimal <ref> [Bitt89] </ref>. The failure of a processor will, however, almost always have a significant negative impact on performance. Consider the failure of processor P1 in Figure 1.
Reference: [Borr81] <author> Borr, A., </author> <title> "Transaction Monitoring in Encompass [TM]: </title> <booktitle> Reliable Distributed Transaction Processing," Proceedings of the 7th International Conference on Very Large Data Base, </booktitle> <year> 1981. </year>
Reference-contexts: 1. Introduction and Motivation While a number of solutions have been proposed for increasing the availability and reliability of computer systems, the most commonly used technique involves the replication of processors and mass storage <ref> [Borr81] </ref>. For database applications, the availability of disk-resident data files is perhaps the major concern. Most database management systems employ a combination of a disk-based log together with periodic checkpointing of memory-resident data to insure the integrity and availability of the database in the event of disk or system failures. <p> The results of our simulation experiments are presented and analyzed in Section 4. Our conclusions and future research directions are contained in Section 5. 2. Existing High Availability Strategies In this section, we briefly describe the three data replication schemes studied in this paper: mirrored disks <ref> [Borr81, Bitt88] </ref>, interleaved declustering [Tera85, Cope89], and chained declustering [Hsia90a]. Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1.
Reference: [Care86] <author> Carey, M. and H. Lu, </author> <title> "Load Balancing in a Locally Distributed Database System," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <year> 1986. </year>
Reference-contexts: In addition, the work of a node may be shifted to its neighbor without physically moving data because nodes are "chained" together through the primary and backup copies of a fragment. These two characteristics of the chained declustering scheme provide a good opportunity for dynamic load balancing <ref> [Care86] </ref> when a hot spot changes over time. Consequently, a reorganization of the database may not be required. 21 Acknowledgements We would like to thank Mike Carey for his invaluable help while developing the simulation model of Gamma and in interpreting the results obtained.
Reference: [Care89] <author> Carey, M., Livny, M., </author> <title> "Parallelism and Concurrency Control Performance in Distributed Database Machines," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Portland, Oregon June 1989. </address>
Reference: [Chen90] <author> Chen, P., Gibson, G., Katz, R., and Patterson D., </author> <title> "An Evaluation of Redundant Arrays of Disks Using an Amdahl 5890," </title> <booktitle> Proceedings of ACM SIGMETRICS conference, </booktitle> <address> Colorado, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: When operating without any failures, [Gray90] hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Some examples of such applications are stock market trading, air defense systems, air traffic control systems, airline reservation-type systems, banking (OLTP), etc. demonstrates that mirrored disks (an identical copy based strategy) provides better performance than RAID for OLTP applications and <ref> [Chen90] </ref> demonstrates that for small requests (less than one track of data), a mirrored disk mechanism provides higher disk throughput (MBbyte/sec./disk) than RAID.
Reference: [Cope88] <author> Copeland, G., Alexander, W., Boughter, E., and T. Keller, </author> <title> "Data Placement in Bubba," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Examples of this mechanism include mirrored disks [Katz78, Bitt88], interleaved declustering [Tera85], the inverted file strategy <ref> [Cope88] </ref>, and chained declustering [Hsia90a]. In the second approach, the data, along with the redundant error detection/correction information (usually parity bytes), is spread across an array of disk drives.
Reference: [Cope89] <author> Copeland, G. and T. Keller, </author> <title> "A Comparison of High-Availability Media Recovery Techniques," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Portland, Oregon June 1989. </address>
Reference-contexts: Our conclusions and future research directions are contained in Section 5. 2. Existing High Availability Strategies In this section, we briefly describe the three data replication schemes studied in this paper: mirrored disks [Borr81, Bitt88], interleaved declustering <ref> [Tera85, Cope89] </ref>, and chained declustering [Hsia90a]. Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1. <p> Optionally, each relation can be replicated. In this case, one copy is designated as the primary copy and the other the backup copy. The tuples in each primary fragment are stored on one node. For backup fragments, Teradata employs a special data placement scheme termed interleaved declustering <ref> [Tera85, Cope89] </ref>. If the cluster size is N, each backup fragment will be subdivided into N-1 subfragments each of which will be stored on a different disk within the same cluster but not the disk containing the primary fragment.
Reference: [DeWi86] <author> DeWitt, D., Gerber, R., Graefe, G., Heytens, M., Kumar, K., and M. Muralikrishna, </author> <title> "GAMMA A High Performance Dataflow Database Machine," </title> <booktitle> Proceedings of the 12th International Conference on Very Large Data Base, </booktitle> <address> Japan, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: On the other hand, the disk array approach is more attractive if the cost of disk space is a major concern. Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation [Tand87, DeWi88].
Reference: [DeWi88] <author> DeWitt, D., Ghandeharizadeh, S., and Schneider, D., </author> <title> "A Performance Analysis of the Gamma Database Machine," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques [Ries78, Tera85, DeWi86, Livn87] facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation <ref> [Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult, as one or more nodes 3 (processor/disk pairs) must assume the workload of the component that has failed.
Reference: [DeWi90] <author> DeWitt, D., Ghandeharizadeh, S.,Schneider, D., Bricker, A., Hsiao, H, and Rasmussen, </author> <title> R, "The Gamma Database Machine Project," </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> Vol. 2, No 1, </volume> <month> March </month> <year> 1990. </year>
Reference-contexts: In this paper, using a simulation model, we study the performance of three identical copy based high availability schemes: chained declustering, mirrored disks, and interleaved declustering. The simulation model is based on the software and hardware architectures of the Gamma database machine <ref> [DeWi90] </ref>. With this simulation model, the performance of the three data replication strategies is evaluated under a number of different workload assumptions. <p> Simulation Model 3.1. Model Overview To evaluate the three availability mechanisms, we constructed a simulation model of the Gamma <ref> [DeWi90] </ref> running on a 32 node Intel iPSC/2 hypercube [Inte88]. Figure 5 depicts the overall structure of the model. Each component is implemented as a DeNet [Livn89] discrete event module.
Reference: [Fran87] <author> Frank, P., </author> <title> "Advances in Head Technology," Presentation at Challenges in Disk Technology Short Course, Institute for Information Storage Technology, </title> <address> Santa Clara University, Santa Clara, CA, </address> <month> December 15-17, </month> <year> 1987. </year>
Reference-contexts: This is because the Remote Update Overhead incurred by CD and ID consumes extra CPU cycles while the Synchronizing Write Overhead associated with MD increases the disk service time. Since advances in CPU technology have occurred much faster than advances in disk technology <ref> [Joy85, Fran87] </ref>, we believe that such operations will almost certainly be I/O bound in the future and thus the additional cost of doing remote updates will not be that significant.
Reference: [Gerb87] <author> Gerber, R. and D. DeWitt, </author> <title> "The Impact of Hardware and Software Alternatives on the Performance of the Gamma Database Machine", </title> <type> Computer Sciences Technical Report #708, </type> <institution> University of Wisconsin-Madison, </institution> <month> July, </month> <year> 1987. </year>
Reference: [Gray88] <author> Gray, J., Sammer, H., and Whitford, S., </author> <title> "Shortest Seek vs Shortest Service Time Scheduling of Mirrored Disc Reads" Tandem Computers, </title> <month> December </month> <year> 1988. </year>
Reference-contexts: An elevator disk scheduling discipline is used except in the case of mirrored disks. In this case, a FIFO (with the shortest seek time) scheduling discipline is used <ref> [Bitt88, Gray88] </ref>. <p> In the Tandem Non-Stop SQL system, and our model of this architecture, the disk with the shortest seek time is assigned to serve a disk read request. As a result, the expected seek distance for random reads is one-sixth of the tracks <ref> [Bitt88, Gray88] </ref>.
Reference: [Gray90] <author> Gray, G., Horst, B., and Walker, M., </author> <title> "Parity Striping of Disc Arrays: Low-Cost Reliable Storage with Acceptable Throughput," </title> <booktitle> Proceedings of the 16th Onternational Conference on Very Large Data Base, </booktitle> <address> Bris-bane, Australia, </address> <month> August </month> <year> 1990. </year>
Reference-contexts: When errors are discovered, the redundant information can be used to restore the data and application programs can continue using the data with minimal interruption. Strategies based on this approach include synchronized disk interleaving [Kim86], redundant array of inexpensive disks (RAID) [Patt88], and parity striping of disk arrays <ref> [Gray90] </ref>. Both approaches have been used in commercial systems. For example, Tandem's NonStop SQL database machine and Teradata's DBC 1012 database machine employ identical copies, while IBM's AS400 system uses a disk array. <p> When operating without any failures, <ref> [Gray90] </ref> hhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhhh 1 Some examples of such applications are stock market trading, air defense systems, air traffic control systems, airline reservation-type systems, banking (OLTP), etc. demonstrates that mirrored disks (an identical copy based strategy) provides better performance than RAID for OLTP applications and [Chen90] demonstrates that for small requests (less than
Reference: [Hsia90a] <author> Hsiao, H. and DeWitt, D., </author> <title> "Chained Declustering: A New Availability Strategy for Multiprocessor Database Machines," </title> <booktitle> Proceedings of the 6th International Conference on Data Engineering, </booktitle> <address> Los Angeles, CA, </address> <month> February </month> <year> 1990. </year> <month> 22 </month>
Reference-contexts: Examples of this mechanism include mirrored disks [Katz78, Bitt88], interleaved declustering [Tera85], the inverted file strategy [Cope88], and chained declustering <ref> [Hsia90a] </ref>. In the second approach, the data, along with the redundant error detection/correction information (usually parity bytes), is spread across an array of disk drives. When errors are discovered, the redundant information can be used to restore the data and application programs can continue using the data with minimal interruption. <p> Our conclusions and future research directions are contained in Section 5. 2. Existing High Availability Strategies In this section, we briefly describe the three data replication schemes studied in this paper: mirrored disks [Borr81, Bitt88], interleaved declustering [Tera85, Cope89], and chained declustering <ref> [Hsia90a] </ref>. Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1. <p> However, this improvement in load balancing is not without a penalty. In particular, the probability of data being unavailable 4 increases proportionately with the size of the cluster <ref> [Hsia90a] </ref>. 4 During the normal mode of operation, read requests are directed to the fragments of the primary copy and write operations update both copies. <p> Chained Declustering With chained declustering <ref> [Hsia90a] </ref>, two physical copies (a primary and a backup) of each relation are declustered over a set of disks such that the primary and backup copies of a fragment are always placed on different nodes. <p> For example, in addition to three declustering alternatives (range, hash, and round-robin), Gamma also provides clustered and non-clustered indices on both the partitioning and non-partitioning attributes 6 . <ref> [Hsia90a] </ref> describe the design of a load balancing algorithm for the chained declustering mechanism that can handle all possible combinations of partitioning methods, storage organizations, and access plans. <p> Notice, however, that with an ID cluster size of 8, besides providing lower throughput, the interleaved declustering scheme is 3.5 times more likely to have data unavailable than the chained declustering scheme <ref> [Hsia90a] </ref>. Our future work includes studying the performance tradeoffs of the three replication schemes with skewed data access and the possibility of dynamic load balancing for the chained declustering scheme.
Reference: [Hsia90b] <author> Hsiao, H., </author> <title> "Performance and Availability in Database Machines with Replicated Data," </title> <type> Computer Sci--ences Technical Report #963, </type> <institution> University of Wisconsin-Madison, </institution> <month> August, </month> <year> 1990. </year>
Reference-contexts: In such cases, the failed disk array will be restricted to serve only one request at a time and its performance will degrade significantly. The availability of a system or data file is greatly influenced by the way data files are placed on disks <ref> [Hsia90b] </ref> and the time needed to recover or restore a failed disk/node. The longer the recovery time is the higher the probability that a second failure will render data unavailable. <p> Model Validation In order to evaluate the accuracy of the results produced by the simulation model, we first configured the model to reflect, as accurately as possible, the characteristics of Gamma, and then ran a number of experiments 10 without replication. As described in <ref> [Hsia90b] </ref>, the model predicted the actually measured performance of Gamma with less than a 10% margin of error. 4.2. Experimental Design Typically, system throughput and average response time are the two key metrics used to evaluate a system.
Reference: [Inte88] <author> Intel Corporation, </author> <title> iPSC/2 User's Guide, </title> <publisher> Intel Corporation Order No. </publisher> <pages> 311532-002, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Simulation Model 3.1. Model Overview To evaluate the three availability mechanisms, we constructed a simulation model of the Gamma [DeWi90] running on a 32 node Intel iPSC/2 hypercube <ref> [Inte88] </ref>. Figure 5 depicts the overall structure of the model. Each component is implemented as a DeNet [Livn89] discrete event module. The arcs in the figure are discrete event connectors and can be thought of as a combination of a preconstructed message path and a set of predefined message types.
Reference: [Joy85] <author> Joy, B., </author> <note> Presentation at ISSCC '85 panel session, </note> <month> February </month> <year> 1985. </year>
Reference-contexts: This is because the Remote Update Overhead incurred by CD and ID consumes extra CPU cycles while the Synchronizing Write Overhead associated with MD increases the disk service time. Since advances in CPU technology have occurred much faster than advances in disk technology <ref> [Joy85, Fran87] </ref>, we believe that such operations will almost certainly be I/O bound in the future and thus the additional cost of doing remote updates will not be that significant.
Reference: [Katz78] <author> Katzman, J., </author> <title> "A Fault-Tolerant Computing System," </title> <booktitle> Proceedings of the 11th Hawaii Conference on System Sciences, </booktitle> <month> January </month> <year> 1978. </year>
Reference-contexts: When one copy fails, the other copy can continue to be used and, unless both copies fail simultaneously, the failure will be transparent to users of the system and no interruption of service will occur. Examples of this mechanism include mirrored disks <ref> [Katz78, Bitt88] </ref>, interleaved declustering [Tera85], the inverted file strategy [Cope88], and chained declustering [Hsia90a]. In the second approach, the data, along with the redundant error detection/correction information (usually parity bytes), is spread across an array of disk drives.
Reference: [Kim86] <author> Kim, M., </author> <title> "Synchronized Disk Interleaving," </title> <journal> IEEE Transactions on Computers, </journal> <volume> Vol. C-35, No. 11, </volume> <month> November </month> <year> 1986. </year>
Reference-contexts: When errors are discovered, the redundant information can be used to restore the data and application programs can continue using the data with minimal interruption. Strategies based on this approach include synchronized disk interleaving <ref> [Kim86] </ref>, redundant array of inexpensive disks (RAID) [Patt88], and parity striping of disk arrays [Gray90]. Both approaches have been used in commercial systems. For example, Tandem's NonStop SQL database machine and Teradata's DBC 1012 database machine employ identical copies, while IBM's AS400 system uses a disk array.
Reference: [Livn87] <author> Livny, M., S. Khoshafian, and H. Boral, </author> <title> "Multi-Disk Management," </title> <booktitle> Proceedings of ACM SIGMETRICS Conference, </booktitle> <address> Alberta, Canada, </address> <year> 1987. </year>
Reference-contexts: On the other hand, the disk array approach is more attractive if the cost of disk space is a major concern. Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation [Tand87, DeWi88].
Reference: [Livn89] <author> Livny, M., </author> <note> "DeNet User's Guide," Version 1.5, </note> <institution> Computer Sciences Department, University of Wisconsin-Madison, </institution> <year> 1989. </year>
Reference-contexts: Simulation Model 3.1. Model Overview To evaluate the three availability mechanisms, we constructed a simulation model of the Gamma [DeWi90] running on a 32 node Intel iPSC/2 hypercube [Inte88]. Figure 5 depicts the overall structure of the model. Each component is implemented as a DeNet <ref> [Livn89] </ref> discrete event module. The arcs in the figure are discrete event connectors and can be thought of as a combination of a preconstructed message path and a set of predefined message types.
Reference: [Patt88] <author> Patterson, D., Gibson, G., and Katz, R., </author> <title> "A Case for Redundant Arrays of Inexpensive Disks (RAID)," </title> <booktitle> Proceedings of the ACM-SIGMOD International Conference on Management of Data, </booktitle> <address> Chicago, </address> <month> May </month> <year> 1988. </year>
Reference-contexts: When errors are discovered, the redundant information can be used to restore the data and application programs can continue using the data with minimal interruption. Strategies based on this approach include synchronized disk interleaving [Kim86], redundant array of inexpensive disks (RAID) <ref> [Patt88] </ref>, and parity striping of disk arrays [Gray90]. Both approaches have been used in commercial systems. For example, Tandem's NonStop SQL database machine and Teradata's DBC 1012 database machine employ identical copies, while IBM's AS400 system uses a disk array.
Reference: [Ries78] <author> Ries, D. and Epstein, R., </author> <title> "Evaluation of Distribution Criteria for Distributed Database Systems," </title> <type> UCB/ERL Technical Report M78/22, </type> <institution> UC Berkeley, </institution> <month> May </month> <year> 1978. </year>
Reference-contexts: On the other hand, the disk array approach is more attractive if the cost of disk space is a major concern. Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation [Tand87, DeWi88].
Reference: [Ston75] <author> Stonebraker, M., </author> <title> "Implementation of Integrity Constraints and Views by Query Modification," </title> <booktitle> Proceedings of the SIGMOD Workshop on Management of Data, </booktitle> <address> San Jose, Calif., </address> <month> May </month> <year> 1975. </year>
Reference-contexts: The keys to the solution are the notion of a responsible range for indexed attributes, the use of query modification techniques <ref> [Ston75] </ref>, and the availability of an extent map for relations stored as a heap. 3. Simulation Model 3.1. Model Overview To evaluate the three availability mechanisms, we constructed a simulation model of the Gamma [DeWi90] running on a 32 node Intel iPSC/2 hypercube [Inte88].
Reference: [Ston86] <author> Stonebraker, M., </author> <title> "The Case for Shared Nothing," </title> <journal> Database Engineering, </journal> <volume> Vol. 9, No. 1, </volume> <year> 1986. </year>
Reference-contexts: On the other hand, the disk array approach is more attractive if the cost of disk space is a major concern. Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture <ref> [Ston86] </ref>. For such systems, the application of horizontal partitioning (i.e. declustering) techniques [Ries78, Tera85, DeWi86, Livn87] facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation [Tand87, DeWi88].
Reference: [Tand87] <author> Tandem Database Group, </author> <title> "NonStop SQL, A Distributed, High-Performance, High-Reliability Implementation of SQL," </title> <booktitle> Workshop on High Performance Transaction Systems, Asilomar, </booktitle> <address> CA, </address> <month> September </month> <year> 1987. </year>
Reference-contexts: Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques [Ries78, Tera85, DeWi86, Livn87] facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation <ref> [Tand87, DeWi88] </ref>. However, when a failure occurs, balancing the workload among the remaining processors and disks can become difficult, as one or more nodes 3 (processor/disk pairs) must assume the workload of the component that has failed. <p> Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1. Tandem's Mirrored Disks Architecture In Tandem's NonStop SQL system <ref> [Tand87] </ref>, each disk drive is connected to two I/O controllers, and each I/O controller is connected to two processors, thus providing two completely independent paths to each disk drive. Furthermore, each disk drive is "mirrored" (duplicated) to further ensure data availability. Relations are generally declustered across multiple disk drives.
Reference: [Tera85] <author> Teradata, </author> <title> "DBC/1012 Database Computer System Manual Release 2.0," Document No. </title> <institution> C10-0001-02, Teradata Corp., </institution> <month> NOV </month> <year> 1985. </year>
Reference-contexts: When one copy fails, the other copy can continue to be used and, unless both copies fail simultaneously, the failure will be transparent to users of the system and no interruption of service will occur. Examples of this mechanism include mirrored disks [Katz78, Bitt88], interleaved declustering <ref> [Tera85] </ref>, the inverted file strategy [Cope88], and chained declustering [Hsia90a]. In the second approach, the data, along with the redundant error detection/correction information (usually parity bytes), is spread across an array of disk drives. <p> On the other hand, the disk array approach is more attractive if the cost of disk space is a major concern. Throughout this paper, we focus on multiprocessor database systems that employ a "shared-nothing" architecture [Ston86]. For such systems, the application of horizontal partitioning (i.e. declustering) techniques <ref> [Ries78, Tera85, DeWi86, Livn87] </ref> facilitates the successful application of inter- and intra-query parallelism in the normal mode of operation [Tand87, DeWi88]. <p> Our conclusions and future research directions are contained in Section 5. 2. Existing High Availability Strategies In this section, we briefly describe the three data replication schemes studied in this paper: mirrored disks [Borr81, Bitt88], interleaved declustering <ref> [Tera85, Cope89] </ref>, and chained declustering [Hsia90a]. Each scheme stores two identical copies of each relation on different disks and each is able to sustain a single node (disk or processor) failure. 2.1. <p> If P2 is already fully utilized when the failure occurs, the response time for queries that access data on either pair of drives may double if the system is CPU bound. 2.2. Teradata's Interleaved Declustering Scheme In the Teradata database machine <ref> [Tera85] </ref>, the processors are divided into clusters of 2 to 16 processors (one or two disk drives may be attached to each processor). Tuples in a relation are declustered among the drives in one or more clusters by hashing on a "key" attribute. <p> Optionally, each relation can be replicated. In this case, one copy is designated as the primary copy and the other the backup copy. The tuples in each primary fragment are stored on one node. For backup fragments, Teradata employs a special data placement scheme termed interleaved declustering <ref> [Tera85, Cope89] </ref>. If the cluster size is N, each backup fragment will be subdivided into N-1 subfragments each of which will be stored on a different disk within the same cluster but not the disk containing the primary fragment.
Reference: [Triv82] <author> Trivedi, K., </author> <title> in Probability and Statistics with Reliability, Queueing, </title> <booktitle> and Computer Science Applications, </booktitle> <publisher> Prentice-Hall Inc., </publisher> <address> New Jersey, </address> <year> 1982. </year> <month> 23 </month>
References-found: 31

