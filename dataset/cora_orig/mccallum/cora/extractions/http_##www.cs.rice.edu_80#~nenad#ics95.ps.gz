URL: http://www.cs.rice.edu:80/~nenad/ics95.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~nenad/papers.html
Root-URL: 
Email: ken@cs.rice.edu nenad@cs.rice.edu sethi@cs.rice.edu  
Title: Efficient Address Generation for Block-Cyclic Distributions  
Author: Ken Kennedy Nenad Nedeljkovic Ajay Sethi 
Affiliation: Department of Computer Science, Rice University  
Note: Center for Research on Parallel Computation  
Abstract: Advanced features of modern data-parallel languages, such as High Performance Fortran, require new techniques in compilers and run-time systems. We present efficient methods for generating local memory addresses in programs containing references to arrays with cyclic(k) distributions. By exploiting the repetitive pattern of memory accesses we can handle arbitrary affine subscripts, while minimizing the space and time overhead. The effectiveness of our approach is demonstrated by extensive experimental results. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Ancourt, F. Coelho, F. Irigoin, and R. Keryell. </author> <title> A linear algebra framework for static HPF code distribution. </title> <booktitle> In Proceedings of the Fourth Workshop on Compilers for Parallel Computers, </booktitle> <address> Delft, The Netherlands, </address> <month> December </month> <year> 1993. </year>
Reference-contexts: Most of the previous efforts in compiling programs with cyclic (k) distributions have dealt only with fully parallel program segments (HPF array statements or independent loops), where array accesses can be enumerated in arbitrary order. This is true for the linear algebra framework by Ancourt et al. <ref> [1] </ref>, the virtual processor scheme by Gupta et al. [3], and the method based on the intersections of array slices by Stichnoth, O'Hallaron, and Gross [7].
Reference: [2] <author> S. Chatterjee, J. Gilbert, F. Long, R. Schreiber, and S. Teng. </author> <title> Generating local addresses and communication sets for data-parallel programs. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, CA, </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Chatterjee et al. exploit the periodicity of memory accesses to generate a table that can be used to traverse local regular section elements even if the code is not fully parallel <ref> [2] </ref>. They show that for an array with the cyclic (k) distribution the table has at most k entries and present a table construction algorithm that takes roughly O (k log k) time. <p> Chat-terjee et al. specify this sequence by its starting location and by the local memory gaps between every two successive array accesses on processor m <ref> [2] </ref>. <p> Therefore, we can generate this sequence using either the table of memory gaps (M ) or the demand-driven approach described in Section 2. For each iteration of the outer loop, the starting location for a given processor m can be computed by solving k linear Diophantine equations <ref> [2] </ref>. Since this incurs the O (k) cost per iteration, we must look for a more efficient solution. <p> Therefore, these should be the methods of choice if memory overhead needs to be reduced or completely eliminated. 4 Coupled subscripts For references to multidimensional arrays that have independent subscripts the memory access problem is solved by multiple applications of the algorithm for the one-dimensional case <ref> [2] </ref>. However, this might not be possible when the subscripts are coupled, i.e., when the deepest loop index variables occurring in two subscripts are identical. An example of a normalized loop with coupled subscripts is do j = 0; u enddo The table-based method by Chatterjee et al. [2] can be <p> one-dimensional case <ref> [2] </ref>. However, this might not be possible when the subscripts are coupled, i.e., when the deepest loop index variables occurring in two subscripts are identical. An example of a normalized loop with coupled subscripts is do j = 0; u enddo The table-based method by Chatterjee et al. [2] can be extended to handle array references with coupled subscripts. <p> a given processor (m 1 ; m 2 ) is determined by the smallest nonnegative integer j which satisfies both inequalities: k 1 m 1 (l 1 + s 1 j) mod p 1 k 1 &lt; k 1 (m 1 + 1); and Much like in the one-dimensional case <ref> [2] </ref>, such a j is found as the minimum of the smallest nonnegative solutions of the set of simultaneous linear Diophantine equations: Block Inner G S M No Run size stride M M Only tables time s j = 3 1.654 1.912 1.988 2.390 62.473 s j = 100 1.654 1.912 <p> In this paper, we have presented efficient techniques for generating local addresses for array references with arbitrary affine subscripts, in the presence of cyclic (k) distributions. We have improved on the table-based address generation scheme by Chatterjee et al. <ref> [2] </ref> in two ways. First, we have shown how the ideas from our linear-time table construction algorithm [5] can be used to generate local addresses without table lookups, with no, or only insignificant, performance degradation.
Reference: [3] <author> S.K.S. Gupta, S.D. Kaushik, C.-H. Huang, and P. Sadayap-pan. </author> <title> On compiling array expressions for efficient execution on distributed-memory machines. </title> <type> Technical Report OSE-CISRC-4/94-TR19, </type> <institution> Department of Computer and Information Science, The Ohio State University, </institution> <month> April </month> <year> 1994. </year>
Reference-contexts: This is true for the linear algebra framework by Ancourt et al. [1], the virtual processor scheme by Gupta et al. <ref> [3] </ref>, and the method based on the intersections of array slices by Stichnoth, O'Hallaron, and Gross [7]. Chatterjee et al. exploit the periodicity of memory accesses to generate a table that can be used to traverse local regular section elements even if the code is not fully parallel [2]. <p> a r k + b r and a l k + b l (Figure 2 (b)). 2.1 Alternative methods We compare the address generation methods described above with two alternative approaches: the run-time address resolution (also known as the guarded execution) and the virtual-block scheme proposed by Gupta et al. <ref> [3] </ref>. In the run-time resolution each processor executes the entire loop, and for each iteration it checks if it owns the array element that is being assigned to, in which case it computes the local memory address and performs the assignment.
Reference: [4] <author> S. Hiranandani, K. Kennedy, J. Mellor-Crummey, and A. Sethi. </author> <title> Compilation techniques for block-cyclic distributions. </title> <booktitle> In Proceedings of the 1994 ACM International Conference on Supercomputing, </booktitle> <address> Manchester, England, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: The local upper bound for a given processor can always be computed in constant time; details of this computation, shown in Figure 4 as the function LocUB, are described by Hiranandani et al. <ref> [4] </ref>. 3.1 Experimental results example loop nest. Both versions use the M table to generate local memory addresses in the inner loop; the starting locations are computed from the S table in Figure 4 (a) and the G table in Figure 4 (b).
Reference: [5] <author> K. Kennedy, N. Nedeljkovic, and A. Sethi. </author> <title> A linear-time algorithm for computing the memory access sequence in data-parallel programs. </title> <booktitle> In Proceedings of the Fifth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> Santa Barbara, CA, </address> <month> July </month> <year> 1995. </year>
Reference-contexts: They show that for an array with the cyclic (k) distribution the table has at most k entries and present a table construction algorithm that takes roughly O (k log k) time. In a related work <ref> [5] </ref>, we present a linear-time algorithm for constructing the table of local memory gaps described by fl This work was supported in part by ARPA contract DABT63-92-C-0038 and NSF Cooperative Agreement Number CCR-9120008. <p> We have developed an improved table construction algorithm with O (k + min (log s; log p)) running time <ref> [5] </ref>. <p> and its offset within the block to which it belongs (offset), the next element accessed by the same processor must be at the distance R (if offset + b r &lt; k), L (if offset + b r k and offset + b l 0), or R + L (otherwise) <ref> [5] </ref>. This is used to build the tables that for each offset in the range (0; k) contain the corresponding local memory gap and the next offset (M and Next in Figure 2 (a)). <p> We have improved on the table-based address generation scheme by Chatterjee et al. [2] in two ways. First, we have shown how the ideas from our linear-time table construction algorithm <ref> [5] </ref> can be used to generate local addresses without table lookups, with no, or only insignificant, performance degradation. Second, we have extended the table lookup method to references with multiple loop index variables and coupled array subscripts.
Reference: [6] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele, Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1994. </year>
Reference-contexts: 1 Introduction Data-parallel languages, such as High Performance Fortran (HPF) <ref> [6] </ref>, provide data layout directives that specify how array elements should be distributed across processors. The most general regular distribution is the block-cyclic distribution (cyclic (k) in HPF), in which contiguous blocks of size k are assigned to processors in a round-robin fashion (Figure 1).
Reference: [7] <author> J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Generating communication for array statements: Design, implementation, and evaluation. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 21(1) </volume> <pages> 150-159, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: This is true for the linear algebra framework by Ancourt et al. [1], the virtual processor scheme by Gupta et al. [3], and the method based on the intersections of array slices by Stichnoth, O'Hallaron, and Gross <ref> [7] </ref>. Chatterjee et al. exploit the periodicity of memory accesses to generate a table that can be used to traverse local regular section elements even if the code is not fully parallel [2].
References-found: 7

