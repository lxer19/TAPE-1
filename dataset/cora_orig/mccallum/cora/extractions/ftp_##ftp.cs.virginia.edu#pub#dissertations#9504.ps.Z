URL: ftp://ftp.cs.virginia.edu/pub/dissertations/9504.ps.Z
Refering-URL: ftp://ftp.cs.virginia.edu/pub/dissertations/README.html
Root-URL: http://www.cs.virginia.edu
Title: A Dissertation  Safety Kernel Enforcement of Software Safety Policies  
Author: Kevin G. Wika 
Degree: Presented to the Faculty of the  In Partial Fulfillment of the Requirements for the Degree Doctor of Philosophy (Computer Science)  
Date: May 1995  
Note: by  
Affiliation: School of Engineering and Applied Science University of Virginia  
Abstract-found: 0
Intro-found: 1
Reference: 1. <author> Addy, E. A., </author> <title> A Case Study on Isolation of Safety-Critical Software, </title> <booktitle> in Proceedings of COMPASS 1991, </booktitle> <address> Washington, D.C., </address> <pages> pp. 75-83. </pages>
Reference-contexts: Common techniques include watchdog timers, input and output assertions [32], sequencing checkers [44], fault tolerant data structures [48], software isolation <ref> [1] </ref>, and software self checking [20]. These techniques have been incorporated largely in an ad hoc fashion. Some of the systems that are presently the state of art in this area are described below.
Reference: 2. <author> Ames, S. R., Jr., M. Gasser and R. R. Schell, </author> <title> Security Kernel Design and Implementation: an Introduction, </title> <journal> IEEE Computer Vol. </journal> <note> 16-7 (July 1983) pp. 14-22. </note>
Reference-contexts: Informally, the goal of a security kernel is to provide assurance that a set of required fundamental properties of a computer system hold at all times during execution <ref> [2] </ref>. These properties are specified as security policies and are enforced by the security kernel independent of the application program. In other words, verification of the security kernel is sufficient to ensure enforcement of those policies encapsulated within the security kernel. <p> The following sections look at these requirements in more detail. 5.1.1 Exclusive Control Requirements Exclusive control of application devices is a requirement that is similar to the security kernel requirement for completeness <ref> [2] </ref>. Completeness in the security context means that a security kernel mediates all access to information, i.e., it has exclusive access to the information. Exclusive access is essential in security systems where any unauthorized access to information has the potential for communication. <p> Five requirements have been identified for reliable safety kernel operation. The fault tree in Fig. 10 shows the failures that the requirements address. The first two requirements, safety kernel correctness and data integrity are directly analogous to requirements that have been established for security kernels <ref> [2] </ref>. The third and fourth requirements are dependable support services and dependable computing services. Both of these are essential for security kernels, but are often assumed since security kernels have typically been built with very little underlying software.
Reference: 3. <author> Ammann, P. E., S. S. Brilliant, and J. C. Knight, </author> <title> The Effect of Imperfect Error Detection on Reliability Assessment via Life Testing, </title> <journal> IEEE Transactions on Software Engineering Vol. </journal> <month> 20-2 (February </month> <year> 1994). </year>
Reference-contexts: In particular, the only conclusion that can safely be made after testing has been performed is that the system functioned correctly on the inputs that were tested (note that even this conclusion depends on the potentially suspect assumption of perfect error detection <ref> [3] </ref>). In a practical system where the number of input combinations is huge, even extended testing will seldom test more than a small fraction of this number. Therefore, testing at the system level cannot be relied on as a verification technique in the sense of verification as a proof.
Reference: 4. <author> Anderson, T. Ed., </author> <title> Safe and Secure Computing Systems (Blackwell Scientific Publications, </title> <year> 1989). </year>
Reference-contexts: This has the additional benefit of simplifying application programs by freeing them from responsibility for implementation and verification of policies that are enforced by a kernel. The general concept of a security kernel is shown in Fig. 1. The similarity between security concerns and safety concerns is considerable <ref> [4] </ref>. Security kernels are used to enforce access-control policies in classified information systems. The idea of trying to exploit this technique to implement safety rather than security, i.e., the concept of a more general safety kernel, was proposed by Rushby [33,44], among others.
Reference: 5. <author> Anderson, T. and P. A. Lee, </author> <title> Fault Tolerance Principles and Practice (Prentice Hall International, </title> <publisher> Inc., </publisher> <address> London 1981) p. </address> <month> 64. </month>
Reference-contexts: The goal of safety engineering is to produce useful devices that are also safe. Notice that a particular device may be useful and safe yet have low availability and reliability. A system failure is said to have occurred when the system no longer complies with its specification <ref> [5] </ref>. This is an important definition because it illustrates the dependence placed on the specification. If nothing is specified about how a system is to behave under certain operating conditions then any behavior must be considered acceptable if those conditions arise.
Reference: 6. <author> Avizienis, A., </author> <title> The N -Version Approach to Fault-Tolerant Software, </title> <journal> IEEE Transactions on Software Engineering Vol. SE-11 (1985) pp. </journal> <pages> 1491-1501. </pages>
Reference: 7. <author> Bricker, A., M. Gien, M. Guillemont, J. Lipkis, D. Orr, and M. Rozier, </author> <title> Architectural Issues in Microkernel-Based Operating Systems: </title> <journal> the CHORUS Experience, Computer Communications Vol. </journal> <note> 14-6 (July/August 1991) pp. 347-357. </note>
Reference-contexts: These two alternatives are explored below. Implementing the safety kernel with supervisor status would provides protection from user processes, but can involve potentially expensive modification of the system kernel. However, some systems, for example the micro-kernel-oriented Chorus system, provide support for moving a process into the system kernel <ref> [7] </ref>. An additional concern, particularly in a monolithic system kernel is the lack of protection within the system kernel address space. The use of a microkernel operating system [7,23] or an object oriented system [10] would provide protection within the system kernel.
Reference: 8. <author> Brilliant, S.S., Knight, J.C., and Leveson, N.G., </author> <title> The Consistent Comparison Problem in N-Version software, </title> <journal> IEEE Transactions on Software Engineering Vol. </journal> <pages> 15-11 (November 1989) pp. 1481-1485. </pages>
Reference: 9. <author> Butler, R. W. and G. B. Finelli, </author> <title> The Infeasibility of Quantifying the Reliability of Life-Critical Real-Time Software, </title> <journal> IEEE Transactions on Software Engineering Vol. </journal> <note> 19-1 (January 1993) pp. 3-12. </note>
Reference-contexts: Even the best software development processes cannot ensure that faults are avoided completely during development. Similarly, fault detection techniques are imperfect. Research has shown, for example, that testing as an approach to verification cannot demonstrate sufficient levels of reliability because of the sheer number of tests that are required <ref> [9] </ref>. Building very small, simple software systems that achieve the extreme dependability necessary with safety-critical applications has proven to be sufficiently challenging.
Reference: 10. <author> Campbell, R. H., N. Islam, D. Raila, and P. Madany, </author> <title> Designing and Implementing CHOICES: An Object-Oriented System in C++, </title> <journal> CACM Vol. </journal> <note> 36-9 (Sept 1993) p. 117-126. </note>
Reference-contexts: An additional concern, particularly in a monolithic system kernel is the lack of protection within the system kernel address space. The use of a microkernel operating system [7,23] or an object oriented system <ref> [10] </ref> would provide protection within the system kernel. The other protection option is to imple 47 ment the safety kernel as a user-level process and utilize virtual memory for protection. The protection provided by virtual memory is in theory just as good as that available within the system kernel.
Reference: 11. <author> Chen, L., and A. Avizienis, </author> <title> N-version Programming: A Fault-Tolerance Approach to Reliability of Software Operation, </title> <booktitle> in Digest of papers of the 8th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Tolouse, France, </address> <year> 1978, </year> <pages> pp. 3-9. </pages>
Reference: 12. <author> Cristian, F., </author> <title> Basic Concepts and Issues in Fault-Tolerant Distributed Systems, Operating Systems of the 90s and Beyond (Springer-Verlag, </title> <note> Berlin 1991) pp. 118-149. 104 </note>
Reference: 13. <author> Eckhardt, D. E, and L. D. Lee, </author> <title> A Theoretical Basis for the Analysis of Multiver-sion Software Subject to Coincident Errors, </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> Vol. SE-11 (1985), </volume> <pages> pp. 1511-1517. </pages>
Reference: 14. <author> Eckhardt, D. E, and L. D. Lee, </author> <title> Fundamental Differences in the Reliability of N - Modular Redundancy and N -Version Programming, </title> <journal> The Journal of Systems and Software, </journal> <volume> Vol. </volume> <pages> 8 (1988) pp. 313-318. </pages>
Reference: 15. <author> Fraim, L. J., Scomp: </author> <title> A Solution to the Multilevel Security Problem, </title> <journal> IEEE Computer, </journal> <volume> Vol. </volume> <pages> 16-7 (July 1983) pp. 26-34. </pages>
Reference: 16. <author> Garman, J. R., </author> <title> The Bug Heard Round the World, </title> <journal> ACM Software Engineering Notes Vol. </journal> <note> 6-5 (October 1981) pp. 3-10. </note>
Reference: 17. <author> Gillies, G. T. et al, </author> <title> Magnetic Manipulation Instrumentation for Medical Physics Research, </title> <journal> Review of Scientific Instruments, </journal> <volume> Vol. </volume> <pages> 65-3 (March 1994) pp. 533 - 562. </pages>
Reference: 18. <author> Goodenough, J. B. and S. L. Gerhart, </author> <title> Toward a Theory of Test Data Selection, </title> <journal> IEEE Transactions on Software Engineering SE-1 (June 1975). </journal>
Reference-contexts: A system for performing this type of testing is described in Section 7.2. 77 Testing can also be employed in a manner that is equivalent to a proof <ref> [18] </ref>. Mathematical verification is used to demonstrate that certain properties, i.e., theorems, are true for two development artifacts. We are investigating the potential for demonstrating certain properties using testing. The technique which is described in Section 7.3 is used to demonstrate properties by exhaustively testing the required inputs.
Reference: 19. <author> Grady, M. S. et al, </author> <title> Preliminary Experimental Investigation of in vivo Magnetic Manipulation: Results and Potential Application in Hyperthermia, </title> <journal> Medical Physics Vol. </journal> <note> 16-2 (Mar/Apr. 1989) pp. 263 - 272. </note>
Reference: 20. <author> Higgs, J. C., </author> <title> A High Integrity Software Based Turbine Governing System, </title> <booktitle> in Proceedings of Safety of Computer Control Systems (SAFECOMP 83). </booktitle> <publisher> Pergamon, Elmsford, N.Y. </publisher> <pages> pp. 207-218. </pages>
Reference-contexts: Common techniques include watchdog timers, input and output assertions [32], sequencing checkers [44], fault tolerant data structures [48], software isolation [1], and software self checking <ref> [20] </ref>. These techniques have been incorporated largely in an ad hoc fashion. Some of the systems that are presently the state of art in this area are described below. <p> For economic reasons, availability is also a significant system goal. J. C. Higgs describes the design of a software-based electric turbine governing system known as MICROGOVERNOR that has several novel features <ref> [20] </ref>. The author cites the following: 13 1. Software assertions are utilized in order to obtain integrity through intelligent self checking rather than through duplex comparison. Failure of the self checks results in fail-safe hardware ensuring the safety of the system outputs. 2.
Reference: 21. <author> Joseph, </author> <title> M.K., Architectural Issues in Fault-Tolerant, Secure Computing Systems, </title> <type> Ph.D. Thesis, </type> <institution> UCLA, </institution> <address> Los Angeles, USA, </address> <year> 1988. </year>
Reference-contexts: Each has an impact on the final performance of a design-diverse software system, and the effects of each must be weighed against any benefits that design diversity might provide. Certainly there are benefits that can accrue from the use of design diversity, such as its application to secure systems <ref> [21] </ref>, but it is not a panacea for achieving high software dependability. It is possible that, by chance, the faults in the various implementations are sufficiently different that their effects are masked. But this cannot be assured with high probability.
Reference: 22. <author> Karger, P. A., et al, </author> <title> A Retrospective on the VAX VMM Security Kernel, </title> <journal> IEEE Transactions on Software Engineering, </journal> <note> 17-11 (Nov. 1991) pp. 1147-1165. </note>
Reference: 23. <author> Kirschen, D., </author> <title> An Overview of the Mach Operating System, </title> <journal> Operating Systems Technical Committee Newsletter, </journal> <volume> Vol. </volume> <pages> 3-2, </pages> <address> p. </address> <month> 57. </month>
Reference: 24. <author> Knight, J. C., A. G. Cass, A. M. Fernandez, and K. G. Wika, </author> <title> Testing a Safety-Critical Application, </title> <institution> Department of Computer Science, University of Virginia, </institution> <type> Technical Report No. </type> <institution> CS-94-08, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: Testing has been conducted using the test harness depicted in Fig. 26. This test harness permits automated testing of the safety kernel operating with control systems such as the UVAR and the MSS <ref> [24] </ref>. In this system, the safety kernel is executing along with the application software which in this case consists of an operator display and a control program. <p> The version of the test harness that has been developed for the MSS <ref> [24] </ref> is shown in Fig. 31. In this system, the devices for the MSS are six servoamplifier current controllers and two uoroscopic imaging systems. Each of the two imaging systems are simulated as two separate components: an x-ray source and an image generator.
Reference: 25. <author> Knight, J. C. and D. M. </author> <title> Kienzle.,Safety-Critical Computer Applications: The Role of Software Engineering, </title> <type> Technical Report TR-92-23, </type> <institution> Department of Computer Science, University of Virginia, </institution> <year> 1993. </year>
Reference-contexts: If the system safety analysis does not identify a critical software safety requirement and an accident results during system operation, this is a case of specification error and not an example of unsafe software <ref> [25] </ref>. 2.2 Safety-Critical Applications Various features that could be included in a safety kernel have been built into almost every safety-critical system utilizing software. Common techniques include watchdog timers, input and output assertions [32], sequencing checkers [44], fault tolerant data structures [48], software isolation [1], and software self checking [20]. <p> The second closed-loop control technique is a detection-and-response method that takes advantage of the fact that externally an exclusive control failure is indistinguishable from a failure of the device itself <ref> [25] </ref>. In either case, the operation of the device deviates from what has been directed by the safety kernel. Therefore, independent sensors, which are typically employed to detect and respond to device failures, can also be used to ensure exclusive control requirements.
Reference: 26. <author> Knight, J.C., and Leveson, N.G., </author> <title> An Empirical Study of Failure Probabilities in Multi-Version Software, </title> <booktitle> Digest of papers of the 16th International Symposium on Fault-Tolerant Computing, </booktitle> <address> Vienna, Austria, </address> <year> 1986, </year> <pages> pp 165-170. </pages>
Reference: 27. <author> Knight, J. C., and N. G. Leveson, </author> <title> An Experimental Evaluation of the Assumption of Independence in Multiversion Programming, </title> <journal> IEEE Transactions on Software 105 Engineering Vol. SE-12 (1986) pp. </journal> <pages> 96-109. </pages>
Reference: 28. <author> Kopetz, H., </author> <title> Event-Triggered Versus Time-Triggered Real-Time Systems, </title> <booktitle> Operating Systems of the 90s and Beyond (Springer-Verlag, Berlin 1991) pp. </booktitle> <pages> 86-101. </pages>
Reference-contexts: To enforce policies of this type, the safety kernel might maintain a timer queue that permits these aperiodic events to be scheduled. Ensuring schedulability in systems with aperiodic events is a general problem in real-time systems <ref> [28] </ref>. We will address this problem by requiring that aperiodic events be limited or that they occur at times when their invocation will not result in scheduling conicts. The interlock policies can be used to regulate the occurrence of aperiodic events.
Reference: 29. <author> Laprie, J. C., </author> <title> The Dependability Approach to Critical Computing Systems, </title> <booktitle> in Proceedings of the 1st European Conference On Software Engineering, </booktitle> <address> Strasbourg, France, </address> <year> 1987, </year> <month> pp.233-243. </month>
Reference-contexts: Previous work in these areas is reviewed in this chapter. 2.1 Building Dependable Software Dependability is defined as that property of a computing system which allows reliance to be justifiably placed on the service it delivers <ref> [29] </ref>. For software, the informal notion that most people have is Will the software perform as I wish?. This question is not in the least precise nor testable, and a clear statement of exactly what goal has to be met by software developers is necessary.
Reference: 30. <author> Leveson, N.G., </author> <title> Software Fault Tolerance in Safety-Critical Applications, </title> <booktitle> in Proceedings of the 3rd International Conference on Fault-Tolerant Computing Systems, </booktitle> <address> Bremerhaven, Germany, </address> <year> 1987. </year>
Reference: 31. <author> Leveson, N. G., </author> <title> Software Safety: Why, What, and How, </title> <journal> ACM Computing Surveys, </journal> <note> Vol. 18 (June 1986) pp. 125-163. </note>
Reference-contexts: Similarly, a specification must state what is required in terms of reliability, availability, and safety, and these requirements must be technically reasonable. The term software safety is used frequently concerning software in safety-critical systems <ref> [31] </ref>. It is generally accepted that because software acts through the other components in a system, the process of building software must focus on the role of the software in the system. <p> Dependable computing services The basic computing services of the computing platform must either function as specified or be fail-stop <ref> [31] </ref>. The memory pages of the kernel will be locked in place to obviate the need for reliable swapping. 51 6.
Reference: 32. <author> Leveson, N. G. and T. J. Shimeall, </author> <title> Safety Assertions for Process-Control Systems, </title> <booktitle> in Proceedings of 13th International Conference on Fault Tolerant Computing, </booktitle> <address> Milan, Italy, </address> <month> June, </month> <year> 1983. </year>
Reference-contexts: Common techniques include watchdog timers, input and output assertions <ref> [32] </ref>, sequencing checkers [44], fault tolerant data structures [48], software isolation [1], and software self checking [20]. These techniques have been incorporated largely in an ad hoc fashion. Some of the systems that are presently the state of art in this area are described below.
Reference: 33. <author> Leveson, N. G., T. J. Shimeall, J. L. Stolzy, and J. C. Thomas, </author> <title> Design for Safe Software, </title> <booktitle> in Proceedings AIAA Space Sciences Meeting, </booktitle> <address> Reno, Nevada, </address> <year> 1983. </year>
Reference-contexts: Others have suggested schemes (known by other names) that have some of the features of a safety kernel. These systems typically have some of the features of a security kernel with a relatively small component providing some form of support for software safety. 15 Leveson et al. <ref> [33] </ref> appear to be the first to have used the term safety kernel. They describe a concept based on a centralized location for a set of safety mechanisms. These mechanisms are used to enforce usage policies that are established for a given system.
Reference: 34. <author> Leveson, N. G. and C. S. Turner, </author> <title> An Investigation of the Therac-25 Accidents, </title> <journal> IEEE Computer, </journal> <note> Vol. 26-7 (July 1993) pp. 18 - 41. </note>
Reference: 35. <author> McCormick, N. J., </author> <title> Reliability and Risk Analysis (Academic Press, </title> <publisher> Inc., </publisher> <address> San Diego, CA, </address> <year> 1981). </year>
Reference-contexts: In a system safety analysis, an initial step is the identification of hazards that could result in injury or damage. Each hazard is subsequently placed at the root of a fault tree and the failures that could result in the hazard are analyzed <ref> [35] </ref>. From this analysis, a complete fault tree is developed which details the failure conditions that could lead to a particular hazard. The exact form of a fault tree depends on the hazard being considered and the details of the particular application.
Reference: 36. <author> Miller, D. R., </author> <title> Making Statistical Inferences About Software Reliability, </title> <type> NASA Contractor Report 4197, </type> <institution> NASA Langley Research Center, Hampton, Virginia, USA, </institution> <year> 1988. </year>
Reference: 37. <author> Miller, D. R., </author> <title> The role of Statistical Modeling and Inference in Software Quality Assurance, in Software Certification, </title> <editor> ed. B. de Neumann, </editor> <publisher> (Elsevier Applied Science, </publisher> <address> London, UK, </address> <note> 1989) pp. 135-152. </note>
Reference: 38. <author> Moffett, J. D. and J. A. McDermid, </author> <title> Policies for Safety-Critical Systems: the Challenge of Formalisation, </title> <booktitle> Fifth IFIP/IEEE International Workshop on Distributed Systems: Operations and Management, </booktitle> <address> Toulouse, France, </address> <month> Oct. </month> <year> 1994. </year>
Reference-contexts: The report also details other safety requirements and provides a fairly comprehensive list of the ad hoc techniques that are commonly applied to safety-critical systems. More recently, Moffett et al. <ref> [38] </ref> have proposed a concept for enforcement of policies in distributed systems. They identify two types of policies. Obligation policies describe actions that must be initiated in order to enforce the policy.
Reference: 39. <editor> NATO AC/310 Ad Hoc Working Group on Munition Related Safety Critical Computing Systems, </editor> <title> Safety Design Requirements and Guidelines for Munition Related Safety Critical Computing Systems, </title> <type> NATO Standardization Agreement (STANAG) 4404 (Draft), </type> <month> March </month> <year> 1990. </year>
Reference-contexts: Communications are monitored to enforce a required status as specified in a policy base. In particular, Rushby mentions the potential for enforcing policies that specify valid sequences of operations. A report by the NATO ad hoc Working Group on Munition Related Safety Critical Computing Systems <ref> [39] </ref> mentions a safety kernel that it defines as follows: Safety Kernel: An independent computer program that monitors the state of the system to determine when potentially unsafe system states occur or when transitions to potentially unsafe system states may occur.
Reference: 40. <author> Neumann, P. G., </author> <title> On Hierarchical Design of Computer Systems for Critical Applications, </title> <journal> IEEE Transactions on Software Engineering Vol. </journal> <note> SE-12 (September 1986) pp. 905-920. </note>
Reference-contexts: The distinction is made to demonstrate that the term kernel, as used here, is not directly analogous to its use in the security context. Neumann <ref> [40] </ref> considers the idea of a safety-trusted computing base as a part of his examination of whether the hierarchical design familiar in secure systems could be generalized to other critical applications.
Reference: 41. <author> Neumann, P.G., </author> <title> Editor, Risks to the Public, </title> <booktitle> Software Engineering Notes. </booktitle> <pages> 106 </pages>
Reference: 42. <author> Parnas, D. L., </author> <title> On the Criteria to be Used in Decomposing Systems Into Modules, </title> <journal> Communications of the ACM Vol. </journal> <note> 15 (Dec. 1972) pp. 220-225. </note>
Reference-contexts: Neumann [40] considers the idea of a safety-trusted computing base as a part of his examination of whether the hierarchical design familiar in secure systems could be generalized to other critical applications. In describing a hierarchical design approach Neumann relies on the standard uses relation <ref> [42] </ref>, but also introduces the notion of associating degrees of criticality with the design levels. Degrees of criticality are applied in secure systems with the most critical component, the security kernel, occupying the lowest level.
Reference: 43. <author> Prieto-Daz, R., </author> <title> Status Report: Software Reusability, </title> <journal> IEEE Software Vol. </journal> <note> 10-5 (May 1993) pp. 61-66. </note>
Reference-contexts: Such a reuse-oriented framework would promote the transfer of innovative and effective concepts and artifacts from one system to another. Reuse can be applied at many levels from abstract general system knowledge to very specific software artifacts <ref> [43] </ref>. The appropriate level of reuse for a class of systems depends on the commonalities between systems and the identification of general require 8 ments or characteristics. <p> However, building from scratch is typically expensive and more importantly does not promote the transfer of innovative and effective concepts and artifacts from one system to another. The transfer of information of this type can positively impact both the cost and reliability of the system <ref> [43] </ref>. This transfer of information is known as reuse and is an important area of research in the field of software engineering. We are motivated to exploit reuse because of the potential cost and reliability benefits.
Reference: 44. <author> Rushby, J., </author> <title> Kernels for Safety?, in Safe and Secure Computing Systems, </title> <editor> T. Anderson Ed. </editor> <publisher> (Blackwell Scientific Publications, </publisher> <pages> 1989) pp. 210-220. </pages>
Reference-contexts: Common techniques include watchdog timers, input and output assertions [32], sequencing checkers <ref> [44] </ref>, fault tolerant data structures [48], software isolation [1], and software self checking [20]. These techniques have been incorporated largely in an ad hoc fashion. Some of the systems that are presently the state of art in this area are described below. <p> This is in contrast to security systems where a relatively concise policy can be formulated and subsequently guaranteed by the security kernel and trusted functions. Rushby has made the strongest theoretical argument for the development of a safety kernel <ref> [44] </ref>. In the process, he has more clearly defined the role of a safety kernel and addressed the concern raised by Neumann. Rushby considers whether the concept of a small component that guarantees the enforcement of some system policy (typically security) could be applied to safety-critical software systems.
Reference: 45. <author> Rushby, J. and B. Randell, </author> <title> A Distributed Secure System, </title> <journal> IEEE Computer Vol. </journal> <note> 16-7 (July 1983) pp. 55-67. </note>
Reference: 46. <author> Siewiorek, </author> <title> D.P., and Swarz, R.S., The Theory and Practice of Reliable System Design (Digital Press, </title> <address> Bedford, MA, USA, </address> <year> 1982). </year>
Reference-contexts: When considered more carefully, it is clear that dependability has many different meanings. The three most important are reliability, availability, and safety: Reliability Reliability is the probability that a particular device will function as required in a specified environment for a particular period of time <ref> [46] </ref>. The notion of reliability is important for devices that must provide continuous service. For example, even a momentary failure of some implanted therapeutic devices is likely to have very seri ous consequences. <p> For example, even a momentary failure of some implanted therapeutic devices is likely to have very seri ous consequences. Availability Availability is the probability that a particular device will be able to provide service at a particular time <ref> [46] </ref>. The notion of availability is important for devices that provide service where brief outages are acceptable; for some devices, even frequent outages will not cause harm provided they are very brief. <p> Safety-critical systems have different requirements, however, so this design may not be necessary or appropriate in the safety context. Reliability is the probability that a particular device will function as required in a specified environment for a particular period of time <ref> [46] </ref>. Therefore, reliable policy enforcement requires that the safety kernel enforce safety policies and do so with an acceptable probability of functioning for a required period of time. In this chapter, the general requirements for reliable policy enforcement are examined.
Reference: 47. <author> Tanenbaum, A. S., R. van Renesse, H. van Staveren, G. J. Sharp, S. J. Mullender, J. Jansen, and G. van Rossum, </author> <title> Experiences with the Amoeba Distributed Operating System, </title> <journal> CACM, </journal> <note> 33-12 (Dec. 1990) pp. 46-63. </note>
Reference-contexts: Unfortunately, although this would prevent direct access, it does not help with access resulting from failures in the computing platform. A more effective authentication technique utilizes capabilities <ref> [47] </ref>. A capability is typically a bit pattern that is sufficiently long to make it improbable to be arrived at randomly. For an authentication technique, the capability serves as a key to a device.
Reference: 48. <author> Taylor, D. J., D. E. Morgan, and J. P. Black, </author> <title> Redundancy in Data Structures: Improving Software Fault Tolerance, </title> <journal> IEEE Transactions on Software Engineering Vol. </journal> <note> SE-6 (Nov. 1980) pp. 585-594. </note>
Reference-contexts: Common techniques include watchdog timers, input and output assertions [32], sequencing checkers [44], fault tolerant data structures <ref> [48] </ref>, software isolation [1], and software self checking [20]. These techniques have been incorporated largely in an ad hoc fashion. Some of the systems that are presently the state of art in this area are described below. <p> Operation During operation, the primary data integrity concern is that some entity in the computer system will be able to access and alter memory that is critical to policy enforcement. Although, some sort of fault tolerant data structures <ref> [48] </ref> might be effective for detecting corruption of this type, there is no means for detecting corruption of all of the data because much of this data is never accessed directly by the safety kernel (e.g., process state data).
Reference: 49. <author> Taylor, R. H., et. al., </author> <title> Augmentation of human precision in computer-integrated surgery, </title> <booktitle> Innovation and Technology in Biology and Medicine Vol. 13 (1992) pp. </booktitle> <pages> 450-468. </pages>
Reference: 50. <author> Taylor, R. H., et al., </author> <title> Taming the Bull: Safety in a Precise Surgical Robot, </title> <booktitle> in Proceedings Fifth International Conference on Advanced Robotics, </booktitle> <address> Pisa, Italy, </address> <month> June </month> <year> 1991, </year> <pages> pp. 865-870. </pages>
Reference: 51. <author> Toy, W. N., </author> <title> Fault-Tolerant Design of Local ESS Processors, </title> <booktitle> Proc. IEEE Vol. 66 (Oct. 1978) pp. </booktitle> <pages> 1126-1145. </pages>
Reference-contexts: This presents a potentially easier implementation and verification task than if the support software needed to achieve high reliability. For example, this could be achieved in the hardware with a dual redundant system that compared the results of each operation and stopped if there was a discrep ancy <ref> [51] </ref>. 5. Dependable computing services The basic computing services of the computing platform must either function as specified or be fail-stop [31]. The memory pages of the kernel will be locked in place to obviate the need for reliable swapping. 51 6.
Reference: 52. <institution> University of Virginia Reactor Safety Committee, University of Virginia Reactor Design and Analysis Handbook, </institution> <note> last modified July 7, </note> <year> 1989. </year>
Reference-contexts: The approach that we have taken to address these questions is to conduct an experiment in which the two safety-critical applications provide a target for study of safety policies. For each application, safety policy data was gathered from a range of sources including reference to approved safety documents <ref> [52] </ref> and interviews with systems engineers. Although likely not complete, the safety policy data represents a wide variety of requirements and we believe it is sufficiently comprehensive to permit the questions posed above to be answered.
Reference: 53. <author> Wika, K. G., </author> <title> A User Interface and Control Algorithm for the Video Tumor Fighter, </title> <type> Masters Thesis, </type> <institution> University of Virginia, </institution> <month> May </month> <year> 1991. </year> <month> 107 </month>
References-found: 53

