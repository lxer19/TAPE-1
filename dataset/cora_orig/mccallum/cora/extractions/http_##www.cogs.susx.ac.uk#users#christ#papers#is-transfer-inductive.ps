URL: http://www.cogs.susx.ac.uk/users/christ/papers/is-transfer-inductive.ps
Refering-URL: http://www.cogs.susx.ac.uk/users/christ/index-noframes.html
Root-URL: 
Email: Email: Chris.Thornton@cogs.susx.ac.uk  
Phone: Tel: (44)1273 678856  
Title: Is Transfer Inductive?  
Author: Chris Thornton 
Date: November 27, 1996  
Web: WWW: http://www.cogs.susx.ac.uk  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Work is currently underway to devise learning methods which are better able to transfer knowledge from one task to another. The process of knowledge transfer is usually viewed as logically separate from the inductive procedures of ordinary learning. However, this paper argues that this `seperatist' view leads to a number of conceptual difficulties. It offers a task analysis which situates the transfer process inside a generalised inductive protocol. It argues that transfer should be viewed as a subprocess within induction and not as an independent procedure for transporting knowledge between learning trials.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rumelhart, D., Hinton, G. and Williams, R. </author> <year> (1986). </year> <title> Learning representa tions by back-propagating errors. </title> <booktitle> Nature, </booktitle> <pages> 323 (pp. 533-6). </pages>
Reference-contexts: The learner should, we feel, be able to transfer knowledge from one task to another. Unfortunately, popular learning methods such as backpropagation <ref> [1] </ref> often exhibit erratic transfer effects [2].
Reference: [2] <author> Murre, J. </author> <title> (Forthcoming). Transfer of learning in backpropagation and in related neural network models. </title> <editor> In J. Levy, D. Bairaktaris, J. Bullinaria and P. Cairns (Eds.), </editor> <title> Connectionist Models of Memory and Language. </title> <publisher> London: UCI Press. </publisher>
Reference-contexts: The learner should, we feel, be able to transfer knowledge from one task to another. Unfortunately, popular learning methods such as backpropagation [1] often exhibit erratic transfer effects <ref> [2] </ref>.
Reference: [3] <author> Harvey, I. and Stone, J. </author> <year> (1995). </year> <title> Unicycling helps your french: spontaneous recovery of associations by learning unrelated tasks. </title> <type> CSRP 379, </type> <institution> School of Cognitive and Computing Sciences, University of Sussex. </institution>
Reference-contexts: The learner should, we feel, be able to transfer knowledge from one task to another. Unfortunately, popular learning methods such as backpropagation [1] often exhibit erratic transfer effects [2]. Sometimes positive transfer effects are obtained 1 but sometimes they 1 In fact Harvey and Stone <ref> [3] </ref> argue that there is always a positive, initial transfer effect with backpropagation learning 1 are exactly reverse of what we want: the acquisition of new knowledge appears to catastrophically interfere with existing knowledge [4].
Reference: [4] <author> McCloskey, M. and Cohen, N. </author> <year> (1989). </year> <title> Catastrophic interference in connec tionist networks: the sequential learning problem. In G.H. </title> <editor> Bower (Ed.), </editor> <booktitle> The Psychology of Learning and Motivation. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference-contexts: Sometimes positive transfer effects are obtained 1 but sometimes they 1 In fact Harvey and Stone [3] argue that there is always a positive, initial transfer effect with backpropagation learning 1 are exactly reverse of what we want: the acquisition of new knowledge appears to catastrophically interfere with existing knowledge <ref> [4] </ref>. Many workers are engaged in the attempt to realise the benefits of knowledge tranfer within learning [cf. 5, 6, 7, 8]. 2 However, there seems to be some residual fuzziness in our thinking about the relationship between transfer and learning.
Reference: [5] <author> Schmidhuber, J. </author> <year> (1996). </year> <title> A theoretical foundation for multi-agent learn ing and incremental self-improvement in unrestricted environments. </title> <editor> In X. Yao (Ed.), </editor> <booktitle> Evolutionary Computation: Theory and Applications. </booktitle> <address> Singa-pore: </address> <publisher> Scientific Publishing Company. </publisher>
Reference: [6] <author> Caruana, R. </author> <year> (1995). </year> <title> Learning many related tasks at the same time with backpropagation. </title> <booktitle> Advances in Neural Information Processing Systems 7 (Proceedings of NIPS-94) (pp. </booktitle> <pages> 657-664). </pages>
Reference-contexts: Many workers are engaged in the attempt to realise the benefits of knowledge tranfer within learning <ref> [cf. 5, 6, 7, 8] </ref>. 2 However, there seems to be some residual fuzziness in our thinking about the relationship between transfer and learning. In particular, different assumptions are made about the way in which these two processes interact.
Reference: [7] <author> Martin, J. and Billman, D. </author> <year> (1994). </year> <title> Acquiring and combining overlapping concepts. </title> <booktitle> Machine Learning, </booktitle> <pages> 16 (pp. 1-37). </pages>
Reference-contexts: Many workers are engaged in the attempt to realise the benefits of knowledge tranfer within learning <ref> [cf. 5, 6, 7, 8] </ref>. 2 However, there seems to be some residual fuzziness in our thinking about the relationship between transfer and learning. In particular, different assumptions are made about the way in which these two processes interact.
Reference: [8] <author> Pratt, L. </author> <year> (1994). </year> <title> Experiments on the transfer of knowledge between neural networks. </title> <editor> In S. Hanson, G. Drastal and R. Rivest (Eds.), </editor> <booktitle> Computational Learning Theory and Natural Learning Systems, Constraints and Prospects (pp. </booktitle> <pages> 523-560). </pages> <publisher> MIT Press. </publisher>
Reference-contexts: Many workers are engaged in the attempt to realise the benefits of knowledge tranfer within learning <ref> [cf. 5, 6, 7, 8] </ref>. 2 However, there seems to be some residual fuzziness in our thinking about the relationship between transfer and learning. In particular, different assumptions are made about the way in which these two processes interact.
Reference: [9] <author> Thornton, C. </author> <year> (1995). </year> <title> Measuring the difficulty of specific learning problems. </title> <journal> Connection Science, </journal> <volume> 7, No. </volume> <pages> 1 (pp. 81-92). </pages>
Reference-contexts: There are thus conceptual problems to deal with whether we treat transfer as separate from induction or as closely integrated with it. To try to resolve these I present a task analysis of induction <ref> [9] </ref>. This differs from some theoretical treatments of learning (e.g., COLT treatments such as [10]) since it concentrates exclusively on properties of the induction problem and ignores possible solutions altogether.
Reference: [10] <author> Kearns, M. </author> <year> (1990). </year> <title> The Computational Complexity of Machine Learning. </title> <publisher> The MIT Press. </publisher>
Reference-contexts: There are thus conceptual problems to deal with whether we treat transfer as separate from induction or as closely integrated with it. To try to resolve these I present a task analysis of induction [9]. This differs from some theoretical treatments of learning (e.g., COLT treatments such as <ref> [10] </ref>) since it concentrates exclusively on properties of the induction problem and ignores possible solutions altogether.
Reference: [11] <author> Shavlik, J. and Dietterich, T. (Eds.) </author> <year> (1990). </year> <booktitle> Readings in Machine Learning. </booktitle> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher> <pages> 8 </pages>
Reference-contexts: The development of these methods is the concern of several research communities including Machine Learning and Connectionism (see <ref> [11, 12, 13] </ref>). 4 Of course this is not the only significant conditional probability. 5 If this seems counter-intuitive note that the third formula acts as a kind of catch-all since it covers any computational, mathematical or functional justification for an inductive guess. 6 In doing so we make the unrealistic
Reference: [12] <author> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </author> <year> (1983). </year> <title> Machine Learn ing: </title> <booktitle> An Artificial Intelligence Approach. </booktitle> <address> Palo Alto: </address> <publisher> Tioga. </publisher>
Reference-contexts: The development of these methods is the concern of several research communities including Machine Learning and Connectionism (see <ref> [11, 12, 13] </ref>). 4 Of course this is not the only significant conditional probability. 5 If this seems counter-intuitive note that the third formula acts as a kind of catch-all since it covers any computational, mathematical or functional justification for an inductive guess. 6 In doing so we make the unrealistic
Reference: [13] <author> Michalski, R., Carbonell, J. and Mitchell, T. (Eds.) </author> <year> (1986). </year> <title> Machine Learn ing: </title> <booktitle> An Artificial Intelligence Approach: Vol II. </booktitle> <address> Los Altos: </address> <publisher> Morgan Kauf-mann. </publisher>
Reference-contexts: The development of these methods is the concern of several research communities including Machine Learning and Connectionism (see <ref> [11, 12, 13] </ref>). 4 Of course this is not the only significant conditional probability. 5 If this seems counter-intuitive note that the third formula acts as a kind of catch-all since it covers any computational, mathematical or functional justification for an inductive guess. 6 In doing so we make the unrealistic
Reference: [14] <author> Thornton, C. </author> <year> (1994). </year> <title> Statistical biases in backpropagation learning. </title> <booktitle> Pro ceedings of the International Conference on Artificial Neural Networks (pp. </booktitle> <pages> 709-712). </pages> <address> Sorrento, Italy. </address>
Reference-contexts: The general implication is that explicit justification is more easily exploited than implicit justification. It is no surprise, then, to find that practical learning methods tend to be predisposed towards the former approach, i.e., they tend to exploit probabilities of the explicit form rather than of the implicit form <ref> [14] </ref>. In this analysis no assumptions are made about the imaginary function g or about how it behaves. However, we can say that it must be doing something more than simply testing for explicit patterns of absolute variable values.
Reference: [15] <author> Dietterich, T., London, B., Clarkson, K. and Dromey, G. </author> <year> (1982). </year> <title> Learn ing and inductive inference. </title> <editor> In P. Cohen and E. Feigenbaum (Eds.), </editor> <booktitle> The Handbook of Artificial Intelligence: Vol III. </booktitle> <address> Los Altos: </address> <publisher> Kaufmann. </publisher>
Reference-contexts: This is a satisfying connection to make since it allows us to say that `hard' learning problems | i.e., those which involve exploitation of implicit justification| are relational. This reaffirms the long-standing Machine Learning heuristic that `learning relationships is hard' (cf. <ref> [15] </ref>). Problems which merely involve exploitation of explicit probabilities can be viewed as `statistical', since they can be solved by deriving frequency statistics over a finite dataset.
Reference: [16] <author> Quinlan, J. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <booktitle> Machine Learning, </booktitle> <pages> 1 (pp. 81-106). </pages>
Reference-contexts: This taxonomy is, of course, purely analytic. In 5 practice it may be hard to allocate a particular method to a particular category. A small number of cases can be conclusively classified within the scheme. The ID3 method <ref> [16] </ref>, now more often used in its updated manifestation as C4.5 [17] is a case in point.
Reference: [17] <author> Quinlan, J. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <address> San Mateo, California: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This taxonomy is, of course, purely analytic. In 5 practice it may be hard to allocate a particular method to a particular category. A small number of cases can be conclusively classified within the scheme. The ID3 method [16], now more often used in its updated manifestation as C4.5 <ref> [17] </ref> is a case in point. ID3 takes a training set of sample input/output pairs from an input/output mapping, and constructs a decision tree (for generating outputs) by recursively partitioning the training set until every pair in a given partition has the same output value.
Reference: [18] <author> Breiman, L., Friedman, J., Olshen, R. and Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth. </publisher>
Reference-contexts: The algorithm is thus guided only by statistical effects in the training data. It is thus an exclusively statistical method. Aside from ID3, learning methods which can be classified as exclusively statistical include the CART algorithms <ref> [18] </ref>, the competitive learning regime of Rumelhart and Zipser [19], the Kohonen net [20] and in fact any algorithmic method which is based on the method of clustering [21]. There are also examples of exclusively relational learning methods.
Reference: [19] <author> Rumelhart, D. and Zipser, D. </author> <year> (1986). </year> <title> Feature discovery by competitive learning. </title> <editor> In D. Rumelhart, J. McClelland and the PDP Research Group (Eds.), </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructures of Cognition. </booktitle> <volume> Vol I (pp. </volume> <pages> 151-193). </pages> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: The algorithm is thus guided only by statistical effects in the training data. It is thus an exclusively statistical method. Aside from ID3, learning methods which can be classified as exclusively statistical include the CART algorithms [18], the competitive learning regime of Rumelhart and Zipser <ref> [19] </ref>, the Kohonen net [20] and in fact any algorithmic method which is based on the method of clustering [21]. There are also examples of exclusively relational learning methods.
Reference: [20] <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-organization and Associative Memory. </title> <publisher> Berlin: Springer-Verlag. </publisher>
Reference-contexts: The algorithm is thus guided only by statistical effects in the training data. It is thus an exclusively statistical method. Aside from ID3, learning methods which can be classified as exclusively statistical include the CART algorithms [18], the competitive learning regime of Rumelhart and Zipser [19], the Kohonen net <ref> [20] </ref> and in fact any algorithmic method which is based on the method of clustering [21]. There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as [26; 27; 28; 29].
Reference: [21] <author> Diday, E. and Simon, J. </author> <year> (1980). </year> <title> Clustering analysis. </title> <editor> In K. Fu (Ed.), </editor> <title> Digital Pattern Recognition. </title> <journal> Communications and Cybernetics, </journal> <volume> No. </volume> <pages> 10 (pp. 47-92). </pages> <address> Berlin: </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Aside from ID3, learning methods which can be classified as exclusively statistical include the CART algorithms [18], the competitive learning regime of Rumelhart and Zipser [19], the Kohonen net [20] and in fact any algorithmic method which is based on the method of clustering <ref> [21] </ref>. There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as [26; 27; 28; 29].
Reference: [22] <author> Langley, P. </author> <year> (1977). </year> <title> Rediscovering physics with bacon-3. </title> <booktitle> Proceedings of the Fifth International Joint Conference on Artificial Intelligence: Vol I. </booktitle>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers <ref> [22; 23; 24; 25] </ref> and related methods such as [26; 27; 28; 29].
Reference: [23] <author> Langley, P. </author> <year> (1978). </year> <title> BACON.1: a general discovery system. </title> <booktitle> Proceedings of the Second National Conference of the Canadian Society for Computational Studies in Intelligence (pp. </booktitle> <pages> 173-180). </pages> <address> Toronto. </address>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers <ref> [22; 23; 24; 25] </ref> and related methods such as [26; 27; 28; 29].
Reference: [24] <author> Langley, P., Bradshaw, G. and Simon, H. </author> <year> (1983). </year> <title> Rediscovering chemistry with the BACON system. </title> <editor> In R. Michalski, J. Carbonell and T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach (pp. </booktitle> <pages> 307-329). </pages> <address> Palo Alto: </address> <publisher> Tioga. </publisher> <pages> 9 </pages>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers <ref> [22; 23; 24; 25] </ref> and related methods such as [26; 27; 28; 29].
Reference: [25] <author> Langley, P., Simon, H., Bradshaw, G. and Zytkow, J. </author> <year> (1987). </year> <title> Scientific Dis covery: Computational Explorations of the Creative Processes. </title> <address> Cambridge, Mass.: </address> <publisher> MIT Press. </publisher>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers <ref> [22; 23; 24; 25] </ref> and related methods such as [26; 27; 28; 29].
Reference: [26] <author> Wolff, J. </author> <year> (1978). </year> <title> Grammar discovery as data compression. </title> <booktitle> Proceedings of the AISB/GI conference on Artificial Intelligence (pp. </booktitle> <pages> 375-379). </pages> <address> Hamburg. </address>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as <ref> [26; 27; 28; 29] </ref>.
Reference: [27] <author> Wolff, J. </author> <year> (1980). </year> <title> Data compression, generalisation and overgeneralisation in an evolving theory of language development. </title> <booktitle> Proceedings of the AISB-80 conference on Artificial Intelligence. </booktitle> <address> Amsterdam. </address>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as <ref> [26; 27; 28; 29] </ref>.
Reference: [28] <author> Lenat, D. </author> <year> (1982). </year> <title> AM: discovery in mathematics as heuristic search. </title> <editor> In R. Davis and D.B. Lenat (Eds.), </editor> <booktitle> Knowledge-Based Systems in Artificial Intelligence (pp. </booktitle> <pages> 1-225). </pages> <address> New York: </address> <publisher> McGraw-Hill. </publisher>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as <ref> [26; 27; 28; 29] </ref>.
Reference: [29] <author> Wnek, J. and Michalski, R. </author> <year> (1994). </year> <title> Hypothesis-driven constructive induc tion in AQ17-HCI: a method and experiments. </title> <booktitle> Machine Learning, </booktitle> <address> 14 (p. 139). Boston: </address> <publisher> Kluwer Academic Publishers. </publisher> <pages> 10 </pages>
Reference-contexts: There are also examples of exclusively relational learning methods. Examples include the `BACON' methods of Langley and co-workers [22; 23; 24; 25] and related methods such as <ref> [26; 27; 28; 29] </ref>.
References-found: 29

