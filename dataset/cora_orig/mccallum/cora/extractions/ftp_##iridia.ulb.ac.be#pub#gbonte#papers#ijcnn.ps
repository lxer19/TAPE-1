URL: ftp://iridia.ulb.ac.be/pub/gbonte/papers/ijcnn.ps
Refering-URL: http://iridia.ulb.ac.be/~gbonte/Papers.html
Root-URL: 
Email: email:bersini,mbiro,gbonte@iridia.ulb.ac.be  
Title: Adaptive Memory Based Regression Methods  
Author: H. Bersini, M. Birattari, G. Bontempi 
Note: IJCNN98 Topics 33) Supervised Learning 26) Prediction  
Date: 194/6  
Address: CP  50, av. Franklin Roosevelt 1050 Bruxelles Belgium  
Affiliation: Iridia  Universit Libre de Bruxelles  
Abstract: The task of approximating a non linear mapping using a limited number of observations, asks the data analyst to make several choices involving the set of relevant variables and observations, the learning algorithm, and the validation protocol. In the case of models which are linear in the parameters (e.g. polynomials), statistical theory and economical cross-validation methods provide fast and effective ways to support these choices. However, when pure approximation performance is at stake, a unique linear structure to cover the whole range of data, is often far from optimal. Memory-based methods in contrast are well known to considerably improve the approximation performance, since all the regression analysis is done locally and repeated for each new query. In this paper, we discuss the use of these cross-validation procedures for selecting the features, the neighbors and the polynomial degree for each prediction. The possible automation of these selections on a query basis provides memory-based methods (generally not used in such a flexible way) with a larger degree of adaptivity. Experimental results in time series prediction are presented. 
Abstract-found: 1
Intro-found: 1
Reference: [ 1 ] <author> Atkeson, C.G., Moore, A.W., Schaal, S. </author> <year> (1995), </year> <title> Locally weighted Learning, </title> <note> Carnegie Mellon University Technical Report submitted to Artificial Intelligence Review. </note>
Reference-contexts: In this paper we will show how the adaptive memory-based methodology is not simply a different tool for better prediction, but a flexible methodology to select online the features, the number of relevant observations and the structure of the local model. Starting from the work of Atkeson at al. <ref> [ 1 ] </ref> and of Cleveland [ 6 ] on local weighted regression and from the one of Hastie and Tibshirani [ 10 ] on classification, we propose an adaptive methodology for nonlinear data analysis where methods and tools from linear statistics are intensively used.
Reference: [ 2 ] <author> Atkeson, C.G. </author> <year> (1992), </year> <title> Memory-based approaches to approximating continuous functions, </title> <editor> in M.Casdagli and S. Eubank (eds.), </editor> <booktitle> Nonlinear modeling and forecasting, </booktitle> <pages> pp 503-521, </pages> <publisher> Addison Wesley. </publisher>
Reference-contexts: Vapnik [ 16 ] showed that the minimization of the risk functional starting from the empirical risk is an ill-posed problem unless some a priori assumption about the functional dependence is made (e.g. regularization [ 9 ] ). Memory-based methods <ref> [ 2 ] </ref> aim to solve the function approximation problem taking the opposite direction: given that the problem of functional estimation is hard to be solved in a generic setting, why not to try a more restricted set of linear models and approximate the function only in the neighborhood of the
Reference: [ 3 ] <author> Bersini, H., Bontempi G., </author> <year> (1997), </year> <title> Now comes the time to defuzzify the neuro-fuzzy models, </title> <journal> Fuzzy Sets and Sytems 90,2, </journal> <pages> pp. 161-170. </pages>
Reference-contexts: There are many other examples in literature where the idea of decomposition appears (e.g. regression trees [ 5 ] , mixture models [ 11 ] and neuro-fuzzy inference systems <ref> [ 3 ] </ref> ). However, unlike the memory-based approach, in these modular architectures the learning process obeys a global function estimation criterion.
Reference: [ 4 ] <author> Bishop, </author> <title> C.M. (1994) Neural Networks for Statistical Pattern Recognition, </title> <publisher> Oxford University Press. </publisher>
Reference-contexts: 1. Introduction Function approximation consists of the estimation of the regression function which assigns to each input x a number y (x) equal to the conditional expectation of the scalar y (regression estimation problem <ref> [ 4 ] </ref> ): y x F dy ( ) ( | ) = y y x =E [y | x] In the classical neural approach, the problem of learning an input-output mapping is postulated as a problem of function estimation, that is of choosing from a given set of parametric
Reference: [ 5 ] <author> Breiman, L., Friedman, J.H., Olshen, R.A., Stone, </author> <month> C.J </month> <year> (1984), </year> <title> Classification and Regression Trees Belmont, </title> <booktitle> CA:Wadsworth International Group. </booktitle>
Reference-contexts: This gives the data analyst the opportunity to exploit a set of theoretical results and techniques from the field of linear statistical analysis, otherwise useless in non linear domains. There are many other examples in literature where the idea of decomposition appears (e.g. regression trees <ref> [ 5 ] </ref> , mixture models [ 11 ] and neuro-fuzzy inference systems [ 3 ] ). However, unlike the memory-based approach, in these modular architectures the learning process obeys a global function estimation criterion.
Reference: [ 6 ] <author> Cleveland, W.S., </author> <year> (1979), </year> <title> Robust locally-weighted regression and smoothing scatterplots, </title> <journal> Journal of the American Statistical Society, </journal> <volume> vol. 74, </volume> <pages> pp. 829-836. </pages>
Reference-contexts: Starting from the work of Atkeson at al. [ 1 ] and of Cleveland <ref> [ 6 ] </ref> on local weighted regression and from the one of Hastie and Tibshirani [ 10 ] on classification, we propose an adaptive methodology for nonlinear data analysis where methods and tools from linear statistics are intensively used.
Reference: [ 7 ] <author> Cleveland, W.S., Devlin, S.J., Grosse, E. </author> <year> (1988), </year> <title> Regression by local fitting: Methods, properties and computational algorithms, </title> <journal> J. Econometrics, </journal> <volume> vol. 37, </volume> <pages> pp. 87-114. </pages>
Reference-contexts: In the first one, each possible degree, from 0 (the constant model) to a given maximum, is evaluated on all the possible neighborhoods using all the possible regressors combination. A second more sophisticated alternative considers as local approximators polynomial mixings which are polynomials of fractional degree <ref> [ 7 ] </ref> .
Reference: [ 8 ] <author> Efron, B., Tibshirani, </author> <title> R.J. (1993) An introduction to the bootstrap, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference-contexts: To this aim, we simply import tools and techniques from the field of linear statistical analysis. The most important and effective of these tools is the PRESS statistic [ 12 ] , which is a simple, well-founded and economical way to perform leave-one-out cross validation <ref> [ 8 ] </ref> and therefore to assess the performance in generalization of local linear models. Due to its short computation time which allows its intensive use, it is the key element of our memory-based approach to data analysis.
Reference: [ 9 ] <author> Girosi, F., Jones, M., Poggio, T. </author> <title> (1995) Regularization Theory and Neural Networks Architectures, </title> <journal> Neural Computation, </journal> <volume> vol. 7, no. 2, </volume> <pages> pp. 219-269. </pages>
Reference-contexts: Vapnik [ 16 ] showed that the minimization of the risk functional starting from the empirical risk is an ill-posed problem unless some a priori assumption about the functional dependence is made (e.g. regularization <ref> [ 9 ] </ref> ).
Reference: [ 10 ] <author> Hastie T., Tibshirani R., </author> <title> (1996) Discriminant adaptive nearest neighbor classification and regression, </title> <editor> in Touretzky D.S., Mozer M.C. and Hasselmo M.E (eds.) </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <address> Cambridge, MA, </address> <publisher> MIT Press. </publisher>
Reference-contexts: Starting from the work of Atkeson at al. [ 1 ] and of Cleveland [ 6 ] on local weighted regression and from the one of Hastie and Tibshirani <ref> [ 10 ] </ref> on classification, we propose an adaptive methodology for nonlinear data analysis where methods and tools from linear statistics are intensively used.
Reference: [ 11 ] <author> Jordan, M.I.,Jacobs, R.A. </author> <year> (1994), </year> <title> Hierarchical Mixtures of Experts and the EM Algorithm, </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214. </pages>
Reference-contexts: There are many other examples in literature where the idea of decomposition appears (e.g. regression trees [ 5 ] , mixture models <ref> [ 11 ] </ref> and neuro-fuzzy inference systems [ 3 ] ). However, unlike the memory-based approach, in these modular architectures the learning process obeys a global function estimation criterion.
Reference: [ 12 ] <author> Myers, R. H. </author> <year> (1990), </year> <title> Classical and modern regression with applications, </title> <address> PWS-Kent, Boston. </address>
Reference-contexts: We extend the classical approach with a method that automatically selects the adequate configuration. To this aim, we simply import tools and techniques from the field of linear statistical analysis. The most important and effective of these tools is the PRESS statistic <ref> [ 12 ] </ref> , which is a simple, well-founded and economical way to perform leave-one-out cross validation [ 8 ] and therefore to assess the performance in generalization of local linear models. <p> Some of the most effective instruments that can be used in the linear case are sequential variable selection procedures (e.g. forward selection, stepwise regression, backward elimination) <ref> [ 12 ] </ref> which search for the optimal subset of regressors, or principal component analysis (PCA) techniques which compute optimal linear combination in order to avoid collinearity and reduce the dimension of the problem. With AMB it is possible to migrate these same techniques to non linear situations.
Reference: [ 13 ] <author> Platt, J. </author> <title> (1991) A Resource-Allocating Network for Function Interpolation, Neural Computation, </title> <journal> vol.3, </journal> <volume> no. </volume> <pages> 2. </pages>
Reference-contexts: We predicted the value of the series at time t+85 from inputs at time t, t-6, t-12 and t-18. We achieved a Normalized RMSE equal to 0.059. One referential result obtained with the RAN approach is NRMSE=0.075 <ref> [ 13 ] </ref> . 1 Web page at http:// legend.gwydion.cs.cmu.edu/neural-bench/benchmarks/mackey-glass.html 0 50 100 150 200 250 300 350 400 450 500 0.5 0.7 0.9 1.1 1.3 fig. 1 Mackey Glass time series and AMB prediction (dotted line) In fig.2 we represent the prediction on a time window of 100-samples (diagram a),
Reference: [ 14 ] <author> Sauer, T. </author> <year> (1994), </year> <title> Time Series Prediction by using delay coordinate embedding in Time Series Prediction: forecasting the future and understanding the past ed. A.S. Weigend, N.A. </title> <publisher> Gershenfeld, Addison Wesley. </publisher>
Reference-contexts: subset among the neighbors, as well as various structural aspects like the features to treat and the degree of the polynomial used as a local approximator. 2.1 Adaptive feature selection A common way to deal with time series is to use a vector of time delayed observations (delay coordinate embedding <ref> [ 14 ] </ref> ) to reconstruct the state space of the dynamical system underlying the time series. Following this approach, time series prediction consists in predicting a future value using time delayed observations as regressors.
Reference: [ 15 ] <editor> Vapnik, V.N. </editor> <booktitle> (1992) Principles of risk minimization for learning theory in Advances in Neural Information Processing Systems, </booktitle> <volume> Vol. 4, </volume> <publisher> Morgan Kaufmann, </publisher> <address> Denver. </address>
Reference-contexts: In this scheme one solves a relatively simple problem (estimation of the function value) by first solving a much more difficult intermediate problem (a function estimation). Function estimation is a complex task, generally treated as a minimization problem of a global cost function J (risk functional) <ref> [ 15 ] </ref> which measures the discrepancy over the whole input space between the function underlying the data set and the approximator f (x, a ).
Reference: [ 16 ] <author> Vapnik V.N. </author> <title> (1995) The nature of statistical learning theory, </title> <publisher> Springer, </publisher> <address> New York. </address>
Reference-contexts: This is essentially the perspective of the neural modeling approach. What makes this approach complex is that only a finite amount of data is available and that the risk functional has to be replaced by an approximation (empirical risk functional) constructed on the basis of the training set. Vapnik <ref> [ 16 ] </ref> showed that the minimization of the risk functional starting from the empirical risk is an ill-posed problem unless some a priori assumption about the functional dependence is made (e.g. regularization [ 9 ] ).
References-found: 16

