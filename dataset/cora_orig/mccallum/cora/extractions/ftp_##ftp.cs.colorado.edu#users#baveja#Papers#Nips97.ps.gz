URL: ftp://ftp.cs.colorado.edu/users/baveja/Papers/Nips97.ps.gz
Refering-URL: http://www.cs.colorado.edu/~baveja/papers.html
Root-URL: 
Email: baveja@cs.colorado.edu  cohn@harlequin.com  
Title: How to Dynamically Merge Markov Decision Processes  
Author: Satinder Singh David Cohn 
Address: Boulder, CO 80309-0430  Menlo Park, CA 94025  
Affiliation: Department of Computer Science University of Colorado  Adaptive Systems Group Harlequin, Inc.  
Abstract: We are frequently called upon to perform multiple tasks that compete for our attention and resource. Often we know the optimal solution to each task in isolation; in this paper, we describe how this knowledge can be exploited to efficiently find good solutions for doing the tasks in parallel. We formulate this problem as that of dynamically merging multiple Markov decision processes (MDPs) into a composite MDP, and present a new theoretically-sound dynamic programming algorithm for finding an optimal policy for the composite MDP. We analyze various aspects of our algorithm and Every day, we are faced with the problem of doing multiple tasks in parallel, each of which competes for our attention and resource. If we are running a job shop, we must decide which machines to allocate to which jobs, and in what order, so that no jobs miss their deadlines. If we are a mail delivery robot, we must find the intended recipients of the mail while simultaneously avoiding fixed obstacles (such as walls) and mobile obstacles (such as people), and still manage to keep ourselves sufficiently charged up. Frequently we know how to perform each task in isolation; this paper considers how we can take the information we have about the individual tasks and combine it to efficiently find an optimal solution for doing the entire set of tasks in parallel. More importantly, we describe a theoretically-sound algorithm for doing this merging dynamically; new tasks (such as a new job arrival at a job shop) can be assimilated online into the solution being found for the ongoing set of simultaneous tasks. illustrate its use on a simple merging problem.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barto, A. G., Bradtke, S. J., & Singh, S. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, </journal> <volume> 72, </volume> <pages> 81-138. </pages> <note> also, </note> <institution> University of Massachusetts, Amherst, </institution> <type> CMPSCI Technical Report 93-02. </type>
Reference: <author> Bertsekas, D. P. </author> <year> (1995). </year> <title> Dynamic Programming and Optimal Control. </title> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference-contexts: Note that the traditional treatment of problems such as job-shop scheduling would formulate them as nonstationary MDPs (however, see Zhang and Dietterich, 1995 for another learning approach). This normally requires augmenting the state space to include a "time" component which indexes all possible state spaces that could arise <ref> (e.g., Bertsekas, 1995) </ref>. This is inefficient, and potentially infeasible unless we know in advance all combinations of possible tasks we will be required to solve.
Reference: <author> Bertsekas, D. P. & Tsitsiklis, J. N. </author> <year> (1996). </year> <title> Neuro-Dynamic Programming. </title> <address> Belmont, MA: </address> <publisher> Athena Scientific. </publisher>
Reference-contexts: 1 The Merging Framework Many decision-making tasks in control and operations research are naturally formulated as Markov decision processes (MDPs) <ref> (e.g., Bertsekas & Tsitsiklis, 1996) </ref>. Here we define MDPs and then formulate what it means to have multiple simultanous MDPs. 1.1 Markov decision processes (MDPs) An MDP is defined via its state set S, action set A, transition probability matrices P , and payoff matrices R. <p> The objective is to find an optimal policy, one that maximizes the value of every state. The optimal value of state s, V fl (s), is its value under the optimal policy. The optimal value function is the solution to the Bellman optimality equations <ref> (e.g., Bertsekas & Tsitsiklis, 1996) </ref>: for all s 2 S, V (s) = max ( s 0 where the discount factor 0 fl &lt; 1 makes future payoffs less valuable than more immediate payoffs. <p> Therefore solving an MDP is tantamount to computing its optimal value function. 1.2 Solving MDPs via Value Iteration Given a model (S; A; P; R) of an MDP value iteration <ref> (e.g., Bertsekas & Tsitsiklis, 1996) </ref> can be used to determine the optimal value function.
Reference: <author> Kaelbling, L. P. </author> <year> (1990). </year> <title> Learning in Embedded Systems. </title> <type> PhD thesis, </type> <institution> Stanford University, Department of Computer Science, Stanford, CA. </institution> <note> Technical Report TR-90-04. </note>
Reference: <author> Moore, A. W. & Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13 (1). </volume>
Reference: <author> Singh, S. </author> <year> (1997). </year> <title> Reinforcement learning in factorial environments. </title> <note> submitted to ICML97. </note>
Reference: <author> Watkins, C. J. C. H. </author> <year> (1989). </year> <title> Learning from Delayed Rewards. </title> <type> PhD thesis, </type> <institution> Cambridge Univ., </institution> <address> Cambridge, England. </address>
Reference-contexts: Note that a Q-value <ref> (Watkins, 1989) </ref> based version of value iteration and our algorithm presented below is also easily defined. 1.3 Multiple Simultaneous MDPs The notion of an optimal policy is well defined for a single task represented as an MDP.
Reference: <author> Zhang, W. & Dietterich, T. G. </author> <year> (1995). </year> <title> High-performance job-shop scheduling with a time delay TD(lambda) network. </title> <booktitle> In Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: To the best of our knowledge, the dynamic merging question has not been studied before. Note that the traditional treatment of problems such as job-shop scheduling would formulate them as nonstationary MDPs <ref> (however, see Zhang and Dietterich, 1995 for another learning approach) </ref>. This normally requires augmenting the state space to include a "time" component which indexes all possible state spaces that could arise (e.g., Bertsekas, 1995).
References-found: 8

