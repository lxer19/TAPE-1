URL: ftp://www.cs.rutgers.edu/pub/technical-reports/ml-tr-40.ps.Z
Refering-URL: http://www.cs.rutgers.edu/pub/technical-reports/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: EXPLOITING KNOWLEDGE OF UNCERTAINTY: INDUCTION OF CLASSIFIERS BY THE INCREMENTAL COMBINATION OF PROBABILISTIC EVIDENCE Written
Author: BY STEVEN W. NORTON Haym Hirsh 
Degree: A dissertation submitted to the Graduate School|New Brunswick  in partial fulfillment of the requirements for the degree of Doctor of Philosophy  and approved by  
Date: May, 1995  
Note: Graduate Program in Computer Science  
Address: New Jersey  Brunswick, New Jersey  
Affiliation: Rutgers, The State University of  New  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Dana Angluin and Philip Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: These unlikely models pertain to situations in which an adversary determines part of the training data. This section focuses instead on a pair of PAC-learning algorithms that exploit knowledge of more typical stochastic noise processes. Angluin and Laird <ref> [1] </ref> present an algorithm for PAC-learning k-CNF formulas from training data subject to uniform classification noise. (These are conjunctive normal form formulas whose clauses contain k or fewer literals.) The algorithm works by identifying clauses that minimize disagreements with the training data, and conjoining them to form the final result.
Reference: [2] <author> Pierre Baldi and Yves Chauvin. </author> <title> Temporal evolution of generalization during learning in linear networks. </title> <journal> Neural Computation, </journal> <volume> 3 </volume> <pages> 589-603, </pages> <year> 1991. </year>
Reference-contexts: Backpropagation neural networks [54] learn by adjusting weights to reduce an error measure according to its gradient. Researchers typically halt network learning when the error rate is sufficiently small or appears to have stabilized, even if the data are not fully separated <ref> [2] </ref>. AQ15 [34] incorporates heuristics for trading off apparent error against concept size. The idea is to remove disjuncts that cover only a few examples, under the assumption that those examples are noisy so the corresponding terms can be removed without increasing the actual error rate.
Reference: [3] <author> Dennis Benson, David J. Lipman, and James Ostell. </author> <title> GenBank. </title> <journal> Nucleic Acids Research, </journal> <volume> 21(13) </volume> <pages> 2963-2965, </pages> <year> 1993. </year>
Reference-contexts: Though this was the original source of the negative examples, it is under-utilized. This non-binding strand could be the source of about 1500 negative examples instead of just a few hundred. As an additional test, I used GenBank sequence data <ref> [3] </ref> for bacteriophage lambda [20] to evaluate the false-positive error rate of the best classifiers. Lambda is a virus that infects E. coli. Only a handful of early genes are expressed by the normal apparatus of E. coli, before the viral systems take over. <p> These important differences are significant at a level better than 10 6 . 89 In 1993, GenBank contained about 130 million nucleotide bases from all sources <ref> [3] </ref>. E. coli itself contains about five million nucleotides. Much of this data has been automatically sequenced, and its biological significance is unknown. Learned classifiers could shed some light on this data, and would be used by molecular biologists to suggest laboratory experiments if they were accurate enough. <p> The first was the 1500-base non-binding fragment mentioned above. The second was the genome for the bacteriophage lambda [20]. This gives about 97,000 bases that should not bind to RNA polymerase. The source for the data is GenBank <ref> [3] </ref>, locus LAMCG, dated September 25, 1991. My program modeled uncertainty due to multiple transcriptional start sites with a uniform distribution. I experimented with two models for the gap length separating the start of transcription and the Pribnow box. The first gives a sharp distribution.
Reference: [4] <author> James A. Borowiec and Jay D. Gralla. </author> <title> All three elements of the lac p s promoter mediate its transcriptional response to DNA supercoiling. </title> <journal> Journal of Molecular Biology, </journal> <volume> 195 </volume> <pages> 89-97, </pages> <year> 1987. </year>
Reference-contexts: Abortive initiation studies on the promoter/RNA polymerase complex interrupt the formation of RNA in the earliest stages. They clearly indicate that the recognition region, Pribnow box, and the spacer are important for characterizing promoter function <ref> [4] </ref>. My learning program IPEC-DNA (Induction as Probabilistic Evidence Combination|DNA) reasons about these entities by encoding them in its concept-description language. Other high-level features could also be used [24], but the Pribnow box, recognition region, and spacer should be a minimum for programs that learn promoter-recognition rules. <p> These regions must be properly oriented relative to the polymerase molecule in order to bind together. The further they are out of alignment, the more difficult and less likely it will be for the promoter and polymerase to bind. Other research <ref> [4] </ref> suggests that twisting of the DNA has a quadratic effect on the rate of closed complex formation, one of several steps in the initiation of transcription. Since rates are proportional to probabilities, the form of the probability distribution over the spacer length should be roughly quadratic.
Reference: [5] <author> Leo Breiman, Jerome H. Friedman, Richard A. Olshen, and Charles J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth and Brooks, </publisher> <year> 1984. </year>
Reference-contexts: By varying the noise rate, the degree of uncertainty can be varied, providing the first opportunity to examine performance differences between IPEC-CTL and more conventional learners. Other typical learning problems involve data subject to attribute noise. Here I chose to use the digit recognition problem in noisy LED data <ref> [5] </ref>, not only because it is a familiar example of uniform attribute noise, but because it affords the opportunity to engineer a pair of more interesting uncertainty models and evidence representations. One of the uncertainty models is strong, the other weak. <p> LED data was used as a running example in Classification and Regression Trees <ref> [5] </ref>. In the LED data, seven binary attributes arranged as in Figure 3.5 represent the digits zero through nine. The correct association between digits and binary attribute patterns is shown in Table 3.1.
Reference: [6] <author> Wray Lindsay Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year>
Reference-contexts: Finally, while STAGGER is tolerant of systematic noise and concept drift, it is does not exploit knowledge about particular kinds of noise or about expected trends in concept drift that might improve learning. 8 1.1.5 An Analytical Bayesian Framework Buntine highlights the importance of probability and uncertainty in learning <ref> [6] </ref>. Elements of his Bayesian approach are closely related to the material in this dissertation. In particular, he shows that a probabilistic formulation of the problem and the incorporation of probabilistic background knowledge can have important roles in learning. Buntine's dissertation [6] includes a framework for characterizing learning problems using Bayesian <p> highlights the importance of probability and uncertainty in learning <ref> [6] </ref>. Elements of his Bayesian approach are closely related to the material in this dissertation. In particular, he shows that a probabilistic formulation of the problem and the incorporation of probabilistic background knowledge can have important roles in learning. Buntine's dissertation [6] includes a framework for characterizing learning problems using Bayesian analysis. Let x and c represent a single example and its classification, H a hypothesis, and any other relevant parameters. <p> This can be a difficult step, especially for sophisticated concept-description languages. The strongest connection between Buntine's dissertation and my own is a general protocol that at a high level describes learning scenarios involving deterministic concepts and noisy data <ref> [6, p. 27] </ref>: P r (x; cjH; ) = E y~d (yj) (N (y; c H (y); x; c)) A protocol is a probability distribution of observations (x; c) given the true hypothesis H and other relevant parameters . <p> These are additional indications of the desirability and utility of exploiting background knowledge during induction. 3.4 Related Work As I mentioned in Chapter 1, Buntine's Bayesian framework for empirical learning <ref> [6] </ref> is complementary to this one. His abstract three-step approach is to develop a learning protocol describing how data are presented, to use the protocol to develop expressions for the posterior probabilities of the hypotheses, and to use those expressions to develop a learning algorithm. <p> Another extension along these lines would be to use a voting scheme to return not a single concept description but a group of concept descriptions that would later vote in order to return a classification. Buntine has demonstrated the advantages of such an approach with his option trees <ref> [6] </ref>. The evidence-combination approach provides ideal support for these schemes. If evidence combination converges to a single evidence 103 tuple, that tuple may represent a number of concept descriptions, or if it has not then it returns a set of sets of concept descriptions, each weighted with a probability.
Reference: [7] <author> Lon R. Cardon and Gary D. Stormo. </author> <title> Expectation maximization algorithm for identifying protein-binding sites with variable lengths from unaligned DNA fragments. </title> <journal> Journal of Molecular Biology, </journal> <volume> 223 </volume> <pages> 159-170, </pages> <year> 1992. </year>
Reference-contexts: The basic idea behind EM is to alternate between creating a model of the available data and reinterpreting the available data in light of the current model. These stages alternate until they converge. Cardon and Stormo <ref> [7] </ref> describe an EM approach to promoter recognition that learns base and spacer length frequencies. Using the compilation of Harley and Reynolds [16] they report 87% accuracy on the training data.
Reference: [8] <author> Jason Catlett. </author> <title> Tailoring rulesets to misclassification costs. </title> <booktitle> In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 88-94, </pages> <year> 1995. </year>
Reference-contexts: Pruning, though helpful in other contexts, did not help the smarter C4.5-based strawman. Pruning tends to return a trivial tree that classifies every example as a non-promoter because the training data are skewed, with about 100 non-promoters for every promoter. Using unequal misclassification costs <ref> [8] </ref> might improve pruning by weighing promoters more heavily than non-promoters. There are two reasons why such an approach was not tried here. First, there is no consensus within the machine-learning community on how to use unequal misclassification costs; it is the subject of current research [8, 46]. <p> There are two reasons why such an approach was not tried here. First, there is no consensus within the machine-learning community on how to use unequal misclassification costs; it is the subject of current research <ref> [8, 46] </ref>. And second, if learned classifiers are used to suggest laboratory experiments, false-positives lead directly to wasted time and money. Hence, there are good reasons for preferring a contrary weighting, one that gives non-promoters more import than promoters.
Reference: [9] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Proceedings of the European Working Session on Machine Learning, </booktitle> <pages> pages 151-163. </pages> <publisher> Springer-Verlag, </publisher> <year> 1991. </year>
Reference-contexts: FOIL's cability of equating attribute values makes single FOIL rules more expressive than IPEC-CTL conjunctions. Instead of concentrating on learning a single rule at a time, CN2 does a beam search through a space of general rules, gradually specializing them <ref> [9] </ref>. One of its guiding heuristics is the entropy of the data covered by a rule. <p> The basic strategy is to apply the IPEC-CTL conjunctive term learner iteratively to learn a term at a time until the entire DNF classifier is formed. In doing so, the new DNF learner is using a separate-and-conquer strategy similar to that of the AQ system and its successors <ref> [32, 34, 9] </ref>. Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. <p> This approach has been implemented in my second learning system, IPEC-DNF (Induction as Probabilistic Evidence Combination|DNF). On synthetic data where strong knowledge is available, IPEC-DNF exhibits statistically significantly lower error rates early in learning when compared to the C4.5 [52] and CN2 <ref> [9] </ref> algorithms. Since in real-world learning problems one is constrained to use whatever amount of data is provided, and in many cases this quantity is limited, an advantage early in learning is important. <p> is weaker but still satisfying: using IPEC-DNF with weak models (i.e. assuming the data are noiseless or affected only by uniform classification noise) often gives competitive results, when strong knowledge is lacking. 4.3.1 Alternative Systems In order to evaluate its performance, I compared IPEC-DNF to C4.5 [52] and to CN2 <ref> [9] </ref>. I chose these programs because they are widely available, well-known, noise-tolerant, and generally perform well across a wide range of problems. In addition, decision trees, decision lists, and rule sets are all appropriate vehicles for learning DNF classifiers.
Reference: [10] <author> Peter Clark and Tim Niblett. </author> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 261-284, </pages> <year> 1989. </year>
Reference-contexts: The next one details experiments involving uniform attribute noise. 3.2.1 Alternative Systems To begin characterizing the performance of IPEC-CTL, I performed a series of experiments on data subject to uniform classification noise. I chose to compare IPEC-CTL to a pair of popular noise-tolerant inductive learners CN2 <ref> [10] </ref> and FOIL [51], and to single-rule variants of these systems. CN2 and FOIL can learn sets of rules. CN2 can also learn decision lists, but that capability is not studied in this chapter. <p> Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. There is a problem facing all separate-and-conquer learners <ref> [10, 34, 45, 51] </ref>, namely that not all positive examples of the full disjunctive expression are actually classified as positive by all disjuncts. Thus some of the positive examples should effectively be treated as negative examples for some disjuncts. <p> This makes lambda an excellent source of about 97,000 negative examples that should not bind to the E. coli RNA polymerase. The experiments reported here involve a number of systems in addition to IPEC-DNA. In particular, I used CN2 <ref> [10] </ref>, C4.5 (revision 3) [52], IPEC-DNF, and a k-nearest-neighbor classifier. Each of these conventional learners uses the raw sequence data directly, even though the biologically meaningful attributes are misaligned. Tree pruning in C4.5 was found to be helpful and is used here. <p> The evidence-combination approach then yields classifiers that are accurate and credible, and the best yet developed for this important problem. 106 Appendix A IPEC-CTL Classification Noise Experiments This group of experiments illustrates the difference in capabilities between IPEC-CTL and two other conjunction learners. I chose FOIL6.1 [51] and CN2 <ref> [10] </ref> because they are well-known, readily available, easily adapted to single rule learning, and noise-tolerant. The concept-description language consisted of 20 attributes, each 0, 1, or fl. <p> I chose FOIL6.1 [51] and CN2 <ref> [10] </ref> because they are well-known, readily available, easily adapted to single rule learning, and noise-tolerant. The concept-description language consists of 7 attributes, each 0, 1, or fl. The target concepts are conjunctions representing the ten LED digit patterns. <p> I chose CN2 <ref> [10] </ref>, and C4.5 [52] because they are well-known, well-developed, and readily available. Examples were described by 20 binary attributes. The target concepts were DNF expressions with 2, 3, or 5 terms.
Reference: [11] <author> Scott Cost and Steven Salzberg. </author> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10(1) </volume> <pages> 57-78, </pages> <year> 1993. </year>
Reference-contexts: Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories <ref> [11, 26, 27, 60] </ref>. The 53 promoter sequences in this Irvine dataset were selected from the compilation of Hawley and McClure [19], and are left-aligned on the putative recognition region. (Sequence data from the Irvine dataset for the bioB promoter is shown at the bottom of Figure 5.1.
Reference: [12] <author> Scott Evan Decatur. </author> <title> Learning in hybrid noise environments using statistical queries. </title> <booktitle> In Preliminary Papers of the Fifth International Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 175-185, </pages> <year> 1995. </year>
Reference-contexts: The key limitation of this approach is the assumption of strict consistency with training data. Noisy real-world datasets are often inconsistent, causing 1 While making the final revisions to this dissertation a paper was published addressesing PAC learning from data subject to classification noise and malicious errors <ref> [12] </ref>. In a forthcoming publication De-catur will address the combination of classification noise and attribute noise [Personal communication]. 6 the version space to collapse and candidate elimination to fail. Hirsh generalizes the definition of a version space by dropping the requirement of strict consistency with data [21].
Reference: [13] <author> Richard E. Dickerson. </author> <title> Base sequence and helix structure variation in B and A DNA. </title> <journal> Journal of Molecular Biology, </journal> <volume> 166 </volume> <pages> 419-441, </pages> <year> 1983. </year>
Reference-contexts: They show that the preferred spacer length is 17 bases [68]. This is consistent with consensus-sequence analysis that indicates spacers of 17 1 base pairs represent 92% of promoters [16]. In helical DNA, each base contributes about 35 degrees of twist <ref> [13] </ref>. It follows that the length of the spacer influences the preferred orientation of the Pribnow box relative to the recognition region by altering helical twist. These regions must be properly oriented relative to the polymerase molecule in order to bind together.
Reference: [14] <author> Richard Duda and Peter Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> John Wiley and Sons, </publisher> <year> 1973. </year> <month> 177 </month>
Reference: [15] <author> Howard B. Gamper and John E. Hearst. </author> <title> A topological model for transcription based on unwinding angle analysis of E. coli RNA polymerase binary, initiation and ternary complexes. </title> <journal> Cell, </journal> <volume> 29 </volume> <pages> 81-90, </pages> <year> 1982. </year>
Reference-contexts: To expose the template strand once the polymerase has bound to the promoter, 17 1 bases are unwound from the middle of the Pribnow box to six or eight bases past the start of transcription <ref> [15] </ref>. Allowing three bases in the Pribnow box leaves between five and nine bases from the start of transcription to the downstream end of the Pribnow box.
Reference: [16] <author> Calvin B. Harley and Robert P. Reynolds. </author> <title> Analysis of E. coli promoter sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 15(5) </volume> <pages> 2343-2361, </pages> <year> 1987. </year>
Reference-contexts: No E. coli promoter has precisely this structure, and most have many differences. Still, the very idea of a canonical sequence capturing the essence of promoter structure and function was so influential that biologists produced compilations of promoter sequences aligned specifically to enhance correspondence to the consensus sequence <ref> [19, 16] </ref>. Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories [11, 26, 27, 60]. <p> The DNA sequences are on the right. The consensus regions as identified in Hawley and McClure's compilation consist of the capitalized bases. As it turns out, Hawley and McClure's compilation is left-aligned on the putative recognition region. More recent approaches perform the alignment automatically <ref> [16] </ref>, but are imperfect none the less. The underlined bases in the same figure are the consensus regions as identified in the Harley and Reynolds' compilation [16]. The doubly-underlined bases in bioA correspond to the best overall consensus alignment independent of transcriptional start data. <p> As it turns out, Hawley and McClure's compilation is left-aligned on the putative recognition region. More recent approaches perform the alignment automatically <ref> [16] </ref>, but are imperfect none the less. The underlined bases in the same figure are the consensus regions as identified in the Harley and Reynolds' compilation [16]. The doubly-underlined bases in bioA correspond to the best overall consensus alignment independent of transcriptional start data. The point is not that one compilation was done by hand and another by computer program. <p> The alignments depicted in Figure 5.2 cannot both be right. Perhaps neither one is right. At a minimum, for every disagreement there is an error in one of the datasets. Harley and Reynolds give an excellent indication of the potential problem <ref> [16] </ref>. Their compilation disagrees with previously published compilations in 58 of 263 promoters. Moreover, the best overall alignment they discovered is at odds with transcriptional start site information in 28 of the 263 promoters. <p> Mutational studies examine the effects of individual base insertions, deletions, or replacements within a promoter region. They show that the preferred spacer length is 17 bases [68]. This is consistent with consensus-sequence analysis that indicates spacers of 17 1 base pairs represent 92% of promoters <ref> [16] </ref>. In helical DNA, each base contributes about 35 degrees of twist [13]. It follows that the length of the spacer influences the preferred orientation of the Pribnow box relative to the recognition region by altering helical twist. <p> These stages alternate until they converge. Cardon and Stormo [7] describe an EM approach to promoter recognition that learns base and spacer length frequencies. Using the compilation of Harley and Reynolds <ref> [16] </ref> they report 87% accuracy on the training data. When they look for a conserved region near the start of transcription, in addition to a Pribnow box and recognition region, they report 94% accuracy on the training data.
Reference: [17] <author> David Haussler. </author> <title> Quantifying inductive bias: AI learning algorithms and Valiant's learning framework. </title> <journal> Artificial Intelligence, </journal> <volume> 36(2) </volume> <pages> 177-222, </pages> <year> 1988. </year>
Reference-contexts: In particular, the bounded-inconsistency approach cannot be used because the corresponding version spaces encompass the entire concept description language making any convergence impossible. Second, the problem of exponential growth in the size of G-sets <ref> [17] </ref> can now occur in the S-sets, even for conjunctive concept-description languages. This will present serious difficulties in scaling up to large problems. And third, picking the best element from the final version space remains difficult if the version space has not converged. <p> In particular, the generalization and specialization operators can cause the boundary sets to grow in response to new examples. For conjunctive concept description languages, Haussler shows that the G-set can grow exponentially <ref> [17] </ref>. It is possible to suffer the same effect with non-singleton S-sets, as used in Hirsh's work on data subject to bounded inconsistency [22]. IPEC-CTL uses Hirsh's efficient [S; N ] representation for version spaces of conjunctive concept descriptions [23].
Reference: [18] <author> David Haussler, Michael Kearns, and Rob Schapire. </author> <title> Bounds on the sample complexity of Bayesian learning using information theory and the VC dimension. </title> <booktitle> In Computational Learning Theory: Proceedings of the Fourth Annual Workshop. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: One of those could be chosen instead, giving a different bias. There is also another way to select the final version space, using a Gibbs algorithm <ref> [18] </ref>. The idea is to select a version space at random according to the posterior probabilities. When the most probable single version space is much more likely than all the others combined, there will not be much difference with this approach. <p> For example, the current method of returning a classifier with maximum a posteriori probability has the virtue of simplicity, but the Gibbs method of choosing a classifier at random according to its posterior probability has better asymptotic properties <ref> [18] </ref>. The difficulties of implementing a Gibbs strategy include estimating the size of the remaining version spaces and selecting one of their elements at random.
Reference: [19] <author> Diane K. Hawley and William R. McClure. </author> <title> Compilation and analysis of Es-cherichia coli promoter DNA sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 11(8) </volume> <pages> 2237-2255, </pages> <year> 1983. </year>
Reference-contexts: No E. coli promoter has precisely this structure, and most have many differences. Still, the very idea of a canonical sequence capturing the essence of promoter structure and function was so influential that biologists produced compilations of promoter sequences aligned specifically to enhance correspondence to the consensus sequence <ref> [19, 16] </ref>. Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories [11, 26, 27, 60]. <p> The 53 promoter sequences in this Irvine dataset were selected from the compilation of Hawley and McClure <ref> [19] </ref>, and are left-aligned on the putative recognition region. (Sequence data from the Irvine dataset for the bioB promoter is shown at the bottom of Figure 5.1. <p> First, there are 1 Researchers working on other problems involving multiply-aligned data, such as to learning to recognize ribosomal binding sites or -independent terminators, should beware this pitfall too. 79 often multiple transcriptional start sites, and during transcription the relevant one is chosen nondeterministically <ref> [19] </ref>. Second, the length of the gaps between the start site, the Pribnow box, and the recognition region vary from promoter to promoter [64, 68]. <p> The least understood of the three uncertainties is the uncertainty concerning multiple start sites. The various compilations indicate each start site, but do not indicate the preferred one, if any <ref> [19] </ref>. Without stronger information, one way to model this uncertainty is with a uniform probability distribution over the possible start sites. Any other choice would indicate an unjustified preference among start sites. 82 The spacer between the Pribnow box and the recognition region is more clearly understood. <p> In particular, I constructed datasets with a biologically-justified alignment, left or right aligned at the start of transcription. By examining the datasets available to me and identifying corresponding elements in the original compilation <ref> [19] </ref> I decided that aligning the raw data on the rightmost transcriptional start site most preserved the relative locations of the recognition regions and the Pribnow boxes. 3 First I constructed a biologically-aligned variant of the Irvine dataset. <p> He reports 50% to 80% true positive rates and extremely low false positive rates. (If O'Neill's testing data were composed of equal numbers of promoters and non-promoters, this would yield an equivalent error rate of between 10% and 25%.) By starting with consensus-aligned data <ref> [19] </ref> O'Neill is subject to the criticisms detailed in Section 5.1. In particular, inaccurate initial alignments leave the spacing-class-dependent training data wrongly biased. Furthermore, the credibility of the suggested classifiers is diminished because most of the referenced bases are located at sites thought 96 to have no particular biological significance.
Reference: [20] <author> R. W. Hendrix, J. W. Roberts, F. W. Stahl, and R. A. Weisberg, </author> <title> editors. Lambda II. </title> <booktitle> Cold Spring Harbor Laboratory, </booktitle> <year> 1983. </year>
Reference-contexts: Though this was the original source of the negative examples, it is under-utilized. This non-binding strand could be the source of about 1500 negative examples instead of just a few hundred. As an additional test, I used GenBank sequence data [3] for bacteriophage lambda <ref> [20] </ref> to evaluate the false-positive error rate of the best classifiers. Lambda is a virus that infects E. coli. Only a handful of early genes are expressed by the normal apparatus of E. coli, before the viral systems take over. <p> I used two data sources in order to better evaluate the false-positive error rates of my classifiers. The first was the 1500-base non-binding fragment mentioned above. The second was the genome for the bacteriophage lambda <ref> [20] </ref>. This gives about 97,000 bases that should not bind to RNA polymerase. The source for the data is GenBank [3], locus LAMCG, dated September 25, 1991. My program modeled uncertainty due to multiple transcriptional start sites with a uniform distribution.
Reference: [21] <author> Haym Hirsh. </author> <title> Incremental Version-Space Merging: A General Framework for Concept Learning. </title> <publisher> Kluwer, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: In a forthcoming publication De-catur will address the combination of classification noise and attribute noise [Personal communication]. 6 the version space to collapse and candidate elimination to fail. Hirsh generalizes the definition of a version space by dropping the requirement of strict consistency with data <ref> [21] </ref>. Instead, he refers to a version space as any set of concept descriptions representable with boundary sets. He shows how to intersect version spaces and develops a learning algorithm based on version space intersection called incremental version space merging (IVSM). <p> For example, if o = h0010010; +i, then s ; = h0010010; +i and s 1 = h1010010; +i. 2 If version spaces are taken to be any set of concept descriptions representable using boundary sets, as in <ref> [21] </ref>, then a single S-set with all 21 or 35 combinations could be used. Using "composite" version spaces like this can help keep down the number of tuples in an evidence representation, at the cost of combining classifiers that should be rated differently. <p> IPEC-CTL embodies an evidence-combination approach based on version spaces of concept descriptions consistent with various supposed sequences. Hirsh's incremental version-space merging (IVSM) approach to data with bounded inconsistency has a very similar flavor <ref> [21] </ref>. In fact, IPEC-CTL uses version-space merging as a subroutine to update version spaces. The single piece of evidence IVSM uses on each round is a version space containing the concept descriptions consistent with any of the observation's neighbors. <p> One solution is to adopt a version-space representation that can cope with several examples at once, and to use it to combine a number of unlikely supposed examples into a single tuple. Hirsh's version space representation, i.e. sets of classifiers represented by boundary sets, has the desired representational properties <ref> [21] </ref>. For example, a group positive supposed examples could easily be combined into a single S-set. Of course exponential growth in the sizes of the S-sets then becomes a problem, in addition to the growth in the G-sets (if they are used).
Reference: [22] <author> Haym Hirsh. </author> <title> Learning from data with bounded inconsistency. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 32-39. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: of examples are noisier than others, or if there is a mixture of classification noise and attribute noise. 1 1.1.3 Version Spaces and Bounded Inconsistency Hirsh's practical approach to learning from noisy data gives weaker theoretical guarantees than the PAC-learning algorithms, but still successfully exploits knowledge of noise during learning <ref> [22] </ref>. It is based on candidate elimination [36], an older learning algorithm requiring noiseless data. Hirsh's work is relevant here because it addresses uncertainty in training data using an evidence-combination approach, though a categorical one rather than a probabilistic one. <p> For conjunctive concept description languages, Haussler shows that the G-set can grow exponentially [17]. It is possible to suffer the same effect with non-singleton S-sets, as used in Hirsh's work on data subject to bounded inconsistency <ref> [22] </ref>. IPEC-CTL uses Hirsh's efficient [S; N ] representation for version spaces of conjunctive concept descriptions [23]. The idea is to use singleton S-sets exclusively, and maintain a list of negative examples explicitly, rather than a G-set.
Reference: [23] <author> Haym Hirsh. </author> <title> Polynomial-time learning with version spaces. </title> <booktitle> In AAAI92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 117-122. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: It is possible to suffer the same effect with non-singleton S-sets, as used in Hirsh's work on data subject to bounded inconsistency [22]. IPEC-CTL uses Hirsh's efficient [S; N ] representation for version spaces of conjunctive concept descriptions <ref> [23] </ref>. The idea is to use singleton S-sets exclusively, and maintain a list of negative examples explicitly, rather than a G-set. This way the size of the representation only grows linearly with the number of examples processed.
Reference: [24] <author> Haym Hirsh and Michiel Noordewier. </author> <title> Using background knowledge to improve inductive learning of DNA sequences. </title> <booktitle> In The Tenth Conference on Artificial Intelligence for Applications, </booktitle> <year> 1994. </year>
Reference-contexts: They clearly indicate that the recognition region, Pribnow box, and the spacer are important for characterizing promoter function [4]. My learning program IPEC-DNA (Induction as Probabilistic Evidence Combination|DNA) reasons about these entities by encoding them in its concept-description language. Other high-level features could also be used <ref> [24] </ref>, but the Pribnow box, recognition region, and spacer should be a minimum for programs that learn promoter-recognition rules. STTGAC (17 18) TATAAT is a conjunctive term learned in one run of IPEC-DNA. <p> For promoter recognition in E. coli, training data would have to be reformulated in a way that does not depend on the precise alignment of the binding regions. Hirsh and Noordewier take this approach in <ref> [24] </ref>. They propose a set of features taken from the molecular-biology literature that they feel will be useful for promoter recognition and for a variety of related problems.
Reference: [25] <author> Michael Kearns and Ming Li. </author> <title> Learning in the presence of malicious errors. </title> <booktitle> In Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <year> 1988. </year>
Reference-contexts: PAC learning algorithms come with a strong guarantee: given 4 enough training data, they rarely return very inaccurate concept descriptions. There are a number of results for PAC learning from noisy data. In the malicious noise model <ref> [63, 25] </ref>, no assumptions can be made about a noisy example. In the malicious misclassification model [59], no assumptions can be made about the class label of a noisy example. These unlikely models pertain to situations in which an adversary determines part of the training data.
Reference: [26] <author> Moshe Koppel, Alberto Maria Segre, and Ronen Feldman. </author> <title> Getting the most from flawed theories. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 139-147. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories <ref> [11, 26, 27, 60] </ref>. The 53 promoter sequences in this Irvine dataset were selected from the compilation of Hawley and McClure [19], and are left-aligned on the putative recognition region. (Sequence data from the Irvine dataset for the bioB promoter is shown at the bottom of Figure 5.1.
Reference: [27] <author> Pat Langley, Wayne Iba, and Kevin Thompson. </author> <title> An analysis of Bayesian classifiers. </title> <booktitle> In AAAI92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 223-228. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories <ref> [11, 26, 27, 60] </ref>. The 53 promoter sequences in this Irvine dataset were selected from the compilation of Hawley and McClure [19], and are left-aligned on the putative recognition region. (Sequence data from the Irvine dataset for the bioB promoter is shown at the bottom of Figure 5.1.
Reference: [28] <author> Shlomit Lisser and Hanah Margalit. </author> <title> Compilation of E. coli mRNA promoter sequences. </title> <journal> Nucleic Acids Research, </journal> <volume> 21(7) </volume> <pages> 1507-1516, </pages> <year> 1993. </year> <month> 178 </month>
Reference-contexts: A supposed example is a guess about the identity of the actual promoter somewhere within the observation, and specifies the three components 83 necessary to describe the actual promoter. Here is the raw data for bioB as it appears in my version of the Lisser and Margalit database <ref> [28] </ref>: TGTCATAATCGACTTGTAAACCAAATTGAAAAGATTTAGGTTTACAAGTCtACACCGAATTAACA, + The transcriptional start site is the lower-case 't'. There are no annotations to suggest a possible consensus alignment, only the raw data aligned on the transcriptional start. <p> Because I used the Irvine dataset extensively in developing the background knowledge and learning methods for promoter recognition, it was necessary to use a separate, more complete dataset to ensure a fair evaluation. Here is how I converted Lisser and Margalit's up-to-date and extensive promoter compilation <ref> [28] </ref> into a form suitable for learning. 4 I aligned the data on the rightmost transcriptional start site, and trimmed each instance to 65 bases (-50 to +15). Four promoters had to be removed because the compilation listed too few upstream bases (argCBH-P2, speC-P1, speC-P2, and speC-P3). <p> For the full spacer model, I used values between 25 and 333 negative examples per non-promoter. 5 5.5 Experimental Results The results presented in Table 5.2 are based on the version of the Lisser and Margalit database just described <ref> [28] </ref>. The overall 10-fold cross-validated error rate for each learner appears under the heading "CV Rate". Using the abbreviated spacer model and the flat gap model, IPEC-DNA learns a 13-term DNF that is far superior to the conventional classifiers. Its overall error rate is just 2.9%. <p> If a more complete dataset was used <ref> [28] </ref>, one with increased variability in the separation between the putative contact regions, limitations of the background knowledge might be highlighted that were not apparent in the original study. In the biology literature, homology studies explore different ways to align promoter sequences to achieve greatest correspondence to the consensus sequence. <p> DNF, 10 Relevant Attributes, and 10% Classification Noise 172 Figure C.24: 5-Term DNF, 10 Relevant Attributes, and 15% Classification Noise 173 Appendix D Experimental Results on Promoter Recognition The classifiers contained in this appendix were learned from a dataset based on the Lisser and Margalit compilation of E. coli promoters <ref> [28] </ref>. To convert it into a form suitable for learning, I aligned the data on the rightmost transcriptional start site, and trimmed each instance to 65 bases (-50 to +15). Four promoters had to be removed because the compilation listed too few upstream bases (argCBH-P2, speC-P1, speC-P2, and speC-P3).
Reference: [29] <editor> Richard Losick and Michael J. Chamberlin, editors. RNA Polymerase. </editor> <booktitle> Cold Spring Harbor Laboratory, </booktitle> <year> 1976. </year>
Reference-contexts: A promoter is a signal that identifies specific segments of DNA that are transcribed into RNA, a necessary precursor to the production of protein [66]. RNA polymerase is the enzyme that produces RNA on the DNA template <ref> [29] </ref>. Before it produces RNA, the polymerase must recognize and bind to a promoter sequence. Characterizing the three-dimensional structure of the polymerase would help in understanding the promoter/polymerase interaction, but the size and complexity of the polymerase have made the approach impractical.
Reference: [30] <author> William R. McClure. </author> <title> Mechanism and control of transcription initiation in prokary-otes. </title> <journal> Annual Review of Biochemistry, </journal> <volume> 54 </volume> <pages> 171-204, </pages> <year> 1985. </year>
Reference-contexts: Recent studies in molecular biology argue that a single consensus-like sequence is inadequate. One argument suggests that a single sequence could not distinguish between promoters biologically optimized in different ways, for example to closely control rates of binding or open complex formation <ref> [30] </ref>. This criticism suggests that a disjunctive concept description language is necessary. IPEC-DNA learns disjunctions of the basic promoter descriptions described above, using the iterative control structure of IPEC-DNF.
Reference: [31] <author> Ryszard Stanislaw Michalski. </author> <title> AQVAL/1 computer implementation of a variable-valued logic system VL1 and examples of its application to pattern recogniton. </title> <booktitle> In Proceedings of the First International Joint Conference on Pattern Recognition, </booktitle> <pages> pages 3-17, </pages> <year> 1973. </year>
Reference-contexts: Perceptron learning, an early neural network model, only halts for linearly separable data [35]. AQ <ref> [31] </ref> and ID3 [48], DNF and decision tree learners respectively, each start with a general classifier and refine it until it perfectly discriminates the training data. Noise is a problem for these learners because it can leave otherwise perfect data unseparable or inconsistent.
Reference: [32] <author> Ryszard Stanislaw Michalski. </author> <title> A theory and methodology of inductive learning. </title> <editor> In Ryszard Stanislaw Michalski, Jaime Guillermo Carbonell, and Tom Michael Mitchell, editors, </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <pages> pages 83-134. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> Los Altos, CA, </address> <year> 1983. </year>
Reference-contexts: The basic strategy is to apply the IPEC-CTL conjunctive term learner iteratively to learn a term at a time until the entire DNF classifier is formed. In doing so, the new DNF learner is using a separate-and-conquer strategy similar to that of the AQ system and its successors <ref> [32, 34, 9] </ref>. Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration.
Reference: [33] <author> Ryszard Stanislaw Michalski and J. B. Larson. </author> <title> Selection of most representative training examples and incremental generation of VL1 hypotheses: The underlying methodology and description of programs ESEL and AQ11. </title> <type> Report 867, </type> <institution> University of Illinois, </institution> <year> 1978. </year>
Reference-contexts: Learning Databases and Domain Theories suggest that the performance of IPEC-DNF is often competitive with that of C4.5 and CN2, even when using weak general uncertainty models rather than strong ones. 2 In some respects this can be viewed as an alternative to intelligent selection of "seed" examples in AQ <ref> [33] </ref>, in that the goal of both techniques is minimize the effects of poor data orderings. 74 Chapter 5 Learning to Recognize Promoter Sequences in E. coli The preceding chapters developed an approach to classifier induction that can be used to exploit knowledge of uncertainty in training data.
Reference: [34] <author> Ryszard Stanislaw Michalski, Igor Mozetic, Jiarong Hong, and Nada Lavrac. </author> <title> The multi-purpose incremental learning system AQ15 and its application to three medical domains. </title> <booktitle> In AAAI86: Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1041-1045, </pages> <address> Philadelphia, PA, </address> <month> August </month> <year> 1986. </year>
Reference-contexts: Backpropagation neural networks [54] learn by adjusting weights to reduce an error measure according to its gradient. Researchers typically halt network learning when the error rate is sufficiently small or appears to have stabilized, even if the data are not fully separated [2]. AQ15 <ref> [34] </ref> incorporates heuristics for trading off apparent error against concept size. The idea is to remove disjuncts that cover only a few examples, under the assumption that those examples are noisy so the corresponding terms can be removed without increasing the actual error rate. C4.5 takes a similar approach [52]. <p> The basic strategy is to apply the IPEC-CTL conjunctive term learner iteratively to learn a term at a time until the entire DNF classifier is formed. In doing so, the new DNF learner is using a separate-and-conquer strategy similar to that of the AQ system and its successors <ref> [32, 34, 9] </ref>. Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. <p> Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. There is a problem facing all separate-and-conquer learners <ref> [10, 34, 45, 51] </ref>, namely that not all positive examples of the full disjunctive expression are actually classified as positive by all disjuncts. Thus some of the positive examples should effectively be treated as negative examples for some disjuncts.
Reference: [35] <author> Marvin Minsky and Seymour Papert. </author> <title> Perceptrons. </title> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: Perceptron learning, an early neural network model, only halts for linearly separable data <ref> [35] </ref>. AQ [31] and ID3 [48], DNF and decision tree learners respectively, each start with a general classifier and refine it until it perfectly discriminates the training data. Noise is a problem for these learners because it can leave otherwise perfect data unseparable or inconsistent.
Reference: [36] <author> Tom Michael Mitchell. </author> <title> Version Spaces: An Approach to Concept Learning. </title> <type> PhD thesis, </type> <institution> Stanford University, </institution> <year> 1978. </year>
Reference-contexts: It is based on candidate elimination <ref> [36] </ref>, an older learning algorithm requiring noiseless data. Hirsh's work is relevant here because it addresses uncertainty in training data using an evidence-combination approach, though a categorical one rather than a probabilistic one. Mitchell [36] defines a version space to be the set of concept descriptions strictly consistent with training data. <p> It is based on candidate elimination <ref> [36] </ref>, an older learning algorithm requiring noiseless data. Hirsh's work is relevant here because it addresses uncertainty in training data using an evidence-combination approach, though a categorical one rather than a probabilistic one. Mitchell [36] defines a version space to be the set of concept descriptions strictly consistent with training data. By exploiting the partial order of relative generality among concept descriptions, he represents a version space without enumerating its elements. <p> Classifiers 16 must be expressed in some representation. That representation is called the concept-description language. 1 The entire set of concept descriptions expressible in the concept-description language and consistent with a set of data is known as a version space <ref> [36] </ref>. The relation "more general than or equal to" imposes a partial ordering on the space of concept descriptions. Two subsets of a version space make up its boundary-set representation: an S-set of the most specific elements, and a G-set of the most general ones. <p> In particular, using disjoint evidence 51 representations makes identifying the set of most probable concept descriptions trivial. Mitchell proposed a method for handling noisy data too. His approach was to maintain version spaces consistent with smaller and smaller subsets of the training data <ref> [36] </ref>. Essentially, for each k from 0 to a small constant K, a version space is computed that contains every concept consistent with all but k examples; the remaining k are effectively ignored. The first problem with this method is under-utilization of observations.
Reference: [37] <author> Tom Michael Mitchell. </author> <title> The need for biases in learning generalizations. </title> <type> Report CBM-TR-117, </type> <institution> Rutgers University, Department of Computer Science, </institution> <year> 1980. </year>
Reference-contexts: Finally, Chapter 6 concludes the main body of the dissertation with discussion, future work and a summary. 12 Chapter 2 A Framework for Learning from Imperfect Data Learning classifiers is a difficult underconstrained problem, even when perfect training data are available <ref> [37] </ref>. Unfortunately, it is common for training data to be imperfect | subject to noise or other sources of uncertainty | making successful induction even more difficult. This problem is the focus of my dissertation: the induction of classifiers from imperfect data. Noise in training data is familiar enough.
Reference: [38] <author> Andrew M. Moore and Mary S. Lee. </author> <title> Efficient algorithms for minimizing cross validation error. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 190-198. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: IPEC-DNF, like other learning algorithms, still seems to form spurious terms that degrade learning performance. Several approaches are possible, including the use of a traditional post-learning pruning method such as reduced-error pruning [49] or cross-validation <ref> [38] </ref>. My program allows the user to invoke a simple coverage-based stopping criterion instead. Specifically, a minimum coverage level can be specified to the learner, and if the best prospective term does not meet or exceed that level then learning stops, returning the disjunction of terms already learned. <p> Thus choosing a larger value of ff makes learning prefer terms covering larger amounts of data, whereas choosing a smaller ff value has the opposite effect. One could even use cross-validation methods to find appropriate values for these parameters for a given learning session <ref> [38] </ref>. 4.3 Experiments with Synthetic DNF Data This section describes the first part of my experimental evaluation of IPEC-DNF. In particular, I examine the performance of the new learning program on synthetic DNF data where the experimental conditions and sources of uncertainty are known.
Reference: [39] <author> Steven W. Norton. </author> <title> Learning to recognize promoter sequences in E. coli by modeling uncertainty in the training data. </title> <booktitle> In AAAI94: Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <year> 1994. </year>
Reference-contexts: Six promoters were eliminated because no transcriptional start could be identified (rpoB, rrnG-P1, rrnG-P2, malT, fol, hisJ) leaving 100. I used this biologically-aligned variation of the Irvine dataset to develop the models and methods described in Section 5.3. The experiments on this small dataset are described elsewhere <ref> [39] </ref>. Because I used the Irvine dataset extensively in developing the background knowledge and learning methods for promoter recognition, it was necessary to use a separate, more complete dataset to ensure a fair evaluation. <p> The remainder of my dataset consists of an equal number of non-promoters (296) 3 This choice was meant to be most favorable to the conventional learners. When I experimented with a biologically-aligned variation of the Irvine dataset, performing the same experiments using left-aligned data gave substantially similar results <ref> [39] </ref>. 4 Unfortunately these two datasets are not entirely independent.
Reference: [40] <author> Steven W. Norton and Haym Hirsh. </author> <title> Classifier learning from noisy data as probabilistic evidence combination. </title> <booktitle> In AAAI92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 141-146. </pages> <publisher> AAAI Press / MIT Press, </publisher> <year> 1992. </year>
Reference-contexts: Checking for collapse requires n comparisons of the S-set element against the negative examples. The worst-case complexity of IPEC-CTL is therefore O (n 2 ) with a large constant. Experience indicates, however, that the typical case is much better. An earlier paper <ref> [40] </ref> illustrated the point at the bottom of its Figure 2, reproduced here in Figure 3.3. Two curves are plotted in the figure.
Reference: [41] <author> Steven W. Norton and Haym Hirsh. </author> <title> Learning DNF via probabilistic evidence combination. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 220-227. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1993. </year> <month> 179 </month>
Reference: [42] <author> Ruth Nussinov and Gregory G. Lennon. </author> <title> Periodic structurally similar oligomers are found on one side of the axes of symetry in the lac, trp, and gal operators. </title> <journal> Journal of Biomolecular Structure and Dynamics, </journal> <volume> 2(2) </volume> <pages> 387-395, </pages> <year> 1984. </year>
Reference-contexts: For instance, there are 12 features describing sharp bends in the DNA. Hirsh and Noordewier use them singly even though it is "the periodic occurrences of hexamers with identical, large twist angles on the left-hand side of the axis of symmetry" that seemed "strikingly non-random" to the original researchers <ref> [42] </ref>. For a general-purpose sequence learner, abstracting from periodic occurrences of these features to one or more occurrences of these features is fine, provided over-generalization is not a problem.
Reference: [43] <author> Michael C. O'Neill. </author> <title> Escherichia coli promoters: I. Consensus as it relates to spacing class, specificity, repeat substructure, and three-dimensional organization. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5522-5530, </pages> <year> 1989. </year>
Reference-contexts: The false-positive rates given in Table 5.2 show that the branches of the C4.5 decision tree and the CN2 rules are insufficiently specific to describe promoters or particular promoter behaviors <ref> [43] </ref>. On the other hand, the multitude of bases referenced by IPEC-DNF's classifier, chiefly outside the contact regions, hurt more than they help. Classifiers that reference so many specific bases outside the contact regions lose credibility. <p> In the biology literature, homology studies explore different ways to align promoter sequences to achieve greatest correspondence to the consensus sequence. O'Neill's work on promoter recognition <ref> [43, 44] </ref> differs from other homology studies because it distinguishes promoters according to spacing class, i.e. 16, 17, or 18 base spacers. Spacing-class dependent promoter recognition rules have a potentially helpful disjunctive character.
Reference: [44] <author> Michael C. O'Neill and Francis Chiafari. </author> <title> Escherichia coli promoters: II. A spacing class-dependent promoter search protocol. </title> <journal> Journal of Biological Chemistry, </journal> <volume> 264 </volume> <pages> 5531-5534, </pages> <year> 1989. </year>
Reference-contexts: In the biology literature, homology studies explore different ways to align promoter sequences to achieve greatest correspondence to the consensus sequence. O'Neill's work on promoter recognition <ref> [43, 44] </ref> differs from other homology studies because it distinguishes promoters according to spacing class, i.e. 16, 17, or 18 base spacers. Spacing-class dependent promoter recognition rules have a potentially helpful disjunctive character.
Reference: [45] <author> Giulia Pagallo and David Haussler. </author> <title> Boolean feature discovery in empirical learning. </title> <journal> Machine Learning, </journal> <volume> 5(1) </volume> <pages> 71-99, </pages> <year> 1990. </year>
Reference-contexts: Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. There is a problem facing all separate-and-conquer learners <ref> [10, 34, 45, 51] </ref>, namely that not all positive examples of the full disjunctive expression are actually classified as positive by all disjuncts. Thus some of the positive examples should effectively be treated as negative examples for some disjuncts. <p> Each experiment consisted of 50 trials. Experiments were parameterized by the number of terms in the given target classifier, the average number of relevant attributes per term <ref> [45] </ref>, and the uniform classification noise rate. The number of terms was either 2, 3, or 5. The average number of relevant attributes per term, the attributes set to 0 or 1 and not to fl, was either 5 or 10, with a standard deviation of 1.0 attributes.
Reference: [46] <author> Michael Pazzani, Christopher Merz, Patrick Murphy, Kamal Ali, Timothy Hume, and Clifford Brunk. </author> <title> Reducing misclassification costs. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 217-225. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1994. </year>
Reference-contexts: There are two reasons why such an approach was not tried here. First, there is no consensus within the machine-learning community on how to use unequal misclassification costs; it is the subject of current research <ref> [8, 46] </ref>. And second, if learned classifiers are used to suggest laboratory experiments, false-positives lead directly to wasted time and money. Hence, there are good reasons for preferring a contrary weighting, one that gives non-promoters more import than promoters.
Reference: [47] <author> David Pribnow. </author> <title> Nucleotide sequence of an RNA polymerase binding site at an early T7 promoter. </title> <journal> Proc. Nat. Acad. Sci., </journal> <volume> 72(3) </volume> <pages> 784-788, </pages> <year> 1975. </year>
Reference-contexts: tertiary structures are the geometrical relationships between nearby nucleotides and the global, 3-dimensional relationships between distant nucleotides, respectively.) 76 In 1975, Pribnow published a seminal paper describing a pattern of bases occurring imperfectly in a region just upstream of the transcriptional start sites of six of the promoters he examined <ref> [47] </ref>. (The transcriptional start site is where the RNA product begins to be transcribed.) He also suggested the existence of an important region 35 bases upstream. These regions have come to be known as the Pribnow box and the recognition region (or the -35 region).
Reference: [48] <author> J. R. Quinlan. </author> <title> Semi-autonomous acquisition of pattern-based knowledge. </title> <editor> In J. E. Hayes, D. Michie, and Y-H. Pao, editors, </editor> <booktitle> Machine Intelligence, </booktitle> <volume> volume 10, </volume> <pages> pages 159-172. </pages> <publisher> Ellis Horwood, </publisher> <year> 1982. </year>
Reference-contexts: Perceptron learning, an early neural network model, only halts for linearly separable data [35]. AQ [31] and ID3 <ref> [48] </ref>, DNF and decision tree learners respectively, each start with a general classifier and refine it until it perfectly discriminates the training data. Noise is a problem for these learners because it can leave otherwise perfect data unseparable or inconsistent.
Reference: [49] <author> J. R. Quinlan. </author> <title> Simplifying decision trees. </title> <journal> International Journal of Man-Machine Studies, </journal> <volume> 27 </volume> <pages> 221-234, </pages> <year> 1987. </year>
Reference-contexts: IPEC-DNF, like other learning algorithms, still seems to form spurious terms that degrade learning performance. Several approaches are possible, including the use of a traditional post-learning pruning method such as reduced-error pruning <ref> [49] </ref> or cross-validation [38]. My program allows the user to invoke a simple coverage-based stopping criterion instead.
Reference: [50] <author> J. R. Quinlan. </author> <title> Unknown attribute values in induction. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> pages 164-168. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1989. </year>
Reference-contexts: Imperfections in training data are pervasive and varied. Noise in training data is probably the most familiar example. Classification noise and attribute noise sometimes obscure the correct class labels and attribute values. Missing attribute values have a similar effect <ref> [50] </ref>. Other imperfections are less well-known, but still relevant. For example, in several important problems in molecular biology, the number and the meaning of biologically active features is known, but their locations vary from example to example. <p> Again, the level of uncertainty depends on the particular noise rates. Each of these noise processes degrades the performance of inductive learners because training data are the main basis for generalization. Missing values is another common problem in training data <ref> [50] </ref>. Sometimes a missing value actually means that a test was inappropriate and not performed. Those so called missing values are meaningful in their own right. But when values are genuinely missing, the learner is obviously uncertain about the actual attribute values.
Reference: [51] <author> J. R. Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5(3) </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: The next one details experiments involving uniform attribute noise. 3.2.1 Alternative Systems To begin characterizing the performance of IPEC-CTL, I performed a series of experiments on data subject to uniform classification noise. I chose to compare IPEC-CTL to a pair of popular noise-tolerant inductive learners CN2 [10] and FOIL <ref> [51] </ref>, and to single-rule variants of these systems. CN2 and FOIL can learn sets of rules. CN2 can also learn decision lists, but that capability is not studied in this chapter. <p> The single-rule variants I constructed simply return the first rule learned. For the sake of completeness, the performance of the full CN2 and FOIL programs is also examined. FOIL is a system that learns Horn clauses from relational data by using a greedy literal-selection strategy <ref> [51] </ref>. Those clauses characterize only one of the classes represented in the data, henceforth the positive class. The algorithm works by successive refinement of an incomplete clause. <p> Two notable features that distinguish this approach are its ability to use knowledge about the nature of stochastic processes affecting the training data and its ability to form multiple terms on each iteration. There is a problem facing all separate-and-conquer learners <ref> [10, 34, 45, 51] </ref>, namely that not all positive examples of the full disjunctive expression are actually classified as positive by all disjuncts. Thus some of the positive examples should effectively be treated as negative examples for some disjuncts. <p> The evidence-combination approach then yields classifiers that are accurate and credible, and the best yet developed for this important problem. 106 Appendix A IPEC-CTL Classification Noise Experiments This group of experiments illustrates the difference in capabilities between IPEC-CTL and two other conjunction learners. I chose FOIL6.1 <ref> [51] </ref> and CN2 [10] because they are well-known, readily available, easily adapted to single rule learning, and noise-tolerant. The concept-description language consisted of 20 attributes, each 0, 1, or fl. <p> I chose FOIL6.1 <ref> [51] </ref> and CN2 [10] because they are well-known, readily available, easily adapted to single rule learning, and noise-tolerant. The concept-description language consists of 7 attributes, each 0, 1, or fl. The target concepts are conjunctions representing the ten LED digit patterns.
Reference: [52] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The idea is to remove disjuncts that cover only a few examples, under the assumption that those examples are noisy so the corresponding terms can be removed without increasing the actual error rate. C4.5 takes a similar approach <ref> [52] </ref>. It incorporates a post-processing pruning step under the assumption that statistically unjustified subtrees result from overfitting noise, and should be removed from the learned concept. <p> (T i+1 ) = log 2 (T + i+1 + T Gain (L i ) = T + i+1 fi (I (T i ) I (T i+1 )) Quinlan used a similar information-gain measure in his earlier work on learning decision trees, though that measure was symmetrical rather than one-sided <ref> [52] </ref>. By setting a command-line parameter in all the FOIL runs (-n), it is possible for FOIL to learn fairly standard conjunctive rules. These simplest FOIL rules are still more expressive than plain conjunctions because they can enforce equality between attributes. <p> This approach has been implemented in my second learning system, IPEC-DNF (Induction as Probabilistic Evidence Combination|DNF). On synthetic data where strong knowledge is available, IPEC-DNF exhibits statistically significantly lower error rates early in learning when compared to the C4.5 <ref> [52] </ref> and CN2 [9] algorithms. Since in real-world learning problems one is constrained to use whatever amount of data is provided, and in many cases this quantity is limited, an advantage early in learning is important. <p> The second lesson is weaker but still satisfying: using IPEC-DNF with weak models (i.e. assuming the data are noiseless or affected only by uniform classification noise) often gives competitive results, when strong knowledge is lacking. 4.3.1 Alternative Systems In order to evaluate its performance, I compared IPEC-DNF to C4.5 <ref> [52] </ref> and to CN2 [9]. I chose these programs because they are widely available, well-known, noise-tolerant, and generally perform well across a wide range of problems. In addition, decision trees, decision lists, and rule sets are all appropriate vehicles for learning DNF classifiers. <p> At that point it constructs a default rule naming the majority class of the remaining data. C4.5 is a recursive decision-tree builder <ref> [52] </ref> that constructs a decision tree by first identifying a test to use in constructing a root decision node, then by building a decision tree for each of the possible outcomes of the test. These become the offspring of the root node. <p> It evaluates each test according to the gain-ratio criterion, an information-based metric that measures the amount of progress a test makes towards the final classifier. The gain-ratio criterion is given below, as in <ref> [52] </ref>: k X f req (C j ; S) fi log 2 f req (C j ; S) n X jT i j fi inf o (T i ) gain (X) = inf o (T ) inf o X (T ) split inf o (X) = n X jT i j <p> This makes lambda an excellent source of about 97,000 negative examples that should not bind to the E. coli RNA polymerase. The experiments reported here involve a number of systems in addition to IPEC-DNA. In particular, I used CN2 [10], C4.5 (revision 3) <ref> [52] </ref>, IPEC-DNF, and a k-nearest-neighbor classifier. Each of these conventional learners uses the raw sequence data directly, even though the biologically meaningful attributes are misaligned. Tree pruning in C4.5 was found to be helpful and is used here. <p> But for IPEC-DNA, a promoter-specific learner, augmenting the feature set would only be appropriate after tightening up the biological significance of the new features. Hirsh and Noordewier experiment with two learning techniques, C4.5rules <ref> [52] </ref> and neural networks [54], on two learning problems, E. coli promoter sequences, and eukary-otic splice junctions. In each case the concepts learned from the augmented datasets are more accurate than those learned from the raw datasets. <p> I chose CN2 [10], and C4.5 <ref> [52] </ref> because they are well-known, well-developed, and readily available. Examples were described by 20 binary attributes. The target concepts were DNF expressions with 2, 3, or 5 terms. Those terms had an average of 5 or 10 relevant attributes taken from f0; 1g, the rest being set to fl.
Reference: [53] <author> Ronald L. Rivest. </author> <title> Learning decision lists. </title> <journal> Machine Learning, </journal> <volume> 2 </volume> <pages> 229-246, </pages> <year> 1987. </year>
Reference-contexts: CN2 was in part described in Section 3.2.1, where I used it as a rule-set learner and did not then exploit its additional capability to learn decision lists. A decision-list is basically an if-then-else style construct made up of rule-like conditions <ref> [53] </ref>. CN2 learns one condition at a time until it runs out of data or until no condition is good enough 62 according to a test for statistical significance. At that point it constructs a default rule naming the majority class of the remaining data. <p> Equation 6.5 instead becomes P (cjx; H) / j P (xjs j )P (s j ) (6.6) A final extension of the basic evidence-combination approach would be to implement a multi-class learner rather than a two-class learner. An obvious candidate is a decision-list learner <ref> [53] </ref> since decision lists can be learned a conjunction at a time. I have a preliminary implementation of just such a learner, but it only uses the simplest generic uncertainty models (i.e. the noiseless model or the uniform classification noise model).
Reference: [54] <author> David E. Rumelhart, Geoffrey E. Hinton, and Ronald J. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. E. Rumelhart and J. L. McClelland, editors, </editor> <booktitle> Parallel distributed processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: It can also obscure the fact that a single general descriptor would be appropriate, by making several very specific descriptors seem better. Recent machine-learning systems are better adapted to a noisy environment. Backpropagation neural networks <ref> [54] </ref> learn by adjusting weights to reduce an error measure according to its gradient. Researchers typically halt network learning when the error rate is sufficiently small or appears to have stabilized, even if the data are not fully separated [2]. <p> But for IPEC-DNA, a promoter-specific learner, augmenting the feature set would only be appropriate after tightening up the biological significance of the new features. Hirsh and Noordewier experiment with two learning techniques, C4.5rules [52] and neural networks <ref> [54] </ref>, on two learning problems, E. coli promoter sequences, and eukary-otic splice junctions. In each case the concepts learned from the augmented datasets are more accurate than those learned from the raw datasets. <p> Towell, Shavlik, and Noordewier [61] also take a knowledge-based approach to the promoter problem. In particular, a set of rules describing consensus-like sequences and certain conformational properties is used to construct a back-propagation neural network <ref> [54] </ref>. One of the well-known problems in applying neural network technology is 95 suggesting the network topology and initial weights. Towell's insight was that background knowledge can be used to make these determinations.
Reference: [55] <author> Jeffrey C. Schlimmer and Richard H. Granger, Jr. </author> <title> Beyond incremental processing: Tracking concept drift. </title> <booktitle> In AAAI86: Proceedings of the Fifth National Conference on Artificial Intelligence, </booktitle> <pages> pages 502-507. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1986. </year>
Reference-contexts: When the learner's current best concept description misclassifies a training example, deciding whether the disagreement is due to noisy data or a concept that has drifted too far is something of a dilemma. STAGGER not only learns from noisy data, but also learns concepts that can drift over time <ref> [55, 56] </ref>. The program uses a set of characterizations to classify data. These can be thought of as conjunctive and disjunctive concept descriptions, but they are used as a set and not as individual rules. Each characterization is annotated with a pair of weights.
Reference: [56] <author> Jeffrey C. Schlimmer and Richard H. Granger, Jr. </author> <title> Incremental learning from noisy data. </title> <journal> Machine Learning, </journal> <volume> 1(3) </volume> <pages> 317-354, </pages> <year> 1986. </year> <month> 180 </month>
Reference-contexts: When the learner's current best concept description misclassifies a training example, deciding whether the disagreement is due to noisy data or a concept that has drifted too far is something of a dilemma. STAGGER not only learns from noisy data, but also learns concepts that can drift over time <ref> [55, 56] </ref>. The program uses a set of characterizations to classify data. These can be thought of as conjunctive and disjunctive concept descriptions, but they are used as a set and not as individual rules. Each characterization is annotated with a pair of weights.
Reference: [57] <author> George Shackelford and Dennis Volper. </author> <title> Learning k-DNF with noise in the attributes. </title> <booktitle> In COLT88: Proceedings of the First Workshop on Computational Learning Theory, </booktitle> <pages> pages 97-103, </pages> <year> 1988. </year>
Reference-contexts: Shackelford and Volper <ref> [57] </ref> present a method for PAC learning k-DNF from binary data subject to uniform attribute noise. (These are disjunctive normal form formulas whose terms contain k or fewer literals.) The sample complexity of their algorithm is m * 2 (1 fi) 2k ln ffi where K is the number of possible
Reference: [58] <author> Ulrich Siebenlist, Robert B. Simpson, and Walter Gilbert. E. </author> <title> coli RNA polymerase interacts homologously with two different promoters. </title> <journal> Cell, </journal> <volume> 20 </volume> <pages> 269-281, </pages> <year> 1980. </year>
Reference-contexts: Figure 5.1 is a highly stylized illustration of the DNA, the polymerase, and the various elements of the promoter. Further research seemed to support the presence of these regions and their biological significance <ref> [58, 68] </ref>. The Pribnow box and the recognition region are thought to be the contact points between the polymerase and the promoter. (The actual contacts can be determined in the laboratory by base conservation studies.) Together the two recurrent patterns are now known as the consensus sequence.
Reference: [59] <author> Robert Sloan. </author> <title> Types of noise in data for concept learning. </title> <booktitle> In COLT88: Proceedings of the First Workshop on Computational Learning Theory, </booktitle> <pages> pages 91-96, </pages> <year> 1988. </year>
Reference-contexts: There are a number of results for PAC learning from noisy data. In the malicious noise model [63, 25], no assumptions can be made about a noisy example. In the malicious misclassification model <ref> [59] </ref>, no assumptions can be made about the class label of a noisy example. These unlikely models pertain to situations in which an adversary determines part of the training data. This section focuses instead on a pair of PAC-learning algorithms that exploit knowledge of more typical stochastic noise processes.
Reference: [60] <author> Geoffrey G. Towell and Jude W. Shavlik. </author> <title> Using symbolic learning to improve knowledge-based neural networks. </title> <booktitle> In AAAI92: Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 177-182. </pages> <publisher> AAAI Press, </publisher> <year> 1992. </year>
Reference-contexts: Machine learning experiments for recognizing promoter sequences typically rely 77 ALAS, aacgcatacggtatTTTACCttcccagtcaagaaaactTATCTTattcccacttttc ARABAD, ttagcggatcctacCTGACGctttttatcgcaactctcTACTGTttctccatacccg BIOA, gccttctccaaaacGTGTTTtttgttgttaattcggtgTAGACTtgt aaa cctaaat DEOP1, cagaaacgttttatTCGAACatcgatctcgtcttgtgtTAGAATtctaacatacggt GALP2, cactaatttattccATGTCAcacttttcgcatctttgtTATGCTatggttatttcat ... on the promoter recognition database from the UCI Repository of Machine Learning Databases and Domain Theories <ref> [11, 26, 27, 60] </ref>. The 53 promoter sequences in this Irvine dataset were selected from the compilation of Hawley and McClure [19], and are left-aligned on the putative recognition region. (Sequence data from the Irvine dataset for the bioB promoter is shown at the bottom of Figure 5.1.
Reference: [61] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In AAAI90: Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: The annotations are based on the original compilation.) The 53 non-promoter sequences were taken from a longer sequence of DNA known not to exhibit promoter activity <ref> [61] </ref>. Hawley and McClure's compilation was based on smaller, earlier compilations and on the consensus sequence for E. coli. They evidently aligned the promoters by hand to enhance correspondence with the consensus sequence. There is no mention of a computer program or an algorithm for alignment. <p> Learning disjunctive promoter recognizers then proceeds exactly as for IPEC-DNF, the program described in the preceding chapter. Because they contain no special regions, non-promoters are handled somewhat differently. Knowing that the polymerase does not bind anywhere in these fragments <ref> [61] </ref>, I generated a number of negative examples chosen at random from each non-promoter, and combined them into a single evidence tuple. 2 Specifically, spacers were chosen at random according to one of the distributions specified previously. <p> This will be addressed later in this section by evaluating the learned classifiers on a large and entirely independent set of non-promoters. 87 generated at random from the 1500-base non-binding sequence mentioned earlier <ref> [61] </ref>. I will refer to this dataset as the Lisser and Margalit dataset. In E. coli there are roughly 5000 nucleotides for every promoter sequence. If the learned rules are to have any practical value, it follows that minimizing and accurately assessing the false-positive error rate are particularly important. <p> High false-positive rates would certainly lead to wasted laboratory effort, if the corresponding rules were used at all. To estimate this rate I first used the 1500-base non-binding strand <ref> [61] </ref>. Though this was the original source of the negative examples, it is under-utilized. This non-binding strand could be the source of about 1500 negative examples instead of just a few hundred. <p> The "FP Rate" column in Table 5.2 shows the false-positive rates for these classifiers, computed by counting the number of locations that the classifiers recognize as promoters in a 1500-base DNA sequence known not to bind to RNA Polymerase <ref> [61] </ref>. Because IPEC-DNA is able to exploit efficiently each non-promoter by proposing a number of negative examples, the IPEC-DNA false-positive rate in Table 5.2 was computed by what amounts to testing on the training data. The reason is that the non-promoters in this dataset overlap. <p> While these numbers are not strictly comparable to those given in the previous section for my learner, they are nonetheless the result of learning on a similar variant of the Lisser and Margalit dataset. Towell, Shavlik, and Noordewier <ref> [61] </ref> also take a knowledge-based approach to the promoter problem. In particular, a set of rules describing consensus-like sequences and certain conformational properties is used to construct a back-propagation neural network [54]. <p> Cross-validated scores alone are insufficient, because the training and testing populations typically differ radically. A minimum improvement would be to test learned classifiers thoroughly on the 1500-base negative strand used by Towell <ref> [61] </ref>. Although I have not used it, the strand complementary to this one is also available, simply by substituting A for T, T for A, C for G, and G for C, and then reading the strand from right to left. <p> Four promoters had to be removed because the compilation listed too few upstream bases (argCBH-P2, speC-P1, speC-P2, and speC-P3). The remainder of my dataset consists of an equal number of non-promoters (296) generated at random from a 1500-base DNA fragment known not to bind to RNA polymerase <ref> [61] </ref>. I used two data sources in order to better evaluate the false-positive error rates of my classifiers. The first was the 1500-base non-binding fragment mentioned above. The second was the genome for the bacteriophage lambda [20]. This gives about 97,000 bases that should not bind to RNA polymerase.
Reference: [62] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: where their assumptions hold, they are limited because they have no means of incorporating external background knowledge important for other problems. 1.1.2 PAC-Learning Algorithms The study of algorithms that produce concept descriptions that are probably approximately correct, the so-called PAC-learners, is one of the best-known areas of theoretical machine learning <ref> [62] </ref>. PAC learning algorithms come with a strong guarantee: given 4 enough training data, they rarely return very inaccurate concept descriptions. There are a number of results for PAC learning from noisy data. In the malicious noise model [63, 25], no assumptions can be made about a noisy example.
Reference: [63] <author> L. G. Valiant. </author> <title> Learning disjunctions of conjunctions. </title> <booktitle> In IJCAI85: Proceedings of the Ninth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 560-566. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1985. </year>
Reference-contexts: PAC learning algorithms come with a strong guarantee: given 4 enough training data, they rarely return very inaccurate concept descriptions. There are a number of results for PAC learning from noisy data. In the malicious noise model <ref> [63, 25] </ref>, no assumptions can be made about a noisy example. In the malicious misclassification model [59], no assumptions can be made about the class label of a noisy example. These unlikely models pertain to situations in which an adversary determines part of the training data.
Reference: [64] <author> Peter H. von Hippel, David G. Bear, William D. Morgan, and James A. McSwiggen. </author> <title> Protein-nucleic acid interactions in transcription: A molecular analysis. </title> <journal> Annual Review of Biochemistry, </journal> <volume> 53 </volume> <pages> 389-446, </pages> <year> 1984. </year>
Reference-contexts: Second, the length of the gaps between the start site, the Pribnow box, and the recognition region vary from promoter to promoter <ref> [64, 68] </ref>. What this means is that it is impossible to represent promoters by single contiguous sequences of DNA and simultaneously align them so that each attribute has a unique and consistent biological significance. <p> As to the probability distribution over this gap, we only know that about 77% of uniquely identified transcriptional start sites are six or seven bases downstream of the Pribnow box <ref> [64] </ref>. Orientation is likely to be key again, suggesting a quadratic form for this distribution. 5.3 Evidence Representations for Promoter Recognition The Lisser and Margalit compilation is the source for the raw sequence data used in my experiments. <p> The second is flatter. It assigns 32% probability to gaps of six or seven bases, 15% probability for gaps of five or eight bases, and 6% probability for a gap of only four bases. Each of 84 these distributions is in accordance with the molecular biology literature <ref> [64] </ref> and the rationale described in the previous section. Once having located the Pribnow box, IPEC-DNA identifies the recognition region, much the same way. I experimented with two spacer-length distributions.
Reference: [65] <author> Michael S. Waterman. </author> <title> Mathematical Methods for DNA Sequences. </title> <publisher> CRC Press, Inc., </publisher> <year> 1989. </year>
Reference-contexts: The unavoidable fact is that no published compilation is the result of optimal alignment to the consensus sequence, because the complexity of optimal multiple 78 sequence alignment is exponential in the number of sequences to be aligned <ref> [65] </ref>. The point is, instead, that the alignment strategies producing this kind of data are heuristic and will disagree among themselves. Although we do not know what it is, there is a ground-truth for every promoter, namely the actual recognition region and Pribnow box.
Reference: [66] <author> James D. Watson, Nancy H. Hopkins, Jeffrey W. Roberts, Joan Argetsinger Steitz, and Alan M. Weiner. </author> <title> Molecular Biology of the Gene. </title> <publisher> Benjamin/Cummings Publishing Company, Inc., </publisher> <year> 1987. </year>
Reference-contexts: Part of understanding gene expression involves understanding the complex regulatory signals present in DNA. A promoter is a signal that identifies specific segments of DNA that are transcribed into RNA, a necessary precursor to the production of protein <ref> [66] </ref>. RNA polymerase is the enzyme that produces RNA on the DNA template [29]. Before it produces RNA, the polymerase must recognize and bind to a promoter sequence.
Reference: [67] <author> Jarryl Wirth and Jason Catlett. </author> <title> Experiments on the costs and benefits of win-dowing in ID3. </title> <booktitle> In Proceedings of the Fifth International Conference on Machine Learning, </booktitle> <pages> pages 87-99. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1988. </year>
Reference-contexts: For example, at 200 observations this is three examples; at 225 observations four examples need to be covered. The default resource parameters given in Chapter 3 were used again here. The parameter ff was set to 0.9 throughout. In every experiment C4.5 was run with windowing disabled <ref> [67] </ref> and with discrete attribute subsetting enabled. 4.3.3 Experimental Results The range of behaviors observed is considerable, so it is difficult to choose a prototypical output. <p> IPEC-DNF was run using the default resource utilization parameters, and without a minimum-coverage threshold. In every experiment C4.5 was run with windowing disabled <ref> [67] </ref> and with discrete attribute subsetting enabled. 4.4.2 Experimental Results Table 4.1 summarizes the experimental results for the databases tested. These error rates are based on 10-fold cross-validation. The mushrooms dataset consists of 8124 examples describing poisonous and edible mushrooms with 22 nominally valued attributes.
Reference: [68] <author> Philip Youderian, Suzanne Bouvier, and Miriam M. Susskind. </author> <title> Sequence determinants of promoter activity. </title> <journal> Cell, </journal> <volume> 30 </volume> <pages> 843-853, </pages> <year> 1982. </year> <month> 181 </month>
Reference-contexts: Figure 5.1 is a highly stylized illustration of the DNA, the polymerase, and the various elements of the promoter. Further research seemed to support the presence of these regions and their biological significance <ref> [58, 68] </ref>. The Pribnow box and the recognition region are thought to be the contact points between the polymerase and the promoter. (The actual contacts can be determined in the laboratory by base conservation studies.) Together the two recurrent patterns are now known as the consensus sequence. <p> Moreover, the best overall alignment they discovered is at odds with transcriptional start site information in 28 of the 263 promoters. The upshot of all this is that while the consensus sequence has proven to be a useful concept <ref> [68] </ref>, it is still only a first-order approximation to an as yet unknown promoter concept. Consensus-sequence alignment may well associate consensus regions with consensus regions. But because the consensus sequence is an imperfect predictor of the contact regions, consensus-sequence alignment does not necessarily associate contact regions with contact regions. <p> Second, the length of the gaps between the start site, the Pribnow box, and the recognition region vary from promoter to promoter <ref> [64, 68] </ref>. What this means is that it is impossible to represent promoters by single contiguous sequences of DNA and simultaneously align them so that each attribute has a unique and consistent biological significance. <p> Mutational studies examine the effects of individual base insertions, deletions, or replacements within a promoter region. They show that the preferred spacer length is 17 bases <ref> [68] </ref>. This is consistent with consensus-sequence analysis that indicates spacers of 17 1 base pairs represent 92% of promoters [16]. In helical DNA, each base contributes about 35 degrees of twist [13].
References-found: 68

