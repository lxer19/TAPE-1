URL: ftp://ftp.ai.univie.ac.at/papers/oefai-tr-93-28.ps.Z
Refering-URL: http://www.ai.univie.ac.at/cgi-bin/tr-online/?number+93-28
Root-URL: 
Email: juffi@ai.univie.ac.at  
Title: Fossil: A robust relational learner  
Author: Johannes Furnkranz 
Address: Schottengasse 3 A-1010 Vienna Austria  
Affiliation: Austrian Research Institute for Artificial Intelligence  
Abstract: The research reported in this paper describes Fossil, an ILP system that uses a search heuristic based on statistical correlation. Several interesting properties of this heuristic are discussed, and a it is shown how it naturally can be extended with a simple, but powerful stopping criterion that is independent of the number of training examples. Instead, Fossil's stopping criterion depends on a search heuristic that estimates the utility of literals on a uniform scale. After a comparison with Foil and mFoil in the KRK domain and on the mesh data, we outline some ideas how Fossil can be adopted for top-down pruning and present some preliminary results. 
Abstract-found: 1
Intro-found: 1
Reference: [Ali and Pazzani, 1993] <author> Kamal M. Ali and Michael J. Pazzani. HYDRA: </author> <title> A noise tolerant relational concept learning algorithm. </title> <booktitle> In Proceedings of the Thirteenth Joint 18 International Conference on Artificial Intelligence, </booktitle> <pages> pages 1064-1071, </pages> <address> Chambery, France, </address> <year> 1993. </year>
Reference-contexts: Statistical measures usually improve with the size of the training sets and so does the quality of the rules induced by Fossil. While both cently been addressed with an algorithm based on Foil using probabilistic concept descriptions <ref> [Ali and Pazzani, 1993] </ref>. 6 This theory correctly classifies all but 4060 of the 262,144 possible domain examples (98.45%). 2940 positions (1.12%)with WK and WR on the same squares and 1120 positions (0.43%) where the WK is between WR and BK on the same row or file are erroneously classified (see
Reference: [Angluin and Laird, 1988] <author> D. Angluin and P. Laird. </author> <title> Learning from noisy examples. </title> <journal> Machine Learning, </journal> <volume> 2(4) </volume> <pages> 343-370, </pages> <year> 1988. </year>
Reference-contexts: Typing constraints were used to speed up the search and recursion was not allowed for efficiency reasons. Class noise in the training instances was generated according to the Classification Noise Process described in <ref> [Angluin and Laird, 1988] </ref>. In this model a noise level of means that the sign of each example is reversed with a probability of .
Reference: [Bain, 1991] <author> Michael Bain. </author> <title> Experiments in non-monotonic learning. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 380-384, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference: [Bratko and Kononenko, 1986] <author> Ivan Bratko and Igor Kononenko. </author> <title> Learning diagnostic rules from incomplete and noisy data. </title> <editor> In B. Phelps, editor, </editor> <booktitle> Interactions in AI and Statistical Methods, </booktitle> <pages> pages 142-153, </pages> <address> London, </address> <year> 1986. </year>
Reference: [Breiman et al., 1984] <author> L. Breiman, J. Friedman, R. Olshen, and C. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA, </address> <year> 1984. </year>
Reference-contexts: Coincidentially, the learned concepts are of about equal complexity at this point. * The curve for the predictive accuracy is U-shaped, similar to some results from Decision Tree learning (see e.g. <ref> [Breiman et al., 1984] </ref>). * There is a transition from overfitting the noise to over-generalizing the rules.
Reference: [Brunk and Pazzani, 1991] <author> Clifford A. Brunk and Michael J. Pazzani. </author> <title> An investigation of noise-tolerant relational concept learning algorithms. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 389-393, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the rapidly growing field of Inductive Logic Programming [Lavrac and Dzeroski, 1993]. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms, others, like mFoil [Dzeroski and Bratko, 1992a] and REP <ref> [Brunk and Pazzani, 1991] </ref>, have adapted well-known methods from attribute-value learning for the ILP framework. This paper presents Fossil, a Foil-like algorithm that uses a search heuristic based on statistical correlation (section 2). <p> In particular we see some relationship to pruning methods used e.g. in <ref> [Brunk and Pazzani, 1991] </ref> or [Srinivasan et al., 1992]. The major difference, however, is that we get a series of different concept descriptions in a general to specific order (top-down) as opposed to pruning methods the generate a most specific theory first and then successively generalize it (bottom-up).
Reference: [Buntine and Niblett, 1992] <author> Wray Buntine and Tim Niblett. </author> <title> A further comparison of splitting rules for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 75-85, </pages> <year> 1992. </year>
Reference-contexts: 0 (which in general will have a different size) and the process continues as described in [Quinlan and Cameron-Jones, 1993]. 2.2 Interesting features of the Correlation Heuristic The information gain heuristic used in C4.5 [Quinlan, 1993] and Foil has been extensively compared to other search heuristics in decision tree generation <ref> [Mingers, 1989b, Buntine and Niblett, 1992] </ref> and Inductive Logic Programming [Lavrac et al., 1992]. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data.
Reference: [Clark and Boswell, 1991] <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session of Learning, </booktitle> <pages> pages 151-163, </pages> <address> Porto, Portugal, </address> <year> 1991. </year>
Reference-contexts: useful theory. 4.4 Comparison with mFoil mFoil [Dzeroski and Bratko, 1992a] is an algorithm based on Foil that has adapted several features from the CN2 learning algorithm, such as the use of the Laplace and m-estimate as a search heuristic and the use of significance testing as a stopping criterion <ref> [Clark and Boswell, 1991] </ref>. These methods have proved very effective for noise handling. In addition mFoil uses beam search (default beam width 5) and can make use of mode and type information to reduce the search space, features that are scheduled to be incorporated into Fossil in the near future.
Reference: [Dzeroski and Bratko, 1992a] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Handling noise in Induc tive Logic Programming. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year>
Reference-contexts: Not surprisingly, noise handling methods have also entered the rapidly growing field of Inductive Logic Programming [Lavrac and Dzeroski, 1993]. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms, others, like mFoil <ref> [Dzeroski and Bratko, 1992a] </ref> and REP [Brunk and Pazzani, 1991], have adapted well-known methods from attribute-value learning for the ILP framework. This paper presents Fossil, a Foil-like algorithm that uses a search heuristic based on statistical correlation (section 2). <p> and BK on the same row or file are erroneously classified (see Appendix). (Remember that we have defined adjacent to mean adjacent or equal). 11 Foil and Fossil successively improve their predictive accuracy with increasing training set sizes, only Fossil converges towards a useful theory. 4.4 Comparison with mFoil mFoil <ref> [Dzeroski and Bratko, 1992a] </ref> is an algorithm based on Foil that has adapted several features from the CN2 learning algorithm, such as the use of the Laplace and m-estimate as a search heuristic and the use of significance testing as a stopping criterion [Clark and Boswell, 1991]. <p> However, one of the points to make here is that a good value of the m parameter is not only dependent on the amount of noise (as can be seen from the results given in <ref> [Dzeroski and Bratko, 1992a] </ref> and [Dzeroski and Bratko, 1992b]), but also on the size of the example set. Also the values for a good m found in our experiments differ considerably from the ones reported in the above papers (as does the classification accuracy for both Foil and mFoil). <p> We followed the same data and the same testing procedure described in <ref> [Dzeroski and Bratko, 1992a] </ref>. <p> That the background knowledge given in the mesh domain is apparently not very good for ILP programs can be seen from the bad results of all programs tried on it. <ref> [Dzeroski and Bratko, 1992a] </ref> discuss some of the problems current algorithms have with this domain and propose some improvements. <p> However, with this approach one has no guarantee that one does not miss a better theory with a different m. The results given in <ref> [Dzeroski and Bratko, 1992a] </ref> also indicate that the choice of a good m depends on the amount of noise in the data, while our experiments in section 4.4 also suggest a dependence on the size of the training set.
Reference: [Dzeroski and Bratko, 1992b] <author> Saso Dzeroski and Ivan Bratko. </author> <title> Using the m-estimate in Inductive Logic Programming. In Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: Thus a noise level of in our experiments is roughly equivalent to a noise level of 2 in the results reported in <ref> [Lavrac and Dzeroski, 1992, Dzeroski and Bratko, 1992b] </ref>. Noise was added incrementally, i.e. instances which had a reversed sign at a noise level 1 also had a reversed sign at a noise level 2 &gt; 1 . <p> However, one of the points to make here is that a good value of the m parameter is not only dependent on the amount of noise (as can be seen from the results given in [Dzeroski and Bratko, 1992a] and <ref> [Dzeroski and Bratko, 1992b] </ref>), but also on the size of the example set. Also the values for a good m found in our experiments differ considerably from the ones reported in the above papers (as does the classification accuracy for both Foil and mFoil).
Reference: [Dzeroski and Lavrac, 1991] <author> Saso Dzeroski and Nada Lavrac. </author> <title> Learning relations from noisy examples: An empirical comparison of LINUS and FOIL. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 399-402, </pages> <address> Evanston, Illinois, </address> <year> 1991. </year>
Reference: [Holte et al., 1989] <author> R. Holte, L. Acker, and B. Porter. </author> <title> Concept learning and the prob lem of small disjuncts. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence, </booktitle> <address> Detroit, MI, </address> <year> 1989. </year>
Reference-contexts: the noise has several disadvantages: Accuracy: The more examples there are in the noisy training set, the more spe cialized are the various clauses in the concept description, which decreases the predictive ability of each clause learned by Foil. 5 5 This Problem is known as the Small Disjuncts Problem <ref> [Holte et al., 1989] </ref> and has re 9 10 illegal (A,B,C,D,E,F) :- C = E. illegal (A,B,C,D,E,F) :- D = F. illegal (A,B,C,D,E,F) :- adjacent (A,E), adjacent (B,F). Efficiency: Foil grows an increasing number of clauses with an increasing num ber of literals.
Reference: [Lavrac and Dzeroski, 1992] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive learning of relations from noisy examples. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming, </booktitle> <pages> pages 495-516. </pages> <publisher> Academic Press Ltd., </publisher> <address> London, </address> <year> 1992. </year> <month> 19 </month>
Reference-contexts: Not surprisingly, noise handling methods have also entered the rapidly growing field of Inductive Logic Programming [Lavrac and Dzeroski, 1993]. Linus <ref> [Lavrac and Dzeroski, 1992] </ref> relies directly on the noise handling abilities of decision tree learning algorithms, others, like mFoil [Dzeroski and Bratko, 1992a] and REP [Brunk and Pazzani, 1991], have adapted well-known methods from attribute-value learning for the ILP framework. <p> Thus a noise level of in our experiments is roughly equivalent to a noise level of 2 in the results reported in <ref> [Lavrac and Dzeroski, 1992, Dzeroski and Bratko, 1992b] </ref>. Noise was added incrementally, i.e. instances which had a reversed sign at a noise level 1 also had a reversed sign at a noise level 2 &gt; 1 .
Reference: [Lavrac and Dzeroski, 1993] <author> Nada Lavrac and Saso Dzeroski. </author> <title> Inductive Logic Program ming: Techniques and Applications. </title> <publisher> Ellis Horwood, </publisher> <year> 1993. </year>
Reference-contexts: Significant effort has been made into investigating the effect of noisy data on attribute-value learning algorithms (see e.g. [Quinlan, 1993, Bratko and Kononenko, 1986, Breiman et al., 1984, Mingers, 1989a]). Not surprisingly, noise handling methods have also entered the rapidly growing field of Inductive Logic Programming <ref> [Lavrac and Dzeroski, 1993] </ref>. Linus [Lavrac and Dzeroski, 1992] relies directly on the noise handling abilities of decision tree learning algorithms, others, like mFoil [Dzeroski and Bratko, 1992a] and REP [Brunk and Pazzani, 1991], have adapted well-known methods from attribute-value learning for the ILP framework.
Reference: [Lavrac et al., 1991] <author> N. Lavrac, S. Dzeroski, and M. Grobelnik. </author> <title> Learning nonrecursive definitions of relations with LINUS. </title> <booktitle> In Proceedings of the European Working Session on Learning, Porto, </booktitle> <address> Portugal, </address> <year> 1991. </year>
Reference: [Lavrac et al., 1992] <author> Nada Lavrac, Bojan Cestnik, and Saso Dzeroski. </author> <title> Search heuris tics in empirical Inductive Logic Programming. In Logical Approaches to Machine Learning, </title> <booktitle> Workshop Notes of the 10th European Conference on AI, </booktitle> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: In the following description of its adaptation as a search heuristic for the Inductive Logic Programming algorithm Foil, we will follow the notational conventions used in <ref> [Lavrac et al., 1992] </ref>. Suppose Fossil has learned a partial clause c. Let the set of tuples T c of size n (c), containing n (c) positive and n (c) negative instances, be the current training set. <p> the process continues as described in [Quinlan and Cameron-Jones, 1993]. 2.2 Interesting features of the Correlation Heuristic The information gain heuristic used in C4.5 [Quinlan, 1993] and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989b, Buntine and Niblett, 1992] and Inductive Logic Programming <ref> [Lavrac et al., 1992] </ref>. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data.
Reference: [Mingers, 1989a] <author> John Mingers. </author> <title> An empirical comparison of pruning methods for de cision tree induction. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 227-243, </pages> <year> 1989. </year>
Reference-contexts: the cutoff of the literal with the maximum correlation has occured. * Pruning and learning are interleaved in this algorithm and can influence each other. * In Decision Tree Learning several methods for selecting the best tree from a series of trees pruned to a different degree have been developed <ref> [Mingers, 1989a] </ref>.
Reference: [Mingers, 1989b] <author> John Mingers. </author> <title> An empirical comparison of selection measures for decision-tree induction. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 319-342, </pages> <year> 1989. </year>
Reference-contexts: 0 (which in general will have a different size) and the process continues as described in [Quinlan and Cameron-Jones, 1993]. 2.2 Interesting features of the Correlation Heuristic The information gain heuristic used in C4.5 [Quinlan, 1993] and Foil has been extensively compared to other search heuristics in decision tree generation <ref> [Mingers, 1989b, Buntine and Niblett, 1992] </ref> and Inductive Logic Programming [Lavrac et al., 1992]. The general consensus seems to be that it is hard to improve on this heuristic in terms of predictive accuracy in learning from noise-free data.
Reference: [Muggleton and Feng, 1990] <author> Stephen H. Muggleton and Cao Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In Proceedings of the 1st Conference on Algorithmic Learning Theory, </booktitle> <pages> pages 1-14, </pages> <address> Tokyo, Japan, </address> <year> 1990. </year>
Reference: [Muggleton et al., 1989] <author> Stephen Muggleton, Michael Bain, Jean Hayes-Michie, and Donald Michie. </author> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning, </booktitle> <pages> pages 113-118, </pages> <year> 1989. </year>
Reference-contexts: mainly this improvement that lead to a relatively good performance of Fossil at tests on the mesh data (see section 4.5). 4 Experimental Evaluation 4.1 Setup of the Experiments For the experiments in this paper we have used the domain of recognizing illegal chess positions in the KRK end game <ref> [Muggleton et al., 1989] </ref>. The goal is to learn the concept of an illegal white-to-move position with only white king, white rook and black king on the board.
Reference: [Quinlan and Cameron-Jones, 1993] <author> John Ross Quinlan and R. M. Cameron-Jones. </author> <title> FOIL: A midterm report. </title> <booktitle> In Proceedings of the European Conference on Machine Learning, </booktitle> <pages> pages 3-20, </pages> <address> Vienna, Austria, </address> <year> 1993. </year>
Reference-contexts: The set T c is then extended to a new set of tuples T c 0 (which in general will have a different size) and the process continues as described in <ref> [Quinlan and Cameron-Jones, 1993] </ref>. 2.2 Interesting features of the Correlation Heuristic The information gain heuristic used in C4.5 [Quinlan, 1993] and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989b, Buntine and Niblett, 1992] and Inductive Logic Programming [Lavrac et al., 1992]. <p> Defining the heuristic value of determinate literals as 1 would put all determinate into the clause body. Irrelevant literals could be removed later in a post-processing phase. Values between 0 and 1 result in the behavior described in <ref> [Quinlan and Cameron-Jones, 1993] </ref>: until a literal with a correlation above a pre-set value is found, determinate literals will be added to the clause body. * Fossil's correlation coefficient | after taking absolute values and choos ing the appropriate, positive or negative, literal | allows to compare the candidate literals on
Reference: [Quinlan, 1990] <author> John Ross Quinlan. </author> <title> Learning logical definitions from relations. </title> <journal> Ma chine Learning, </journal> <volume> 5 </volume> <pages> 239-266, </pages> <year> 1990. </year>
Reference-contexts: What seems to be responsible for the drastic increase in the complexity of the learned clauses is that Foil's stopping criterion <ref> [Quinlan, 1990] </ref> is dependent on the size of the training set. In the KRK domain it performs very well on sample sizes of 100 training examples. The more this number increases, the more bits are allowed for the theory to explain the data.
Reference: [Quinlan, 1991] <author> John Ross Quinlan. </author> <title> Determinate literals in inductive logic program ming. </title> <booktitle> In Proceedings of the 8th International Workshop on Machine Learning, </booktitle> <pages> pages 442-446, </pages> <year> 1991. </year>
Reference-contexts: Note that it may happen that Fossil "refuses" to learn anything in cases where no predicate in the background knowledge has a significant correlation with the training data. 2 1 This is a super-set of Quinlan's determinate literals, but it causes the same problems as described in <ref> [Quinlan, 1991] </ref>. 2 This has actually happened several times, and is evident in the result with 50% Noise (i.e. random classification) in table 2, where Fossil did not learn a single clause in any of the 10 training sets. 4 If a clause that cannot be further extended still covers negative
Reference: [Quinlan, 1993] <author> John Ross Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, </address> <year> 1993. </year>
Reference-contexts: The set T c is then extended to a new set of tuples T c 0 (which in general will have a different size) and the process continues as described in [Quinlan and Cameron-Jones, 1993]. 2.2 Interesting features of the Correlation Heuristic The information gain heuristic used in C4.5 <ref> [Quinlan, 1993] </ref> and Foil has been extensively compared to other search heuristics in decision tree generation [Mingers, 1989b, Buntine and Niblett, 1992] and Inductive Logic Programming [Lavrac et al., 1992].
Reference: [Srinivasan et al., 1992] <author> A. Srinivasan, S. H. Muggleton, and M. E. Bain. </author> <title> Distinguish ing noise from exceptions in non-monotonic learning. </title> <booktitle> In Proceedings of the International Workshop on Inductive Logic Programming, </booktitle> <address> Tokyo, Japan, </address> <year> 1992. </year> <month> 20 </month>
Reference-contexts: In particular we see some relationship to pruning methods used e.g. in [Brunk and Pazzani, 1991] or <ref> [Srinivasan et al., 1992] </ref>. The major difference, however, is that we get a series of different concept descriptions in a general to specific order (top-down) as opposed to pruning methods the generate a most specific theory first and then successively generalize it (bottom-up).
References-found: 25

