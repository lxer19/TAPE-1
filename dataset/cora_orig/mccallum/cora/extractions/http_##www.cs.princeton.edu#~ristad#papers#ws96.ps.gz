URL: http://www.cs.princeton.edu/~ristad/papers/ws96.ps.gz
Refering-URL: http://www.cs.princeton.edu/~ristad/papers/ws96.html
Root-URL: http://www.cs.princeton.edu
Title: DEPENDENCY LANGUAGE MODELING  
Author: Andreas Stolcke (SRI, Team Leader) Ciprian Chelba (JHU) David Engle (DoD) Victor Jimenez Lidia Mangu (JHU) Harry Printz (IBM) Eric Ristad (Princeton) Roni Rosenfeld (CMU) Dekai Wu (Hong Kong UST) collaborators Fred Jelinek (JHU) Sanjeev Khudanpur (JHU) 
Date: March 19, 1997  
Affiliation: (Univ. Politecnica Valencia)  
Abstract: WS96 Project Report 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Adam L. Berger, Stephen A. Della Pietra, and Vincent J. Della Pietra. </author> <title> A maximum entropy approach to natural language processing. </title> <journal> Computational Linguistics, </journal> <volume> 22(1) </volume> <pages> 39-71, </pages> <year> 1996. </year>
Reference-contexts: For example, the maximum entropy trigram outperforms both the interpolated trigram [12] and the backoff trigram [14] in test set perplexity as well as in speech recognizer word error rate. For all these reasons, the maximum entropy framework has become the framework of choice for statistical language modeling <ref> [18, 1, 28] </ref>. 4 This section has a dual purpose. First, it reviews the basics of maximum entropy modeling, as required to understand the discussion of our language model in Section 4. <p> describes a generic software tool for training and using maximum entropy models|the Maximum Entropy Modeling Toolkit (MEMT), first designed and implemented by Eric Ristad at WS96. 3.1 The Maximum Entropy Modeling Framework The fundamental problem of statistical modeling is to induce a joint probability model p : X; Y ! <ref> [0; 1] </ref> from a finite corpus of observations f (x 1 ; y 1 ); : : : ; (x T ; y T )g drawn from a discrete joint domain X; Y . <p> Consequently, there have been only a handful of successful maximum entropy models in the LVCSR community, and these have all occurred at IBM Watson Research <ref> [18, 24, 1] </ref>. The third obstacle to the use of maximum entropy techniques in the LVCSR community is that it is not obvious how to support arbitrary maximum entropy models in a clean and efficient manner. Consequently, no general-purpose software for maximum entropy modeling currently exists. <p> Moreover, this choice for has the advantage of discarding the words and grammatical structure that we believe to be irrelevant (or at least less relevant) to the prediction at hand. 4.4 Maximum Entropy Formulation: Constraints We chose to formulate our model using the method of constrained maximum entropy <ref> [1] </ref>.
Reference: [2] <author> Michael John Collins. </author> <title> A new statistical parser based on bigram lexical dependencies. </title> <booktitle> In Proceedings of the 34th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pages 184-191, </pages> <address> Santa Cruz, CA, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: fl ) is not a good approximation of P (S) we can assume that the probabilities of all recognition hypotheses to be compared will be underestimated by roughly the same factor, which can therefore be ignored. 2.4.1 Parsing and Tagging We used an existing state-of-the-art parser developed by Michael Collins <ref> [2] </ref> to find the best parse K fl and then simply evaluate our model on the pair (S; K fl ). <p> For the tagger, this is the percentage of tags that were labeled correctly. For the parser, it is customary to evaluate the precision and recall of constituents (bracket pairs). For comparison purposes, we also give the corresponding figures for the Wall Street Journal corpus, as reported in [25] and <ref> [2] </ref>, respectively. Note, however, that the Switchboard tagging and parsing tasks were actually made more difficult by the way the data was conditioned, in particular by removing all case distinctions and punctuation. Case is used by the tagger, and punctuation benefits both tagger and parser.
Reference: [3] <author> I. Cziszar. </author> <title> Why least squares and maximum entropy? An axiomatic approach to inference. </title> <journal> Annals of Statistics, </journal> <volume> 19(4) </volume> <pages> 2032-2067, </pages> <year> 1991. </year>
Reference-contexts: (x; y) in P that maximizes the entropy H (p) with respect to all distributions in P: H (p) = x;y The maximum entropy distribution p fl (x; y) is the one that is most faithful to our constraints, because it makes no additional assumptions beyond what has been specified <ref> [8, 9, 3] </ref>.
Reference: [4] <author> J. N. Darroch and D. Ratcliff. </author> <title> Generalized iterative scaling for log-linear models. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 43(5) </volume> <pages> 1470-1480, </pages> <year> 1972. </year>
Reference-contexts: This criterion shaped the overall design of the toolkit and also guided our choice of parameter estimation algorithms. In particular, it led us to use the recently developed improved iterative scaling (IIS) algorithm [23] for parameter estimation rather than the classic generalized iterative scaling (GIS) algorithm <ref> [4] </ref>. GIS requires that the 1 Sanjeev Khudanpur, personal communication. 8 feature activations always sum to a constant. In contrast, IIS imposes no requirements on the features and it therefore provides a simpler interface.
Reference: [5] <author> S. Della Pietra, V. Della Pietra, J. Gillett, J. Lafferty, H. Printz, and L. Ures. </author> <title> Inference and Estimation of a Long-Range Trigram Model. </title> <type> Technical Report CMU-CS-94-188, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: For example, the word barked is the main verb in the example above, bound to its subject by the link type S. Both the link type and the word dog at the other end should therefore help predict barked. The earlier work reported in <ref> [5] </ref> employed just such an approach, using a link grammar. However, the particular grammar used there did not have a strong linguistic motivation, since it was based exclusively upon informative word pairs, and not upon their parts of speech.
Reference: [6] <author> J. J. Godfrey, E. C. Holliman, and J. McDaniel. </author> <title> SWITCHBOARD: Telephone speech corpus for research and development. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 517-520, </pages> <address> San Francisco, </address> <month> March </month> <year> 1992. </year> <month> 28 </month>
Reference-contexts: out some ancillary experiments that were not related to the dependency language model per se, but which were important to calibrate performance and to verify various components of the experimental setup. 6.1 Training and Testing Corpora All experiments were carried out on the Switchboard corpus of spontaneous conversational English speech <ref> [6] </ref>. A hand-annotated and hand-parsed version of much of this corpus was created by the Penn Treebank project [19] and made available to us in preliminary form by the LDC.
Reference: [7] <author> Peter A. Heeman, Kyung ho Loken-Kim, and James F. Allen. </author> <title> Combining the detection and correction of speech repairs. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <address> Philadelphia, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Otherwise, we have to deal with various spontaneous speech effects, such as disfluencies and syntactically incomplete constructions that might affect the dependency model in particular. For example, we might want to try to filter disfluencies prior to parsing. This requires a separate model for disfluencies, possible similar to <ref> [7, 31] </ref>. Finally, there is a fundamental concern about the applicability of an N -best rescoring approach to a complex linguistically based model like ours. The best achievable error rate in our N -best lists was 30%, making at least every third word incorrect on average.
Reference: [8] <author> E. T. Jaynes. </author> <booktitle> Probability Theory in Science and Engineering, volume 4 of Colloquium Lectures in Pure and Applied Science. </booktitle> <publisher> Socony Mobil Oil Company, </publisher> <address> Dallas, TX, </address> <year> 1958. </year>
Reference-contexts: The encoding for a particular domain is not necessarily a trivial matter, and reflects the structural complexity of the domain. Still, the separation of tasks makes the overall implementation effort much less cumbersome. 3 Maximum Entropy Modeling The maximum entropy framework <ref> [8, 9] </ref> is a powerful method for building statistical models. It is expressive, allowing modelers to easily represent their special insights into the data generating machinery. It is statistically efficient, because it models the intersection of complex events without increasing the number of parameters or fragmenting the training data. <p> (x; y) in P that maximizes the entropy H (p) with respect to all distributions in P: H (p) = x;y The maximum entropy distribution p fl (x; y) is the one that is most faithful to our constraints, because it makes no additional assumptions beyond what has been specified <ref> [8, 9, 3] </ref>.
Reference: [9] <author> E. T. Jaynes. </author> <title> Where do we stand on maximum entropy? In R. </title> <editor> D. Levine and M. Tribus, editors, </editor> <title> The Maximum Entropy Formalism. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1978. </year>
Reference-contexts: The encoding for a particular domain is not necessarily a trivial matter, and reflects the structural complexity of the domain. Still, the separation of tasks makes the overall implementation effort much less cumbersome. 3 Maximum Entropy Modeling The maximum entropy framework <ref> [8, 9] </ref> is a powerful method for building statistical models. It is expressive, allowing modelers to easily represent their special insights into the data generating machinery. It is statistically efficient, because it models the intersection of complex events without increasing the number of parameters or fragmenting the training data. <p> (x; y) in P that maximizes the entropy H (p) with respect to all distributions in P: H (p) = x;y The maximum entropy distribution p fl (x; y) is the one that is most faithful to our constraints, because it makes no additional assumptions beyond what has been specified <ref> [8, 9, 3] </ref>.
Reference: [10] <author> Frederick Jelinek. </author> <title> Up from trigrams! The struggle for improved language models. </title> <booktitle> In Proceedings 2nd European Conference on Speech Communication and Technology, </booktitle> <pages> pages 1037-1040, </pages> <address> Genova, Italy, </address> <month> September </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Current language models for speech recognition are based on the statistics of N -grams, which are tuples of N adjacent words. In an N -gram model, a given word is predicted by its N 1 immediate predecessors. For the case N = 3, the so-called trigram model <ref> [10] </ref>, this method is both computationally feasible and remarkably successful. The success of this approach indicates that for predicting a given word, its two immediate predecessors contain much useful information.
Reference: [11] <author> Frederick Jelinek, John D. Lafferty, and Robert L. Mercer. </author> <title> Basic methods of probabilistic context free grammars. </title> <editor> In Pietro Laface and Renato De Mori, editors, </editor> <booktitle> Speech Recognition and Understanding. Recent Advances, Trends, and Applications, volume F75 of NATO ASI Series, </booktitle> <pages> pages 345-360. </pages> <publisher> Springer Verlag, </publisher> <address> Berlin, </address> <year> 1992. </year> <booktitle> Proceedings of the NATO Advanced Study Institute, </booktitle> <address> Cetraro, Italy, </address> <month> July </month> <year> 1990. </year>
Reference-contexts: The first forays in this direction were based on stochastic context-free grammars <ref> [11] </ref>, which assume that words and phrases of a sentence form a tree-like structure.
Reference: [12] <author> Frederick Jelinek and Robert L. Mercer. </author> <title> Interpolated estimation of Markov source parameters from sparse data. </title> <booktitle> In Proceedings Workshop on Pattern Recognition in Practice, </booktitle> <pages> pages 381-397, </pages> <address> Amsterdam, </address> <year> 1980. </year>
Reference-contexts: It is statistically efficient, because it models the intersection of complex events without increasing the number of parameters or fragmenting the training data. And it provides strong models, models that outperform their traditional variants with much less tweaking. For example, the maximum entropy trigram outperforms both the interpolated trigram <ref> [12] </ref> and the backoff trigram [14] in test set perplexity as well as in speech recognizer word error rate. For all these reasons, the maximum entropy framework has become the framework of choice for statistical language modeling [18, 1, 28]. 4 This section has a dual purpose.
Reference: [13] <author> Daniel Jurafsky, Chuck Wooters, Jonathan Segal, Andreas Stolcke, Eric Fosler, Gary Tajchman, and Nelson Morgan. </author> <title> Using a stochastic context-free grammar as a language model for speech recognition. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume 1, </volume> <pages> pages 189-192, </pages> <address> Detroit, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: The first forays in this direction were based on stochastic context-free grammars [11], which assume that words and phrases of a sentence form a tree-like structure. While these approaches have been successful in some applications <ref> [33, 17, 13] </ref>, language models based on context-free grammars still cannot compete with N -gram models in domains with large vocabularies, relatively unrestricted speech, and large amounts of training data (such as the Wall Street Journal domain).
Reference: [14] <author> Slava M. Katz. </author> <title> Estimation of probabilities from sparse data for the language model component of a speech recognizer. </title> <journal> IEEE Transactions on Acoustics, Speech, and Signal Processing, </journal> <volume> 35(3) </volume> <pages> 400-401, </pages> <month> March </month> <year> 1987. </year>
Reference-contexts: And it provides strong models, models that outperform their traditional variants with much less tweaking. For example, the maximum entropy trigram outperforms both the interpolated trigram [12] and the backoff trigram <ref> [14] </ref> in test set perplexity as well as in speech recognizer word error rate. For all these reasons, the maximum entropy framework has become the framework of choice for statistical language modeling [18, 1, 28]. 4 This section has a dual purpose.
Reference: [15] <author> S. Kullback. </author> <title> Information Theory and Statistics. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1959. </year>
Reference-contexts: This is the beauty of the maximum entropy framework <ref> [15] </ref>. 3.1.3 Conditional Models Various difficulties arise when these ideas are applied to discrete time series problems. The first difficulty is that we must assign probability to strings of arbitrary length, a task for which we cannot employ joint models over finite dimensional spaces.
Reference: [16] <author> John Lafferty, Daniel Sleator, and Davy Temperley. </author> <title> Grammatical trigrams: A probabilistic model of link grammar. </title> <booktitle> In Proceedings of the 1992 AAAI Fall Symposium on Probabilistic Approaches to Natural Language, </booktitle> <address> Cambridge, MA, </address> <year> 1992. </year>
Reference-contexts: Current approaches in linguistically motivated language modeling therefore strive to use grammars that are lexicalized, that is, where all structural elements are directly anchored in words. This is true of so-called dependency grammars and a variant known as link grammars <ref> [29, 16] </ref>. A dependency grammar or link grammar represents grammatical structure by connecting linguistically related words with labeled arcs. The labels denote the type of grammatical relation, such as `subject of verb' or `adjective modifying noun'. Consider the link structure for the example sentence, shown in Figure 1.
Reference: [17] <author> K. Lari and S. J. Young. </author> <title> Applications of stochastic context-free grammars using the Inside-Outside algorithm. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 5 </volume> <pages> 237-257, </pages> <year> 1991. </year>
Reference-contexts: The first forays in this direction were based on stochastic context-free grammars [11], which assume that words and phrases of a sentence form a tree-like structure. While these approaches have been successful in some applications <ref> [33, 17, 13] </ref>, language models based on context-free grammars still cannot compete with N -gram models in domains with large vocabularies, relatively unrestricted speech, and large amounts of training data (such as the Wall Street Journal domain).
Reference: [18] <author> Raymond Lau, Ronald Rosenfeld, and Salim Roukos. </author> <title> Trigger-based language models: a maximum entropy approach. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume II, </volume> <pages> pages 45-48, </pages> <address> Minneapolis, </address> <month> April </month> <year> 1993. </year>
Reference-contexts: For example, the maximum entropy trigram outperforms both the interpolated trigram [12] and the backoff trigram [14] in test set perplexity as well as in speech recognizer word error rate. For all these reasons, the maximum entropy framework has become the framework of choice for statistical language modeling <ref> [18, 1, 28] </ref>. 4 This section has a dual purpose. First, it reviews the basics of maximum entropy modeling, as required to understand the discussion of our language model in Section 4. <p> Consequently, there have been only a handful of successful maximum entropy models in the LVCSR community, and these have all occurred at IBM Watson Research <ref> [18, 24, 1] </ref>. The third obstacle to the use of maximum entropy techniques in the LVCSR community is that it is not obvious how to support arbitrary maximum entropy models in a clean and efficient manner. Consequently, no general-purpose software for maximum entropy modeling currently exists. <p> If we retain only one word of finite context, we dispense with trigram constraints; if we retain no words of finite context, we dispense with bigram constraints as well. These notions are more fully described in <ref> [18, 27] </ref>. 4.4.2 Other Constraints We depart from other maximum entropy language models by incorporating two other kinds of constraints: (word, disjunct class) and link bigram. <p> For all constraints, we used ratios of counts, or ratios of smoothed counts, to compute empirical expectations. 13 links are at the bottom of the page, outermost at the top. 4.5 Comparison with Triggers We caution the reader against equating link bigrams with triggers, a notion investigated in <ref> [18, 27] </ref>. When dog operates as a trigger, it elevates the probability of barked (and presumably depresses the probability of non-canine verbs like watusied) at every subsequent position of the sentence.
Reference: [19] <author> Mitchell P. Marcus, Beatrice Santorini, and Mary Ann Marcinkiewicz. </author> <title> Building a large annotated corpus of English: The Penn Treebank. </title> <journal> Computational Linguistics, </journal> <volume> 19(2) </volume> <pages> 313-330, </pages> <year> 1993. </year>
Reference-contexts: The link labels are derived from the nonterminal categories found in the Treebank annotation system <ref> [19] </ref>. 20 Frequency Link a ll lc lr 37360 ! CC S VP 24091 ! DT NP NN 22206 IN SBAR S 14745 VB VP NP 12525 VBZ VP NP 9587 VBP VP SBAR 9343 VBD VP NP 7892 ! PRN S VP 7847 WHADVP SBAR S 7565 WHNP SBAR S <p> A hand-annotated and hand-parsed version of much of this corpus was created by the Penn Treebank project <ref> [19] </ref> and made available to us in preliminary form by the LDC.
Reference: [20] <author> Marie Meteer et al. </author> <title> Dysfluency annotation stylebook for the Switchboard corpus. Distributed by LDC, </title> <address> ftp://ftp.cis.upenn.edu/pub/treebank/swbd/doc/DFL-book.ps.gz, </address> <month> February </month> <year> 1995. </year> <note> Revised June 1995 by Ann Taylor. </note>
Reference-contexts: The automatic linguistic segmentation of spontaneous speech is the subject of ongoing research [30]. To work around this problem we relied on the hand-segmentation of the word-level transcripts that had been done as part of the treebanking effort <ref> [20] </ref>. A total of 1.4 million words from the training corpus had been annotated for linguistic segment (utterance) boundaries, from which we drew the training material for our tagger, parser, and the dependency model itself.
Reference: [21] <author> Herman Ney and Ute Essen. </author> <title> On smoothing techniques for bigram-based natural language modelling. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <pages> pages 825-828, </pages> <address> Toronto, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: We refer to such a model as a "2-4-2 trigram." (For comparison, the standard Switchboard backoff model is a 1-1-2 trigram.) For smoothing in both backoff and ME models, the N -gram counts were discounted using the "absolute discounting" scheme described in <ref> [21] </ref>. Table 12 summarizes word error rates obtained for various N -gram models, of both the backoff and the maximum entropy varieties. All results were obtained using the same language model weight (12) and insertion penalty (-10). For the bigram, backoff and ME models perform almost identically.
Reference: [22] <author> M. Ostendorf, A. Kannan, S. Austin, O. Kimball, R. Schwartz, and J. R. Rohlicek. </author> <title> Integration of diverse recognition methodologies through reevaluation of n-best sentence hypotheses. </title> <booktitle> In Proceedings DARPA Speech and Natural Language Processing Workshop, </booktitle> <pages> pages 83-87, </pages> <address> Pacific Grove, CA, </address> <month> February </month> <year> 1991. </year> <institution> Defense Advanced Research Projects Agency, Information Science and Technology Office. </institution> <month> 29 </month>
Reference-contexts: Since the dependency model does not operate in an incremental, left-to-right fashion, it was not possible to integrate the language model directly into the search. Instead, we employed a rescoring approach using N -best lists <ref> [22] </ref>. For each utterance we generated the 100 best hypotheses using a standard trigram model. These N -best lists were then reordered using language model scores obtained from the dependency model. To this end, each hypothesis was tagged and parsed.
Reference: [23] <author> S. D. Pietra, V. D. Pietra, and J. Lafferty. </author> <title> Inducing features of random fields. </title> <type> Technical Report CMU-CS-95-144, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> May </month> <year> 1995. </year>
Reference-contexts: Consequently, the toolkit must support arbitrary features in a conceptually simple manner. This criterion shaped the overall design of the toolkit and also guided our choice of parameter estimation algorithms. In particular, it led us to use the recently developed improved iterative scaling (IIS) algorithm <ref> [23] </ref> for parameter estimation rather than the classic generalized iterative scaling (GIS) algorithm [4]. GIS requires that the 1 Sanjeev Khudanpur, personal communication. 8 feature activations always sum to a constant. In contrast, IIS imposes no requirements on the features and it therefore provides a simpler interface. <p> The executable me.checker returns success (0) if and only if all files are compatible. me.estimate model.in events n model.out Given a parameters file model.in and an events file events, the executable me.estimate performs n iterations of improved iterative scaling <ref> [23] </ref> and writes the revised parameters to model.out after each iteration.
Reference: [24] <author> A. Ratnaparkhi, S. Roukos, and R. T. Ward. </author> <title> A maximum entropy model for parsing. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <pages> pages 803-806, </pages> <address> Yokohama, </address> <month> September </month> <year> 1994. </year>
Reference-contexts: Consequently, there have been only a handful of successful maximum entropy models in the LVCSR community, and these have all occurred at IBM Watson Research <ref> [18, 24, 1] </ref>. The third obstacle to the use of maximum entropy techniques in the LVCSR community is that it is not obvious how to support arbitrary maximum entropy models in a clean and efficient manner. Consequently, no general-purpose software for maximum entropy modeling currently exists.
Reference: [25] <author> Adwait Ratnaparkhi. </author> <title> A maximum entropy model for part-of-speech tagging. </title> <editor> In Eric Brill and Kenneth Church, editors, </editor> <booktitle> Proceedings of the Conference on Empirical Methods in Natural Language Processing, </booktitle> <pages> pages 133-142, </pages> <institution> University of Pennsylvania, </institution> <address> Philadelphia, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Collins's parser does not operate directly on word sequences, however. Rather, it expects a sequence of part-of-speech tags T as input. It finds K fl that maximizes P (S; K fl ; T ). We relied on another existing tool, Adwait Ratnaparkhi's maximum entropy part-of-speech tagger <ref> [25] </ref> to compute the best tagging T fl for a 3 given sentence S, and then applied the parser to T fl . Note that this represents another approximation since we are not jointly optimizing T and K. <p> For the tagger, this is the percentage of tags that were labeled correctly. For the parser, it is customary to evaluate the precision and recall of constituents (bracket pairs). For comparison purposes, we also give the corresponding figures for the Wall Street Journal corpus, as reported in <ref> [25] </ref> and [2], respectively. Note, however, that the Switchboard tagging and parsing tasks were actually made more difficult by the way the data was conditioned, in particular by removing all case distinctions and punctuation. Case is used by the tagger, and punctuation benefits both tagger and parser.
Reference: [26] <author> E. S. Ristad. </author> <title> Maximum entropy modeling toolkit. </title> <type> Technical report, </type> <institution> Department of Computer Science, Princeton University, Princeton, NJ, </institution> <month> January </month> <year> 1997. </year> <note> 1.5 Beta. </note>
Reference-contexts: Consequently, no general-purpose software for maximum entropy modeling currently exists. At WS96, we overcame these three obstacles by designing and implementing a general purpose Maximum Entropy Modeling Toolkit <ref> [26] </ref>. Our design was shaped by the four criteria of generality, efficiency, interoperability, and robustness. These criteria led to a file-based interface consisting of three file formats, along with three primary executables. Our prototype implementation is general, easy to use, and fast. <p> Special thanks go to Adwait Rathnaparki for making his tagger available, and to Michael Collins for a close collaboration concerning the use of his parser. This report was written and edited by an assortment of group members. Eric Ristad wrote Section 3, with portions excerpted from <ref> [26] </ref>. Harry Printz wrote Section 4 and portions of Section 2, and helped with editing. Dekai Wu wrote Section 5. Andreas Stolcke wrote the remaining sections and was responsible for overall editing. Fred Jelinek and Ciprian Chelba commented on drafts and made numerous corrections and valuable suggestions.
Reference: [27] <author> Ronald Rosenfeld. </author> <title> Adaptive Statistical Language Modeling: A Maximum Entropy Approach. </title> <type> PhD thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, Pittsburgh, </institution> <month> April </month> <year> 1994. </year> <note> Technical Report CMU-CS-94-138. </note>
Reference-contexts: If we retain only one word of finite context, we dispense with trigram constraints; if we retain no words of finite context, we dispense with bigram constraints as well. These notions are more fully described in <ref> [18, 27] </ref>. 4.4.2 Other Constraints We depart from other maximum entropy language models by incorporating two other kinds of constraints: (word, disjunct class) and link bigram. <p> For all constraints, we used ratios of counts, or ratios of smoothed counts, to compute empirical expectations. 13 links are at the bottom of the page, outermost at the top. 4.5 Comparison with Triggers We caution the reader against equating link bigrams with triggers, a notion investigated in <ref> [18, 27] </ref>. When dog operates as a trigger, it elevates the probability of barked (and presumably depresses the probability of non-canine verbs like watusied) at every subsequent position of the sentence.
Reference: [28] <author> Ronald Rosenfeld. </author> <title> A maximum entropy approach to adaptive statistical language modeling. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 10 </volume> <pages> 187-228, </pages> <year> 1996. </year>
Reference-contexts: For example, the maximum entropy trigram outperforms both the interpolated trigram [12] and the backoff trigram [14] in test set perplexity as well as in speech recognizer word error rate. For all these reasons, the maximum entropy framework has become the framework of choice for statistical language modeling <ref> [18, 1, 28] </ref>. 4 This section has a dual purpose. First, it reviews the basics of maximum entropy modeling, as required to understand the discussion of our language model in Section 4.
Reference: [29] <author> Daniel D. K. Sleator and Davy Temperley. </author> <title> Parsing english with a link grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> Carnegie Mellon University, School of Computer Science, Pittsburgh, </institution> <year> 1991. </year>
Reference-contexts: Current approaches in linguistically motivated language modeling therefore strive to use grammars that are lexicalized, that is, where all structural elements are directly anchored in words. This is true of so-called dependency grammars and a variant known as link grammars <ref> [29, 16] </ref>. A dependency grammar or link grammar represents grammatical structure by connecting linguistically related words with labeled arcs. The labels denote the type of grammatical relation, such as `subject of verb' or `adjective modifying noun'. Consider the link structure for the example sentence, shown in Figure 1.
Reference: [30] <author> Andreas Stolcke and Elizabeth Shriberg. </author> <title> Automatic linguistic segmentation of conversational speech. </title> <booktitle> In Proceedings of the International Conference on Spoken Language Processing, </booktitle> <volume> volume 2, </volume> <pages> pages 1005-1008, </pages> <address> Philadelphia, </address> <month> October </month> <year> 1996. </year>
Reference-contexts: Unfortunately, the Switchboard corpus as supplied to us was not segmented in that way, since spontaneous speech has no simple cues for utterance boundaries. The automatic linguistic segmentation of spontaneous speech is the subject of ongoing research <ref> [30] </ref>. To work around this problem we relied on the hand-segmentation of the word-level transcripts that had been done as part of the treebanking effort [20]. <p> That model used a somewhat different smoothing method, which might account for the difference; in any case, 2 To extend the linguistic segmentation to the full training corpus, the automatic segmenter of <ref> [30] </ref> was used. 25 Language model WER 2-4 backoff bigram 48.0 2-4 ME bigram 48.3 2-4-2 backoff trigram 46.3 1-1-2 backoff trigram 46.2 2-4-2 ME trigram 48.4 2-4-2 ME trigram (no smoothing) 48.7 Table 12: N -gram results for backoff and ME models.
Reference: [31] <author> Andreas Stolcke and Elizabeth Shriberg. </author> <title> Statistical language modeling for speech disfluencies. </title> <booktitle> In Proceedings of the IEEE Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> volume I, </volume> <pages> pages 405-408, </pages> <address> Atlanta, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Otherwise, we have to deal with various spontaneous speech effects, such as disfluencies and syntactically incomplete constructions that might affect the dependency model in particular. For example, we might want to try to filter disfluencies prior to parsing. This requires a separate model for disfluencies, possible similar to <ref> [7, 31] </ref>. Finally, there is a fundamental concern about the applicability of an N -best rescoring approach to a complex linguistically based model like ours. The best achievable error rate in our N -best lists was 30%, making at least every third word incorrect on average.
Reference: [32] <author> M. Weintraub, Y. Aksu, S. Dharanipragada, S. Khudanpur, H. Ney, J. Prange, A. Stolcke, F. Jelinek, and E. Shriberg. </author> <title> Lm95 project report: Fast training and portability. Research Notes No. 1, Center for Language and Speech Processing, </title> <institution> Johns Hopkins University, Baltimore, </institution> <month> February </month> <year> 1996. </year>
Reference-contexts: The second obstacle is that a straightforward implementation of the maximum entropy framework will be computationally infeasible. For example, a recent implementation of a maximum entropy trigram for the Switchboard corpus <ref> [32] </ref> required over four months to train on a top-of-the-line workstation. 1 An efficient implementation of the maximum entropy framework requires careful programming and many little-known computational tricks. <p> This is especially surprising as a trigram ME model developed at last year's workshop (LM95) was competitive with the standard trigram <ref> [32] </ref>.
Reference: [33] <author> J. H. Wright. </author> <title> LR parsing of probabilistic grammars with input uncertainty for speech recognition. </title> <booktitle> Computer Speech and Language, </booktitle> <volume> 4 </volume> <pages> 297-323, </pages> <year> 1990. </year> <month> 30 </month>
Reference-contexts: The first forays in this direction were based on stochastic context-free grammars [11], which assume that words and phrases of a sentence form a tree-like structure. While these approaches have been successful in some applications <ref> [33, 17, 13] </ref>, language models based on context-free grammars still cannot compete with N -gram models in domains with large vocabularies, relatively unrestricted speech, and large amounts of training data (such as the Wall Street Journal domain).
References-found: 33

