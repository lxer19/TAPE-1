URL: ftp://ftp.cs.umass.edu/pub/osl/papers/nips97.ps.gz
Refering-URL: http://spa-www.cs.umass.edu/bibliography.html
Root-URL: 
Title: Learning to Schedule Straight-Line Code  
Author: J. Eliot B. Moss Paul E. Utgoff John Cavazos Doina Precup Darko Stefanovi c Carla Brodley David Scheeff 
Address: Mass. Amherst, MA 01003  Mass. Amherst, MA 01003  Mass. Amherst, MA 01003  Mass. Amherst, MA 01003  Mass. Amherst, MA 01003  W. Lafayette, IN 47907  W. Lafayette, IN 47907  
Affiliation: Dept. of Comp. Sci. Univ. of  Dept. of Comp. Sci. Univ. of  Dept. of Comp. Sci. Univ. of  Dept. of Comp. Sci. Univ. of  Dept. of Comp. Sci. Univ. of  Sch. of Elec. and Comp. Eng. Purdue University  Sch. of Elec. and Comp. Eng. Purdue University  
Abstract: Execution speed of programs on modern computer architectures is sensitive, by a factor of two or more, to the order in which instructions are presented to the processor. To realize potential execution efficiency, it is now customary for an optimizing compiler to employ a heuristic algorithm for instruction scheduling. These algorithms are painstakingly hand-crafted, which is expenseive and time-consuming. We show how to cast the instruction scheduling problem as a learning task, so that one obtains the heuristic scheduling algorithm automatically. Our focus is the narrower problem of scheduling straight-line code, also known as a basic block of instructions. Our empirical results show that just a few features are adequate for quite good performance at this task for a real modern processor, and that any of several supervised learning methods perform nearly optimally with respect to the features used. Category: Applications (compiler optimization) Original: This work has not been submitted elsewhere. Presentation: We prefer oral presentation. Contact author: Eliot Moss 
Abstract-found: 1
Intro-found: 1
Reference: [Haykin, 1994] <author> Haykin, S. </author> <year> (1994). </year> <title> Neural networks: A comprehensive foundation. </title> <address> New York, NY: </address> <publisher> Macmillan. </publisher>
Reference: [Rumelhart, Hinton & Williams, 1986] <author> Rumelhart, D. E., Hinton, G. E., & Williams, R.J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart & McClelland (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference-contexts: Though the second layer is linear in the instruction features, the boolean features are nonlinear in the instruction features. Finally, the fourth method is a feed-forward artificial neural network (NN) <ref> [Rumelhart, Hinton & Williams, 1986] </ref>. Our particular network uses scaled conjugate gradient descent in its back-propagation, which gives results comparable to back-propagation with momentum, but converges much faster.
Reference: [Scheeff, et al., 1997] <author> Scheeff, D., Brodley, C., Moss, E., Cavazos, J., Stefanovic, D. </author> <year> (1997). </year> <title> Applying Reinforcement Learning to Instruction Scheduling within Basic Blocks. </title> <note> Submitted for publication. </note>
Reference-contexts: Still, both measures of performance are quite good. What about reinforcement learning? We ran experiments with temporal difference (TD) learning, some of which are described in <ref> [Scheeff, et al., 1997] </ref> and the results are not as good. This problem appears to be tricky to cast in a form suitable for TD, because TD looks at candidate instructions in isolation, rather than in a preference setting.
Reference: [Sutton, 1988] <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the method of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: [Utgoff, Berkman & Clouse, in press] <author> Utgoff, P. E., Berkman, N. C., & Clouse, J. A. </author> <title> (in press). Decision tree induction based on efficient tree restructuring. </title> <booktitle> Machine Learning. </booktitle>
Reference-contexts: We consider four methods here. The first is the decision tree induction program ITI <ref> [Utgoff, Berkman & Clouse, in press] </ref>. Each triple that is an example of the relation is translated into a vector of feature values, as described in more detail below. Some of the features pertain to the current partial schedule, and others pertain to the pair of candidate instructions.
Reference: [Utgoff & Precup, 1997] <author> Utgoff, P. E., & Precup, D. </author> <year> (1997). </year> <title> Constructive function approximation, </title> <type> (Technical Report 97-04), </type> <institution> Amherst, MA: University of Massachusetts, Department of Computer Science. </institution>
Reference-contexts: Thus, table lookup is unbiased and one would expect it to give the best predictions possible for the chosen features, assuming the statistics of the training and test sets are consistent. The third method is the ELF function approximator <ref> [Utgoff & Precup, 1997] </ref>, which constructs additional features (much like a hidden unit) as necessary while it updates its representation of the function that it is learning. The function is represented by two layers of mapping.
References-found: 6

