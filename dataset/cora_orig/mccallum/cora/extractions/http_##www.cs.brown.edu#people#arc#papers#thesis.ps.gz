URL: http://www.cs.brown.edu/people/arc/papers/thesis.ps.gz
Refering-URL: http://www.cs.brown.edu/people/kek/ai-lunch/ai-fall97.html
Root-URL: 
Title: Abstract of "Exact and Approximate Algorithms for Partially Observ--able Markov Decision Processes" by  
Author: Anthony Rocco Cassandra, Ph.D., Brown 
Date: May 1998  
Pubnum: University,  
Abstract: Automated sequential decision making is crucial in many contexts. In the face of uncertainty, this task becomes even more important, though at the same time, computing optimal decision policies becomes more complex. The more sources of uncertainty there are, the harder the problem becomes to solve. In this work, we look at sequential decision making in environments where the actions have probabilistic outcomes and in which the system state is only partially observable. We focus on using a model called a partially observable Markov decision process (POMDP) and explore algorithms which address computing both optimal and approximate policies for use in controlling processes that are modeled using POMDPs. Although solving for the optimal policy is PSPACE-complete (or worse), the study and improvements of exact algorithms lends insight into the optimal solution structure as well as providing a basis for approximate solutions. We present some improvements, analysis and empirical comparisons for some existing and some novel approaches for computing the optimal POMDP policy exactly. Since it is also hard (NP-complete or worse) to derive close approximations to the optimal solution for POMDPs, we consider a number of approaches for deriving policies that yield sub-optimal control and empirically explore their performance on a range of problems. These approaches 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Aoki. </author> <title> Optimization of Stochastic Systems. </title> <publisher> Academic Press, </publisher> <address> New York, NY, </address> <year> 1967. </year>
Reference-contexts: information state is the sum of the probabil ities of all the observations that would lead to this information state. 2.3.3 Value Functions The most interesting result concerning the use of information states is that, having regained the Markov property, the pomdp can be reformulated as a continuous space comdp <ref> [2, 1, 109] </ref>. The fact that Equations 2.8 and 2.10 38 still apply to the continuous space problem (as do the related equations) means that we can borrow many of the theoretical results and algorithmic ideas to apply to the pomdp problem. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm. <p> These longer training phases necessitate an adjusted step-size schedule, and Tables 5.4 through 5.7 show the schedule used for the k-pwlc experiments. Step Size Training Steps Interval 0:1 [ 0 750; 000 ] 0:001 <ref> [ 1; 500; 001 2; 250; 000 ] </ref> Table 5.5: Step-size adjustment schedule for 3; 000; 000 training step 3-pwlc experiments. 237 Step Size Training Steps Interval 0:1 [ 0 175; 000 ] 0:001 [ 350; 001 525; 000 ] Table 5.6: Step-size adjustment schedule for 700; 000 training step 7-pwlc <p> The obvious approach would be to first generate N real numbers, p (1); p (2) : : : p (N ), each being drawn from a uniform distribution on the interval <ref> [0; 1] </ref> and then normalize this vector so that p := P N ; to satisyfy the simplex constraints. This generates a probability distribution, but does not generate distributions uniformly randomly over the probability space. <p> correct algorithm for ensuring distributions are chosen uniformly 324 325 Figure C.1: Random probability points generated according to a naive algorithm. at random is given in Table C.1 1 In this algorithm, the function rand () is simply a routine that returns a uniformly random real number on the interval <ref> [0; 1] </ref>.
Reference: [2] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 10 </volume> <pages> 174-205, </pages> <year> 1965. </year>
Reference-contexts: Unlike the entire history, the information state size is of fixed dimension. An information state is a sufficient statistic for the history, which means that optimal behavior can be achieved using the information state in place of the history <ref> [120, 2, 117] </ref>. An information state, b, is simply a probability distribution over the set of states, (S), with b (s) being the probability of occupying state s. We define B = (S) to be the space of all probability distributions over S. <p> information state is the sum of the probabil ities of all the observations that would lead to this information state. 2.3.3 Value Functions The most interesting result concerning the use of information states is that, having regained the Markov property, the pomdp can be reformulated as a continuous space comdp <ref> [2, 1, 109] </ref>. The fact that Equations 2.8 and 2.10 38 still apply to the continuous space problem (as do the related equations) means that we can borrow many of the theoretical results and algorithmic ideas to apply to the pomdp problem. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [3] <author> K. J. Astrom. </author> <title> Optimal control of Markov decision processes with incomplete state estimation II. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 26 </volume> <pages> 403-406, </pages> <year> 1969. </year>
Reference-contexts: However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm. <p> Step Size Training Steps Interval 0:1 [ 0 1; 750; 000 ] 0:001 <ref> [ 3; 500; 001 5; 250; 000 ] </ref> Table 5.7: Step-size adjustment schedule for 7; 000; 000 training step 7-pwlc experiments.
Reference: [4] <author> K. J. Astrom. </author> <title> Theory and applications of adaptive control | A survey. </title> <journal> Automatica, </journal> <volume> 19 </volume> <pages> 471-486, </pages> <year> 1983. </year>
Reference-contexts: These two problems can combine to cause these comdp-based schemes to degrade into a constant cycle of random action selection, never receiving much reward and never doing much to disambiguate the current state. There is a general concept, known as dual control, from the research on adaptive control <ref> [4, 60] </ref> that concerns itself with the tradeoff between the control objective and the parameter estimation objective.
Reference: [5] <author> Leemon Baird. </author> <title> Residual algorithms: Reinforcement learning with function approximation. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 30-37, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Although many successful rl/ndp methods use the zero derivative assumption of Equation 5.11, called direct gradient methods by Baird, they have inferior theoretical convergence properties to the methods, called residual gradient, which do not make this assumption <ref> [5, 11] </ref>. There are simple examples that can be constructed where the direct method diverges. Incorporation of the extra gradient term gives the residual gradient algorithm a more solid theoretical basis, but complicates the replacements of the explicit summations with samples. <p> This two-sample version is referred to as the residual gradient, and as mentioned has better theoretical convergence guarantees. However, this method is slow to converge, whereas the direct gradient, if it converges, converges faster <ref> [5] </ref>. Baird proposed using a weighting between the two methods to get the advantages of both methods.
Reference: [6] <author> Andrew G. Barto, Richard S. Sutton, and Christopher J. C. H. Watkins. </author> <title> Learning and sequential decision making. </title> <note> In M. Gabriel 415 416 and J.W. </note> <editor> Moore, editors, </editor> <booktitle> Learning and Computational Neuroscience: Foundations of Adaptive Networks, </booktitle> <pages> pages 539-602. </pages> <publisher> MIT Press, </publisher> <address> Cam-bridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: Chapter 5 Reinforcement Learning In this chapter we will look at some applications of reinforcement learning (rl) algorithms <ref> [6, 53] </ref>. These techniques are a way to solve large, often continuous, state-space mdps.
Reference: [7] <author> Richard Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference-contexts: In these processes the decision maker has access to the current state of the system at each decision point. Many more extensive and mathematically rigorous treatments have been given. <ref> [7, 49, 12, 102, 9] </ref>. 2.2.1 Policies The entire problem to be tackled in solving an mdp is to find a good policy based upon the past history, H, of the process. <p> need to consider deterministic policies since for the mdp models we consider 20 an optimal deterministic policy always exists. 2.2.2 Value Functions In this section we will briefly review the optimality equations for comdps which are covered with significantly more depth in many texts [102, 9] and early research papers <ref> [7, 49, 12] </ref>. The results in this section will serve as the basis for the remainder of the discussion. Although the policy is the item of interest in solving an mdp, most of what we will discuss concerns itself with the value of a policy or the value function. <p> Evaluating a policy using Equation 2.4 directly, working from time 0 to time T 1, results in a very inefficient procedure, since there is much duplication of effort down in the recursion tree. The preferred method takes 21 advantage of the principle of optimality <ref> [7] </ref> and uses dynamic programming (dp) to compute a policy's value by working from time t = T 1 down to decision time t = 0. <p> However, this is terribly inefficient since there are an exponential number of policies. The more efficient and practical approach, attributed to Bellman <ref> [7] </ref> 27 and Howard [49], finds a sequence of policies of increasing quality and thus avoids the consideration of many suboptimal policies. This savings is purely empirical or average case, since it is possible to construct comdps where policy iteration would have to evaluate every possible policy. <p> For this thesis we will use it to derive approximate value functions (approximations in value space), though it is not infeasible to use these techniques to compute an approximate policies directly. There are two main ideas that distinguish the rl/ndp framework: first, to combat the curse of dimensionality <ref> [7] </ref>, a function approximator is used; second, it uses simulated experiences to generate trajectories through the state-space, thereby avoiding explicit computations for all possible states and focusing the computational effort on the more likely parts of the state space.
Reference: [8] <author> Dimitri P. Bertsekas. </author> <title> Distributed dynamic programming. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-27:610-616, </volume> <year> 1982. </year>
Reference-contexts: Although not necessarily an intuitive result, this algorithm will actually converge to the optimal value function, given that every state is selected infinitely often <ref> [8] </ref>. Note that this still requires a full model for the single-step value computation in the sum on s 0 and that we still need to explicitly store a value for each state.
Reference: [9] <author> Dimitri P. Bertsekas. </author> <title> Dynamic Programming and Optimal Control, Vols. 1 and 2. </title> <publisher> Athena Scientific., </publisher> <address> Belmont, Massachusetts, </address> <year> 1995. </year>
Reference-contexts: We use the terms process and system interchangeably to refer to the particular problem domain that is represented by the model. The formal model we use is the Markov decision process (mdp) and is treated much more thoroughly in many texts <ref> [102, 9] </ref>. The model itself is fairly simple and it is only when trying to use these models to determine optimal behavior that any complications arise. <p> We assume that there is either a discrete, finite or infinite, sequence of time points at which we get to make decisions. It is possible to consider continuous time processes <ref> [102, 9] </ref>, but we will not discuss the issues that arise from this added complexity. Example Each time a batter comes to bat is a decision point. Note that the "time" points are based more upon logical organization than upon some fixed increment of a clock. <p> Note that these assumptions are not necessary for mdps in general, but without them the theory and the algorithms become much more complex <ref> [12, 13, 9] </ref>. We will see later a specific instance of an mdp with a continuous state set, but this will be the only time we consider mdps without finite sets. <p> In these processes the decision maker has access to the current state of the system at each decision point. Many more extensive and mathematically rigorous treatments have been given. <ref> [7, 49, 12, 102, 9] </ref>. 2.2.1 Policies The entire problem to be tackled in solving an mdp is to find a good policy based upon the past history, H, of the process. <p> We will only need to consider deterministic policies since for the mdp models we consider 20 an optimal deterministic policy always exists. 2.2.2 Value Functions In this section we will briefly review the optimality equations for comdps which are covered with significantly more depth in many texts <ref> [102, 9] </ref> and early research papers [7, 49, 12]. The results in this section will serve as the basis for the remainder of the discussion. <p> The proof of this uses the fact that the one-step dp operator is a contraction mapping when 0 &lt; 1, though we defer an explanation of this part of the theory to the more rigorous treatments <ref> [102, 9] </ref>. However, we will later use the fact that Equation 2.7 holds for both discrete and continuous space comdps. 23 2.2.3 Value Iteration The dynamic programming approach does more than give us a way to evaluate a policy. <p> This chapter barely scratches the surface of the theory and formalisms for mdps and research in this area fills many volumes, though the majority of the research has been on comdps. Good starting references for comdps are Puterman's text [102] and Bertsekas' text <ref> [9] </ref>, with the latter touching upon the work in pomdps. Sondik's thesis [117] and the survey articles by Monahan [87], Lovejoy [76] and White [131] give nice overviews of both the history of the study of pomdps, as well as the existing work in the operations research field. <p> Most of these heuristics first appeared in research by Cassandra, Kaelbling and Kurien [21]. There is a class of techniques in control theory called certainty equivalent 257 258 controllers (cec) which are closely related to the approach of some of these heuristics <ref> [9] </ref>. The controllers make the assumption that the state transition functions are deterministic, and control proceeds accordingly, even though this assumption is violated. Also from control theory are the ideas of open-loop and closed-loop controllers. <p> For the first case, actual bounds can be placed on the quality of the solution for both finite <ref> [9] </ref> and continuous space comdps [108], making them applicable to the pomdp problems. For the second case, one can also put bounds on the approximation, where the error and the discount factor provide a limit on how wrong the values can be [26, 102, 140].
Reference: [10] <author> Dimitri P. Bertsekas and R. G. Gallagher. </author> <title> Data Networks. </title> <publisher> Prentice Hall., </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1992. </year>
Reference-contexts: Additionally, the amount of information that will be electronically available will continue its explosive growth. There are, and will continue to be, a host of important decision making tasks in these domains <ref> [111, 10] </ref>. From routing decisions to distributed database queries, the need to account for uncertainty will be the key to robust systems. <p> All transmitting stations are synchronized with respect to the clock, but there is no other way for them to communicate. The slotted Aloha protocol is a strategy for scheduling packet transmissions, where each packet waiting to be transmitted is transmitted with probability a <ref> [10] </ref>. If at a given time, there are s b backlogged packets and all transmitting stations have access to the this number, then the optimal strategy is to transmit each backlogged packet with probability 1=s b 4 . However, the transmission stations do not have access to the total backlog.
Reference: [11] <author> Dimitri P. Bertsekas and John N. Tsitsiklis. </author> <title> Neuro-Dynamic Programming. </title> <publisher> Athena Scientific., </publisher> <address> Belmont, Massachusetts, </address> <year> 1996. </year>
Reference-contexts: Recently, Bertsekas and Tsitsiklis have used the name neuro-dynamic programming (ndp) in their attempt to connect the rl area to its mathematical basis in dynamic programming, function approximation and iterative stochastic approximations <ref> [11] </ref>. In this work refer to this class of techniques rl/ndp algorithms to reflect both their rl origins and the contributions of Bertsekas and Tsitsiklis. rl/ndp is a framework for approximations in problems where solving directly would be intractable. <p> It is an involved process to systematically move from one extreme to the other and the book Neuro-dynamic Programming by Bertsekas and Tsit-siklis <ref> [11] </ref> shows this development in detail, as well and discussing many peripheral issues that arise along the way. <p> This incremental variation of the gradient method can also be shown to converge by casting it as a regular gradient method with independent errors <ref> [11] </ref>. <p> Convergence The convergence proofs for the stochastic approximation algorithms are quite complicated and we refer the reader to Bertsekas and Tsitsiklis <ref> [11] </ref> for a comprehensive treatment. However, in this work they show convergence for the cases of interest to the techniques described here; iterations based upon the dynamic programming operators and based upon gradient descent directions. However, there are many technical conditions which must hold for these to be valid. <p> Problems with this type of structure are often referred to as stochastic shortest path problems. For general infinite horizon mdps, we may never be able to reach a zero-cost absorbing state. Although any discounted infinite horizon mdp can be converted into 212 a stochastic shortest path version <ref> [11] </ref>, this is not usually desirable. When zero-cost absorbing states are not available, some form of truncation of the trajectory is required. For any given discount factor, we can make the error due to truncating the trajectory as small as desired. <p> Although many successful rl/ndp methods use the zero derivative assumption of Equation 5.11, called direct gradient methods by Baird, they have inferior theoretical convergence properties to the methods, called residual gradient, which do not make this assumption <ref> [5, 11] </ref>. There are simple examples that can be constructed where the direct method diverges. Incorporation of the extra gradient term gives the residual gradient algorithm a more solid theoretical basis, but complicates the replacements of the explicit summations with samples. <p> More importantly, if the heuristics can capture the salient non-linearities of the environment, then simpler and more effective linear approximations could be used. There are also many interesting ways in which the heuristics could be combined using roll-out policies <ref> [11] </ref> to help these algorithms to arrive at better solutions with fewer simulations. This is one of the more promising areas for future research.
Reference: [12] <author> David Blackwell. </author> <title> Discrete dynamic programming. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 33(2) </volume> <pages> 719-726, </pages> <month> June </month> <year> 1962. </year>
Reference-contexts: Note that these assumptions are not necessary for mdps in general, but without them the theory and the algorithms become much more complex <ref> [12, 13, 9] </ref>. We will see later a specific instance of an mdp with a continuous state set, but this will be the only time we consider mdps without finite sets. <p> In these processes the decision maker has access to the current state of the system at each decision point. Many more extensive and mathematically rigorous treatments have been given. <ref> [7, 49, 12, 102, 9] </ref>. 2.2.1 Policies The entire problem to be tackled in solving an mdp is to find a good policy based upon the past history, H, of the process. <p> need to consider deterministic policies since for the mdp models we consider 20 an optimal deterministic policy always exists. 2.2.2 Value Functions In this section we will briefly review the optimality equations for comdps which are covered with significantly more depth in many texts [102, 9] and early research papers <ref> [7, 49, 12] </ref>. The results in this section will serve as the basis for the remainder of the discussion. Although the policy is the item of interest in solving an mdp, most of what we will discuss concerns itself with the value of a policy or the value function.
Reference: [13] <author> David Blackwell. </author> <title> Discounted dynamic programming. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 36 </volume> <pages> 226-235, </pages> <year> 1965. </year>
Reference-contexts: Note that these assumptions are not necessary for mdps in general, but without them the theory and the algorithms become much more complex <ref> [12, 13, 9] </ref>. We will see later a specific instance of an mdp with a continuous state set, but this will be the only time we consider mdps without finite sets.
Reference: [14] <author> Craig Boutilier, Richard Dearden, and Moises Goldszmidt. </author> <title> Exploiting structure in policy construction. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <address> Montreal, Canada, </address> <year> 1995. </year>
Reference-contexts: For problems with more than two attributes, the compositional representation 307 can be exponentially smaller than the enumerative scheme, which leads to the research direction of finding algorithms that can work directly on the more compact form. Boutilier, et al <ref> [14, 15] </ref> have shown how to adapt some of the comdp and pomdp algorithms to operate directly on the compact representation, but experience with these algorithms is severely limited.
Reference: [15] <author> Craig Boutilier and David Poole. </author> <title> Computing optimal policies for partially observable decision processes using compact representations. </title> <booktitle> In 417 Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1168-1175, </pages> <address> Portland, Oregon, </address> <year> 1996. </year>
Reference-contexts: For problems with more than two attributes, the compositional representation 307 can be exponentially smaller than the enumerative scheme, which leads to the research direction of finding algorithms that can work directly on the more compact form. Boutilier, et al <ref> [14, 15] </ref> have shown how to adapt some of the comdp and pomdp algorithms to operate directly on the compact representation, but experience with these algorithms is severely limited. <p> Although there is some early 315 work in this area <ref> [15] </ref>, the effectiveness of these techniques remains largely unexplored. A related approach is to decompose the problem hierarchically, but there are many details that must first be worked.
Reference: [16] <author> Ronen I. Brafman. </author> <title> A heuristic variable grid solution method for POMDPs. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 727-733, </pages> <address> Providence, Rhode Island, </address> <year> 1997. </year>
Reference-contexts: Recently, Hauskrecht [46] has developed some techniques for quickly getting upper and lower bounds for the value functions, which do not depend on any particular fixed grid. The applicability to an arbitrary set of points makes his interpolation techniques especially useful for approximation schemes. Brafman <ref> [16] </ref> has also looked at applying variable grid-based methods to pomdps, where he uses heuristic rules to decide where and when to add points to the grid.
Reference: [17] <author> William L. Briggs. </author> <title> A multigrid tutorial. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1987. </year>
Reference-contexts: though our rough characterization into topics is more for organizational purposes than it is precisely correct. 6.9.1 Grid-based One common method to deal with continuous state spaces for mdps includes laying a grid of points over the state space, thereby transforming the problem into a discrete problem; e.g. multi-grid techniques <ref> [17, 107] </ref>. Aside from scaling poorly with the dimensionality of the state space, these are general techniques which were not specifically developed for pomdps and thus do nothing to exploit the shape of the value function.
Reference: [18] <author> J. Buhmann, W. Burgard, Cremers A., D. Fox, T. Hofmann, F. Schei-der, J. Strikos, and S. Thrun. </author> <title> The mobile robot RHINO. </title> <journal> AI Magazine, </journal> <volume> 16(2) </volume> <pages> 31-37, </pages> <month> Summer </month> <year> 1995. </year>
Reference-contexts: The need for algorithms to handle noisy environments is quite noticeable in autonomous robot research <ref> [18] </ref>. It is the robot navigation domain which motivated the development of these heuristics.
Reference: [19] <author> Bruce Bukiet, Elliotte Rusty Harold, and Jose Luis Palacios. </author> <title> A Markov chain approach to baseball. </title> <journal> Operations Research, </journal> <volume> 45(1) </volume> <pages> 14-23, </pages> <year> 1997. </year>
Reference-contexts: be made to ensure that the actual problems used for the empirical results are available through the author or Brown University. 369 370 H.1 Large Baseball Domain This domain is roughly based upon the example that appears in Howard's book [49] though others have applied decision theory to this domain <ref> [19] </ref>. It considers the strategic decisions of a baseball manager, for some number of innings, when their team is batting 2 . The discussion in here assumes some knowledge about the game of baseball.
Reference: [20] <author> Dima Burago, Michel de Rougemont, and Anatol Slissenko. </author> <title> On the complexity of partially observed Markov decision processes. </title> <journal> Theoretical Computer Science, </journal> <volume> 157(2) </volume> <pages> 161-183, </pages> <year> 1996. </year>
Reference-contexts: Clearly, the more general pomdps discussed here can be no easier to solve for the finite horizon. Even under many other forms of restrictions in model size, horizon length and type of policy, exact pomdp solutions are hard <ref> [20, 92] </ref>. Also pointed out by Papadimitriou and Tsitsiklis is the unlikeliness of even being able to represent a finite horizon policy with a polynomial-sized data structure. <p> Because the optimal answer reported is based upon simulation, the randomness of the simulation does not preclude another algorithm from performing better on a particular problem instance. The initial value function vectors were chosen to have random vectors where their components were randomly set to values in the interval <ref> [20; +20] </ref>. Table 5.8 shows the results for these small problems where the best entry for a problem is boxed. The lighter boxed entries indicate that these are not significantly worse than the best answer, where significance was determined by a simple two-sample T -test with p = 0:995. <p> For both of these problems, the initial state is equally likely to be any of the non-goal locations and the discount factor used is 0:95. We first ran lin-q and used the same initial random range of <ref> [20; +20] </ref> with the same training set-up of 100; 000 steps and evaluation with 10; 000 trajectories of length 100. The first difficulty we encounter is in gauging the performance of the resulting policies, since the optimal answer is not 241 known. <p> The natural extension of this heuristic for the case where we consider the action entropy is AWE (b) = ~ H (w a (b))(b V a CO ) : 1 Computing the optimal solution to a cumdp is NP-complete [95, 90] as are computing weak approximations to cumdp <ref> [20] </ref>. 269 6.6 Approximate Value Iteration An intriguing heuristic uses the exact algorithms discussed in Chapter 3 and value iteration to get approximate solutions which can be used to control a pomdp. <p> Some of the rl/ndp 306 techniques would also fall into the finite memory approach; for instance McCallum's rl work decides how much history is needed to do a good job predicting rewards. 6.9.3 Exploiting Structure Although the general pomdp problem is computationally hard <ref> [95, 20, 92] </ref>, there has been little work done in examining the complexity of sub-classes of pomdps to see if certain useful restrictions could be put on the model which would make their solution tractable.
Reference: [21] <author> Anthony Cassandra, Leslie Kaelbling, and James Kurien. </author> <title> Acting under uncertainty: Discrete bayesian models for mobile-robot navigation. </title> <booktitle> In IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1996. </year>
Reference-contexts: For all of the methods discussed, we are assuming that we can model the domain as a pomdp and that an explicit information state can be maintained at each step. Most of these heuristics first appeared in research by Cassandra, Kaelbling and Kurien <ref> [21] </ref>. There is a class of techniques in control theory called certainty equivalent 257 258 controllers (cec) which are closely related to the approach of some of these heuristics [9]. <p> More discussion of the similarities and differences between their approach and those presented here can be found in Cassandra, Kaelbling and Kurien <ref> [21] </ref> along with some empirical comparisons. One of the shortcomings of all of these approaches in the need for a full explicit model of the environment. A more natural situation is for the robot to uncover the structure and model parameters. <p> Additionally, there are also some related heuristics which were tried, but never proved superior in any of our experiments. We have eliminated these results as well, and refer the reader to the original paper which introduced the results given here <ref> [21] </ref>. For the experiments of this section we performed a T-test to determine whether or not there were significant differences between the heuristics. In the tables, the best performance is outlined darkly and those that are not significantly worse are lightly outlined. <p> The interested reader is referred to the original paper <ref> [21] </ref>, however there are a few points to be made about Table 6.11 2 : * the number of trajectories run for each experiment was extremely small: three for experiments 1, 2 and 3, and six for experiments 4, 5 and 6 (three for each of the two starting locations); * <p> Aside from evaluating these heuristics in synthetic simulations, along with Kurien and Kaelbling <ref> [21] </ref>, we have helped to successfully apply and evaluate the pomdp model and these heuristics on a mobile robot for the purpose of navigation.
Reference: [22] <author> Anthony R. Cassandra. </author> <title> Algorithms for partially observable Markov decision processes. </title> <type> Technical Report CS-94-14, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year> <month> 418 </month>
Reference-contexts: The discussion in this chapter does not dwell on implementation issues or cover many of the details of the previous algorithmic approaches. Details of this sort are covered at length in earlier work <ref> [22] </ref>. Since we only discuss optimal value functions in this chapter, we will drop some of the notational clutter and let V n = V fl n with the related functions being similarly simplified. <p> This issue will be discussed further in Chapter 5. 68 3.2 Witness Algorithm There are a number of papers related to the witness algorithm; from when it was first proposed by Cassandra, Littman and Kaelbling [23], to elaborations on its implementation <ref> [22] </ref>, and in a more formal treatment where it was proved correct [70, 74, 72]. The witness algorithm computes a n and is based upon the idea of exploring a finite number of regions in information state space. <p> Although the witness and incremental pruning algorithms were advances over the previous exact algorithms, they owe a great deal to the previous algorithmic approaches. More detailed discussion of these algorithms and their drawbacks can be found in other work <ref> [22, 74] </ref>. 3.4.1 Sondik's Two-Pass The witness, ip and gip algorithms compute the value function one action at a time and then merge the resulting sets. This approach allows them to use techniques which would not be directly applicable if they tried to construct n directly. <p> However, as has been discovered independently by many researchers [89, 76], the constraint set defined in the journal article is inadequate for finding the optimal solution. The details are also described by Cassandra <ref> [22] </ref>. 99 the region. then we are assured that R (fl a fl n (b); n ) = R (fl a fl n ), since there is no other action which will have a vector giving a higher value over the entire region.
Reference: [23] <author> Anthony R. Cassandra, Leslie Pack Kaelbling, and Michael L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, Washington, </address> <year> 1994. </year>
Reference-contexts: This issue will be discussed further in Chapter 5. 68 3.2 Witness Algorithm There are a number of papers related to the witness algorithm; from when it was first proposed by Cassandra, Littman and Kaelbling <ref> [23] </ref>, to elaborations on its implementation [22], and in a more formal treatment where it was proved correct [70, 74, 72]. The witness algorithm computes a n and is based upon the idea of exploring a finite number of regions in information state space. <p> Stages jV f j Reference 4x3 11 4 6 9 1375 [96] cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 <ref> [23] </ref> network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. Table 4.5 shows the result of the four principle algorithms on these problems. <p> Chapter 7 Conclusions 7.1 Contributions This thesis has contributed to advances in the exact and approximate solution of partially observable Markov decision processes. We have organized the contributions as we have organized the thesis, broken down into exact, heuristic and rl/ndp contributions. In conjunction with Littman, Kaelbling and Zhang <ref> [23, 72, 24] </ref>, this work has helped develop the witness algorithm and has helped to make improvements to the incremental pruning algorithm, both of which are currently the best exact pomdp algorithms available. <p> For this problem, the discount factor will be = 0:95 and we will be considering the policy shown in Figure D.1. Since this is a two state problem, an information state can be 1 This problem has appeared elsewhere and is known as the "tiger" problem. <ref> [23] </ref> 327 328 s 1:00 0:00 t (; f0; 1g; ) s s 0 s 0 0:50 0:50 s 0:85 0:15 o (f1; 2g; ; ) z z 0 s 0 0:50 0:50 0 1 2 s 0 1 10 100 Table D.1: Model parameters for f.t. example.
Reference: [24] <author> Anthony R. Cassandra, Michael L. Littman, and Nevin L. Zhang. </author> <title> Incremental pruning: A simple, fast, exact method for partially observable Markov decision processes. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), </booktitle> <address> Providence, Rhode Island, </address> <year> 1997. </year>
Reference-contexts: for typical pomdp problems, which is mostly unknown at this point, though Zhang [139] has some preliminary results which point to this relationship being relatively sparse. 79 3.3 Incremental Pruning Algorithms The incremental pruning algorithm was first proposed by Zhang and Liu [140] and was subsequently analyzed, compared and improved <ref> [24] </ref>. Like witness, it breaks down the problem into constructing the a n sets individually. <p> prune calls, the algorithm becomes the batch enumeration algorithm for constructing the a n sets. 84 3.3.3 Generalized Incremental Pruning The generalization of the incremental pruning algorithm discussed in this section grew out of trying to refine the incremental pruning algorithm and was first presented by Cassandra, Littman and Zhang <ref> [24] </ref>. The basic structure of the incremental algorithm is the same; the main difference lies in the way the individual prune (A B) operations are performed. In the normal application of the prune operation, there is nothing but a set of vectors to work on. <p> Stages jV f j Reference 4x3 11 4 6 9 1375 [96] cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 <ref> [24] </ref> nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. Table 4.5 shows the result of the four principle algorithms on these problems. <p> 1375 [96] cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 <ref> [24] </ref> nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. Table 4.5 shows the result of the four principle algorithms on these problems. This table shows the total execution time in seconds for value iteration for the number of stages indicated in Table 4.4. <p> With the development of the witness algorithm, more extensive comparisons were undertaken [74] using a broader range of problem sizes. This has continued through the development and implementation of the incremental pruning and generalized incremental pruning algorithms <ref> [140, 24] </ref>. This chapter has presented both detailed analysis and empirical evaluations of the exact algorithms; a combination which has proven quite fruitful for the insight and development of this research, which has resulted in the current best exact pomdp algorithms, both in theory and in practice. <p> Chapter 7 Conclusions 7.1 Contributions This thesis has contributed to advances in the exact and approximate solution of partially observable Markov decision processes. We have organized the contributions as we have organized the thesis, broken down into exact, heuristic and rl/ndp contributions. In conjunction with Littman, Kaelbling and Zhang <ref> [23, 72, 24] </ref>, this work has helped develop the witness algorithm and has helped to make improvements to the incremental pruning algorithm, both of which are currently the best exact pomdp algorithms available.
Reference: [25] <author> David A. Castanon. </author> <title> Approximate dynamic programming for sensor management. </title> <booktitle> In Proceedings of the Conference on Decision and Control To appear, </booktitle> <address> San Diego, CA, </address> <year> 1997. </year>
Reference-contexts: We empirically explore these methods on a range of domains. We note that there are intriguing, though unexplored, possibilities for combining these heuristics with an rl/ndp approach, where the heuristics are treated as features of the environment <ref> [25] </ref>. For all of the methods discussed, we are assuming that we can model the domain as a pomdp and that an explicit information state can be maintained at each step. Most of these heuristics first appeared in research by Cassandra, Kaelbling and Kurien [21].
Reference: [26] <author> Hsien-Te Cheng. </author> <title> Algorithms for Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> University of British Columbia, British Columbia, Canada, </institution> <year> 1988. </year>
Reference-contexts: These regions are a partition of the state space imposed by the pwlc property of the value function. Exactly how the witness algorithm does this will be elaborated upon below, but we note that some previously proposed techniques <ref> [117, 116, 26] </ref> also employ a region-based approach, though they construct n directly. <p> We discuss four algorithms; two developed by Sondik [117] and two developed by Cheng <ref> [26] </ref>. Although the witness and incremental pruning algorithms were advances over the previous exact algorithms, they owe a great deal to the previous algorithmic approaches. <p> Despite some of the shortcomings mentioned, Sondik's one-pass algorithm was the first exact algorithm and provided the inspiration for all of the subsequent algorithms. 3.4.3 Cheng's Relaxed Region In his thesis <ref> [26] </ref>, Cheng presents a variation of the one-pass algorithm which avoids the overly restrictive regions. Cheng refers to this as the relaxed region algorithm, since he simply removes some of the one-pass algorithms constraints. <p> Although there was subsequent research <ref> [129, 87, 39, 26, 132] </ref>, the computational complexity of the problems themselves, the intricacies of the algorithm and the lack of computing power of the day all combined and seemed to limit the amount of exploration and number of researchers involved in further developing the theory and algorithms for pomdps. <p> Prior to the mid-1980's, the empirical comparisons of pomdp algorithms were of very limited scope. The lack of realistic problems, combined with the restricted computing power of the day forced researchers to limit their experiments to problems with a handful of states and observations. Cheng <ref> [26] </ref> revisted the pomdp problem with some new algorithms and some of the most extensive empirical comparisons to date, but the problem sizes were still quite small and did not expose the real weakness with his schemes, which require enumerating all vertices of a convex polytope. <p> For the second case, one can also put bounds on the approximation, where the error and the discount factor provide a limit on how wrong the values can be <ref> [26, 102, 140] </ref>. We do not undertake a disciplined approach to this problem, but our implementation makes doing some form of an approximate dp stage readily available. <p> Our approx-vi scheme is a general, though not yet precise, method for adapting these exact methods to be approximations. The work by Cheng <ref> [26] </ref> shows how his linear support algorithm can be adopted for use in a successive approximation scheme and Zhang and Liu [140] show the same for the incremental pruning algorithm. 6.9.2 Finite Memory Another approach to approximating pomdp solutions is to only keep a finite amount of history of the process
Reference: [27] <author> Hsien-Te Cheng. </author> <type> Personal communication, </type> <year> 1994. </year>
Reference-contexts: Also, Cheng's linear support algorithm is our own implementation where every effort at efficiency was made. Here too, the ideal scenario would be to use the author's code directly, but this too seem to be no longer available <ref> [27] </ref>. We present our results as three-dimensional plots in Figure 4.12, where 185 the z-axis is the execution time in seconds and the x and y-axes show the effects of varying the numbers of states and observations.
Reference: [28] <author> Lonnie Chrisman. </author> <title> Reinforcement learning with perceptual aliasing: The perceptual distinctions approach. </title> <booktitle> In Proceedings of the Tenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 183-188, </pages> <address> San Jose, California, 1992. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Stages jV f j Reference 4x3 11 4 6 9 1375 [96] cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 <ref> [28] </ref> tiger 2 3 2 19 61 [23] network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. Table 4.5 shows the result of the four principle algorithms on these problems. <p> Work by Chrisman <ref> [28] </ref> in trying to simultaneously learn and act in pomdps used a similar, though slightly incorrect, update rule. <p> This greatly restricts the type of policies that are considered and requires some initial knowledge about how many tasks might be needed. Chrisman <ref> [28] </ref> presented an indirect method where a predictive model of the pomdp is maintained and updated based upon experience. <p> Either the model parameters are unknown or they may change over time. Thus, the problem becomes complicated because there is now a parameter estimation problem along with a planning problems. Some early work in learning pomdp models exist <ref> [28, 112] </ref>, but more more work still needs to be done. Another shortcoming of the pomdp approach is its limitations to discrete states. Many problems are more naturally specified as continuous space problems or have components of their states that are continuous valued.
Reference: [29] <author> Gregg Collins and Louise Pryor. </author> <title> Achieving the functionality of filter conditions in a partial order planner. </title> <booktitle> In Proceedings of the 10th National Conference on Artificial Intelligence, </booktitle> <pages> pages 375-380, </pages> <year> 1992. </year> <month> 419 </month>
Reference-contexts: Discussed previously were techniques which attempt to adapt the mdp-based algorithms to compact forms; similarly, there has been much work trying to extend classical planning to handle the full generality of the mdp formulations. Although there have been many extensions to the classical 309 planning algorithms <ref> [88, 29, 79, 136] </ref>, the one that comes closest to capturing the true mdp flavor is the work on the buridan [62, 61] and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability.
Reference: [30] <author> Anne Condon, Joan Feigenbaum, Carsten Lund, and Peter Shor. </author> <title> Probabilistic checkable debate systems and nonapproximability of PSPACE-hard functions. </title> <journal> Chicago Journal of Theoretical Computer Science, </journal> <volume> (4), </volume> <year> 1995. </year>
Reference-contexts: Within this sub-field, researchers have also come across classes of problems which are just as hard to approximate as they are to compute exactly. Unfortunately, pomdps are one such class of problems, even for the finite horizon case. Condon and Feigenbaum <ref> [30] </ref> have some results about the hardness of approximating some PSPACE-hard problems, which translate nearly directly into the hardness of approximating finite horizon pomdp problems.
Reference: [31] <author> Thomas H. Cormen, Charles E. Leiserson, and Ronald L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990. </year>
Reference-contexts: We have seen that the ordering of the sets could have an impact on the number and sizes of the lps that need to be solved, especially in the restricted region cross-sum variation. Additionally, this problem seems to have the same basic flavor as the matrix chain multiplication problem <ref> [31] </ref>, which is a problem where the proper parenthesization can have a profound effect on the computational requirements.
Reference: [32] <author> Robert Harry Crites. </author> <title> Large-scale Dynamic Optimization Using Teams of Reinforcement Learning Agents. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, </institution> <month> September </month> <year> 1996. </year>
Reference-contexts: Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control <ref> [32] </ref>. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis. <p> Additionally, we have seen that incremental gradient methods for solving a least squares optimization problem also have some pleasing convergence guarantees. Unfortunately, combining function approximation and simulation-based dynamic programming do not generally lead to pleasing convergence results. However, they are often combined in practice with impressive results <ref> [122, 32] </ref>. Additionally, though there is little theoretical basis for the simulation-based dp algorithm for the case of continuous state spaces, with function approximation, there is no inherent limitations on the state space size and we proceed here assuming we are in the pomdp realm of continuous state spaces. <p> It would be interesting to explore adding some bias on the value function to 254 these connectionist schemes, since there have been a number of impressive application using neural network function approximators <ref> [122, 32] </ref>. Schmidhuber [110] has also looked at applying recurrent neural networks to deal with the problem of hidden state. Some later work by Wiering and Schmidhuber [135] deals with the non-Markovian nature of the pomdp control problem by breaking it down into a sequence of Markovian tasks. <p> This is similar to Ring's work, though McCallum uses a tree-based representation instead of recurrent neural networks. Some preliminary research in applying Q-learning directly to pomdps has been undertaken by D'Ambrosio and Fung [33] using a table-based function approximator which maintains an entry for each belief state visited. Crites <ref> [32] </ref> has successfully applied rl to the problem of elevator control using teams of reinforcement learning agents. Although not explicitly handling partial observability, the elevator control domain does have elements of hidden state; e.g., the actual number of persons waiting to board an elevator.
Reference: [33] <author> Bruce D'Ambrosio and Robert Fung. </author> <title> Far sighted approaches to sensor management experiments in reinforcement learning. </title> <type> Technical report, </type> <institution> Prevision, </institution> <year> 1996. </year>
Reference-contexts: This is similar to Ring's work, though McCallum uses a tree-based representation instead of recurrent neural networks. Some preliminary research in applying Q-learning directly to pomdps has been undertaken by D'Ambrosio and Fung <ref> [33] </ref> using a table-based function approximator which maintains an entry for each belief state visited. Crites [32] has successfully applied rl to the problem of elevator control using teams of reinforcement learning agents. <p> Repairing requires more effort and 383 has a reward of 3. Finally, replacing the machine is the most costly and has a reward of 15. 384 H.4 Aircraft Identification (IFF) This example is loosely based upon a model used by D'ambrosio and Fung <ref> [33] </ref>. The scenario involves an incoming aircraft where using various forms of sensors available at a base, the task is to determine if the aircraft is a threat or not.
Reference: [34] <author> Thomas Dean, Leslie Pack Kaelbling, Jak Kirman, and Ann Nicholson. </author> <title> Planning with deadlines in stochastic domains. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence, </booktitle> <address> Washington, DC, </address> <year> 1993. </year>
Reference-contexts: Although these algorithms are presented as a way to arrive at optimal answers, they would seem to have the most potential as approximate algorithms where small differences between the states could be ignored. There has been work by Dean et al. in solving comdps using compact representations <ref> [34, 35] </ref> which would seem to have natural extensions to pomdps, though the additional complications introduced with the addition of partial observability have not yet been fully addressed. 6.9.4 Classical AI Planning The previous discussion concerning the need for compact representations would seem strange to researchers that have been working in
Reference: [35] <author> Thomas L. Dean, Robert Givan, and Sonia M. Leach. </author> <title> Model reduction techniques for computing aproximately optimal solutions for Markov decision processes. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), </booktitle> <pages> pages 124-131, </pages> <address> Providence, Rhode Island, </address> <year> 1997. </year> <month> 420 </month>
Reference-contexts: Although these algorithms are presented as a way to arrive at optimal answers, they would seem to have the most potential as approximate algorithms where small differences between the states could be ignored. There has been work by Dean et al. in solving comdps using compact representations <ref> [34, 35] </ref> which would seem to have natural extensions to pomdps, though the additional complications introduced with the addition of partial observability have not yet been fully addressed. 6.9.4 Classical AI Planning The previous discussion concerning the need for compact representations would seem strange to researchers that have been working in
Reference: [36] <author> M. </author> <title> DeGroot. Optimal Statistical Decisions. </title> <publisher> McGraw-Hill, </publisher> <address> New York, N.Y., </address> <year> 1970. </year>
Reference-contexts: However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [37] <author> Denise Draper, Steve Hanks, and Daniel Weld. </author> <title> Probabilistic planning with information gathering and contingent execution. </title> <type> Technical Report 93-12-04, </type> <institution> University of Washington, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: Although there have been many extensions to the classical 309 planning algorithms [88, 29, 79, 136], the one that comes closest to capturing the true mdp flavor is the work on the buridan [62, 61] and c-buridan <ref> [37] </ref> systems, both of which allow actions with probabilistic effects and the latter which allows partial observability. However, the plans derived from the c-buridan system have a limited expressibility and are not as general as possible in the full pomdp framework.
Reference: [38] <author> E. B. Dynkin. </author> <title> Controlled random sequences. Theory of Probability and its Applications, </title> <address> X:1-14, </address> <year> 1965. </year>
Reference-contexts: However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [39] <author> James N. Eagle. </author> <title> The optimal search for a moving target when the search path is constrained. </title> <journal> Operations Research, </journal> <volume> 32(5) </volume> <pages> 1107-1115, </pages> <year> 1984. </year>
Reference-contexts: Since there is a single unique hyper-plane that can be fit over any particular region, the parsimonious representation is unique [72]. Next, we develop the routines necessary for reducing a set to its parsimonious representation. Simple Domination Checking There is a very simple procedure, first discussed by Eagle <ref> [39] </ref>, to remove some useless vectors from a non-parsimonious set e . <p> Subsequent improvements to this batch enumerative scheme were merely ways to do this reduction (or pruning) phase more efficiently. Eagle <ref> [39] </ref> added the domination checks discussed in Section 3.1.1 and Lark [131] devised a more efficient linear programming approach which is the basis of the prune routine of Section 3.1.1. <p> Although there was subsequent research <ref> [129, 87, 39, 26, 132] </ref>, the computational complexity of the problems themselves, the intricacies of the algorithm and the lack of computing power of the day all combined and seemed to limit the amount of exploration and number of researchers involved in further developing the theory and algorithms for pomdps.
Reference: [40] <author> James E. Eckles. </author> <title> Optimum maintenance with incomplete information. </title> <journal> Operations Research, </journal> <volume> 16 </volume> <pages> 1058-1067, </pages> <year> 1968. </year>
Reference-contexts: However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm. <p> Some of the earliest work in pomdps, predating all of the exact algorithms, approached the problem by discretizing the state space <ref> [40, 54] </ref>. Although specifically geared toward partially observable environments, these methods scale miserably with the dimensionality of the state space. Love-joy's [75] grid-based algorithm uses a more flexible scheme for establishing the grid, but still is problematic for anything but a small number of states.
Reference: [41] <author> Hugh Ellis, Mingxiang Jiang, and Ross B. Corotis. </author> <title> Inspection, maintenance, and repair with partial observability. </title> <journal> Journal of Infrastructure Systems, </journal> <volume> 1(2) </volume> <pages> 92-99, </pages> <year> 1995. </year>
Reference-contexts: This inspection, maintenance and repair problem has a broader 6 application than simply toward manufacturing machines; developing policies for infrastructure systems must also deal with stochastic state transitions (e.g., structural deterioration) and partially observable components (e.g., surface coatings mask the crucial structural components) <ref> [41] </ref>. Medical Diagnosis Doctors are constantly faced with sequential decisions making tasks under uncertainty [46, 125]. They must prescribe medicines and recommend tests or treatments based upon the internal state of the patient.
Reference: [42] <author> Adriane V. Gheorghe. </author> <title> Decision Processes in Dynamic Probabilistic Systems. </title> <publisher> Kluwer Academic Publishers., Norwell, </publisher> <address> MA., </address> <year> 1990. </year>
Reference-contexts: Other optimality criteria used for making this tradeoff are discussed elsewhere <ref> [49, 48, 100, 102, 42] </ref>, though not all are directly applicable to the methods discussed in this thesis. With this criterion, rewards received later in time will have less value than an equivalent reward received closer to the present.
Reference: [43] <author> Robert Givan. </author> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: Region Adjacency Information There is an optimization that Small-wood and indexSondik, Edward J.Sondik [116] propose for their one-pass algorithm which uses information about the adjacency of regions to reduce the amount of computation that is required. Givan <ref> [43] </ref> has pointed out that this same idea could be applied to other algorithms, including witness, which search in information space.
Reference: [44] <author> Judy Goldsmith, Chris Lusena, and Martin Mundhenk. </author> <title> The complexity of deterministically observable finite-horizon Markov decision processes. </title> <type> Technical Report 268-96, </type> <institution> University of Kentucky, Lexing-ton, Kentucky, </institution> <month> December </month> <year> 1996. </year> <month> 421 </month>
Reference-contexts: A thorough complexity analysis of pomdps and their hardness of approximation results can be found in the work by Goldsmith, Mundhenk, Lusena and Allender <ref> [44, 91, 92] </ref>.
Reference: [45] <author> Eric A. Hansen. </author> <title> An improved policy iteration algorithm for partially observable MDPs. </title> <booktitle> NIPS, </booktitle> <year> 1997. </year>
Reference-contexts: Thus, exact policy iteration 47 algorithms for general pomdps do not exist and approximation methods are required. Although we do not address policy iteration techniques in this thesis, two approximate pi algorithms, one by Sondik [117, 118] and one by Hansen <ref> [45] </ref>, use the single dp step of value iteration in their policy improvement phase. The next chapter addresses the single dp step for pomdps in detail. 48 2.4 Conclusions This chapter has given the basic framework for Markov decision process formulations and solutions. <p> However, Sondik [118] presented a policy iteration algorithm for pomdps, though the implementation of this algorithm presents many challenges and has not yet been shown to be useful for more realistic sized problems. More recently Hansen <ref> [45] </ref> has revisited the policy iteration approach, resulting in a simpler and more effective algorithm. Although the dynamic programming updates discussed in this chapter are the basis of value iteration solutions, Hansen's policy iteration 106 algorithm uses these dp updates in his policy improvement phase. <p> Since many of these exact algorithms may have effective approximations, improvements in the exact algorithms could lead directly to effective approximations. There are some efforts currently in progress on policy iteration algorithms <ref> [45] </ref> using a single exact dp step. Given that pi iteration algorithms are often more effective than vi in the comdp context, this hints that the same can be true for pomdps.
Reference: [46] <author> Milos Hauskrecht. </author> <title> Incremental methods for computing bounds in partially observable Markov decision processes. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 734-739, </pages> <address> Providence, Rhode Island, </address> <year> 1997. </year>
Reference-contexts: Medical Diagnosis Doctors are constantly faced with sequential decisions making tasks under uncertainty <ref> [46, 125] </ref>. They must prescribe medicines and recommend tests or treatments based upon the internal state of the patient. However, accessing the true internal state of the patient is either impossible or highly undesirable, resulting is significant cost and risk to the patient. <p> Unfortunately, there are some problems with using this expression. The most critical problem is that computing V CU exactly is hard 1 . However, there are many ways in which lower bounds can be computed <ref> [75, 46] </ref> which can be used in place of V CU . We define V L to be any value function which is a lower bound for the pomdp value function. Another problem to address is the relative quality of the upper and lower bounds. <p> All of the above grid-based algorithms could be classified as fixed-grid 305 methods, since the grid is established in a very regular way and never changed. These fixed-grid methods are very rigid, but allow easy interpolation for points not in the grid. Recently, Hauskrecht <ref> [46] </ref> has developed some techniques for quickly getting upper and lower bounds for the value functions, which do not depend on any particular fixed grid. The applicability to an arbitrary set of points makes his interpolation techniques especially useful for approximation schemes.
Reference: [47] <author> John E. Hopcroft and Jeffrey D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1979. </year>
Reference-contexts: This simplified view will provide enough background to understand the main complexity results as they pertain to mdps. Much more comprehensive treatments of computational complexity theory are found in many textbooks <ref> [66, 47, 124] </ref>. Theoretical computer science has been useful in classifying the problems according to their computational hardness. The class P represents the easy or tractable problems which can be solved in a polynomial amount of time.
Reference: [48] <author> Howard and Matheson. </author> <title> Risk sensitive Markov decision processes. </title> <journal> Management Science, </journal> <volume> 18(7) </volume> <pages> 356-370, </pages> <year> 1972. </year>
Reference-contexts: Other optimality criteria used for making this tradeoff are discussed elsewhere <ref> [49, 48, 100, 102, 42] </ref>, though not all are directly applicable to the methods discussed in this thesis. With this criterion, rewards received later in time will have less value than an equivalent reward received closer to the present.
Reference: [49] <author> Ronald A. Howard. </author> <title> Dynamic Programming and Markov Processes. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1960. </year>
Reference-contexts: Other optimality criteria used for making this tradeoff are discussed elsewhere <ref> [49, 48, 100, 102, 42] </ref>, though not all are directly applicable to the methods discussed in this thesis. With this criterion, rewards received later in time will have less value than an equivalent reward received closer to the present. <p> In these processes the decision maker has access to the current state of the system at each decision point. Many more extensive and mathematically rigorous treatments have been given. <ref> [7, 49, 12, 102, 9] </ref>. 2.2.1 Policies The entire problem to be tackled in solving an mdp is to find a good policy based upon the past history, H, of the process. <p> need to consider deterministic policies since for the mdp models we consider 20 an optimal deterministic policy always exists. 2.2.2 Value Functions In this section we will briefly review the optimality equations for comdps which are covered with significantly more depth in many texts [102, 9] and early research papers <ref> [7, 49, 12] </ref>. The results in this section will serve as the basis for the remainder of the discussion. Although the policy is the item of interest in solving an mdp, most of what we will discuss concerns itself with the value of a policy or the value function. <p> However, this is terribly inefficient since there are an exponential number of policies. The more efficient and practical approach, attributed to Bellman [7] 27 and Howard <ref> [49] </ref>, finds a sequence of policies of increasing quality and thus avoids the consideration of many suboptimal policies. This savings is purely empirical or average case, since it is possible to construct comdps where policy iteration would have to evaluate every possible policy. <p> in the empirical comparisons 1 . 1 Every effort will be made to ensure that the actual problems used for the empirical results are available through the author or Brown University. 369 370 H.1 Large Baseball Domain This domain is roughly based upon the example that appears in Howard's book <ref> [49] </ref> though others have applied decision theory to this domain [19]. It considers the strategic decisions of a baseball manager, for some number of innings, when their team is batting 2 . The discussion in here assumes some knowledge about the game of baseball.
Reference: [50] <author> J. Hughes. </author> <title> Optimal internal audit timing. Accounting Review, </title> <address> LII:56-58, </address> <year> 1977. </year>
Reference-contexts: We will present a particularly simple network application in Appendix H.2. Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting [56]; corporate structure internal audit timing <ref> [50] </ref>; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential <p> Thus, for those problems there could be up to 99 more training steps than the non-absorbing state problems. 235 Step Size Training Steps Interval 0:1 [ 0 25; 000 ] 0:001 <ref> [ 50; 001 75; 000 ] </ref> Table 5.2: Step-size adjustment schedule for 100; 000 training step rl/ndp experiments.
Reference: [51] <author> Tommi Jaakkola, Satinder P. Singh, and Michael I. Jordan. </author> <title> Monte-carlo reinforcement learning in non-Markovian decision problems. </title> <booktitle> In Advances in Neural Information Processing Systems 7, </booktitle> <year> 1995. </year>
Reference-contexts: Removing the pitcher after the first hit is not necessarily going to be desirable, since they are equally likely to end up with a bad match-up after the pitching change. For a pomdp there may be no policy of this type which is optimal <ref> [51] </ref>. Some research exploring policies based upon the last observation only can be found in work by Littman [69]. A slightly better approach than the deterministic observation-based policy is to define the policy, : Z ! (A), as a mapping from observations to a distribution over actions. <p> This is commonly referred to as a probabilistic policy and is slightly more general since it includes the deterministic observation-based policies. Although there has been some research into these types of policies <ref> [51] </ref>, these too can be arbitrarily poor. It turns out that the optimal policy for a pomdp is not necessarily a Markov policy with respect to observations or any finite history of observations.
Reference: [52] <author> Leslie Pack Kaelbling, Michael L. Littman, and Anthony Cassandra. </author> <title> Planning and acting in partially observable stochastic domains. </title> <note> Artificial Intelligence To appear, 1998. 422 </note>
Reference-contexts: In these planning schemes and their derivatives, compact representation are, and always have been used, which raises the question concerning the connection between mdp algorithms and those used in these planning algorithms. This connection is elaborated in more detail in Kaelbling et al. <ref> [52] </ref> and here we just highlight the main points. The basic partial order planners have actions which are deterministic, a starting state and some set of goal states which were to be achieved.
Reference: [53] <author> Leslie Pack Kaelbling, Michael L. Littman, and Andrew W. Moore. </author> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research (JAIR), </journal> <volume> 4, </volume> <year> 1996. </year>
Reference-contexts: Chapter 5 Reinforcement Learning In this chapter we will look at some applications of reinforcement learning (rl) algorithms <ref> [6, 53] </ref>. These techniques are a way to solve large, often continuous, state-space mdps. <p> In reinforcement learning there is the same problem where it is commonly referred to as the exploitation vs. exploration problem <ref> [53] </ref>. Systems that explicitly attempt to trade off these two objectives would generally be considered dual controllers. For the case of a pomdp controller, we know the model, so there is not a problem learning the model parameters and the quantity we want to estimate is the current state.
Reference: [54] <author> J. S. Kakalik. </author> <title> Optimal policies for partially observable Markov systems. </title> <type> Technical Report TR-18, </type> <institution> Massachusetts Institute of Technology, </institution> <address> Cambridge, MA., </address> <month> October </month> <year> 1965. </year>
Reference-contexts: Some of the earliest work in pomdps, predating all of the exact algorithms, approached the problem by discretizing the state space <ref> [40, 54] </ref>. Although specifically geared toward partially observable environments, these methods scale miserably with the dimensionality of the state space. Love-joy's [75] grid-based algorithm uses a more flexible scheme for establishing the grid, but still is problematic for anything but a small number of states.
Reference: [55] <author> R. E. </author> <title> Kalman. A new apporach to linear filtering and prediction problems. </title> <journal> Journal of Basic Engineering, </journal> <pages> pages 35-45, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: Another shortcoming of the pomdp approach is its limitations to discrete states. Many problems are more naturally specified as continuous space problems or have components of their states that are continuous valued. Kalman filter approaches <ref> [55, 65] </ref> to localization are the analog of the the information state update equation for pomdps. Although this requires the state transition and observational noise to be Gaussian, Kalman filtering and the extended Kalman filter have been employed successfully and extensively in many control applications.
Reference: [56] <author> R. Kaplan. </author> <title> Optimal investigation strategies with imperfect information. </title> <journal> Journal of Accounting Research, </journal> <volume> 7 </volume> <pages> 32-43, </pages> <year> 1969. </year>
Reference-contexts: We will present a particularly simple network application in Appendix H.2. Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting <ref> [56] </ref>; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2
Reference: [57] <author> W. Karush and R. Dear. </author> <title> Optimal strategy for item presentation in learning models. </title> <journal> Management Science, </journal> <volume> 13 </volume> <pages> 773-785, </pages> <year> 1967. </year>
Reference-contexts: We will present a particularly simple network application in Appendix H.2. Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes <ref> [57] </ref>; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that
Reference: [58] <author> Sven Koenig. </author> <title> Optimal probabilistic and decision-theoretic planning using Markovian decision theory. </title> <type> Technical Report UCB/CSD 92/685, </type> <institution> Berkeley, </institution> <month> May </month> <year> 1992. </year>
Reference-contexts: With a known starting state and deterministic operators, a full policy over all possible states is not required and a plan or sequence of actions suffices. Although their state and action representations are compact, finding a plan requires a search through an exponential space of possibilities. Koenig <ref> [58] </ref> has shown how to recast such problems as comdps, which permits solution by polynomial-time algorithms. The catch here is that the conversion makes a problem that is exponential in size, since the compositional representation must now be converted into the full cross-product of all attributes.
Reference: [59] <author> Sven Koenig and Reid Simmons. </author> <title> Unsupervised learning of probabilistic models for robot navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference-contexts: A more natural situation is for the robot to uncover the structure and model parameters. We present no solutions to this problem but refer to the work by Koenig and Simmons <ref> [59] </ref> and Shatkay and Kaelbling [112]. Synthetic Environments The general robot navigation domain is discussed in Appendix H.5. Here we explore 24 specific pomdp model instances of this domain which correspond to 4 different location configurations, 3 different starting/goal state configurations and 2 different noise models.
Reference: [60] <author> P. R. Kumar. </author> <title> A survey of some results in stochastic adaptive control. </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 23 </volume> <pages> 329-380, </pages> <year> 1985. </year> <month> 423 </month>
Reference-contexts: These two problems can combine to cause these comdp-based schemes to degrade into a constant cycle of random action selection, never receiving much reward and never doing much to disambiguate the current state. There is a general concept, known as dual control, from the research on adaptive control <ref> [4, 60] </ref> that concerns itself with the tradeoff between the control objective and the parameter estimation objective.
Reference: [61] <author> Nicholas Kushmeric, Steve Hanks, and Daniel Weld. </author> <title> An algorithm for probabilistic planning. </title> <type> Technical Report 93-06-03, </type> <institution> Department of Computer Science, University of Washington, </institution> <year> 1993. </year>
Reference-contexts: Although there have been many extensions to the classical 309 planning algorithms [88, 29, 79, 136], the one that comes closest to capturing the true mdp flavor is the work on the buridan <ref> [62, 61] </ref> and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability. However, the plans derived from the c-buridan system have a limited expressibility and are not as general as possible in the full pomdp framework.
Reference: [62] <author> Nicholas Kushmerick, Steve Hanks, and Daniel S. Weld. </author> <title> An algorithm for probabilistic planning. </title> <journal> Artificial Intelligence, </journal> <volume> 76(1-2):239-286, </volume> <month> September </month> <year> 1995. </year>
Reference-contexts: Stages jV f j Reference 4x3 11 4 6 9 1375 [96] cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 <ref> [62] </ref> shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. <p> Although there have been many extensions to the classical 309 planning algorithms [88, 29, 79, 136], the one that comes closest to capturing the true mdp flavor is the work on the buridan <ref> [62, 61] </ref> and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability. However, the plans derived from the c-buridan system have a limited expressibility and are not as general as possible in the full pomdp framework.
Reference: [63] <author> Harold J. Kushner and A. J. Kleinman. </author> <title> Mathematical programming and the control of Markov chains. </title> <journal> International Journal of Control, </journal> <volume> 13(5) </volume> <pages> 801-820, </pages> <year> 1971. </year>
Reference-contexts: are ones that are considered solvable; NP-complete problems are considered to require an exponential amount of time; and PSPACE-complete problems are considered to be even harder than NP-complete problems. 4.1.2 Complexity of Exact Algorithms COMDPs We can solve the infinite horizon comdp problem by casting it a (reasonably sized) lp <ref> [78, 63] </ref> and solving. Since the complexity of linear programming is a well known P-complete problem, we can see that solving an infinite horizon comdp is easy from a complexity perspective.
Reference: [64] <author> Daniel E. Lane. </author> <title> A partially observable model of decision making by fishermen. </title> <journal> Operations Research, </journal> <volume> 37:240, </volume> <year> 1989. </year>
Reference-contexts: Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies <ref> [64] </ref>; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis.
Reference: [65] <author> J.J. Leonard and Hugh Durrant-Whyte. </author> <title> Localization by tracking geometric beacons. </title> <journal> IEEE Transactions on Robotics and Automation, </journal> <volume> 7(6), </volume> <year> 1991. </year>
Reference-contexts: Another shortcoming of the pomdp approach is its limitations to discrete states. Many problems are more naturally specified as continuous space problems or have components of their states that are continuous valued. Kalman filter approaches <ref> [55, 65] </ref> to localization are the analog of the the information state update equation for pomdps. Although this requires the state transition and observational noise to be Gaussian, Kalman filtering and the extended Kalman filter have been employed successfully and extensively in many control applications.
Reference: [66] <author> Harry R. Lewis and Christos H. Papadimitriou. </author> <title> Elements of the Theory of Computation. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1981. </year>
Reference-contexts: This simplified view will provide enough background to understand the main complexity results as they pertain to mdps. Much more comprehensive treatments of computational complexity theory are found in many textbooks <ref> [66, 47, 124] </ref>. Theoretical computer science has been useful in classifying the problems according to their computational hardness. The class P represents the easy or tractable problems which can be solved in a polynomial amount of time.
Reference: [67] <author> Long-Ji Lin and Tom M. Mitchell. </author> <title> Memory approaches to reinforcement learning in non-Markovian domains. </title> <type> Technical Report CMU-CS-92-138, </type> <institution> Carnegie Mellon University, Pittsburgh, Pennsylvania, </institution> <month> May </month> <year> 1992. </year> <month> 424 </month>
Reference-contexts: An early attempt to deal with partial observability was by Whitehead and Ballard [134], but it is only effective when the partial observability takes a special form, since it attempts to avoid the states which appear confusing. The work by Lin and Mitchell <ref> [67] </ref> used recurrent neural networks to cope with partial observability. Knowing that good policies is these domains will require some type of memory, they present three architectures for maintaining this memory.
Reference: [68] <author> Michael Littman, Anthony Cassandra, and Leslie Kaelbling. </author> <title> Learning policies for partially observable environments: Scaling up. </title> <booktitle> In Machine Learning: Proceedings of the Twelfth International Conference, </booktitle> <pages> pages 362-370, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Work by Chrisman [28] in trying to simultaneously learn and act in pomdps used a similar, though slightly incorrect, update rule. This is discussed more extensively in work by Littman, Cassandra and Kaelbling <ref> [68] </ref>. k-PWLC Representation The natural extension to the linear Q-function representation is to allow a general pwlc representation with multiple vectors for each e V a (b). With more vectors in the pwlc approximation, the value function has the ability to closely represent a much wider range of value functions. <p> We first applied these algorithms to the two robot navigation domains shown in Figures 5.2 and 5.3 which have 57 and 89 states respectively. These domains are described in more detail elsewhere <ref> [68] </ref>, but are similar to the navigation domains described in Appendix H.5. The starred locations are the goal locations which yields a +1 reward and resets the state to a random non-goal state. <p> Like the lin-q algorithm, the value function consists of a single vector per action and the update rule is similar to lin-q, though not identical. The differences are discussed, highlighted and empirically compared in work by Littman, Cassandra and Kaelbling <ref> [68] </ref>. Ring [104] combines the use of a recurrent neural network for the predictive model with rules for adjusting the model when a richer representation is needed. <p> In conjunction with Littman and Kaelbling <ref> [68] </ref>, his thesis has helped derive some novel reinforcement learning rules that are applicable to pomdps and presented some variations of previously existing techniques. We have implemented these ideas, presented some empirical comparisons using these techniques and explored some of the many possible variations available within the rl/ndp framework.
Reference: [69] <author> Michael L. Littman. </author> <title> Memoryless policies: Theoretical limitations and practical results. </title> <booktitle> In From Animals to Animats 3, </booktitle> <address> Brighton, UK, </address> <year> 1994. </year>
Reference-contexts: For a pomdp there may be no policy of this type which is optimal [51]. Some research exploring policies based upon the last observation only can be found in work by Littman <ref> [69] </ref>. A slightly better approach than the deterministic observation-based policy is to define the policy, : Z ! (A), as a mapping from observations to a distribution over actions. This is commonly referred to as a probabilistic policy and is slightly more general since it includes the deterministic observation-based policies.
Reference: [70] <author> Michael L. Littman. </author> <title> The witness algorithm for solving partially observable Markov decision processes. </title> <type> Technical Report CS-94-40, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1994. </year>
Reference-contexts: be discussed further in Chapter 5. 68 3.2 Witness Algorithm There are a number of papers related to the witness algorithm; from when it was first proposed by Cassandra, Littman and Kaelbling [23], to elaborations on its implementation [22], and in a more formal treatment where it was proved correct <ref> [70, 74, 72] </ref>. The witness algorithm computes a n and is based upon the idea of exploring a finite number of regions in information state space. These regions are a partition of the state space imposed by the pwlc property of the value function.
Reference: [71] <author> Michael L. </author> <title> Littman. </title> <type> Personal communication, </type> <year> 1996. </year>
Reference-contexts: This gives a slightly better guarantee than is available for a random set of points. 141 Saving Points The most intriguing and useful method for generating a good set of information points, as suggested by Littman <ref> [71] </ref>, is to associate an information point with each vector maintained by the algorithms. We discuss how this is useful below, but note that this results in a slightly more complex implementation.
Reference: [72] <author> Michael L. Littman. </author> <title> Algorithms for Sequential Decision Making. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Brown University, </institution> <month> February </month> <year> 1996. </year> <note> Also Technical Report CS-96-09. </note>
Reference-contexts: The unfortunate aspect of this is that there would not seem to be a one-to-one correspondence between a pwlc value function and its representation, nor between the size of the representation and the complexity of the value function. In fact, it can be shown <ref> [72, 74] </ref> that any pwlc value function does have unique minimal representation. We use the term parsimonious set 1 when referring to the unique minimal set of vectors representing a value function. <p> Since there is a single unique hyper-plane that can be fit over any particular region, the parsimonious representation is unique <ref> [72] </ref>. Next, we develop the routines necessary for reducing a set to its parsimonious representation. Simple Domination Checking There is a very simple procedure, first discussed by Eagle [39], to remove some useless vectors from a non-parsimonious set e . <p> We would like a procedure for deterministically selecting one of these and, furthermore, to select one that is guaranteed to have a non-empty region in the final parsimonious set. To accomplish this, we define a lexicographic ordering scheme <ref> [72] </ref> over S. The bestVector routine will use this ordering to deterministically decide which vector to select when more than one vector produces the same maximal value. <p> We then have the following: Theorem 3.1.1 If there exists fl 2 fl such that fl L &gt; for all other 2 fl, then R ( fl ; ) is non-empty. Proof This is proved in Appendix G:2 of Littman's thesis <ref> [72] </ref>. ffi Since fl is the vector returned by bestVector and it has a non-empty region, it must be part of the parsimonious representation of . 3.1.2 Vector at a Point Recall that we are only concerning ourselves with a single dp step to compute V n (b) from V n1 <p> be discussed further in Chapter 5. 68 3.2 Witness Algorithm There are a number of papers related to the witness algorithm; from when it was first proposed by Cassandra, Littman and Kaelbling [23], to elaborations on its implementation [22], and in a more formal treatment where it was proved correct <ref> [70, 74, 72] </ref>. The witness algorithm computes a n and is based upon the idea of exploring a finite number of regions in information state space. These regions are a partition of the state space imposed by the pwlc property of the value function. <p> But if R (-; b ) = ; then 8b we must have b- bbfl for all bfl 2 b . This contradicts the statement that b - &gt; b bfl. ffi The proof that the witness algorithm correctly computes a n first appeared elsewhere <ref> [72] </ref>. Appendix F.2 shows an example of how this algorithm works on the simple baseball problem which was introduced in the previous chapters. The witness algorithm has some appealing theoretical guarantees which are discussed further in Section 4.1.4. <p> The conclusion from this is that we cannot have a polynomial-time algorithm for computing n from n1 . The question that arises first is whether or not there exist pomdp problems can really exhibit this worst case behavior. Littman <ref> [72] </ref> shows just such a class of pomdps, which leads to the question of whether or not useful pomdps (i..e, ones that need to be solved) exhibit this worst case behavior. <p> This leads to considering a class of problems that are polynomial output-bounded, which simply says that the size of the answer is restricted to be polynomial in the size of the input. Littman <ref> [72] </ref> has also shown that this too is a hard problem. 4.1.3 Complexity of Approximations We now briefly divert ourselves from exact solutions to discuss some complexity results for approximate solutions. Considering the computational challenges presented by pomdps, Chapters 5 and 6 will explore developing approximate solutions. <p> Chapter 7 Conclusions 7.1 Contributions This thesis has contributed to advances in the exact and approximate solution of partially observable Markov decision processes. We have organized the contributions as we have organized the thesis, broken down into exact, heuristic and rl/ndp contributions. In conjunction with Littman, Kaelbling and Zhang <ref> [23, 72, 24] </ref>, this work has helped develop the witness algorithm and has helped to make improvements to the incremental pruning algorithm, both of which are currently the best exact pomdp algorithms available.
Reference: [73] <author> Michael L. Littman. </author> <title> Probabilistic propositional planning: Representations and complexity. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 748-754, </pages> <address> Providence, Rhode Island, </address> <year> 1997. </year>
Reference-contexts: However, the plans derived from the c-buridan system have a limited expressibility and are not as general as possible in the full pomdp framework. Further exploration into the representational issues for classical planners is provided by Littman <ref> [73] </ref>. 6.10 Conclusions This chapter has presented an array of approximation schemes and evaluated them on a range of problems. The first major conclusion is that small pomdp problems do not pose any great difficulty for getting high quality heuristic solutions.
Reference: [74] <author> Michael L. Littman, Anthony R. Cassandra, and Leslie Pack Kael-bling. </author> <title> Efficient dynamic-programming updates in partially observable Markov decision processes. </title> <type> Technical Report CS-95-19, </type> <institution> Brown University, </institution> <address> Providence, Rhode Island, </address> <year> 1995. </year> <month> 425 </month>
Reference-contexts: The unfortunate aspect of this is that there would not seem to be a one-to-one correspondence between a pwlc value function and its representation, nor between the size of the representation and the complexity of the value function. In fact, it can be shown <ref> [72, 74] </ref> that any pwlc value function does have unique minimal representation. We use the term parsimonious set 1 when referring to the unique minimal set of vectors representing a value function. <p> be discussed further in Chapter 5. 68 3.2 Witness Algorithm There are a number of papers related to the witness algorithm; from when it was first proposed by Cassandra, Littman and Kaelbling [23], to elaborations on its implementation [22], and in a more formal treatment where it was proved correct <ref> [70, 74, 72] </ref>. The witness algorithm computes a n and is based upon the idea of exploring a finite number of regions in information state space. These regions are a partition of the state space imposed by the pwlc property of the value function. <p> Although the witness and incremental pruning algorithms were advances over the previous exact algorithms, they owe a great deal to the previous algorithmic approaches. More detailed discussion of these algorithms and their drawbacks can be found in other work <ref> [22, 74] </ref>. 3.4.1 Sondik's Two-Pass The witness, ip and gip algorithms compute the value function one action at a time and then merge the resulting sets. This approach allows them to use techniques which would not be directly applicable if they tried to construct n directly. <p> In this section we briefly compare witness to these two algorithms and show that the empirical performance exactly matches this analysis. These empirical results first appeared in Littman et al <ref> [74] </ref>. Ideally, Sondik's one-pass algorithm should be included in this comparison, but the previous empirical and analytical results on the two-pass algorithm led us not to undertake the complications with this implementation. We attempted to get Sondik's original code, but no copies seem to exist [119]. <p> Although Sondik's one-pass algorithm has some theoretical complexity problems that need to be worked out, there is 189 still an open question regarding the empirical effectiveness of this algorithm. With the development of the witness algorithm, more extensive comparisons were undertaken <ref> [74] </ref> using a broader range of problem sizes. This has continued through the development and implementation of the incremental pruning and generalized incremental pruning algorithms [140, 24].
Reference: [75] <author> William S. Lovejoy. </author> <title> Computationally feasible bounds for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 39(1) </volume> <pages> 162-175, </pages> <year> 1991. </year>
Reference-contexts: Unfortunately, there are some problems with using this expression. The most critical problem is that computing V CU exactly is hard 1 . However, there are many ways in which lower bounds can be computed <ref> [75, 46] </ref> which can be used in place of V CU . We define V L to be any value function which is a lower bound for the pomdp value function. Another problem to address is the relative quality of the upper and lower bounds. <p> Some of the earliest work in pomdps, predating all of the exact algorithms, approached the problem by discretizing the state space [40, 54]. Although specifically geared toward partially observable environments, these methods scale miserably with the dimensionality of the state space. Love-joy's <ref> [75] </ref> grid-based algorithm uses a more flexible scheme for establishing the grid, but still is problematic for anything but a small number of states. One of the more interesting aspects of Lovejoy's work is his methods and insights for establishing upper and lower bounds on the pomdp value function.
Reference: [76] <author> William S. Lovejoy. </author> <title> A survey of algorithmic methods for partially observed Markov decision processes. </title> <journal> Annals of Operations Research, </journal> <volume> 28(1) </volume> <pages> 47-65, </pages> <year> 1991. </year>
Reference-contexts: Good starting references for comdps are Puterman's text [102] and Bertsekas' text [9], with the latter touching upon the work in pomdps. Sondik's thesis [117] and the survey articles by Monahan [87], Lovejoy <ref> [76] </ref> and White [131] give nice overviews of both the history of the study of pomdps, as well as the existing work in the operations research field. <p> However, as has been discovered independently by many researchers <ref> [89, 76] </ref>, the constraint set defined in the journal article is inadequate for finding the optimal solution. <p> For this reason, it is necessary to add the region restrictions R (fl a (b); a n ) for all actions. In the original Smallwood and Sondik paper [116] description of the algorithm, this restric tion was inadvertently omitted as was also discovered by a number of other researchers <ref> [76, 89] </ref>. This makes the final set of constraints " R (fl a;z n ) R (fl a fl n j8ag) : The unfortunate aspect of adding this last set of constraints is that we could now be defining a subset of the true region as shown in Figure 3.16.
Reference: [77] <author> Marc Mangel and Colin W. Clark. </author> <title> Dynamic Modeling in Behavioral Ecology. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1988. </year>
Reference-contexts: Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology <ref> [77] </ref>; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis.
Reference: [78] <author> A. Manne. </author> <title> Linear programming and sequential decisions. </title> <journal> Management Science, </journal> <volume> 6 </volume> <pages> 259-267, </pages> <year> 1960. </year>
Reference-contexts: are ones that are considered solvable; NP-complete problems are considered to require an exponential amount of time; and PSPACE-complete problems are considered to be even harder than NP-complete problems. 4.1.2 Complexity of Exact Algorithms COMDPs We can solve the infinite horizon comdp problem by casting it a (reasonably sized) lp <ref> [78, 63] </ref> and solving. Since the complexity of linear programming is a well known P-complete problem, we can see that solving an infinite horizon comdp is easy from a complexity perspective.
Reference: [79] <author> T. M. Mansell. </author> <title> A method for planning given uncertain and incomplete information. </title> <booktitle> In Proceedings of the 9th Conference on Uncertainty in Artificial Intelligence, </booktitle> <pages> pages 350-358. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> July </month> <year> 1993. </year>
Reference-contexts: Discussed previously were techniques which attempt to adapt the mdp-based algorithms to compact forms; similarly, there has been much work trying to extend classical planning to handle the full generality of the mdp formulations. Although there have been many extensions to the classical 309 planning algorithms <ref> [88, 29, 79, 136] </ref>, the one that comes closest to capturing the true mdp flavor is the work on the buridan [62, 61] and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability.
Reference: [80] <author> T. H. Mattheis. </author> <title> An algorithm for determining irrelevant constraints and all verticies in systems of linear inequalities. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 247-260, </pages> <year> 1973. </year>
Reference-contexts: To ensure that the relaxed region algorithm doesn't miss any interior regions, it must check every vertex of the defined region. Cheng uses a vertex enumeration algorithm <ref> [80] </ref> on the regions defined and checks each vertex to see if a previously undiscovered vector is maximal there. The computational complexity of doing this enumeration is exponential in either the dimension of the state space or the number of binding constraints of the region, whichever is smaller.
Reference: [81] <author> T. H. Mattheis and David S. Rubin. </author> <title> A survey and comparison of methods for finding all vertices of convex polyhedral sets. </title> <journal> Mathematics of Operations Research, </journal> <volume> 5(2) </volume> <pages> 167-185, </pages> <year> 1980. </year> <month> 426 </month>
Reference-contexts: In the relaxed region algorithm, the number of binding constraints is related to the number of observations and the size of n1 . This exponentiality is a direct result of the complexity of the number of vertices in a convex polytope <ref> [81] </ref>.
Reference: [82] <author> David McAllester and David Rosenblitt. </author> <title> Systematic nonlinear planning. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence, </booktitle> <year> 1991. </year>
Reference-contexts: In particular, we refer to classical ai planning schemes as those employing strips-like operators and which derive partially ordered sequences of actions <ref> [82, 97, 98] </ref>. In these planning schemes and their derivatives, compact representation are, and always have been used, which raises the question concerning the connection between mdp algorithms and those used in these planning algorithms.
Reference: [83] <author> Andrew Kachites McCallum. </author> <title> Efficient exploration in reinforcement learning with hidden state. </title> <type> Technical report, </type> <institution> University of Rochester, Rochester, </institution> <address> New York, </address> <year> 1996. </year>
Reference-contexts: This is more of a hierarchical approach and allows handling of hidden state with a varying amout of representational complexity, depending upon the need for additional bits of information. McCallum has worked extensively on applying rl to problems with hidden state <ref> [85, 86, 83, 84] </ref>. The end result of his efforts is a finite memory approach, where the amount of history required to make a decision can vary. The idea is to only add more history information if it will increase the util 255 ity of the policy.
Reference: [84] <author> Andrew Kachites McCallum. </author> <title> Reinforcement Learning with Selective Perception and Hidden State. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <year> 1996. </year>
Reference-contexts: This is more of a hierarchical approach and allows handling of hidden state with a varying amout of representational complexity, depending upon the need for additional bits of information. McCallum has worked extensively on applying rl to problems with hidden state <ref> [85, 86, 83, 84] </ref>. The end result of his efforts is a finite memory approach, where the amount of history required to make a decision can vary. The idea is to only add more history information if it will increase the util 255 ity of the policy.
Reference: [85] <author> R. Andrew McCallum. </author> <title> Overcoming incomplete perception with utile distinction memory. </title> <booktitle> In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <address> Amherst, Massachusetts, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Stages jV f j Reference 4x3 11 4 6 9 1375 [96] cheese 11 4 7 373 14 <ref> [85] </ref> paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. <p> This is more of a hierarchical approach and allows handling of hidden state with a varying amout of representational complexity, depending upon the need for additional bits of information. McCallum has worked extensively on applying rl to problems with hidden state <ref> [85, 86, 83, 84] </ref>. The end result of his efforts is a finite memory approach, where the amount of history required to make a decision can vary. The idea is to only add more history information if it will increase the util 255 ity of the policy.
Reference: [86] <author> R. Andrew McCallum. </author> <title> Instance-based utile distinctions for reinforcement learning with hidden state. </title> <booktitle> In Proceedings of the Twelfth International Conference Machine Learning, </booktitle> <pages> pages 387-395, </pages> <address> San Francisco, CA, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is more of a hierarchical approach and allows handling of hidden state with a varying amout of representational complexity, depending upon the need for additional bits of information. McCallum has worked extensively on applying rl to problems with hidden state <ref> [85, 86, 83, 84] </ref>. The end result of his efforts is a finite memory approach, where the amount of history required to make a decision can vary. The idea is to only add more history information if it will increase the util 255 ity of the policy.
Reference: [87] <author> George E. Monahan. </author> <title> A survey of partially observable Markov decision processes: Theory, models, and algorithms. </title> <journal> Management Science, </journal> <volume> 28(1) </volume> <pages> 1-16, </pages> <year> 1982. </year>
Reference-contexts: This same technique can be applied in 1 It is possible to make the observations dependent upon the initial state of the transition, or both the initial and final state, but these alternative formulations are expressively equivalent. <ref> [87] </ref> 31 o (pitch; s 0 ; z) z out hit s 0 good 0:85 0:15 bad 0:65 0:35 o (bullpen; s 0 ; z) z out hit s 0 good 0:85 0:15 bad 0:65 0:35 Table 2.6: Observation probabilities, o (a; s 0 ; z), for simplified baseball example. the <p> Good starting references for comdps are Puterman's text [102] and Bertsekas' text [9], with the latter touching upon the work in pomdps. Sondik's thesis [117] and the survey articles by Monahan <ref> [87] </ref>, Lovejoy [76] and White [131] give nice overviews of both the history of the study of pomdps, as well as the existing work in the operations research field. <p> The simplest approach towards finding a parsimonious set is to look at the regions R (fl; e ) for every vector fl. Those with non-empty regions are then added to the parsimonious set. This is the method described by Monahan <ref> [87] </ref>, but is not the most efficient method. Table 3.4 gives a more efficient routine that will reduce a set of vectors to its unique parsimonious set. <p> considers constructing each possible fl a n 2 a ever, it constructs each vector in an incremental fashion which reduces the computational complexity significantly. 3.3.1 Batch Enumeration To understand to basic approach of the incremental pruning algorithm, it helps to first discuss the batch enumeration algorithm as presented by Mon-ahan <ref> [87] </ref> 3 , since it is conceptually the simplest of all the exact algorithms. The witness algorithm, and the algorithms to be discussed in Section 3.4, search in information space for a set of points that are able to yield the full parsimonious representation of the value function. <p> To complete the enumeration from above, we need to repeat this for each action and so we define n = a a which is the complete enumeration of all possible vectors that could be in n . 81 This complete, or batch, enumeration algorithm as presented in Mona-han <ref> [87] </ref> gives a simple linear programming scheme for reducing the n set to its parsimonious representation. Subsequent improvements to this batch enumerative scheme were merely ways to do this reduction (or pruning) phase more efficiently. <p> Although there was subsequent research <ref> [129, 87, 39, 26, 132] </ref>, the computational complexity of the problems themselves, the intricacies of the algorithm and the lack of computing power of the day all combined and seemed to limit the amount of exploration and number of researchers involved in further developing the theory and algorithms for pomdps.
Reference: [88] <author> Leora Morgenstern. </author> <title> Knowledge preconditions for actions and plans. </title> <booktitle> In Proceedings of the 10th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 867-874, </pages> <year> 1987. </year> <month> 427 </month>
Reference-contexts: Discussed previously were techniques which attempt to adapt the mdp-based algorithms to compact forms; similarly, there has been much work trying to extend classical planning to handle the full generality of the mdp formulations. Although there have been many extensions to the classical 309 planning algorithms <ref> [88, 29, 79, 136] </ref>, the one that comes closest to capturing the true mdp flavor is the work on the buridan [62, 61] and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability.
Reference: [89] <author> Sraban Mukherjee and Kiran Seth. </author> <title> A corrected and improved computational scheme for partially observable Markov processes. </title> <journal> INFOR, </journal> <volume> 29(3) </volume> <pages> 206-212, </pages> <year> 1991. </year>
Reference-contexts: However, as has been discovered independently by many researchers <ref> [89, 76] </ref>, the constraint set defined in the journal article is inadequate for finding the optimal solution. <p> For this reason, it is necessary to add the region restrictions R (fl a (b); a n ) for all actions. In the original Smallwood and Sondik paper [116] description of the algorithm, this restric tion was inadvertently omitted as was also discovered by a number of other researchers <ref> [76, 89] </ref>. This makes the final set of constraints " R (fl a;z n ) R (fl a fl n j8ag) : The unfortunate aspect of adding this last set of constraints is that we could now be defining a subset of the true region as shown in Figure 3.16.
Reference: [90] <author> Martin Mundhenk, Judy Goldsmith, and Eric Allender. </author> <title> The complexity of unobservable finite-horizon Markov decision processes. </title> <type> Technical Report 269-96, </type> <institution> University of Kentucky, Lexington, Kentucky, </institution> <month> Decem-ber </month> <year> 1996. </year>
Reference-contexts: The natural extension of this heuristic for the case where we consider the action entropy is AWE (b) = ~ H (w a (b))(b V a CO ) : 1 Computing the optimal solution to a cumdp is NP-complete <ref> [95, 90] </ref> as are computing weak approximations to cumdp [20]. 269 6.6 Approximate Value Iteration An intriguing heuristic uses the exact algorithms discussed in Chapter 3 and value iteration to get approximate solutions which can be used to control a pomdp.
Reference: [91] <author> Martin Mundhenk, Judy Goldsmith, and Eric Allender. </author> <title> The complexity of policy evaluation for finite-horizon partially-observable markov decision processes. </title> <booktitle> In Proceedings of the 25th Mathematical Foundations of Computer Sciences, </booktitle> <pages> pages 129-138. </pages> <booktitle> Lecture Notes in Computer Science #1295, </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1997. </year>
Reference-contexts: A thorough complexity analysis of pomdps and their hardness of approximation results can be found in the work by Goldsmith, Mundhenk, Lusena and Allender <ref> [44, 91, 92] </ref>.
Reference: [92] <author> Martin Mundhenk, Judy Goldsmith, Chris Lusena, and Eric Allen-der. </author> <title> Encyclopaedia of complexity results for finite-horizon Markov decision process problems. </title> <type> Technical Report TR 273-97, </type> <institution> University of Kentucky, Lexington, Kentucky, </institution> <month> September </month> <year> 1997. </year>
Reference-contexts: Clearly, the more general pomdps discussed here can be no easier to solve for the finite horizon. Even under many other forms of restrictions in model size, horizon length and type of policy, exact pomdp solutions are hard <ref> [20, 92] </ref>. Also pointed out by Papadimitriou and Tsitsiklis is the unlikeliness of even being able to represent a finite horizon policy with a polynomial-sized data structure. <p> A thorough complexity analysis of pomdps and their hardness of approximation results can be found in the work by Goldsmith, Mundhenk, Lusena and Allender <ref> [44, 91, 92] </ref>. <p> Some of the rl/ndp 306 techniques would also fall into the finite memory approach; for instance McCallum's rl work decides how much history is needed to do a good job predicting rewards. 6.9.3 Exploiting Structure Although the general pomdp problem is computationally hard <ref> [95, 20, 92] </ref>, there has been little work done in examining the complexity of sub-classes of pomdps to see if certain useful restrictions could be put on the model which would make their solution tractable.
Reference: [93] <author> Remi Munos. </author> <title> A convergent reinforcement learning algorithm in the continuous case: the finite-element reinforcement learning. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <year> 1996. </year>
Reference-contexts: However, this successful use of teams of rl agents holds promise for dealing with partial observability directly. There is alos some work in dealing with reinforcement learning in continuous spaces <ref> [93] </ref> and future advances in continuous state space rl algorithms would have direct applicability to pomdp problems. 5.5 Conclusions In this chapter we have overviewed one general scheme for reinforcement learning (or neuro-dynamic programming) and then presented some instances for pomdps that exploit knowledge of the shape of the value function.
Reference: [94] <author> Illah Nourbakhsh, Rob Powers, and Stan Birchfield. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <pages> pages 53-60, </pages> <month> Summer </month> <year> 1995. </year> <month> 428 </month>
Reference-contexts: It is an open question whether the results here are applicable to models constructed with their methods, though we suspect they are. Another related effort is that by Nourbakhsh et al. <ref> [94] </ref> on the robot 279 dervish. Although not explicitly grounded in an mdp model, they use a probabilistic model to help predict potential resulting states.
Reference: [95] <author> Christos H. Papadimitriou and John N. Tsitsiklis. </author> <title> The complexity of Markov decision processes. </title> <journal> Mathematics of Operations Research, </journal> <volume> 12(3) </volume> <pages> 441-450, </pages> <year> 1987. </year>
Reference-contexts: For an arbitrary infinite horizon policy, it is not even known if its value function is finitely representable <ref> [95] </ref>, which calls into question the existence of an algorithm for the value determination step. Thus, exact policy iteration 47 algorithms for general pomdps do not exist and approximation methods are required. <p> More generally, for both the infinite and finite horizon comdp problem, computing the optimal policy is a P-complete problem as shown by Papadimitriou and Tsitsiklis <ref> [95] </ref>. 111 POMDPs Unfortunately, the nice computational aspects of the problem disappear with the introduction of partial observability. As was also shown by Papadim-itriou and Tsitsiklis [95], finding the optimal policy for even a simplified finite horizon pomdp is PSPACE-complete. <p> generally, for both the infinite and finite horizon comdp problem, computing the optimal policy is a P-complete problem as shown by Papadimitriou and Tsitsiklis <ref> [95] </ref>. 111 POMDPs Unfortunately, the nice computational aspects of the problem disappear with the introduction of partial observability. As was also shown by Papadim-itriou and Tsitsiklis [95], finding the optimal policy for even a simplified finite horizon pomdp is PSPACE-complete. In their work, they consider a pomdp where jZj &lt; jSj, the observations are deterministic and where the horizon length is T &lt; jSj. <p> The natural extension of this heuristic for the case where we consider the action entropy is AWE (b) = ~ H (w a (b))(b V a CO ) : 1 Computing the optimal solution to a cumdp is NP-complete <ref> [95, 90] </ref> as are computing weak approximations to cumdp [20]. 269 6.6 Approximate Value Iteration An intriguing heuristic uses the exact algorithms discussed in Chapter 3 and value iteration to get approximate solutions which can be used to control a pomdp. <p> Some of the rl/ndp 306 techniques would also fall into the finite memory approach; for instance McCallum's rl work decides how much history is needed to do a good job predicting rewards. 6.9.3 Exploiting Structure Although the general pomdp problem is computationally hard <ref> [95, 20, 92] </ref>, there has been little work done in examining the complexity of sub-classes of pomdps to see if certain useful restrictions could be put on the model which would make their solution tractable.
Reference: [96] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1088-1094. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1995. </year>
Reference-contexts: Stages jV f j Reference 4x3 11 4 6 9 1375 <ref> [96] </ref> cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table <p> j Reference 4x3 11 4 6 9 1375 <ref> [96] </ref> cheese 11 4 7 373 14 [85] paint 4 4 2 23 90 [62] shuttle 8 3 5 8 991 [28] tiger 2 3 2 19 61 [23] network 7 4 2 18 578 [24] nonlin 7 3 6 404 5 [96] saci 12 6 5 4 258 [24] Table 4.4: Small problem sizes, parameters and references. Table 4.5 shows the result of the four principle algorithms on these problems. This table shows the total execution time in seconds for value iteration for the number of stages indicated in Table 4.4. <p> To combat this, the spova algorithm of Parr and Russell <ref> [96] </ref> uses the L k norm, which is a smooth approximation for the max operator. The L k norm is a continuous function with the nice property that the simple maximization of a regular pwlc function is its limiting case. <p> The two main difficulties with the spova algorithm is in selecting and adjusting the exponent k and deciding the number of vectors, L, to use in the approximation. The reported results for spova <ref> [96] </ref> used a heuristic schedule for the exponent, starting k around 1:2 and increasing it linearly until it reached 8:0. A problem with adjusting the exponent is that changing the exponent Russell's [96] original paper. <p> The reported results for spova <ref> [96] </ref> used a heuristic schedule for the exponent, starting k around 1:2 and increasing it linearly until it reached 8:0. A problem with adjusting the exponent is that changing the exponent Russell's [96] original paper. The gradient formula in that paper appears with both the term in the numerator and denominator being raised to the k th power instead of the k 1 st power. 229 k. changes both the shape and range of the function.
Reference: [97] <author> J. S. Penberthy and D. Weld. UCPOP: </author> <title> A sound, complete, partial order planner for ADL. </title> <booktitle> In Proceedings of the third international conference on principles of knowledge representation and reasoning, </booktitle> <pages> pages 103-114, </pages> <year> 1992. </year>
Reference-contexts: In particular, we refer to classical ai planning schemes as those employing strips-like operators and which derive partially ordered sequences of actions <ref> [82, 97, 98] </ref>. In these planning schemes and their derivatives, compact representation are, and always have been used, which raises the question concerning the connection between mdp algorithms and those used in these planning algorithms.
Reference: [98] <author> Mark A. Peot and David E. Smith. </author> <title> Conditional nonlinear planning. </title> <booktitle> In Proceedings of the First International Conference on Artificial Intelligence Planning Systems, </booktitle> <pages> pages 189-197, </pages> <year> 1992. </year>
Reference-contexts: In particular, we refer to classical ai planning schemes as those employing strips-like operators and which derive partially ordered sequences of actions <ref> [82, 97, 98] </ref>. In these planning schemes and their derivatives, compact representation are, and always have been used, which raises the question concerning the connection between mdp algorithms and those used in these planning algorithms.
Reference: [99] <author> W. Pierskalla and J. Voelker. </author> <title> A survey of maintenance models: </title> <journal> The control and surveillance of deteriorating systems. Naval Research Logistics Quarterly, </journal> <volume> 23 </volume> <pages> 353-388, </pages> <year> 1976. </year>
Reference-contexts: There has been a great deal of work using the models addressed in this thesis to address just such a problem <ref> [106, 99, 105] </ref> and we present a specific example of this in Appendix H.3, which we use in evaluating the techniques developed in this thesis.
Reference: [100] <author> Loren K. Platzman. </author> <title> Optimal infinite-horizon undiscounted control of finite probabilistc systems. </title> <journal> SIAM Journal of Control and Optimization, </journal> <volume> 18 </volume> <pages> 362-380, </pages> <year> 1980. </year>
Reference-contexts: Other optimality criteria used for making this tradeoff are discussed elsewhere <ref> [49, 48, 100, 102, 42] </ref>, though not all are directly applicable to the methods discussed in this thesis. With this criterion, rewards received later in time will have less value than an equivalent reward received closer to the present. <p> shows how his linear support algorithm can be adopted for use in a successive approximation scheme and Zhang and Liu [140] show the same for the incremental pruning algorithm. 6.9.2 Finite Memory Another approach to approximating pomdp solutions is to only keep a finite amount of history of the process <ref> [100, 133] </ref>. This can also be viewed as a discretization of the continuous information state space space, but the discretization is in a slightly different form; now decisions are made based upon some discrete number of possible histories, instead of some interpolation from some discrete number of information points.
Reference: [101] <author> Pollock. </author> <title> A simple model of search for a moving target. </title> <journal> Operations Research, </journal> <volume> 18 </volume> <pages> 883-903, </pages> <year> 1970. </year> <month> 429 </month>
Reference-contexts: Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search <ref> [101] </ref>; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [102] <author> Martin L. Puterman. </author> <title> Markov Decision Processes | Discrete Stochastic Dynamic Programming. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, New York, </address> <year> 1994. </year>
Reference-contexts: We use the terms process and system interchangeably to refer to the particular problem domain that is represented by the model. The formal model we use is the Markov decision process (mdp) and is treated much more thoroughly in many texts <ref> [102, 9] </ref>. The model itself is fairly simple and it is only when trying to use these models to determine optimal behavior that any complications arise. <p> We assume that there is either a discrete, finite or infinite, sequence of time points at which we get to make decisions. It is possible to consider continuous time processes <ref> [102, 9] </ref>, but we will not discuss the issues that arise from this added complexity. Example Each time a batter comes to bat is a decision point. Note that the "time" points are based more upon logical organization than upon some fixed increment of a clock. <p> Other optimality criteria used for making this tradeoff are discussed elsewhere <ref> [49, 48, 100, 102, 42] </ref>, though not all are directly applicable to the methods discussed in this thesis. With this criterion, rewards received later in time will have less value than an equivalent reward received closer to the present. <p> In these processes the decision maker has access to the current state of the system at each decision point. Many more extensive and mathematically rigorous treatments have been given. <ref> [7, 49, 12, 102, 9] </ref>. 2.2.1 Policies The entire problem to be tackled in solving an mdp is to find a good policy based upon the past history, H, of the process. <p> However, it can be shown that when the processes state is fully observable, optimal performance can be achieved by using only the current state to decide what action to take <ref> [102] </ref>. A policy that uses only the current state is called a Markov policy and all comdp policies we will consider will be Markov. We define a decision rule as a complete mapping from the set of states to the set of actions, d t : S ! A. <p> We will only need to consider deterministic policies since for the mdp models we consider 20 an optimal deterministic policy always exists. 2.2.2 Value Functions In this section we will briefly review the optimality equations for comdps which are covered with significantly more depth in many texts <ref> [102, 9] </ref> and early research papers [7, 49, 12]. The results in this section will serve as the basis for the remainder of the discussion. <p> The proof of this uses the fact that the one-step dp operator is a contraction mapping when 0 &lt; 1, though we defer an explanation of this part of the theory to the more rigorous treatments <ref> [102, 9] </ref>. However, we will later use the fact that Equation 2.7 holds for both discrete and continuous space comdps. 23 2.2.3 Value Iteration The dynamic programming approach does more than give us a way to evaluate a policy. <p> The rates of convergence, stopping criteria, upper bounds and many other results relating to the convergence behavior of mdps are interesting by themselves, though not discussed here <ref> [102] </ref>. <p> This chapter barely scratches the surface of the theory and formalisms for mdps and research in this area fills many volumes, though the majority of the research has been on comdps. Good starting references for comdps are Puterman's text <ref> [102] </ref> and Bertsekas' text [9], with the latter touching upon the work in pomdps. <p> For the second case, one can also put bounds on the approximation, where the error and the discount factor provide a limit on how wrong the values can be <ref> [26, 102, 140] </ref>. We do not undertake a disciplined approach to this problem, but our implementation makes doing some form of an approximate dp stage readily available.
Reference: [103] <author> Martin L. Puterman and Moon Chirl Shin. </author> <title> Modified policy iteration algorithms for discounted Markov decision problems. </title> <journal> Management Science, </journal> <volume> 24 </volume> <pages> 1127-1137, </pages> <year> 1978. </year>
Reference-contexts: value iteration operator with fixed policy given by Equation 5.1 The most interesting aspect of the asynchronous policy iteration algorithm is that by selecting appropriate orderings of states and ordering of value/policy updates, many variations of algorithms can be constructed, including Gauss-Seidel value iteration, normal policy iteration, modified policy iteration <ref> [103] </ref> and others.
Reference: [104] <author> Mark B. </author> <title> Ring. Continual Learning in Reinforcement Environments. </title> <type> PhD thesis, </type> <institution> University of Texas, Austin, </institution> <year> 1994. </year>
Reference-contexts: Like the lin-q algorithm, the value function consists of a single vector per action and the update rule is similar to lin-q, though not identical. The differences are discussed, highlighted and empirically compared in work by Littman, Cassandra and Kaelbling [68]. Ring <ref> [104] </ref> combines the use of a recurrent neural network for the predictive model with rules for adjusting the model when a richer representation is needed.
Reference: [105] <author> Donald Rosenfeld. </author> <title> Markovian deterioration with uncertain information. </title> <journal> Operations Research, </journal> <volume> 24(1) </volume> <pages> 141-155, </pages> <year> 1976. </year>
Reference-contexts: There has been a great deal of work using the models addressed in this thesis to address just such a problem <ref> [106, 99, 105] </ref> and we present a specific example of this in Appendix H.3, which we use in evaluating the techniques developed in this thesis.
Reference: [106] <author> Sheldon M. Ross. </author> <title> Quality control under Markovian deterioration. </title> <booktitle> Management Science, </booktitle> <address> 17(9):587596, </address> <year> 1971. </year>
Reference-contexts: There has been a great deal of work using the models addressed in this thesis to address just such a problem <ref> [106, 99, 105] </ref> and we present a specific example of this in Appendix H.3, which we use in evaluating the techniques developed in this thesis. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [107] <author> Ulrich Rude. </author> <title> Mathematical and computational techniques for multilevel adaptive methods. </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Philadelphia, Pennsylvania, </address> <year> 1993. </year>
Reference-contexts: though our rough characterization into topics is more for organizational purposes than it is precisely correct. 6.9.1 Grid-based One common method to deal with continuous state spaces for mdps includes laying a grid of points over the state space, thereby transforming the problem into a discrete problem; e.g. multi-grid techniques <ref> [17, 107] </ref>. Aside from scaling poorly with the dimensionality of the state space, these are general techniques which were not specifically developed for pomdps and thus do nothing to exploit the shape of the value function.
Reference: [108] <author> Katsushige Sawaki and Akira Ichikawa. </author> <title> Optimal control for partially observable Markov decision processes over an infinite horizon. </title> <journal> Journal of the Operations Research Society of Japan, </journal> <volume> 21(1) </volume> <pages> 1-14, </pages> <month> March </month> <year> 1978. </year>
Reference-contexts: Furthermore, some of the subproblems solved in the course of the exact algorithms can be used in approximation schemes <ref> [108, 132] </ref>. 4.1 Computational Complexity of POMDPs Before undertaking our more detailed analysis, we present the existing complexity results for solving pomdps in general, solving pomdps via value iteration and the relative complexity of the existing algorithms. 4.1.1 Background Before discussing the existing complexity results on mdps, we will very briefly <p> For the first case, actual bounds can be placed on the quality of the solution for both finite [9] and continuous space comdps <ref> [108] </ref>, making them applicable to the pomdp problems. For the second case, one can also put bounds on the approximation, where the error and the discount factor provide a limit on how wrong the values can be [26, 102, 140].
Reference: [109] <author> Y. Sawaragi and T. Yoshikawa. </author> <title> Discrete time Markov decision processes with incomplete state information. </title> <journal> Annals of Mathematics and Statistics, </journal> <volume> 41 </volume> <pages> 78-86, </pages> <year> 1970. </year> <month> 430 </month>
Reference-contexts: information state is the sum of the probabil ities of all the observations that would lead to this information state. 2.3.3 Value Functions The most interesting result concerning the use of information states is that, having regained the Markov property, the pomdp can be reformulated as a continuous space comdp <ref> [2, 1, 109] </ref>. The fact that Equations 2.8 and 2.10 38 still apply to the continuous space problem (as do the related equations) means that we can borrow many of the theoretical results and algorithmic ideas to apply to the pomdp problem. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [110] <author> Jurgen Schmidhuber. </author> <title> Reinforcement learning in Markovian and non-Markovian environments. </title> <booktitle> In Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 500-506, </pages> <year> 1991. </year>
Reference-contexts: It would be interesting to explore adding some bias on the value function to 254 these connectionist schemes, since there have been a number of impressive application using neural network function approximators [122, 32]. Schmidhuber <ref> [110] </ref> has also looked at applying recurrent neural networks to deal with the problem of hidden state. Some later work by Wiering and Schmidhuber [135] deals with the non-Markovian nature of the pomdp control problem by breaking it down into a sequence of Markovian tasks.
Reference: [111] <author> A. Segall. </author> <title> Dynamic file assignment in a computer network. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> AC-21:161-173, </volume> <year> 1976. </year>
Reference-contexts: Additionally, the amount of information that will be electronically available will continue its explosive growth. There are, and will continue to be, a host of important decision making tasks in these domains <ref> [111, 10] </ref>. From routing decisions to distributed database queries, the need to account for uncertainty will be the key to robust systems.
Reference: [112] <author> Hagit Shatkay and Leslie Pack Kaelbling. </author> <title> Learning topological maps with weak local odometric information. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence, </booktitle> <address> Nagoya, Japan, </address> <month> August </month> <year> 1997. </year>
Reference-contexts: A more natural situation is for the robot to uncover the structure and model parameters. We present no solutions to this problem but refer to the work by Koenig and Simmons [59] and Shatkay and Kaelbling <ref> [112] </ref>. Synthetic Environments The general robot navigation domain is discussed in Appendix H.5. Here we explore 24 specific pomdp model instances of this domain which correspond to 4 different location configurations, 3 different starting/goal state configurations and 2 different noise models. <p> Either the model parameters are unknown or they may change over time. Thus, the problem becomes complicated because there is now a parameter estimation problem along with a planning problems. Some early work in learning pomdp models exist <ref> [28, 112] </ref>, but more more work still needs to be done. Another shortcoming of the pomdp approach is its limitations to discrete states. Many problems are more naturally specified as continuous space problems or have components of their states that are continuous valued.
Reference: [113] <author> Reid Simmons and Sven Koenig. </author> <title> Probabilistic navigation in partially observable environments. </title> <booktitle> In Fourteenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 1080-1087, </pages> <address> Montreal, Canada, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Then the av control strategy becomes AV (b) = argmax a This basic voting idea was first used by Simmons and Koenig <ref> [113] </ref>, but they used a planning algorithm based upon a model with deterministic transitions to compute the best action for each state instead of solving the underlying comdp. <p> The need for algorithms to handle noisy environments is quite noticeable in autonomous robot research [18]. It is the robot navigation domain which motivated the development of these heuristics. Previous work using pomdp models for planning in mobile robots by Simmons and Keonig <ref> [113] </ref> on the robot xavier used a single heuristic (essentially the voting heuristic) and our research has explored the question of whether there are better heuristics as well as whether pomdp models can be applied to realistic problems.
Reference: [114] <author> Richard Smallwood. </author> <title> The analysis of economic teaching strategies for a simple learning model. </title> <journal> Journal of Math Psych, </journal> <volume> 8 </volume> <pages> 285-301, </pages> <year> 1971. </year>
Reference-contexts: Other Domains The examples above are but a small portion of the domains where the techniques of this thesis are applicable. Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies <ref> [114] </ref>; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm.
Reference: [115] <author> Richard Smallwood, Edward Sondik, and F. Offensend. </author> <title> Toward and integrated methodology for the analysis of health-care systems. </title> <journal> Operations Research, </journal> <volume> 19 </volume> <pages> 1300-1322, </pages> <year> 1971. </year>
Reference-contexts: Thus, determining good policies for patient diagnosis in the face of these uncertainties is a challenging, real and important problem. In addition to these individual patient decision-making tasks, the models used in this thesis are applicable to the higher level problem of developing health care system policies <ref> [115] </ref>. Computer Networks Although high-speed computer networks are likely to make significant bandwidth improvements in the coming years, the improvements in data storage and computer capabilities will continue to make the communication channel the major bottleneck in future information pro 7 cessing tasks.
Reference: [116] <author> Richard D. Smallwood and Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over a finite horizon. </title> <journal> Operations Research, </journal> <volume> 21 </volume> <pages> 1071-1088, </pages> <year> 1973. </year> <month> 431 </month>
Reference-contexts: down the discussion into finite and infinite horizon value functions, since there are slightly different properties for each. 2 This particular decomposition was proposed by Michael Littman. 41 Finite Horizon Properties Sondik showed that the optimal finite horizon value function is piecewise linear and convex (pwlc) for any horizon T <ref> [117, 116] </ref>. This piecewise linear property is useful because it allows the value function to be represented using finite resources. It was this insight that allowed the development of the first exact algorithm for general finite horizon pomdps. <p> These regions are a partition of the state space imposed by the pwlc property of the value function. Exactly how the witness algorithm does this will be elaborated upon below, but we note that some previously proposed techniques <ref> [117, 116, 26] </ref> also employ a region-based approach, though they construct n directly. <p> If the lp determines that the item must be added, then we have done two lps where normally one would have sufficed. Region Adjacency Information There is an optimization that Small-wood and indexSondik, Edward J.Sondik <ref> [116] </ref> propose for their one-pass algorithm which uses information about the adjacency of regions to reduce the amount of computation that is required. Givan [43] has pointed out that this same idea could be applied to other algorithms, including witness, which search in information space. <p> Restricting our attention to the region R (fl a fl n ), if the fl a (b) for a 6= a fl are always as shown in Figure 3.13, 5 The one-pass algorithm is presented in both Sondik's thesis [117] and a journal article <ref> [116] </ref>. However, as has been discovered independently by many researchers [89, 76], the constraint set defined in the journal article is inadequate for finding the optimal solution. <p> As Figure 3.15 shows, this other vector may yield higher values than fl a fl (b) at b 0 . For this reason, it is necessary to add the region restrictions R (fl a (b); a n ) for all actions. In the original Smallwood and Sondik paper <ref> [116] </ref> description of the algorithm, this restric tion was inadvertently omitted as was also discovered by a number of other researchers [76, 89]. <p> prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114], the first exact algorithm for solving the general pomdp problem was developed by Sondik <ref> [117, 116] </ref> under the name of the "one-pass" algorithm.
Reference: [117] <author> Edward J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, Cali-fornia, </institution> <year> 1971. </year>
Reference-contexts: Unlike the entire history, the information state size is of fixed dimension. An information state is a sufficient statistic for the history, which means that optimal behavior can be achieved using the information state in place of the history <ref> [120, 2, 117] </ref>. An information state, b, is simply a probability distribution over the set of states, (S), with b (s) being the probability of occupying state s. We define B = (S) to be the space of all probability distributions over S. <p> down the discussion into finite and infinite horizon value functions, since there are slightly different properties for each. 2 This particular decomposition was proposed by Michael Littman. 41 Finite Horizon Properties Sondik showed that the optimal finite horizon value function is piecewise linear and convex (pwlc) for any horizon T <ref> [117, 116] </ref>. This piecewise linear property is useful because it allows the value function to be represented using finite resources. It was this insight that allowed the development of the first exact algorithm for general finite horizon pomdps. <p> When V n () is pwlc, Using Proposition 2.3.2 and Equation 2.20 we conclude that V fl n () is pwlc. ffi We now have all the required information to present and prove the fol lowing theorem which was first proven for the general case by Sondik <ref> [117] </ref>. Theorem 2.3.2 For any T , the optimal finite horizon value function for a pomdp is pwlc. Proof The proof proceeds by induction on the horizon length. For a finite horizon problem, after the last action is taken, no more rewards are accumulated. <p> Infinite Horizon Properties Although V fl n () is piecewise linear, and lim jjV fl this does not imply that V fl () is piecewise linear and there are pomdp problems whose optimal value functions are not piecewise linear <ref> [117] </ref>. However, there are a class of infinite horizon pomdp problems for which the optimal value function is piecewise linear. <p> This issue is of theoretical importance, but practically we can use a piecewise linear function to approximate any non-linear value function as closely as desired. The property of infinite horizon pomdp policies alluded to above is called finite transience, which was originally defined by Sondik <ref> [117] </ref>. When a policy is finitely transient, then its value function is piecewise linear. However, there are policies with piecewise linear value functions that are not finitely transient as well as policies whose value function is not piecewise linear at all. <p> Thus, exact policy iteration 47 algorithms for general pomdps do not exist and approximation methods are required. Although we do not address policy iteration techniques in this thesis, two approximate pi algorithms, one by Sondik <ref> [117, 118] </ref> and one by Hansen [45], use the single dp step of value iteration in their policy improvement phase. The next chapter addresses the single dp step for pomdps in detail. 48 2.4 Conclusions This chapter has given the basic framework for Markov decision process formulations and solutions. <p> Good starting references for comdps are Puterman's text [102] and Bertsekas' text [9], with the latter touching upon the work in pomdps. Sondik's thesis <ref> [117] </ref> and the survey articles by Monahan [87], Lovejoy [76] and White [131] give nice overviews of both the history of the study of pomdps, as well as the existing work in the operations research field. <p> This approach was suggested by Sondik <ref> [117] </ref>, but first taken with the witness algorithm (Section 3.2) and subsequently used by the incremental pruning algorithm (Section 3.3.2). Therefore, both algorithms share the common operation of constructing n from the a n sets. <p> These regions are a partition of the state space imposed by the pwlc property of the value function. Exactly how the witness algorithm does this will be elaborated upon below, but we note that some previously proposed techniques <ref> [117, 116, 26] </ref> also employ a region-based approach, though they construct n directly. <p> The alternative approach comes from looking at Equation 3.2 in a slightly different manner. We repeat this equation here for convenience: fl a;z 1 r (a) + P a;z n1 (b a The only function that the specific information state plays in this formula 3 Sondik <ref> [117] </ref> actually proposes such a scheme, but never presents it as an algorithm in its own right. Curiously, Monahan presents this algorithm under the guise of Sondik's one-pass algorithm. 80 is in the n1 () term, which does nothing other than select a vector from n1 . <p> We discuss four algorithms; two developed by Sondik <ref> [117] </ref> and two developed by Cheng [26]. Although the witness and incremental pruning algorithms were advances over the previous exact algorithms, they owe a great deal to the previous algorithmic approaches. <p> This approach allows them to use techniques which would not be directly applicable if they tried to construct n directly. Although the witness algorithm was the first to use the single action value function approach by design, Sondik <ref> [117] </ref> uses this idea to motivate the design of the one-pass algorithm. The first pass sweeps through the information space to construct a n and the second pass is required to merge these sets. <p> Restricting our attention to the region R (fl a fl n ), if the fl a (b) for a 6= a fl are always as shown in Figure 3.13, 5 The one-pass algorithm is presented in both Sondik's thesis <ref> [117] </ref> and a journal article [116]. However, as has been discovered independently by many researchers [89, 76], the constraint set defined in the journal article is inadequate for finding the optimal solution. <p> prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114], the first exact algorithm for solving the general pomdp problem was developed by Sondik <ref> [117, 116] </ref> under the name of the "one-pass" algorithm. <p> Figure C.2: Random probability points generated according to the correct algorithm. Appendix D Finitely Transient Policies For a stationary policy of an infinite horizon pomdp problem, there is a property called finite transience (f.t.) which was introduced by Sondik <ref> [117] </ref>. The interest in f.t. policies lies in Sondik's theorem that if an infinite horizon policy is f.t., then its value function is p.w.l. and can be computed relatively easily by solving a system of equations. <p> Also for jSj &gt; 2 all of the results of this section still apply, but the representation of the partitions and discontinuities must be done using a more complex system of hyper-planes instead of simple intervals and points. Sondik <ref> [117] </ref> shows how this is done for jSj &gt; 2. <p> We can find these sets by using the inverse of the information state transformation function and Sondik's construction methods <ref> [117] </ref>, though we must handle the special case of a non-invertible transformation function explicitly. For actions 1 and 2 of our example, the information state transformation is non-invertible, since no matter what the information state is, the result of these actions is the information state [ 0:5 0:5 ]. <p> From Lemma D.0.3, we see it does not matter which information state we select for each partition element. Returning to our example, we can construct the mapping shown in Table D.2 and illustrated in Figure D.3. The following lemma appears, and is proved in Sondik's thesis <ref> [117] </ref> as Lemma 3.4 on page 72. It shows the form of the value function for any stationary policy evaluated over the infinite horizon. Note that this theorem does not say that all policies have p.w.l. value functions. <p> This can only be done because of the property of the optimal policy for this problem: this policy is finitely transient 1 , as was discussed on Page 45. 1 This policy is finitely transient with degree 4 <ref> [117] </ref>. 366 Figure G.3: Repeated policy graph structure. Figure G.4: Policy graph structure shown in relation to information state space partitions. 367 Figure G.5: Redrawing the edges for finitely transient policy. The end result is an optimal controller that is a finite state machine as shown in Figure G.6.
Reference: [118] <author> Edward J. Sondik. </author> <title> The optimal control of partially observable Markov processes over the infinite horizon: Discounted costs. </title> <journal> Operations Research, </journal> <volume> 26(2) </volume> <pages> 282-304, </pages> <year> 1978. </year>
Reference-contexts: Unfortunately, there is no easy general way to determine when the optimal infinite horizon policy is finitely transient. How 46 ever, all policies can be approximated with a finitely transient policy, which is exploited in an infinite horizon policy iteration algorithm by Sondik <ref> [118] </ref>. Despite the uncertainty about the optimal infinite horizon value function's piecewise linearity, the convexity of V fl () is preserved. <p> Thus, exact policy iteration 47 algorithms for general pomdps do not exist and approximation methods are required. Although we do not address policy iteration techniques in this thesis, two approximate pi algorithms, one by Sondik <ref> [117, 118] </ref> and one by Hansen [45], use the single dp step of value iteration in their policy improvement phase. The next chapter addresses the single dp step for pomdps in detail. 48 2.4 Conclusions This chapter has given the basic framework for Markov decision process formulations and solutions. <p> The witness, incremental pruning and generalized incremental pruning algorithms are results of this more recent algorithmic perspective. The majority of the effort into exact or near exact algorithms for solving pomdps has been based upon value iteration. However, Sondik <ref> [118] </ref> presented a policy iteration algorithm for pomdps, though the implementation of this algorithm presents many challenges and has not yet been shown to be useful for more realistic sized problems. More recently Hansen [45] has revisited the policy iteration approach, resulting in a simpler and more effective algorithm.
Reference: [119] <author> Edward J. Sondik. </author> <type> Personal communication, </type> <year> 1994. </year>
Reference-contexts: Ideally, Sondik's one-pass algorithm should be included in this comparison, but the previous empirical and analytical results on the two-pass algorithm led us not to undertake the complications with this implementation. We attempted to get Sondik's original code, but no copies seem to exist <ref> [119] </ref>. Also, Cheng's linear support algorithm is our own implementation where every effort at efficiency was made. Here too, the ideal scenario would be to use the author's code directly, but this too seem to be no longer available [27].
Reference: [120] <author> C. T. Striebel. </author> <title> Sufficient statistics in the optimal control of stochastic systems. </title> <journal> Journal of Mathematical Analysis and Applications, </journal> <volume> 12 </volume> <pages> 576-592, </pages> <year> 1965. </year>
Reference-contexts: Unlike the entire history, the information state size is of fixed dimension. An information state is a sufficient statistic for the history, which means that optimal behavior can be achieved using the information state in place of the history <ref> [120, 2, 117] </ref>. An information state, b, is simply a probability distribution over the set of states, (S), with b (s) being the probability of occupying state s. We define B = (S) to be the space of all probability distributions over S. <p> However, the idea of comparing a vector to an evolving subset of the true value function representation is very useful and appears in the prune, the witness and the gip algorithms. 105 3.5 Conclusions Although there were many preliminary results and basic theory development for pomdps prior to 1970 <ref> [38, 2, 120, 1, 40, 3, 106, 101, 36, 109, 114] </ref>, the first exact algorithm for solving the general pomdp problem was developed by Sondik [117, 116] under the name of the "one-pass" algorithm. <p> The information state is the best state estimate we could hope to find and for the task of behaving optimally, it is a sufficient statistic <ref> [120] </ref> for the entire past history of the process. The state with the most probability mass in the information state at a given state, truly is the state that the system is most likely to be in.
Reference: [121] <author> Richard S. Sutton. </author> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3 </volume> <pages> 9-44, </pages> <year> 1988. </year>
Reference-contexts: These algorithms form the basis of simulation-based dynamic programming (e.g., Q-learning [126], TD () <ref> [121] </ref>) as well as for simulation-based gradient methods. These iterative stochastic algorithms will allow us to remove the explicit summation over next states, by allowing us to take samples of the next states, which is exactly what will be required in the simulation context. <p> This parameter is serving to adjust the sample and not the optimal value function. We do not use such a technique in this work, but mention that this sample discounting parameter, , is basis of the TD () approach <ref> [121] </ref>. Using the entire trajectory without any sample discounting is equivalent to TD (1). Simulating a long trajectory starting from a single state and getting a single sample turns out to be very data inefficient.
Reference: [122] <author> G. J. Tesauro. </author> <title> TD-Gammon, a self-teaching backgammon program, achieves master-level play. </title> <journal> Neural Computation, </journal> <volume> 6 </volume> <pages> 215-219, </pages> <year> 1994. </year>
Reference-contexts: Additionally, we have seen that incremental gradient methods for solving a least squares optimization problem also have some pleasing convergence guarantees. Unfortunately, combining function approximation and simulation-based dynamic programming do not generally lead to pleasing convergence results. However, they are often combined in practice with impressive results <ref> [122, 32] </ref>. Additionally, though there is little theoretical basis for the simulation-based dp algorithm for the case of continuous state spaces, with function approximation, there is no inherent limitations on the state space size and we proceed here assuming we are in the pomdp realm of continuous state spaces. <p> It would be interesting to explore adding some bias on the value function to 254 these connectionist schemes, since there have been a number of impressive application using neural network function approximators <ref> [122, 32] </ref>. Schmidhuber [110] has also looked at applying recurrent neural networks to deal with the problem of hidden state. Some later work by Wiering and Schmidhuber [135] deals with the non-Markovian nature of the pomdp control problem by breaking it down into a sequence of Markovian tasks.
Reference: [123] <author> Sylvie Thiebeaux, Marie-Odile Cordier, Olivier Jehl, and Jean-Paul Krivine. </author> <title> Supply restoration in power distribution systems | a case study in integrating model-based diagnosis and repair planning. </title> <booktitle> In Proceedings of the Twelfth Annual Conference on Uncertainty in Artificial Intelligence (UAI-96), </booktitle> <pages> pages 525-532, </pages> <address> Portland, Oregon, </address> <year> 1996. </year>
Reference-contexts: Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting <ref> [123] </ref>; questionnaire design [128]; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis.
Reference: [124] <editor> J. van Leeuwen, editor. </editor> <title> Algorithms and Complexity. </title> <publisher> Elsevier Science Publishers, </publisher> <year> 1990. </year> <month> 432 </month>
Reference-contexts: This simplified view will provide enough background to understand the main complexity results as they pertain to mdps. Much more comprehensive treatments of computational complexity theory are found in many textbooks <ref> [66, 47, 124] </ref>. Theoretical computer science has been useful in classifying the problems according to their computational hardness. The class P represents the easy or tractable problems which can be solved in a polynomial amount of time.
Reference: [125] <author> Rich Washington. </author> <title> Uncertainty and real-time therapy planning: incremental Markov-model approaches. </title> <booktitle> AAAI Spring Symposium on Artificial Intelligence in Medicine, </booktitle> <year> 1996. </year>
Reference-contexts: Medical Diagnosis Doctors are constantly faced with sequential decisions making tasks under uncertainty <ref> [46, 125] </ref>. They must prescribe medicines and recommend tests or treatments based upon the internal state of the patient. However, accessing the true internal state of the patient is either impossible or highly undesirable, resulting is significant cost and risk to the patient.
Reference: [126] <author> C. J. C. H. Watkins and P. </author> <title> Dayan. </title> <journal> Q-learning. Machine Learning, </journal> <volume> 8(3) </volume> <pages> 279-292, </pages> <year> 1992. </year>
Reference-contexts: These algorithms form the basis of simulation-based dynamic programming (e.g., Q-learning <ref> [126] </ref>, TD () [121]) as well as for simulation-based gradient methods. These iterative stochastic algorithms will allow us to remove the explicit summation over next states, by allowing us to take samples of the next states, which is exactly what will be required in the simulation context. <p> When the samples are generated from simulations, using Equation 5.10 results in exactly Watkins' Q-learning algorithm <ref> [126] </ref>. Using the convergence results of the stochastic approximation algorithms, we immediately get the convergence of Q-learning. The experiments we will use for our rl/ndp pomdp algorithms are based precisely on simulation-based value iteration, using Q-functions to represent the value function.
Reference: [127] <author> Chelsea C. White, III. </author> <title> Cost equality and inequality results for a partially observed stochastic optimization problem. </title> <journal> IEEE Transactions on Systems, Man, and Cybernetics, </journal> <volume> SMC-5(6):576-582, </volume> <month> Novem-ber </month> <year> 1975. </year>
Reference-contexts: The remainder of this thesis will look at algorithms for this more general class of problems, although further generalizations are possible <ref> [127, 129] </ref>. 11 12 2.1 Markov Decision Processes We are concerned with sequential decision problems where there is a need to make many decisions in the lifetime of the system.
Reference: [128] <author> Chelsea C. White, III. </author> <title> Optimal diagnostic questionaires which allow less than truthful responses. </title> <journal> Information and Control, </journal> <volume> 32 </volume> <pages> 61-74, </pages> <year> 1976. </year>
Reference-contexts: Additional applications include: cost control in accounting [56]; corporate structure internal audit timing [50]; learning processes [57]; teaching strategies [114]; moving target search [101]; fishery policies [64]; electric distribution network troubleshooting [123]; questionnaire design <ref> [128] </ref>; behavioral Ecology [77]; and elevator control [32]. 8 Thesis Outline and Summary The remainder of this thesis is organized as follows: Chapter 2 presents the basic model for sequential decision making that will be used throughout this thesis.
Reference: [129] <author> Chelsea C. White, III. </author> <title> Procedures for the solution of a finite-horizon partially observed, semi-Markov optimization problem. </title> <journal> Operations Research, </journal> <volume> 24(2) </volume> <pages> 348-358, </pages> <year> 1976. </year>
Reference-contexts: The remainder of this thesis will look at algorithms for this more general class of problems, although further generalizations are possible <ref> [127, 129] </ref>. 11 12 2.1 Markov Decision Processes We are concerned with sequential decision problems where there is a need to make many decisions in the lifetime of the system. <p> Although there was subsequent research <ref> [129, 87, 39, 26, 132] </ref>, the computational complexity of the problems themselves, the intricacies of the algorithm and the lack of computing power of the day all combined and seemed to limit the amount of exploration and number of researchers involved in further developing the theory and algorithms for pomdps.
Reference: [130] <author> Chelsea C. White, III. </author> <title> Monotone control laws for noisy, countable-state Markov chains. </title> <journal> European Journal of Operations Research, </journal> <volume> 5 </volume> <pages> 124-132, </pages> <year> 1980. </year>
Reference-contexts: White <ref> [130] </ref> shows how structure, in the form of an order on action quality, can be exploited to speed up Sondik's one-pass algorithm. A similar idea is used by Zhang [138], where he shows how to speed up the witness algorithm for problems with the structure of having relatively informative observations. <p> This effort will require finding real pomdp problems and exploring what type of structure they may have that could be exploited. One example of an effort along these lines is some work by White <ref> [130] </ref> which exploits structure in the problem to speed up Sondik's one-pass algorithm. Insights from the structure of the problem and the nature of the algorithms could make the exact solutions of larger problems possible.
Reference: [131] <author> Chelsea C. White, III. </author> <title> Partially observed Markov decision processes: A survey. </title> <journal> Annals of Operations Research, </journal> <volume> 32, </volume> <year> 1991. </year>
Reference-contexts: Good starting references for comdps are Puterman's text [102] and Bertsekas' text [9], with the latter touching upon the work in pomdps. Sondik's thesis [117] and the survey articles by Monahan [87], Lovejoy [76] and White <ref> [131] </ref> give nice overviews of both the history of the study of pomdps, as well as the existing work in the operations research field. <p> This is the method described by Monahan [87], but is not the most efficient method. Table 3.4 gives a more efficient routine that will reduce a set of vectors to its unique parsimonious set. This pruning procedure was first proposed by Lark and White <ref> [131] </ref>, though there is a subtlety involved in implementing the bestVector routine which is discussed below. The algorithm works by building up the parsimonious set one vector at a time. It starts with an empty set b and loops over the vectors in e . <p> Subsequent improvements to this batch enumerative scheme were merely ways to do this reduction (or pruning) phase more efficiently. Eagle [39] added the domination checks discussed in Section 3.1.1 and Lark <ref> [131] </ref> devised a more efficient linear programming approach which is the basis of the prune routine of Section 3.1.1.
Reference: [132] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Solution procedures for partially observed Markov decision processes. </title> <journal> Operations Research, </journal> <volume> 37(5) </volume> <pages> 791-797, </pages> <year> 1989. </year> <month> 433 </month>
Reference-contexts: Although there was subsequent research <ref> [129, 87, 39, 26, 132] </ref>, the computational complexity of the problems themselves, the intricacies of the algorithm and the lack of computing power of the day all combined and seemed to limit the amount of exploration and number of researchers involved in further developing the theory and algorithms for pomdps. <p> Furthermore, some of the subproblems solved in the course of the exact algorithms can be used in approximation schemes <ref> [108, 132] </ref>. 4.1 Computational Complexity of POMDPs Before undertaking our more detailed analysis, we present the existing complexity results for solving pomdps in general, solving pomdps via value iteration and the relative complexity of the existing algorithms. 4.1.1 Background Before discussing the existing complexity results on mdps, we will very briefly
Reference: [133] <author> Chelsea C. White, III and William T. Scherer. </author> <title> Finite memory suboptimal design for partially observed Markov decision processes. </title> <journal> Operations Research, 42(3):439, </journal> <volume> 455. </volume>
Reference-contexts: Since this history can be arbitrarily long, one might imagine that events far in the past might have minimal effects on our current decision. For this reason, researchers have explored policies based on a finite history of the process <ref> [133] </ref>. Although these methods can yield good solutions, they too can be arbitrarily poor. As a example of how a finite history can be poor, consider the simple case where knowledge of some sort of parity in the history will be required to behave optimally. <p> shows how his linear support algorithm can be adopted for use in a successive approximation scheme and Zhang and Liu [140] show the same for the incremental pruning algorithm. 6.9.2 Finite Memory Another approach to approximating pomdp solutions is to only keep a finite amount of history of the process <ref> [100, 133] </ref>. This can also be viewed as a discretization of the continuous information state space space, but the discretization is in a slightly different form; now decisions are made based upon some discrete number of possible histories, instead of some interpolation from some discrete number of information points.
Reference: [134] <author> Steven D. Whitehead and Dana H. Ballard. </author> <title> Learning to perceive and act by trial and error. </title> <journal> Machine Learning, </journal> <volume> 7(1) </volume> <pages> 45-83, </pages> <year> 1991. </year>
Reference-contexts: However, there has always been interest in attacking the problem of partial observability. An early attempt to deal with partial observability was by Whitehead and Ballard <ref> [134] </ref>, but it is only effective when the partial observability takes a special form, since it attempts to avoid the states which appear confusing. The work by Lin and Mitchell [67] used recurrent neural networks to cope with partial observability.
Reference: [135] <author> Marco Wiering and Jurgen Schmidhuber. HQ-learning: </author> <title> discovering Markovian subgoals for non-Markovian reinforcement learning. </title> <type> Technical Report IDSIA-95-96, </type> <institution> IDSIA, Switzerland, </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: Schmidhuber [110] has also looked at applying recurrent neural networks to deal with the problem of hidden state. Some later work by Wiering and Schmidhuber <ref> [135] </ref> deals with the non-Markovian nature of the pomdp control problem by breaking it down into a sequence of Markovian tasks. This greatly restricts the type of policies that are considered and requires some initial knowledge about how many tasks might be needed.
Reference: [136] <author> David Wilkins, Karen Myers, John Lowrance, and Leonard Wesley. </author> <title> Planning and reacting in uncertain and dynamic environments. </title> <journal> J. Expt. Theor. Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 121-152, </pages> <year> 1995. </year>
Reference-contexts: Discussed previously were techniques which attempt to adapt the mdp-based algorithms to compact forms; similarly, there has been much work trying to extend classical planning to handle the full generality of the mdp formulations. Although there have been many extensions to the classical 309 planning algorithms <ref> [88, 29, 79, 136] </ref>, the one that comes closest to capturing the true mdp flavor is the work on the buridan [62, 61] and c-buridan [37] systems, both of which allow actions with probabilistic effects and the latter which allows partial observability.
Reference: [137] <author> Wayne L. Winston. </author> <title> Introduction to Mathematical Programming: Applications and Algorithms. </title> <address> PWS-KENT, Boston, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: This routine sets up and solves a linear program (lp) <ref> [137] </ref> to find such a point, where the lp is shown in Table 3.3. When the lp is infeasible or the objective function is not greater than zero, then null is returned, otherwise the solution point of the lp is returned.
Reference: [138] <author> Nevin L. Zhang. </author> <title> Efficient planning in stochastic domains through exploiting problem characteristics. </title> <type> Technical Report HKUST-CS95-40, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology, </institution> <month> August </month> <year> 1995. </year>
Reference-contexts: The set of 1 This term is borrowed from Nevin Zhang <ref> [138] </ref>. 2 This can be made more mathematically precise, using measure theory, by eliminating consideration for regions with Lebesque measure of zero 53 points which are not in any region define the borders of the partition and are points where more than one vector gives the same maximal value. <p> White [130] shows how structure, in the form of an order on action quality, can be exploited to speed up Sondik's one-pass algorithm. A similar idea is used by Zhang <ref> [138] </ref>, where he shows how to speed up the witness algorithm for problems with the structure of having relatively informative observations. Another effort along these lines is the work by Zhang and Liu [141] which solves specific deterministically observable pomdps as approximations to the true pomdp.
Reference: [139] <author> Nevin L. Zhang. </author> <type> Personal communication, </type> <year> 1997. </year>
Reference-contexts: This will be directly related to the relative adjacency of regions in high dimensional space for typical pomdp problems, which is mostly unknown at this point, though Zhang <ref> [139] </ref> has some preliminary results which point to this relationship being relatively sparse. 79 3.3 Incremental Pruning Algorithms The incremental pruning algorithm was first proposed by Zhang and Liu [140] and was subsequently analyzed, compared and improved [24].
Reference: [140] <author> Nevin L. Zhang and Wenju Liu. </author> <title> Planning in stochastic domains: Problem characteristics and approximation. </title> <type> Technical Report HKUST-CS96-31, </type> <institution> Department of Computer Science, Hong Kong University of Science and Technology, </institution> <year> 1996. </year> <month> 434 </month>
Reference-contexts: relative adjacency of regions in high dimensional space for typical pomdp problems, which is mostly unknown at this point, though Zhang [139] has some preliminary results which point to this relationship being relatively sparse. 79 3.3 Incremental Pruning Algorithms The incremental pruning algorithm was first proposed by Zhang and Liu <ref> [140] </ref> and was subsequently analyzed, compared and improved [24]. Like witness, it breaks down the problem into constructing the a n sets individually. <p> With the development of the witness algorithm, more extensive comparisons were undertaken [74] using a broader range of problem sizes. This has continued through the development and implementation of the incremental pruning and generalized incremental pruning algorithms <ref> [140, 24] </ref>. This chapter has presented both detailed analysis and empirical evaluations of the exact algorithms; a combination which has proven quite fruitful for the insight and development of this research, which has resulted in the current best exact pomdp algorithms, both in theory and in practice. <p> For the second case, one can also put bounds on the approximation, where the error and the discount factor provide a limit on how wrong the values can be <ref> [26, 102, 140] </ref>. We do not undertake a disciplined approach to this problem, but our implementation makes doing some form of an approximate dp stage readily available. <p> Our approx-vi scheme is a general, though not yet precise, method for adapting these exact methods to be approximations. The work by Cheng [26] shows how his linear support algorithm can be adopted for use in a successive approximation scheme and Zhang and Liu <ref> [140] </ref> show the same for the incremental pruning algorithm. 6.9.2 Finite Memory Another approach to approximating pomdp solutions is to only keep a finite amount of history of the process [100, 133].
Reference: [141] <author> Nevin L. Zhang and Wenju Liu. </author> <title> Region-based approximations for planning in stochastic domains. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference on Uncertainty in Artificial Intelligence (UAI-97), </booktitle> <pages> pages 472-480, </pages> <address> Providence, Rhode Island, </address> <year> 1997. </year> <title> Notation Symbols </title>
Reference-contexts: A similar idea is used by Zhang [138], where he shows how to speed up the witness algorithm for problems with the structure of having relatively informative observations. Another effort along these lines is the work by Zhang and Liu <ref> [141] </ref> which solves specific deterministically observable pomdps as approximations to the true pomdp. By exploiting characteristics of solving these special pomdps, more effective solution procedures can be developed and become a basis for approximations schemes.
References-found: 141

