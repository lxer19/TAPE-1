URL: http://www.cs.cmu.edu/afs/cs/project/jair/pub/volume2/grove94a.ps.Z
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/grove94a.html
Root-URL: 
Email: grove@research.nj.nec.com  halpern@almaden.ibm.com  daphne@cs.berkeley.edu  
Title: Random Worlds and Maximum Entropy  
Author: Adam J. Grove Joseph Y. Halpern Daphne Koller 
Address: 4 Independence Way Princeton, NJ 08540  650 Harry Rd. San Jose, CA 95120  Berkeley, CA 94720  
Affiliation: NEC Research Institute,  IBM Almaden Research Center,  Computer Science Division, University of California  
Note: Journal of Artificial Intelligence Research 2 (1994) 33-88 Submitted 4/94; published 8/94  
Abstract: Given a knowledge base KB containing first-order and statistical facts, we consider a principled method, called the random-worlds method, for computing a degree of belief that some formula ' holds given KB. If we are reasoning about a world or system consisting of N individuals, then we can consider all possible worlds, or first-order models, with domain f1; : : :; N g that satisfy KB, and compute the fraction of them in which ' is true. We define the degree of belief to be the asymptotic value of this fraction as N grows large. We show that when the vocabulary underlying ' and KB uses constants and unary predicates only, we can naturally associate an entropy with each world. As N grows larger, there are many more worlds with higher entropy. Therefore, we can use a maximum-entropy computation to compute the degree of belief. This result is in a similar spirit to previous work in physics and artificial intelligence, but is far more general. Of equal interest to the result itself are the limitations on its scope. Most importantly, the restriction to unary predicates seems necessary. Although the random-worlds method makes sense in general, the connection to maximum entropy seems to disappear in the non-unary case. These observations suggest unexpected limitations to the applicability of maximum-entropy methods.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bacchus, F. </author> <year> (1990). </year> <title> Representing and Reasoning with Probabilistic Knowledge. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: However, when the interpretation is clear, we often abuse notation and drop the set delimiters. 37 Grove, Halpern, & Koller One important difference between our syntax and that of <ref> (Bacchus, 1990) </ref> is the use of approximate equality to compare proportion expressions. There are both philosophical and practical reasons why exact comparisons can be inappropriate. Consider a statement such as "80% of patients with jaundice have hepatitis". <p> Note that L allows the use of equality when comparing terms, but not when comparing proportion expressions. This definition allows arbitrary nesting of quantifiers and proportion expressions. As observed in <ref> (Bacchus, 1990) </ref>, the subscript x in a proportion expressions binds the variable x in the expression; indeed, we can view jjjj x as a new type of quantification. 38 Random Worlds and Maximum Entropy We now need to define the semantics of the logic.
Reference: <author> Bacchus, F., Grove, A. J., Halpern, J. Y., & Koller, D. </author> <year> (1994). </year> <title> From statistical knowledge bases to degrees of belief. </title> <type> Tech. rep. 9855, </type> <institution> IBM. </institution> <note> Available by anonymous ftp from logos.uwaterloo.ca/pub/bacchus or via WWW at http://logos.uwaterloo.ca. A preliminary version of this work appeared in Proc. Thirteenth International Joint Conference on Artificial Intelligence (IJCAI '93), </note> <year> 1993, </year> <pages> pages 563-569. </pages>
Reference-contexts: Are such intuitive results typical? When do we get convergence? And when we do, is there a practical way to compute degrees of belief? The answer to the first question is yes, as we discuss in detail in <ref> (Bacchus, Grove, Halpern, & Koller, 1994) </ref>. In that paper, we show that the random-worlds method is remarkably successful at satisfying the desiderata of both nonmonotonic (default) reasoning (Ginsberg, 1987) and reference class reasoning (Kyburg, 1983). <p> In that paper, we show that the random-worlds method is remarkably successful at satisfying the desiderata of both nonmonotonic (default) reasoning (Ginsberg, 1987) and reference class reasoning (Kyburg, 1983). The results of <ref> (Bacchus et al., 1994) </ref> show that the behavior we saw in the example above holds quite generally, as do many other properties we would hope to have satisfied. <p> Thus, in this paper we do not spend time justifying the random-worlds approach, nor do we discuss its strengths and weaknesses; the reader is referred to <ref> (Bacchus et al., 1994) </ref> for such discussion and for an examination of previous work in the spirit of random worlds (most notably (Carnap, 1950, 1952) and subsequent work). Rather, we focus on the latter two questions asked above. <p> We conclude in Section 6 with some discussion. 2. Technical preliminaries In this section, we give the formal definition of our language and the random-worlds method. The material is largely taken from <ref> (Bacchus et al., 1994) </ref>. 2.1 The language We are interested in a formal logical language that allows us to express both statistical information and first-order information. We therefore define a statistical language L , which is a variant of a language designed by Bacchus (1990). <p> Clearly, if we know N and ~t , then it seems more reasonable to use Pr ~t N rather than Pr 1 as our degree of belief. Indeed, as shown in <ref> (Bacchus et al., 1994) </ref>, many of the important properties that hold for the degree of belief defined by Pr 1 hold for Pr ~t N , for all choices of N and ~t . <p> Therefore, we can conclude that Pr ~t 1 (jjP (x)jj x 2 [0:5 *; 0:5 + *] jtrue) = 1: As we show in <ref> (Bacchus et al., 1994) </ref>, formulas with degree of belief 1 can essentially be treated just like other knowledge in KB . That is, the degrees of belief relative to KB and KB ^ will be identical (even if KB and KB ^ are not logically equivalent). <p> That is, the degrees of belief relative to KB and KB ^ will be identical (even if KB and KB ^ are not logically equivalent). More formally: 49 Grove, Halpern, & Koller Theorem 3.16: <ref> (Bacchus et al., 1994) </ref> If Pr ~t 1 (jKB) = 1 and lim fl 2 flim sup; lim infg, then for any formula ': lim fl Pr ~t N!1 N ('jKB ^ ): Proof: For completeness, we repeat the proof from (Bacchus et al., 1994) here. <p> More formally: 49 Grove, Halpern, & Koller Theorem 3.16: <ref> (Bacchus et al., 1994) </ref> If Pr ~t 1 (jKB) = 1 and lim fl 2 flim sup; lim infg, then for any formula ': lim fl Pr ~t N!1 N ('jKB ^ ): Proof: For completeness, we repeat the proof from (Bacchus et al., 1994) here. <p> We show that they can, but several difficult and subtle issues arise. 4.1 The general strategy Although the random-worlds method is defined by counting worlds, we can sometimes find more direct ways to calculate the degrees of belief it yields. In <ref> (Bacchus et al., 1994) </ref> we present a number of such techniques, most of which apply only in very special cases. One of the simplest and most intuitive is the following version of what philosophers have termed direct inference (Reichenbach, 1949). <p> Therefore, we might hope to use the statistics directly, and conclude that Pr ~t 1 ('(c)jKB) 2 [ff; fi]: This is indeed the case, as the following theorem shows. Theorem 4.1: <ref> (Bacchus et al., 1994) </ref> Let KB be a knowledge base of the form (~c ) ^ KB 0 , and assume that for all sufficiently small tolerance vectors ~t , KB [~t] j= k'(~x)j (~x)k ~x 2 [ff; fi]: If no constant in ~c appears in KB 0 , in '(~x), <p> Unfortunately, we show in Section 4.4 that this requirement is, in fact, a severe one; in particular, it prevents the theorem from being applied to most examples derived from default reasoning, using our statistical interpretation of defaults <ref> (Bacchus et al., 1994) </ref>. We close this subsection with an example of the theorem in action. Example 4.12: Let the language consist of P = fHepatitis; Jaundice; BlueEyedg and the constant Eric. There are eight atoms in this language. <p> The third answer shows that BlueEyed and Hepatitis are being treated as independent. It is a special case of a more general independence phenomenon that applies to random worlds; see <ref> (Bacchus et al., 1994, Theorem 5.27) </ref>. 4.3 Probabilistic propositional logic In this section we consider two variants of probabilistic propositional logic. As the following discussion shows, both can easily be captured by our framework. <p> As shown in (Geffner & Pearl, 1990), *-entailment possesses a number of reasonable properties typically associated with default reasoning, including a preference for more specific information. However, there are a number of desirable properties that it does not have. Among other things, irrelevant information is not ignored. (See <ref> (Bacchus et al., 1994) </ref> for an extensive discussion of this issue.) To obtain additional desirable properties, *-semantics is extended in (Goldszmidt et al., 1990) by an application of the principle of maximum entropy. <p> Another source of hope is to remember that maximum entropy is, for us, merely one tool for computing random-worlds degrees of belief. There may be other approaches that bypass entropy entirely. In particular, some of the theorems we give in <ref> (Bacchus et al., 1994) </ref> can be seen as doing this; these theorems will often apply even if F [ ] = 0. <p> In addition, maximum entropy is known to have many attractive properties (Jaynes, 1978). Our result shows these properties are shared by the random-worlds approach in the domain where these two approaches agree. Indeed, as shown in <ref> (Bacchus et al., 1994) </ref>, the random-worlds approach has many of these properties for the full (non-unary) language. <p> Not surprisingly, these criticisms apply to random worlds as well. A discussion of these criticisms, and whether they really should be viewed as shortcomings of the random-worlds method, is beyond the scope of this paper; the interested reader should consult <ref> (Bacchus et al., 1994, Section 7) </ref> for a more thorough discussion of these issues and additional references. We believe that our observations regarding the limits of the connection between the random-worlds method and maximum entropy are also significant. The question of how widely maximum entropy applies is quite important.
Reference: <author> Bochnak, J., Coste, M., & Roy, M. </author> <year> (1987). </year> <journal> Geometrie Algebrique Reelle, </journal> <volume> Vol. </volume> <booktitle> 12 of A Series of Modern Surveys in Mathematics. </booktitle> <publisher> Springer-Verlag, </publisher> <address> Berlin Heidelberg. </address>
Reference-contexts: Condition (i), and hence the entire theorem, now follows. It now remains to prove Lemma B.5, which was used in the proof just given. As we hinted earlier, this requires tools from algebraic geometry. We base our definitions on the presentation in <ref> (Bochnak, Coste, & Roy, 1987) </ref>. A subset A of IR ` is said to be semi-algebraic if it is definable in the language of real-closed fields. <p> The main tool we use is the following Curve Selection Lemma (see <ref> (Bochnak et al., 1987, p. 34) </ref>): Lemma B.3: Suppose that A is a semi-algebraic set in IR ` and ~u 2 A. <p> Thus, u is bad. But that means that there is a point u 0 &lt; u for which g (u 0 ) g (u), which contradicts the choice of v and u. We can now prove Lemma B.5. Recall, the result we need is as follows. 12. In <ref> (Bochnak et al., 1987) </ref>, a set is taken to be semi-algebraic if it is definable by a quantifier-free formula in the language of real closed fields. However, as observed in (Bochnak et al., 1987), since the theory of real closed fields admits elimination of quantifiers (Tarski, 1951), the two definitions are <p> We can now prove Lemma B.5. Recall, the result we need is as follows. 12. In <ref> (Bochnak et al., 1987) </ref>, a set is taken to be semi-algebraic if it is definable by a quantifier-free formula in the language of real closed fields. However, as observed in (Bochnak et al., 1987), since the theory of real closed fields admits elimination of quantifiers (Tarski, 1951), the two definitions are equivalent. 74 Random Worlds and Maximum Entropy Lemma B.5: For all sufficiently small ~t , S &lt;~t [KB] = S ~t [KB ].
Reference: <author> Carnap, R. </author> <year> (1950). </year> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, Chicago. </publisher>
Reference-contexts: Thus, in this paper we do not spend time justifying the random-worlds approach, nor do we discuss its strengths and weaknesses; the reader is referred to (Bacchus et al., 1994) for such discussion and for an examination of previous work in the spirit of random worlds (most notably <ref> (Carnap, 1950, 1952) </ref> and subsequent work). Rather, we focus on the latter two questions asked above. These questions may seem quite familiar to readers aware of the work on asymptotic probabilities for various logics.
Reference: <author> Carnap, R. </author> <year> (1952). </year> <title> The Continuum of Inductive Methods. </title> <publisher> University of Chicago Press, Chicago. </publisher>
Reference: <author> Cheeseman, P. C. </author> <year> (1983). </year> <title> A method of computing generalized Bayesian probability values for expert systems. </title> <booktitle> In Proc. Eighth International Joint Conference on Artificial Intelligence (IJCAI '83), </booktitle> <pages> pp. 198-202. </pages>
Reference-contexts: For example, in physics applications we are interested in such predicates as quantum state (see (Denbigh & Denbigh, 1985)). Similarly, AI applications and expert systems typically use only unary predicates such as symptoms and diseases <ref> (Cheeseman, 1983) </ref>. We suspect that this is not an accident, and that deep problems will arise in more general cases.
Reference: <author> Denbigh, K. G., & Denbigh, J. S. </author> <year> (1985). </year> <title> Entropy in Relation to Incomplete Knowledge. </title>
Reference-contexts: For example, in physics applications we are interested in such predicates as quantum state (see <ref> (Denbigh & Denbigh, 1985) </ref>). Similarly, AI applications and expert systems typically use only unary predicates such as symptoms and diseases (Cheeseman, 1983). We suspect that this is not an accident, and that deep problems will arise in more general cases.
Reference: <institution> Cambridge University Press, </institution> <address> Cambridge, U.K. </address>
Reference: <author> Fagin, R. </author> <year> (1976). </year> <title> Probabilities on finite models. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 41 (1), </volume> <pages> 50-58. </pages>
Reference-contexts: Furthermore, we can decide which (Grandjean, 1983). However, the 0-1 law fails if the language includes constants or if we look at conditional probabilities <ref> (Fagin, 1976) </ref>, and we need both these features in order to reason about degrees of belief for formulas involving particular individuals, conditioned on what is known.
Reference: <author> Fagin, R., Halpern, J. Y., & Megiddo, N. </author> <year> (1990). </year> <title> A logic for reasoning about probabilities. </title> <journal> Information and Computation, </journal> <volume> 87 (1/2), </volume> <pages> 78-128. </pages>
Reference-contexts: Consider the set of K = 2 k truth assignments these propositions. We give semantics to probabilistic statements over this language in terms of a probability distribution over the set (see <ref> (Fagin, Halpern, & Megiddo, 1990) </ref> for details). Since each truth assignment ! 2 determines the truth value of every propositional formula fi, we can determine the probability of every such formula: Pr (fi) = !j=fi Clearly, we can determine whether a probability distribution satisfies a set fl of probabilistic constraints.
Reference: <author> Geffner, H., & Pearl, J. </author> <year> (1990). </year> <title> A framework for reasoning with defaults. </title> <editor> In Kyburg, Jr., H. E., Loui, R., & Carlson, G. (Eds.), </editor> <booktitle> Knowledge Representation and Defeasible Reasoning, </booktitle> <pages> pp. 245-26. </pages> <publisher> Kluwer Academic Press, </publisher> <address> Dordrecht, Netherlands. </address> <booktitle> 86 Random Worlds and Maximum Entropy Ginsberg, </booktitle> <editor> M. L. (Ed.). </editor> <booktitle> (1987). Readings in Nonmonotonic Reasoning. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, San Francisco. </address>
Reference-contexts: Moreover, as we show in Section 4.3, this restricted sublanguage is rich enough to allow us to embed two well-known propositional approaches that make use of maximum entropy: Nilsson's probabilistic logic (Nilsson, 1986) and the maximum-entropy extension of *-semantics <ref> (Geffner & Pearl, 1990) </ref> due to Goldszmidt, Morris, Pearl (1990) (see also (Goldszmidt, Morris, & Pearl, 1993)). In Section 4.4, we consider whether the results for the restricted language can be extended. <p> A set R of default rules *-entails B ! C if for every PPD that *-satisfies R, lim *!0 * (CjB) = 1. As shown in <ref> (Geffner & Pearl, 1990) </ref>, *-entailment possesses a number of reasonable properties typically associated with default reasoning, including a preference for more specific information. However, there are a number of desirable properties that it does not have.
Reference: <author> Glebski, Y. V., Kogan, D. I., Liogon'ki, M. I., & Talanov, V. A. </author> <year> (1969). </year> <title> Range and degree of realizability of formulas in the restricted predicate calculus. </title> <journal> Kibernetika, </journal> <volume> 2, </volume> <pages> 17-28. </pages>
Reference: <author> Goldman, S. A. </author> <year> (1987). </year> <title> Efficient methods for calculating maximum entropy distributions. </title> <type> Master's thesis, </type> <institution> MIT EECS Department. </institution>
Reference-contexts: The fact that we have a connection between maximum entropy and random worlds is significant. For one thing, it allows us to utilize all the tools that have been developed for computing maximum entropy efficiently (see <ref> (Goldman, 1987) </ref> and the further references therein), and may thus lead to efficient algorithms for computing degrees of belief for a large class of knowledge bases. In addition, maximum entropy is known to have many attractive properties (Jaynes, 1978).
Reference: <author> Goldszmidt, M., Morris, P., & Pearl, J. </author> <year> (1990). </year> <title> A maximum entropy approach to nonmono-tonic reasoning. </title> <booktitle> In Proc. National Conference on Artificial Intelligence (AAAI '90), </booktitle> <pages> pp. 646-652. </pages>
Reference-contexts: However, there are a number of desirable properties that it does not have. Among other things, irrelevant information is not ignored. (See (Bacchus et al., 1994) for an extensive discussion of this issue.) To obtain additional desirable properties, *-semantics is extended in <ref> (Goldszmidt et al., 1990) </ref> by an application of the principle of maximum entropy. <p> <ref> (Goldszmidt et al., 1990) </ref> by an application of the principle of maximum entropy. Instead of considering all possible PPD's, as above, we consider only the PPD n *;R *&gt;0 such that, for each *, fl *;R has the maximum entropy among distributions that *-satisfy all the rules in R. (See (Goldszmidt et al., 1990) for precise definitions and technical details.) Note that, since the constraints used to define fl *;R are all linear, there is indeed a unique such point of maximum entropy. <p> A rule B ! C is an ME-plausible consequence of R if lim *!0 fl *;R (CjB) = 1. The notion of ME-plausible consequence is analyzed in detail in <ref> (Goldszmidt et al., 1990) </ref>, where it is shown to inherit all the nice properties of *-entailment (such as the preference for more specific information), while successfully ignoring irrelevant information. <p> Equally importantly, algorithms are provided for computing the ME-plausible consequences of a set of rules in certain cases. 59 Grove, Halpern, & Koller Our maximum-entropy results can be used to show that the approach of <ref> (Goldszmidt et al., 1990) </ref> can be embedded in our framework in a straightforward manner. <p> Note that the formulas that arise under this translation all use the same approximate equality connective 1 . The reason is that the approach of <ref> (Goldszmidt et al., 1990) </ref> uses the same * for all default rules. <p> Using the translation described above, for a set R of defeasible rules, B ! C is an ME-plausible consequence of R iff Pr 1 ~ C (c) fi fi ~ B (c) ^ r2R ! In particular, this theorem implies that all the computational techniques and results described in <ref> (Goldszmidt et al., 1990) </ref> carry over to this special case of the random-worlds method. It also shows that random-world provides a principled justification for the approach (Goldszmidt et al., 1990) present (one which is quite different from the justification given in (Goldszmidt et al., 1990) itself). 4.4 Beyond simple queries In <p> C (c) fi fi ~ B (c) ^ r2R ! In particular, this theorem implies that all the computational techniques and results described in <ref> (Goldszmidt et al., 1990) </ref> carry over to this special case of the random-worlds method. It also shows that random-world provides a principled justification for the approach (Goldszmidt et al., 1990) present (one which is quite different from the justification given in (Goldszmidt et al., 1990) itself). 4.4 Beyond simple queries In Section 4.2 we restricted attention to simple queries. <p> that all the computational techniques and results described in <ref> (Goldszmidt et al., 1990) </ref> carry over to this special case of the random-worlds method. It also shows that random-world provides a principled justification for the approach (Goldszmidt et al., 1990) present (one which is quite different from the justification given in (Goldszmidt et al., 1990) itself). 4.4 Beyond simple queries In Section 4.2 we restricted attention to simple queries. Our main result, Theorem 4.11, needed other assumptions as well: essential positivity, the existence of a unique maximum-entropy point ~v, and the requirement that F [ ] (~v) &gt; 0. <p> In general, this seems very hard. But, interestingly, the computational technique of <ref> (Goldszmidt et al., 1990) </ref> does use this type of parametric analysis, demonstrating that things might not be so bad for various restricted cases. Another source of hope is to remember that maximum entropy is, for us, merely one tool for computing random-worlds degrees of belief.
Reference: <author> Goldszmidt, M., Morris, P., & Pearl, J. </author> <year> (1993). </year> <title> A maximum entropy approach to nonmono-tonic reasoning. </title> <journal> IEEE Transactions of Pattern Analysis and Machine Intelligence, </journal> <volume> 15 (3), </volume> <pages> 220-232. </pages>
Reference-contexts: as we show in Section 4.3, this restricted sublanguage is rich enough to allow us to embed two well-known propositional approaches that make use of maximum entropy: Nilsson's probabilistic logic (Nilsson, 1986) and the maximum-entropy extension of *-semantics (Geffner & Pearl, 1990) due to Goldszmidt, Morris, Pearl (1990) (see also <ref> (Goldszmidt, Morris, & Pearl, 1993) </ref>). In Section 4.4, we consider whether the results for the restricted language can be extended.
Reference: <author> Grandjean, E. </author> <year> (1983). </year> <title> Complexity of the first-order theory of almost all structures. </title> <journal> Information and Control, </journal> <volume> 52, </volume> <pages> 180-204. </pages>
Reference-contexts: For example, in the context of first-order formulas, it is well-known that a formula with no constant or function symbols has an asymptotic probability of either 0 or 1 (Fagin, 1976; Glebski, Kogan, Liogon'ki, & Talanov, 1969). Furthermore, we can decide which <ref> (Grandjean, 1983) </ref>. However, the 0-1 law fails if the language includes constants or if we look at conditional probabilities (Fagin, 1976), and we need both these features in order to reason about degrees of belief for formulas involving particular individuals, conditioned on what is known.
Reference: <author> Grove, A. J., Halpern, J. Y., & Koller, D. </author> <year> (1992). </year> <title> Random worlds and maximum entropy. </title> <booktitle> In Proc. 7th IEEE Symp. on Logic in Computer Science, </booktitle> <pages> pp. 22-33. </pages>
Reference-contexts: This convention represents one reasonable choice.) We used the same approach in an earlier version of this paper <ref> (Grove, Halpern, & Koller, 1992) </ref> in the context of a language that uses approximate equality. Unfortunately, as the following example shows, this has problems.
Reference: <author> Grove, A. J., Halpern, J. Y., & Koller, D. </author> <year> (1993a). </year> <title> Asymptotic conditional probabilities: the non-unary case. </title> <type> Research report RJ 9564, </type> <institution> IBM. </institution>
Reference-contexts: However, the 0-1 law fails if the language includes constants or if we look at conditional probabilities (Fagin, 1976), and we need both these features in order to reason about degrees of belief for formulas involving particular individuals, conditioned on what is known. In two companion papers <ref> (Grove, Halpern, & Koller, 1993a, 1993b) </ref>, we consider the question of what happens in the pure first-order case (where there is no statistical information) in greater detail. <p> In this paper, we consider the much more useful case where the knowledge base has statistical as well as first-order information. In light of the results of <ref> (Grove et al., 1993a, 1993b) </ref>, for most of the paper we restrict attention to the case when the knowledge base is expressed in a unary language. <p> The difficulty of finding an analogue to entropy in the presence of higher-arity predicates is supported by results from <ref> (Grove et al., 1993a) </ref>. In this paper we have shown that maximum entropy can be a useful tool for computing degrees of belief in certain cases, if the KB involves only unary predicates. In (Grove et al., 1993a) we show that there can be no general computational technique to compute degrees <p> finding an analogue to entropy in the presence of higher-arity predicates is supported by results from <ref> (Grove et al., 1993a) </ref>. In this paper we have shown that maximum entropy can be a useful tool for computing degrees of belief in certain cases, if the KB involves only unary predicates. In (Grove et al., 1993a) we show that there can be no general computational technique to compute degrees of belief once we have non-unary predicate symbols in the KB . The problem of finding degrees of belief in this case is highly undecidable.
Reference: <author> Grove, A. J., Halpern, J. Y., & Koller, D. </author> <year> (1993b). </year> <title> Asymptotic conditional probabilities: the unary case. </title> <type> Research report RJ 9563, </type> <institution> IBM. </institution> <note> To appear in SIAM Journal on Computing. </note>
Reference-contexts: , then there is a constant c such that for all formulas over the vocabulary , #[ 0 ]worlds ~t N () = c#[]worlds ~t This result, from which it follows that the degree of belief Pr ~t N ('jKB) is independent of our choice of vocabulary, is proved in <ref> (Grove et al., 1993b) </ref>. Typically, we know neither N nor ~t exactly. All we know is that N is "large" and that ~t is "small". <p> As we mentioned, we can extend our approach to deal with formulas ' that also use non-unary predicate symbols. Our computational procedure for such formulas uses the maximum-entropy approach described above combined with the techniques of <ref> (Grove et al., 1993b) </ref>. These latter were used in (Grove et al., 1993b) to compute asymptotic conditional probabilities when conditioning on a first-order knowledge base KB fo . <p> As we mentioned, we can extend our approach to deal with formulas ' that also use non-unary predicate symbols. Our computational procedure for such formulas uses the maximum-entropy approach described above combined with the techniques of <ref> (Grove et al., 1993b) </ref>. These latter were used in (Grove et al., 1993b) to compute asymptotic conditional probabilities when conditioning on a first-order knowledge base KB fo . The basic idea in that case is as follows: To compute Pr 1 ('jKB fo ), we examine the behavior of ' in finite models of KB fo . <p> Since both ' and fl ^ D are first-order formulas and fl ^ D is precisely of the required form in <ref> (Grove et al., 1993b) </ref>, then Pr 1 ('j fl ^ D) is either 0 or 1, and we can use the algorithm of (Grove et al., 1993b) to compute this limit, in the time bounds outlined there. <p> Since both ' and fl ^ D are first-order formulas and fl ^ D is precisely of the required form in <ref> (Grove et al., 1993b) </ref>, then Pr 1 ('j fl ^ D) is either 0 or 1, and we can use the algorithm of (Grove et al., 1993b) to compute this limit, in the time bounds outlined there. One corollary of the above is that the formula 6= holds with probability 1 given any knowledge base KB of the form we are interested in. <p> Note that since the latter probability only refers to first-order formulas, it is independent of the tolerance values. Proof: That the right-hand side is either 0 or 1 is proved in <ref> (Grove et al., 1993b) </ref>, where it is shown that the asymptotic probability of any pure first-order sentence when conditioned on knowledge of the form fl ^ D (which is, essentially, what was called a model description in (Grove et al., 1993b)) is either 0 or 1. <p> Proof: That the right-hand side is either 0 or 1 is proved in <ref> (Grove et al., 1993b) </ref>, where it is shown that the asymptotic probability of any pure first-order sentence when conditioned on knowledge of the form fl ^ D (which is, essentially, what was called a model description in (Grove et al., 1993b)) is either 0 or 1. <p> We briefly sketch the relevant details here, referring the reader to <ref> (Grove et al., 1993b) </ref> for full details. The idea (which actually goes back to Fagin (1976)) is to associate with a model description such as fl ^ D a theory T which essentially consists of extension axioms. <p> From (b) it easily follows that if T j= ~, then Pr 1 (~j fl ^ D) is also 1. Using (a), the desired 0-1 law follows. The only difference from the proof in <ref> (Grove et al., 1993b) </ref> is that we need to show that (b) holds even when we condition on KB ^ [*] ^ fl ^ D, instead of just on fl ^ D.
Reference: <author> Halpern, J. Y. </author> <year> (1990). </year> <title> An analysis of first-order logics of probability. </title> <journal> Artificial Intelligence, </journal> <volume> 46, </volume> <pages> 311-350. </pages>
Reference-contexts: The basic idea is quite straightforward. Suppose we are interested in attaching a degree of belief to a formula ' given a knowledge base KB . One useful way of assigning semantics to degrees of belief formulas is to use a probability distribution over a set of possible worlds <ref> (Halpern, 1990) </ref>. More concretely, suppose for now that we are reasoning about N individuals, 1; : : : ; N . A world is a complete description of which individuals have each of the properties of interest. Formally, a world is just a model, or interpretation, over our first-order language. <p> When standard equality is used rather than approximate equality this problem is easily overcome, simply by avoiding conditional probabilities in the semantics altogether. Following <ref> (Halpern, 1990) </ref>, we can eliminate conditional proportion expressions altogether by viewing a statement such as k jk X = ff as an abbreviation for jj ^ jj X = ffjjjj X . Thus, we never actually form quotients of probabilities. <p> We give semantics to the language L by providing a translation from formulas in L to formulas in a language L = whose semantics is more easily described. The language L = is essentially the language of <ref> (Halpern, 1990) </ref>, that uses true equality rather than approximate equality when comparing proportion expressions. <p> This embedding avoids the problem encountered in Example 2.2, because when we multiply to clear conditional proportions the tolerances are explicit, and so are also multipled as appropriate. The semantics for L = is quite straightforward, and is similar to that in <ref> (Halpern, 1990) </ref>. We give semantics to L = in terms of worlds, or finite first-order models. For any natural number N , let W N consist of all worlds with domain f1; : : : ; N g. <p> We remark that the length of the formula b ~ is typically exponential in the length of ~. Such a blowup seems inherent in any scheme defined in terms of atoms. Theorem 3.5 is a generalization of Claim 5.7.1 in <ref> (Halpern, 1990) </ref>. It, in turn, is a generalization of a well-known result which says that any first-order formula with only unary predicates is equivalent to one with only depth-one quantifier nesting. <p> Consider the set of K = 2 k truth assignments these propositions. We give semantics to probabilistic statements over this language in terms of a probability distribution over the set (see <ref> (Fagin, Halpern, & Megiddo, 1990) </ref> for details). Since each truth assignment ! 2 determines the truth value of every propositional formula fi, we can determine the probability of every such formula: Pr (fi) = !j=fi Clearly, we can determine whether a probability distribution satisfies a set fl of probabilistic constraints.
Reference: <author> Jaynes, E. T. </author> <year> (1957). </year> <journal> Information theory and statistical mechanics. Physical Review, </journal> <volume> 106 (4), </volume> <pages> 620-630. </pages>
Reference: <author> Jaynes, E. T. </author> <year> (1978). </year> <title> Where do we stand on maximum entropy?. </title> <editor> In Levine, R. D., & Tribus, M. (Eds.), </editor> <booktitle> The Maximum Entropy Formalism, </booktitle> <pages> pp. 15-118. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, Mass. </address>
Reference-contexts: In addition, maximum entropy is known to have many attractive properties <ref> (Jaynes, 1978) </ref>. Our result shows these properties are shared by the random-worlds approach in the domain where these two approaches agree. Indeed, as shown in (Bacchus et al., 1994), the random-worlds approach has many of these properties for the full (non-unary) language.
Reference: <author> Jaynes, E. T. </author> <year> (1982). </year> <title> On the rationale of maximum-entropy methods. </title> <journal> Proc. IEEE, </journal> <volume> 70 (9), </volume> <pages> 939-952. </pages>
Reference-contexts: The concentration phenomenon relating entropy to the random-worlds method is well-known <ref> (Jaynes, 1982, 1983) </ref>. In physics, the "worlds" are the possible configurations of a system typically consisting of many particles or molecules, and the mutually exclusive properties (our atoms) can be, for example, quantum states. The corresponding entropy measure is at the heart of statistical mechanics and thermodynamics.
Reference: <author> Jaynes, E. T. </author> <year> (1983). </year> <title> Concentration of distributions at entropy maxima. </title> <editor> In Rosenkrantz, R. D. (Ed.), E. T. </editor> <title> Jaynes: Papers on Probability, </title> <journal> Statistics, and Statistical Physics, </journal> <pages> pp. 315-336. </pages> <publisher> Kluwer, </publisher> <address> Dordrecht, Netherlands. </address>
Reference: <author> Keynes, J. M. </author> <year> (1921). </year> <title> A Treatise on Probability. </title> <publisher> Macmillan, </publisher> <address> London. </address> <note> 87 Grove, </note> <author> Halpern, & Koller Koller, D., & Halpern, J. Y. </author> <year> (1992). </year> <title> A logic for approximate reasoning. </title> <editor> In Nebel, B., Rich, C., & Swartout, W. (Eds.), </editor> <booktitle> Proc. Third International Conference on Principles of Knowledge Representation and Reasoning (KR '92), </booktitle> <pages> pp. 153-164. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA, San Francisco. </address>
Reference-contexts: It is essentially an application of what has been c fl1994 AI Access Foundation and Morgan Kaufmann Publishers. All rights reserved. Grove, Halpern, & Koller called the principle of indifference <ref> (Keynes, 1921) </ref>. The basic idea is quite straightforward. Suppose we are interested in attaching a degree of belief to a formula ' given a knowledge base KB .
Reference: <author> Kyburg, Jr., H. E. </author> <year> (1983). </year> <title> The reference class. </title> <journal> Philosophy of Science, </journal> <volume> 50 (3), </volume> <pages> 374-397. </pages>
Reference-contexts: In that paper, we show that the random-worlds method is remarkably successful at satisfying the desiderata of both nonmonotonic (default) reasoning (Ginsberg, 1987) and reference class reasoning <ref> (Kyburg, 1983) </ref>. The results of (Bacchus et al., 1994) show that the behavior we saw in the example above holds quite generally, as do many other properties we would hope to have satisfied.
Reference: <author> Laplace, P. S. d. </author> <title> (1820). Essai Philosophique sur les Probabilites. English translation is Philosophical Essay on Probabilities, </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1951. </year>
Reference: <author> Luce, R. D., & Raiffa, H. </author> <year> (1957). </year> <title> Games and Decisions. </title> <publisher> Wiley, </publisher> <address> New York. </address>
Reference-contexts: Suppose the agent wants to use this information to make decisions. For example, a doctor might need to decide whether to administer antibiotics to a particular patient Eric. To apply standard tools of decision theory (see <ref> (Luce & Raiffa, 1957) </ref> for an introduction), the agent must assign probabilities, or degrees of belief, to various events. For example, the doctor may need to assign a degree of belief to an event such as "Eric has hepatitis".
Reference: <author> Nilsson, N. </author> <year> (1986). </year> <title> Probabilistic logic. </title> <journal> Artificial Intelligence, </journal> <volume> 28, </volume> <pages> 71-87. </pages>
Reference-contexts: In spite of this restriction, many of the issues arising in the general case can be seen here. Moreover, as we show in Section 4.3, this restricted sublanguage is rich enough to allow us to embed two well-known propositional approaches that make use of maximum entropy: Nilsson's probabilistic logic <ref> (Nilsson, 1986) </ref> and the maximum-entropy extension of *-semantics (Geffner & Pearl, 1990) due to Goldszmidt, Morris, Pearl (1990) (see also (Goldszmidt, Morris, & Pearl, 1993)). In Section 4.4, we consider whether the results for the restricted language can be extended.
Reference: <author> Paris, J. B., & Vencovska, A. </author> <year> (1989). </year> <title> On the applicability of maximum entropy to inexact reasoning. </title> <journal> International Journal of Approximate Reasoning, </journal> <volume> 3, </volume> <pages> 1-34. </pages>
Reference-contexts: Assume, on the other hand, that fl (B) = 0, so that Pr fl (CjB) is not well-defined. In this case, we can use a known result (see <ref> (Paris & Vencovska, 1989) </ref>) for the maximum-entropy point over a space defined by linear constraints, and conclude that for all satisfying R, necessarily (B) = 0.
Reference: <author> Pearl, J. </author> <year> (1988). </year> <title> Probabilistic Reasoning in Intelligent Systems. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, Calif. </address>
Reference: <author> Pearl, J. </author> <year> (1989). </year> <title> Probabilistic semantics for nonmonotonic reasoning: A survey. </title> <editor> In Brach-man, R. J., Levesque, H. J., & Reiter, R. (Eds.), </editor> <booktitle> Proc. First International Conference on Principles of Knowledge Representation and Reasoning (KR '89), </booktitle> <pages> pp. 505-516. </pages>
Reference-contexts: Clearly, such constraints are not linear. Nevertheless, our Theorem 4.11 covers such cases and much more. A version of probabilistic propositional reasoning has also been used to provide probabilistic semantics for default reasoning <ref> (Pearl, 1989) </ref>. Here also, the connection to random worlds is of interest. In particular, it follows from Corollary 4.10 that the recent work of Goldszmidt, Morris, and Pearl (1990) can be embedded in the random-worlds framework. In the rest of this subsection, we explain their approach and the embedding.
Reference: <editor> Reprinted in Readings in Uncertain Reasoning, G. Shafer and J. Pearl (eds.), </editor> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, Calif., </address> <year> 1990, </year> <pages> pp. 699-710. </pages>
Reference: <author> Pollock, J. L. </author> <year> (1984). </year> <title> Foundations for direct inference. </title> <journal> Theory and Decision, </journal> <volume> 17, </volume> <pages> 221-256. </pages>
Reference: <author> Reichenbach, H. </author> <year> (1949). </year> <title> Theory of Probability. </title> <institution> University of California Press, Berkeley. </institution>
Reference-contexts: In (Bacchus et al., 1994) we present a number of such techniques, most of which apply only in very special cases. One of the simplest and most intuitive is the following version of what philosophers have termed direct inference <ref> (Reichenbach, 1949) </ref>. Suppose that all we know about an individual c is some assertion (c); in other words, KB has the form (c) ^ KB 0 , and the constant c does not appear in KB 0 .
Reference: <author> Shannon, C., & Weaver, W. </author> <year> (1949). </year> <title> The Mathematical Theory of Communication. </title> <publisher> University of Illinois Press. </publisher>
Reference-contexts: Using the appropriate definitions, it can be shown that there is a sense in which this fl incorporates the "least" additional information <ref> (Shannon & Weaver, 1949) </ref>. For example, if we have no constraints on , then fl will be the measure that assigns equal probability to all elements of .
Reference: <author> Shastri, L. </author> <year> (1989). </year> <title> Default reasoning in semantic networks: a formalization of recognition and inheritance. </title> <journal> Artificial Intelligence, </journal> <volume> 39 (3), </volume> <pages> 285-355. </pages>
Reference: <author> Spiegelhalter, D. J. </author> <year> (1986). </year> <title> Probabilistic reasoning in predictive expert systems. </title> <editor> In Kamal, L. N., & Lemmer, J. F. (Eds.), </editor> <booktitle> Uncertainty in Artificial Intelligence, </booktitle> <pages> pp. 47-68. </pages> <publisher> North-Holland, Amsterdam. </publisher>
Reference: <author> Tarski, A. </author> <year> (1951). </year> <title> A Decision Method for Elementary Algebra and Geometry (2nd edition). </title> <publisher> Univ. of California Press. </publisher> <pages> 88 </pages>
Reference-contexts: In (Bochnak et al., 1987), a set is taken to be semi-algebraic if it is definable by a quantifier-free formula in the language of real closed fields. However, as observed in (Bochnak et al., 1987), since the theory of real closed fields admits elimination of quantifiers <ref> (Tarski, 1951) </ref>, the two definitions are equivalent. 74 Random Worlds and Maximum Entropy Lemma B.5: For all sufficiently small ~t , S &lt;~t [KB] = S ~t [KB ]. Proof: Clearly S &lt;~t [KB] S ~t [KB].
References-found: 39

