URL: http://www.cs.toronto.edu/~eparsons/depth.ps.gz
Refering-URL: http://www.cs.toronto.edu/~eparsons/
Root-URL: http://www.cs.toronto.edu
Title: Multiprogrammed Multiprocessor Scheduling  
Author: Eric W. Parsons 
Note: Contents  
Date: November 1994  
Abstract: Depth Report 
Abstract-found: 1
Intro-found: 1
Reference: [ALL89] <author> Thomas E. Anderson, Edward D. Lazowska, and Henry M. Levy. </author> <title> The performance implications of thread management alternatives for shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 1989 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 49-60, </pages> <year> 1989. </year>
Reference-contexts: We first consider message-passing architectures, which have received the majority of attention, followed by NUMA architectures. It must be remarked that, for either class of distributed-memory machines, an important property of the design of a scheduling discipline is that it be able to scale well <ref> [FR90, ALL89, NW89] </ref>.
Reference: [Amd67] <author> G. M. </author> <title> Amdahl. Validity of the single processor approach to achieving large scale computing capabilities. </title> <booktitle> In Proceedings of the AFIPS Spring Joint Computer Conference, </booktitle> <pages> pages 483-485, </pages> <month> April </month> <year> 1967. </year>
Reference-contexts: A parallel system, in this context, refers to a specific combination of a hardware architecture and an application. The first notable development in application scalability, known as Amdahl's Law, was that the fraction of sequential computation places an upper bound on the achievable speedup <ref> [Amd67] </ref>. The next notable development, usually attributed to Gustafson, was the observation that practically speaking, one does not just increases the number of processors for a fixed problem size [Gus88]. Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). <p> Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). Since this observation was made, a large number of scalability metrics have been proposed and examined, including speedup <ref> [Amd67] </ref>, scaled speedup [Gus88], sizeup [SG91], and isoefficiency [GGK93].
Reference: [BB90] <author> Krishna P. Belkhale and Prithviraj Banerjee. </author> <title> Approximate algorithms for the parti-tionable independent task scheduling problem. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing (ICPP), </booktitle> <volume> volume I, </volume> <pages> pages 72-75, </pages> <year> 1990. </year>
Reference-contexts: Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [Bla90] <author> David L. Black. </author> <title> Scheduling support for concurrency and parallelism in the Mach operating system. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 35-43, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: One approach for doing this in a hierarchical ring network is to hierarchically define 16 logical clusters of processors that are "close" to each other <ref> [Bre93b, Bla90] </ref>. The operating system can schedule the threads of a job within a cluster (or super-cluster), ensuring that communication latencies are minimized. It has been shown that matching the logical cluster to the physical machine leads to the best performance [Bre93b].
Reference: [Bre93a] <author> Timothy Brecht. </author> <title> Multiprogrammed Parallel Application Scheduling in NUMA Multiprocessors. </title> <type> PhD thesis, </type> <institution> University of Toronto, </institution> <month> December </month> <year> 1993. </year>
Reference-contexts: But as the non-uniformity increases, it becomes increasingly important to schedule cooperating threads in relative proximity (with respect to the communication distance) to each other <ref> [Bre93b, Bre93a] </ref>. One approach for doing this in a hierarchical ring network is to hierarchically define 16 logical clusters of processors that are "close" to each other [Bre93b, Bla90]. The operating system can schedule the threads of a job within a cluster (or super-cluster), ensuring that communication latencies are minimized.
Reference: [Bre93b] <author> Timothy Brecht. </author> <title> On the importance of parallel application placement in NUMA multiprocessors. </title> <booktitle> In Symposium on Experiences with Distributed and Multiprocessor Systems (SEDMS IV), </booktitle> <pages> pages 1-18, </pages> <year> 1993. </year>
Reference-contexts: But as the non-uniformity increases, it becomes increasingly important to schedule cooperating threads in relative proximity (with respect to the communication distance) to each other <ref> [Bre93b, Bre93a] </ref>. One approach for doing this in a hierarchical ring network is to hierarchically define 16 logical clusters of processors that are "close" to each other [Bre93b, Bla90]. The operating system can schedule the threads of a job within a cluster (or super-cluster), ensuring that communication latencies are minimized. <p> One approach for doing this in a hierarchical ring network is to hierarchically define 16 logical clusters of processors that are "close" to each other <ref> [Bre93b, Bla90] </ref>. The operating system can schedule the threads of a job within a cluster (or super-cluster), ensuring that communication latencies are minimized. It has been shown that matching the logical cluster to the physical machine leads to the best performance [Bre93b]. <p> The operating system can schedule the threads of a job within a cluster (or super-cluster), ensuring that communication latencies are minimized. It has been shown that matching the logical cluster to the physical machine leads to the best performance <ref> [Bre93b] </ref>. Some studies have compared gang scheduling to dynamic partitioning in NUMA architectures. It is generally recognized that dynamic partitioning enables jobs to run at a better efficiency point as the load increases, improving overall response time [CDV + 94, CDD + 91].
Reference: [CDD + 91] <author> Mark Crovella, Prakash Das, Czarek Dubnicki, Thomas LeBlanc, and Evangelos Markatos. </author> <title> Multiprogramming on multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Systems, </booktitle> <year> 1991. </year>
Reference-contexts: Some studies have compared gang scheduling to dynamic partitioning in NUMA architectures. It is generally recognized that dynamic partitioning enables jobs to run at a better efficiency point as the load increases, improving overall response time <ref> [CDV + 94, CDD + 91] </ref>. The problem is that applications designed for distributed-memory architectures often explicitly design an algorithm for the target memory hierarchy, making it more difficult for these to adapt to changing partition sizes.
Reference: [CDV + 94] <author> Rohit Chandra, Scott Devine, Ben Verghese, Anoop Gupta, and Mendel Rosenblum. </author> <title> Scheduling and page migration for multiprocessor compute servers. </title> <booktitle> In Proceedings of the Sixth International Conference on Architectural Support for Programming Language and Operating Systems (ASPLOS-VI), </booktitle> <year> 1994. </year>
Reference-contexts: The first thing to recognize about scheduling in a NUMA system is that memory-affinity scheduling becomes increasingly important. Even if data is migrated to where a thread is executing, substantial savings can be gained from giving preference to threads that have previously run on a given processor <ref> [CDV + 94] </ref>. But as the non-uniformity increases, it becomes increasingly important to schedule cooperating threads in relative proximity (with respect to the communication distance) to each other [Bre93b, Bre93a]. <p> Some studies have compared gang scheduling to dynamic partitioning in NUMA architectures. It is generally recognized that dynamic partitioning enables jobs to run at a better efficiency point as the load increases, improving overall response time <ref> [CDV + 94, CDD + 91] </ref>. The problem is that applications designed for distributed-memory architectures often explicitly design an algorithm for the target memory hierarchy, making it more difficult for these to adapt to changing partition sizes.
Reference: [CL91] <author> E. G. Coffman, Jr. and George S. Lueker. </author> <title> Probabilistic Analysis of Packing and Partitioning Algorithms. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1991. </year>
Reference-contexts: To rectify this problem, the matrix must be periodically reordered, and in this case it is not clear how bin packing will help. 19 Finally, run-to-completion scheduling can be related to what is sometimes termed strip pack-ing <ref> [CL91] </ref>. In strip packing, objects are placed on a semi-infinite strip in such a way that the length of the strip is minimized. This corresponds to, in scheduling terms, minimizing the makespan. Again, it is possible to distinguish between vector and rectangular strip packing. <p> This corresponds to, in scheduling terms, minimizing the makespan. Again, it is possible to distinguish between vector and rectangular strip packing. Recent work on bin and strip packing has been focussed primarily on on-line algorithms [CR89, CWK93] and on average case behaviour <ref> [KLMS84, CL91] </ref>. In the latter, the size of objects in each dimension is modeled by independent uniform distributions (which is unlikely to be representative of parallel environments). 6 Conclusions Many different facets of scheduling multiprogrammed, multiprocessor systems have been examined in this paper.
Reference: [CL94] <author> Mark E. Crovella and Thomas J. LeBlanc. </author> <title> Parallel performance prediction using lost cycles analysis. </title> <booktitle> In Supercomputing '94, </booktitle> <year> 1994. </year> <month> 22 </month>
Reference-contexts: In scalability measurement, data points are obtained from a small number of runs of the program and some technique is used to extrapolate the performance for other conditions (hardware speed, problem size, processors). One project that is particularly interesting is the Lost Cycles Analysis taking place at Ro-chester <ref> [CL94] </ref>. An instrumentation tool has been designed to instrument an application so that, when executed on the target hardware, the run-time system captures the number of cycles spent (lost) in various categories of overhead.
Reference: [CMV94] <author> Su-Hui Chiang, Rajesh K. Mansharamani, and Mary K. Vernon. </author> <title> Use of application characteristics and limited preemption for run-to-completion parallel processor scheduling policies. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 33-44, </pages> <year> 1994. </year>
Reference-contexts: Through analytic models, it has been shown that ASP is superior to PWS for a certain 2-point service time distribution. Subsequent simulation studies provide further evidence that a variant of ASP which bounds the maximum processor allocation to a job can perform at least as well as PWS <ref> [CMV94] </ref>. Our research shows that for a different kind of workload model, PWS usually performs better than ASP. Another discipline that takes only maximum parallelism into consideration is Adaptive Partitioning (AP). This discipline varies its target partition size gradually in response to changing load [RSD + 94]. <p> This discipline varies its target partition size gradually in response to changing load [RSD + 94]. Our research shows that this discipline is not competitive against PWS or ASP, however. Other static RTC disciplines that have been proposed and are worth mentioning include A+&mM [Sev89] and AVG <ref> [LV90, CMV94] </ref>. 4.5.3 Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations. <p> Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand <ref> [LV90, CMV94, PS94] </ref>. When system loads increase, allocations to jobs decrease allowing them to operate at a more favourable point on their efficiency curve. Also, 13 because equipartition is effectively the analog of RR in uniprocessing, it is relatively insensitive to variability in service requirement. <p> Fueling the question even further is a recent study showing that popular types of job-specific knowledge were not useful as currently used in static run-to-completion disciplines <ref> [CMV94] </ref>. It is the author's belief that these two are linked. On the more theoretical side, much work remains to be done on optimizing mean response time in run-to-completion environments.
Reference: [Cof76] <author> E. G. Coffman, Jr., </author> <title> editor. Computer and Job-Shop Scheduling Theory. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1976. </year>
Reference-contexts: Therefore, makespan is only relevant when a predefined number of jobs or tasks are known to the system. Two examples of where the makespan is relevant is in batch systems, where the goal is to service all jobs as quickly as possible <ref> [Cof76] </ref>, and in mapping subtasks of a large parallel job to processors, where the goal is to execute the entire set of subtask as quickly as possible [NT93]. The objective in both cases is to minimize the measure concerned. <p> We can also schedule on a mesh, hypercube, or any other interconnection network, where the processor allocation must be contiguous in those forms. Scheduling on a PRAM is a specific case of Garey and Graham's general result for scheduling jobs having multiple resources <ref> [Cof76] </ref>. By using a simple list-processing scheme, one can come within a factor of s + 1 of the optimal makespan, where s is the number of resources (in this case just one|the processors).
Reference: [CR89] <author> Don Coppersmith and Prabhakar Raghavan. </author> <title> Multidimensional on-line bin packing: Algorithms and worst-case analysis. </title> <journal> Operations Research Letters, </journal> <volume> 8 </volume> <pages> 17-20, </pages> <year> 1989. </year>
Reference-contexts: This corresponds to, in scheduling terms, minimizing the makespan. Again, it is possible to distinguish between vector and rectangular strip packing. Recent work on bin and strip packing has been focussed primarily on on-line algorithms <ref> [CR89, CWK93] </ref> and on average case behaviour [KLMS84, CL91]. In the latter, the size of objects in each dimension is modeled by independent uniform distributions (which is unlikely to be representative of parallel environments). 6 Conclusions Many different facets of scheduling multiprogrammed, multiprocessor systems have been examined in this paper.
Reference: [CS87] <author> Ming-Syan Chen and Kang G. Shin. </author> <title> Processor allocation in an n-cube multiprocessor using gray codes. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-36(12):1396-1407, </volume> <month> December </month> <year> 1987. </year>
Reference-contexts: In fact, finding a good partitioning scheme for hypercubes can be quite difficult, and several techniques have been proposed for this problem <ref> [CS87] </ref>. Even static RTC or QB scheduling policies may require processor reallocation to reduce the effects of fragmentation. For this reason (among others), most commercial distributed-memory machines are configured at system initialization to have a number of fixed-sized partitions, each of which is used for a particular class of jobs.
Reference: [CWK93] <author> Ee-Chien Chang, Weiguo Wang, and Mohan S. Kankanhalli. </author> <title> Multidimensional online bin-packing: An algorithm and its average-case analysis. </title> <journal> Information Processing Letters, </journal> <volume> 48 </volume> <pages> 121-125, </pages> <year> 1993. </year>
Reference-contexts: This corresponds to, in scheduling terms, minimizing the makespan. Again, it is possible to distinguish between vector and rectangular strip packing. Recent work on bin and strip packing has been focussed primarily on on-line algorithms <ref> [CR89, CWK93] </ref> and on average case behaviour [KLMS84, CL91]. In the latter, the size of objects in each dimension is modeled by independent uniform distributions (which is unlikely to be representative of parallel environments). 6 Conclusions Many different facets of scheduling multiprogrammed, multiprocessor systems have been examined in this paper.
Reference: [DCDP90] <author> K. Dussa, B. Carlson, L. Dowdy, and K-H. Park. </author> <title> Dynamic partitioning in a trans-puter environment. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 203-213, </pages> <year> 1990. </year>
Reference-contexts: Jobs typically wait in first-come first-served order and run-to-completion when activated. Research in scheduling for distributed-memory architectures, however, has focussed on dynamic partitioning, where processors are dynamically reallocated in response to the number of jobs in the system <ref> [MZ94, NSS93, DCDP90] </ref>. In work by McCann and Zahorjan, applications are assumed to be designed for a virtual machine that matches the characteristics of the physical machine, which in this case is a mesh. <p> A similar approach was also used in an earlier study of a two-job workload running on a small transputer-based system arranged in a ring, where it was concluded that dynamic partitioning was both feasible and desirable <ref> [DCDP90] </ref>. Others have demonstrated that application redistribution, as required by equipartitioning, is also feasible at moderate cost, and that under high variability in service demand, can offer significantly better performance than a static partitioning scheme [NSS93, Set93].
Reference: [Dow88] <author> L. W. Dowdy. </author> <title> On the partitioning of multiprocessor systems. </title> <type> Technical Report 88-06, </type> <institution> Dept. of Computer Science, Vanderbuilt University, </institution> <year> 1988. </year>
Reference-contexts: Dowdy uses the simple function: T (p) = C 1;j + C 2;j =p where C 1;j and C 2;j are constants obtained from empirical measurements <ref> [Dow88] </ref>. Although widely used, this characterization suffers from two problems. First, it is unrealistic because an increased number of processors always leads to a reduction in execution time (assuming C 2;j &gt; 0).
Reference: [EZL89] <author> Derek L. Eager, John Zahorjan, and Edward D. Lazowska. </author> <title> Speedup versus efficiency in parallel systems. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 38(3) </volume> <pages> 408-423, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: In fact, the average parallelism, which is the area under the parallelism profile curve divided by the execution time, is the same as the speedup for an unlimited number of processors S (1) <ref> [EZL89] </ref>. The lower bound is a little more difficult to derive, and so the argument is not reproduced here. the maximum attainable speedup permitted by the hardware. <p> Early work in this area established that that a good processor allocation is one which corresponds to the smallest ratio of execution time to efficiency (i.e., the value at the knee of the execution time-efficiency profile), as this is a point that maximizes the ratio of benefit to cost <ref> [EZL89] </ref>. In absence of that information, the average parallelism offers a good alternative, as it gives similar performance guarantees as the value at the knee [EZL89]. It has been shown, however, that the overall workload volume should be taken into account in the scheduling decision [Sev89]. <p> efficiency (i.e., the value at the knee of the execution time-efficiency profile), as this is a point that maximizes the ratio of benefit to cost <ref> [EZL89] </ref>. In absence of that information, the average parallelism offers a good alternative, as it gives similar performance guarantees as the value at the knee [EZL89]. It has been shown, however, that the overall workload volume should be taken into account in the scheduling decision [Sev89].
Reference: [FR90] <author> Dror G. Feitelson and Larry Rudolph. </author> <title> Distributed hierarchical control for parallel processing. </title> <journal> Computer, </journal> <volume> 23(5) </volume> <pages> 65-77, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: We first consider message-passing architectures, which have received the majority of attention, followed by NUMA architectures. It must be remarked that, for either class of distributed-memory machines, an important property of the design of a scheduling discipline is that it be able to scale well <ref> [FR90, ALL89, NW89] </ref>.
Reference: [FR92] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang scheduling performance benefits for fine-grain synchronization. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 16 </volume> <pages> 306-318, </pages> <year> 1992. </year>
Reference-contexts: In one study using four applications, the processor utilization measured for dynamic thread scheduling combined with cache-affinity scheduling and spin-block locking decreased by a moderate 9% over that of an (optimal) batch strategy [GTU91]. For applications that exhibit fine-grained synchronization, such techniques will not be effective <ref> [FR92] </ref> because too much overhead is incurred at synchronization points. In this case, it is necessary to use a static allocation or a process-control approach to match the number of threads in a job to the current processor allocation.
Reference: [GGK93] <author> Ananth Y. Grama, Anshul Gupta, and Vipin Kumar. Isoefficiency: </author> <title> Measuring the scalability of parallel algorithms and architectures. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 1(3) </volume> <pages> 12-21, </pages> <month> August </month> <year> 1993. </year>
Reference-contexts: Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). Since this observation was made, a large number of scalability metrics have been proposed and examined, including speedup [Amd67], scaled speedup [Gus88], sizeup [SG91], and isoefficiency <ref> [GGK93] </ref>. <p> The most recent approach to studying application scalability is to determine the rate at which a problem size must increase in order to maintain a certain level of efficiency. The function that characterizes the increase is called the isoefficiency function <ref> [GGK93] </ref>. A scalable system is one which possesses an isoefficiency function for all possible efficiency values. The reason why these metrics are used in preference to (fixed) speedup is that more realistic conclusions can be drawn regarding the scalability of an application-architecture combination.
Reference: [GST91] <author> Dipak Ghosal, Guiseppe Serazzi, and Satish K. Tripathi. </author> <title> The processor working set and its use in scheduling multiprocess systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 17(5) </volume> <pages> 443-453, </pages> <month> May </month> <year> 1991. </year>
Reference-contexts: Ghosal considers an interesting generalization of efficiency which uses the ratio of a job's speedup to an arbitrary cost function instead of to the number of processors. This ratio, called the efficacy, is used to determine a job's processor working set (pws) <ref> [GST91] </ref>. With the particular cost function chosen for use, p=S (p), the pws can be shown to be equivalent to the number of processors at the knee of the execution time-efficiency profile. A number of static RTC disciplines were investigated assuming knowledge of a job's pws.
Reference: [GTS91] <author> Anoop Gupta, Andrew Tucker, and Luis Stevens. </author> <title> Making effective use of shared-memory multiprocessors: The process control approach. </title> <type> Technical Report CSL-TR-91-475A, </type> <institution> Computer Systems Laboratory, Stanford University, </institution> <month> July </month> <year> 1991. </year>
Reference-contexts: In the process control approach, jobs and the operating system cooperate in processor reallocations, allowing a job to dynamically match its degree of parallelism to the current allocation <ref> [TG89, GTS91] </ref>. Using this approach, the problems of losing cache context and the blocking critical threads are avoided altogether. Two important disciplines that have been studied in the context of process control are equipar-tition and dynamic partitioning.
Reference: [GTU91] <author> Anoop Gupta, Andrew Tucker, and Shigeru Urushibara. </author> <title> The impact of operating system scheduling policies and synchronization methods on the performance of parallel applications. </title> <booktitle> In Proceedings of the 1991 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 120-132, </pages> <year> 1991. </year> <month> 23 </month>
Reference-contexts: With spin-block locking, threads requesting a lock spin for a short duration, after which point they block and yield their processor to another thread <ref> [GTU91] </ref>. In two-level scheduling, the operating system determines how many processors are allocated to a job and the job determines which threads to run on each processor (presumably avoiding the situation where a critical thread is blocked). The use of these techniques can be quite effective. <p> The use of these techniques can be quite effective. In one study using four applications, the processor utilization measured for dynamic thread scheduling combined with cache-affinity scheduling and spin-block locking decreased by a moderate 9% over that of an (optimal) batch strategy <ref> [GTU91] </ref>. For applications that exhibit fine-grained synchronization, such techniques will not be effective [FR92] because too much overhead is incurred at synchronization points.
Reference: [Gus88] <author> John L. Gustafson. </author> <title> Reevaluating Amdahl's Law. </title> <journal> Communications of the ACM, </journal> <volume> 31(5) </volume> <pages> 532-533, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: Given this fact, speedup can never be greater than 1=(f fl (1 f )=p). What is important to realize, however, is that for most applications, increasing the problem (data) size increases the potential parallelism, or decreases the fraction that is sequential <ref> [Gus88] </ref>. Therefore, it is possible to approach a speedup bound arbitrarily close to n by sufficiently increasing the problem size (at least in theory). <p> The next notable development, usually attributed to Gustafson, was the observation that practically speaking, one does not just increases the number of processors for a fixed problem size <ref> [Gus88] </ref>. Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). Since this observation was made, a large number of scalability metrics have been proposed and examined, including speedup [Amd67], scaled speedup [Gus88], sizeup [SG91], and isoefficiency [GGK93]. <p> number of processors for a fixed problem size <ref> [Gus88] </ref>. Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). Since this observation was made, a large number of scalability metrics have been proposed and examined, including speedup [Amd67], scaled speedup [Gus88], sizeup [SG91], and isoefficiency [GGK93].
Reference: [KG94] <author> Vipin Kumar and Anshul Gupta. </author> <title> Analyzing scalability of parallel algorithms and architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <note> 1994. To appear. </note>
Reference-contexts: It is beyond the scope of this depth paper to cover all work in scalability analysis (as this would constitute a whole topic of research in itself ), but a relatively comprehensive survey has recently been published <ref> [KG94] </ref>. 17 Increasing the problem size with the number of processors is generally referred to as scaled speedup. In one form of this metric, the problem size is scaled in such a way that the theoretical per-processor computation load remains fixed.
Reference: [Kle79] <author> Leonard Kleinrock. </author> <title> Power and deterministic rules of thumb for probabilistic problems in computer communications. </title> <booktitle> In Proceedings of the International Conference on Communications, </booktitle> <pages> pages 43.1.1-43.1.10, </pages> <month> June </month> <year> 1979. </year>
Reference-contexts: In many cases, reducing the mean response time leads to a reduction in efficiency. A third measure which has been used in some studies is the power, which is usually defined to be the ratio of the throughput to the mean response time <ref> [Kle79] </ref>. Kleinrock uses this ratio to determine a "proper" operating point in communication systems where a tradeoff exists between efficiency (utilization) and queueing delays or packet losses. But there does not appear to be strong evidence as to why this ratio is better than another 1 . <p> But there does not appear to be strong evidence as to why this ratio is better than another 1 . In fact, Kleinrock generalizes the power measure in such a way that the relative importance of throughput versus response time can be varied <ref> [Kle79] </ref>. Since we are mostly concerned with scheduling independent jobs in a multiprogrammed multiprocessor environment, our primary measure of interest is mean response time. 3 Review of Uniprocessor Scheduling Results A scheduling problem is defined by the objective function of interest and a number of distinguishing characteristics or constraints.
Reference: [KLMS84] <author> Richard M. Karp, Michael Luby, and A. Marchetti-Spaccamela. </author> <title> A probabilistic analysis of multidimensional bin packing problems. </title> <booktitle> In Proceedings of the Sixteenth Annual ACM Symposium on the Theory of Computing, </booktitle> <pages> pages 289-298, </pages> <year> 1984. </year>
Reference-contexts: Extending this to multiple dimensions, it is possible to represent the all system resources (memory, processors, and I/O) as bins in which jobs are packed. Research in multidimensional bin packing typically distinguishes between two types of packings, vector and rectangular <ref> [KLMS84] </ref>. In vector bin packing, the sum of the resource requirements of a job must not exceed the limits of the bin in any dimension. In rectangular bin packing, the objects cannot overlap each other in geometric space. <p> This corresponds to, in scheduling terms, minimizing the makespan. Again, it is possible to distinguish between vector and rectangular strip packing. Recent work on bin and strip packing has been focussed primarily on on-line algorithms [CR89, CWK93] and on average case behaviour <ref> [KLMS84, CL91] </ref>. In the latter, the size of objects in each dimension is modeled by independent uniform distributions (which is unlikely to be representative of parallel environments). 6 Conclusions Many different facets of scheduling multiprogrammed, multiprocessor systems have been examined in this paper.
Reference: [LT94] <author> Walter Ludwig and Prasoon Tiwari. </author> <title> Scheduling malleable and nonmalleable parallel tasks. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 167-176, </pages> <year> 1994. </year>
Reference-contexts: Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [LV90] <author> Scott T. Leutenegger and Mary K. Vernon. </author> <title> The performance of multiprogrammed multiprocessor scheduling policies. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 226-236, </pages> <year> 1990. </year>
Reference-contexts: Because of the multiprocessor aspect, some anomalies can occasionally occur. In particular, RR will tend to give proportionately more processing time to jobs having a larger number of threads. RRJob avoids this by timeslicing equally among jobs as well as among the threads of a job <ref> [LV90] </ref>. 4.5.2 Static Run-To-Completion Scheduling The fundamental issue in static run-to-completion scheduling is choosing an appropriate number of processors to allocate a job. <p> Research in this area has typically assumed that only some job characteristics are known (at the very least the maximum parallelism), and in some cases, consider the situation in which the amount of work to be done is statistically correlated to the maximum parallelism <ref> [MEB88, LV90] </ref>. <p> This discipline varies its target partition size gradually in response to changing load [RSD + 94]. Our research shows that this discipline is not competitive against PWS or ASP, however. Other static RTC disciplines that have been proposed and are worth mentioning include A+&mM [Sev89] and AVG <ref> [LV90, CMV94] </ref>. 4.5.3 Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations. <p> Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand <ref> [LV90, CMV94, PS94] </ref>. When system loads increase, allocations to jobs decrease allowing them to operate at a more favourable point on their efficiency curve. Also, 13 because equipartition is effectively the analog of RR in uniprocessing, it is relatively insensitive to variability in service requirement. <p> Ousterhout proposed the first static quantum-based scheduling system, originally called co-scheduling but now more commonly known as gang scheduling, where sets of jobs are actively timesliced. Various methods have been studied for deciding how to most effectively timeslice jobs <ref> [Ous82, LV90, MVZ93] </ref>, but only recently has the issue of number of processors to allocate an arriving job been considered [PS94].
Reference: [MEB88] <author> S. Majumdar, D. L. Eager, and R. B. Bunt. </author> <title> Scheduling in multiprogrammed parallel systems. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 104-113, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: be suspended, either at the termination of a quantum or when an arrival or departure occurs, and then resumed repeatedly until its service requirement is satisfied. 4.5.1 Independent Scheduling The first independent multiprocessor scheduling strategies that were proposed were simple extensions of the uniprocessor FCFS, RR, SPT, and SRPT disciplines <ref> [MEB88] </ref>. In FCFS and RR, threads are ordered according to the time at which they were placed on the queue, while in SPT and SRPT, threads are ordered according to increasing cumulative service demand remaining for the job. <p> Research in this area has typically assumed that only some job characteristics are known (at the very least the maximum parallelism), and in some cases, consider the situation in which the amount of work to be done is statistically correlated to the maximum parallelism <ref> [MEB88, LV90] </ref>.
Reference: [ML92] <author> Evangelos P. Markatos and Thomas J. LeBlanc. </author> <title> Using processor affinity in loop scheduling on shared-memory multiprocessors. </title> <booktitle> In Proceedings Supercomputing '92, </booktitle> <pages> pages 104-113, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: Several techniques have been examined to minimize these problems, at least when the granularity of synchronization is relatively coarse. With cache-affinity scheduling, preference is given to threads that have previously run on a given processor in order to reuse previously-established cache contexts <ref> [SL93, TTG93, VZ91, ML92] </ref>. With spin-block locking, threads requesting a lock spin for a short duration, after which point they block and yield their processor to another thread [GTU91].
Reference: [MVZ93] <author> Cathy McCann, Raj Vaswani, and John Zahorjan. </author> <title> A dynamic processor allocation policy for multiprogrammed shared-memory multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 11(2) </volume> <pages> 146-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: In order to handle jobs with varying degrees of parallelism, dynamic partitioning uses process control both to allow the system to take into account changes in a job's parallelism and to allow the job to adapt to changes in processor allocation <ref> [ZM90, MVZ93] </ref>. Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand [LV90, CMV94, PS94]. When system loads increase, allocations to jobs decrease allowing them to operate at a more favourable point on their efficiency curve. <p> Ousterhout proposed the first static quantum-based scheduling system, originally called co-scheduling but now more commonly known as gang scheduling, where sets of jobs are actively timesliced. Various methods have been studied for deciding how to most effectively timeslice jobs <ref> [Ous82, LV90, MVZ93] </ref>, but only recently has the issue of number of processors to allocate an arriving job been considered [PS94].
Reference: [MZ94] <author> Cathy McCann and John Zahorjan. </author> <title> Processor allocation strategies for message-passing parallel computers. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 19-32, </pages> <year> 1994. </year>
Reference-contexts: Jobs typically wait in first-come first-served order and run-to-completion when activated. Research in scheduling for distributed-memory architectures, however, has focussed on dynamic partitioning, where processors are dynamically reallocated in response to the number of jobs in the system <ref> [MZ94, NSS93, DCDP90] </ref>. In work by McCann and Zahorjan, applications are assumed to be designed for a virtual machine that matches the characteristics of the physical machine, which in this case is a mesh. <p> As the system load increases, the threads of a job are multiplexed in such a way that the load on each processor remains evenly balanced <ref> [MZ94] </ref>. Through a rotation scheme, imbalances in processor allocation are evened out in the long run.
Reference: [NSS93] <author> Vijay K. Naik, Sanjeev K. Setia, and Mark S. Squillante. </author> <title> Performance analysis of job scheduling policies in parallel supercomputing environments. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 824-833, </pages> <year> 1993. </year>
Reference-contexts: Jobs typically wait in first-come first-served order and run-to-completion when activated. Research in scheduling for distributed-memory architectures, however, has focussed on dynamic partitioning, where processors are dynamically reallocated in response to the number of jobs in the system <ref> [MZ94, NSS93, DCDP90] </ref>. In work by McCann and Zahorjan, applications are assumed to be designed for a virtual machine that matches the characteristics of the physical machine, which in this case is a mesh. <p> Others have demonstrated that application redistribution, as required by equipartitioning, is also feasible at moderate cost, and that under high variability in service demand, can offer significantly better performance than a static partitioning scheme <ref> [NSS93, Set93] </ref>. But the high cost of redistribution makes it necessary to carefully choose parameters limiting the frequency of these events. It has also been noted that distributed-memory applications often have very low processor utilization due synchronous messaging, particularly when a job has a high degree of load imbalance.
Reference: [NT93] <author> Michael G. Norman and Peter Thanisch. </author> <title> Models of machines and computation for mapping in multicomputers. </title> <journal> ACM Computing Surveys, </journal> <volume> 25(3) </volume> <pages> 263-302, </pages> <month> September </month> <year> 1993. </year>
Reference-contexts: examples of where the makespan is relevant is in batch systems, where the goal is to service all jobs as quickly as possible [Cof76], and in mapping subtasks of a large parallel job to processors, where the goal is to execute the entire set of subtask as quickly as possible <ref> [NT93] </ref>. The objective in both cases is to minimize the measure concerned. The problem is that these measures are usually at odds with each other. Minimizing the mean response time (a "user-centric" objective) is obtained by, as we shall see, favouring as much as possible short running jobs. <p> The task mapping problem is to map the set of tasks corresponding to the job to a restricted number of processors in order to minimize the total execution time <ref> [NT93] </ref>.
Reference: [NW89] <author> Lionel M. Ni and Ching-Farn E. Wu. </author> <title> Design tradeoffs for process scheduling in shared memory multiprocessor systems. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> 15(3) </volume> <pages> 327-334, </pages> <month> March </month> <year> 1989. </year>
Reference-contexts: We first consider message-passing architectures, which have received the majority of attention, followed by NUMA architectures. It must be remarked that, for either class of distributed-memory machines, an important property of the design of a scheduling discipline is that it be able to scale well <ref> [FR90, ALL89, NW89] </ref>.
Reference: [Ous82] <author> John K. Ousterhout. </author> <title> Scheduling techniques for concurrent systems. </title> <booktitle> In Proceedings of the 3rd International Conference on Distributed Computing (ICDCS), </booktitle> <pages> pages 22-30, </pages> <month> October </month> <year> 1982. </year> <month> 24 </month>
Reference-contexts: Ousterhout proposed the first static quantum-based scheduling system, originally called co-scheduling but now more commonly known as gang scheduling, where sets of jobs are actively timesliced. Various methods have been studied for deciding how to most effectively timeslice jobs <ref> [Ous82, LV90, MVZ93] </ref>, but only recently has the issue of number of processors to allocate an arriving job been considered [PS94]. <p> As a result, we only explore the relationship between coordinated resource allocation (scheduling) and multidimensional packing without discussing any particular results. In its traditional form, gang scheduling uses a matrix to represent the time slices in which applications are active <ref> [Ous82] </ref>. The rows of the matrix represent different time slices, and the columns represent the P processors.
Reference: [PS94] <author> Eric W. Parsons and Kenneth C. Sevcik. </author> <title> Effects of high service-time variability in mul-tiprocessor scheduling. </title> <note> Submitted to SIGMETRICS/PERFORMANCE'95, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand <ref> [LV90, CMV94, PS94] </ref>. When system loads increase, allocations to jobs decrease allowing them to operate at a more favourable point on their efficiency curve. Also, 13 because equipartition is effectively the analog of RR in uniprocessing, it is relatively insensitive to variability in service requirement. <p> Various methods have been studied for deciding how to most effectively timeslice jobs [Ous82, LV90, MVZ93], but only recently has the issue of number of processors to allocate an arriving job been considered <ref> [PS94] </ref>. In particular, if a discipline does not reduce processor allocation as the load increases (as was the case up to our research), a system can easily saturate when the aggregate efficiency is not sufficient to keep up with the load. <p> In addition to the service time distribution, the performance of these disciplines can be highly dependent on the speedup exhibited by the applications. In particular, with sufficiently good speedup, an FB-like scheme can offer better performance than Equipartition given a hyperexponential service-time distribution. service times <ref> [PS94] </ref>. Summarizing the results as in the uniprocessor scheduling case, Table 2 indicates the disciplines that have been shown to perform well (as of today) for minimizing mean response time under various conditions. <p> Along the way, I plan to develop new results in workload models, multiprocessor scheduling, and coordinated multi-resource allocation. For one of these (multiprocessor scheduling), a contribution has already been made (in the form of a paper submission <ref> [PS94] </ref>) in which new static quantum-based scheduling disciplines are proposed and evaluated. One problem that became apparent in this research, however, is that we (as a multiprocessor scheduling community) still lack good workload models upon which we can base our results.
Reference: [PSN94] <author> Vinod G. J. Peris, Mark S. Squillante, and Vijay K. Naik. </author> <title> Analysis of the impact of memory in distributed parallel processing systems. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 5-18, </pages> <year> 1994. </year>
Reference-contexts: Recently, however, a model of the memory reference patterns of parallel applications has been proposed and used to estimate the paging overhead when an application is run in varying memory sizes <ref> [PSN94] </ref>. It is then shown that, for a distributed memory machine, it is possible to determine how many processors are necessary (assuming a fixed amount of memory per processor) to achieve acceptable performance. This is the first result that ties memory and processor requirements together.
Reference: [RSD + 94] <author> E. Rosti, E. Smirni, L. W. Dowdy, G. Serazzi, and B. M. Carlson. </author> <title> Robust partitioning policies of multiprocessor systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 141-165, </pages> <year> 1994. </year>
Reference-contexts: Our research shows that for a different kind of workload model, PWS usually performs better than ASP. Another discipline that takes only maximum parallelism into consideration is Adaptive Partitioning (AP). This discipline varies its target partition size gradually in response to changing load <ref> [RSD + 94] </ref>. Our research shows that this discipline is not competitive against PWS or ASP, however.
Reference: [Sch70] <author> Linus E. </author> <title> Schrage. Optimal scheduling rules for information systems. </title> <journal> Operations Research, </journal> <volume> 26, </volume> <month> August </month> <year> 1970. </year>
Reference-contexts: It has been shown that for some distributions (e.g., a 3-point distribution with high C d ), the performance of FB can be quite poor <ref> [Sch70] </ref>. For the hyperexponential class of distributions, as are most often used, FB is markedly better than RR. 4 | | | | | | | 0 | 6 | 12 | 18 Coefficient of Variation Mean Response Time FCFS SPT SRPT RR disciplines. meeting the processor constraints.
Reference: [Set93] <author> Sanjeev Setia. </author> <title> Scheduling on Multiprogrammed, Distributed Memory Parallel Computers. </title> <type> PhD thesis, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: Others have demonstrated that application redistribution, as required by equipartitioning, is also feasible at moderate cost, and that under high variability in service demand, can offer significantly better performance than a static partitioning scheme <ref> [NSS93, Set93] </ref>. But the high cost of redistribution makes it necessary to carefully choose parameters limiting the frequency of these events. It has also been noted that distributed-memory applications often have very low processor utilization due synchronous messaging, particularly when a job has a high degree of load imbalance.
Reference: [Sev89] <author> Kenneth C. Sevcik. </author> <title> Characterizations of parallelism in applications and their use in scheduling. </title> <booktitle> In Proceedings of the 1988 ACM SIGMETRICS International Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <month> May </month> <year> 1989. </year>
Reference-contexts: In absence of that information, the average parallelism offers a good alternative, as it gives similar performance guarantees as the value at the knee [EZL89]. It has been shown, however, that the overall workload volume should be taken into account in the scheduling decision <ref> [Sev89] </ref>. By reducing the number of processors allocated to jobs as the system load increases, the mean response time can be greatly improved because jobs operate at a better efficiency point. <p> This discipline varies its target partition size gradually in response to changing load [RSD + 94]. Our research shows that this discipline is not competitive against PWS or ASP, however. Other static RTC disciplines that have been proposed and are worth mentioning include A+&mM <ref> [Sev89] </ref> and AVG [LV90, CMV94]. 4.5.3 Dynamic Scheduling Most research in dynamic scheduling has been directed towards reducing or eliminating the cost of processor reallocations.
Reference: [Sev94] <author> K. C. Sevcik. </author> <title> Application scheduling and processor allocation in multiprogrammed parallel processing systems. Performance Evaluation, </title> <booktitle> 19 </booktitle> <pages> 107-140, </pages> <year> 1994. </year>
Reference-contexts: Second, it does not explicitly separate the various components affecting an application's performance, making it difficult to use for prediction. Sevcik proposes the following function, addressing some of the deficiencies of the Dowdy function <ref> [Sev94] </ref>: T (p) = OE (p) p where OE (p) corresponds to the multiplicative effect of load imbalance on the total amount of work W , ff corresponds to the per-processor overhead, and fi corresponds to contention. <p> All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases <ref> [Sev94] </ref>. First, consider the following extremes. If the speedup of every job is perfect, then any packing of jobs that does not waste any space will give the same makespan; the area of the rectangle representing a job will always be the same regardless of the number of processors allocated.
Reference: [SG91] <author> X-H. Sun and J. L. Gustafson. </author> <title> Towards a better parallel performance metric. </title> <journal> Parallel Computing, </journal> <volume> 17 </volume> <pages> 1093-1109, </pages> <year> 1991. </year>
Reference-contexts: Rather, increasing the number of processors is usually accompanied by an increase in problem size (either data or total computation). Since this observation was made, a large number of scalability metrics have been proposed and examined, including speedup [Amd67], scaled speedup [Gus88], sizeup <ref> [SG91] </ref>, and isoefficiency [GGK93].
Reference: [SHG93] <author> Jaswinder Pal Singh, John L. Hennessy, and Anoop Gupta. </author> <title> Scaling parallel programs for multiprocessors: Methodology and examples. </title> <journal> Computer, </journal> <volume> 26(7) </volume> <pages> 42-50, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: For instance, it has been shown that for a Barnes-Hut algorithm simulating the evolution of galaxies, an increase in the number of objects (to improve the accuracy of the results) requires an increase in the number of time steps over the same time interval <ref> [SHG93] </ref>. This illustrates the fact that any given scientific application may not necessary scale as well as traditional scalability analysis suggests. Scalability Measurements Another way of studying the scalability of applications is to examine their behaviour on a real system.
Reference: [SL93] <author> Mark S. Squillante and Edward D. Lazowska. </author> <title> Using processor-cache affinity information in shared-memory multiprocessor scheduling. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 4(2) </volume> <pages> 131-143, </pages> <month> February </month> <year> 1993. </year>
Reference-contexts: Several techniques have been examined to minimize these problems, at least when the granularity of synchronization is relatively coarse. With cache-affinity scheduling, preference is given to threads that have previously run on a given processor in order to reuse previously-established cache contexts <ref> [SL93, TTG93, VZ91, ML92] </ref>. With spin-block locking, threads requesting a lock spin for a short duration, after which point they block and yield their processor to another thread [GTU91].
Reference: [Sle80] <author> D. Sleator. </author> <title> A 2.5 times optimal algorithm for packing in two dimensions. </title> <journal> Information Processing Letters, </journal> <volume> 10(1) </volume> <pages> 37-40, </pages> <month> February </month> <year> 1980. </year>
Reference-contexts: It was later shown how scheduling on a line could be achieved within a factor of 2:5 times optimal <ref> [Sle80] </ref>. Scheduling on a line is the same as packing rectangles of size (p i ; t i ) in a strip of width P . Thus, minimizing the length of the strip needed is equivalent to finding the minimum makespan.
Reference: [SSRV94] <author> Anand Sivasubramanian, Aman Singla, Umakishore Ramachandran, and H. Venkateswaran. </author> <title> An approach to scalability study of shared memory parallel systems. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 171-180, </pages> <year> 1994. </year>
Reference-contexts: Preliminary results are encouraging, but it is not yet clear how widely applicable the approach will be. Another project similar to this one, but based on execution-driven simulations rather than actual executions, is taking place at Georgia Tech <ref> [SSRV94] </ref>. 5.2 Multi-resource Scheduling One of the major thrusts of the Tornado project is to support coordinated allocation of resources to applications. Quite simply, the operating system will treat the multiple resource requirements of applications, which to begin with will be processors, I/O, and memory, in a unified manner.
Reference: [SST93] <author> Sanjeev K. Setia, Mark S. Squillante, and Satish K. Tripathi. </author> <title> Processor scheduling on multiprogrammed, distributed memory parallel computers. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 158-170, </pages> <year> 1993. </year>
Reference-contexts: If this is indeed true, timesharing between several applications within a given partition can also become desirable (going against the general view that space-sharing is better than timesharing) <ref> [SST93] </ref>. 4.6.2 NUMA Architectures Many of the same issues for message-passing architectures apply to NUMA (shared-memory) architectures. The placement of the threads of a job is just as important and the possibility of changes in processor allocation requires equal consideration.
Reference: [ST93] <author> Sanjeev Setia and Satish Tripathi. </author> <title> A comparative analysis of static processor partitioning policies for parallel computers. </title> <booktitle> In Proceedings of the International Workshop on Modeling and Simulation of Computer and Telecommunication Systems (MASCOTS), </booktitle> <pages> pages 283-286, </pages> <month> January </month> <year> 1993. </year> <month> 25 </month>
Reference-contexts: A discipline that assumes (more realistically) that only the maximum parallelism is known is Adaptive Static Partitioning (ASP) <ref> [ST93] </ref>. In this discipline, an arriving job is allocated the lesser of the number of free processors and its maximum parallelism. When a job leaves, the pending jobs are all allocated an equal fraction of the freed processors.
Reference: [TG89] <author> Andrew Tucker and Anoop Gupta. </author> <title> Process control and scheduling issues for multipro--grammed shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 159-166, </pages> <year> 1989. </year>
Reference-contexts: In the process control approach, jobs and the operating system cooperate in processor reallocations, allowing a job to dynamically match its degree of parallelism to the current allocation <ref> [TG89, GTS91] </ref>. Using this approach, the problems of losing cache context and the blocking critical threads are avoided altogether. Two important disciplines that have been studied in the context of process control are equipar-tition and dynamic partitioning.
Reference: [TSWY94] <author> John Turek, Uwe Schwiegelshohn, Joel L. Wolf, and Philip S. Yu. </author> <title> Scheduling parallel tasks to minimize average response time. </title> <booktitle> In Proceedings of the Fifth Annual ACM-SIAM Symposium on Discrete Algorithms (SODA), </booktitle> <pages> pages 112-121, </pages> <year> 1994. </year>
Reference-contexts: Minimum response time is not quite as natural, and it is only very recently that it has been examined from a theoretical perspective <ref> [TSWY94] </ref>. (Of course, it is even more questionable whether the worst-case behaviour is relevant for mean response time as opposed to average-case behaviour). 4 A parallel random access machine (PRAM) is a theoretical model of a ideal parallel machine having infinite global memory and identical processors. 9 Shelf 1 Shelf 4 <p> A number of techniques exist to improve the solution from this point, including dropping jobs from a higher shelf to a lower one and combining shelves when possible <ref> [TSWY94] </ref>. In the original paper, it was proven that the average response time must be within a bound of 32 of the minimum, which is acknowledged to not be that encouraging. In private communication since then, the authors claim that the bound is now down to a factor of 8. <p> Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [TTG93] <author> Josep Torrellas, Andrew Tucker, and Anoop Gupta. </author> <title> Benefits of cache-affinity scheduling in shared-memory multiprocessors: A summary. </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 272-274, </pages> <year> 1993. </year>
Reference-contexts: Several techniques have been examined to minimize these problems, at least when the granularity of synchronization is relatively coarse. With cache-affinity scheduling, preference is given to threads that have previously run on a given processor in order to reuse previously-established cache contexts <ref> [SL93, TTG93, VZ91, ML92] </ref>. With spin-block locking, threads requesting a lock spin for a short duration, after which point they block and yield their processor to another thread [GTU91].
Reference: [TWPY92] <author> J. Turek, J. Wolf, K. Pattipati, and P. Yu. </author> <title> Scheduling parallelizable tasks: Putting it all on the shelf. </title> <booktitle> In Proceedings of the 1992 ACM SIGMETRICS and PERFORMANCE '92 International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 225-236, </pages> <year> 1992. </year>
Reference-contexts: Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [TWY92] <author> J. Turek, J. Wolf, and P. Yu. </author> <title> Approximate algorithms for scheduling parallelizable tasks. </title> <booktitle> In 4th Annual ACM Symposium on Parallel Algorithms and Architectures (SPAA '92), </booktitle> <pages> pages 323-332, </pages> <year> 1992. </year>
Reference-contexts: Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [VZ91] <author> Raj Vaswani and John Zahorjan. </author> <title> The implications of cache affinity on processor scheduling for multiprogrammed, shared memory multiprocessors. </title> <booktitle> In Proceedings of the Thirteenth Symposium on Operating System Principles (SOSP), </booktitle> <pages> pages 26-40, </pages> <year> 1991. </year>
Reference-contexts: Several techniques have been examined to minimize these problems, at least when the granularity of synchronization is relatively coarse. With cache-affinity scheduling, preference is given to threads that have previously run on a given processor in order to reuse previously-established cache contexts <ref> [SL93, TTG93, VZ91, ML92] </ref>. With spin-block locking, threads requesting a lock spin for a short duration, after which point they block and yield their processor to another thread [GTU91].
Reference: [WTCY94] <author> Joel L. Wolf, John Turek, Ming-Syan Chen, and Philip S. Yu. </author> <title> Scheduling multiple queries on a parallel machine. </title> <booktitle> In Proceedings of the 1994 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 45-55, </pages> <year> 1994. </year>
Reference-contexts: Recently, attention in this area has turned to malleable scheduling, where the execution time of an application is determined by the number of processors it is allocated <ref> [BB90, TWPY92, TWY92, LT94, TSWY94, WTCY94] </ref>. All work is for optimizing the makespan; no work has yet been done for mean response time in the malleable case, except for some very simple cases [Sev94]. First, consider the following extremes.
Reference: [Wu93] <author> Chee-Shong Wu. </author> <title> Processor scheduling in multiprogrammed shared memory NUMA multiprocessors. </title> <type> Master's thesis, </type> <institution> University of Toronto, </institution> <year> 1993. </year>
Reference-contexts: To simplify matters, the value of OE (p) is assumed to approach a constant value for large p. Although it has been shown that values of OE, ff, and fi can be chosen so that Sevcik's function approximates real execution times much better than the Dowdy function <ref> [Wu93] </ref>, these values do not necessarily reflect the factors they are supposed to represent.
Reference: [ZM90] <author> John Zahorjan and Cathy McCann. </author> <title> Processor scheduling in shared memory multiprocessors. </title> <booktitle> In Proceedings of the 1990 ACM SIGMETRICS Conference on Measurement and Modelling of Computer Systems, </booktitle> <pages> pages 214-225, </pages> <year> 1990. </year> <month> 26 </month>
Reference-contexts: Two important disciplines that have been studied in the context of process control are equipar-tition and dynamic partitioning. In equipartition, each job is allocated an equal fraction of the processors, up to their maximum parallelism <ref> [ZM90] </ref>. This assumes that all jobs have a degree of parallelism that is constant throughout their lifetime. <p> In order to handle jobs with varying degrees of parallelism, dynamic partitioning uses process control both to allow the system to take into account changes in a job's parallelism and to allow the job to adapt to changes in processor allocation <ref> [ZM90, MVZ93] </ref>. Equipartition has been shown to be effective over a wide range of workloads and a wide range of distributions in service demand [LV90, CMV94, PS94]. When system loads increase, allocations to jobs decrease allowing them to operate at a more favourable point on their efficiency curve.
References-found: 61

