URL: http://www-eksl.cs.umass.edu/papers/firoiu-FIC98.ps
Refering-URL: http://www-eksl.cs.umass.edu/iil/iil-papers.html
Root-URL: 
Email: (lfiroiu@cs.umass.edu)  (oates@cs.umass.edu)  (cohen@cs.umass.edu)  
Title: Learning a deterministic finite automaton with a recurrent neural network  
Author: Laura Firoiu Tim Oates Paul R. Cohen 
Address: Box 34610 Amherst, MA 01003-4610  
Affiliation: Computer Science Department, LGRC, University of Massachusetts,  
Abstract: We consider the problem of learning a finite automaton with recurrent neural networks, given a training set of sentences in a language. We train Elman recurrent neural networks on the prediction task and study experimentally what these networks learn. We found that the network tends to encode an approximation of the minimum automaton that accepts only the sentences in the training set.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Cleeremans, D. Servan-Schreiber, and J.L. McClelland. </author> <title> Finite state automata and simple recurrent networks. </title> <booktitle> Neural Computation, </booktitle> <address> 1:372381, </address> <year> 1989. </year>
Reference: [2] <author> P. R. Cohen. </author> <title> Empirical Methods for Artificial Intelligence. </title> <publisher> The MIT Press, </publisher> <year> 1995. </year>
Reference-contexts: This criterion tests for the similarity of the distributions over the state classes that follow and precede the two classes. The initial state classes are formed by the identical network states. We use a G statistic, which has a 2 distribution (see <ref> [2] </ref>) to test if there is a statistically significant difference in the two probability distributions.
Reference: [3] <author> J. L. Elman. </author> <title> Distributed representations, simple recurrent networks, and grammatical structure. </title> <booktitle> Machine Learning, </booktitle> <year> 1992. </year>
Reference-contexts: Due to this property, different network architectures, for example second order networks (with multiplicative units) have been used for the task of DFA induction. For output-less automata, the network can be trained either with the word following the current input (the prediction task) as in <ref> [3] </ref> , or if both positive and negative examples are present, with a target encoding the membership in the language of the current string as in [4] . For automata with output, the natural choice of the target is the output symbol of the current transition. As in [1]and [3] we <p> in <ref> [3] </ref> , or if both positive and negative examples are present, with a target encoding the membership in the language of the current string as in [4] . For automata with output, the natural choice of the target is the output symbol of the current transition. As in [1]and [3] we use an Elman recurrent network trained on the prediction task to induce a DFA , and are interested in understanding what automaton is learned by the network. In section 2 we describe in more detail the setting of the learning task, the experiments and results. <p> set and conclude that this network architecture and training regime are biased towards the extreme case where the training set is the entire language. 2 Experiments 2.1 The grammar and the languages We wrote a small context free grammar (CFG), that generates natural-sounding sentences, similar with the one used in <ref> [3] </ref> . Some of the constraints imposed syntactically by this grammar are subject-verb and noun phrase-relative clause number agreement and transitive/intransitive verb distinctions. From this CFG we obtained a regular grammar by expanding the start symbol with all possible productions, up to an arbitrary depth in the derivation trees.
Reference: [4] <author> C. L. Giles, C. B. Miller, D. Chen, G. Z. Sun, H. H. Chen, and Y. C. Lee. </author> <title> Extracting and learning an unknown grammar with recurrent neural networks. </title> <booktitle> In Advances in Neural Information Processing Systems 4. </booktitle> <year> 1992. </year>
Reference-contexts: For output-less automata, the network can be trained either with the word following the current input (the prediction task) as in [3] , or if both positive and negative examples are present, with a target encoding the membership in the language of the current string as in <ref> [4] </ref> . For automata with output, the natural choice of the target is the output symbol of the current transition. <p> The weight update equations for this architecture and training regime are given in appendix A. 2.3 DFA extraction One method used so far for extracting a DFA from a RNN , for example see <ref> [4] </ref> , is to assume that the network states whose values are close in R n form well separated clusters that represent the automaton states.
Reference: [5] <author> E. M. Gold. </author> <title> Language identification in the limit. </title> <journal> Information and control, </journal> <volume> 10:447474, </volume> <year> 1967. </year>
Reference: [6] <author> E. M. Gold. </author> <title> Complexity of automaton identification from given data. </title> <journal> Information and control, </journal> <volume> 37:302420, </volume> <year> 1978. </year>
Reference: [7] <author> Michael Kearns and Leslie Valiant. </author> <title> Cryptographic limitations on learning boolean formulae and finite automata. </title> <journal> Journal of the ACM, </journal> <volume> 41(1):6795, </volume> <year> 1994. </year>
Reference: [8] <author> John F. Kolen. </author> <title> Fool's gold: Extracting finite state machines from recurrent network dynamics. </title> <booktitle> In Advances in Neural Information Processing Systems 6, </booktitle> <year> 1994. </year>
Reference-contexts: Because the network state is set to 0 at each beginning of sentence, this additional input unit provides the initial context. The (approximately) periodic reset and the short sentences help avoid the state instability mentioned by Kolen in <ref> [8] </ref> . Thus, we avoid the solution proposed by Giles in [10] , that relies on large weights and biases, because we noticed that the network learns better when it starts with small weights.
Reference: [9] <author> E. Makinen. </author> <title> Inferring regular languages by merging nonterminals. </title> <type> Technical Report A-1997-6, </type> <institution> Department of Computer Science, University of Tampere, </institution> <year> 1997. </year>
Reference-contexts: The language alphabet becomes the set of terminal (word) instances that occur between two states of its automaton. The language with the set of symbols thus modified becomes a Szilard language of a regular grammar. As described in <ref> [9] </ref> , this kind of languages has a polynomial inference algorithm that induces the 4 grammar from a set of positive examples. Obviously, any regular language can be Szilard-ified in this manner.
Reference: [10] <author> Christian W. Omlin and C. Lee Giles. </author> <title> Constructing deterministic finite-state automata in recurrent neural networks. </title> <type> Technical report, </type> <institution> Computer Science Department, Rensselaer Polytechnic Institute, </institution> <year> 1994. </year>
Reference-contexts: Because the network state is set to 0 at each beginning of sentence, this additional input unit provides the initial context. The (approximately) periodic reset and the short sentences help avoid the state instability mentioned by Kolen in [8] . Thus, we avoid the solution proposed by Giles in <ref> [10] </ref> , that relies on large weights and biases, because we noticed that the network learns better when it starts with small weights. The network is trained for the prediction task: the word following the input word in the current sentence represents the target output.
Reference: [11] <author> Leonard Pitt and Manfred K. Warmuth. </author> <title> The minimum consistent dfa problem cannot be approximated within any polynomial. </title> <journal> Journal of the ACM, </journal> <volume> 40(1):95142, </volume> <year> 1993. </year>
Reference: [12] <author> D. E. Rumelhart, R. Durbin, R. Golden, and Y. Chauvin. </author> <title> Backpropagation: The basic theory. In Backpropagation: Theory, architectures, and applications. </title> <publisher> Erlbaum, </publisher> <year> 1993. </year>
Reference-contexts: There is also one output unit for each word in the alphabet, and a special marker end of sentence. The cost to be optimized is the cross-entropy error function. As described in <ref> [12] </ref> , the cross-entropy function forces the network to learn the probability distribution over the next words, conditioned on the input symbol and the network: P (x (t + 1) j N; x (t)). The network weights are updated on-line, with the backpropagation through time algorithm. <p> As in <ref> [12] </ref> , the cost function for a set of T input-output vector pairs fhx (t); d (t)i; 1 t T g is: C = t=1 T Y P ( ~ d (t) j ~x (t); N (t)).
Reference: [13] <author> H. </author> <title> T Siegelmann. Theoretical Foundations of Recurrent Neural Networks. </title> <type> PhD thesis, </type> <institution> Rutgers, </institution> <year> 1992. </year>
Reference-contexts: Recurrent neural networks and deterministic finite automata have similar behaviors. They are both state devices and their state functions have the same form : state (t) = f (state (t 1); input symbol (t)). It has been shown (see <ref> [13] </ref> ) that there is an immediate encoding of a DFA with n states and m input symbols into a simple recurrent neural network with m fi n state units. Conversely, a DFA can be easily extracted from such a RNN .
Reference: [14] <author> P. N. Werbos. </author> <title> The roots of backpropagation. </title> <publisher> John Wiley & Sons, Inc., </publisher> <year> 1994. </year> <month> 11 </month>
Reference-contexts: The activation function of the hidden and output nodes is the sigmoid (x) = 1=(1 + exp (x)). The network is trained with the backpropagation through time algorithm (see <ref> [14] </ref> ) and with the cross-entropy error function. All sentences in the training set are presented to the network, one word at each time step. There is one input unit for each word of the alphabet. An extra symbol, start of sentence, marks the beginning of each sentence. <p> The weight changes are : / T X @C t , with 2 ffl i;j ; ff l;i ; fi k;i g. By applying the chain rule as in <ref> [14] </ref> , the weight changes at a time step t are: * The weights from hidden to output units: @C t @fl i;j * Let inf l h i (t) = j fl i;j [d j (t) o j (t)].
References-found: 14

