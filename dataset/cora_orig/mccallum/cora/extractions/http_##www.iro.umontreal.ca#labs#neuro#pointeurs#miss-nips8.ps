URL: http://www.iro.umontreal.ca/labs/neuro/pointeurs/miss-nips8.ps
Refering-URL: http://www.cs.cmu.edu/Web/Groups/NIPS/NIPS95/Papers.html
Root-URL: 
Email: bengioy@iro.umontreal.ca  gingras@iro.umontreal.ca  
Title: Recurrent Neural Networks for Missing or Asynchronous Data  
Author: Yoshua Bengio Francois Gingras 
Address: Montreal, Qc H3C-3J7  Montreal, Qc H3C-3J7  
Affiliation: Dept. Informatique et Recherche Operationnelle Universite de Montreal  Dept. Informatique et Recherche Operationnelle Universite de Montreal  
Abstract: In this paper we propose recurrent neural networks with feedback into the input units for handling two types of data analysis problems. On the one hand, this scheme can be used for static data when some of the input variables are missing. On the other hand, it can also be used for sequential data, when some of the input variables are missing or are available at different frequencies. Unlike in the case of probabilistic models (e.g. Gaussian) of the missing variables, the network does not attempt to model the distribution of the missing variables given the observed variables. Instead it is a more "discriminant" approach that fills in the missing variables for the sole purpose of minimizing a learning criterion (e.g., to minimize an output error).
Abstract-found: 1
Intro-found: 1
Reference: <author> Ahmad, S. and Tresp, V. </author> <year> (1993). </year> <title> Some solutions to the missing feature problem in vision. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <address> San Mateo, CA. </address> <publisher> Morgan Kaufman Publishers. </publisher>
Reference: <author> Almeida, L. </author> <year> (1987). </year> <title> A learning rule for asynchronous perceptrons with feedback in a combinatorial environment. </title> <editor> In Caudill, M. and Butler, C., editors, </editor> <booktitle> IEEE International Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 609-618, </pages> <address> San Diego 1987. </address> <publisher> IEEE, </publisher> <address> New York. </address>
Reference: <author> Bareiss, E. and Porter, B. </author> <year> (1987). </year> <title> Protos: An exemplar-based learning apprentice. </title> <booktitle> In Proceedings of the 4th International Workshop on Machine Learning, </booktitle> <pages> pages 12-23, </pages> <address> Irvine, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This database was made public thanks to Jergen and Quinlan, was used by <ref> (Bareiss and Porter, 1987) </ref>, and was obtained from the UCI Repository of machine learning databases (ftp.ics.uci.edu:pub/machine-learning-databases). The original database has 226 patterns, with 69 attributes, and 24 classes. Unfortunately, most of the classes have only 1 exemplar. Hence we decided to cluster the classes into four groups.
Reference: <author> Bottou, L. and Gallinari, P. </author> <year> (1991). </year> <title> A framework for the cooperation of learning algorithms. </title> <editor> In Lippman, R. P., Moody, R., and Touretzky, D. S., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 781-788, </pages> <address> Denver, </address> <publisher> CO. </publisher>
Reference-contexts: The parameter fl was fixed by hand. In the experiments described below, a value of 0.7 was used, but near values yielded similar results. This module can therefore be combined within a hybrid system composed of several modules by propagating gradient through the combined system (as in <ref> (Bottou and Gallinari, 1991) </ref>). For example, as in Figure 2, there might be another module taking as input the recurrent network's output.
Reference: <author> Ghahramani, Z. and Jordan, M. I. </author> <year> (1994). </year> <title> Supervised learning from incomplete data via an EM approach. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <address> page , San Mateo, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In the experiments presented here, the proposed recurrent network is compared with a Gaussian mixture model trained with EM to handle missing values <ref> (Ghahramani and Jordan, 1994) </ref>. The approach proposed in section 2 is more economical than the traditional Gaussian-based approaches for two reasons. Firstly, we take advantage of hidden units in a recurrent network, which might be less numerous than the inputs. <p> The recurrent network was also compared with an approach based on a Gaussian and Gaussian mixture model of the data. We used the algorithm described in <ref> (Ghahramani and Jordan, 1994) </ref> for supervised leaning from incomplete data with the EM algorithm. <p> Dotted arrows show the backward flow of gradients. the feedforward networks (90-15-4): average classification error w.r.t. training epoch, (with 1 standard deviation error bars, computed over 10 trials). and covariance matrix for component j. Maximum likelihood training is applied as explained in <ref> (Ghahramani and Jordan, 1994) </ref>, taking missing values into account (as additional missing variables of the EM algorithm). For each architecture in Table 1, 10 training trials were run with a different subset of 200 training and 26 test patterns (and different initial weights for the neural networks).
Reference: <author> Lang, K. J., Waibel, A. H., and Hinton, G. E. </author> <year> (1990). </year> <title> A time-delay neural network architecture for isolated word recognition. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 23-43. </pages>
Reference-contexts: The recognition recurrent network is shown on the right of Figure 1. It has multiple time scales (implemented with subsampling and oversampling, as in TDNNs <ref> (Lang, Waibel and Hinton, 1990) </ref> and reverse-TDNNs (Simard and LeCun, 1992)), to facilitate the learning of such asynchronous data.
Reference: <author> Pineda, F. </author> <year> (1989). </year> <title> Recurrent back-propagation and the dynamical approach to adaptive neural computation. </title> <journal> Neural Computation, </journal> <volume> 1 </volume> <pages> 161-172. </pages>
Reference: <author> Simard, P. and LeCun, Y. </author> <year> (1992). </year> <title> Reverse TDNN: An architecture for trajectory generation. </title> <editor> In Moody, J., Hanson, S., and Lipmann, R., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 579-588, </pages> <address> Denver, </address> <publisher> CO. Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: The recognition recurrent network is shown on the right of Figure 1. It has multiple time scales (implemented with subsampling and oversampling, as in TDNNs (Lang, Waibel and Hinton, 1990) and reverse-TDNNs <ref> (Simard and LeCun, 1992) </ref>), to facilitate the learning of such asynchronous data. The static network is a time-delay neural network with 6 input, 8 hidden, and 1 output unit, and connections with delays 0, 2, and 4 from the input to hidden and hidden to output units.
Reference: <author> Tresp, V., Ahmad, S., and Neuneier, R. </author> <year> (1994). </year> <title> Training neural networks with deficient data. </title> <editor> In Cowan, J., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 128-135. </pages> <publisher> Morgan Kaufman Publishers, </publisher> <address> San Mateo, CA. </address>
References-found: 9

