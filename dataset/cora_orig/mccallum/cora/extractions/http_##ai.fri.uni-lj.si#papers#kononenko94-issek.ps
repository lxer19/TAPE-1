URL: http://ai.fri.uni-lj.si/papers/kononenko94-issek.ps
Refering-URL: http://ai.fri.uni-lj.si/papers/index.html
Root-URL: 
Email: e-mail: igor.kononenko@ninurta.fer.uni-lj.si  
Phone: tel: +386-61-1768390; fax: +386-61-264990  
Title: Induction of decision trees using RELIEFF  
Author: Igor Kononenko, Edvard Simec 
Keyword: machine learning, greedy search, impurity function, RELIEFF  
Address: Trzaska 25, SI-61001 Ljubljana, Slovenia  
Affiliation: University of Ljubljana, Faculty of electrical engineering computer science,  
Abstract: In the context of machine learning from examples this paper deals with the problem of estimating the quality of attributes with and without dependencies between them. Greedy search prevents current inductive machine learning algorithms to detect significant dependencies between the attributes. Recently, Kira and Rendell developed the RELIEF algorithm for estimating the quality of attributes that is able to detect dependencies between attributes. We show strong relation between RELIEF's estimates and impurity functions, that are usually used for heuristic guidance of inductive learning algorithms. We propose to use RELIEFF, an extended version of RELIEF, instead of myopic impurity functions. We have reimplemented Assistant, a system for top down induction of decision trees, using RELIEFF as an estimator of attributes at each selection step. The algorithm is tested on several artificial and several real world problems. Results show the advantage of the presented approach to inductive learning and open a wide rang of possibilities for using RELIEFF. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Breiman L., Friedman J.H., Olshen R.A. & Stone C.J. </author> <title> (1984) Classification and Regression Trees, </title> <publisher> Wadsworth International Group. </publisher>
Reference-contexts: The heuristic functions that estimate the potential successors of the current state in the search space play a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio (Quinlan, 1986), gini-index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), weight of evidence (Michie & Al Attar, 1992), and j-measure (Smyth & Goodman, 1990). <p> we obtain: W [A] = P eqval fi Gini 0 (A) P samecl (1 P samecl ) = constant fi X P (V ) 2 fi Gini 0 (A) (4) 4 where Gini 0 (A) = X P X P (CjV ) 2 C is highly correlated with the gini-index <ref> (Breiman et al., 1984) </ref> for classes C and values V of attribute A.
Reference: <author> Cestnik B. </author> <title> (1990) Estimating probabilities: A crucial task in machine learning, </title> <booktitle> Proc. European Conference on Artificial Intelligence 90, </booktitle> <address> Stockholm, </address> <month> August </month> <year> 1990, </year> <month> pp.147-149. </month>
Reference-contexts: Experiments on a series of artificial and real-world data sets are described and the results obtained using RELIEFF as a selection criterion are compared to results obtained by using information gain instead of RELIEFF and to the results of the naive Bayesian classifier that uses the m-estimate of probabilities <ref> (Cestnik, 1990) </ref>.
Reference: <author> Cestnik B. & Bratko I. </author> <title> (1991) On estimating probabilities in tree pruning, </title> <booktitle> Proc. European Working Session on Learning, (Porto, </booktitle> <month> March </month> <year> 1991), </year> <editor> Y.Kodratoff (ed.), </editor> <publisher> Springer Verlag. pp.138-150. </publisher>
Reference: <author> Cestnik, B., Kononenko, I. & Bratko, I. </author> <title> (1987) ASSISTANT 86 : A knowledge elicitation tool for sophisticated users. </title> <editor> In: I. Bratko & N. Lavrac (eds.), </editor> <booktitle> Progress in Machine Learning. </booktitle> <address> Wilmslow, England: </address> <publisher> Sigma Press. </publisher>
Reference-contexts: RELIEFF seems to be a promising heuristic function that may overcome the myopia of current inductive learning algorithms. It is general, efficient, and reliable enough to guide the search in the learning process. In this paper a reimplementation of Assistant learning algorithm for top down induction of decision trees <ref> (Cestnik et al., 1987) </ref> is described, named Assistant-R. Instead of information gain, Assistant-R uses RELIEFF as a heuristic function for estimating the attributes quality at each step during the tree generation. <p> Impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio (Quinlan, 1986) and binarization of attributes <ref> (Cestnik et al., 1987) </ref>). Equation (3) shows that RELIEF implicitly uses such a normalization. In the above derivation we eliminated the "nearest instance" condition from the probabilities. If we put it back we can interpret RELIEF's estimates as the average over local estimates in smaller parts of the instance space. <p> Note that the complexity of RELIEFF is O (N 2 fi #all attributes), where N is the number of training instances. 3 Induction of decision trees with Assistant-R Assistant-R is a reimplementation of the Assistant learning system for top down induction of decision trees <ref> (Cestnik et al., 1987) </ref>. The basic algorithm goes back to CLS (Concept 6 Learning System) developed by Hunt et al. (1966) and reimplemented by several authors (see (Quinlan, 1986) for an overview).
Reference: <editor> Dietterich T.G. & Shavlik J.W. (eds.) </editor> <booktitle> (1990) Readings in machine learning, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is arguable whether learning of neural networks and statistical classification tehniques can be put under the Machine Learning umbrella. For a collection of relevant papers that illustrate the development of the field see <ref> (Dietterich & Shavlik, 1990) </ref>. An extensive bibliography of the field can be found in (Michalski & Tecuci, 1994). In this contribution we are concerend with inductive learning from examples. 1 Typical inductive learning algorithm uses a greedy search strategy to overcome the combinatorial explosion during the search for good hypotheses.
Reference: <author> Dolsak, B. & Muggleton, S. </author> <title> (1992) The application of inductive logic programming to finite element mesh design. </title> <editor> In: Muggleton S. (ed.) </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference-contexts: IRIS: The well known Fisher's problem of determining the type of iris flower. MESH3,MESH15: The problem of determining the number of elements for each of the edges of an object in the finite element mesh design problem <ref> (Dolsak & Muggleton, 1992) </ref>. There are five objects for which experts have constructed appropriate meshes. In each of five experiments one object is used for testing and the other four for learning and the results are averaged.
Reference: <author> Dzeroski S. </author> <title> (1991) Handling noise in inductive logic programming, M.SC. </title> <type> Thesis, </type> <institution> University of Ljubljana, Faculty of electrical engineering & computer science, Ljubljana, Slovenia. </institution>
Reference-contexts: The attributes describe the relevant relations between pieces, such as "same rank" and "adjacent file". Originally the data included five sets of 1000 examples (1000 for learning and 4000 for testing) and was used to test Inductive Logic Programming algorithms <ref> (Dzeroski, 1991) </ref>. The reported classification accuracy is 99.70.1 %. We used only one set of 1000 examples (i.e. 700 instances for training). KRK2: Same as KRK1 except that the only available attributes are the coordinates of pieces. The same data set was used by Mladenic (1993).
Reference: <author> Fayyad U.M. </author> <title> (1991) On the induction of decision trees for multiple concept learning. </title> <type> PhD dissertation, </type> <institution> The University of Michigan. </institution>
Reference: <author> Fayyad U.M. </author> & <title> Irani K.B. (1992) The attribute selection problem in decision tree generation. </title> <booktitle> Proc. AAAI-92, </booktitle> <address> July 1992, San Jose, CA. </address> <publisher> MIT Press. </publisher>
Reference: <author> Hunt E., Martin J & Stone P. </author> <title> (1966) Experiments in Induction, </title> <address> New York, </address> <publisher> Academic Press. </publisher>
Reference: <author> Kira K. & Rendell L. </author> <title> (1992a) A practical approach to feature selection, </title> <booktitle> Proc. Intern. Conf. on Machine Learning (Aberdeen, </booktitle> <address> July 1992) D.Sleeman & P.Edwards (eds.), </address> <publisher> Morgan Kaufmann, pp.249-256. </publisher>
Reference: <author> Kira K. & Rendell L. </author> <title> (1992b) The feature selection problem: traditional methods and new algorithm. </title> <booktitle> Proc. AAAI'92, </booktitle> <address> San Jose, CA, </address> <month> July </month> <year> 1992. </year>
Reference: <author> Kononenko I. </author> <title> (1991) Semi-naive Bayesian classifier, </title> <booktitle> Proc. European Working Session on Learning, </booktitle> <editor> Y.Kodratoff (ed.), </editor> <publisher> Springer Verlag, </publisher> <pages> pp. 206-219. </pages>
Reference-contexts: The only exception from the above methodology were the experiments in the finite element mesh design problem, where the experimental methodology was dictated by previous published results, as described in subsection 5.4. 9 Besides the classification accuracy, we measured also the average information score <ref> (Kononenko & Bratko, 1991) </ref>. This measure eliminates the influence of prior probabilities and appropriately treats probabilistic answers of a classifier. <p> Although RELIEFF may overcome the myopia, it is useless in Assistant-R when a 16 representation change is required. In such cases, constructive induction should be ap-plied. For example, in the KRK2 problem, Assistant-R achieves good result, which can not be further improved without constructive induction. Semi-naive Bayesian classifier <ref> (Kononenko, 1991) </ref> achied 86% classification accuracy. This algorithm explicitly searches for dependencies between attributes and costructs a new attribute when significant dependency is discovered. Such learning, when new attributes are constructed from existing ones, is called constructive induction.
Reference: <author> Kononenko I. </author> <title> (1993) Inductive and Bayesian learning in medical diagnosis. </title> <journal> Applied Artificial Intelligence, </journal> <volume> 7 </volume> <pages> 317-337. </pages>
Reference-contexts: The results of experiments on these data sets are provided in tables 5 and 6. In medical data sets, attributes are typically independent. Therefore, it is not surprising that the naive Bayesian classifier shows clear advantage on these data sets <ref> (Kononenko, 1993) </ref>. The information score (table 6) for BREA data set indicates that no learning algorithm was able to solve this problem. This suggests that the attributes are not relevant.
Reference: <author> Kononenko I. </author> <title> (1994) Estimating attributes: Analysis and extensions of RELIEF. </title> <booktitle> Proc. Eu-ropean Conf. on Machine Learning (Catania, </booktitle> <month> April </month> <year> 1994), </year> <editor> L. De Raedt & F.Bergadano (eds.), </editor> <publisher> Springer Verlag. (in press) Kononenko, </publisher> <editor> I. & Bratko, I. </editor> <title> (1991) Information based evaluation criterion for classifier's performance. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 67-80. </pages>
Reference-contexts: To increase the reliability of the probability approximation RELIEFF searches for k nearest hits/misses instead of only one near hit/miss and averages the contribution of all k nearest hits/misses. It was shown that this extension significantly improves the reliability of estimates of attributes' 5 qualities <ref> (Kononenko, 1994) </ref>. To overcome the problem of parameter tuning, in all our experiments k was set to 10 which, empirically, gives satisfactory results. In some problems significantly better results can be obtained with tuning (as is typical for the majority of machine learning algorithms).
Reference: <author> Mantaras R.L. </author> <title> (1989) ID3 Revisited: A distance based criterion for attribute selection, </title> <booktitle> Proc. Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, U.S.A., </address> <month> Oct. </month> <year> 1989. </year>
Reference-contexts: The heuristic functions that estimate the potential successors of the current state in the search space play a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio (Quinlan, 1986), gini-index (Breiman et al., 1984), distance measure <ref> (Mantaras, 1989) </ref>, weight of evidence (Michie & Al Attar, 1992), and j-measure (Smyth & Goodman, 1990). However, all these measures assume that attributes are independent and therefore in domains with strong dependencies between attributes the greedy search has poor chances of revealing a good hypothesis.
Reference: <author> Michalski, </author> <title> R.S. & Chilausky, R.L. (1980) Learning by being told and learning from examples: An experimental comparison of the two methods of knowledge acquisition in 18 the context of developing an expert system for soybean disease diagnosis. </title> <journal> International Journal of Policy Analysis and Information Systems, </journal> <volume> 4 </volume> <pages> 125-161. </pages>
Reference: <editor> Michalski, R.S. & Tecuci G. (eds.) </editor> <booktitle> (1994) Machine learning: A multistrategy approach, Vol. IV, </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: It is arguable whether learning of neural networks and statistical classification tehniques can be put under the Machine Learning umbrella. For a collection of relevant papers that illustrate the development of the field see (Dietterich & Shavlik, 1990). An extensive bibliography of the field can be found in <ref> (Michalski & Tecuci, 1994) </ref>. In this contribution we are concerend with inductive learning from examples. 1 Typical inductive learning algorithm uses a greedy search strategy to overcome the combinatorial explosion during the search for good hypotheses.
Reference: <author> Mladenic D. </author> <title> (1993) Combinatorial optimization in inductive concept learning. </title> <booktitle> Proc. 10th Intern. Conf. on Machine Learning. </booktitle> <address> (Amherst, June 1993), </address> <publisher> Morgan Kaufmann, </publisher> <pages> pp. </pages> <editor> 205-211 Muggleton S. (ed.) </editor> <booktitle> (1992) Inductive Logic Programming. </booktitle> <publisher> Academic Press. </publisher>
Reference: <author> Murphy, P. M., & Aha, D. W. </author> <year> (1991). </year> <title> UCI Repository of machine learning databases [Machine-readable data repository]. </title> <address> Irvine, CA: </address> <institution> University of California, Department of Information and Computer Science. </institution>
Reference-contexts: The optimal recognition rate is estimated to be 74%. Smyth et al. (1990) report 68.11.7% of the classification accuracy for naive Bayes, 64.63.5 for backpropagation, and 72.71.3 for their rule-based classifier. This data set can be obtained from Irvine database <ref> (Murphy & Aha, 1991) </ref>. KRK1: The problem of legality of King-Rook-King chess endgame positions. The attributes describe the relevant relations between pieces, such as "same rank" and "adjacent file". <p> The data was provided by Gail Gong from Carnegie-Mellon University. We also compared the performance of the algorithms on the following non-medical real world data sets (SOYB, IRIS, and VOTE are obtained from the Irvine database <ref> (Murphy & Aha, 1991) </ref>): SOYB: The famous soybean data set used by Michalski & Chilausky (1980). IRIS: The well known Fisher's problem of determining the type of iris flower.
Reference: <author> Niblett, T. & Bratko, I. </author> <title> (1986) Learning decision rules in noisy domains, </title> <booktitle> Proc. Expert Systems 86, </booktitle> <address> Brighton, UK, </address> <month> December </month> <year> 1986. </year> <title> U.Pompe, M.Kovacic & I.Kononenko (1993) SFOIL: Stochastic approach to inductive logic programming. </title> <booktitle> Proc. Slovenian Conf. </booktitle> <institution> on Electrical Engineering and Computer Science. Portoroz, Slovenia, </institution> <month> Sept. </month> <year> 1993, </year> <pages> pp 189-192. </pages>
Reference: <author> Quinlan R. </author> <title> (1986) Induction of decision trees, </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 81-106. </pages>
Reference-contexts: The heuristic functions that estimate the potential successors of the current state in the search space play a major role in the greedy search. Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio <ref> (Quinlan, 1986) </ref>, gini-index (Breiman et al., 1984), distance measure (Mantaras, 1989), weight of evidence (Michie & Al Attar, 1992), and j-measure (Smyth & Goodman, 1990). <p> Impurity functions tend to overestimate multi valued attributes and various normalization heuristics are needed to avoid this tendency (e.g. gain ratio <ref> (Quinlan, 1986) </ref> and binarization of attributes (Cestnik et al., 1987)). Equation (3) shows that RELIEF implicitly uses such a normalization. In the above derivation we eliminated the "nearest instance" condition from the probabilities. <p> The basic algorithm goes back to CLS (Concept 6 Learning System) developed by Hunt et al. (1966) and reimplemented by several authors (see <ref> (Quinlan, 1986) </ref> for an overview). In the following we describe the main features of Assistant and the differences implemented in Assistant-R. 3.1 Binarization of attributes The algorithm generates binary decision trees.
Reference: <author> Smyth P. & Goodman R.M. </author> <title> (1990) Rule induction using information theory. </title> <editor> In. G.Piatetsky-Shapiro & W.Frawley (eds.) </editor> <title> Knowledge Discovery in Databases, </title> <publisher> MIT Press. </publisher>
Reference-contexts: Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio (Quinlan, 1986), gini-index (Breiman et al., 1984), distance measure (Mantaras, 1989), weight of evidence (Michie & Al Attar, 1992), and j-measure <ref> (Smyth & Goodman, 1990) </ref>. However, all these measures assume that attributes are independent and therefore in domains with strong dependencies between attributes the greedy search has poor chances of revealing a good hypothesis.
Reference: <author> Smyth P., Goodman R.M. & Higgins C. </author> <title> (1990) A hybrid Rule-based Bayesian Classifier, </title> <booktitle> Proc.European Conf. on Artificial Intelligence, </booktitle> <address> Stockholm, </address> <month> August, </month> <year> 1990, </year> <pages> pp. 610-615. 19 </pages>
Reference-contexts: Current inductive learning algorithms use variants of impurity functions like information gain, gain ratio (Quinlan, 1986), gini-index (Breiman et al., 1984), distance measure (Mantaras, 1989), weight of evidence (Michie & Al Attar, 1992), and j-measure <ref> (Smyth & Goodman, 1990) </ref>. However, all these measures assume that attributes are independent and therefore in domains with strong dependencies between attributes the greedy search has poor chances of revealing a good hypothesis.
References-found: 24

