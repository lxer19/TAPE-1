URL: http://iacoma.cs.uiuc.edu/iacoma-papers/fowlong.ps
Refering-URL: http://iacoma.cs.uiuc.edu/papers.html
Root-URL: http://www.cs.uiuc.edu
Email: koufaty,xchen,poulsen,torrella@csrd.uiuc.edu  
Title: Data Forwarding in Scalable Shared-Memory Multiprocessors 1 2  
Author: David A. Koufaty Xiangfeng Chen David K. Poulsen Josep Torrellas 
Keyword: Index Terms: Memory latency hiding, forwarding and prefetching, multiprocessor caches, scalable shared-memory multiprocessors, address trace analysis.  
Address: IL 61801  
Affiliation: Center for Supercomputing Research and Development University of Illinois at Urbana-Champaign,  
Abstract: Scalable shared-memory multiprocessors are often slowed down by long-latency memory accesses. One way to cope with this problem is to use data forwarding to overlap memory accesses with computation. With data forwarding, when a processor produces a datum, in addition to updating its cache, it sends a copy of the datum to the caches of the processors that the compiler identified as consumers of it. As a result, when the consumer processors access the datum, they find it in their caches. This paper addresses two main issues. First, it presents a framework for a compiler algorithm for forwarding. Second, using address traces, it evaluates the performance impact of different levels of support for forwarding. Our simulations of a 32-processor machine show that an optimistic support for forwarding speeds up five applications by an average of 50% for large caches and 30% for small caches. For large caches, most sharing read misses are eliminated, while for small caches, forwarding does not increase the number of conflict misses significantly. Overall, support for forwarding in shared-memory multiprocessors promises to deliver good application speedups. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Adve and M. Hill. </author> <title> Weak Ordering ANew Definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 1-13, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: However, while these techniques indeed reduce the average memory access latency significantly, they tend to handle sharing-induced misses poorly. To deal with sharing-induced misses, we often need techniques that overlap memory accesses with computation or with other memory accesses. These techniques include multithreading [2], relaxed memory consistency models <ref> [1, 7] </ref>, data prefetching [13] and data forwarding [19].
Reference: [2] <author> A. Agarwal. </author> <title> Performance Tradeoffs in Multithreaded Processors. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> volume 3, </volume> <pages> pages 525-539, </pages> <month> Septem-ber </month> <year> 1992. </year>
Reference-contexts: However, while these techniques indeed reduce the average memory access latency significantly, they tend to handle sharing-induced misses poorly. To deal with sharing-induced misses, we often need techniques that overlap memory accesses with computation or with other memory accesses. These techniques include multithreading <ref> [2] </ref>, relaxed memory consistency models [1, 7], data prefetching [13] and data forwarding [19].
Reference: [3] <author> U. Banerjee. </author> <title> Dependence Analysis for Supercomputing. </title> <publisher> Kluwer Academic Publishers, Norwell, </publisher> <address> MA, </address> <year> 1988. </year>
Reference-contexts: Clearly, in the presence of procedure calls and aliasing, the compiler analysis can be complex. Such analysis needs to be handled by data dependence algorithms between loops. Discussion of data dependence algorithms <ref> [3] </ref> is beyond the scope of this paper. Instead, we will assume that we have already identified the producer and consumer DOALL I = 3, 16 A (2I+1) = ... ENDDOALL DOALL I = 0, 9 ... = A (I+2) ENDDOALL loops.
Reference: [4] <author> W. </author> <note> Berke. </note>
Reference-contexts: Furthermore, they only evaluated a simple strategy for forwarding. There are other related schemes that allow a processor to broadcast the updated value of a variable. These schemes are the KSR Poststore mechanism [21], the DASH Update Write [12], the Ultracomputer UpdateAll primitive <ref> [4] </ref> and the Multicube Notify primitive [9]. However, in both the KSR Poststore mechanism and the DASH Update Write, only the processors holding a copy of that data receive the update.
References-found: 4

