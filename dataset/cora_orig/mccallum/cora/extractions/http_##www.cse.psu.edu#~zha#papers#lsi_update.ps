URL: http://www.cse.psu.edu/~zha/papers/lsi_update.ps
Refering-URL: http://www.cse.psu.edu/~zha/papers.html
Root-URL: http://www.cse.psu.edu
Title: ON UPDATING PROBLEMS IN LATENT SEMANTIC INDEXING  
Author: HONGYUAN ZHA AND HORST D. SIMON 
Keyword: Key words. singular value decomposition, updating problems, latent semantic indexing, information retrieval  
Note: AMS subject classifications. 15A06, 15A18, 65A15  
Abstract: We develop new SVD-updating algorithms for three types of updating problems arising from Latent Semantic Indexing (LSI) for information retrieval to deal with rapidly changing text document collections. We also provide theoretical justification for using a reduced-dimension representation of the original document collection in the updating process. Numerical experiments using several standard text document collections show that the new algorithms give higher (interpolated) average precisions than the existing algorithms and the retrieval accuracy is comparable to that obtained using the complete document collection. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M.W. Berry, S.T. Dumais and G.W. O'Brien. </author> <title> Using linear algebra for intelligent information retrieval. </title> <journal> SIAM Review, </journal> <volume> 37 </volume> <pages> 573-595, </pages> <year> 1995. </year>
Reference-contexts: Corresponding to each of the k reduced dimensions is associated a pseudo-concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [1, 3] </ref>. fl This work was supported by the Director, Office of Energy Research, Office of Laboratory Policy and Infrastructure Management, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences, of the U.S. Department of Energy under Contract No. <p> Updating the LSI-generated RDR can be carried out using a process called folding-in [3]. Folding-in is less expensive. However, since folding-in is based on the old RDR, it does not adjust the representation of existing terms and documents, and therefore retrieval accuracy may suffer. In <ref> [1, 8] </ref>, three SVD-updating algorithms are derived focusing on the balance among memory usage, computational complexity and retrieval accuracy. The purpose of this paper is to develop a more accurate mathematical model than that in [1, 8], and to show that better retrieval accuracy can be obtained with our new algorithms. <p> In <ref> [1, 8] </ref>, three SVD-updating algorithms are derived focusing on the balance among memory usage, computational complexity and retrieval accuracy. The purpose of this paper is to develop a more accurate mathematical model than that in [1, 8], and to show that better retrieval accuracy can be obtained with our new algorithms. In particular we show that no retrieval accuracy degradation will occur if updating is done with our new algorithms. <p> Section 5 concludes the paper and points out some future research topics. 2. New Updating Algorithms. Let A 2 R mfin be the original term-document matrix, and A k = P k k Q T k be the best rank-k RDR of A. Following <ref> [1, 8] </ref>, we specify three types of updating problems in LSI: 1. Updating Documents. Let D 2 R mfip be the p new documents. Compute the best rank-k approximation of B j [A k ; D]: 2. Updating Terms. Let T 2 R qfin be the q new term vectors. <p> This replacement procedure needs to be justified and we will have more to say on this later in Section 3. Now we present our new algorithms for the three types of updating problems mentioned above. During the presentation, we will also compare our approaches with those used in <ref> [1, 8] </ref>. Updating Documents. Let the QR decomposition of (I P k P T k )D be k )D = ^ P k R; where ^ P k is orthonormal, and R is upper triangular. <p> Then the best rank-k approximation of B is given by B k j ([P k ; ^ P k ]U k ) ^ k Q k 0 T In <ref> [1, 8] </ref>, only [ k ; P T k D] instead of ^ B in (2.1) is used to construct the SVD of B. The R matrix in ^ B is completely discarded. <p> This situation can happen when the added new documents alter the original low-dimension representation significantly. Numerical experiments in Section 4 bear this out. Our approach is certainly more expensive than the less accurate alternative in <ref> [1, 8] </ref>: for one thing we need to compute the SVD of ^ B instead of a submatrix of it; and also in order to form the left singular vector matrix of B we need to compute [P k ; ^ P k ]U k instead of P k ~ U <p> However, if p, the number of documents added is relatively small, the added computational cost is not much. 4 Our presentation for updating terms and for term weight corrections will be brief. The above comments regarding the algorithms in <ref> [1, 8] </ref> also apply in these two up dating problems as well. Updating Terms. Let the QR decomposition of (I Q k Q T k )T T be k )T T = ^ Q k L T ; where L is lower triangular. <p> Numerical Experiments. In this section we use several examples to illustrate the algorithms developed in Section 2 and compared them with those in <ref> [1, 8] </ref>. In all of the examples, we use the weighting scheme lxn.bpx [5, 9]. 8 The partial SVD of the original term-document matrix is computed using Lanczos process with one-sided reorthogonalization scheme proposed in [10]. <p> In Table 1, k = 100, p is the number of new documents added, Meth 1 is the updating algorithm in Section 2 and Meth 2 is that used in <ref> [1, 8] </ref>. Row 3 and row 4 of the table gives the average precisions in percentage. As is expected Meth 1 performs much better than Meth 2 for those seven combinations of p and s. <p> Since the algorithms in <ref> [1, 8] </ref> always discard the R matrix in (2.1) therefore it makes no difference to the updated low-rank approximation whether it is computed with all the new documents all at once or incrementally with each subgroup at a time. Now we present some timing and flop counts results. <p> Then for the remaining documents we add g documents at a time using the updating algorithms with g = 10; 25; and 50. In each cell of Table 2, there are two numbers: the first one is computed using the method developed in <ref> [1, 8] </ref> while the second one is computed using the method in Section 2. Both the CPU time (in seconds) and flop counts are per update quantities averaged over all the update steps performed to integrate the remaining documents. <p> Both the CPU time (in seconds) and flop counts are per update quantities averaged over all the update steps performed to integrate the remaining documents. As is expected, our method is more expensive when g is relatively large, but is more comparable to that of the method in <ref> [1, 8] </ref> when g is relatively small. However, our method achieves much higher retrieval accuracy: 66% versus 46%. Example 2. We repeat the tests in Example 1 for the CRANFIELD collection [2]. The term-document matrix is 2331fi1400 and the number of queries is 225. <p> Another approach will be first to find the RDR of the set of new documents and then merge it with the RDR of the original document collection. In practice a trade-off can always be made as to whether to use the less accurate but cheaper methods developed in <ref> [1, 8] </ref> or the more accurate methods reported in this paper. A hybrid approach can also be explored whereby the norm of the matrix R in Equation (2.1) can be first assessed, and then if the norm is small we can use the algorithms in [1, 8], otherwise switch to our <p> but cheaper methods developed in <ref> [1, 8] </ref> or the more accurate methods reported in this paper. A hybrid approach can also be explored whereby the norm of the matrix R in Equation (2.1) can be first assessed, and then if the norm is small we can use the algorithms in [1, 8], otherwise switch to our algorithms. These issues will be discussed in a forthcoming paper. Acknowledgement. The authors thank the referees for many helpful comments which improve the presentation of the paper.
Reference: [2] <institution> Cornell SMART System, ftp://ftp.cs.cornell.edu/pub/smart. </institution>
Reference-contexts: Of course the ultimate test of the utility of this concept is through assessing the retrieval accuracy as is done in Section 4. Figure 1 plots the singular value distributions of two term-document matrices, one from the MEDLINE collection, the other CRANFIELD collection <ref> [2] </ref> used in the next section as well. <p> All the computations are done on a Sun Ultra I workstation using MATLAB 5.0. Example 1. We use the MEDLINE text collection <ref> [2] </ref>. The term-document matrix is 3681 fi 1033 and the number of queries is 30. <p> However, our method achieves much higher retrieval accuracy: 66% versus 46%. Example 2. We repeat the tests in Example 1 for the CRANFIELD collection <ref> [2] </ref>. The term-document matrix is 2331fi1400 and the number of queries is 225. Table 3 gives the results of the computations. For this example, the dimension for the RDR is chosen to be k = 200. <p> For this example, the dimension for the RDR is chosen to be k = 200. In the incremental method we again update a subgroup of 100 documents at a time. Example 3. We use the 4322 fi 11429 term-document matrix from the NPL collection <ref> [2] </ref>. The number of queries is 100. We apply the Term-Updating algorithm in Section 2. Since the original term-document matrix has the terms sorted in nonincreasing document frequency, we apply a random permutation to the rows of the term-document matrix before we extract any submatrix.
Reference: [3] <author> S. Deerwester, S.T. Dumais, T.K. Landauer, G.W. Furnas and R.A. Harshman. </author> <title> Indexing by latent semantic analysis. </title> <journal> Journal of the Society for Information Science, </journal> <volume> 41 </volume> <pages> 391-407, </pages> <year> 1990. </year>
Reference-contexts: 1. Introduction. Latent semantic indexing (LSI) is a concept-based automatic indexing method that tries to overcome the two fundamental problems which plague traditional lexical-matching indexing schemes: synonymy and polysemy <ref> [3] </ref>. 1 Synonymy refers to the problem that several different words can be used to express a concept and the keywords in a user's query may not match those in the relevant documents while polysemy means that words can have multiple meanings and user's words may match those in irrelevant documents <p> Corresponding to each of the k reduced dimensions is associated a pseudo-concept which may not have any explicit semantic content yet helps to discriminate documents <ref> [1, 3] </ref>. fl This work was supported by the Director, Office of Energy Research, Office of Laboratory Policy and Infrastructure Management, Office of Computational and Technology Research, Division of Mathematical, Information, and Computational Sciences, of the U.S. Department of Energy under Contract No. <p> Updating the LSI-generated RDR can be carried out using a process called folding-in <ref> [3] </ref>. Folding-in is less expensive. However, since folding-in is based on the old RDR, it does not adjust the representation of existing terms and documents, and therefore retrieval accuracy may suffer.
Reference: [4] <author> D. Harman. </author> <note> TREC-3 conference report. NIST Special Publication 500-225, </note> <year> 1995. </year>
Reference-contexts: For each method and the corresponding parameters, we tabulate the average precision in percentage which is computed using the 11-point interpolated formula (roughly, averaging the precisions obtained at 11 recall levels) <ref> [4, 5] </ref>. All the computations are done on a Sun Ultra I workstation using MATLAB 5.0. Example 1. We use the MEDLINE text collection [2]. The term-document matrix is 3681 fi 1033 and the number of queries is 30.
Reference: [5] <author> T.G. Kolda and D.P. O'Leary. </author> <title> A semi-discrete matrix decomposition for latent semantic indexing in information retrieval. </title> <type> Technical Report UMCP-CSD CS-TR-3724, </type> <institution> Department of Computer Science, University of Maryland, </institution> <year> 1996. </year>
Reference-contexts: Notice that alternative decompositions have also been used for LSI <ref> [5] </ref>. 1 2 HONGYUAN ZHA and HORST SIMON In rapidly changing environments such as the World Wide Web, the document collection is frequently updated with new documents and terms constantly being added. Updating the LSI-generated RDR can be carried out using a process called folding-in [3]. Folding-in is less expensive. <p> Numerical Experiments. In this section we use several examples to illustrate the algorithms developed in Section 2 and compared them with those in [1, 8]. In all of the examples, we use the weighting scheme lxn.bpx <ref> [5, 9] </ref>. 8 The partial SVD of the original term-document matrix is computed using Lanczos process with one-sided reorthogonalization scheme proposed in [10]. <p> For each method and the corresponding parameters, we tabulate the average precision in percentage which is computed using the 11-point interpolated formula (roughly, averaging the precisions obtained at 11 recall levels) <ref> [4, 5] </ref>. All the computations are done on a Sun Ultra I workstation using MATLAB 5.0. Example 1. We use the MEDLINE text collection [2]. The term-document matrix is 3681 fi 1033 and the number of queries is 30.
Reference: [6] <author> G. Kowalski. </author> <title> Information Retrieval System: Theory and Implementation. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston, </address> <year> 1997. </year>
Reference-contexts: LSI is an extension of the vector space model for information retrieval <ref> [6, 9] </ref>.
Reference: [7] <author> R. Krovetz and W.B. Croft. </author> <title> Lexical ambiguity and information retrieval. </title> <journal> ACM Transactions on Information Systems, </journal> <volume> 10 </volume> <pages> 115-141, </pages> <year> 1992. </year>
Reference-contexts: 1 Synonymy refers to the problem that several different words can be used to express a concept and the keywords in a user's query may not match those in the relevant documents while polysemy means that words can have multiple meanings and user's words may match those in irrelevant documents <ref> [7] </ref>. LSI is an extension of the vector space model for information retrieval [6, 9].
Reference: [8] <author> G.W. O'Brien. </author> <title> Information Management Tools for Updating an SVD-Encoded Indexing Scheme. M.S. </title> <type> Thesis, </type> <institution> Department of Computer Science, Univ. of Tennessee, </institution> <year> 1994. </year>
Reference-contexts: Updating the LSI-generated RDR can be carried out using a process called folding-in [3]. Folding-in is less expensive. However, since folding-in is based on the old RDR, it does not adjust the representation of existing terms and documents, and therefore retrieval accuracy may suffer. In <ref> [1, 8] </ref>, three SVD-updating algorithms are derived focusing on the balance among memory usage, computational complexity and retrieval accuracy. The purpose of this paper is to develop a more accurate mathematical model than that in [1, 8], and to show that better retrieval accuracy can be obtained with our new algorithms. <p> In <ref> [1, 8] </ref>, three SVD-updating algorithms are derived focusing on the balance among memory usage, computational complexity and retrieval accuracy. The purpose of this paper is to develop a more accurate mathematical model than that in [1, 8], and to show that better retrieval accuracy can be obtained with our new algorithms. In particular we show that no retrieval accuracy degradation will occur if updating is done with our new algorithms. <p> Section 5 concludes the paper and points out some future research topics. 2. New Updating Algorithms. Let A 2 R mfin be the original term-document matrix, and A k = P k k Q T k be the best rank-k RDR of A. Following <ref> [1, 8] </ref>, we specify three types of updating problems in LSI: 1. Updating Documents. Let D 2 R mfip be the p new documents. Compute the best rank-k approximation of B j [A k ; D]: 2. Updating Terms. Let T 2 R qfin be the q new term vectors. <p> This replacement procedure needs to be justified and we will have more to say on this later in Section 3. Now we present our new algorithms for the three types of updating problems mentioned above. During the presentation, we will also compare our approaches with those used in <ref> [1, 8] </ref>. Updating Documents. Let the QR decomposition of (I P k P T k )D be k )D = ^ P k R; where ^ P k is orthonormal, and R is upper triangular. <p> Then the best rank-k approximation of B is given by B k j ([P k ; ^ P k ]U k ) ^ k Q k 0 T In <ref> [1, 8] </ref>, only [ k ; P T k D] instead of ^ B in (2.1) is used to construct the SVD of B. The R matrix in ^ B is completely discarded. <p> This situation can happen when the added new documents alter the original low-dimension representation significantly. Numerical experiments in Section 4 bear this out. Our approach is certainly more expensive than the less accurate alternative in <ref> [1, 8] </ref>: for one thing we need to compute the SVD of ^ B instead of a submatrix of it; and also in order to form the left singular vector matrix of B we need to compute [P k ; ^ P k ]U k instead of P k ~ U <p> However, if p, the number of documents added is relatively small, the added computational cost is not much. 4 Our presentation for updating terms and for term weight corrections will be brief. The above comments regarding the algorithms in <ref> [1, 8] </ref> also apply in these two up dating problems as well. Updating Terms. Let the QR decomposition of (I Q k Q T k )T T be k )T T = ^ Q k L T ; where L is lower triangular. <p> Numerical Experiments. In this section we use several examples to illustrate the algorithms developed in Section 2 and compared them with those in <ref> [1, 8] </ref>. In all of the examples, we use the weighting scheme lxn.bpx [5, 9]. 8 The partial SVD of the original term-document matrix is computed using Lanczos process with one-sided reorthogonalization scheme proposed in [10]. <p> In Table 1, k = 100, p is the number of new documents added, Meth 1 is the updating algorithm in Section 2 and Meth 2 is that used in <ref> [1, 8] </ref>. Row 3 and row 4 of the table gives the average precisions in percentage. As is expected Meth 1 performs much better than Meth 2 for those seven combinations of p and s. <p> Since the algorithms in <ref> [1, 8] </ref> always discard the R matrix in (2.1) therefore it makes no difference to the updated low-rank approximation whether it is computed with all the new documents all at once or incrementally with each subgroup at a time. Now we present some timing and flop counts results. <p> Then for the remaining documents we add g documents at a time using the updating algorithms with g = 10; 25; and 50. In each cell of Table 2, there are two numbers: the first one is computed using the method developed in <ref> [1, 8] </ref> while the second one is computed using the method in Section 2. Both the CPU time (in seconds) and flop counts are per update quantities averaged over all the update steps performed to integrate the remaining documents. <p> Both the CPU time (in seconds) and flop counts are per update quantities averaged over all the update steps performed to integrate the remaining documents. As is expected, our method is more expensive when g is relatively large, but is more comparable to that of the method in <ref> [1, 8] </ref> when g is relatively small. However, our method achieves much higher retrieval accuracy: 66% versus 46%. Example 2. We repeat the tests in Example 1 for the CRANFIELD collection [2]. The term-document matrix is 2331fi1400 and the number of queries is 225. <p> Another approach will be first to find the RDR of the set of new documents and then merge it with the RDR of the original document collection. In practice a trade-off can always be made as to whether to use the less accurate but cheaper methods developed in <ref> [1, 8] </ref> or the more accurate methods reported in this paper. A hybrid approach can also be explored whereby the norm of the matrix R in Equation (2.1) can be first assessed, and then if the norm is small we can use the algorithms in [1, 8], otherwise switch to our <p> but cheaper methods developed in <ref> [1, 8] </ref> or the more accurate methods reported in this paper. A hybrid approach can also be explored whereby the norm of the matrix R in Equation (2.1) can be first assessed, and then if the norm is small we can use the algorithms in [1, 8], otherwise switch to our algorithms. These issues will be discussed in a forthcoming paper. Acknowledgement. The authors thank the referees for many helpful comments which improve the presentation of the paper.
Reference: [9] <author> G. Salton. </author> <title> Automatic Text Processing. </title> <publisher> Addison-Wesley, </publisher> <address> New York, </address> <year> 1989. </year>
Reference-contexts: LSI is an extension of the vector space model for information retrieval <ref> [6, 9] </ref>. <p> PA 16802-6103. z NERSC, Lawrence Berkeley National Laboratory, 1 Cyclotron Road, Berkeley, CA 94720. 1 LSI does a better job dealing with synonymy while polysemy still remains to be a problem unless word senses are used. 2 Various weighting schemes can be applied to A before its SVD is computed <ref> [9] </ref>. Notice that alternative decompositions have also been used for LSI [5]. 1 2 HONGYUAN ZHA and HORST SIMON In rapidly changing environments such as the World Wide Web, the document collection is frequently updated with new documents and terms constantly being added. <p> Numerical Experiments. In this section we use several examples to illustrate the algorithms developed in Section 2 and compared them with those in [1, 8]. In all of the examples, we use the weighting scheme lxn.bpx <ref> [5, 9] </ref>. 8 The partial SVD of the original term-document matrix is computed using Lanczos process with one-sided reorthogonalization scheme proposed in [10].
Reference: [10] <author> H.D. Simon and H. Zha. </author> <title> Low rank matrix approximation using the Lanczos bidiagonalization process with applications. </title> <type> Technical Report CSE-97-008, </type> <institution> Department of Computer Science and Engineering, The Pennsylvania State University, </institution> <year> 1997. </year>
Reference-contexts: In all of the examples, we use the weighting scheme lxn.bpx [5, 9]. 8 The partial SVD of the original term-document matrix is computed using Lanczos process with one-sided reorthogonalization scheme proposed in <ref> [10] </ref>. For each method and the corresponding parameters, we tabulate the average precision in percentage which is computed using the 11-point interpolated formula (roughly, averaging the precisions obtained at 11 recall levels) [4, 5]. All the computations are done on a Sun Ultra I workstation using MATLAB 5.0. Example 1.
Reference: [11] <author> G. Xu and T. Kailath. </author> <title> Fast subspace decompsotion. </title> <journal> IEEE Transactions on Signal Processing, </journal> <volume> 42 </volume> <pages> 539-551, </pages> <year> 1994. </year>
Reference-contexts: The matrix ^ B T ^ B in Theorem 3.5 has a so-called low-rank-plus-shift structure, a concept that has been used in sensor array signal processing <ref> [11, 12] </ref>. We now assess how well this structure can fit the term-document matrices of some standard document collections. Of course the ultimate test of the utility of this concept is through assessing the retrieval accuracy as is done in Section 4.
Reference: [12] <author> G. Xu, H. Zha, G. Golub, and T. Kailath. </author> <title> Fast algorithms for updating signal subspaces. </title> <journal> IEEE Transactions on Circuits and Systems, </journal> <volume> 41 </volume> <pages> 537-549, </pages> <year> 1994. </year>
Reference-contexts: The matrix ^ B T ^ B in Theorem 3.5 has a so-called low-rank-plus-shift structure, a concept that has been used in sensor array signal processing <ref> [11, 12] </ref>. We now assess how well this structure can fit the term-document matrices of some standard document collections. Of course the ultimate test of the utility of this concept is through assessing the retrieval accuracy as is done in Section 4.
References-found: 12

