URL: http://www.cag.lcs.mit.edu/~kenmac/thesis.ps.gz
Refering-URL: http://www.cag.lcs.mit.edu/~kenmac/
Root-URL: 
Title: An Efficient Virtual Network Interface in the FUGU Scalable Workstation  
Author: by Kenneth Martin Mackenzie Anant Agarwal M. Frans Kaashoek Arthur C. Smith 
Degree: (1990) Submitted to the Department of Electrical Engineering and Computer Science in partial fulfillment of the requirements for the degree of DOCTOR OF PHILOSOPHY at the  All rights reserved. Author  Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Certified by  Associate Professor of Computer Science and Engineering Thesis Supervisor Accepted by  Chairman, Departmental Committee on Graduate Students  
Date: February 1998  22 December 1997  
Address: (1990)  
Affiliation: S.B., Massachusetts Institute of Technology  S.M., Massachusetts Institute of Technology  MASSACHUSETTS INSTITUTE OF TECHNOLOGY  c Massachusetts Institute of Technology 1998.  Department of Electrical Engineering and Computer Science  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Anant Agarwal, Ricardo Bianchini, David Chaiken, Kirk Johnson, David Kranz, John Kubia-towicz, Beng-Hong Lim, Kenneth Mackenzie, and Donald Yeung. </author> <title> The MIT Alewife Machine: Architecture and Performance. </title> <booktitle> In Proceedings of the 22nd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 213, </pages> <month> June </month> <year> 1995. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes <ref> [46, 1, 41] </ref> and high-end commercial machines [36, 42, 49, 79]. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. <p> Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication <ref> [62, 1] </ref>. Clusters of workstations or SMPs interconnected by a LAN provide support for multiple users and can provide scalable cost and performance, but current clusters provide only inefficient inter-node communication. Clusters typically support only message-passing for inter-node communication in hardware and synthesize shared memory (if desired) in software. <p> Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> Finally, one reading of technology trends is that shared memory systems increasingly will be implemented in software using messages to minimize hardware <ref> [1, 31] </ref> and/or to take advantage of application-specific knowledge [62]. Finally, a scalable workstation is made scalable by its point-to-point interconnect which allows the communication performance of the machine to scale with the number of processors in the machine. Scalability has two consequences. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines <ref> [16, 7, 61, 1, 56] </ref> and one commercial machine, the CM-5 [45]. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> We performed our direct VNI experiments on two platforms. First, we constructed an emulator of FUGU hardware by adding a small amount of additional hardware to an existing experimental multiprocessor, Alewife <ref> [1] </ref>. Second, we built a custom simulator of the system, T2. The two 73 build a limited version of the right thing, (B) build something different but related on stock hardware, (C) build a full version on emulated (slow) hardware. platforms are described in Section 6.1. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect.
Reference: [2] <author> Boon S. Ang, Derek Chiou, Larry Rudolph, and Arvind. </author> <title> Message Passing Support on StartT-Voyager. CSG Memo 387, Computation Structures Group, </title> <institution> MIT Laboratory for Computer Science, </institution> <month> July </month> <year> 1996. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> We proposed a solution to the VNI problem in [50] and partly evaluated it in [51]. This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI [58], the *T family <ref> [61, 2] </ref> and the M-machine [25]. These projects are described as related work in Chapter 8. * Second is the DMA problem. Efficient bulk transfer through messages requires the support of Direct Memory Access (DMA) hardware or equivalent functionality provided by a coprocessor. <p> The role of a coprocessor can be interpreted two ways, either as part of the hardware or as an independent, programmable entity. Simple coprocessors, as in the SP-2 [72] or in *T-Voyager <ref> [2] </ref>, can be viewed as a hardware implementation technique. More elaborate coprocessors, as in the CS-2 [66], Flash [41] or Typhoon [62], that are programmable need a network interface themselves. <p> The direct VNI employs a hybrid approach to network interface design that uses both a direct interface for speed and provides buffering for convenience. Other interfaces take a similar approach, notably Wisconsin's CNI [56, 57] and the descendants of *T, *T-NG [11] and *T-Voyager <ref> [2] </ref>. Figure 8-1 (identical to Figure 2-3) gives a schematic view of different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively. Part (c) represents the direct VNI approach in which the hardware is direct but the message system supports two paths. <p> Again, a range of implementation options exist and the best decision about when to switch is likely to be in between these two extremes. A different hybrid approach is taken in the *T-NG [11] system and the subsequent *T-Voyager system <ref> [2] </ref>. In *T-NG, the network interface hardware demultiplexes incoming messages into one of several moderate-sized hardware queues implemented as dual-ported RAM on the L2 cache bus. The multiple queues allow a limited number of applications to be active simultaneously.
Reference: [3] <author> R. Arpaci, A. Dusseau, A. Vahdat, L. Liu, T. Anderson, and D. Patterson. </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations. </title> <booktitle> In Proceedings of the Joint International Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pages 267278, </pages> <month> May </month> <year> 1995. </year>
Reference-contexts: If the interactive demands are sufficiently low, it is clearly useful for the system to employ independent, priority-based scheduling because the disruptions to the compute-intensive application are low. If disruptions are too high, there is emerging agreement <ref> [15, 22, 3, 73, 44] </ref>, that it becomes useful to explicitly co-schedule the processes of the compute-intensive application by one means or another. Section 7.2.1 introduces a parameterized workload that pits a compute-intensive application against an interactive application. <p> Assume f represents the fraction of CPU time used by the interactive application. The slowdown of the compute-intensive application ranges from the best case (Figure 7-3) of 1 to a worst case (Figure 7-4) of 1 for P processors if work arrives independently for each processor. Arpaci et al <ref> [3] </ref> document this effect in a network of workstations for disruptions caused by periodic daemon processes. Coscheduling improves the performance of the compute application although possibly at the expense of the interactive application. In coscheduling mode, the scheduler allocates fixed-size times-lices across the machine to each application using synchronized clocks.
Reference: [4] <author> Robert C. Bedichek. Talisman: </author> <title> Fast and Accurate Multicomputer Simulation. </title> <booktitle> In Proceedings of the 1995 SIGMETRICS Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1995. </year>
Reference-contexts: The simulator design emphasizes speed and employs an incremental approach to timing accuracy <ref> [4] </ref>. Compared to measurements of the emulated hardware, the simulator reports cycle counts within +0/-30%. We believe the distortions introduced do not qualitatively affect the results. The simulator models timing using an instruction-driven approach.
Reference: [5] <author> Robert D. Blumofe, Christopher F. Joerg, Bradley C. Kuszmaul, Charles E. Leiserson, Keith H. Randall, and Yuli Zhou. Cilk: </author> <title> An Efficient Multithreaded Runtime System. </title> <booktitle> In Proceedings of the 5th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 207216, </pages> <month> July </month> <year> 1995. </year>
Reference-contexts: One is a minimal abstraction atop the UDM primitives and is defined in this section. The second is the CRL software shared memory system [30] ported to use UDM. The third (unused in the experiments) is the Cilk language and its associated load-balancing runtime system <ref> [5] </ref>. The minimal abstraction atop UDM (conventionally referred to as just UDM), wraps the inject and extract operations in do on statements and the abstraction of handler declarations for procedures, respectively. The do on/handler abstraction implemented for FUGU is strictly interrupt-driven. <p> Beyond raw UDM, we ported two libraries to the machine. One is CRL [30], an all-software shared-memory system. CRL is strictly a runtime library that operates by passing messages. We use this library extensively in our experiments in the next chapter. The other is Cilk <ref> [5] </ref>, a programming language and runtime system that performs automatic load balancing through work stealing. This version of Cilk (version 4) communicates through shared memory. Since the focus of the thesis is on message-passing, we do not include any Cilk-based applications in our evaluation.
Reference: [6] <author> Matthias A. Blumrich, Kai Li, Richard Alpert, Cezary Dubnicki, Edward W. Felten, and Jonathan Sandberg. </author> <title> Virtual Memory Mapped Network Interface for the SHRIMP Multicom-puter. </title> <booktitle> In Proceedings 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 142153, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72]. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers <ref> [6, 9, 66, 69, 72] </ref> and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers. <p> Sender-based message systems use network interfaces in memory but allow the sending processor to participate in managing the receiving processor's memory. Remote-memory communication as in SHRIMP <ref> [6] </ref> and the DEC memory-channel [24] give a sending process the means to directly write memory in another process on another processor.
Reference: [7] <author> S. Borkar, R. Cohn, G. Cox, T. Gross, H.T. Kung, M. Lam, M. Levine, B. Moore, W. Moore, C. Peterson, J. Susman, J. Sutton, J. Urbanski, and J. Webb. </author> <title> Supporting Systolic and Memory Communication in iWarp. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 7081, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines <ref> [16, 7, 61, 1, 56] </ref> and one commercial machine, the CM-5 [45]. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect.
Reference: [8] <author> Eric Brewer, Fred Chong, Lok Liu, Shamik Sharma, and John Kubiatowicz. </author> <title> Remote Queues: Exposing Message Queues for Optimization and Atomicity. </title> <booktitle> In Proceedings of the Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1995. </year>
Reference-contexts: This is an advantage because periodic polling is often difficult to synthesize accurately. With polling, the atomicity model is clearer and less error-prone: a faulty polling-based application tends to deadlock, which is much easier to debug than a synchronization failure. <ref> [8] </ref>. The fundamental costs of communication arise from the operations of data and control transfer. <p> However, not all protocols are easily mapped to the request/reply discipline. For instance, the remote-writer optimization in a shared-memory protocol results in a three-way trip. Solving deadlock in such a protocol requires three logical networks. The Remote Queues <ref> [8] </ref> model defines communication through multiple named queues. An application can define as many queues (which correspond to multiple logical networks) as necessary to avoid deadlock, presuming the queues are big enough. <p> UDM provides kernel-like control over interrupts and the ability to treat the network as private, reliable and with unbounded buffering. UDM is powerful enough to implement the efficient Active Messages [78] and Remote Queues <ref> [8] </ref> models directly. * Protection is enabled because the combination of two-case delivery and virtual buffering solves the undeliverable message problem (Section 2.2), permitting protected multiprogramming and demand-paged virtual memory. 83 * Performance in terms of speed comes from the direct interface. <p> The Active Messages work [78] gave a name to this style of model which appeared earlier in Mosaic [70], the J-machine [16] and others. UDM is similar to Active Messages and related to Remote Queues (RQ) <ref> [8] </ref>. UDM shares the Active Message goal of providing a minimal building block and uses the same convention of specifying a handler by a raw procedure address. UDM differs from Active Messages in two important ways.
Reference: [9] <author> Greg Buzzard, David Jacobson, Milon Mackey, Scott Marovich, and John Wilkes. </author> <title> An Implementation of the Hamlyn Sender-Managed Interface Architecture. </title> <booktitle> In Proceedings of the Second Symposium on Operating System Design and Implementation, </booktitle> <pages> pages 245259, </pages> <year> 1996. </year> <month> 123 </month>
Reference-contexts: A point-to-point network in one cabinet (a System Area Network or SAN) may be made to be sufficiently reliable that protocol-level fault tolerance becomes unattractive compared to a combination of link-level fault tolerance (e.g., via ECC) and simple end-to-end fault detection. Other researchers take the same view <ref> [9, 25] </ref>. MPP manufacturers have found it feasible to build reliable networks for machines with several hundreds of nodes [69]. Placing the network in one cabinet avoids most of the practical causes of failures, e.g., those due to unpluggings, independent power supply failures, cable damage and electromagnetic interference. Protection. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers <ref> [6, 9, 66, 69, 72] </ref> and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers. <p> Remote-memory communication as in SHRIMP [6] and the DEC memory-channel [24] give a sending process the means to directly write memory in another process on another processor. More elaborate sender-based models as in Hamlyn <ref> [81, 9] </ref> and Hybrid Deposit [59] support more communication options, e.g., the ability to insert into a remote queue, but retain the idea that the sender manages the memory. The sender-based approach is interesting because it exposes the fundamental costs of memory-based communication in its programming model.
Reference: [10] <author> John B. Carter. </author> <title> Efficient Distributed Shared Memory Based On Multi-Protocol Release Con--sistency. </title> <type> PhD thesis, </type> <institution> Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [11] <author> Derek Chiou, Boon S. Ang, Arvind, Michael J. Beckerle, Andy Boughton, Robert Greiner, James E. Hicks, and James C. Hoe. START-NG: </author> <title> Delivering Seamless Parallel Computing. </title> <booktitle> In Proceedings of the 1st International Conference on Parallel Proceessing (Euro-Par '95), </booktitle> <month> August </month> <year> 1995. </year> <note> Also available as CSG Memo 371, </note> <institution> Computation Structures Group, MIT Laboratory for Computer Science. </institution>
Reference-contexts: The direct VNI employs a hybrid approach to network interface design that uses both a direct interface for speed and provides buffering for convenience. Other interfaces take a similar approach, notably Wisconsin's CNI [56, 57] and the descendants of *T, *T-NG <ref> [11] </ref> and *T-Voyager [2]. Figure 8-1 (identical to Figure 2-3) gives a schematic view of different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively. Part (c) represents the direct VNI approach in which the hardware is direct but the message system supports two paths. <p> Again, a range of implementation options exist and the best decision about when to switch is likely to be in between these two extremes. A different hybrid approach is taken in the *T-NG <ref> [11] </ref> system and the subsequent *T-Voyager system [2]. In *T-NG, the network interface hardware demultiplexes incoming messages into one of several moderate-sized hardware queues implemented as dual-ported RAM on the L2 cache bus. The multiple queues allow a limited number of applications to be active simultaneously.
Reference: [12] <author> Frederic T. Chong, Rajeev Barua, Fredrik Dahlgren, John D. Kubiatowicz, and Anant Agarwal. </author> <title> The Sensitivity of Communication Mechanisms to Bandwidth and Latency. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture (HPCA-4), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Challenges arising from the integration of four features desired in a scalable workstation. Message passing and shared memory are features desired for high performance parallel processing while multiprogramming and virtual memory are features desired for general-purpose, multiuser operation. tolerance <ref> [12] </ref>. In particular, even if written assuming a shared-memory programming model, a program in which communication patterns are amenable to compile-time analysis might make better use of compiler-generated message passing communication than of even hardware-supported shared memory [14, 12]. <p> In particular, even if written assuming a shared-memory programming model, a program in which communication patterns are amenable to compile-time analysis might make better use of compiler-generated message passing communication than of even hardware-supported shared memory <ref> [14, 12] </ref>. Third, distributed shared memory and message passing implementations are naturally similar at a low level, so if an implementation provides shared memory in hardware the additional cost of exposing message passing is low [40].
Reference: [13] <author> Compaq Computer Corporation. </author> <title> Virtual Interface Architecture for SANs. Technology Brief, </title> <month> May </month> <year> 1997. </year> <title> Document Number 508A/0597. </title>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72].
Reference: [14] <author> Alan L. Cox, Sandhya Dwarkadas, Honghui Lu, and Willy Zwaenepoel. </author> <title> Evaluating the Performance of Software Distributed Shared Memory as a Target for Parallelizing Compilers. </title> <booktitle> In Proceedings of the 11th International Symposium on Paralel Processing, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: In particular, even if written assuming a shared-memory programming model, a program in which communication patterns are amenable to compile-time analysis might make better use of compiler-generated message passing communication than of even hardware-supported shared memory <ref> [14, 12] </ref>. Third, distributed shared memory and message passing implementations are naturally similar at a low level, so if an implementation provides shared memory in hardware the additional cost of exposing message passing is low [40].
Reference: [15] <author> M. Crovella, P. Das, C. Dubnicki, T. LeBlanc, and E. Markatos. </author> <title> Multiprogramming on Multiprocessors. </title> <booktitle> In Proceedings of the Third IEEE Symposium on Parallel and Distributed Processing, </booktitle> <year> 1991. </year>
Reference-contexts: If the interactive demands are sufficiently low, it is clearly useful for the system to employ independent, priority-based scheduling because the disruptions to the compute-intensive application are low. If disruptions are too high, there is emerging agreement <ref> [15, 22, 3, 73, 44] </ref>, that it becomes useful to explicitly co-schedule the processes of the compute-intensive application by one means or another. Section 7.2.1 introduces a parameterized workload that pits a compute-intensive application against an interactive application.
Reference: [16] <author> William J. Dally et al. </author> <title> The J-Machine: A Fine-Grain Concurrent Computer. </title> <booktitle> In Proceedings of the IFIP (International Federation for Information Processing), 11th World Congress, </booktitle> <pages> pages 11471153, </pages> <address> New York, 1989. </address> <publisher> Elsevier Science Publishing. </publisher>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines <ref> [16, 7, 61, 1, 56] </ref> and one commercial machine, the CM-5 [45]. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> The second network is used infrequently for this purpose so its performance is not critical. The network might be shared with some other use, such as supporting shared memory. An extra virtual channel <ref> [16] </ref> in the main network, a LAN or a service network could serve as an adequate second network. Our emulator hardware provides a custom but very simple, bit-serial network. <p> The Active Messages work [78] gave a name to this style of model which appeared earlier in Mosaic [70], the J-machine <ref> [16] </ref> and others. UDM is similar to Active Messages and related to Remote Queues (RQ) [8]. UDM shares the Active Message goal of providing a minimal building block and uses the same convention of specifying a handler by a raw procedure address. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect.
Reference: [17] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lumley. </author> <title> Afterburner. </title> <journal> IEEE Network, </journal> <pages> pages 3643, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72]. <p> Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers [6, 9, 66, 69, 72] and workstations <ref> [17, 19, 76, 77] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately. <p> Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers [6, 9, 66, 69, 72, 24] and workstations <ref> [17, 19, 76, 77] </ref> limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [18] <author> Peter Druschel and Larry L. Peterson. Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility. </title> <booktitle> In Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <pages> pages 189202, </pages> <month> December </month> <year> 1993. </year>
Reference-contexts: The Active Message implementation in SUNMOS [63] on the Intel Paragon uses kernel code to unload the message interface and to queue messages to be handled by a user thread. The SUNMOS approach corresponds to using the software-buffered path in UDM continuously. Fbufs <ref> [18] </ref> are an operating-system construct used to efficiently feed streams of data across protection domains. The UDM virtual buffering system employs similar techniques in a specialized 116 implementation to manage its buffer memory. Network overflow in Alewife [40] is a form of two-case delivery used for a restricted purpose.
Reference: [19] <author> Peter Druschel, Larry L. Peterson, and Bruce S. Davie. </author> <title> Experiences with a High-Speed Network Adaptor: A Software Perspective. </title> <booktitle> In Proceedings of the Conference on Communication Architectures, Protocols and Applications, </booktitle> <pages> pages 213, </pages> <year> 1994. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72]. <p> Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers [6, 9, 66, 69, 72] and workstations <ref> [17, 19, 76, 77] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately. <p> Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers [6, 9, 66, 69, 72, 24] and workstations <ref> [17, 19, 76, 77] </ref> limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [20] <author> Dawson R. Engler, M. Frans Kaashoek, and James O'Toole, Jr. Exokernel: </author> <title> An Operating System Architecture for Application-Level Resource Management. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: The bulk of the direct VNI system is implemented in operating system software. The FUGU operating system is based on Exokernel techniques <ref> [20] </ref>. Glaze is the kernel part of the OS and PhOS is the library part of the OS. The FUGU system also makes use of an external scheduler that runs as a separate application, microkernel-style. Section 6.2 describes the operating system and the scheduler. <p> As an exokernel, the OS consists of two pieces: Glaze is the in-kernel portion and PhOS is the library operating system portion. Glaze and PhOS are directly derived from the original Aegis/ExOS exokernel <ref> [20] </ref> and reuse much of that code. Glaze and PhOS support preemptive multiprogramming, virtual memory, user-level threads and a low-cost interprocess communication (IPC) mechanism. The virtual memory implementation is limited in that there is no paging to disk.
Reference: [21] <author> Andrew Erlichson, Neal Nuckolls, Greg Chesson, and John Hennessy. SoftFLASH: </author> <title> Analyzing the Performance of Clustered Distributed Virtual Shared Memory. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 210220, </pages> <month> October 15, </month> <year> 1996. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [22] <author> D. G. Feitelson and L. Rudolph. </author> <title> Gang Scheduling Performance Benefits for Fine-Grain Synchronization. </title> <journal> In Journal of Parallel and Distributed Computing, </journal> <pages> pages 306318, </pages> <month> December </month> <year> 1992. </year>
Reference-contexts: A flexible coscheduling parallel scheduler was proposed in [44] and is partly implemented in FUGU. Others are working on the same problem <ref> [22, 73] </ref>. <p> The conflict in scheduler requirements is stark in a scalable workstation where we want to run mixed workloads. A standard solution does not yet exist although a number of researchers are working on the problem <ref> [22, 73, 44] </ref>. Essentially what is required is a scheduler that works well in both interactive- and coscheduled modes. Beyond the scheduler, the virtual network interface itself needs to operate efficiently in both modes. <p> If the interactive demands are sufficiently low, it is clearly useful for the system to employ independent, priority-based scheduling because the disruptions to the compute-intensive application are low. If disruptions are too high, there is emerging agreement <ref> [15, 22, 3, 73, 44] </ref>, that it becomes useful to explicitly co-schedule the processes of the compute-intensive application by one means or another. Section 7.2.1 introduces a parameterized workload that pits a compute-intensive application against an interactive application.
Reference: [23] <author> Edward W. Felten. </author> <title> Protocol Compilation: High-Performance Communication for Parallel Programs. </title> <type> PhD thesis, </type> <institution> University of Washington, Department of Computer Science and Engineering, </institution> <year> 1993. </year>
Reference-contexts: More important than emulation, a low-level model offers the promise that network traffic and protocol overhead may be reduced over that required by a higher-level model by programmer specialization [78] or through automatic, compile-time analysis and specialization <ref> [23, 34] </ref>. An ideal low-level model provides a complete set of communication operations and exposes fundamental costs. The programmer is thus given the ability to craft communication protocols tailored to the application and to minimize communication costs using application-specific knowledge.
Reference: [24] <author> Marco Fillo and Richard B. Gillett. </author> <title> Architecture and Implementation of Memory Channel 2. </title> <journal> Digital Technical Journal, </journal> <volume> 9(1):2741, </volume> <year> 1997. </year>
Reference-contexts: Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers. <p> Sender-based message systems use network interfaces in memory but allow the sending processor to participate in managing the receiving processor's memory. Remote-memory communication as in SHRIMP [6] and the DEC memory-channel <ref> [24] </ref> give a sending process the means to directly write memory in another process on another processor.
Reference: [25] <author> Marco Fillo, Stephen W. Keckler, W.J. Dally, Nicholas P. Carter, Andrew Chang, Yevgeny Gurevich, and Whay S. Lee. </author> <title> The M-Machine Multicomputer. </title> <booktitle> In Proceedings of the 28th Annual International Symposium on Microarchitecture, </booktitle> <pages> pages 146156. </pages> <publisher> IEEE Computer Society, </publisher> <month> November </month> <year> 1995. </year>
Reference-contexts: This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI [58], the *T family [61, 2] and the M-machine <ref> [25] </ref>. These projects are described as related work in Chapter 8. * Second is the DMA problem. Efficient bulk transfer through messages requires the support of Direct Memory Access (DMA) hardware or equivalent functionality provided by a coprocessor. <p> A point-to-point network in one cabinet (a System Area Network or SAN) may be made to be sufficiently reliable that protocol-level fault tolerance becomes unattractive compared to a combination of link-level fault tolerance (e.g., via ECC) and simple end-to-end fault detection. Other researchers take the same view <ref> [9, 25] </ref>. MPP manufacturers have found it feasible to build reliable networks for machines with several hundreds of nodes [69]. Placing the network in one cabinet avoids most of the practical causes of failures, e.g., those due to unpluggings, independent power supply failures, cable damage and electromagnetic interference. Protection. <p> NACKs introduce complexity into the network interface, add traffic and overhead for buffer management and require a reserved back-path. The return-to-sender strategy, used in the T3E [69] and the M-machine <ref> [25] </ref> is a form of NACK that avoids much of the common-case buffer overhead. Return-to-sender assumes a reliable network. Both dropping and NACKing have the effect of increasing network demand under load. Further, dropping and NACKing are subject to livelock unless additional steps are taken to prevent it. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. <p> The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. A few machines, the CM-5 [45], *T [61] and the M-machine <ref> [25] </ref> and have provided this kind of interface with protection. The CM-5 reconciles its direct interface with a restricted form of multiprogramming via par-tioning, strict gang scheduling and by context-switching the network partitions with the processors.
Reference: [26] <author> John Heinlein, Kourosh Gharachorloo, Scott Dresser, and Anoop Gupta. </author> <title> Integration of Message Passing and Shared Memory in the Stanford FLASH Multiprocessor. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 3850, </pages> <month> October </month> <year> 1994. </year>
Reference-contexts: Using virtual memory is particularly natural when the 71 processor initiates all the buffering because existing support for virtual memory (e.g., the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [26, 57, 66, 80] </ref>. However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers [56, 57, 80].
Reference: [27] <author> John Heinlein, Kourosh Gharachorloo, and Anoop Gupta. </author> <title> Integrating Multiple Communication Paradigms in High Performance Multiprocessors. </title> <type> Technical Report CSL-TR-94-604, </type> <institution> Stanford, </institution> <month> February </month> <year> 1994. </year>
Reference-contexts: as a compiled SRAM array, so the comparison is highly unfavorable to the network interface, which is dominated by the area of random gate array logic. 3 300MHz processors are common in 1997. 4 Heinlein describes a scheme for safely passing both virtual and physical addresses to an NI in <ref> [27] </ref> 76 A two-node FUGU machine exists running a subset of our applications. We used the machine to run the microbenchmarks behind Tables 4-4 and 4-5 and for gross calibration of the simulator against applications. <p> The handling of the physical address from the TLB probe operation is a security hole that we accept in the prototype. A real implementation would pass the physical address securely from the TLB to the DMA engine, for instance using the technique described by Heinlein in <ref> [27] </ref>. void dma handler (handlername [, arg0 [, ... [, arg3]]], addrexp, lenexp) f hatomic handler codei user active global (); hthread codei g The DMA block address in the handler, addrexp, and the block length, lenexp are expressions in terms of variables available at the receiver, including arg0 through arg3
Reference: [28] <author> Dana S. Henry and Christopher F. Joerg. </author> <title> A Tightly-Coupled Processor-Network Interface. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: Although a definitive conclusion awaits further 28 research, past research indicates that direct interfaces tend to be more efficient than memory-based interfaces. Direct interfaces that can be accessed at cache speeds offer even better performance <ref> [28] </ref>. For example, the CNI paper [56] showed that a direct, cache-level interface exhibited 50% higher bandwidth than their best interface placed on the memory bus. Direct interfaces are challenging to protect without sacrificing efficiency or seriously impairing the multiprogramming model. <p> Others have shown that tightly-coupled, direct interfaces tend to be more efficient than indirect, memory-based interfaces <ref> [28, 56] </ref>. Thus FUGU's peak performance is high. Two-case delivery and virtual buffering provide good system performance in terms other than speed as well. Virtual buffering enables low memory consumption compared to a system that must provide a fixed amount of physical buffering per application. <p> Others have shown that direct interfaces and low-level messaging models give good application performance. We show here that the virtualized interface achieves performance close to a dedicated interface. Others have shown that tightly-coupled direct interaces and reliable message subsystems offer performance benefits to applications. Henry and Joerg <ref> [28] </ref> quantified the value of placing the message interface close to the processor and of providing hardware support for buffer management. The CNI work [56] shows the benefit of a tightly-coupled direct interface versus the best current 87 using a buffer-always policy normalized to the runtime using upcalls.
Reference: [29] <author> Robert W. Horst. TNET: </author> <title> A Reliable System Area Network. </title> <booktitle> IEEE Micro, </booktitle> <pages> pages 3745, </pages> <month> February </month> <year> 1995. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72].
Reference: [30] <author> Kirk L. Johnson, M. Frans Kaashoek, and Deborah A. Wallach. </author> <title> CRL: High-Performance All-Software Distributed Shared Memory. </title> <booktitle> In Proceedings of the Fifteenth Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1]. <p> We have used three communication models with FUGU. One is a minimal abstraction atop the UDM primitives and is defined in this section. The second is the CRL software shared memory system <ref> [30] </ref> ported to use UDM. The third (unused in the experiments) is the Cilk language and its associated load-balancing runtime system [5]. <p> If the system must switch to buffered mode while a handler is running but before the dispose point, the handler is merely restarted using the buffer-mode stub. Beyond raw UDM, we ported two libraries to the machine. One is CRL <ref> [30] </ref>, an all-software shared-memory system. CRL is strictly a runtime library that operates by passing messages. We use this library extensively in our experiments in the next chapter. The other is Cilk [5], a programming language and runtime system that performs automatic load balancing through work stealing. <p> Compute-Intensive Applications. Three applications, LU, Water and Barnes are standard scientific benchmarks. Water and Barnes are from the SPLASH [71] suite. LU, Water and Barnes are shared-memory programs that have been modified to make use of the CRL all-software shared-memory system <ref> [30] </ref>. CRL presents a message-passing load that is representative of other coherence protocols such as Stache [62] or Shasta [65] and can be considered operating-system-like: many low-latency protocol packets mixed with larger data packets. <p> The compute-intensive parallel applications obtain competitive speedups on FUGU. Figure 7-1 and Table 7-3 summarize the speedups achieved by the parallel applications. The speedups are relative to the parallel code running on one processor. The speedups for the CRL applications are comparable to those reported in <ref> [30] </ref> for the Alewife machine. The speedup for enum is noticeably superlinear due to cache and TLB capacity misses that decrease with machine size. Finally for the compute-intensive, parallel applications, barrier and null are synthetic applications included to illustrate opposite extremes of sensitivity to scheduling.
Reference: [31] <author> Kirk Lauritz Johnson. </author> <title> High-Performance All-Software Distributed Shared Memory. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> December </month> <year> 1995. </year> <note> Also available as Technical Report LCS-TR-674, </note> <institution> MIT Laboratory for Computer Science. </institution>
Reference-contexts: Finally, one reading of technology trends is that shared memory systems increasingly will be implemented in software using messages to minimize hardware <ref> [1, 31] </ref> and/or to take advantage of application-specific knowledge [62]. Finally, a scalable workstation is made scalable by its point-to-point interconnect which allows the communication performance of the machine to scale with the number of processors in the machine. Scalability has two consequences.
Reference: [32] <author> M. Frans Kaashoek, Dawson R. Engler, Gregory R. Ganger, H ector M. Brice no, Russell Hunt, David Mazi eres, Thomas Pinckney, Robert Grimm, John Jannotti, and Kenneth Mackenzie. </author> <title> Application Performance and Flexibility on Exokernel Systems. </title> <booktitle> In Proceedings of the Sixteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 5265, </pages> <month> December </month> <year> 1997. </year>
Reference-contexts: All parts of the system system and applications are compiled with the optimizer on (-O3). 6.2.1 Operating System The FUGU operating system is a custom multiuser operating system organized as an Exokernel <ref> [32] </ref>. As an exokernel, the OS consists of two pieces: Glaze is the in-kernel portion and PhOS is the library operating system portion. Glaze and PhOS are directly derived from the original Aegis/ExOS exokernel [20] and reuse much of that code.
Reference: [33] <author> Vijay Karamcheti and Andrew Chien. </author> <title> Do Faster Routers Imply Faster Communication? In Proceedings of Parallel Computer Routing and Communications Workshop, </title> <month> May </month> <year> 1994. </year>
Reference-contexts: Memory consumption is a per-message cost when buffering is used. In addition: * A message system incurs additional, possibly substantial, costs in the network, the endpoints and memory if a layer of protocol is required to synthesize message reliability, ordering, flow control, etc. <ref> [33] </ref>, needed by the application. The ideal communication model is both natural to program and is effective at minimizing the costs of communication. <p> As discussed in Chapter 2, direct interfaces offer lower latencies for single messages because they eliminate cache miss costs and buffer management overhead. Karamcheti and Chien <ref> [33] </ref> found that 50-70% of active message overhead in the CM-5 was attributable to the costs of providing flow control, ordering and reliability in software.
Reference: [34] <author> Vijay Karamcheti and Andrew A. Chien. </author> <title> View Caching: Efficient Software Shared Memory for Dynamic Computations. </title> <booktitle> In Proceedings of the 11th International Symposium on Parallel Processing, </booktitle> <month> April </month> <year> 1997. </year>
Reference-contexts: More important than emulation, a low-level model offers the promise that network traffic and protocol overhead may be reduced over that required by a higher-level model by programmer specialization [78] or through automatic, compile-time analysis and specialization <ref> [23, 34] </ref>. An ideal low-level model provides a complete set of communication operations and exposes fundamental costs. The programmer is thus given the ability to craft communication protocols tailored to the application and to minimize communication costs using application-specific knowledge.
Reference: [35] <author> Pete Keleher, Sandhya Dwarkadas, Alan Cox, and Willy Zwaenepoel. TreadMarks: </author> <title> Distributed Shared Memory on Standard Workstations and Operating Systems. </title> <booktitle> In Proceedings of the 1994 Winter Usenix Conference, </booktitle> <pages> pages 115131, </pages> <month> January </month> <year> 1994. </year> <month> 125 </month>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [36] <institution> Kendall Square Research. </institution> <type> KSR-1 Technical Summary, </type> <year> 1992. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines <ref> [36, 42, 49, 79] </ref>. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [37] <author> Leonidas Kontothanassis, Galen Hunt, Robert Stets, Nikolaos Hardavellas, Micha Cierniak, Srinivasan Parthasarathy, Wagner Meira, Jr., Sandhya Dwarkadas, and Michael Scott. </author> <title> VM-Based Shared Memory on Low-Latency, Remote-Memory-Access Networks. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 157169, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [38] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating Message-Passing and Shared-Memory; Early Experience. </title> <booktitle> In Proceedings of the 4th Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 5463, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Shared memory is widely considered more easily programmable than message passing. Message passing remains desirable as well for several reasons. First, the strengths of message-passing for bulk transfer and explicit synchronization are complementary to the strengths of shared memory for automatic communication <ref> [38] </ref>, making a mixed model attractive. <p> Shared memory integrates naturally with virtual memory and multiprogramming so we consider only message-passing issues here. Further, message passing has two main purposes with different requirements: small, synchronizing messages and large, bulk transfer messages <ref> [38] </ref>. Fine-grain messages for combined data transfer and synchronization require low overhead and latency for maximum utility. Messages for bulk transfer demand chiefly high bandwidth because latency and per-message overhead are amortized over the time of the transfer.
Reference: [39] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a Message in the Alewife Multiprocessor. </title> <booktitle> In Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pages 195206, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The injection operation is atomic in that messages are committed to the network in their entirety; no partial packets are ever seen by the communication substrate <ref> [39] </ref>. This atomicity property is useful for multiprogramming because it allows the output interface to be multiplexed preemptively easily. Message injection can be viewed in the following fashion: 1 The details of the data message constraints and layout are outside the scope of the UDM model. <p> Further discussion of buffering is deferred to Section 4.2. Send and Receive. The inject operation of the abstract model is decomposed into a two-phase process of describe and launch, as in <ref> [39] </ref>. To send a message, an application first writes all of the message data into the output message buffer starting at zero offset from the beginning of this buffer.
Reference: [40] <author> John D. Kubiatowicz. </author> <title> Integrated Message-Passing and Shared-Memory Communication in the Alewife Multiprocessor. </title> <type> PhD thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> February </month> <year> 1998. </year>
Reference-contexts: Third, distributed shared memory and message passing implementations are naturally similar at a low level, so if an implementation provides shared memory in hardware the additional cost of exposing message passing is low <ref> [40] </ref>. Finally, one reading of technology trends is that shared memory systems increasingly will be implemented in software using messages to minimize hardware [1, 31] and/or to take advantage of application-specific knowledge [62]. <p> The thesis includes a detailed description of the two techniques. The essence of the UDM programming model and the specific hardware used to virtualize user interrupts have also been described by Kubiatowicz in the context of the Alewife machine <ref> [40] </ref>. 3. We report on the implementation of a scalable workstation, FUGU, that uses the direct virtual network interface. FUGU consists of emulation-based hardware, a companion instruction-level simulator and a custom operating system. Features of the hardware have been previously described by Michelson [55] and by Lee [43]. 4. <p> The direct VNI achieves the goals of programmability, protection and performance: * The direct VNI achieves the goal of programmability through its low-level model. The low-level model used is User Direct Messages (UDM), described here as well as by Kubia-towicz <ref> [40] </ref>. The particular model is arguably desirable, as we will show, but amounts to a design choice. * The direct VNI provides protection that is compatible with multiprogramming, virtual memory and arbitrary scheduling policies, with some caveats. <p> Buffering breaks the circular dependence that caused the deadlock. The approach of detecting deadlock when it happens is based on the assumption that deadlock situations are rare. The Alewife machine provides software buffering in Network Overflow to break deadlocks in it's hardware shared memory system <ref> [40] </ref>. 4 It is insightful to compare the deadlock avoidance strategy provided by the direct VNI to other possibilities. Many systems (e.g. Active Messages [78]) define separate logical request and reply networks and handlers to solve exactly the situation depicted in Figure 3-3. <p> Support for threads could be made as fast as 10s of cycles (as in the featherweight threads proposed in <ref> [40] </ref>). With fast threads, the cost a single message handled on the buffered path would be dominated by the cost of servicing the inevitable cache miss. Applicability. Two-case delivery is the primary architectural technique that gives the direct VNI performance given the UDM model and the requirements for protection. <p> Fbufs [18] are an operating-system construct used to efficiently feed streams of data across protection domains. The UDM virtual buffering system employs similar techniques in a specialized 116 implementation to manage its buffer memory. Network overflow in Alewife <ref> [40] </ref> is a form of two-case delivery used for a restricted purpose. Alewife supports distributed shared memory in hardware using a single network. Network overflow uses software buffering to simulate infinite buffering in the network for the purpose of breaking deadlocks in the shared-memory protocol.
Reference: [41] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Ghara-chorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH Multiprocessor. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 302313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes <ref> [46, 1, 41] </ref> and high-end commercial machines [36, 42, 49, 79]. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. <p> Simple coprocessors, as in the SP-2 [72] or in *T-Voyager [2], can be viewed as a hardware implementation technique. More elaborate coprocessors, as in the CS-2 [66], Flash <ref> [41] </ref> or Typhoon [62], that are programmable need a network interface themselves.
Reference: [42] <author> James Laudon and Daniel Lenoski. </author> <title> The SGI Origin: A ccNUMA Highly Scalable Server. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 241251, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines <ref> [36, 42, 49, 79] </ref>. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [43] <author> Victor Lee. </author> <title> An Evaluation of Fugu's Network Deadlock Avoidance Strategy. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> June </month> <year> 1996. </year>
Reference-contexts: We report on the implementation of a scalable workstation, FUGU, that uses the direct virtual network interface. FUGU consists of emulation-based hardware, a companion instruction-level simulator and a custom operating system. Features of the hardware have been previously described by Michelson [55] and by Lee <ref> [43] </ref>. 4. We present an evaluation of the direct virtual network interface based on simulation and emulated hardware. The evaluation makes three basic points. <p> A complimentary mechanism (not implemented) is that the occurrence of an episode of overflow control should be treated by the scheduler as a suggestion to coschedule that application. Lee <ref> [43] </ref> experimented with stopping one process (just the apparent sending process) of an application. <p> However, the exact performance requirement of the second network in a real system is unknown. The bandwidth and latency issues described in Sections 5.1 and 5.2 are described in additional detail by Lee <ref> [43] </ref>. Second, the domain of overflow control mechanisms and policies is broad; the scheme described in Section 5.2 is only one simple possibility. Third, as mentioned in Section 5.3, we have not addressed the resource management questions in virtual buffering beyond the use of our overflow control policy. Cross-Domain Messages. <p> The TLB is 4-way associative with 128 entries. It supports user-level probe operations to support translation for DMA [55]. 2. A GID-check mechanism intended to be used during polling (not tested). 3. A rudimentary second network. The second network is a 1-bit, token-passing ring network <ref> [43] </ref>. The direct VNI approach allows the network interface hardware to remain quite small. The network interface consumes about 20K gates plus 640 bytes in compiled SRAM arrays in about 0:40mm 2 of the CMMU, an LSI 300K-series gate array. <p> to user code; the hardware GID check on polling relies on a particular access idiom in user code (the header must be touched first); DMA translation uses an unprotected TLB probe operation followed by a user-level write to the send- or receive-descriptor. 4 1 Jon Michelson [55] and Victor Lee <ref> [43] </ref> designed and implemented the UCU FPGA. Victor also built the PCB. 2 The size estimate is based on the relative areas of the network interface to the cache tags array in the CMMU. <p> Several versions of overflow control are implemented in PhOS, including the version integrated with the virtual memory system, as described in Chapter 5 and a simpler version used for experiments in Chapter 7. A third version was implemented by Lee for <ref> [43] </ref>. Since overflow control is implemented in PhOS, the mechanism is partly cooperative. As with most resource management 78 operating system. Glaze is the kernel part of the exokernel; PhOS is the library part, which is shared by all applications.
Reference: [44] <author> Walter Lee, Matthew Frank, Victor Lee, Kenneth Mackenzie, and Larry Rudolph. </author> <title> Implications of I/O for Gang Scheduled Workloads. In Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '97. </booktitle> <publisher> Springer Verlag, </publisher> <year> 1997. </year>
Reference-contexts: For instance, a cache miss to a remote shared memory location is similar to a page fault and could be handled in the same way. 17 however, network effects must be considered by a parallel scheduler. A flexible coscheduling parallel scheduler was proposed in <ref> [44] </ref> and is partly implemented in FUGU. Others are working on the same problem [22, 73]. <p> The conflict in scheduler requirements is stark in a scalable workstation where we want to run mixed workloads. A standard solution does not yet exist although a number of researchers are working on the problem <ref> [22, 73, 44] </ref>. Essentially what is required is a scheduler that works well in both interactive- and coscheduled modes. Beyond the scheduler, the virtual network interface itself needs to operate efficiently in both modes. <p> Finally, the Scheduler and Propagator (a part of the scheduler), described next, are parts of the global runtime system. 6.2.2 Scheduler The FUGU scheduler is Walter Lee's experimental flexible coscheduling scheduler that supports both conventional scheduling based on priority queues and coordinated (gang or co-) scheduling based on synchronized clocks <ref> [44] </ref>. The desired scheduling mode is applied per-application. The flexibility in the scheduler allows us to explore the tradeoffs that two-case delivery permits. <p> If the interactive demands are sufficiently low, it is clearly useful for the system to employ independent, priority-based scheduling because the disruptions to the compute-intensive application are low. If disruptions are too high, there is emerging agreement <ref> [15, 22, 3, 73, 44] </ref>, that it becomes useful to explicitly co-schedule the processes of the compute-intensive application by one means or another. Section 7.2.1 introduces a parameterized workload that pits a compute-intensive application against an interactive application.
Reference: [45] <author> Charles E. Leiserson, Aahil S. Abuhamdeh, and David C. Douglas et al. </author> <title> The Network Architecture of the Connection Machine CM-5. </title> <booktitle> In Proceedings of the Fourth Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <year> 1992. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines <ref> [45, 69, 72] </ref>. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. A scalable workstation represents one vision of the convergence of SMP, cluster and MPP goals and technologies that combines efficient communication, a scalable interconnect and multiuser support. <p> Rigid scheduling solves both the problems of isolation of data and of delay effects between individual messages. The CM-5 <ref> [45] </ref>, for instance, provides protected multiprogramming by strict gang scheduling. Application messages are isolated because only one application is active in a partition at a time. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines [16, 7, 61, 1, 56] and one commercial machine, the CM-5 <ref> [45] </ref>. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. <p> Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. A few machines, the CM-5 <ref> [45] </ref>, *T [61] and the M-machine [25] and have provided this kind of interface with protection. The CM-5 reconciles its direct interface with a restricted form of multiprogramming via par-tioning, strict gang scheduling and by context-switching the network partitions with the processors.
Reference: [46] <author> D. Lenoski, J. Laudon, K. Gharachorloo, W. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. Lam. </author> <title> The Stanford DASH Multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3):6379, </volume> <month> March </month> <year> 1992. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes <ref> [46, 1, 41] </ref> and high-end commercial machines [36, 42, 49, 79]. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques.
Reference: [47] <author> Kevin Lew, Kirk Johnson, and Frans Kaashoek. </author> <title> A Case Study of Shared-Memory and Message-Passing Implementations of Parallel Breadth-First Search: The Triangle Puzzle. </title> <booktitle> In Third DIMACS International Algorithm Implementation Challenge Workshop, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Of the three CRL applications, Barnes exhibits the finest granularity in terms of message rate (smallest T interhandler ). A fourth application, enum, solves a simple triangle puzzle game by exhaustive, breadth-first search <ref> [47] </ref>. The implementation used here is a fine-grain, data-parallel version that exchanges numerous unacknowledged short messages and synchronizes only infrequently. From Table 7-1, the ratio of T handler to T interhandler in enum is extremely high: a third of the time of the application is spent in handlers.
Reference: [48] <author> Kai Li. IVY: </author> <title> A Shared Virtual Memory System for Parallel Computing. </title> <booktitle> In Proceedings of the International Conference on Parallel Computing, </booktitle> <pages> pages 94101, </pages> <year> 1988. </year> <month> 126 </month>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [49] <author> Tom Lovett and Russell Clapp. STiNG: </author> <title> A CC-NUMA Computer System for the Commer--cial Marketplace. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 308317, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines <ref> [36, 42, 49, 79] </ref>. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [50] <author> Kenneth Mackenzie, John Kubiatowicz, Anant Agarwal, and M. Frans Kaashoek. FUGU: </author> <title> Implementing Protection and Virtual Memory in a Multiuser, Multimodel Multiprocessor. </title> <note> Technical Memo MIT/LCS/TM-503, </note> <month> October </month> <year> 1994. </year>
Reference-contexts: Demand-paged virtual memory causes a similar effects: a page fault (or a remote shared memory miss) in message handling code introduces a delay in message reception that may be intolerable to the network. We proposed a solution to the VNI problem in <ref> [50] </ref> and partly evaluated it in [51]. This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI [58], the *T family [61, 2] and the M-machine [25]. <p> Further, combining DMA with virtual memory (or with shared memory) introduces a data coherence problem because the DMA engine becomes an additional source of memory operations. We proposed a solution to the DMA problem in <ref> [50] </ref> and some elements of that solution are implemented in FUGU (Appendix A). Others have addressed DMA with virtual memory in network interfaces as well [80, 68]. * Third is the translation coherence problem. <p> Virtual memory combined with shared memory introduces coherence problem with cached translations because virtual-to-physical mappings are conventionally cached at the processors. A solution to translation coherence needn't be as efficient as data cache coherence but must be scalable. We proposed a scalable solution to translation coherence in <ref> [50] </ref>. Teller examined a number of solutions to translation coherence [75]. * Finally, there is the scheduling problem. Parallel schedulers for mixed workloads are not yet fully understood. It appears important to be able to schedule some applications with traditional, per-processor, priority-based scheduling and others with coordinated scheduling (coscheduling). <p> Large messages for bulk transfer have the different goal of high bandwidth from memory to memory. The direct VNI is compatible with efficient solutions to bulk transfer (see <ref> [50] </ref>). Appendix A describes limited extensions for bulk transfer actually implemented and used in FUGU. Source Buffering. The direct VNI applies buffering only at the receiver and largely for reasons of protection. It is possible to invoke buffering for performance reasons as well. <p> The space-available register, used to implement injectc, reflects the number of send buffer words that may be written without blocking. The buffer in our implementation is limited to 16 words; larger messages utilize an associated user-level DMA mechanism <ref> [50] </ref> (described in Appendix A). Once the message has been completely described, it is guaranteed that the network will accept it. At that point, the message is injected into the network with an atomic launch instruction whose operand reflects the length of the message. <p> The focus of the direct virtual network interface is on support for small messages. There is, however, a related bulk transfer mechanism using that uses DMA for high bandwidth. The DMA implementation follows the ideas in <ref> [50] </ref>, but is incomplete. Model. Bulk transfer is integrated with the UDM model as an option on the inject and extract operations defined in Section 3.1. <p> Copying large blocks is a poor idea that is worth some extra effort to avoid. There are a couple of possible options that fall between fast mode and the slow mode, much like cross-domain upcalls in Figure 3-5. First, as suggested in <ref> [50] </ref>, extractdma operations that specify a destination block in an inaccessible page could write to a freshly allocated page and patch up the differences afterward.
Reference: [51] <author> Kenneth Mackenzie, John Kubiatowicz, Matthew Frank, Walter Lee, Victor Lee, Anant Agar-wal, and M. Frans Kaashoek. </author> <title> Exploiting Two-Case Delivery for Fast Protected Messaging. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture, </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: Demand-paged virtual memory causes a similar effects: a page fault (or a remote shared memory miss) in message handling code introduces a delay in message reception that may be intolerable to the network. We proposed a solution to the VNI problem in [50] and partly evaluated it in <ref> [51] </ref>. This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI [58], the *T family [61, 2] and the M-machine [25]. <p> Limiting the number of outstanding requests guarantees that, if buffering 5 This section is largely the work of Matt Frank as part of <ref> [51] </ref> 107 synth-N ) sent per synchronization point and 1% scheduler skew (four processors). The fraction is measured over the whole run of the application. The synth-10 application synchronizes sufficiently often that very few messages are buffered.
Reference: [52] <author> Alan Mainwaring. </author> <title> Active Message Application Programming Interface and Communication Subsystem Organization. </title> <institution> UC Berkeley, Computer Science Department, </institution> <month> December </month> <year> 1995. </year>
Reference-contexts: The original Active Messages work was on the CM-5 and coexisted with multiprogramming only through strict gang scheduling. Subsequent work on Active Messages broadens its applicability to general multiprogramming <ref> [52] </ref> by defining indirection tables to safely map handler specifiers to handlers. UDM is closely related to Remote Queues. The RQ implementation on Alewife used a software version of user-controlled atomicity and the RQ paper outlined a hardware design in progress.
Reference: [53] <author> Olivier Maquelin, Guang R. Gao, Herbert H. J. Hum, Kevin Theobald, and Xin-Min Tian. </author> <title> Polling Watchdog: Combining Polling and Interrupts for Efficient Message Handling. </title> <booktitle> In Proceedings of the 23rd Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 179188, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Alewife supports distributed shared memory in hardware using a single network. Network overflow uses software buffering to simulate infinite buffering in the network for the purpose of breaking deadlocks in the shared-memory protocol. The Polling Watchdog <ref> [53] </ref> integrates polling and interrupts for performance improvement. The resulting programming model is interrupt-based in that application code may receive an interrupt at any point; the application cannot rely on atomicity implicit in a polling model.
Reference: [54] <author> Message Passing Interface Forum. </author> <title> MPI: A Message-Passing Interface Standard, </title> <month> June </month> <year> 1995. </year>
Reference-contexts: Simultaneously, the model must be implementable with high speed and using acceptable amounts of hardware. The tension between expressiveness and implementability leads to a tradeoff. Figure 2-1 sketches the tradeoff and loosely places some existing models in the space of the tradeoff. For instance, MPI <ref> [54] </ref> is a relatively high-level model. MPI defines synchronous send/receive operations and multicast/reduction operations as primitives. These primitives correspond to multiple messages in the network hardware. Close to the other extreme, Active Messages [78] is a low-level model with primitives that correspond closely to fundamental hardware operations.
Reference: [55] <author> Jonathan E. Michelson. </author> <title> Design and Optimization of Fugu's User Communication Unit. </title> <type> Master's thesis, </type> <institution> Massachusetts Institute of Technology, Department of Electrical Engineering and Computer Science, </institution> <month> May </month> <year> 1996. </year>
Reference-contexts: We report on the implementation of a scalable workstation, FUGU, that uses the direct virtual network interface. FUGU consists of emulation-based hardware, a companion instruction-level simulator and a custom operating system. Features of the hardware have been previously described by Michelson <ref> [55] </ref> and by Lee [43]. 4. We present an evaluation of the direct virtual network interface based on simulation and emulated hardware. The evaluation makes three basic points. <p> A translation cache (TLB) to support virtual memory. The TLB is 4-way associative with 128 entries. It supports user-level probe operations to support translation for DMA <ref> [55] </ref>. 2. A GID-check mechanism intended to be used during polling (not tested). 3. A rudimentary second network. The second network is a 1-bit, token-passing ring network [43]. The direct VNI approach allows the network interface hardware to remain quite small. <p> GID stamp is left to user code; the hardware GID check on polling relies on a particular access idiom in user code (the header must be touched first); DMA translation uses an unprotected TLB probe operation followed by a user-level write to the send- or receive-descriptor. 4 1 Jon Michelson <ref> [55] </ref> and Victor Lee [43] designed and implemented the UCU FPGA. Victor also built the PCB. 2 The size estimate is based on the relative areas of the network interface to the cache tags array in the CMMU.
Reference: [56] <author> Shubhendu S. Mukherjee, Babak Falsafi, Mark D. Hill, and David A. Wood. </author> <title> Coherent Network Interfaces for Fine-Grain Communication. </title> <booktitle> In Proceedings of the 23rd International Symposium on Computer Architecture, </booktitle> <pages> pages 247258, </pages> <month> May </month> <year> 1996. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> Although a definitive conclusion awaits further 28 research, past research indicates that direct interfaces tend to be more efficient than memory-based interfaces. Direct interfaces that can be accessed at cache speeds offer even better performance [28]. For example, the CNI paper <ref> [56] </ref> showed that a direct, cache-level interface exhibited 50% higher bandwidth than their best interface placed on the memory bus. Direct interfaces are challenging to protect without sacrificing efficiency or seriously impairing the multiprogramming model. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines <ref> [16, 7, 61, 1, 56] </ref> and one commercial machine, the CM-5 [45]. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support <ref> [56] </ref>. Memory-based interfaces, Figure 8-1b in multicomputers [6, 9, 66, 69, 72] and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers <ref> [56, 57, 80] </ref>. One simple way to perform limited buffering would be to give the hardware a small table of active page frames indexed by the GID. The hardware would perform buffer insertion into these pages automatically until the page frame was full and then cause an interrupt. <p> Others have shown that tightly-coupled, direct interfaces tend to be more efficient than indirect, memory-based interfaces <ref> [28, 56] </ref>. Thus FUGU's peak performance is high. Two-case delivery and virtual buffering provide good system performance in terms other than speed as well. Virtual buffering enables low memory consumption compared to a system that must provide a fixed amount of physical buffering per application. <p> Others have shown that tightly-coupled direct interaces and reliable message subsystems offer performance benefits to applications. Henry and Joerg [28] quantified the value of placing the message interface close to the processor and of providing hardware support for buffer management. The CNI work <ref> [56] </ref> shows the benefit of a tightly-coupled direct interface versus the best current 87 using a buffer-always policy normalized to the runtime using upcalls. The buffer-always path incurs as much as 1400 cycles of overhead per message while upcalls cost only 115 cycles per message. memory-based interface with real applications. <p> The direct VNI employs a hybrid approach to network interface design that uses both a direct interface for speed and provides buffering for convenience. Other interfaces take a similar approach, notably Wisconsin's CNI <ref> [56, 57] </ref> and the descendants of *T, *T-NG [11] and *T-Voyager [2]. Figure 8-1 (identical to Figure 2-3) gives a schematic view of different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively.
Reference: [57] <author> Shubhendu S. Mukherjee and Mark D. Hill. </author> <title> A Survey of User-Level Network Interfaces for System Area Networks. </title> <type> Technical Report 1340, </type> <institution> Computer Sciences Dept., University of Wisconsin, </institution> <month> February </month> <year> 1997. </year>
Reference-contexts: Using virtual memory is particularly natural when the 71 processor initiates all the buffering because existing support for virtual memory (e.g., the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [26, 57, 66, 80] </ref>. However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers [56, 57, 80]. <p> However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers <ref> [56, 57, 80] </ref>. One simple way to perform limited buffering would be to give the hardware a small table of active page frames indexed by the GID. The hardware would perform buffer insertion into these pages automatically until the page frame was full and then cause an interrupt. <p> The direct VNI employs a hybrid approach to network interface design that uses both a direct interface for speed and provides buffering for convenience. Other interfaces take a similar approach, notably Wisconsin's CNI <ref> [56, 57] </ref> and the descendants of *T, *T-NG [11] and *T-Voyager [2]. Figure 8-1 (identical to Figure 2-3) gives a schematic view of different approaches to message delivery. Parts (a) and (b) show direct and memory-based delivery, respectively.
Reference: [58] <author> Subhendu S. Mukherjee and Mark D. Hill. </author> <title> The Impact of Data Transfer and Buffering Alternatives on Network Interface Design. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture (HPCA-4), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: We proposed a solution to the VNI problem in [50] and partly evaluated it in [51]. This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI <ref> [58] </ref>, the *T family [61, 2] and the M-machine [25]. These projects are described as related work in Chapter 8. * Second is the DMA problem. Efficient bulk transfer through messages requires the support of Direct Memory Access (DMA) hardware or equivalent functionality provided by a coprocessor. <p> However, there is also a similar performance issue: it might be beneficial to remove messages from the network (or, symmetrically, not to inject them) just to improve traffic flow within the network. Mukherjee, et al <ref> [58] </ref>, found it beneficial to buffer messages at the receiver automatically in some applications. Direct vs. Buffered Interfaces. Message passing network interfaces developed for high-performance parallel machines have taken two general approaches: direct and memory-based. Direct interfaces allow the processor to handle messages directly out of the network. <p> Further, the timeout might be useful as a means of introducing buffer to benefit even a single application. Mukherjee notes in <ref> [58] </ref> that judicious use of buffering can improve the performance of some applications running standalone by reducing sender stall time and network congestion. The timeout in message handlers in FUGU could be used as a heuristic for detecting opportunities to improve performance through buffering. <p> A subtle difference between the CNI 16 Q m and the direct VNI as each is currently implemented is in when they choose to initiate buffering. The CNI 16 Q m apparently switches to buffering in memory whenever the small buffer in the NI is full. <ref> [58] </ref> In contrast, the direct VNI requires some other event (like a timeout) to enable a switch to software buffering. The hardware support for buffering in the CNI 16 Q m makes it relatively lower cost to switch whereas software buffering in the direct VNI is relatively more expensive.
Reference: [59] <author> Randy Osborne. </author> <title> A Hybrid Deposit Model for Low Overhead Communication in High Speed LANs. </title> <type> Technical Report 94-02v3, </type> <address> MERL, 201 Broadway, Cambridge, MA 02139, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: Remote-memory communication as in SHRIMP [6] and the DEC memory-channel [24] give a sending process the means to directly write memory in another process on another processor. More elaborate sender-based models as in Hamlyn [81, 9] and Hybrid Deposit <ref> [59] </ref> support more communication options, e.g., the ability to insert into a remote queue, but retain the idea that the sender manages the memory. The sender-based approach is interesting because it exposes the fundamental costs of memory-based communication in its programming model.
Reference: [60] <author> J. K. Ousterhout. </author> <title> Scheduling Techniques for Concurrent Systems. </title> <booktitle> In 3rd International Conference on Distributed Computing Systems, </booktitle> <pages> pages 2230, </pages> <year> 1982. </year>
Reference-contexts: Standard, priority-based scheduling addresses this requirement well [74]. The natural extension of a priority-based scheduler to a multiprocessor is to schedule processes independently on each processor of the multiprocessor. However, parallel jobs, particularly ones that perform inter-process synchronization frequently, often require some form of coscheduling for best performance <ref> [60] </ref>. The conflict in scheduler requirements is stark in a scalable workstation where we want to run mixed workloads. A standard solution does not yet exist although a number of researchers are working on the problem [22, 73, 44].
Reference: [61] <author> Gregory M. Papadopoulos, G. Andy Boughton, Robert Greiner, and Michael J. Beckerle. </author> <title> *T: Integrated Building Blocks for Parallel Computing. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 624635, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> We proposed a solution to the VNI problem in [50] and partly evaluated it in [51]. This VNI solution is implemented in FUGU and is the focus of this thesis. Other recent network interface work addresses the VNI problem with similar goals, notably CNI [58], the *T family <ref> [61, 2] </ref> and the M-machine [25]. These projects are described as related work in Chapter 8. * Second is the DMA problem. Efficient bulk transfer through messages requires the support of Direct Memory Access (DMA) hardware or equivalent functionality provided by a coprocessor. <p> Hybrid solutions will be discussed in more detail in Chapter 8. Direct network interfaces, Figure 8-1a have been used in research machines <ref> [16, 7, 61, 1, 56] </ref> and one commercial machine, the CM-5 [45]. These interfaces feature low latency by allowing the processor direct access to the network queue. Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. <p> Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect. A few machines, the CM-5 [45], *T <ref> [61] </ref> and the M-machine [25] and have provided this kind of interface with protection. The CM-5 reconciles its direct interface with a restricted form of multiprogramming via par-tioning, strict gang scheduling and by context-switching the network partitions with the processors.
Reference: [62] <author> Steve K. Reinhardt, James R. Larus, and David A. Wood. Tempest and Typhoon: </author> <title> User-Level Shared Memory. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 325336, </pages> <month> April </month> <year> 1994. </year> <month> 127 </month>
Reference-contexts: Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication <ref> [62, 1] </ref>. Clusters of workstations or SMPs interconnected by a LAN provide support for multiple users and can provide scalable cost and performance, but current clusters provide only inefficient inter-node communication. Clusters typically support only message-passing for inter-node communication in hardware and synthesize shared memory (if desired) in software. <p> Finally, one reading of technology trends is that shared memory systems increasingly will be implemented in software using messages to minimize hardware [1, 31] and/or to take advantage of application-specific knowledge <ref> [62] </ref>. Finally, a scalable workstation is made scalable by its point-to-point interconnect which allows the communication performance of the machine to scale with the number of processors in the machine. Scalability has two consequences. First, the global bandwidth of the network potentially scales up as processors are added. <p> Water and Barnes are from the SPLASH [71] suite. LU, Water and Barnes are shared-memory programs that have been modified to make use of the CRL all-software shared-memory system [30]. CRL presents a message-passing load that is representative of other coherence protocols such as Stache <ref> [62] </ref> or Shasta [65] and can be considered operating-system-like: many low-latency protocol packets mixed with larger data packets. Of the three CRL applications, Barnes exhibits the finest granularity in terms of message rate (smallest T interhandler ). <p> Simple coprocessors, as in the SP-2 [72] or in *T-Voyager [2], can be viewed as a hardware implementation technique. More elaborate coprocessors, as in the CS-2 [66], Flash [41] or Typhoon <ref> [62] </ref>, that are programmable need a network interface themselves. Either the coprocessor is an opaque hardware component and the VNI problem arises as the processor, or the coprocessor is a user-programmable entity in which case we have to think about the VNI problem at the coprocessor's interface to the network.
Reference: [63] <author> Rolf Riesen, Arthur B. Maccabe, and Stephen R. Wheat. </author> <title> Split-C and Active Messages under SUNMOS on the Intel Paragon. </title> <type> Unpublished, </type> <month> April </month> <year> 1994. </year>
Reference-contexts: Enum slows down by 65% at eight processors over the direct VNI with the slow message system, compared to the 6% projected gain possible from using unprotected hardware. The buffer-always policy has to be considered a worst case, although it's essentially what SUNMOS <ref> [63] </ref> does. This section has shown that the direct VNI offers good performance to applications running standalone. The performance with the (protected) direct VNI is close to that with an unprotected direct interface on the same hardware base. <p> Like the direct VNI, *T-Voyager overflows its queues to memory if necessary. Unlike the direct VNI, *T-Voyager can use its coprocessor to perform this work. 8.3 Miscellaneous Techniques used in our virtual buffering system are related to several other systems. The Active Message implementation in SUNMOS <ref> [63] </ref> on the Intel Paragon uses kernel code to unload the message interface and to queue messages to be handled by a user thread. The SUNMOS approach corresponds to using the software-buffered path in UDM continuously.
Reference: [64] <author> Marcel-C ff at ff alin Rosu, Karsten Schwan, and Richard Fujimoto. </author> <title> Supporting Parallel Applications on Clusters of Workstations: The Intelligent Network Interface Approach. </title> <booktitle> In Proceedings of the 6th IEEE International Symposium on High-Performance Distributed Computing, </booktitle> <year> 1997. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72].
Reference: [65] <author> Daniel J. Scales, Kourosh Gharachorloo, and Chandramohan A. Thekkath. </author> <title> Shasta: A Low Overhead, Software-Only Approach for Supporting Fine-Grain Shared Memory. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 174185, </pages> <month> October 15, </month> <year> 1996. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1]. <p> Water and Barnes are from the SPLASH [71] suite. LU, Water and Barnes are shared-memory programs that have been modified to make use of the CRL all-software shared-memory system [30]. CRL presents a message-passing load that is representative of other coherence protocols such as Stache [62] or Shasta <ref> [65] </ref> and can be considered operating-system-like: many low-latency protocol packets mixed with larger data packets. Of the three CRL applications, Barnes exhibits the finest granularity in terms of message rate (smallest T interhandler ). A fourth application, enum, solves a simple triangle puzzle game by exhaustive, breadth-first search [47].
Reference: [66] <author> Klaus E. Schauser and Chris J. Scheiman. </author> <title> Experience with Active Messages on the Meiko CS-2. </title> <booktitle> In Proceedings of the 9th International Symposium on Parallel Processing, </booktitle> <year> 1995. </year>
Reference-contexts: Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers <ref> [6, 9, 66, 69, 72] </ref> and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Using virtual memory is particularly natural when the 71 processor initiates all the buffering because existing support for virtual memory (e.g., the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [26, 57, 66, 80] </ref>. However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers [56, 57, 80]. <p> The role of a coprocessor can be interpreted two ways, either as part of the hardware or as an independent, programmable entity. Simple coprocessors, as in the SP-2 [72] or in *T-Voyager [2], can be viewed as a hardware implementation technique. More elaborate coprocessors, as in the CS-2 <ref> [66] </ref>, Flash [41] or Typhoon [62], that are programmable need a network interface themselves. <p> Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [67] <author> Ioannis Schoinas, Babak Falsafi, Alvin R. Lebeck, Steven K. Reinhardt, James R. Larus, and David A. Wood. </author> <title> Fine-grain Access Control for Distributed Shared Memory. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> October </month> <year> 1994. </year>
Reference-contexts: Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines [36, 42, 49, 79]. prototypes <ref> [48, 10, 35, 30, 65, 67, 37, 21] </ref> using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [68] <author> Ioannis Schoinas and Mark D. Hill. </author> <title> Address Translation Mechanisms in Network Interfaces. </title> <booktitle> In Proceedings of the Fourth International Symposium on High-Performance Computer Architecture (HPCA-4), </booktitle> <month> February </month> <year> 1998. </year>
Reference-contexts: We proposed a solution to the DMA problem in [50] and some elements of that solution are implemented in FUGU (Appendix A). Others have addressed DMA with virtual memory in network interfaces as well <ref> [80, 68] </ref>. * Third is the translation coherence problem. Virtual memory combined with shared memory introduces coherence problem with cached translations because virtual-to-physical mappings are conventionally cached at the processors. A solution to translation coherence needn't be as efficient as data cache coherence but must be scalable.
Reference: [69] <author> Steven L. Scott. </author> <title> Synchronization and Communication in the T3E Multiprocessor. </title> <booktitle> In Seventh International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 2636, </pages> <month> October </month> <year> 1996. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines <ref> [45, 69, 72] </ref>. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. A scalable workstation represents one vision of the convergence of SMP, cluster and MPP goals and technologies that combines efficient communication, a scalable interconnect and multiuser support. <p> Other researchers take the same view [9, 25]. MPP manufacturers have found it feasible to build reliable networks for machines with several hundreds of nodes <ref> [69] </ref>. Placing the network in one cabinet avoids most of the practical causes of failures, e.g., those due to unpluggings, independent power supply failures, cable damage and electromagnetic interference. Protection. <p> Dealing with unreliability adds the extra costs of buffering copies at the sender and of extra traffic to manage the copies. NACKs introduce complexity into the network interface, add traffic and overhead for buffer management and require a reserved back-path. The return-to-sender strategy, used in the T3E <ref> [69] </ref> and the M-machine [25] is a form of NACK that avoids much of the common-case buffer overhead. Return-to-sender assumes a reliable network. Both dropping and NACKing have the effect of increasing network demand under load. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers <ref> [6, 9, 66, 69, 72] </ref> and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [70] <author> C.L. Seitz, N.J. Boden, J. Seizovic, and W.K. Su. </author> <title> The Design of the Caltech Mosaic C Multicomputer. </title> <booktitle> In Research on Integrated Systems Symposium Proceedings, </booktitle> <pages> pages 122, </pages> <address> Cambridge, MA, 1993. </address> <publisher> MIT Press. </publisher>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes <ref> [70, 7, 16, 1, 61, 2, 56] </ref> and as commercial machines [45, 69, 72]. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. <p> The Active Messages work [78] gave a name to this style of model which appeared earlier in Mosaic <ref> [70] </ref>, the J-machine [16] and others. UDM is similar to Active Messages and related to Remote Queues (RQ) [8]. UDM shares the Active Message goal of providing a minimal building block and uses the same convention of specifying a handler by a raw procedure address. <p> Protected Direct Interfaces. The direct VNI in FUGU builds on work in direct network interfaces in a number of previous machines <ref> [70, 16, 7, 61, 1, 45, 25] </ref>. Direct interfaces allow and require the processor to handle messages directly out of the network with minimal buffering. The direct approach maps naturally to a programmable, low-level messaging model and has high performance but has been difficult to protect.
Reference: [71] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford Parallel Applications for Shared-Memory. </title> <booktitle> Computer Architecture News, </booktitle> <pages> pages 544, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Compute-Intensive Applications. Three applications, LU, Water and Barnes are standard scientific benchmarks. Water and Barnes are from the SPLASH <ref> [71] </ref> suite. LU, Water and Barnes are shared-memory programs that have been modified to make use of the CRL all-software shared-memory system [30].
Reference: [72] <author> Marc Snir and Peter Hochschild. </author> <title> The Communication Software and Parallel Environment of the IBM SP-2. </title> <type> Technical Report IBM-RC-19812, </type> <institution> IBM, IBM Research Center, </institution> <address> Yorktown Heights, NY, </address> <month> January </month> <year> 1995. </year>
Reference-contexts: Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines <ref> [45, 69, 72] </ref>. However, MPP work has largely ignored issues of mixed workloads that require multiprogramming, demand paging and interactive scheduling. A scalable workstation represents one vision of the convergence of SMP, cluster and MPP goals and technologies that combines efficient communication, a scalable interconnect and multiuser support. <p> However, buffering only postpones the undeliverability problem unless the buffer is effectively infinite in size or the total buffer space required by the workload is provably limited by some means. The SP-2 takes the effectively infinite approach by providing a very large (8MB) physical buffer <ref> [72] </ref>. Infinite buffering offers the lowest overhead provided the real costs of buffering can be kept low. The next section talks about the performance cost of providing support for buffering in hardware. Undeliverability is described here in terms of protection for correctness but there is also a related performance issue. <p> Direct NIs can be inefficient unless placed close to the processor. Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers <ref> [6, 9, 66, 69, 72] </ref> and workstations [17, 19, 76, 77] provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. <p> Guaranteed delivery allows the fast path to avoid all buffer management overhead (pointer manipulation and acknowledgment messages). Guaranteed delivery requires a reliable network and unlimited buffering. While some systems have provided effectively unlimited buffering by providing large amounts of physical buffering (e.g., the SP-2 <ref> [72] </ref>), pinning down physical memory is inconvenient in general, particularly in a multiprogrammed system where it is desirable to be able to guarantee some minimum amount of buffering to each application. This chapter has described the first architectural technique, two-case delivery, used to support the direct VNI. <p> The role of a coprocessor can be interpreted two ways, either as part of the hardware or as an independent, programmable entity. Simple coprocessors, as in the SP-2 <ref> [72] </ref> or in *T-Voyager [2], can be viewed as a hardware implementation technique. More elaborate coprocessors, as in the CS-2 [66], Flash [41] or Typhoon [62], that are programmable need a network interface themselves. <p> Second, a bulk transfer mechanism for coarse-grain computing is widely accepted as beneficial while fine-grain mechanisms remain controversial. Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers <ref> [6, 9, 66, 69, 72, 24] </ref> and workstations [17, 19, 76, 77] limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [73] <author> P. G. Sobalvarro and W. E. Weihl. </author> <title> Demand-based Coscheduling of Parallel Jobs on Multi-programmed Multiprocessors. </title> <booktitle> In Lecture Notes in Computer Science, </booktitle> <volume> 949, </volume> <pages> pages 106126. </pages> <publisher> Springer Verlag, </publisher> <year> 1995. </year> <title> Workshop on Parallel Job Scheduling, </title> <booktitle> IPPS '95. </booktitle>
Reference-contexts: A flexible coscheduling parallel scheduler was proposed in [44] and is partly implemented in FUGU. Others are working on the same problem <ref> [22, 73] </ref>. <p> The conflict in scheduler requirements is stark in a scalable workstation where we want to run mixed workloads. A standard solution does not yet exist although a number of researchers are working on the problem <ref> [22, 73, 44] </ref>. Essentially what is required is a scheduler that works well in both interactive- and coscheduled modes. Beyond the scheduler, the virtual network interface itself needs to operate efficiently in both modes. <p> If the interactive demands are sufficiently low, it is clearly useful for the system to employ independent, priority-based scheduling because the disruptions to the compute-intensive application are low. If disruptions are too high, there is emerging agreement <ref> [15, 22, 3, 73, 44] </ref>, that it becomes useful to explicitly co-schedule the processes of the compute-intensive application by one means or another. Section 7.2.1 introduces a parameterized workload that pits a compute-intensive application against an interactive application.
Reference: [74] <author> Andrew S. Tanenbaum. </author> <title> Modern Operating Systems. </title> <publisher> Prentice-Hall, </publisher> <year> 1992. </year>
Reference-contexts: A scalable workstation requires a system scheduler that caters to needs of both interactive, response-time-sensitive applications and parallel, synchronization-intensive applications. A system with an interactive workload needs a scheduler that provides good response time to interactive events. Standard, priority-based scheduling addresses this requirement well <ref> [74] </ref>. The natural extension of a priority-based scheduler to a multiprocessor is to schedule processes independently on each processor of the multiprocessor. However, parallel jobs, particularly ones that perform inter-process synchronization frequently, often require some form of coscheduling for best performance [60]. <p> When the job is released from overflow mode, all processes in the job return to normal mode. in the message buffer. Note that the high and low water mark in the paging system represent an ordinary implementation of virtual memory management <ref> [74] </ref>.
Reference: [75] <author> Patricia Jane Teller. </author> <title> Translation-Lookaside Buffer Consistency in Highly-Parallel Shard-Memory Multiprocessors. </title> <type> Technical Report RC 16858, </type> <institution> IBM, </institution> <address> Yorktown Heights, NY, </address> <month> May </month> <year> 1991. </year>
Reference-contexts: A solution to translation coherence needn't be as efficient as data cache coherence but must be scalable. We proposed a scalable solution to translation coherence in [50]. Teller examined a number of solutions to translation coherence <ref> [75] </ref>. * Finally, there is the scheduling problem. Parallel schedulers for mixed workloads are not yet fully understood. It appears important to be able to schedule some applications with traditional, per-processor, priority-based scheduling and others with coordinated scheduling (coscheduling).
Reference: [76] <author> Chandramohan A. Thekkath, Henry M. Levy, and Edward D. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <type> Technical Report UW-CSE-93-04-03, </type> <institution> University of Washington, </institution> <address> Seattle, WA, </address> <month> April </month> <year> 1993. </year> <month> 128 </month>
Reference-contexts: Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers [6, 9, 66, 69, 72] and workstations <ref> [17, 19, 76, 77] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately. <p> Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers [6, 9, 66, 69, 72, 24] and workstations <ref> [17, 19, 76, 77] </ref> limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [77] <author> Thorsten von Eicken, Anindya Basu, Vineet Buch, and Werner Vogels. U-Net: </author> <title> A User-Level Network Interface for Parallel and Distributed Computing. </title> <booktitle> In Proceedings of the Fifteenth ACM Symposium on Operating Systems Principles, </booktitle> <month> December </month> <year> 1995. </year>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72]. <p> Anticipating continued system integration, we place our NI on the processor-cache bus. The CNI paper showed how to partly compensate for a more distant NI by exploiting standard cache-coherence support [56]. Memory-based interfaces, Figure 8-1b in multicomputers [6, 9, 66, 69, 72] and workstations <ref> [17, 19, 76, 77] </ref> provide easy protection for multiprogramming if the NI also demultiplexes messages into per-process buffers. Automatic hardware buffering also deals well with sinking bursts of messages and provides the lowest overhead (by avoiding the processors) when messages are not handled immediately. <p> Karamcheti and Chien [33] found that 50-70% of active message overhead in the CM-5 was attributable to the costs of providing flow control, ordering and reliability in software. The uNet paper <ref> [77] </ref> suggests that buffer management overhead would cost 6uS/message on their Sparc-10-based system. 6uS is considered relatively low extra overhead (10%) in their system but amounts to hundreds of cycles. <p> Since efficient bulk transfer requires DMA, it has been natural to make the network interface design memory-centric with support for small messages a secondary consideration. Memory-based interfaces in multicomputers [6, 9, 66, 69, 72, 24] and workstations <ref> [17, 19, 76, 77] </ref> limit the performance of the interface to the speed of memory, but provide easy protection for multiprogramming if the network interface also demultiplexes messages into per-process buffers.
Reference: [78] <author> Thorsten von Eicken, David E. Culler, Seth Copen Goldstein, and Klaus Erik Schauser. </author> <title> Active Messages: A Mechanism for Integrated Communication and Computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256266, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: For instance, MPI [54] is a relatively high-level model. MPI defines synchronous send/receive operations and multicast/reduction operations as primitives. These primitives correspond to multiple messages in the network hardware. Close to the other extreme, Active Messages <ref> [78] </ref> is a low-level model with primitives that correspond closely to fundamental hardware operations. Each user message in Active Messages corresponds to a single hardware message 22 through the network which invokes a user handler at the destination. <p> More important than emulation, a low-level model offers the promise that network traffic and protocol overhead may be reduced over that required by a higher-level model by programmer specialization <ref> [78] </ref> or through automatic, compile-time analysis and specialization [23, 34]. An ideal low-level model provides a complete set of communication operations and exposes fundamental costs. The programmer is thus given the ability to craft communication protocols tailored to the application and to minimize communication costs using application-specific knowledge. <p> UDM primitives correspond one-to-one with network hardware primitives and UDM is thus programmable in that it exposes the fundamental costs of the hardware. The novelty of UDM with respect to other low-level models (such as Active Messages <ref> [78] </ref>) is that UDM defines the control transfer mechanisms along with the data transfer mechanisms as part of the model. UDM includes both polling and a user-level interrupt for message delivery notification. <p> The Alewife machine provides software buffering in Network Overflow to break deadlocks in it's hardware shared memory system [40]. 4 It is insightful to compare the deadlock avoidance strategy provided by the direct VNI to other possibilities. Many systems (e.g. Active Messages <ref> [78] </ref>) define separate logical request and reply networks and handlers to solve exactly the situation depicted in Figure 3-3. The two logical networks are can be two actual, physical networks or just one network with two priorities where reply messages use the higher priority. <p> UDM provides kernel-like control over interrupts and the ability to treat the network as private, reliable and with unbounded buffering. UDM is powerful enough to implement the efficient Active Messages <ref> [78] </ref> and Remote Queues [8] models directly. * Protection is enabled because the combination of two-case delivery and virtual buffering solves the undeliverable message problem (Section 2.2), permitting protected multiprogramming and demand-paged virtual memory. 83 * Performance in terms of speed comes from the direct interface. <p> The Active Messages work <ref> [78] </ref> gave a name to this style of model which appeared earlier in Mosaic [70], the J-machine [16] and others. UDM is similar to Active Messages and related to Remote Queues (RQ) [8].
Reference: [79] <author> Wolf-Dietrich Weber, Stephen Gold, Pat Helland, Takeshi Shimizu, Thomas Wicki, and Win-fried Wilcke. </author> <title> The Mercury Intercoonect Architecture: A Cost-Effective Infrastructure for High-Performance Servers. </title> <booktitle> In Proceedings of the 24th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 98107, </pages> <month> June </month> <year> 1997. </year>
Reference-contexts: The solution is to replace the bus with a point-to-point network and to synthesize shared memory communication by using hardware or software to communicate via messages over this network. Hardware solutions using directory-based cache coherence have been 14 demonstrated in research prototypes [46, 1, 41] and high-end commercial machines <ref> [36, 42, 49, 79] </ref>. prototypes [48, 10, 35, 30, 65, 67, 37, 21] using a variety of techniques. Future solutions are likely to make use of judicious combinations of hardware and software support in hybrid machines that support both shared memory and message passing communication [62, 1].
Reference: [80] <author> Matt Welsh, Anindya Basu, and Thorsten von Eicken. </author> <title> Incorporating memory management into user-level network interfaces. </title> <booktitle> In Proceedings of Hot Interconnects IV, </booktitle> <year> 1997. </year>
Reference-contexts: We proposed a solution to the DMA problem in [50] and some elements of that solution are implemented in FUGU (Appendix A). Others have addressed DMA with virtual memory in network interfaces as well <ref> [80, 68] </ref>. * Third is the translation coherence problem. Virtual memory combined with shared memory introduces coherence problem with cached translations because virtual-to-physical mappings are conventionally cached at the processors. A solution to translation coherence needn't be as efficient as data cache coherence but must be scalable. <p> Using virtual memory is particularly natural when the 71 processor initiates all the buffering because existing support for virtual memory (e.g., the processor's TLB) is reused. It requires a relatively complex DMA engine or coprocessor to manipulate virtual memory independently <ref> [26, 57, 66, 80] </ref>. However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers [56, 57, 80]. <p> However, virtual buffering is usable in any system that employs buffering. For instance, a system that performs limited buffering in hardware could implement virtual buffering by using interrupts to dynamically expand the buffers <ref> [56, 57, 80] </ref>. One simple way to perform limited buffering would be to give the hardware a small table of active page frames indexed by the GID. The hardware would perform buffer insertion into these pages automatically until the page frame was full and then cause an interrupt.
Reference: [81] <author> John Wilkes. </author> <title> Hamlyn An Interface for Sender-Based Communications. </title> <type> Department Technical Report HPL-OSR-92-13, </type> <institution> HP Labs OS Research, </institution> <month> November </month> <year> 1992. </year> <month> 129 </month>
Reference-contexts: Current work in cluster NIs seeks to reduce protocol overhead, to tighten the integration of the NI with the processors and to take advantage of the characteristics of the so-called System Area Network (SAN) environment <ref> [77, 64, 81, 6, 19, 17, 29, 13] </ref>. Higher performance network interfaces suitable for significantly finer-grain parallel problems have been demonstrated in massively-parallel processors as research prototypes [70, 7, 16, 1, 61, 2, 56] and as commercial machines [45, 69, 72]. <p> Remote-memory communication as in SHRIMP [6] and the DEC memory-channel [24] give a sending process the means to directly write memory in another process on another processor. More elaborate sender-based models as in Hamlyn <ref> [81, 9] </ref> and Hybrid Deposit [59] support more communication options, e.g., the ability to insert into a remote queue, but retain the idea that the sender manages the memory. The sender-based approach is interesting because it exposes the fundamental costs of memory-based communication in its programming model.
References-found: 81

