URL: http://www-robotics.usc.edu/~maja/publications/aaai97-my.ps.gz
Refering-URL: http://www-robotics.usc.edu/~maja/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: maja@cs.brandeis.edu  
Title: Using Communication to Reduce Locality in Multi-Robot Learning  
Author: Maja J Mataric 
Address: Waltham, MA 02254  
Affiliation: Volen Center for Complex Systems Computer Science Department Brandeis University  
Abstract: This paper attempts to bridge the fields of machine learning, robotics, and distributed AI. It discusses the use of communication in reducing the undesirable effects of locality in fully distributed multi-agent systems with multiple agents/robots learning in parallel while interacting with each other. Two key problems, hidden state and credit assignment, are addressed by applying local undirected broadcast communication in a dual role: as sensing and as reinforcement. The methodology is demonstrated on two multi-robot learning experiments. The first describes learning a tightly-coupled coordination task with two robots, the second a loosely-coupled task with four robots learning social rules. Communication is used to share sensory data to overcome hidden state and reinforcement to overcome the credit assignment problem between the agents and to bridge the gap between local and global payoff. 1 
Abstract-found: 1
Intro-found: 1
Reference: <author> Altenburg, K. & Pavicic, M. </author> <year> (1993), </year> <title> Initial Results in the Use of Inter-Robot Communication for a Multiple, Mobile Robotic System, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 96-100. </pages>
Reference: <author> Axelrod, R. </author> <year> (1984), </year> <title> The Evolution of Cooperation, </title> <publisher> Basic Books, </publisher> <address> NY. </address>
Reference-contexts: Yet, from the multi-agent perspective, the ability to sense and correctly distinguish members of one's group from others, from obstacles and from various features in the the environment is crucial for most tasks. While the inability to make such distinctions does not preclude symbiotic relationships <ref> (Axelrod 1984) </ref>, the lack of sophisticated perceptual discrimination is a critical limitation in multi-robot work. Non-visual sensors such as infra-red, contact sensors and sonars are all of limited use in the social recognition task. <p> Thus, it is difficult for individual agents to learn such rules without having them centrally imposed or pre-programmed, as advocated in some game-theoretic work <ref> (Axelrod 1984) </ref>. Our experiments demonstrated that through the use of simple communication between spatially local agents, the described social rules can be learned. The communication channel was used to provide payoff, i.e., generate reinforcement, for observing the behavior of the near-by agent and mimicking it.
Reference: <author> Donald, B. R., Jennings, J. & Rus, D. </author> <year> (1993), </year> <title> Information Invariants for Cooperating Autonomous Mobile Robots, </title> <booktitle> in `Proc. International Symposium on Robotics Research', </booktitle> <address> Hidden Valley, PA. </address>
Reference: <author> Dudek, G., Jenkin, M., Milios, E. & Wilkes, D. </author> <year> (1993), </year> <title> On the utility of multi-agent autonomous robot systems, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 101-108. </pages>
Reference: <author> Horswill, I. D. </author> <year> (1993), </year> <title> Specialization of Perceptual Processes, </title> <type> PhD thesis, </type> <institution> MIT. </institution>
Reference: <author> Littman, M. L. </author> <year> (1994), </year> <title> Markov games as a framework for multi-agent reinforcement learning, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> `Proceedings of the Eleventh International Conference on Machine Learning (ML-94)', </booktitle> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> New Brunswick, NJ, </address> <pages> pp. 157-163. </pages>
Reference: <author> Mahadevan, S. & Connell, J. </author> <year> (1991), </year> <title> Automatic Programming of Behavior-based Robots using Reinforcement Learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pitts-burgh, PA, </address> <pages> pp. 8-14. </pages>
Reference: <author> Mahadevan, S. & Kaelbling, L. P. </author> <year> (1996), </year> <booktitle> `The National Science Foundation Workshop on Reinforcement Learning', AI Magazine 17(4), </booktitle> <pages> 89-97. </pages>
Reference-contexts: In DAI learning work, several related contributions have been made in the area of studying multiple simulated reinforcement learners (see collection by Weiss & Sen (1996)), but have not yet been applied to robotics, where multi-robot learning was identified as an area to be addressed <ref> (Mahadevan & Kaelbling 1996) </ref>.
Reference: <author> Mataric, M. J. </author> <year> (1994), </year> <title> Reward Functions for Accelerated Learning, </title> <editor> in W. W. Cohen & H. Hirsh, eds, </editor> <booktitle> `Proceedings of the Eleventh International Conference on Machine Learning (ML-94)', </booktitle> <publisher> Morgan Kauffman Publishers, Inc., </publisher> <address> New Brunswick, NJ, </address> <pages> pp. 181-189. </pages>
Reference-contexts: In contrast, our work uses communication to compensate for the robots' sensory limitations and to facilitate learning. Our previous work <ref> (Mataric 1994) </ref> treated learning in a multi-robot system, and focused on using internal progress estimators to address the temporal credit assignment problem on each agent individually, without the use of communication.
Reference: <author> Mataric, M. J. </author> <year> (1995), </year> <title> `Designing and Understanding Adaptive Group Behavior', </title> <booktitle> Adaptive Behavior 4(1), </booktitle> <pages> 50-81. </pages>
Reference-contexts: The experiment was set up within a reinforcement learning (RL) framework, so that each of the robots was learning a mapping between its sensory perceptions and its set of pre-programmed fixed-duration basis behaviors <ref> (Mataric 1995) </ref>: find-box, push-forward, push-left, push-right, stop, send-msg. The sensory data was based on the whiskers (contact or no contact) and the light sensors (the location of the maximum brightness along the 5-sensor array).
Reference: <author> Mataric, M. J. </author> <year> (1996), </year> <title> `Reinforcement Learning in the Multi-Robot Domain', </title> <booktitle> Autonomous Robots 4(1), </booktitle> <pages> 73-83. </pages>
Reference-contexts: Most of the commonly used sen sors provide noisy data and are difficult to accurately characterize and model, presenting a major challenge for real-time robot learning <ref> (Mataric 1996) </ref>. Yet, from the multi-agent perspective, the ability to sense and correctly distinguish members of one's group from others, from obstacles and from various features in the the environment is crucial for most tasks.
Reference: <author> McCallum, A. R. </author> <year> (1996), </year> <title> Learning to use selective attention and short-term memory in sequential tasks, </title> <editor> in P. Maes, M. Mataric, J.-A. Meyer, J. Pollack & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats 4: Proceedings of the Fourth International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> The MIT Press, </publisher> <pages> pp. 315-324. </pages>
Reference-contexts: Related Work The described work applies concepts from machine learning (ML) and distributed AI (DAI) to robotics. The problems of hidden state and credit assignment have been dealt with extensively in the ML literature <ref> (McCallum 1996, Sutton 1992) </ref>. Whitehead (1991) analyzed cooperative RL mechanisms and Littman (1994) used the Markov games framework for simulated RL soccer-playing agents.
Reference: <author> Parker, L. E. </author> <year> (1993), </year> <title> Learning in Cooperative Robot Teams, </title> <booktitle> in `Proceedings, IJCAI-93 Workshop on Dynamically Interacting Robots', </booktitle> <address> Chambery, France, </address> <pages> pp. 12-23. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1992), </year> <title> Machine Learning, Special Issue on Reinforcement Learning, </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Tan, M. </author> <year> (1993), </year> <title> Multi-Agent Reinforcement Learning: Independent vs. </title> <booktitle> Cooperative Agents, in `Proceedings, Tenth International Conference on Machine Learning', </booktitle> <address> Amherst, MA, </address> <pages> pp. 330-337. </pages>
Reference: <author> Weiss, G. & Sen, S. </author> <year> (1996), </year> <title> Adaptation and Learning in Multi-Agent Systems, </title> <booktitle> Lecture Notes in Artificial Intelligence, </booktitle> <volume> Vol. 1042, </volume> <publisher> Springer-Verlag. </publisher>
Reference: <author> Whitehead, S. D. </author> <year> (1991), </year> <title> A complexity analysis of cooperative mechanisms in reinforcement learning, </title> <booktitle> in `Proceedings, AAAI-91', </booktitle> <address> Pittsburgh, PA. </address>
Reference: <author> Whitehead, S. D. & Ballard, D. H. </author> <year> (1990), </year> <title> Active Perception and Reinforcement Learning, </title> <booktitle> in `Proceedings, Seventh International Conference on Machine Learning', </booktitle> <address> Austin, Texas. </address>
Reference-contexts: Some effective low-overhead approaches to robot vision have been implemented on single robots (e.g., Horswill (1993)) but have not yet been scaled up to groups. The inability to obtain sufficient sensory information to properly discriminate results in perceptual aliasing or the hidden state problem <ref> (Whitehead & Bal-lard 1990) </ref>. Due to sensory limitations, multiple world states are perceived as the same input state, inducing serious problems for learning in any domain.
Reference: <author> Yanco, H. & Stein, L. A. </author> <year> (1993), </year> <title> An Adaptive Communication Protocol for Cooperating Mobile Robots, </title> <editor> in J.-A. Meyer, H. Roitblat & S. Wilson, eds, </editor> <booktitle> `From Animals to Animats: International Conference on Simulation of Adaptive Behavior', </booktitle> <publisher> MIT Press, </publisher> <pages> pp. 478-485. </pages>
References-found: 19

