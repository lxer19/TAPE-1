URL: ftp://ftp.cc.gatech.edu/pub/coc/tech_reports/1995/GIT-CC-95-02.ps.Z
Refering-URL: http://www.cs.gatech.edu/computing/Database/students/kiran/kiran.html
Root-URL: 
Email: e-mail: fkiran,edwardo,shamg@cc.gatech.edu  
Title: The Impact of Data Placement Strategies on Reorganization Costs in Parallel Databases  
Author: Kiran J. Achyutuni Edward Omiecinski Shamkant B. Navathe 
Date: October 29, 1994  
Address: Atlanta, GA 30332  
Affiliation: College of Computing Georgia Institute of Technology  
Pubnum: Technical Report No. GIT-CC-95/02  
Abstract: In this paper, we study the data placement problem from a reorganization point of view. Effective placement of the declustered fragments of a relation is crucial to the performance of parallel database systems having multiple disks. Given the dynamic nature of database systems, the optimal placement of fragments will change over time and this will necessitate a reorganization in order to maintain the performance of the database system at acceptable levels. This study shows that the choice of data placement strategy can have a significant impact on the reorganization costs. Until now, data placement heuristics have been designed with the principal purpose of balancing the load. However, this paper shows that such a policy can be beneficial only in the short term. Long term database designs should take reorganization costs into consideration while making design choices.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> G. Copeland, W. Alexander, E. Boughter, and T. Keller. </author> <title> Data placement in Bubba. </title> <booktitle> In ACM SIGMOD, </booktitle> <year> 1989. </year>
Reference-contexts: After a relation has been declustered, decisions must be made regarding the number of disks over which the relation will be distributed, and the particular disks on which to place the data <ref> [1] </ref>. In full declustering, a relation is declustered across all the available disks, whereas in no declustering, the entire relation is placed on a single disk. In many situations, less than full declustering outperforms both no declustering and full declustering. In this case, data placement is a non-trivial problem [1]. <p> data <ref> [1] </ref>. In full declustering, a relation is declustered across all the available disks, whereas in no declustering, the entire relation is placed on a single disk. In many situations, less than full declustering outperforms both no declustering and full declustering. In this case, data placement is a non-trivial problem [1]. Effective data placement is crucial to the performance of database systems having multiple disks. <p> A fundamental parameter that directly controls the performance of online reorganization is the amount of data to be reorganized. In this paper, we seek the relationship between data placement strategies and the amount of data to be reorganized. We consider four data placement strategies classical Bubba placement strategy <ref> [1] </ref>, an optimization to the classical Bubba called Bubba Opt1, a hash placement strategy, and a hybrid hash placement strategy. The data placement strategies are compared with the following metric: the amount of data to be reorganized (also referred to as the reorganization cost). <p> In section 6, we develop and validate the analytical models that describe the relationship. Finally, the conclusions are made in section 7. 2 Related Work In <ref> [1] </ref>, the authors introduce the Bubba placement strategy, which places relation fragments in descending order of their weights, and selecting the least loaded disk in every step. The initial placement for the relations is determined by applying the Bubba placement strategy for all the relations simultaneously. <p> After the degree of allocation for each of the TPC-C relations has been determined, they are placed on the disks using the Bubba placement strategy <ref> [1] </ref>. When queries make requests for data, requests are queued at the disks which hold the relation fragments. Table 3 shows the number of items requested by each query type from various relations. <p> Classical Bubba Bubba's placement strategy <ref> [1] </ref> uses the following simple algorithm: place hash buckets on disks in the descending order of their weights, and choose the disk with the least cumulative weight in every step. 2.
Reference: [2] <author> D. J. DeWitt and J. Gray. </author> <title> Parallel database systems: The future of high performance database systems. </title> <journal> Comm. ACM, </journal> <volume> 35(6) </volume> <pages> 85-98, </pages> <month> June </month> <year> 1992. </year>
Reference-contexts: When reorganization is performed without taking the database off-line or quiescing the transactions, it is called concurrent reorganization or on-line reorganization. On-line reorganization has been identified as a challenging problem by the database community <ref> [14, 17, 2, 15, 16] </ref>. The single most compelling reason to do on-line reorganization is the availability of the database during the reorganization. The conventional approach to reorganization is to take the database o*ine.
Reference: [3] <author> J. Gray. </author> <title> The Benchmark Handbook for Database and Transaction Processing Systems. </title> <publisher> Morgan Kaufmann Publishers, Inc, </publisher> <year> 1991. </year>
Reference-contexts: For the simulation study, we chose the 3 Table Name Initial Size after Size 3 months WAREHOUSE 0.089 0.089 DISTRICT 0.950 0.950 CUSTOMER 19650 19650 HISTORY 1380 45450 ORDER 720 23850 NEW-ORDER 72 72 ORDER-LINE 16200 238482 STOCK 30600 30600 ITEM 8200 8200 TPC (Transaction Processing Council) Benchmark C (TPC-C) <ref> [3] </ref>, which models a medium complexity online transaction processing workload.
Reference: [4] <author> K. A. Hua and C. Lee. </author> <title> An adaptive data placement scheme for parallel database computer systems. </title> <booktitle> In Proceedings of the 16th VLDB Conference, </booktitle> <pages> pages 493-506, </pages> <year> 1990. </year>
Reference-contexts: But the degree of allocation for the relation remains unchanged. In this paper, we follow a similar method, i.e., we apply the placement strategy to one relation at a time, but systematically vary the degree of allocation and measure the reorganization cost. In <ref> [4] </ref>, the authors present an adaptive data placement scheme to rebalance the disks when the disks become imbalanced due to insertions and deletions. Their scheme tries to minimize the data movement by retaining as many fragments on the disks as possible.
Reference: [5] <author> M.Y. Kim. </author> <title> Synchronized disk interleaving. </title> <journal> IEEE Trans. Computers, </journal> <volume> C-35(11), </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Declustering is a technique that partitions a file and spreads it across many disks <ref> [5] </ref>, [11]. Performance studies have shown that, except under extremely high utilization conditions, declustering is consistently a better approach than placing the entire relation on a single disk [9], [10].
Reference: [6] <author> M. Kitsuregawa and Y. Ogawa. </author> <title> Bucket spreading parallel hash: A new, robust, parallel hash join method for data skew in the super database computer. </title> <booktitle> In Proc. 16th Intl. Conf. VLDB, </booktitle> <pages> pages 210-220, </pages> <month> August </month> <year> 1990. </year> <month> 23 </month>
Reference-contexts: to the number of tuples in bucket i. (It is important to note that it is not essential for a relation to be hash declustered for the study in this paper to be valid.) It is reasonable to assume that the hash bucket weights have a Zipf-like distribution with parameter <ref> [6] </ref> (see Figure 7 Transaction with INITIAL DEGREE with FINAL DEGREE OF ALLOCATION OF ALLOCATION Mixed 2.0 6.33 New Order 2.0 11.94 Payment 1.76 0.486 Order Status 0.84 1.24 Delivery 2.60 1.73 Stock Level 50.89 191.3 with two different degree of allocations. 7). 1.
Reference: [7] <author> D. E. Knuth. </author> <title> The Art of Computer Programming: Volume 3. Sorting and Searching. </title> <publisher> Addison-Wesley, </publisher> <year> 1973. </year>
Reference-contexts: Since the bucket weights have a Zipf-like distribution, we have, after normalization, p (M; N; i; ) = Z (M; i; ) fl k=1 Z (M;k;) where Z (M; k; ) is the weight of bucket k, and belongs to the Zipf-like distribution is given by <ref> [7] </ref> Z (M; k; ) = H M fl k 1 where H (1) M is the harmonic number of order (1).
Reference: [8] <author> S. T. Leutenegger and D. Dias. </author> <title> A modeling study of the tpc-c benchmark. </title> <booktitle> ACM SIGMOD 1993 Conference Proceedings, </booktitle> <year> 1993. </year>
Reference-contexts: The workload also includes a join query. In <ref> [8] </ref>, the authors make a modeling study of the TPC-C benchmark and determine the access skew for the relations, and the access pattern of the queries for the various relations. There are 9 relations and 5 query types in the TPC-C benchmark.
Reference: [9] <author> M. Livny, S. Khoshafian, and H. Boral. </author> <title> Multi-disk management algorithms. </title> <booktitle> In ACM SIGMETRICS, </booktitle> <pages> pages 69-77, </pages> <year> 1987. </year>
Reference-contexts: 1 Introduction Declustering is a technique that partitions a file and spreads it across many disks [5], [11]. Performance studies have shown that, except under extremely high utilization conditions, declustering is consistently a better approach than placing the entire relation on a single disk <ref> [9] </ref>, [10]. After a relation has been declustered, decisions must be made regarding the number of disks over which the relation will be distributed, and the particular disks on which to place the data [1]. <p> In our experiments, we chose multiples of bucket 7 to have the largest weights. The bad case load variance is shown in 9. To put these results into perspective, we quote Livny, Khoshafian, and Boral from their paper <ref> [9] </ref>: "In the recent International Workshop on High Performance Transaction Processing, a United Airlines representative reported a differential of only 1% between the busiest and most idle disks. This is achieved by hand tuning the data placement from day to day".
Reference: [10] <author> A. L. N. Reddy and P. Banerjee. </author> <title> An evaluation of multiple-disk I/O systems. </title> <journal> IEEE Trans. Computers, </journal> <volume> 38(12) </volume> <pages> 1680-1690, </pages> <month> december </month> <year> 1989. </year>
Reference-contexts: 1 Introduction Declustering is a technique that partitions a file and spreads it across many disks [5], [11]. Performance studies have shown that, except under extremely high utilization conditions, declustering is consistently a better approach than placing the entire relation on a single disk [9], <ref> [10] </ref>. After a relation has been declustered, decisions must be made regarding the number of disks over which the relation will be distributed, and the particular disks on which to place the data [1].
Reference: [11] <author> K. Salem and H. Garcia-Molina. </author> <title> Disk striping. </title> <booktitle> In Proc. 2nd Intl. Conf. on Data Engg., </booktitle> <year> 1986. </year>
Reference-contexts: 1 Introduction Declustering is a technique that partitions a file and spreads it across many disks [5], <ref> [11] </ref>. Performance studies have shown that, except under extremely high utilization conditions, declustering is consistently a better approach than placing the entire relation on a single disk [9], [10].
Reference: [12] <author> P. Scheuermann, G. Weikum, and P. Zabback. </author> <title> Adaptive load balancing in disk arrays. </title> <booktitle> In Proceedings of the 4th Intl. Conference on Foundations of Data Organization and Algorithms (FODO), </booktitle> <year> 1993. </year>
Reference-contexts: Their scheme tries to minimize the data movement by retaining as many fragments on the disks as possible. Their technique is useful only when the degree of allocation of the relation remains unchanged. In <ref> [13, 12] </ref>, the authors describe an adaptive method for data allocation and load balancing in disk arrays that responds to evolving access patterns. Their methods place more emphasis on maintaining the load balance rather than maintaining the degree of allocation.
Reference: [13] <author> P. Scheuermann, G. Weikum, and P. Zabback. </author> <title> Disk cooling in parallel disk systems. </title> <journal> Bulletin of the Technical Committee on Data Engineering, </journal> <volume> 17(3) </volume> <pages> 29-40, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Their scheme tries to minimize the data movement by retaining as many fragments on the disks as possible. Their technique is useful only when the degree of allocation of the relation remains unchanged. In <ref> [13, 12] </ref>, the authors describe an adaptive method for data allocation and load balancing in disk arrays that responds to evolving access patterns. Their methods place more emphasis on maintaining the load balance rather than maintaining the degree of allocation.
Reference: [14] <author> P. G. Selinger. </author> <title> Predictions and challenges for database systems in the year 2000. </title> <booktitle> In Proc. 19th Intl. Conf. VLDB, </booktitle> <pages> pages 667-675. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: When reorganization is performed without taking the database off-line or quiescing the transactions, it is called concurrent reorganization or on-line reorganization. On-line reorganization has been identified as a challenging problem by the database community <ref> [14, 17, 2, 15, 16] </ref>. The single most compelling reason to do on-line reorganization is the availability of the database during the reorganization. The conventional approach to reorganization is to take the database o*ine.
Reference: [15] <author> G. Sockut and R. Goldberg. </author> <title> Database reorganization principles and practice. </title> <journal> ACM Computing Surveys, </journal> <volume> 11(4) </volume> <pages> 371-395, </pages> <month> Dec </month> <year> 1979. </year>
Reference-contexts: When reorganization is performed without taking the database off-line or quiescing the transactions, it is called concurrent reorganization or on-line reorganization. On-line reorganization has been identified as a challenging problem by the database community <ref> [14, 17, 2, 15, 16] </ref>. The single most compelling reason to do on-line reorganization is the availability of the database during the reorganization. The conventional approach to reorganization is to take the database o*ine.
Reference: [16] <author> G. H. Sockut and B. R. Iyer. </author> <title> Reorganizing databases concurrently with usage: A survey. </title> <type> Technical Report TR 03.488, </type> <institution> IBM, Santa Teresa Laboratory, </institution> <address> San Jose, CA, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: When reorganization is performed without taking the database off-line or quiescing the transactions, it is called concurrent reorganization or on-line reorganization. On-line reorganization has been identified as a challenging problem by the database community <ref> [14, 17, 2, 15, 16] </ref>. The single most compelling reason to do on-line reorganization is the availability of the database during the reorganization. The conventional approach to reorganization is to take the database o*ine.
Reference: [17] <author> M. Stonebraker, R. Agrawal, U. Dayal, E. J. Neuhold, and A. Reuter. </author> <title> DBMS research at crossroads: The vienna update. </title> <booktitle> In Proc. 19th Intl. Conf. VLDB, </booktitle> <pages> pages 688-692. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <month> August </month> <year> 1993. </year>
Reference-contexts: When reorganization is performed without taking the database off-line or quiescing the transactions, it is called concurrent reorganization or on-line reorganization. On-line reorganization has been identified as a challenging problem by the database community <ref> [14, 17, 2, 15, 16] </ref>. The single most compelling reason to do on-line reorganization is the availability of the database during the reorganization. The conventional approach to reorganization is to take the database o*ine.
Reference: [18] <author> P. Valduriez. </author> <title> Parallel database systems: Open problems and new issues. Distributed and Parallel Databases: </title> <journal> An International Journal, </journal> <volume> 1(2) </volume> <pages> 137-166, </pages> <month> April </month> <year> 1993. </year>
Reference: [19] <author> G. Weikum, P. Zabback, and P. Scheuermann. </author> <title> Dynamic file allocation in disk arrays. </title> <booktitle> In Proceeding of the ACM SIGMOD Conference, </booktitle> <year> 1991. </year> <month> 24 </month>
Reference-contexts: It is important to maintain the degree of allocation of relations because it has a direct bearing on the response time of transactions. The methods in their paper can unintentionally cause the degree of allocation of relations to change. This is not a desirable feature. In <ref> [19] </ref>, the authors present a method to determine the degree of allocation of a relation. Their method assumes the system is operated in single user mode, and all read and write requests are for the entire file, rather than for fractions of files as in a database environment. <p> Table 3 shows the number of items requested by each query type from various relations. The following method is used to determine the degree of allocation for each relation in the TPC-C benchmark. Earlier methods to determine the degree of allocation <ref> [19] </ref> were geared towards a single user system and for one relation at a time. This method is geared towards multiple user system and can determine the degree of allocation for all the relations in the database schema at the database design time.
References-found: 19

