URL: ftp://ftp.cs.columbia.edu/reports/reports-1994/cucs-032-94.ps.gz
Refering-URL: http://www.cs.columbia.edu/~library/1994.html
Root-URL: http://www.cs.columbia.edu
Email: pkc@cs.columbia.edu and sal@cs.columbia.edu  
Title: Toward Scalable and Parallel Inductive Learning: A Case Study in Splice Junction Prediction  
Author: Philip K. Chan and Salvatore J. Stolfo 
Note: Presented at the ML94 Workshop on Machine Learning and Molecular Biology  
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University  
Pubnum: CUCS-032-94  
Abstract: Much of the research in inductive learning concentrates on problems with relatively small amounts of training data. With the steady progress of the Human Genome Project, it is likely that orders of magnitude more data in sequence databases will be available in the near future for various learning problems of biological importance. Thus, techniques that provide the means of scaling machine learning algorithms requires considerable attention. Meta-learning is proposed as a general technique to integrate a number of distinct learning processes that aims to provide a means of scaling to large problems. This paper details several meta-learning strategies for integrating independently learned classifiers on subsets of training data by the same learner in a parallel and distributed computing environment. Our strategies are particularly suited for massive amounts of data that main-memory-based learning algorithms cannot handle efficiently. The strategies are also independent of the particular learning algorithm used and the underlying parallel and distributed platform. Preliminary experiments using different learning algorithms in a simulated parallel environment demonstrate encouraging results: parallel learning by meta-learning can achieve comparable prediction accuracy in less space and time than serial learning. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Boose, J. </author> <year> (1986). </year> <title> Expertise Transfer for Expert System Design. </title> <address> Amsterdam, Netherlands: Else-vier. </address>
Reference-contexts: However, most of the systems developed to date require the translation of analysis techniques developed by human experts to computer programs. It is well known that this process, called knowledge engineering, can be lengthy and problematic <ref> (Boose, 1986) </ref>. Machine learning allows classification systems to be generated automatically by identifying patterns and causal relationships in the data obtained from a user or sensed by interactions with some task environment.
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R. A., & Stone, C. J. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <address> Belmont, CA: </address> <publisher> Wadsworth. </publisher>
Reference-contexts: In this study we concentrate on inductive learning in non-incremental mode, which requires all the training data to be present when training commences. Four inductive learning algorithms were used in this study. ID3 (Quinlan, 1986) and CART <ref> (Breiman et al., 1984) </ref> were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana, 1991). They are both decision tree learning algorithms. WPEBLS is the weighted version of PEBLS (Cost & Salzberg, 1993), which is a memory-based learning algorithm.
Reference: <author> Buntine, W. & Caruana, R. </author> <year> (1991). </year> <title> Introduction to IND and Recursive Partitioning. </title> <institution> NASA Ames Research Center. </institution>
Reference-contexts: Four inductive learning algorithms were used in this study. ID3 (Quinlan, 1986) and CART (Breiman et al., 1984) were obtained from NASA Ames Research Center in the IND package <ref> (Buntine & Caruana, 1991) </ref>. They are both decision tree learning algorithms. WPEBLS is the weighted version of PEBLS (Cost & Salzberg, 1993), which is a memory-based learning algorithm. In memory-based learning, a similarity or "closeness" measure is learned and the examples (or a subset of them) are stored.
Reference: <author> Catlett, J. </author> <year> (1991). </year> <title> Megainduction: A test flight. </title> <booktitle> Proc. Eighth Intl. Work. Machine Learning (pp. </booktitle> <pages> 596-599). </pages>
Reference: <author> Chan, P. </author> <year> (1991). </year> <title> Machine Learning in Molecular Biology Sequence Analysis. </title> <type> (Technical Report CUCS-041-91), </type> <address> New York, NY: </address> <institution> Department of Computer Science, Columbia University. </institution>
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993a). </year> <title> Experiments on multistrategy learning by meta-learning. </title> <booktitle> Proc. Second Intl. Conf. Info. Know. Manag. </booktitle> <pages> 314-323. </pages>
Reference-contexts: Mitchell (1980) refers to this phenomenon as inductive bias. We postulate that by combining the different results intelligently through meta-learning, higher accuracy can be obtained. We call this approach multistrategy hypothesis boosting. Preliminary results reported in <ref> (Chan & Stolfo, 1993a) </ref> are encouraging. Zhang et al.'s (1992) and Wolpert's (1992) work is in this direction. Silver et al.'s (1990) and Holder's (1991) work also employs multiple learners, but no learning is involved at the meta level.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993b). </year> <title> Meta-learning for multistrategy and parallel learning. </title> <booktitle> Proc. Second Intl. Work. on Multistrategy Learning (pp. </booktitle> <pages> 150-165). </pages>
Reference-contexts: Work in progress In addition to applying meta-learning to combining results from a set of parallel or distributed learning processes, meta-learning can also be used to coalesce the results from multiple different inductive learning algorithms applied to the same set of data to improve accuracy <ref> (Chan & Stolfo, 1993b) </ref>. The premise is that different algorithms have different representations and search heuristics, different search spaces are being explored and hence potentially diversed results can be obtained from different algorithms. Mitchell (1980) refers to this phenomenon as inductive bias.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993c). </year> <title> Toward multistrategy parallel and distributed learning in sequence analysis. </title> <booktitle> Proc. First Intl. Conf. Intel. Sys. Mol. Biol. </booktitle> <pages> (pp. 65-73). </pages>
Reference-contexts: Since the ultimate goal of this work is to improve both the accuracy and efficiency of machine learning, we have been working on combining ideas in parallel learning, described in this paper, with those in multistrategy hypothesis boosting. We call this approach multistrategy parallel learning. Preliminary results reported in <ref> (Chan & Stolfo, 1993c) </ref> are encouraging. To our knowledge, not much work in this direction has been attempted by others. 8 Concluding Remarks Several meta-learning schemes for parallel learning are presented in this paper. In particular, schemes for building arbiter trees are detailed.
Reference: <author> Chan, P. & Stolfo, S. </author> <year> (1993d). </year> <title> Toward parallel and distributed learning by meta-learning. </title> <booktitle> Working Notes AAAI Work. Know. Disc. </booktitle> <pages> Databases (pp. 227-240). </pages>
Reference-contexts: The algorithms developed so far are generally not scalable to large databases as envisaged by the Genome Project. The complexity of typical machine learning algorithms renders their use infeasible in problems with massive amounts of data <ref> (Chan & Stolfo, 1993d) </ref>. For instance, Catlett (1991) projects that the well-known ID3 algorithm (Quinlan, 1986) on modern machines will require several months of computing to learn a decision tree from a million records in the flight data set obtained from NASA. <p> In this paper we present the concept of meta-learning and its use in combining results from a set of parallel or distributed learning processes, which was introduced in <ref> (Chan & Stolfo, 1993d) </ref>. We applied our techniques to the splice junction prediction task and conducted more thorough experiments. Here we present our new findings including measured speed improvements. Section 2 introduces the splice junction prediction task. Section 3 describes the learning algorithms used in this study. <p> Sample training sets generated by the two schemes are depicted in Figure 6. (A more sophisticated third scheme, which utilizes three subarbiters, was investigated in <ref> (Chan & Stolfo, 1993d) </ref>. <p> If the prediction accuracy is high, the arbiter training sets will be small because the predictions will usually be correct and few disagreements will occur. In our earlier experiments reported in <ref> (Chan & Stolfo, 1993d) </ref>, the partitioning of data in the subsets was random and later we discovered that half of the final arbiter tree was trained on examples with only two of the three classes.
Reference: <author> Clark, P. & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 261-285. </pages>
Reference-contexts: They are both decision tree learning algorithms. WPEBLS is the weighted version of PEBLS (Cost & Salzberg, 1993), which is a memory-based learning algorithm. In memory-based learning, a similarity or "closeness" measure is learned and the examples (or a subset of them) are stored. BAYES, described in <ref> (Clark & Niblett, 1989) </ref>, is a Bayesian learner that compiles conditional probabilities and uses Bayes' Rule for classification. The latter two algorithms were reimplemented in C for this study. <p> For a attributes, O (anv 2 ) time is needed for a VDMs. The weight vector is incrementally updated and takes O (n 2 ) time. The time complexity for WPEBLS is therefore O (anv 2 + n 2 ) in the worst case. BAYES <ref> (Clark & Niblett, 1989) </ref> calculates the conditional probabilities for each attribute value given a class. The time complexity of BAYES is simply O (avn).
Reference: <author> Cost, S. & Salzberg, S. </author> <year> (1993). </year> <title> A weighted nearest neighbor algorithm for learning with symbolic features. </title> <journal> Machine Learning, </journal> <volume> 10, </volume> <pages> 57-78. </pages>
Reference-contexts: Four inductive learning algorithms were used in this study. ID3 (Quinlan, 1986) and CART (Breiman et al., 1984) were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana, 1991). They are both decision tree learning algorithms. WPEBLS is the weighted version of PEBLS <ref> (Cost & Salzberg, 1993) </ref>, which is a memory-based learning algorithm. In memory-based learning, a similarity or "closeness" measure is learned and the examples (or a subset of them) are stored. <p> The total time complexity for CART is therefore O ((av + an)2 a ) in the worst case. WPEBLS <ref> (Cost & Salzberg, 1993) </ref> calculates a set of value distance matrices (VDMs) and a vector of weights for the exemplars. Each attribute has a VDM of size v by v, which takes O (nv 2 ) to calculate.
Reference: <author> DeLisi, C. </author> <year> (1988). </year> <title> The human genome project. </title> <journal> American Scientist, </journal> <volume> 76, </volume> <pages> 488-493. </pages>
Reference-contexts: It has been reported that in some cases, classification systems generated by learning techniques outperform human-designed systems (Chan, 1991; Qian & Sejnowski, 1988; Towell et al., 1990; Zhang et al., 1992). The Human Genome Project <ref> (DeLisi, 1988) </ref>, initiated by the National Institutes of Health (NIH) and Department of Energy (DOE), aims to map the entire human genome and will inevitably generate orders of magnitude more sequence data than exist today.
Reference: <author> Freund, Y. </author> <year> (1990). </year> <title> Boosting a weak learning algorithm by majority. </title> <booktitle> Proc. 3rd Work. Comp. Learning Theory (pp. </booktitle> <pages> 202-216). </pages>
Reference: <author> Holder, L. </author> <year> (1991). </year> <title> Selection of learning methods using an adaptive model of knowledge utility. </title> <booktitle> Proc. </booktitle> <pages> MSL-91 (pp. 247-254). </pages>
Reference: <author> Hunter, L. </author> <year> (1993). </year> <title> Molecular biology for computer scientists. </title> <editor> In L. Hunter (Ed.), </editor> <booktitle> Aritficial Intelligence and Molecular Biology. </booktitle> <publisher> AAAI Press. 20 Lewin, B. </publisher> <year> (1987). </year> <title> Genes. </title> <address> New York, NY: </address> <publisher> John Wiley & Son. </publisher>
Reference: <author> Michalski, R. </author> <year> (1983). </year> <title> A theory and methodology of inductive learning. </title> <editor> In R. Michalski, J. Car-bonell, & T. Mitchell (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Table 2 shows some of the examples in the data set. Although this data set is relatively small, our intention is to verify the effectiveness of our techniques on a smaller sequence data set, before we attempt much larger ones. 3 Inductive Learning Inductive learning (or learning from examples <ref> (Michalski, 1983) </ref>) is the task of identifying regularities in some given set of examples with little or no knowledge about the domain from which the 3 Table 2: Sample splice junction sequences. Class Sequence intron-exon CTTTAAAAAATTAACATTTTTCTTTTATAGGGATCTGAAACAACATTCATGTGTGAATAT exon-intron GAGATCGACCTGGACTCCATGAGAAATCTGGTGAGTGCCTTCACATCACCTGCCCAGCTC neither TACTGTATCAAGTCATGGCAGGTACAGTAGGATAAGCCACTCTGTCCCTTCCTGGGCAAA examples are drawn.
Reference: <author> Mitchell, T. M. </author> <year> (1980). </year> <title> The Need for Biases in Learning Generalizaions. </title> <type> (Technical Report CBM-TR-117): </type> <institution> Dept. Comp. Sci., Rutgers Univ. </institution>
Reference: <author> Noordewier, M., Towell, G., & Shavlik, J. </author> <year> (1991). </year> <title> Training knowledge-based neural networks to recognize genes in DNA sequences. </title> <booktitle> Proc. </booktitle> <pages> NIPS-91 (pp. 530-536). </pages>
Reference: <author> Qian, N. & Sejnowski, T. </author> <year> (1988). </year> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> J. Mol. Biol., </journal> <volume> 202, </volume> <pages> 865-884. </pages>
Reference: <author> Quinlan, J. R. </author> <year> (1979). </year> <title> Induction over large data bases. </title> <type> (Technical Report STAN-CS-79-739): </type> <institution> Comp. Sci. Dept., Stanford Univ. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference-contexts: The complexity of typical machine learning algorithms renders their use infeasible in problems with massive amounts of data (Chan & Stolfo, 1993d). For instance, Catlett (1991) projects that the well-known ID3 algorithm <ref> (Quinlan, 1986) </ref> on modern machines will require several months of computing to learn a decision tree from a million records in the flight data set obtained from NASA. In addition, typical learning algorithms like ID3 rely on a monolithic memory to fit all of its training data. <p> In this study we concentrate on inductive learning in non-incremental mode, which requires all the training data to be present when training commences. Four inductive learning algorithms were used in this study. ID3 <ref> (Quinlan, 1986) </ref> and CART (Breiman et al., 1984) were obtained from NASA Ames Research Center in the IND package (Buntine & Caruana, 1991). They are both decision tree learning algorithms. WPEBLS is the weighted version of PEBLS (Cost & Salzberg, 1993), which is a memory-based learning algorithm. <p> For the splice junction prediction task, a is 60, v is 8, and n is 2,552 (80% of the data set for purposes of our study). The time complexity of ID3 <ref> (Quinlan, 1986) </ref> is a function of the number of nodes in the decision tree it forms.
Reference: <author> Schapire, R. </author> <year> (1990). </year> <title> The strength of weak learnability. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 197-226. </pages>
Reference: <author> Silver, B., Frawley, W., Iba, G., Vittal, J., & Bradford, K. </author> <year> (1990). </year> <title> ILS: A framework for multi-paradigmatic learning. </title> <booktitle> Proc. Seventh Intl. Conf. Machine Learning (pp. </booktitle> <pages> 348-356). </pages>
Reference: <author> Stolfo, S., Galil, Z., McKeown, K., & Mills, R. </author> <year> (1989). </year> <title> Speech recognition in parallel. </title> <booktitle> Proc. Speech Nat. Lang. Work. </booktitle> <pages> (pp. 353-373). </pages>
Reference-contexts: This involves applying the same algorithm on different subsets of the data in parallel and the use of meta-learning to combine the partial results. We are not aware of any work in the literature on this approach beyond what was first reported in <ref> (Stolfo et al., 1989) </ref> in the domain of speech recognition. Work on using meta-learning for combining different learning systems is reported elsewhere (Chan & Stolfo, 1993a; Chan & Stolfo, 1993c) and is further elaborated later in the paper.
Reference: <author> Sun, X. & Ni, L. </author> <year> (1993). </year> <title> Scalable problems and memory-bounded speedup. </title> <journal> J. Parallel & Distributed Comp., </journal> <volume> 19, </volume> <pages> 27-37. </pages>
Reference-contexts: In addition, these analyses are based on a fixed problem size. Speed up in this case is the processing speed differential with increasing number of processors. An alternate speed up measure is the memory-bound scaled speed up <ref> (Sun & Ni, 1993) </ref>, which measures the increase in possible problem size with increasing number of processors, each with limited available memory.
Reference: <author> Towell, G., Shavlik, J., & Noordewier, M. </author> <year> (1990). </year> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> Proc. </booktitle> <pages> AAAI-90 (pp. 861-866). </pages> <editor> von Heijne, G. </editor> <year> (1987). </year> <title> Sequence Analysis in Molecular Biology. </title> <address> San Diego, CA: </address> <publisher> Academic Press. </publisher>
Reference: <author> Wirth, J. & Catlett, J. </author> <year> (1988). </year> <title> Experiments on the costs and benefits of windowing in ID3. </title> <booktitle> Proc. Fifth Intl. Conf. Machine Learning (pp. </booktitle> <pages> 87-99). </pages>
Reference: <author> Wolpert, D. </author> <year> (1992). </year> <title> Stacked generalization. </title> <booktitle> Neural Networks, </booktitle> <volume> 5, </volume> <pages> 241-259. </pages>
Reference: <author> Zhang, X., Mckenna, M., Mesirov, J., & Waltz, D. </author> <year> (1989). </year> <title> An Efficient Implementation of the Backpropagation Algorithm on the Connection Machine CM-2. </title> <type> (Technical Report RL89-1): </type> <institution> Thinking Machines Corp. </institution>
Reference: <author> Zhang, X., Mesirov, J., & Waltz, D. </author> <year> (1992). </year> <title> A hybrid system for protein secondary structure prediction. </title> <journal> J. Mol. Biol., </journal> <volume> 225, </volume> <pages> 1049-1063. 21 </pages>
References-found: 30

