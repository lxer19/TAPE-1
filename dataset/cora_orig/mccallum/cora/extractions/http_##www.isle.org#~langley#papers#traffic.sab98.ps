URL: http://www.isle.org/~langley/papers/traffic.sab98.ps
Refering-URL: http://www.isle.org/~langley/pubs.html
Root-URL: 
Email: fmoriarty,handley,langleyg@rtna.daimlerbenz.com  
Title: Learning Distributed Strategies for Traffic Control  
Author: David E. Moriarty, Simon Handley, and Pat Langley 
Address: 1510 Page Mill Road, Palo Alto, CA 94304  
Affiliation: Daimler-Benz Research and Technology Center  
Abstract: In this paper, we cast the problem of managing traffic flow in terms of a distributed collection of independent agents that adapt to their environment. We describe an evolutionary algorithm that learns strategies for lane selection, using local information, on a simulated highway that contains hundreds of agents. Experimental studies suggest that the learned controllers lead to better traffic flow than ones constructed manually, and that the learned controllers are robust with respect to to blocked lanes and changes in the number of lanes on the highway.
Abstract-found: 1
Intro-found: 1
Reference: <author> Carrara, M., & Morello, E. </author> <title> Advanced control strategies and methods for motorway of the future. In The drive project DOMINC: New concepts and research under way. </title>
Reference: <author> Eskafi, F. </author> <year> (1996). </year> <title> Modeling and simulation of the automated highway system. </title> <type> Ph.D. thesis, </type> <institution> Department of Electrical Engineering and Computer Science, University of California, Berkeley. </institution>
Reference-contexts: Although none of these assumptions hold for real-world traffic, they do not appear crucial for evaluating the merits of intelligent lane selection, and removing them unnecessarily complicates the model. In future work, however, we hope to expand our experiments to more realistic simulators such as SmartPATH <ref> (Eskafi, 1996) </ref>. During training, the learning system uses the traffic simulator to evaluate candidate lane-selection strategies. Each evaluation or trial lasts 400 simulated seconds and begins with a random dispersement of 200 cars over three lanes on the 3.3 mile roadway.
Reference: <author> Grand, S., Cliff, D., & Malhotra, A. </author> <year> (1997). </year> <title> Creatures: Artificial life autonomous software agents for home entertainment. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pp. 22-29. </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Grefenstette, J. J., Ramsey, C. L., & Schultz, A. </author> <year> (1990). </year> <title> Learning sequential decision rules using simulation models and competition. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 355-381. </pages>
Reference: <author> Holland, J. H. </author> <year> (1996). </year> <title> Hidden order: How adaptation builds complexity. </title> <address> Reading, MA: </address> <publisher> Addison-Wesley. </publisher>
Reference: <author> Holland, J. H., & Reitman, J. S. </author> <year> (1978). </year> <title> Cognitive systems based on adaptive algorithms. </title> <editor> In Waterman, D. A., & Hayes-Roth, F. (Eds.), </editor> <booktitle> Pattern-directed inference systems. </booktitle> <address> New York: </address> <publisher> Academic Press. </publisher>
Reference: <author> Kaelbling, L. P., Littman, M. L., & Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4, </volume> <pages> 237-285. </pages>
Reference: <author> Mataric, M. J. </author> <year> (1994). </year> <title> Learning to behave socially. </title> <booktitle> In Proceedings of the Third International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. 453-462. </pages> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> McCallum, A. K. </author> <year> (1996). </year> <title> Learning to use selective attention and short-term memory in sequential tasks. </title> <booktitle> In Proceedings of Fourth International Conference on Simulation of Adaptive Behavior, </booktitle> <pages> pp. </pages> <address> 315-324 Cape Cod, MA. </address>
Reference: <author> Moriarty, D. E. </author> <year> (1997). </year> <title> Symbiotic evolution of neural networks in sequential decision tasks. </title> <type> Ph.D. thesis, </type> <institution> Department of Computer Sciences, University of Texas at Austin. </institution>
Reference: <author> Moriarty, D. E., & Langley, P. </author> <year> (1998). </year> <title> Learning cooperative lane selection strategies for highways. </title> <booktitle> In Proceedings of the Fifeenth National Conference on Artificial Intelligence Menlo Park, </booktitle> <address> CA: </address> <publisher> AAAI Press. </publisher>
Reference-contexts: We next describe the inputs and outputs of our reactive vehicle controllers, along with our genetic approach to learning distributed control strategies. After this, we characterize our simulated traffic environment and report experimental studies of the system under a variety of conditions. Elsewhere <ref> (Moriarty & Langley, 1998) </ref>, we showed that our approach learns controllers that are robust to changes in the proportion of learned to `selfish' cars on the highway and to changes in traffic density. <p> Our approach, to which we now turn, relies on an evolutionary algorithm as the primary mechanism for reinforcement learning, but it also incorporates a technique similar to temporal-difference learning to handle smaller strategy refinements. 2.4 Machine Learning for Lane Selection Elsewhere <ref> (Moriarty & Langley, 1998) </ref>, we have described our distributed learning system in detail, so here we only review its main components. The system represents its control knowledge as a feedforward neural network with one hidden layer, as depicted in Figure 3. <p> We based these rules on our interpretation of the "slower traffic yield to the right" signs posted on the highways. The selfish strategy described earlier uses only the last two rules. 3.2 Evaluation of Intelligent Lane Selection Our earlier studies <ref> (Moriarty & Langley, 1998) </ref> evaluated the learned controllers' behavior as we varied the density of traffic and the ratio of learned to selfish controllers. <p> There also exists a substantial literature on more traditional approaches to traffic management that typically involve more centralized control, which we have reviewed in a separate paper <ref> (Moriarty & Langley, 1998) </ref>. Although we have found no other work on distributed learning in the traffic domain, we remain excited about its potential as a fertile research testbed. In future work, we plan to improve our traffic simulator to include dura-tive lane changes, entrance ramps, and exit ramps.
Reference: <author> Moriarty, D. E., & Miikkulainen, R. </author> <year> (1996). </year> <title> Efficient reinforcement learning through symbiotic evolution. </title> <journal> Machine Learning, </journal> <volume> 22, </volume> <pages> 11-32. </pages>
Reference: <author> Pomerleau, D. </author> <year> (1995). </year> <title> Ralph: Rapidly adapting lat-eral position handler. </title> <booktitle> In Proceedings of the 1995 IEEE Symposium on Intelligent Vehicles, </booktitle> <pages> pp. </pages> <address> 506-511 Detroit, MI. </address>
Reference: <author> Pomerleau, D. A. </author> <year> (1992). </year> <title> Neural network perception for mobile robot guidance. </title> <type> Ph.D. thesis, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pitts-burgh, PA. </address>
Reference: <author> Quinlan, J. R. </author> <year> (1986). </year> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> 81-106. </pages>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., & McClelland, J. L. (Eds.), </editor> <booktitle> Parallel distributed processing: Explorations in the microstructure of cognition, Volume 1: Foundations. </booktitle> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher>
Reference: <author> Sammut, C., Hurst, S., Kedzier, D., & Michie, D. </author> <year> (1992). </year> <title> Learning to fly. </title> <booktitle> In Proceedings of the Ninth International Workshop on Machine Learning, </booktitle> <pages> pp. 385-393. </pages> <address> San Francisco: </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Schmitz, O., & Booth, G. </author> <year> (1997). </year> <title> Modelling food web complexity: The consequences of individual-based, spatially explicit behavioral ecology on trophic interactions. </title> <journal> Evolutionary Ecology, </journal> <volume> 11, </volume> <pages> 379-398. </pages>
Reference-contexts: Perhaps the best-known effort of this sort revolves around Holland's (1996) Echo, a simulation framework designed to study the behavior of complex biological systems, such as the interaction of plants, herbivores, and carnivores in an ecosystem <ref> (Schmitz & Booth, 1997) </ref>. Schoonderwoerd, Holland, and Bruten (1997) use distributed agents to balance loads in telecommunications networks, but learning occurs only in the sense that agents lay down ant-like trails to improve performance.
Reference: <author> Schoonderwoerd, R., Holland, O., & Bruten, J. </author> <year> (1997). </year> <title> Ant-like agents for load balancing in telecommunications networks. </title> <booktitle> In Proceedings of the First International Conference on Autonomous Agents, </booktitle> <pages> pp. 209-216. </pages> <address> New York: </address> <publisher> ACM Press. </publisher>
Reference: <author> Schultz, A. C., Grefenstette, J. J., & Adams, W. </author> <year> (1996). </year> <title> RoboShepherd: Learning a complex behavior. </title> <booktitle> In Proceedings of RoboLearn-96: International Workshop for Learning in Autonomous Robots Key West, </booktitle> <address> FL. </address>
Reference: <author> Sen, S., & Sekaran, M. </author> <year> (1998). </year> <title> Individual learning of coordination knowledge. </title> <journal> Journal of Experimental & Theoretical Artificial Intelligence, </journal> <volume> 10. </volume>
Reference: <author> Stone, P., & Veloso, M. </author> <year> (1997). </year> <title> Multiagent systems: A survey from a machine learning perspective. </title> <type> Tech. rep. </type> <institution> CMU-CS-97-193, School of Computer Science, Carnegie Mellon University. </institution>
Reference: <author> Sukthankar, R., Hancock, J., Baluja, S., Pomerleau, D., & Thorpe, C. </author> <year> (1996). </year> <title> Adaptive intelligent vehicle modules for tactical driving. </title> <booktitle> In Proceedings of the AAAI-96 Workshop on Intelligent Adaptive Agents, </booktitle> <pages> pp. </pages> <address> 13-22 Portland, OR. </address> <note> Also available at http://www.cs.cmu.edu/~rahuls/Shiva/. </note>
Reference: <author> Sutton, R. S. </author> <year> (1988). </year> <title> Learning to predict by the methods of temporal differences. </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> 9-44. </pages>
Reference: <author> Tan, M. </author> <year> (1993). </year> <title> Multi-agent reinforcement learning: Independent vs. </title> <booktitle> cooperative agents. In Proceedings of the Tenth International Conference on Machine Learning, </booktitle> <pages> pp. 330-337. </pages> <address> San Francisco: </address> <publisher> Mor-gan Kaufmann. </publisher>
Reference: <author> Varaiya, P. </author> <year> (1993). </year> <title> Smart cars on smart roads: Problems of control. </title> <journal> IEEE Transactions on Automatic Control, </journal> <volume> 38, </volume> <pages> 195-207. </pages>
Reference: <author> Watkins, C. J. C. H., & Dayan, P. </author> <year> (1992). </year> <title> Q-learning. </title> <journal> Machine Learning, </journal> <volume> 8, </volume> <pages> 279-292. </pages>
Reference: <author> Whitley, D., Dominic, S., Das, R., & Anderson, C. W. </author> <year> (1993). </year> <title> Genetic reinforcement learning for neuro-control problems. </title> <journal> Machine Learning, </journal> <volume> 13, </volume> <pages> 259-284. </pages>
Reference: <author> Wilson, S. W. </author> <year> (1994). </year> <title> ZCS: A zeroth level classifier system. </title> <journal> Evolutionary Computation, </journal> <volume> 2, </volume> <pages> 1-18. </pages>
References-found: 29

