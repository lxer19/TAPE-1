URL: http://www.cs.washington.edu/research/jair/volume6/wermter97a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/volume6/wermter97a-html/abstract.html
Root-URL: 
Email: wermter@informatik.uni-hamburg.de  weber@informatik.uni-hamburg.de  
Title: SCREEN: Learning a Flat Syntactic and Semantic Spoken Language Analysis Using Artificial Neural Networks  
Author: Stefan Wermter Volker Weber 
Address: 22527 Hamburg, Germany  
Affiliation: Department of Computer Science University of Hamburg  
Note: Journal of Artificial Intelligence Research 6 (1997) 35-85 Submitted 7/96; published 1/97  
Abstract: Previous approaches of analyzing spontaneously spoken language often have been based on encoding syntactic and semantic knowledge manually and symbolically. While there has been some progress using statistical or connectionist language models, many current spoken-language systems still use a relatively brittle, hand-coded symbolic grammar or symbolic semantic component. In contrast, we describe a so-called screening approach for learning robust processing of spontaneously spoken language. A screening approach is a flat analysis which uses shallow sequences of category representations for analyzing an utterance at various syntactic, semantic and dialog levels. Rather than using a deeply structured symbolic analysis, we use a flat connectionist analysis. This screening approach aims at supporting speech and language processing by using (1) data-driven learning and (2) robustness of connectionist networks. In order to test this approach, we have developed the screen system which is based on this new robust, learned and flat analysis. In this paper, we focus on a detailed description of screen's architecture, the flat syntactic and semantic analysis, the interaction with a speech recognizer, and a detailed evaluation analysis of the robustness under the influence of noisy or incomplete input. The main result of this paper is that flat representations allow more robust processing of spontaneous spoken language than deeply structured representations. In particular, we show how the fault-tolerance and learning capability of connectionist networks can support a flat analysis for providing more robust spoken-language processing within an overall hybrid symbolic/connectionist framework.
Abstract-found: 1
Intro-found: 1
Reference: <author> Allen, J. F. </author> <year> (1995). </year> <title> The TRAINS-95 parsing system: A user's manual. </title> <type> Tech. rep. TRAINS TN 95-1, </type> <institution> University of Rochester, Computer Science Department. </institution>
Reference: <author> Allen, J. F., Schubert, L. K., Ferguson, G., Heeman, P., Hwang, C. H., Kato, T., Light, M., Martin, N. G., Miller, B. W., Poesio, M., & Traum, D. R. </author> <year> (1995). </year> <title> The TRAINS project: A case study in building a conversational planning agent. </title> <journal> Journal of Experimental and Theoretical AI, </journal> <volume> 7, </volume> <pages> 7-48. </pages> <note> 81 Wermter & Weber Barnden, </note> <editor> J. A., & Holyoak, K. J. (Eds.). </editor> <booktitle> (1994). Advances in Connectionist and Neural Computation Theory, </booktitle> <volume> Vol. 3., </volume> <publisher> Ablex Publishing Corporation. </publisher>
Reference: <author> But, F. D. </author> <year> (1996). </year> <title> FeasPar A Feature Structure Parser Learning to Parse Spontaneous Speech. </title> <type> Ph.D. thesis, </type> <institution> University of Karlsruhe, Karlsruhe, </institution> <address> FRG. </address>
Reference-contexts: Although additional improvements can be shown using subsequent search techniques on the parsing results, we did not consider such subsequent search techniques for better parses since they would violate incremental processing <ref> (But, 1996) </ref>. Without using subsequent search techniques screen reaches an overall semantic and syntactic accuracy between 72% and 74% as shown in Table 6. However it should be pointed out, that screen and feaspar use different input sentences, features and architectures.
Reference: <author> But, F. D., Polzin, T. S., & Waibel, A. </author> <year> (1994). </year> <title> Learning complex output representations in connectionist parsing of spoken language. </title> <booktitle> In Proceedings of the International Conference on Acoustics, Speech and Signal Processing, </booktitle> <volume> Vol. 1, </volume> <pages> pp. 365-368, </pages> <address> Adelaide, Australia. </address>
Reference-contexts: Learning dialog act processing is important for determining the intended meaning of an utterance (Wermter & Lochel, 1996). Recent further extensions based on parsec provide more structure and use annotated linguistic features <ref> (But et al., 1994) </ref>. The authors state that they "implemented (based on parsec) a connectionist system" which should approximate a shift reduce parser. This connectionist shift-reduce parser substantially differs from the original parsec architecture. 75 Wermter & Weber We will refer to it as the "parsec extension". <p> Comparing parsec and screen, parsec aims more at supporting symbolic rules by using symbolic transformations (triggered by connectionist networks) and by integrating linguistic features. Currently, the linguistic features in the recent parsec extension <ref> (But et al., 1994) </ref> provide more structural and morphological knowledge than screen does. Therefore, currently it appears to be easier to integrate the parsec extension into larger systems of high level linguistic processing. In fact, parsec has been used in the context of the janus framework.
Reference: <author> Charniak, E. </author> <year> (1993). </year> <title> Statistical Language Learning. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: In this respect, our approach also differs from other candidates for robust processing, like statistical taggers or statistical n-grams. These statistical techniques can be used for robust analysis <ref> (Charniak, 1993) </ref> but statistical techniques like n-grams do not relate to the human cognitive language capabilities while simple recurrent connectionist networks have more relationships to the human cognitive language capabilities (Elman, 1990). screen is a new hybrid connectionist system developed for the examination of flat syntactic and semantic analysis of spoken
Reference: <author> Dyer, M. G. </author> <year> (1983). </year> <title> In-Depth Understanding: A Computer Model of Integrated Processing for Narrative Comprehension. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Furthermore, a deep highly structured representation may have many more restrictions than appropriate for spontaneously spoken language. However, and maybe even more important, for certain tasks it is not necessary to perform an in-depth analysis. While, for instance, inferences about story understanding require an in-depth understanding <ref> (Dyer, 1983) </ref>, tasks like information extraction from spoken language do not need much of an in-depth analysis.
Reference: <author> Elman, J. L. </author> <year> (1990). </year> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 (2), </volume> <pages> 179-211. </pages>
Reference-contexts: These statistical techniques can be used for robust analysis (Charniak, 1993) but statistical techniques like n-grams do not relate to the human cognitive language capabilities while simple recurrent connectionist networks have more relationships to the human cognitive language capabilities <ref> (Elman, 1990) </ref>. screen is a new hybrid connectionist system developed for the examination of flat syntactic and semantic analysis of spoken language. In earlier work we have explored a flat scanning understanding for written texts (Wermter, 1995; Wermter & Lochel, 1994; Wermter & Peters, 1994). <p> However, we will not describe the details of these two parts in this paper since they have been described elsewhere (Wermter & Lochel, 1996). Learning in screen is based on concepts of supervised learning as for instance in feed-forward networks (Rumelhart et al., 1986), simple recurrent networks <ref> (Elman, 1990) </ref> and more general recurrent plausibility networks (Wermter, 1995). In general, recurrent plausibility networks allow an arbitrary number of context and hidden layers for considering long distance dependencies. However, for the many network modules in screen we attempted to keep the individual networks simple and homogeneous. <p> However, for the many network modules in screen we attempted to keep the individual networks simple and homogeneous. Therefore, in our first version described here we used only variations of feedforward networks (Rumelhart et al., 1986) and simple recurrent networks <ref> (Elman, 1990) </ref>. Due to their greater potential for sequential context representations, recurrent plausibility networks might provide improvements and optimizations of simple recurrent networks. However, for now we are primarily interested in an overall real-world hybrid connectionist architecture screen rather than the optimization of single networks. <p> Since decisions about the current state of a whole sequence have to be made, the preceding context is represented by copying the hidden layer for the current word to the context layer for the next word based on an SRN network structure <ref> (Elman, 1990) </ref>. All connections in the network are n:m connections except for the connections between the hidden layer and the context layer which are simply used to copy and store the internal preceding state in the context layer for later processing when the next word comes in.
Reference: <author> Faisal, K. A., & Kwasny, S. C. </author> <year> (1990). </year> <title> Design of a hybrid deterministic parser. </title> <booktitle> In Proceedings of the 13 th International Conference on Computational Linguistics, </booktitle> <pages> pp. 11-16, </pages> <address> Helsinki, Finnland. </address>
Reference: <author> Feldman, J. A. </author> <year> (1993). </year> <title> Structured connectionist models and language learning. </title> <journal> Artificial Intelligence Review, </journal> <volume> 7 (5), </volume> <pages> 301-312. </pages>
Reference: <author> Geutner, P., Suhm, B., But, F.-D., Kemp, T., Mayfield, L., McNair, A. E., Rogina, I., Schultz, T., Sloboda, T., Ward, W., Woszczyna, M., & Waibel, A. </author> <year> (1996). </year> <title> Integrating different learning approaches into a multilingual spoken language translation system. </title>
Reference: <editor> In Wermter, S., Riloff, E., & Scheler, G. (Eds.), </editor> <booktitle> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing, </booktitle> <pages> pp. 117-131, </pages> <publisher> Springer, </publisher> <address> Heidelberg. </address>
Reference: <author> Gupta, P., & Touretzky, D. S. </author> <year> (1994). </year> <title> Connectionist models and linguistic theory: Investigations of stress systems in language. </title> <journal> Cognitive Science, </journal> <volume> 18 (1), </volume> <pages> 1-50. </pages>
Reference: <author> Hauenstein, A., & Weber, H. H. </author> <year> (1994). </year> <title> An investigation of tightly coupled time synchronous speech language interfaces using a unification grammar. </title> <booktitle> In Proceedings of the 12 th National Conference on Artificial Intelligence Workshop on the Integration of Natural Language and Speech Processing, </booktitle> <pages> pp. 42-49, </pages> <address> Seattle, Washington. </address>
Reference: <author> Heeman, P. A., & Allen, J. </author> <year> (1994a). </year> <title> Detecting and correcting speech repairs. </title> <booktitle> In Proceedings of the 32 nd Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 295-302, </pages> <address> Las Cruces, NM. </address>
Reference: <author> Heeman, P. A., & Allen, J. </author> <year> (1994b). </year> <title> Tagging speech repairs. </title> <booktitle> In Proceedings of the Human Language Technology Workshop, </booktitle> <pages> pp. 187-192, </pages> <address> Plainsboro, NJ. </address>
Reference-contexts: After the linguistic reasoning, conversation acts are determined by a system dialog manager and responses are generated based on a template-driven natural language generator. Performance phenomena in spoken language like repairs and false starts can currently be dealt with already <ref> (Heeman & Allen, 1994b, 1994a) </ref>. Compared to screen, the trains project focuses more on processing spoken language at an in-depth planning level. While screen uses primarily a flat connectionist language analysis, trains uses a chart parser with a generalized phrase structure grammar. 7.
Reference: <author> Hendler, J. A. </author> <year> (1989). </year> <title> Marker-passing over microfeatures: Towards a hybrid symbolic/connectionist model. </title> <journal> Cognitive Science, </journal> <volume> 13 (1), </volume> <month> 79-106. </month> <title> 82 SCREEN: Flat Syntactic and Semantic Spoken Language Analysis Hirschberg, </title> <editor> J. </editor> <year> (1993). </year> <title> Pitch accent in context: Predicting intonational prominence from text. </title> <journal> Artificial Intelligence, </journal> <volume> 63, </volume> <pages> 305-340. </pages>
Reference: <author> Jacobs, R. A., Jordan, M. I., & Barto, A. G. </author> <year> (1991a). </year> <title> Task decomposition through competition in a modular connectionist architecture: The what and where vision tasks. </title> <journal> Cognitive Science, </journal> <volume> 15, </volume> <pages> 219-250. </pages>
Reference-contexts: Task knowledge is learned in individual task networks, and higher control networks are responsible for learning when a single task network is responsible for producing the output. Originally it was an open question whether a connectionist control would be possible for processing spoken language. While automatic modular task decomposition <ref> (Jacobs et al., 1991a) </ref> can be done for simple forms of function approximation, more complex problems like understanding spoken language in real-world environments still need designer-based modular task decomposition for the necessary tasks.
Reference: <author> Jacobs, R. A., Jordan, M. I., Nowlan, S. J., & Hinton, G. E. </author> <year> (1991b). </year> <title> Adaptive mixtures of local experts. </title> <journal> Neural Computation, </journal> <volume> 3 (1), </volume> <pages> 79-87. </pages>
Reference: <author> Jain, A. N. </author> <year> (1991). </year> <title> Parsing complex sentences with structured connectionist networks. </title> <journal> Neural Computation, </journal> <volume> 3 (1), </volume> <pages> 110-120. </pages>
Reference-contexts: Furthermore, different systems are typically used for different purposes with different language corpora, grammars, rules, etc. However, we have made an extensive effort for a fair conceptual comparison. parsec <ref> (Jain, 1991) </ref> is a hybrid connectionist system which is embedded in a larger speech translation effort janus (Waibel et al., 1992). The input for parsec is sentences, the output is case role representations.
Reference: <author> Jordan, M. I., & Jacobs, R. A. </author> <year> (1992). </year> <title> Hierarchies of adaptive experts. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. R. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 985-992, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke, A., Fosler, E., & Morgan, N. </author> <year> (1994a). </year> <title> The Berkeley Restaurant Project. </title> <booktitle> In Proceedings of the International Conference on Speech and Language Processing. </booktitle> <pages> pp. 2139-2142, </pages> <address> Yokohama, Japan. </address>
Reference: <author> Jurafsky, D., Wooters, C., Tajchman, G., Segal, J., Stolcke, A., & Morgan, N. </author> <year> (1994b). </year> <title> Integrating experimental models of syntax, phonology, and accent/dialect in a speech rec-ognizer. An investigation of tightly coupled time synchronous speech. </title> <booktitle> In Proceedings of the 12 th National Conference on Artificial Intelligence Workshop on the Integration of Natural Language and Speech Processing, </booktitle> <pages> pp. 107-115, </pages> <address> Seattle, Washington. </address>
Reference: <author> Medsker, L. R. </author> <year> (1994). </year> <title> Hybrid Neural Network and Expert Systems. </title> <publisher> Kluwer Academic Publishers, </publisher> <address> Boston. </address>
Reference: <author> Mellish, C. S. </author> <year> (1989). </year> <title> Some chart-based techniques for parsing ill-formed input. </title> <booktitle> In Proceedings of the 27 th Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. 102-109, </pages> <address> Vancouver, Canada. </address>
Reference: <author> Menzel, W. </author> <year> (1994). </year> <title> Parsing of spoken language under time constraints. </title> <editor> In Cohn, A. G. (Ed.), </editor> <booktitle> Proceedings of the 11 th European Conference on Artificial Intelligence, </booktitle> <pages> pp. 561-564, </pages> <address> Amsterdam. </address>
Reference: <author> Miikkulainen, R. </author> <year> (1993). </year> <title> Subsymbolic Natural Language Processing. An integrated model of scripts, lexicon and memory. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: Currently we plan to build a more sophisticated lexicon component which will provide support for automatic lexicon design (Riloff, 1993) and dynamic lexicon entry determination using local context <ref> (Miikkulainen, 1993) </ref>. Furthermore, screen could be expanded at the speech construction and evaluation part. The syntactic and semantic hypotheses could be used for more interaction with the speech recognizer.
Reference: <author> Miikkulainen, R. </author> <year> (1996). </year> <title> Subsymbolic case-role analysis of sentences with embedded clauses. </title> <journal> Cognitive Science, </journal> <volume> 20, </volume> <pages> 47-73. </pages>
Reference: <author> Nakatani, C., & Hirschberg, J. </author> <year> (1993). </year> <title> A speech-first model for repair detection and correction. </title> <booktitle> In Proceedings of the 31 st Annual Meeting of the Association for Computational Linguistics, </booktitle> <pages> pp. </pages> <address> 46-53 Columbus, Ohio. </address>
Reference-contexts: An elimination of interjections and pauses is desired for instance in a speech translation task in order to provide an inter 8. Pauses and interjections can sometimes provide clues for repairs <ref> (Nakatani & Hirschberg, 1993) </ref> although currently we do not use these clues for repair detection. Compared to the lexical, syntactic, and semantic equality of constituents, interjections and pauses provide relatively weak indicators for repairs since they also occur relatively often at other places in a sentence.
Reference: <editor> Reilly, R. G., & Sharkey, N. E. (Eds.). </editor> <year> (1992). </year> <title> Connectionist Approaches to Natural Language Processing. </title> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ. </address> <note> 83 Wermter & Weber Riloff, </note> <author> E. </author> <year> (1993). </year> <title> Automatically constructing a dictionary for information extraction tasks. </title> <booktitle> In Proceedings of the 11 th National Conference on Artificial Intelligence, </booktitle> <pages> pp. 811-816, </pages> <address> Washington, DC. </address>
Reference: <author> Rumelhart, D. E., Hinton, G. E., & Williams, R. J. </author> <year> (1986). </year> <title> Learning internal representations by error propagation. </title> <editor> In Rumelhart, D. E., McClelland, J. L., </editor> & <booktitle> The PDP research group (Eds.), Parallel Distributed Processing, </booktitle> <volume> Vol. 1., </volume> <pages> pp. 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA. </address>
Reference-contexts: However, we will not describe the details of these two parts in this paper since they have been described elsewhere (Wermter & Lochel, 1996). Learning in screen is based on concepts of supervised learning as for instance in feed-forward networks <ref> (Rumelhart et al., 1986) </ref>, simple recurrent networks (Elman, 1990) and more general recurrent plausibility networks (Wermter, 1995). In general, recurrent plausibility networks allow an arbitrary number of context and hidden layers for considering long distance dependencies. <p> However, for the many network modules in screen we attempted to keep the individual networks simple and homogeneous. Therefore, in our first version described here we used only variations of feedforward networks <ref> (Rumelhart et al., 1986) </ref> and simple recurrent networks (Elman, 1990). Due to their greater potential for sequential context representations, recurrent plausibility networks might provide improvements and optimizations of simple recurrent networks. <p> This configuration had provided the best performance for most of the network architectures. In general we tested network architectures from 7 to 28 hidden units, learning parameters from 0.1 to 0.0001. As learning rule we used the generalized delta rule <ref> (Rumelhart et al., 1986) </ref>. An assigned output category representation for a word was counted as correct if the category with the maximum activation was the desired category.
Reference: <author> Sauerland, U. </author> <year> (1996). </year> <title> Konzeption und Implementierung einer Speech/Language Schnittstelle. </title> <type> Master's thesis, </type> <institution> University of Hamburg, Computer Science Department, Hamburg, </institution> <address> FRG. </address>
Reference-contexts: In statistical models for speech recognition, bigram or trigram models are used as language models for filtering out the best possible hypotheses. We used simple recurrent networks since these networks performed slightly better than a bigram and a trigram model which had been implemented for comparison <ref> (Sauerland, 1996) </ref>. Later in Section 6.1 we will also show a detailed comparison of simple recurrent networks and n-gram models (for n = 1,...,5).
Reference: <author> Sumida, R. A. </author> <year> (1991). </year> <title> Dynamic inferencing in parallel distributed semantic networks. </title> <booktitle> In Proceedings of the 13 th Annual Meeting of the Cognitive Science Society, </booktitle> <pages> pp. 913-917, </pages> <address> Boston, Chicago. </address>
Reference: <author> Sun, R. </author> <year> (1994). </year> <title> Integrating Rules and Connectionism for Robust Common Sense Reasoning. </title> <publisher> Wiley and Sons, </publisher> <address> New York. </address> <publisher> von Hahn, W., </publisher> & <address> Pyka, C. </address> <year> (1992). </year> <title> System architectures for speech understanding and language processing. </title> <editor> In Heyer, G., & Haugeneder, H. (Eds.), </editor> <publisher> Applied Linguistics. Wiesbaden. </publisher>
Reference: <author> Waibel, A., Jain, A. N., McNair, A., Tebelskis, J., Osterholtz, L., Saito, H., Schmidbauer, O., Sloboda, T., & Woszczyna, M. </author> <year> (1992). </year> <title> JANUS: Speech-to-speech translation using connectionist and non-connectionist techniques. </title> <editor> In Moody, J. E., Hanson, S. J., & Lippmann, R. R. (Eds.), </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pp. 183-190, </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: Furthermore, different systems are typically used for different purposes with different language corpora, grammars, rules, etc. However, we have made an extensive effort for a fair conceptual comparison. parsec (Jain, 1991) is a hybrid connectionist system which is embedded in a larger speech translation effort janus <ref> (Waibel et al., 1992) </ref>. The input for parsec is sentences, the output is case role representations. The system consists of several connectionist modules with associated symbolic transformation rules for providing transformations suggested by the connectionist networks. <p> Many hybrid systems contain just a small portion of connectionist representations in addition to many other modules, e.g. berp (Wooters, 1993; Jurafsky et al., 1994, 1994b), janus <ref> (Waibel et al., 1992) </ref>, trains (Allen, 1995; Allen et al., 1995). In contrast, most of the important subtasks in screen are performed directly by many connectionist networks.
Reference: <author> Ward, N. </author> <year> (1994). </year> <title> An approach to tightly-coupled syntactic/semantic processing for speech understanding. </title> <booktitle> In Proceedings of the 12 th National Conference on Artificial Intelligence Workshop on the Integration of Natural Language and Speech Processing, </booktitle> <pages> pp. 50-57, </pages> <address> Seattle, Washington. </address>
Reference: <author> Weber, V., & Wermter, S. </author> <year> (1995). </year> <title> Towards learning semantics of spontaneous dialog utterances in a hybrid framework. </title> <editor> In Hallam, J. (Ed.), </editor> <title> Hybrid Problems, </title> <booktitle> Hybrid Solutions | Proceedings of the 10 th Biennial Conference on AI and Cognitive Science, </booktitle> <pages> pp. 229-238, </pages> <address> Sheffield, UK. </address>
Reference-contexts: loc-to (LC-TO) end location: to Hamburg confirmation (CONF) confirmation phrase: ok great, yes wonderful negation (NEG) negation phrase: no stop, not question (QUEST) question phrases: at what time misc (MISC) miscellaneous words, e.g., for politeness: please, eh Table 4: Abstract semantic categories railway counter interactions were described in previous work <ref> (Weber & Wermter, 1995) </ref>. Here we will primarily focus on the semantic categories of the meeting corpus. The basic semantic categories for a word are shown in Table 3. At a higher level of abstraction, each word can belong to an abstract semantic category.
Reference: <author> Weber, V., & Wermter, S. </author> <year> (1996). </year> <title> Artificial neural networks for repairing language. </title> <booktitle> In Proceedings of the 8 th International Conference on Neural Networks and their Applications, </booktitle> <pages> pp. 117-123, </pages> <address> Marseille, FRA. </address>
Reference: <author> Wermter, S., & Weber, V. </author> <year> (1996a). </year> <title> Interactive spoken-language processing in a hybrid connectionist system. </title> <booktitle> IEEE Computer Theme Issue on Interactive Natural Language Processing, </booktitle> <month> July, 65-74. </month> <title> 84 SCREEN: Flat Syntactic and Semantic Spoken Language Analysis Wermter, </title> <editor> S. (1994). Hybride symbolische und subsymbolische Verarbeitung am Beispiel der Sprachverarbeitung. In Duwe, I., Kurfe, F., Paa, G., & Vogel, S. (Eds.), </editor> <booktitle> Herbstschule Konnektionismus und Neuronale Netze. </booktitle> <institution> Gesellschaft fur Mathematik und Datenverarbeitung (GMD), </institution> <address> Sankt Augustin, FRG. </address>
Reference-contexts: After preliminary successful case studies with transcripts we have developed the screen system for using knowledge generated from a speech recognizer. In previous work, we gave a brief summary of screen with a specific focus on segmentation parsing and dialog act processing <ref> (Wermter & Weber, 1996a) </ref>. <p> Based on the combined acoustic, syntactic, and semantic knowledge, first tests on the 184 turns show that the accuracy of the constructed sentence hypotheses of screen could be increased by about 30% using acoustic and syntactic plausibilities and by about 50% using acoustic, syntactic, and semantic plausibilities <ref> (Wermter & Weber, 1996a) </ref>. 11. In our experiments low values (n = 10) provided the best overall performance. 12.
Reference: <author> Wermter, S. </author> <year> (1995). </year> <title> Hybrid Connectionist Natural Language Processing. </title> <publisher> Chapman and Hall, Thompson International, </publisher> <address> London, UK. </address>
Reference-contexts: loc-to (LC-TO) end location: to Hamburg confirmation (CONF) confirmation phrase: ok great, yes wonderful negation (NEG) negation phrase: no stop, not question (QUEST) question phrases: at what time misc (MISC) miscellaneous words, e.g., for politeness: please, eh Table 4: Abstract semantic categories railway counter interactions were described in previous work <ref> (Weber & Wermter, 1995) </ref>. Here we will primarily focus on the semantic categories of the meeting corpus. The basic semantic categories for a word are shown in Table 3. At a higher level of abstraction, each word can belong to an abstract semantic category. <p> Learning in screen is based on concepts of supervised learning as for instance in feed-forward networks (Rumelhart et al., 1986), simple recurrent networks (Elman, 1990) and more general recurrent plausibility networks <ref> (Wermter, 1995) </ref>. In general, recurrent plausibility networks allow an arbitrary number of context and hidden layers for considering long distance dependencies. However, for the many network modules in screen we attempted to keep the individual networks simple and homogeneous. <p> Focusing on the syntactic analysis, we used an existing chart parser and an existing grammar which had been used extensively for other real-world parsing up to the sentence level <ref> (Wermter, 1995) </ref>. The only necessary significant adaptation was the addition of a rule N G ! U for pronouns, which had not been part of the original grammar. This rule states that a pronoun U (e.g., "I") can be a noun group (NG). <p> In some of our previous work we had made early experiences with related connectionist networks for analyzing text phrases. Moving from analyzing text phrases to analyzing unrestricted spoken utterances, there are tremendous differences in the two tasks. We found that the phrase-oriented flat analysis used in scan <ref> (Wermter, 1995) </ref> is advantageous in principle for spoken-language analysis and the phrase-oriented analysis is common to learning text and speech processing. However, we learned that spoken-language analysis needs a much more sophisticated architecture.
Reference: <author> Wermter, S., & Lochel, M. </author> <year> (1994). </year> <title> Connectionist learning of flat syntactic analysis for speech/language systems. </title> <editor> In Marinaro, M., & Morasso, P. G. (Eds.), </editor> <booktitle> Proceedings of the International Conference on Artificial Neural Networks, </booktitle> <volume> Vol. 2, </volume> <pages> pp. 941-944, </pages> <address> Sorrento, Italy. </address>
Reference-contexts: On the other hand for learning robust analysis, we use feedforward and simple recurrent networks in many modules and try to use rather homogeneous, supervised networks. 4.2 An Overview of the Architecture screen has a parallel integrated hybrid architecture <ref> (Wermter, 1994) </ref> which has various main properties: 1. Outside of a module, there is no difference in communication between a symbolic and a connectionist module. While previous hybrid architectures emphasized different symbolic and connectionist representations, the different representations in screen benefit from a common module interface. <p> In Figure 7 we show the influence of the phrase start delimiter on the abstract syntactic and semantic categorization with dotted lines. 54 SCREEN: Flat Syntactic and Semantic Spoken Language Analysis The abbreviations are explained in Table 2. indicators for abstract syntactic categories <ref> (Wermter & Lochel, 1994) </ref>. On the other hand, earlier experiments supported to take the abstract semantic category of the last word of a phrase as the final abstract semantic category of a phrase, since phrase ends (e.g., nouns) are good indicators for abstract semantic categories (Wermter & Peters, 1994). <p> On the other hand, earlier experiments supported to take the abstract semantic category of the last word of a phrase as the final abstract semantic category of a phrase, since phrase ends (e.g., nouns) are good indicators for abstract semantic categories <ref> (Wermter & Peters, 1994) </ref>. Furthermore, the phrase start gives us an opportunity to distinguish two equal subsequent abstract categories of two phrases.
Reference: <author> Wermter, S., & Lochel, M. </author> <year> (1996). </year> <title> Learning dialog act processing. </title> <booktitle> In Proceedings of the 16 th International Conference on Computational Linguistics, </booktitle> <pages> pp. 740-745, </pages> <address> Kopen-hagen, Denmark. </address>
Reference-contexts: In fact, we have already fully implemented the case frame part and the dialog part for all our utterances. However, we will not describe the details of these two parts in this paper since they have been described elsewhere <ref> (Wermter & Lochel, 1996) </ref>. Learning in screen is based on concepts of supervised learning as for instance in feed-forward networks (Rumelhart et al., 1986), simple recurrent networks (Elman, 1990) and more general recurrent plausibility networks (Wermter, 1995). <p> Since this paper focuses on the syntactic and semantic aspects of screen we do not further elaborate on the implemented dialog part here. Further details on dialog act processing have been described previously <ref> (Wermter & Lochel, 1996) </ref>. 10. In the snapshots in Figure 12 the abstract syntactic and semantic categories have not yet been computed and therefore are represented as NIL. <p> On the other hand, screen also contains modules for learning dialog act assignment while such modules are currently not part of parsec. Learning dialog act processing is important for determining the intended meaning of an utterance <ref> (Wermter & Lochel, 1996) </ref>. Recent further extensions based on parsec provide more structure and use annotated linguistic features (But et al., 1994). The authors state that they "implemented (based on parsec) a connectionist system" which should approximate a shift reduce parser.
Reference: <author> Wermter, S., & Peters, U. </author> <year> (1994). </year> <title> Learning incremental case assignment based on modular connectionist knowledge sources. </title> <editor> In Werbos, P., Szu, H., & Widrow, B. (Eds.), </editor> <booktitle> Proceedings of the World Congress on Neural Networks, </booktitle> <volume> Vol. 4, </volume> <pages> pp. 538-543, </pages> <address> San Diego, CA. </address>
Reference-contexts: On the other hand for learning robust analysis, we use feedforward and simple recurrent networks in many modules and try to use rather homogeneous, supervised networks. 4.2 An Overview of the Architecture screen has a parallel integrated hybrid architecture <ref> (Wermter, 1994) </ref> which has various main properties: 1. Outside of a module, there is no difference in communication between a symbolic and a connectionist module. While previous hybrid architectures emphasized different symbolic and connectionist representations, the different representations in screen benefit from a common module interface. <p> In Figure 7 we show the influence of the phrase start delimiter on the abstract syntactic and semantic categorization with dotted lines. 54 SCREEN: Flat Syntactic and Semantic Spoken Language Analysis The abbreviations are explained in Table 2. indicators for abstract syntactic categories <ref> (Wermter & Lochel, 1994) </ref>. On the other hand, earlier experiments supported to take the abstract semantic category of the last word of a phrase as the final abstract semantic category of a phrase, since phrase ends (e.g., nouns) are good indicators for abstract semantic categories (Wermter & Peters, 1994). <p> On the other hand, earlier experiments supported to take the abstract semantic category of the last word of a phrase as the final abstract semantic category of a phrase, since phrase ends (e.g., nouns) are good indicators for abstract semantic categories <ref> (Wermter & Peters, 1994) </ref>. Furthermore, the phrase start gives us an opportunity to distinguish two equal subsequent abstract categories of two phrases.
Reference: <editor> Wermter, S., Riloff, E., & Scheler, G. (Eds.). </editor> <year> (1996). </year> <title> Connectionist, Statistical and Symbolic Approaches to Learning for Natural Language Processing. </title> <publisher> Springer, </publisher> <address> Berlin. </address>
Reference-contexts: In fact, we have already fully implemented the case frame part and the dialog part for all our utterances. However, we will not describe the details of these two parts in this paper since they have been described elsewhere <ref> (Wermter & Lochel, 1996) </ref>. Learning in screen is based on concepts of supervised learning as for instance in feed-forward networks (Rumelhart et al., 1986), simple recurrent networks (Elman, 1990) and more general recurrent plausibility networks (Wermter, 1995). <p> Since this paper focuses on the syntactic and semantic aspects of screen we do not further elaborate on the implemented dialog part here. Further details on dialog act processing have been described previously <ref> (Wermter & Lochel, 1996) </ref>. 10. In the snapshots in Figure 12 the abstract syntactic and semantic categories have not yet been computed and therefore are represented as NIL. <p> On the other hand, screen also contains modules for learning dialog act assignment while such modules are currently not part of parsec. Learning dialog act processing is important for determining the intended meaning of an utterance <ref> (Wermter & Lochel, 1996) </ref>. Recent further extensions based on parsec provide more structure and use annotated linguistic features (But et al., 1994). The authors state that they "implemented (based on parsec) a connectionist system" which should approximate a shift reduce parser.
Reference: <author> Wermter, S., & Weber, V. </author> <year> (1996b). </year> <title> Artificial neural networks for automatic knowledge acquisition in multiple real-world language domains. </title> <booktitle> In Proceedings of the 8 th International Conference on Neural Networks and their Applications, </booktitle> <pages> pp. 289-296, </pages> <address> Marseille, FRA. </address>
Reference-contexts: from destination (DEST) time or location destination words, prepositions: to location (LOC) Hamburg, Pittsburgh time (TIME) tomorrow, at 3 o' clock, April negative evaluation (NO) no, bad positive evaluation (YES) yes, good nil (NIL) words "without" specific semantics, e.g., determiner: a Table 3: Basic semantic categories and the meeting corpus <ref> (Wermter & Weber, 1996b) </ref>. Differences occurred mainly for verbs, e.g., NEED-events are very frequent in the railway counter interactions while SUGGEST-events are frequent in the business meeting interactions. <p> Only for semantic processing we retrained the semantic networks. Different categories had to be used for semantic classification, in particular for actions. While actions about meetings (e.g., visit, meet) were predominant in the meeting corpus, actions about selecting connections (e.g., choose, select) were important in the train corpus <ref> (Wermter & Weber, 1996b) </ref>. Just to give the reader an impression of the portability of screen, we would estimate that 90% of the original human effort (system architecture, networks) could be used in this new domain.
Reference: <author> Wooters, C. C. </author> <year> (1993). </year> <title> Lexical modeling in a speaker independent speech understanding system. </title> <type> Tech. rep. </type> <institution> TR-93-068, International Computer Science Institute, Berkeley. </institution>
Reference: <author> Young, S. R., Hauptmann, A. G., Ward, W. H., Smith, E., & Werner, P. </author> <year> (1989). </year> <title> High level knowledge sources in usable speech recognition systems. </title> <journal> Communications of the ACM, </journal> <volume> 32, </volume> <pages> 183-194. 85 </pages>
References-found: 46

