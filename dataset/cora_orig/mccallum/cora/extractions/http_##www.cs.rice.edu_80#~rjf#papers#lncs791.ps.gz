URL: http://www.cs.rice.edu:80/~rjf/papers/lncs791.ps.gz
Refering-URL: http://www.cs.rice.edu:80/~rjf/pubs.html
Root-URL: 
Title: Architectural Convergence and The Granularity of Objects in Distributed Systems  
Author: Robert J. Fowler 
Address: DK-2100 Copenhagen, Denmark  
Affiliation: DIKU (Department of Computer Science), University of Copenhagen,  
Abstract: Recent dramatic speedups in processor speeds have not been matched by comparable reductions in communication latencies, either in MIMD systems designed for parallel computation or in workstation networks. A consequence is that these two classes of concurrent architectures are becoming more alike. This architectural convergence is affecting the software techniques and programming styles used: the distinctions are beginning to fade and all software systems are looking increasingly "distributed." We discuss these architectural trends from the standpoint of providing a single, uniform object-based programming abstraction that 1 Where We Were accommodates both large and small objects.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> A. Agarwal, J. Kubiatowicz, D. Kranz, B-H. Lim, D. Yeung, G. D'Souza, and M. Parkin. Sparcle: </author> <title> an evolutionary processor design for large-scale multiprocessors. </title> <journal> IEEE Micro, </journal> <volume> 13(3) </volume> <pages> 48-60, </pages> <month> June </month> <year> 1993. </year>
Reference-contexts: While the term "distributed memory architecture" has been synonymous in some circles with "mul-ticomputer", all scalable multiprocessor designs distribute main memory among the processing nodes. Thus, whether or not memory is distributed will not be a relevant distinction. The underlying interconnection hardware is converging. The MIT Alewife <ref> [2, 1] </ref> is based on a two-dimensional mesh network using Elko-series wormhole mesh routing chips from Caltech like those used in Intel multicomputers designed around the same time. These routers provide switching delays of 30 ns per hop and 60 Mbytes per second per channel [1]. <p> The MIT Alewife [2, 1] is based on a two-dimensional mesh network using Elko-series wormhole mesh routing chips from Caltech like those used in Intel multicomputers designed around the same time. These routers provide switching delays of 30 ns per hop and 60 Mbytes per second per channel <ref> [1] </ref>. The Stanford DASH [22] uses a similar network. On DASH, a cache miss that requires internode communication can be handled with a latency of about 3.2 microseconds [22], i.e., one order of magnitude larger than a miss on an SGI 4D. <p> Latency tolerance in hardware can allow very fine-grain interaction. For example, the Tera computer [4] is designed to perform an "instantaneous" context switch on ev ery memory operation. Less extreme designs in the hardware of Alewife <ref> [1] </ref> and flT [25] or the software of Active Messages [30] attack both of these potential problems by minimizing operation overhead and by facilitating concurrent activity. Future object-based systems must be designed so that applications exhibit enough concurrency at each node to hide internode latencies.
Reference: 2. <author> A. Agarwal, B.-H. Lim, D. Kranz, and J. Kubiatowicz. </author> <month> APRIL: </month> <title> a processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: While the term "distributed memory architecture" has been synonymous in some circles with "mul-ticomputer", all scalable multiprocessor designs distribute main memory among the processing nodes. Thus, whether or not memory is distributed will not be a relevant distinction. The underlying interconnection hardware is converging. The MIT Alewife <ref> [2, 1] </ref> is based on a two-dimensional mesh network using Elko-series wormhole mesh routing chips from Caltech like those used in Intel multicomputers designed around the same time. These routers provide switching delays of 30 ns per hop and 60 Mbytes per second per channel [1].
Reference: 3. <author> G.T. Almes, A.P. Black, E.D. Lazowska, and J.D. Noe. </author> <title> The Eden system: a technical review. </title> <journal> IEEE Transactions on Software Engineering, </journal> <volume> SE-11(1):43-59, </volume> <month> January </month> <year> 1985. </year>
Reference-contexts: Both PRESTO and the applications written for it on the Sequent freely used fine-grain sharing of data and C++ objects, respectively, among the available processors [7] with little concern for locality of reference. Nevertheless, its performance was considered to be acceptable. Near the opposite extreme, both Eden <ref> [3] </ref> and Argus [24] were built on top of a loosely coupled network of minicomputers running Unix and using standard network protocols. In these systems, Unix processes encapsulated heavyweight objects, while different programming models based on lightweight objects and threads were used within the heavyweight objects.
Reference: 4. <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. Porterfield, and B. Smith. </author> <title> The Tera computer system. </title> <booktitle> In Proceedings of the 1990 International Conference on Supercomputing, </booktitle> <pages> pages 1-6, </pages> <month> September </month> <year> 1990. </year>
Reference-contexts: Multiprogramming to avoid the penalty of I/O latency is the oldest and most prominent example of hiding latency through concurrency. System designs that preclude effective multitasking to tolerate interprocessor communication latency should be avoided. Latency tolerance in hardware can allow very fine-grain interaction. For example, the Tera computer <ref> [4] </ref> is designed to perform an "instantaneous" context switch on ev ery memory operation. Less extreme designs in the hardware of Alewife [1] and flT [25] or the software of Active Messages [30] attack both of these potential problems by minimizing operation overhead and by facilitating concurrent activity.
Reference: 5. <author> B.N. Bershad, E.D. Lazowska, and H.M. Levy. </author> <title> PRESTO: a system for object-oriented parallel programming. </title> <journal> Software: Practice and Experience, </journal> <volume> 18(8) </volume> <pages> 713-732, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: The relative costs of communication in each architecture class have determined the style of object-oriented systems that have been built upon them. For example, PRESTO <ref> [5] </ref> is a straightforward "threads" library that extends C++ for shared-memory-style parallel programming. It was written originally for a 1987-vintage Sequent multiprocessor, a machine with memories and bus that are fast relative to processor speed.
Reference: 6. <author> A. Black, N. Hutchinson, E. Jul, and H. Levy. </author> <title> Object structure in the Emerald system. </title> <booktitle> In Proceedings of the ACM Conference on Object-Oriented Programming Systems, Languages and Applications, </booktitle> <pages> pages 78-86, </pages> <month> October </month> <year> 1986. </year> <journal> Special Issue of SIGPLAN Notices, </journal> <volume> Volume 21, Number 11, </volume> <month> November, </month> <year> 1986. </year>
Reference: 7. <author> W.J. Bolosky, M.L. Scott, R.P. Fitzgerald, R.J. Fowler, and A.L. Cox. </author> <title> NUMA poli-cies and their relation to memory architecture. </title> <booktitle> In Proceedings of the 4th Symposium on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 212-221, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: It was written originally for a 1987-vintage Sequent multiprocessor, a machine with memories and bus that are fast relative to processor speed. Both PRESTO and the applications written for it on the Sequent freely used fine-grain sharing of data and C++ objects, respectively, among the available processors <ref> [7] </ref> with little concern for locality of reference. Nevertheless, its performance was considered to be acceptable. Near the opposite extreme, both Eden [3] and Argus [24] were built on top of a loosely coupled network of minicomputers running Unix and using standard network protocols.
Reference: 8. <author> R. Chandra, A. Gupta, and J. Hennessy. </author> <title> Data locality and load balancing in COOL. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 249-259, </pages> <month> April </month> <year> 1991. </year>
Reference: 9. <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Integrating Concurrency and Data Abstraction in a Parallel Programming Language. </title> <type> Technical Report No. </type> <institution> CSL-TR-92-511, Computer Systems Laboratory, Stanford University, </institution> <month> February </month> <year> 1992. </year>
Reference: 10. <institution> Alpha AXP architecture and sytems. </institution> <note> Digital Technical Journal, 4(4), Special Issue 1992. </note>
Reference-contexts: Dramatic improvements in processor speed due to more sophisticated designs and better integrated circuit technologies require performance-conscious software designers to consider the physical distribution of the components of shared-memory multiprocessors. A bus transaction on the DEC 10000 AXP multiprocessor transfers 64 bytes and has a latency of 340 nanoseconds <ref> [10] </ref>. 5 Measured by the cycle time of a dual-issue 200-MHz processor, this constitutes 68 processor clocks in which as many as 136 instructions could be issued.
Reference: 11. <author> S. Dwarkadas, P. Kelleher, A.L. Cox, and W. Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 244-255, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: When combined with high performance (ATM) network technology, software distributed shared memory systems can perform competitively with hardware shared memory systems on medium- and coarse-grain applications <ref> [11] </ref>. Related techniques are being applied to object-based software distributed memory systems [12]. Operation latency is a problem only if that latency cannot be hidden by concurrent execution of other useful tasks.
Reference: 12. <author> M.J. Feeley and H.M. Levy. </author> <title> Distributed shared memory with versioned objects. </title> <booktitle> In OOPSLA '92 Conference Proceedings, </booktitle> <pages> pages 247-262, </pages> <month> October </month> <year> 1992. </year>
Reference-contexts: When combined with high performance (ATM) network technology, software distributed shared memory systems can perform competitively with hardware shared memory systems on medium- and coarse-grain applications [11]. Related techniques are being applied to object-based software distributed memory systems <ref> [12] </ref>. Operation latency is a problem only if that latency cannot be hidden by concurrent execution of other useful tasks. Multiprogramming to avoid the penalty of I/O latency is the oldest and most prominent example of hiding latency through concurrency.
Reference: 13. <author> R.J. Fowler and L.I. Kontothanassis. </author> <title> Improving Processor and Cache Locality in Fine-Grain Parallel Computations using Object-Affinity Scheduling and Continuation Passing. </title> <type> Technical Report TR-411, </type> <institution> University of Rochester, Department of Computer Science, </institution> <year> 1992. </year>
Reference-contexts: A comparison of the speedups obtained for a successive over-relaxation program. A coarse-grain program is compared to fine-grain alternatives available in Mercury. 7 40-MHz R3000 processors, 80-MB/sec bus, 360-nanosecond (15 processor cycles) bus latency for a 16-byte bus transfer, 1-MB secondary caches. For example, the Mercury runtime library <ref> [13] </ref> reimplements and extends PRESTO. The reimplementation aspect retains backward compatibility while improving locality of reference of internal mechanisms. The extensions provide a mechanism called "Object-Affinity Scheduling" (O-AS). 8 An object invocation either can be performed as a local procedure call or can be encapsulated into a lightweight task.
Reference: 14. <author> John L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: A Quantitative Approach. </title> <publisher> Morgan Kaufman, </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: High processor-memory bandwidth is attained by transferring a large amount of data in each operation. Write buffers allow processors to continue execution before a write operation is complete <ref> [14] </ref>, thus reducing the cost of writing to memory. While these techniques are extremely successful at reducing average memory access cost in a uniprocessor, the fact remains that interprocessor communication through a shared variable still requires a relatively expensive communication operation. 5 Bus transactions are pipelined.
Reference: 15. <author> Mark D. Hill and James R. Larus. </author> <title> Cache considerations for multiprocessor programmers. </title> <journal> Communications of the ACM, </journal> <volume> 33(8) </volume> <pages> 97-102, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: Any computer system with a memory hierarchy is sensitive to the locality of reference exhibited by the programs it executes. To achieve good performance, a programmer must avoid algorithms and data structures with poor locality. The problem is especially important on multiprocessors <ref> [15] </ref>. In each case, designers and programmers strike a bargain in which an easy-to-program, general, and uniform set of abstractions is obtained in exchange for a loss of direct control over performance. As long as the system does reasonably well, this is a good compromise.
Reference: 16. <author> D.B. Johnson and W. Zwaenepoel. </author> <title> The Peregrine high-performance RPC system. </title> <journal> Software: Practice and Experience, </journal> <volume> 23(2) </volume> <pages> 201-221, </pages> <month> February </month> <year> 1993. </year>
Reference: 17. <author> Norman P. Jouppi. </author> <title> Cache write policies and performance. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 191-201, </pages> <year> 1993. </year>
Reference-contexts: If the granularity of interaction is coarse enough after the compiler and run-time libraries have done their work, good performance can be obtained from Software Distributed Shared Memory systems [23]. If memory need only be consistent at synchronization points, finer granularities can be handled by multi-writer protocols in hardware <ref> [17] </ref> or software [19] that can consolidate many small memory operations into a larger message. When combined with high performance (ATM) network technology, software distributed shared memory systems can perform competitively with hardware shared memory systems on medium- and coarse-grain applications [11].
Reference: 18. <author> E. Jul, H. Levy, N. Hutchinson, and A. Black. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference: 19. <author> P. Keleher, A. Cox, and W. Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: If memory need only be consistent at synchronization points, finer granularities can be handled by multi-writer protocols in hardware [17] or software <ref> [19] </ref> that can consolidate many small memory operations into a larger message. When combined with high performance (ATM) network technology, software distributed shared memory systems can perform competitively with hardware shared memory systems on medium- and coarse-grain applications [11].
Reference: 20. <author> D. Kranz, K. Johnson, A. Agarwal, J. Kubiatowicz, and B-H. Lim. </author> <title> Integrating message-passing and shared-memory: early experience. </title> <booktitle> In Proceedings of the Fourth ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <address> San Diego, </address> <month> May </month> <year> 1993. </year>
Reference: 21. <author> J. Kubiatowicz and A. Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the 1993 ACM International Conference on Supercomputing, </booktitle> <pages> pages 195-206, </pages> <address> Tokyo, Japan, </address> <month> July </month> <year> 1993. </year>
Reference: 22. <author> D. Lenoski, J. Laudon, K. Gharachorloo, A. Gupta, and J. Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: These routers provide switching delays of 30 ns per hop and 60 Mbytes per second per channel [1]. The Stanford DASH <ref> [22] </ref> uses a similar network. On DASH, a cache miss that requires internode communication can be handled with a latency of about 3.2 microseconds [22], i.e., one order of magnitude larger than a miss on an SGI 4D. <p> These routers provide switching delays of 30 ns per hop and 60 Mbytes per second per channel [1]. The Stanford DASH <ref> [22] </ref> uses a similar network. On DASH, a cache miss that requires internode communication can be handled with a latency of about 3.2 microseconds [22], i.e., one order of magnitude larger than a miss on an SGI 4D. Since these shared-memory systems are built upon a hardware message-passing layer, it is natural to expose this layer to the software. This strategy is part of the current Alewife design [20,21].
Reference: 23. <author> K. Li. Ivy: </author> <title> a shared virtual memory system for parallel computing. </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing, </booktitle> <volume> 2 </volume> <pages> 94-101, </pages> <month> August </month> <year> 1988. </year>
Reference-contexts: In such programs, spatial, temporal, and processor locality all work to control cache miss rates. If the granularity of interaction is coarse enough after the compiler and run-time libraries have done their work, good performance can be obtained from Software Distributed Shared Memory systems <ref> [23] </ref>. If memory need only be consistent at synchronization points, finer granularities can be handled by multi-writer protocols in hardware [17] or software [19] that can consolidate many small memory operations into a larger message.
Reference: 24. <author> B. Liskov. </author> <title> Distributed programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: Nevertheless, its performance was considered to be acceptable. Near the opposite extreme, both Eden [3] and Argus <ref> [24] </ref> were built on top of a loosely coupled network of minicomputers running Unix and using standard network protocols. In these systems, Unix processes encapsulated heavyweight objects, while different programming models based on lightweight objects and threads were used within the heavyweight objects.
Reference: 25. <author> R. S. Nikhil, G. M. Papadopoulos, and Arvind. </author> <title> *T: a multithreaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <address> Gold Coast, Australia, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: Latency tolerance in hardware can allow very fine-grain interaction. For example, the Tera computer [4] is designed to perform an "instantaneous" context switch on ev ery memory operation. Less extreme designs in the hardware of Alewife [1] and flT <ref> [25] </ref> or the software of Active Messages [30] attack both of these potential problems by minimizing operation overhead and by facilitating concurrent activity. Future object-based systems must be designed so that applications exhibit enough concurrency at each node to hide internode latencies.
Reference: 26. <author> L. Snyder. </author> <title> Type architectures, shared memory, and the corollary of modest potential. </title> <booktitle> In Annual Review of Computer Science, </booktitle> <pages> pages 298-317, </pages> <publisher> Annual Reviews Inc., </publisher> <year> 1986. </year>
Reference-contexts: As long as the system does reasonably well, this is a good compromise. If the system cannot make the right decisions automatically, then it is appropriate that the costs of exported abstractions and of operations on them be made as explicit as possible <ref> [26] </ref>.
Reference: 27. <author> A.S. Tanenbaum, R. van Renesse, H. van Staveren, G.J. Sharp, S.J. Mullender, J. Jansen, and G. van Rossum. </author> <title> Experiences with the Amoeba distributed operating system. </title> <journal> Communications of the ACM, </journal> <volume> 33(12) </volume> <pages> 46-63, </pages> <month> December </month> <year> 1990. </year>
Reference-contexts: Workstation Networks consist of standalone processing elements 3 joined by a commodity message-passing network. These architectures are often called "tightly coupled" or "loosely coupled", based on several criteria, one of which is the granularity of interaction the system can support 4 . 3 Exceptions include the Amoeba <ref> [27] </ref> "processor pool" and recently announced "workstation cluster" products. These consist of workstation-class single-board PEs sharing a common power supply and cabinet, but still connected using conventional LAN hardware. 4 Other criteria include administrative autonomy, protection boundaries, and the pos sibility of independent failure.
Reference: 28. <author> C. Thekkath, H. Levy, and E. Lazowska. </author> <title> Efficient Support for Multicomputing on ATM Networks. </title> <type> Technical Report TR93-04-03, </type> <institution> Department of Computer Science and Engineering, University of Washington, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: For example, on a network of DECstations connected by a 140 Mb/s ATM network, a group at the University of Washington implemented a network interface based on an explicitly shared memory segment model with block reads and writes <ref> [28] </ref>. They claim an end-to-end latency of 30 microseconds to transfer a 40-byte block with a write operation 10 and 45 microseconds to transfer the same data using a read involving a round trip on the network. <p> The latency for small messages is less than that seen on either the CM-5 or the Intel Touchstone DELTA. In terms of sustained throughput between a pair of nodes, this system is within a factor of two of the CM-5 and within a factor of 3 of the DELTA <ref> [28] </ref>. 2.4 What Do the Numbers Mean? In the systems described, there is a gap of about two orders of magnitude in the latency of minimal interprocessor interaction (including software overhead) between a bus-based shared memory system and a high-performance workstation network.
Reference: 29. <author> R. van Renesse, H. van Staveren, </author> <title> and A.S. Tanenbaum. Performance of the Amoeba distributed operating system. </title> <journal> Software: Practice and Experience, </journal> <volume> 19 </volume> <pages> 223-234, </pages> <month> March </month> <year> 1989. </year>
Reference: 30. <author> T. von Eicken, D.E. Culler, S.C. Goldstein, and K.E. Schauser. </author> <title> Active messages: a mechanism for integrated communication and computation. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 256-266, </pages> <month> May </month> <year> 1992. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Latency tolerance in hardware can allow very fine-grain interaction. For example, the Tera computer [4] is designed to perform an "instantaneous" context switch on ev ery memory operation. Less extreme designs in the hardware of Alewife [1] and flT [25] or the software of Active Messages <ref> [30] </ref> attack both of these potential problems by minimizing operation overhead and by facilitating concurrent activity. Future object-based systems must be designed so that applications exhibit enough concurrency at each node to hide internode latencies.
References-found: 30

