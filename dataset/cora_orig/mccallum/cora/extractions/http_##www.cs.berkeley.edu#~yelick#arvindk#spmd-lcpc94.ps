URL: http://www.cs.berkeley.edu/~yelick/arvindk/spmd-lcpc94.ps
Refering-URL: http://www.cs.berkeley.edu/~yelick/papers.html
Root-URL: 
Email: farvindk,yelickg@cs.berkeley.edu  
Title: Optimizing Parallel SPMD Programs  
Author: Arvind Krishnamurthy and Katherine Yelick 
Address: Berkeley  
Affiliation: University of California at  
Abstract: We present compiler optimization techniques for explicitly parallel programs that communicate through a shared address space. The source programs are written in a single program multiple data (SPMD) style, and the machine target is a multiprocessor with physically distributed memory and hardware or software support for a single address space. Unlike sequential programs or data-parallel programs, SPMD programs require cycle detection, as defined by Shasha and Snir, to perform any kind of code motion on shared variable accesses. Cycle detection finds those accesses that, if reordered by either the hardware or software, could violate sequential consistency. We improve on Shasha and Snir's algorithm for cycle detection by providing a polynomial time algorithm for SPMD programs, whereas their formulation leads to an algorithm that is exponential in the number of processors. Once cycles and local dependencies have been computed, we perform optimizations to overlap communication and computation, change two-way communication into one-way communication, and apply scalar code optimizations. Using these optimizations, we improve the execution times of certain application kernels by about 20-50%.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> B. Alpern, L. Carter, and E. Feig. </author> <title> Uniform memory hierarchies, </title> <address> Oct. </address> <year> 1990. </year>
Reference-contexts: These include improved scalar optimizations, making local cached copies of remote values, and storing a shared value in a register. The last two fall into the general class of optimizations that move values up the memory hierarchy to keep them closer to the processor <ref> [1] </ref>. The primary contribution of this paper is a new polynomial time algorithm for cycle detection in SPMD programs. This improves on the running time of the algorithm by Shasha and Snir, which is exponential in the number of processors.
Reference: 2. <author> H. Berryman, J. Saltz, and J. Scroggs. </author> <title> Execution time support for adaptive scientific algorithms on distributed memory multiprocessors. </title> <journal> Concurrenty: Practice and Experience, </journal> <pages> pages 159-178, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Compilers and runtime systems for data parallel languages like HPF and Fortran-D [7] implement message pipelining optimizations. The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to generate code for overlapping communication, aggregating groups of messages, and other optimizations <ref> [2] </ref>. These optimizations have also been studied in the context of parallelizing compilers [14]. However, as discussed earlier, compiling data parallel programs is fundamentally different than compiling SPMD programs.
Reference: 3. <author> F. Bodin, P. Beckman, D. Gannon, S. Yang, S. Kesavan, A. Maloney, and B. Mohr. </author> <title> Implementing a parallel C++ runtime system for scalable parallel system. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 588-597, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: This model is popular for writing libraries like ScaLapack and runtime systems for high level languages like HPF [6] and pC++ <ref> [3] </ref>. The uniprocessor compilers that analyze and compile the SPMD programs are ill-suited to the task, because they do not have information about the semantics of the communication and synchronization mechanisms.
Reference: 4. <author> S. Chatterjee, J. Gilbert, R. Schreiber, and S.-H. Teng. </author> <title> Optimal evaluation of array expressions on massively parallel machines. </title> <booktitle> In Workshop on Languages, Compilers and Run-Time Environments for Distributed Memory Multiprocessors, </booktitle> <pages> pages 68-71, </pages> <year> 1993. </year>
Reference-contexts: The shared array construct is directly mapped into a Split-C spread array declaration. The problem of choosing layouts to reduce communication is orthogonal; layout information could come from the programmer, as in HPF or Split-C [6, 5], or from a separate analysis phase <ref> [4] </ref>. The most important feature of the Split-C language is its support for split-phase memory operations.
Reference: 5. <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel programming in Split-C. </title> <booktitle> In Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <address> Portland, Oregon, </address> <month> November </month> <year> 1993. </year>
Reference-contexts: In this paper, we consider the common special case of Single Program Multiple Data (SPMD) programs, where multiple copies of a uniprocessor program communicate either through a shared address space <ref> [5] </ref> or through messages. This model is popular for writing libraries like ScaLapack and runtime systems for high level languages like HPF [6] and pC++ [3]. <p> Two important optimizations for these multiprocessors are communication overlap and the elimination of round-trip message traffic. The first optimization, message pipelining, changes remote read and write operations into their split-phase analogs, get and put. In a split-phase operation, the initiation of an access is separated from its completion <ref> [5] </ref>. The operation to force completion of outstanding split-phase operations comes in many forms, the simplest of which (called sync or fence) blocks until all outstanding accesses are complete. To improve communication overlap, puts and gets are moved backwards in the program execution and syncs are moved forward. <p> This improves on the running time of the algorithm by Shasha and Snir, which is exponential in the number of processors. This analysis is used to perform optimizations such as message pipelining by using the portable Split-C runtime system as an example backend <ref> [5] </ref>. We describe the compilation and optimization problem for a simple shared memory language. The target language is Split-C, which is described in section 2. We present basic terminology needed for the analysis in section 3 and the analysis itself in section 5.1. <p> Section 7 estimates the potential payoffs of our approach by optimizing a few application kernels. Related work is surveyed in section 8, and conclusions drawn in section 9. 2 The Target Language Our target language is Split-C, a SPMD language for programming distributed memory machines <ref> [5] </ref>. Split-C provides a global address space through two mechanisms: a global distributed heap and distributed arrays. <p> The shared array construct is directly mapped into a Split-C spread array declaration. The problem of choosing layouts to reduce communication is orthogonal; layout information could come from the programmer, as in HPF or Split-C <ref> [6, 5] </ref>, or from a separate analysis phase [4]. The most important feature of the Split-C language is its support for split-phase memory operations.
Reference: 6. <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification version 1.0. </title> <type> Draft, </type> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: This model is popular for writing libraries like ScaLapack and runtime systems for high level languages like HPF <ref> [6] </ref> and pC++ [3]. The uniprocessor compilers that analyze and compile the SPMD programs are ill-suited to the task, because they do not have information about the semantics of the communication and synchronization mechanisms. <p> The shared array construct is directly mapped into a Split-C spread array declaration. The problem of choosing layouts to reduce communication is orthogonal; layout information could come from the programmer, as in HPF or Split-C <ref> [6, 5] </ref>, or from a separate analysis phase [4]. The most important feature of the Split-C language is its support for split-phase memory operations.
Reference: 7. <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng. </author> <title> Compiler optimziations for Fortran D on MIMD distributed-memory machines. </title> <booktitle> In Proceedings of the 1991 International Conference on Supercomputing, </booktitle> <year> 1991. </year>
Reference-contexts: The algorithm presented in this paper for SPMD programs does not deal with array analysis, but we believe their techniques for handling array subscripts could be incorporated into our SPMD framework. Compilers and runtime systems for data parallel languages like HPF and Fortran-D <ref> [7] </ref> implement message pipelining optimizations. The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to generate code for overlapping communication, aggregating groups of messages, and other optimizations [2]. These optimizations have also been studied in the context of parallelizing compilers [14].
Reference: 8. <author> A. Krishnamurthy. </author> <title> Optimizing explicitly parallel programs. </title> <type> Technical Report CSD-94-835, </type> <institution> University of California, Berkeley, </institution> <month> September </month> <year> 1994. </year>
Reference-contexts: It eliminates the condition that a back-path must pass through each program segment at most once. For SPMD programs, the delay edges computed by our algorithm is still minimal. 2 The construction and the proof for the following theorem is in <ref> [8] </ref> We first describe a transformation of the given control flow graph and then present an algorithm for detecting back-paths in the resulting graph. We show that the delay edges computed for the transformed graph are the same as in Shasha and Snir's approach. <p> Thus, a relative speed of 0.5 corresponds to a factor of 2 speedup. Other optimizations, such as caching remote values, are also enabled by our analysis, and result in additional performance improvements on some of these applications <ref> [8] </ref>. 8 Related Work Most of the research in optimizing parallel programs has been for data parallel programs. In the more general control parallel setting, Midkiff and Padua [11] describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied.
Reference: 9. <author> L. Lamport. </author> <title> How to make a multiprocessor computer that correctly executes mul-tiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> Septem-ber </month> <year> 1979. </year>
Reference-contexts: Intuitively, the parallel programmer relies on the notion of sequential consistency, which says the parallel execution must behave as if it is an interleaving of the sequences of memory operations from each of the processors <ref> [9] </ref>. As a more useful program example, assume that X is a data structure being produced by processor 2 and Y is a "presence" bit to denote that it has been produced.
Reference: 10. <author> S. Luna. </author> <title> Implementing an efficient global memory portability layer on distributed memory multiprocessors. </title> <type> Master's thesis, </type> <institution> University of California, Berkeley, </institution> <month> May </month> <year> 1994. </year>
Reference-contexts: It exposes the efficiency of one-way communication in those cases where the communication pattern is well understood. Split-C runs on top of Active Messages on the CM5, and there are prototype implementations for the Paragon, SP-1, and a workstation network <ref> [10] </ref>. It defines a portability layer with fast, non-blocking remote accesses that, unlike large message passing systems, can be implemented without message buffering on both ends [16].
Reference: 11. <author> S. Midkiff and D. Padua. </author> <title> Issues in the optimization of parallel programs. </title> <booktitle> In International Conference on Parallel Processing - Vol II, </booktitle> <pages> pages 105-113, </pages> <year> 1990. </year>
Reference-contexts: As a result, they miss opportunities for optimizing communication and synchronization, and the quality of the scalar code is limited by the inability to move code around parallelism primitives <ref> [11] </ref>. Cycle detection requires finding conflicts between concurrent code blocks, which are pairs of accesses to the same location from two different processors where at least one is a write. Cycle detection requires alias information, and is therefore similar to dependence analysis in parallelizing compilers. <p> In the more general control parallel setting, Midkiff and Padua <ref> [11] </ref> describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir [15], which was later extended by Midkiff et al [12] to handle array based accesses.
Reference: 12. <author> S. P. Midkiff, D. Padua, and R. G. Cytron. </author> <title> Compiling programs with user parallelism. </title> <booktitle> In Languages and Compilers for Parallel Computing, </booktitle> <pages> pages 402-422, </pages> <year> 1990. </year>
Reference-contexts: Analysis for these programs is based on the pioneering work by Shasha and Snir [15], which was later extended by Midkiff et al <ref> [12] </ref> to handle array based accesses. However, their analysis technique is computationally expensive even for programs with a small degree of parallelism since both the minimal cycle detection problem and the array subscript analysis problem have exponential running times.
Reference: 13. <author> W. Oed. </author> <title> The Cray research massively processor system: T3D. </title> <note> Ftp from ftp.cray.com, </note> <month> Nov. </month> <year> 1993. </year>
Reference-contexts: Our target machine is a multiprocessor with physically distributed memory and hardware or software support for a global address space. A remote reference on such a machine has a long latency, from roughly 80 cycles on a Cray T3D <ref> [13] </ref> to 400 cycles on a CM5 using Active Messages [16]. However, most of this latency can be overlapped with local computation or with the initiation of more communication, especially on machines like the J-Machine and *T, with their low overheads for communication startup.
Reference: 14. <author> A. Rogers and K. Pingali. </author> <title> Compiling for distributed memory architectures. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <month> march </month> <year> 1994. </year>
Reference-contexts: The Parti runtime system and associated HPF compiler uses a combination of compiler and runtime analysis to generate code for overlapping communication, aggregating groups of messages, and other optimizations [2]. These optimizations have also been studied in the context of parallelizing compilers <ref> [14] </ref>. However, as discussed earlier, compiling data parallel programs is fundamentally different than compiling SPMD programs. First, it is the compiler's responsibility to map parallelism of degree n (the size of a data structure) to a machine with PROCS processors, which can sometimes lead to significant runtime overhead.
Reference: 15. <author> D. Shasha and M. Snir. </author> <title> Efficient and correct execution of parallel programs that share memory. </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> 10(2) </volume> <pages> 282-312, </pages> <month> April </month> <year> 1988. </year>
Reference-contexts: However, our goal in optimizing SPMD is more modest: the main source of parallelism has been exposed by the application programmer, so our job is to optimize the parallel code by making better use of local processor and network resources. Cycle detection was first described by Shasha and Snir <ref> [15] </ref> and later extended by Midkiff, Padua, and Cytron to handle array indices. Their formulation gives an algorithm that is exponential in the number of processors and requires P ROCS (the number of processors) copies of the program. <p> + possessing back-paths: D S&S = f [a i ; a j ] 2 P + k [a i ; a j ] has a back-path in P + [ Cg Shasha and Snir proved that if D S&S is observed, the execution will be se quentially consistent: Theorem 3. <ref> [15] </ref> D S&S is sufficient. They also proved that D S&S in some sense characterizes the minimal delays: in any execution in which an edge in D S&S executes out of order, there could be a violation of sequential consistency. <p> as we would like, because it ignores the existence of control structures and synchronization that can prevent reorderings from happening even though the cycles exist statically. 5 Shasha and Snir's Algorithm is Exponential Although Shasha and Snir do not specify the details of an algorithm for computing back-paths, they claim <ref> [15] </ref> there is a polynomial time algorithm for detection of backpaths in a program that "consists of a fixed number of serial program segments." In practice, one does not (typically) compile a program for a fixed number of processors: either the language contains constructs for dynamically creating parallel threads, or there <p> In the more general control parallel setting, Midkiff and Padua [11] describe eleven different instances where standard optimizations (like code motion and dead code elimination) cannot be directly applied. Analysis for these programs is based on the pioneering work by Shasha and Snir <ref> [15] </ref>, which was later extended by Midkiff et al [12] to handle array based accesses. However, their analysis technique is computationally expensive even for programs with a small degree of parallelism since both the minimal cycle detection problem and the array subscript analysis problem have exponential running times.
Reference: 16. <author> T. von Eicken, D. E. Culler, S. C. Goldstein, and K. E. Schauser. </author> <title> Active Messages: a Mechanism for Integrated Communication and Computation. </title> <booktitle> In International Symposium on Computer Architecture, </booktitle> <year> 1992. </year>
Reference-contexts: Our target machine is a multiprocessor with physically distributed memory and hardware or software support for a global address space. A remote reference on such a machine has a long latency, from roughly 80 cycles on a Cray T3D [13] to 400 cycles on a CM5 using Active Messages <ref> [16] </ref>. However, most of this latency can be overlapped with local computation or with the initiation of more communication, especially on machines like the J-Machine and *T, with their low overheads for communication startup. Two important optimizations for these multiprocessors are communication overlap and the elimination of round-trip message traffic. <p> Split-C runs on top of Active Messages on the CM5, and there are prototype implementations for the Paragon, SP-1, and a workstation network [10]. It defines a portability layer with fast, non-blocking remote accesses that, unlike large message passing systems, can be implemented without message buffering on both ends <ref> [16] </ref>.
References-found: 16

