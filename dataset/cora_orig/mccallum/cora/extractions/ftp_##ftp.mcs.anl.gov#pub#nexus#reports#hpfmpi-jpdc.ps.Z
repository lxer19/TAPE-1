URL: ftp://ftp.mcs.anl.gov/pub/nexus/reports/hpfmpi-jpdc.ps.Z
Refering-URL: http://www.mcs.anl.gov/fortran-m/
Root-URL: http://www.mcs.anl.gov
Email: ffoster,kohrg@mcs.anl.gov  rakesh@cat.syr.edu  choudhar@ece.nwu.edu  
Title: A Library-Based Approach to Task Parallelism in a Data-Parallel Language  
Author: Ian Fostery David R. Kohr, Jr. Rakesh Krishnaiyerz Alok Choudharyx 
Address: Argonne, IL 60439  Syracuse, NY 13244  2145 Sheridan Road Evanston, Illinois 60208-3118  
Affiliation: yMathematics and Computer Science Division Argonne National Laboratory  zDepartment of Computer and Information Science Syracuse University  xECE Department, Technological Institute Northwestern University,  
Date: November 1996.  
Note: Submitted to the Journal of Parallel and Distributed Computing,  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> The Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 16, 22, 27] </ref>. 7 High Performance Fortran [17] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> An HPF compiler normally generates a single-program, multiple-data (SPMD) parallel program by applying the owner computes rule to partition the operations performed by the program; the processor that "owns" a value is responsible for updating its value <ref> [1, 22, 27] </ref>. The compiler also introduces communication operations when local computation requires remote data. An attractive feature of this implementation strategy is that the mapping from user program to executable code is fairly straightforward. Hence, programmers can understand how changes in program text affect performance.
Reference: [2] <author> N. Carriero and D. Gelernter. </author> <title> Linda in context. </title> <journal> Communications of the ACM, </journal> <volume> 32(4) </volume> <pages> 444-458, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: Language-based approaches. Advocates of explicit, language-based approaches propose new language constructs that allow programmers to specify the creation and coordination of tasks explicitly. The basic concept is that of a coordination language <ref> [2, 9] </ref>, except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel 10 tasks.
Reference: [3] <author> G. Cheng, G. Fox, and K. Mills. </author> <title> Integrating multiple programming paradigms on Connection Machine CM5 in a dataflow-based software environment. </title> <type> Technical report, </type> <institution> Northeast Parallel Architectures Center, Syracuse University, </institution> <year> 1993. </year>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [11, 14, 21], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 6, 19, 24] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, and language-based approaches also introduce the need to standardize new language features. <p> The basic concept is that of a coordination language [2, 9], except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel 10 tasks. Language-based approaches have been proposed that use a graphical notation <ref> [3] </ref>, channels [6], remote procedure calls [19], and a simple pipeline notation [24] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [4] <author> A. N. Choudhary, B. Narahari, D. M. Nicol, and R. Simha. </author> <title> Optimal processor assignment for pipeline computations. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 5(4) </volume> <pages> 439-445, </pages> <year> 1994. </year>
Reference-contexts: However, signal-processing systems must often process quickly a stream of arrays of relatively small size. (The array size corresponds to the sensor resolution and might be 256fi256 or less.) In these situations, an alternative pipelined algorithm is often more efficient <ref> [4, 14] </ref>. The alternative algorithm partitions the FFT computation among the processors such that P=2 processors perform the read and the first set of 1-D FFTs, while the other P=2 perform the second set of 1-D FFTs and the write. <p> The performance of the HPF/MPI version is generally better. In particular, for a fixed image size, HPF/MPI provides an increasing improvement in speedup as P increases. 21 6.2 2-D Convolution Convolution is a standard technique used to extract feature information from images <ref> [4, 23] </ref>. It involves two 2-D FFTs, an elementwise multiplication, and an inverse 2-D FFT and is applied to two streams of input images to generate a single output stream.
Reference: [5] <author> G. Edjlali, A. Sussman, and J. Saltz. </author> <title> Interoperability of data parallel runtime libraries with Meta-Chaos. </title> <institution> Technical Report CS-TR-3633 and UMIACS-TR-96-30, University of Maryland, Department of Computer Science and UMIACS, </institution> <month> May </month> <year> 1996. </year> <note> A condensed version submitted to Supercomputing'96. </note>
Reference-contexts: Groups building multidisciplinary models frequently build specialized "couplers", responsible for transferring data from one model to another. Coupler toolkits have been proposed and built, but not widely adopted. MetaCHAOS <ref> [5] </ref> provides a more general coupling tool by defining a model in which programs can export and import distributed data structures; MetaCHAOS handles communication scheduling.
Reference: [6] <author> I. Foster, B. Avalani, A. Choudhary, and M. Xu. </author> <title> A compilation system that integrates High Performance Fortran and Fortran M. </title> <booktitle> In Proceedings of the 1994 Scalable High Performance Computing Conference, </booktitle> <pages> pages 293-300. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1994. </year>
Reference-contexts: Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. Examples of applications that are not easily expressed using HPF alone <ref> [6, 14] </ref> include multidisciplinary applications where different modules represent distinct scientific disciplines, programs that interact with user interface devices, applications involving irregularly structured data such as multiblock codes, and image-processing applications in which pipeline structures can be used to increase performance. <p> These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [11, 14, 21], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 6, 19, 24] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, and language-based approaches also introduce the need to standardize new language features. <p> This distribution allows the rowfft routine to proceed without communication. However, the transposition A=transpose (A) involves all-to-all communication. 8 2.2 Task Parallelism Certain important program structures and application classes are not directly expressible in HPF <ref> [6, 14] </ref>. For example, both real-time monitoring and computational steering require that programmers connect a data-parallel simulation code to another sequential or parallel program that handles I/O. <p> Language-based approaches have been proposed that use a graphical notation [3], channels <ref> [6] </ref>, remote procedure calls [19], and a simple pipeline notation [24] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [7] <author> I. Foster, D. R. Kohr, Jr., R. Krishnaiyer, and A. Choudhary. </author> <title> Double standards: Bringing task parallelism to HPF via the Message Passing Interface. </title> <booktitle> In Proceedings of Supercomputing '96. </booktitle> <publisher> ACM Press, </publisher> <year> 1996. </year>
Reference-contexts: A preliminary report on some of the techniques and results presented here appeared as <ref> [7] </ref>; the present paper provides a more detailed description of our techniques and introduces additional optimizations that improve performance by a factor of two or more in some situations. The problem of parallel program coupling has been investigated by a number of other groups, although not in this standards-based fashion.
Reference: [8] <author> I. Foster, D. R. Kohr, Jr., R. Krishnaiyer, and A. Choudhary. </author> <title> MPI as a coordination layer for communicating HPF tasks. </title> <booktitle> In Proceedings of the 1996 MPI Developers Conference, </booktitle> <pages> pages 68-78. </pages> <publisher> IEEE Computer Society Press, </publisher> <year> 1996. </year>
Reference-contexts: For brevity, we examine only the case of point-to-point operations on distributed-memory multi-computers; elsewhere we discuss techniques for implementing other operations <ref> [8] </ref>. Figure 3 illustrates the basic processing steps performed by our library for a single point-to-point transfer. The actions taken by senders and receivers are symmetrical, so it suffices to examine just the processing steps of a send operation. These seven steps are as follows: 1. Distribution inquiry. <p> Our HPF/MPI implementation of these calls computes a communication schedule just once, when the request is defined. Subsequent calls to MPI Start reuse the schedule, so that costs associated with Steps 1, 3, and 4 can be amortized over many transfers. In <ref> [8] </ref> we discuss how other MPI optimization features could be incorporated into HPF/MPI. 16 5 Performance Studies We use a simple microbenchmark to quantify the costs associated with the implementation scheme just described.
Reference: [9] <author> I. Foster and S. Taylor. Strand: </author> <title> New Concepts in Parallel Programming. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1990. </year>
Reference-contexts: Language-based approaches. Advocates of explicit, language-based approaches propose new language constructs that allow programmers to specify the creation and coordination of tasks explicitly. The basic concept is that of a coordination language <ref> [2, 9] </ref>, except that because the tasks are themselves data-parallel programs, we obtain a hierarchical execution model in which task-parallel computation structures orchestrate the execution of multiple data-parallel 10 tasks.
Reference: [10] <author> K. S. Gatlin and S. B. Baden. </author> <title> Brick: A benchmark for irregular block structured applications. </title> <type> Technical report, </type> <institution> University of California at San Diego, Department of Computer Science and Engineering, </institution> <year> 1996. </year>
Reference-contexts: A solver is run within each block, and boundary data is exchanged between blocks periodically. For our experiments, we use a program that applies a simple Poisson solver within each block and that 22 supports only simple geometries <ref> [10] </ref>. For ease in HPF implementation, we fixed the number of blocks to 3. We chose a geometry such that each block is square, but the middle block has one-fourth the area of the end blocks.
Reference: [11] <author> M. Girkar and C. Polychronopoulos. </author> <title> Automatic extraction of functional parallelism from ordinary programs. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 166-178, </pages> <year> 1992. </year>
Reference-contexts: Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [11, 14, 21] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 6, 19, 24]. Both approaches have shown promise in certain application areas, but each also has disadvantages.
Reference: [12] <author> W. Gropp, E. Lusk, N. Doss, and A. Skjellum. </author> <title> A high-performance, portable implementa-tion of the MPI message passing interface standard. </title> <type> Technical Report ANL/MCS-TM-213, </type> <institution> Mathematics and Computer Science Division, Argonne National Laboratory, Argonne, Ill., </institution> <year> 1996. </year>
Reference-contexts: MPI standardizes an interaction model that has been widely used and is well understood within the high-performance computing community. It defines functions for both point-to-point and collective communication among tasks executing in separate address spaces; its definition permits efficient implementations on both shared and distributed-memory computers <ref> [12] </ref>. Our HPF/MPI library allows these same functions to be used to communicate and synchronize among HPF tasks. This integration of two parallel programming standards allows us to incorporate useful new functionality into HPF programming environments without requiring complex new directives or compiler technology. <p> Previous MPI implementations have supported bindings only for the sequential languages C and Fortran 77 <ref> [12] </ref>. However, there is no reason why MPI functions may not also be used for communication among data-parallel tasks. Our HPF binding for MPI makes this possible.
Reference: [13] <author> W. Gropp, E. Lusk, and A. Skjellum. </author> <title> Using MPI: Portable Parallel Processing with the Message-Passing Interface. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1994. </year>
Reference-contexts: The potentially complex communication and synchronization operations required to transfer data among process groups are encapsulated within the coordination library implementations. To illustrate and explore this approach, we have defined and implemented a library that allows the use of a subset of the Message Passing Interface (MPI) <ref> [13] </ref> to coordinate HPF tasks. MPI standardizes an interaction model that has been widely used and is well understood within the high-performance computing community.
Reference: [14] <author> T. Gross, D. O'Hallaron, and J. Subhlok. </author> <title> Task parallelism in a High Performance Fortran framework. </title> <journal> IEEE Parallel and Distributed Technology, </journal> <volume> 2(2) </volume> <pages> 16-26, </pages> <month> Fall </month> <year> 1994. </year>
Reference-contexts: Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. Examples of applications that are not easily expressed using HPF alone <ref> [6, 14] </ref> include multidisciplinary applications where different modules represent distinct scientific disciplines, programs that interact with user interface devices, applications involving irregularly structured data such as multiblock codes, and image-processing applications in which pipeline structures can be used to increase performance. <p> Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [11, 14, 21] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 6, 19, 24]. Both approaches have shown promise in certain application areas, but each also has disadvantages. <p> This distribution allows the rowfft routine to proceed without communication. However, the transposition A=transpose (A) involves all-to-all communication. 8 2.2 Task Parallelism Certain important program structures and application classes are not directly expressible in HPF <ref> [6, 14] </ref>. For example, both real-time monitoring and computational steering require that programmers connect a data-parallel simulation code to another sequential or parallel program that handles I/O. <p> However, signal-processing systems must often process quickly a stream of arrays of relatively small size. (The array size corresponds to the sensor resolution and might be 256fi256 or less.) In these situations, an alternative pipelined algorithm is often more efficient <ref> [4, 14] </ref>. The alternative algorithm partitions the FFT computation among the processors such that P=2 processors perform the read and the first set of 1-D FFTs, while the other P=2 perform the second set of 1-D FFTs and the write. <p> Compiler-based approaches. Advocates of implicit, compiler-based approaches seek to develop more sophisticated compilers capable of extracting task-parallel algorithms from data-parallel specifications. Frequently, they will use new directives to trigger the application of specific transformations. This general approach has been used to exploit pipeline <ref> [14] </ref> and functional parallelism [21], for example. Implicit, compiler-based approaches maintain a deterministic, sequential reading for programs. However, these approaches also tend to increase the complexity of the mapping from user program to executable code. This increased complexity can be a disadvantage for both programmers and compiler writers.
Reference: [15] <author> W. Hillis and G. Steele, Jr. </author> <title> Data parallel algorithms. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1170-1183, </pages> <year> 1986. </year>
Reference-contexts: data parallelism by discussing data parallelism and HPF and then reviewing approaches to the extension of the data-parallel model. 2.1 Data Parallelism and HPF Data-parallel languages allow programmers to exploit the concurrency that derives from the application of the same operation to all or most elements of large data structures <ref> [15] </ref>. Data-parallel languages have significant advantages relative to the lower-level mechanisms that might otherwise be used to develop parallel programs. Programs are deterministic and have a sequential reading. This simplifies development and allows reuse of existing program development methodologies|and, with some modification, tools.
Reference: [16] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> Communications of the ACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 16, 22, 27] </ref>. 7 High Performance Fortran [17] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops.
Reference: [17] <author> C. Koelbel, D. Loveman, R. Schreiber, G. Steele Jr., and M. Zosel. </author> <title> The High Performance Fortran Handbook. </title> <publisher> The MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: 1 Introduction The data-parallel language High Performance Fortran (HPF) provides a portable, high-level notation for expressing data-parallel algorithms <ref> [17] </ref>. An HPF computation has a single-threaded control structure, global name space, and loosely synchronous parallel execution model. Many problems requiring high-performance implementations can be expressed succinctly in HPF. However, HPF does not adequately address task parallelism or heterogeneous computing. <p> In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs [1, 16, 22, 27]. 7 High Performance Fortran <ref> [17] </ref> is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> This causes the execution model of each processor in the task to change from data-parallel (globally single-threaded) to SPMD (separate threads of control on each processor, as in HPF's local mode of execution <ref> [17] </ref>). 13 3. Array descriptor exchange. Sending processors exchange distribution information with receiving processors about the source and destination arrays. After Step 1, all senders have distribution descriptors for the source array and all receivers have descriptors for the destination.
Reference: [18] <author> D. R. Kohr, Jr. </author> <title> Design and optimization of coordination mechanisms for data-parallel tasks. </title> <type> Master's thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1996. </year> <note> Online at http://www.mcs.anl.gov/fortran-m. </note>
Reference-contexts: Steps 5 and 6 are repeated once for each processor to which data must be sent. The order in which each sender transfers array subsections to each receiver is chosen so as to maximize parallelism among the individual transfers; a detailed description of this ordering appears in <ref> [18] </ref>. 14 4.2 Implementation Details Based on the above design, we have implemented a prototype HPF/MPI library that supports a subset of MPI's point-to-point communication functions. <p> The experimental data fit this simple model reasonably well. A more detailed model and more extensive analysis appear in <ref> [18] </ref>. 5.3 Performance Summary For large arrays, HPF/MPI achieves a bandwidth of about 12 megabytes/sec. in the two non-direct cases, and up to about 17 megabytes/sec. in the Persistent/Direct case. The underlying MPICH library can transfer data at a maximum rate of about 30 megabytes/sec on the SP.
Reference: [19] <author> P. Mehrotra and M. Haines. </author> <title> An overview of the Opus language and runtime system. </title> <type> ICASE Report 94-39, </type> <institution> Institute for Computer Application in Science and Engineering, Hampton, Va., </institution> <month> May </month> <year> 1994. </year> <month> 27 </month>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [11, 14, 21], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 6, 19, 24] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, and language-based approaches also introduce the need to standardize new language features. <p> Language-based approaches have been proposed that use a graphical notation [3], channels [6], remote procedure calls <ref> [19] </ref>, and a simple pipeline notation [24] to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [20] <author> S. Ramaswamy and P. Banerjee. </author> <title> Automatic generation of efficient array redistribution routines for distributed memory multicomputers. </title> <booktitle> In Frontiers '95: The 5th Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <pages> pages 342-349, </pages> <address> McLean, Va., </address> <month> February </month> <year> 1995. </year>
Reference-contexts: In the next section we quantify the overhead of using the extrinsic call mechanism. Communication schedules are generated in Step 4 using algorithms based on the FALLS (FAmiLy of Line Segments) distributed array representation of Ramaswamy and Banerjee <ref> [20] </ref>. These algorithms compute the minimal sets of array elements that must be transferred from sending to receiving processors. The algorithms rely on modulo arithmetic, and are highly efficient: for typical redistributions, their running time is proportional to the number of participating processors.
Reference: [21] <author> S. Ramaswamy, S. Sapatnekar, and P. Banerjee. </author> <title> A framework for exploiting data and functional parallelism on distributed memory multicomputers. </title> <type> Technical Report CRHC-94-10, </type> <institution> Center for Reliable and High-Performance Computing, University of Illinois, Urbana, Ill., </institution> <year> 1994. </year>
Reference-contexts: Yet they may incorporate significant data-parallel substructures. These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications <ref> [11, 14, 21] </ref>, while language-based approaches provide new language constructs for specifying task parallelism explicitly [3, 6, 19, 24]. Both approaches have shown promise in certain application areas, but each also has disadvantages. <p> Compiler-based approaches. Advocates of implicit, compiler-based approaches seek to develop more sophisticated compilers capable of extracting task-parallel algorithms from data-parallel specifications. Frequently, they will use new directives to trigger the application of specific transformations. This general approach has been used to exploit pipeline [14] and functional parallelism <ref> [21] </ref>, for example. Implicit, compiler-based approaches maintain a deterministic, sequential reading for programs. However, these approaches also tend to increase the complexity of the mapping from user program to executable code. This increased complexity can be a disadvantage for both programmers and compiler writers.
Reference: [22] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proceedings of the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <address> Portland, OR, June 1989. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In addition, programmers need not specify how data is moved between processors. On the other hand, the high level of specification introduces significant challenges for compilers, which must be able to translate data-parallel specifications into efficient programs <ref> [1, 16, 22, 27] </ref>. 7 High Performance Fortran [17] is perhaps the best-known data-parallel language. HPF exploits the data parallelism resulting from concurrent operations on arrays. These operations may be specified either explicitly by using parallel constructs (e.g., array expressions and FORALL) or implicitly by using traditional DO loops. <p> An HPF compiler normally generates a single-program, multiple-data (SPMD) parallel program by applying the owner computes rule to partition the operations performed by the program; the processor that "owns" a value is responsible for updating its value <ref> [1, 22, 27] </ref>. The compiler also introduces communication operations when local computation requires remote data. An attractive feature of this implementation strategy is that the mapping from user program to executable code is fairly straightforward. Hence, programmers can understand how changes in program text affect performance.
Reference: [23] <author> A. Rosenfeld and A. Kak. </author> <title> Digital Picture Processing. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: The performance of the HPF/MPI version is generally better. In particular, for a fixed image size, HPF/MPI provides an increasing improvement in speedup as P increases. 21 6.2 2-D Convolution Convolution is a standard technique used to extract feature information from images <ref> [4, 23] </ref>. It involves two 2-D FFTs, an elementwise multiplication, and an inverse 2-D FFT and is applied to two streams of input images to generate a single output stream.
Reference: [24] <author> B. Seevers, M. Quinn, and P. Hatcher. </author> <title> A parallel programming environment supporting data-parallel modules. </title> <journal> International Journal of Parallel Programming, </journal> <volume> 21(5), </volume> <month> October </month> <year> 1992. </year>
Reference-contexts: These observations have motivated proposals for the integration of task and data parallelism. Two principal approaches have been investigated. Compiler-based approaches seek to identify task-parallel structures automatically, within data-parallel specifications [11, 14, 21], while language-based approaches provide new language constructs for specifying task parallelism explicitly <ref> [3, 6, 19, 24] </ref>. Both approaches have shown promise in certain application areas, but each also has disadvantages. Compiler-based approaches complicate compiler development and performance tuning, and language-based approaches also introduce the need to standardize new language features. <p> Language-based approaches have been proposed that use a graphical notation [3], channels [6], remote procedure calls [19], and a simple pipeline notation <ref> [24] </ref> to connect data-parallel computations. Promising results have been obtained. Nevertheless, there is as yet no consensus on which language constructs are best.
Reference: [25] <institution> The Portland Group, Inc. </institution> <note> pghpf Reference Manual. </note> <institution> 9150 SW Pioneer Ct., Suite H, Wilsonville, Oregon 97070. </institution>
Reference-contexts: This prototype operates with the commercial HPF compiler pghpf (version 2.0), developed by the Portland Group, Inc. <ref> [25] </ref> Because of our desire for portability, we defined a run-time initialization interface between pghpf and HPF/MPI that minimizes the dependence of HPF/MPI upon the internals of the HPF runtime system.
Reference: [26] <author> V. N. Vatsa, M. D. Sanetrik, and E. B. Parlette. </author> <title> Development of a flexible and efficient multigrid-based muliblock flow solver; AIAA-93-0677. </title> <booktitle> In Proc. 31st Aerospace Sciences Meeting and Exhibit, </booktitle> <month> January </month> <year> 1993. </year> <month> 28 </month>
Reference-contexts: On the largest image size plotted (1024 x 1024), HPF/MPI provides an improvement of up to 37% over pure HPF. A comparison of the speedups is shown in Figure 11. 6.3 Multiblock Multiblock codes decompose a complex geometry into multiple simpler blocks <ref> [26] </ref>. A solver is run within each block, and boundary data is exchanged between blocks periodically. For our experiments, we use a program that applies a simple Poisson solver within each block and that 22 supports only simple geometries [10].

References-found: 26

