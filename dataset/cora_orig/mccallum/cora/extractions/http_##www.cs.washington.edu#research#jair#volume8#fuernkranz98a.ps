URL: http://www.cs.washington.edu/research/jair/volume8/fuernkranz98a.ps
Refering-URL: http://www.cs.washington.edu/research/jair/abstracts/fuernkranz98a.html
Root-URL: 
Email: juffi@cs.cmu.edu  
Title: Integrative Windowing  
Author: Johannes Furnkranz 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Note: Journal of Artificial Intelligence Research 8 (1998) 129-164 Submitted 11/97; published 5/98  
Abstract: In this paper we re-investigate windowing for rule learning algorithms. We show that, contrary to previous results for decision tree learning, windowing can in fact achieve significant run-time gains in noise-free domains and explain the different behavior of rule learning algorithms by the fact that they learn each rule independently. The main contribution of this paper is integrative windowing, a new type of algorithm that further exploits this property by integrating good rules into the final theory right after they have been discovered. Thus it avoids re-learning these rules in subsequent iterations of the windowing process. Experimental evidence in a variety of noise-free domains shows that integrative windowing can in fact achieve substantial run-time gains. Furthermore, we discuss the problem of noise in windowing and present an algorithm that is able to achieve run-time gains in a set of experiments in a simple domain with artificial noise.
Abstract-found: 1
Intro-found: 1
Reference: <author> Ali, K. M., & Pazzani, M. J. </author> <year> (1993). </year> <title> HYDRA: A noise-tolerant relational concept learning algorithm. </title> <editor> In Bajcsy, R. (Ed.), </editor> <booktitle> Proceedings of the 13th Joint International Conference on Artificial Intelligence (IJCAI-93), </booktitle> <pages> pp. 1064-1071, </pages> <address> Chambery, France. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Blum, A. L., & Langley, P. </author> <year> (1997). </year> <title> Selection of relevant features and examples in machine learning. </title> <journal> Artificial Intelligence, </journal> <volume> 97, </volume> <pages> 245-271. </pages> <note> Special Issue on Relevance. </note>
Reference: <author> Blum, A. L., & Mitchell, T. </author> <year> (1998). </year> <title> Combining labeled and unlabeled data with co-training. </title> <booktitle> In Proceedings of the 11th Annual Conference on Computational Learning Theory (COLT-98), </booktitle> <address> Madison, WI. </address> <publisher> ACM Inc. Forthcoming. </publisher>
Reference-contexts: Proposals include the use of a committee of learners for co-training <ref> (Blum & Mitchell, 1998) </ref> or techniques based on the Expectation Maximization (EM) algorithm (Nigam, McCallum, Thrun, & Mitchell, 1998), possibly coupled with an active learning procedure (McCallum & Nigam, 1998).
Reference: <author> Breiman, L. </author> <year> (1996a). </year> <title> Arcing classifiers. </title> <type> Tech. rep. 460, </type> <institution> Statistics Department, University of California at Berkeley. </institution>
Reference-contexts: Such approaches include bagging (Breiman, 1996b), where all examples are sub-sampled with equal probabilities, and boosting (Drucker, Schapire, & Simard, 1993; Freund & Schapire, 1996) | also known as arcing <ref> (Breiman, 1996a) </ref> | where examples that have been misclassified in previous iterations are more likely to be selected in the next iterations.
Reference: <author> Breiman, L. </author> <year> (1996b). </year> <title> Bagging predictors. </title> <journal> Machine Learning, </journal> <volume> 24 (2), </volume> <pages> 123-140. </pages>
Reference-contexts: Sub-sampling techniques have recently been frequently used for increasing the accuracy of a classifier. The basic idea is to train classifiers on multiple subsamples of the data and combine their predictions, usually by voting. Such approaches include bagging <ref> (Breiman, 1996b) </ref>, where all examples are sub-sampled with equal probabilities, and boosting (Drucker, Schapire, & Simard, 1993; Freund & Schapire, 1996) | also known as arcing (Breiman, 1996a) | where examples that have been misclassified in previous iterations are more likely to be selected in the next iterations.
Reference: <author> Breiman, L. </author> <year> (1996c). </year> <title> Pasting bites together for prediction in large data sets and on-line. </title> <type> Unpublished manuscript, </type> <note> available from ftp://ftp.stat.berkeley.edu/ users/breiman/pastebite.ps.Z. </note>
Reference: <author> Breiman, L., Friedman, J. H., Olshen, R., & Stone, C. </author> <year> (1984). </year> <title> Classification and Regression Trees. </title> <publisher> Wadsworth & Brooks, </publisher> <address> Pacific Grove, CA. </address>
Reference-contexts: This is ensured by requiring that AccW in (r) SE (AccW in (r)) &gt; DA (3) where DA is the default accuracy, and SE (p) = q n is the standard error of classification <ref> (Breiman et al., 1984) </ref>. 147 F urnkranz 2.
Reference: <author> Caruana, R., & Freitag, D. </author> <year> (1994). </year> <title> Greedy attribute selection. </title> <editor> In Cohen, W., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pp. 28-36, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991a). </year> <title> Megainduction: A test flight. </title> <editor> In Birnbaum, L., & Collins, G. (Eds.), </editor> <booktitle> Proceedings of the 8th International Workshop on Machine Learning (ML-91), </booktitle> <pages> pp. 596-599, </pages> <address> Evanston, IL. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Catlett, J. </author> <year> (1991b). </year> <title> Megainduction: Machine Learning on Very Large Databases. </title> <type> Ph.D. thesis, </type> <institution> Basser Department of Computer Science, University of Sydney. </institution>
Reference-contexts: It differs from the simple windowing version originally proposed for ID3 (Quinlan, 1983) in several ways: * While selecting examples, it takes care to make the class distribution as uniform as possible. This can lead to accuracy gains in domains with skewed distributions <ref> (Catlett, 1991b) </ref>. * It includes at least half of the misclassified examples into next window, which suppos edly guarantees faster convergence (fewer iterations) in noisy domains. * It can stop before all examples are correctly classified, if it appears that no further gains in accuracy are possible. * C4.5's -t parameter,
Reference: <author> Catlett, J. </author> <year> (1992). </year> <title> Peepholing: Choosing attributes efficiently for megainduction. </title> <booktitle> In Proceedings of the 9th International Conference on Machine Learning (ML-91), </booktitle> <pages> pp. 49-54. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Whether this has a positive (less over-fitting), negative (the optimal threshold may be missed), or no effect at all is an open question, which has already been raised by Breiman et al. (1984) and has been further explored in Catlett's work on peepholing <ref> (Catlett, 1992) </ref>. 12. <p> For decision tree algorithms it has been proposed to use dynamic sub-sampling at each node in order to determine the optimal test. This idea has been originally proposed, but not evaluated by Breiman et al. (1984). It was further explored in Catlett's work on peepholing <ref> (Catlett, 1992) </ref>, which is a sophisticated procedure for using sub-sampling to eliminate unpromising attributes and thresholds from consideration. 154 Integrative Windowing John and Langley (1996) discuss a different approach that successively expands the learning window.
Reference: <author> Clark, P., & Boswell, R. </author> <year> (1991). </year> <title> Rule induction with CN2: Some recent improvements. </title> <booktitle> In Proceedings of the 5th European Working Session on Learning (EWSL-91), </booktitle> <pages> pp. 151-163, </pages> <address> Porto, Portugal. </address> <publisher> Springer-Verlag. </publisher>
Reference: <author> Clark, P., & Niblett, T. </author> <year> (1989). </year> <title> The CN2 induction algorithm. </title> <journal> Machine Learning, </journal> <volume> 3 (4), </volume> <pages> 261-283. </pages>
Reference-contexts: For example, the decision tree learner C4.5 (Quinlan, 1993) selects a condition by minimizing the average entropy over all its outcomes, 6 while the original version of the rule learner CN2 <ref> (Clark & Niblett, 1989) </ref> selects the test that minimizes the entropy for all examples that pass the selected test. <p> We have experimented with a variety of criteria known from the literature, but found that they are insufficient for our purposes. For example, it turned out that, at higher training set sizes, CN2's likelihood ratio significance test <ref> (Clark & Niblett, 1989) </ref>, which accepts a rule when the distribution of positive and negative examples covered by the rule differs significantly from the class distribution in the entire set, will deem any rule learned from the window as significant. <p> A straightforward adaption to multi-class problems can be performed in several ways (Clark & Boswell, 1991; Ali & Pazzani, 1993; Dietterich & Bakiri, 1995). A different approach might adapt the algorithm for learning ordered rule sets as in CN2 <ref> (Clark & Niblett, 1989) </ref>. For example, one could use a separate windowing process for each rule, so that the rules that are successively added to the final theory can be of different classes. Orthogonally, an extension of windowing for first-order learning techniques is another challenging and rewarding task.
Reference: <author> Cohen, W. W. </author> <year> (1995). </year> <title> Fast effective rule induction. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> pp. 115-123, </pages> <address> Lake Tahoe, CA. </address> <note> Morgan Kaufmann. 160 Integrative Windowing Cohn, </note> <author> D. A., Atlas, L., & Ladner, R. </author> <year> (1994). </year> <title> Improving generalization with active learning. </title> <journal> Machine Learning, </journal> <volume> 15 (2), </volume> <pages> 201-221. </pages>
Reference-contexts: The noise-tolerant learning algorithm used was I-RIP, a separate-and-conquer learner halfway between I-REP (Furnkranz & Widmer, 1994; Furnkranz, 1997e) and RIPPER <ref> (Cohen, 1995) </ref>. Like I-REP, it learns single rules by greedily adding one condition at a time (using Foil's information gain heuristic (Quinlan, 1990)) until the rule no longer makes incorrect predictions on the growing set, a randomly chosen set of 2=3 of the training examples. <p> These choices have been shown to outperform I-REP's original choices <ref> (Cohen, 1995) </ref>. I-RIP is quite similar to I-REP*, which is also described by Cohen (1995), but it retains I-REP's method of considering all conditions for pruning (instead of considering a final sequence of conditions). 150 Integrative Windowing Around I-RIP, we wrapped the windowing procedure of Figure 9.
Reference: <author> Dagan, I., & Engelson, S. P. </author> <year> (1995). </year> <title> Committee-based sampling for training probabilistic classifiers. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> pp. 150-157, </pages> <address> Lake Tahoe, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: If their predictions differ, the algorithm asks for the true label of the example and adds it to the training set. Committee-based sampling <ref> (Dagan & Engelson, 1995) </ref> is an adaptation of this idea to probabilistic classifiers.
Reference: <author> De Raedt, L., & Bruynooghe, M. </author> <year> (1990). </year> <title> Indirect relevance and bias in inductive concept learning. </title> <journal> Knowledge Acquisition, </journal> <volume> 2, </volume> <pages> 365-390. </pages> <editor> desJardins, M., & Gordon, D. F. (Eds.). </editor> <year> (1995). </year> <title> Special issue on bias evaluation and selection. </title> <booktitle> Machine Learning, </booktitle> <pages> 20 (1-2). </pages>
Reference-contexts: For example, many Inductive Logic Programming systems provide some explicit control over the complexity of their hypothesis space, which might be controlled with an instantiation of the generalized windowing algorithm. Such an approach has in fact been realized in CLINT <ref> (De Raedt & Bruynooghe, 1990) </ref>, which has a predefined set of hypothesis spaces with increasing complexity, and is able to switch to a more expressive hypothesis language if it is not able to find a satisfactory theory in its current search space.
Reference: <author> Dietterich, T. G., & Bakiri, G. </author> <year> (1995). </year> <title> Solving multiclass learning problems via error-correcting output codes. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 2, </volume> <pages> 263-286. </pages>
Reference: <author> Domingos, P. </author> <year> (1996a). </year> <title> Efficient specific-to-general rule induction. </title> <editor> In Simoudis, E., & Han, J. (Eds.), </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 319-322. </pages> <publisher> AAAI Press. </publisher>
Reference-contexts: This technique produced promising results in noisy domains, but has substantially decreased learning accuracy in non-noisy domains. Besides, the technique seems to be tailored to a specific learning algorithm and not applicable to a wider group of rule learning algorithms <ref> (Domingos, 1996a) </ref>. Sub-sampling techniques have recently been frequently used for increasing the accuracy of a classifier. The basic idea is to train classifiers on multiple subsamples of the data and combine their predictions, usually by voting.
Reference: <author> Domingos, P. </author> <year> (1996b). </year> <title> Using partitioning to speed up specific-to-general rule induction. </title> <booktitle> In Proceedings of the AAAI-96 Workshop on Integrating Multiple Learned Models, </booktitle> <pages> pp. 29-34. </pages>
Reference: <author> Drucker, H., Schapire, R. E., & Simard, P. </author> <year> (1993). </year> <title> Boosting performance in neural networks. </title> <journal> International Journal of Pattern Recognition and Artificial Intelligence, </journal> <volume> 7 (4), </volume> <pages> 705-720. </pages>
Reference: <author> Fayyad, U. M., & Irani, K. B. </author> <year> (1992). </year> <title> On the handling of continuous-valued attributes in decision tree generation. </title> <journal> Machine Learning, </journal> <volume> 8 (1). </volume>
Reference-contexts: We handled these in the common way using threshold tests. The candidate thresholds for each test are selected between changes in the class variables in the sorted list of values of the continuous attributes <ref> (Fayyad & Irani, 1992) </ref>. 12 As the windowing algorithms will typically have to choose a threshold from a much smaller example set, they are likely to choose different thresholds.
Reference: <author> Freund, Y., & Schapire, R. E. </author> <year> (1996). </year> <title> Experiments with a new boosting algorithm. </title> <editor> In Saitta, L. (Ed.), </editor> <booktitle> Proceedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pp. 148-156, </pages> <address> Bari, Italy. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Freund, Y., Seung, H. S., Shamir, E., & Tishby, N. </author> <year> (1997). </year> <title> Selective sampling using the query by committee algorithm. </title> <journal> Machine Learning, </journal> <volume> 28, </volume> <pages> 133-168. </pages>
Reference: <author> Furnkranz, J. </author> <year> (1997a). </year> <title> Dimensionality reduction in ILP: A call to arms. </title> <editor> In De Raedt, L., & Muggleton, S. (Eds.), </editor> <booktitle> Proceedings of the IJCAI-97 Workshop on Frontiers of Inductive Logic Programming, </booktitle> <pages> pp. 81-86, </pages> <address> Nagoya, Japan. </address>
Reference: <author> Furnkranz, J. </author> <year> (1997b). </year> <title> Knowledge discovery in chess databases: A research proposal. </title> <type> Tech. rep. </type> <institution> OEFAI-TR-97-33, Austrian Research Institute for Artificial Intelligence. </institution>
Reference-contexts: Think, e.g., of Ken Thompson's 3 CD-Roms of chess endgames, to which every competitive chess program has an interface. A compression of these endgames into a simple set of rules would certainly be appreciated by the computer chess industry <ref> (Furnkranz, 1997b) </ref>. 146 Integrative Windowing Consistency-Check: When is a rule learned from the window good enough? In the noise-free case, all rules that do not cover any negative examples are added to the final theory.
Reference: <author> Furnkranz, J. </author> <year> (1997c). </year> <title> More efficient windowing. </title> <booktitle> In Proceedings of the 14th National Conference on Artificial Intelligence (AAAI-97), </booktitle> <pages> pp. 509-514, </pages> <address> Providence, RI. </address> <publisher> AAAI Press. </publisher> <address> 161 F urnkranz Furnkranz, J. </address> <year> (1997d). </year> <title> Noise-tolerant windowing. </title> <booktitle> In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI-97), </booktitle> <pages> pp. 852-857, </pages> <address> Nagoya, Japan. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: While we have primarily worked with noise-free domains, section 5 will discuss the problem of noise in windowing as well as some preliminary work that shows how windowing techniques can be adapted for noisy domains. Parts of this work have previously appeared as <ref> (Furnkranz, 1997c, 1997d, 1997a) </ref>. 2.
Reference: <author> Furnkranz, J. </author> <year> (1997e). </year> <title> Pruning algorithms for rule learning. </title> <journal> Machine Learning, </journal> <volume> 27 (2), </volume> <pages> 139-171. </pages>
Reference: <author> Furnkranz, J. </author> <year> (1998). </year> <title> Separate-and-conquer rule learning. </title> <journal> Artificial Intelligence Review. </journal> <note> In press. </note>
Reference: <author> Furnkranz, J., & Widmer, G. </author> <year> (1994). </year> <title> Incremental Reduced Error Pruning. </title> <editor> In Cohen, W., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pp. 70-77, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Gamberger, D., & Lavrac, N. </author> <year> (1997). </year> <title> Conditions for Occam's razor applicability and noise elimination. </title> <editor> In van Someren, M., & Widmer, G. (Eds.), </editor> <booktitle> Proceedings of the 9th Eu-ropean Conference on Machine Learning (ECML-97), </booktitle> <pages> pp. 108-123, </pages> <address> Prague, Czech Republic. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: They propose to use the notion of n-saturation for approximating the concept of saturation, which intuitively means that the training set contains enough examples for inducing the correct target hypothesis <ref> (Gamberger & Lavrac, 1997) </ref>. This notion corresponds quite closely to our intuitive concept of redundancy. However, the computational complexity of testing for n-saturation can be quite high.
Reference: <author> Holte, R., Acker, L., & Porter, B. </author> <year> (1989). </year> <title> Concept learning and the problem of small disjuncts. </title> <booktitle> In Proceedings of the 11th International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pp. 813-818, </pages> <address> Detroit, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is consistent with previous findings, which showed that in this domain, a few accurate rules with high coverage can be found easily from small training sets, while the majority of the rules have low coverage and are very error-prone <ref> (Holte et al., 1989) </ref>. Interestingly enough, however, Mtller's redundancy estimate, which seemed to correlate well with C4.5's performance in these domains in Table 2, is a poor predictor for the performance of our windowing algorithms.
Reference: <author> John, G. H., & Langley, P. </author> <year> (1996). </year> <title> Static versus dynamic sampling for data mining. </title> <editor> In Simoudis, E., & Han, J. (Eds.), </editor> <booktitle> Proceedings of the 2nd International Conference on Knowledge Discovery and Data Mining (KDD-96), </booktitle> <pages> pp. 367-370. </pages> <publisher> AAAI Press. </publisher>
Reference: <author> Kivinen, J., & Mannila, H. </author> <year> (1994). </year> <title> The power of sampling in knowledge discovery. </title> <booktitle> In Proceedings of the 13th ACM SIGACT-SIGMOD-SIGART Symposium on Principles of Database Systems (PODS-94), </booktitle> <pages> pp. 77-85. </pages>
Reference-contexts: Kivinen and Mannila give theoretical bounds for the sample size that is required for establishing (with a given maximum error probability) the truth of association rules <ref> (Kivinen & Mannila, 1994) </ref> or functional dependencies (Kivinen & Mannila, 1995) that have been discovered from the sample. 6.2 Active Learning Windowing techniques are also closely related to the field of active learning.
Reference: <author> Kivinen, J., & Mannila, H. </author> <year> (1995). </year> <title> Approximate dependency inference from relations. </title> <journal> Theoretical Computer Science, </journal> <volume> 149 (1), </volume> <pages> 129-149. </pages>
Reference-contexts: Kivinen and Mannila give theoretical bounds for the sample size that is required for establishing (with a given maximum error probability) the truth of association rules (Kivinen & Mannila, 1994) or functional dependencies <ref> (Kivinen & Mannila, 1995) </ref> that have been discovered from the sample. 6.2 Active Learning Windowing techniques are also closely related to the field of active learning.
Reference: <author> Kohavi, R., & John, G. H. </author> <year> (1995). </year> <title> Automatic parameter selection by minimizing estimated error. </title> <editor> In Prieditis, A., & Russell, S. (Eds.), </editor> <booktitle> Proceedings of the 12th International Conference on Machine Learning (ML-95), </booktitle> <pages> pp. 304-312, </pages> <address> Lake Tahoe, CA. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kohavi, R., & John, G. H. </author> <year> (1997). </year> <title> Wrappers for feature subset selection. </title> <journal> Artificial Intelligence, </journal> <volume> 97 (1-2), </volume> <pages> 273-324. </pages> <note> Special Issue on Relevance. </note>
Reference-contexts: Our results suggest the opposite strategy if efficiency is the main concern. This is consistent with results in comparing forward and backward feature subset selection <ref> (Kohavi & John, 1997) </ref>. We believe that thinking in this framework may lead to more general approaches for reducing the complexity of a learning problem, which aim at reducing both hypothesis and example space at the same time.
Reference: <author> Lewis, D. D., & Catlett, J. </author> <year> (1994). </year> <title> Heterogeneous uncertainty sampling for supervised learning. </title> <booktitle> In Proceedings of the 11th International Conference on Machine Learning (ML-94), </booktitle> <pages> pp. 148-156, </pages> <address> New Brunswick, NJ. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Lewis, D. D., & Gale, W. </author> <year> (1994). </year> <title> Training text classifiers by uncertainty sampling. </title> <booktitle> In Proceedings of the 17th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval (SIGIR-94), </booktitle> <pages> pp. 3-12. </pages> <note> 162 Integrative Windowing Liere, </note> <author> R., & Tadepalli, P. </author> <year> (1997). </year> <title> Active learning with committees for text categorization. </title> <booktitle> In Proceedings of the 14th National Conference on Artificial Intelligence (AAAI-97), </booktitle> <pages> pp. 591-597, </pages> <address> Providence, RI. </address> <publisher> AAAI Press. </publisher>
Reference: <author> McCallum, A., & Nigam, K. </author> <year> (1998). </year> <title> Employing EM in pool-based active learning for text classification. </title> <booktitle> In Proceedings of the 15th International Conference on Machine Learning (ICML-98), </booktitle> <address> Madison, WI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Proposals include the use of a committee of learners for co-training (Blum & Mitchell, 1998) or techniques based on the Expectation Maximization (EM) algorithm <ref> (Nigam, McCallum, Thrun, & Mitchell, 1998) </ref>, possibly coupled with an active learning procedure (McCallum & Nigam, 1998). If research in this direction is pushed further, the number of labeled examples available to a learning algorithm might be greatly enlarged at the expense of some incorrectly labeled examples. <p> Proposals include the use of a committee of learners for co-training (Blum & Mitchell, 1998) or techniques based on the Expectation Maximization (EM) algorithm (Nigam, McCallum, Thrun, & Mitchell, 1998), possibly coupled with an active learning procedure <ref> (McCallum & Nigam, 1998) </ref>. If research in this direction is pushed further, the number of labeled examples available to a learning algorithm might be greatly enlarged at the expense of some incorrectly labeled examples.
Reference: <author> Michalski, R. S. </author> <year> (1969). </year> <title> On the quasi-minimal solution of the covering problem. </title> <booktitle> In Proceedings of the 5th International Symposium on Information Processing (FCIP-69), Vol. A3 (Switching Circuits), </booktitle> <pages> pp. 125-128, </pages> <address> Bled, Yugoslavia. </address>
Reference: <author> Mtller, M. </author> <year> (1993). </year> <title> Supervised learning on large redundant training sets. </title> <journal> International Journal of Neural Systems, </journal> <volume> 4 (1), </volume> <pages> 15-25. </pages>
Reference: <author> Muggleton, S., Bain, M., Hayes-Michie, J., & Michie, D. </author> <year> (1989). </year> <title> An experimental comparison of human and machine learning formalisms. </title> <booktitle> In Proceedings of the 6th International Workshop on Machine Learning (ML-89), </booktitle> <pages> pp. 113-118. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This is even worse in the usual representation of this problem <ref> (Muggleton, Bain, Hayes-Michie, & Michie, 1989) </ref>, where instead of the between relation, a less than predicate is given in the background knowledge.
Reference: <author> Muggleton, S. H. </author> <year> (1995). </year> <title> Inverse entailment and Progol. </title> <journal> New Generation Computing, </journal> <volume> 13 (3,4), </volume> <pages> 245-286. </pages> <note> Special Issue on Inductive Logic Programming. </note>
Reference-contexts: Orthogonally, an extension of windowing for first-order learning techniques is another challenging and rewarding task. Note that the sub-sampling problem in inductive logic programming is considerably harder than for propositional learning. For example, first-order learning systems often allow an implicit definition of training examples <ref> (Muggleton, 1995) </ref> or learn from positive examples only by making some form of closed-world assumption (Quinlan, 1990), which prevents the straight-forward use of sub-sampling. Furthermore, the examples in training sets for ILP algorithms can depend on each other.
Reference: <author> Nigam, K., McCallum, A., Thrun, S., & Mitchell, T. </author> <year> (1998). </year> <title> Learning to classify from labeled and unlabeled documents. </title> <booktitle> In Proceedings of the 15th National Conference on Artificial Intelligence (AAAI-98) Madison, </booktitle> <address> WI. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Proposals include the use of a committee of learners for co-training (Blum & Mitchell, 1998) or techniques based on the Expectation Maximization (EM) algorithm <ref> (Nigam, McCallum, Thrun, & Mitchell, 1998) </ref>, possibly coupled with an active learning procedure (McCallum & Nigam, 1998). If research in this direction is pushed further, the number of labeled examples available to a learning algorithm might be greatly enlarged at the expense of some incorrectly labeled examples. <p> Proposals include the use of a committee of learners for co-training (Blum & Mitchell, 1998) or techniques based on the Expectation Maximization (EM) algorithm (Nigam, McCallum, Thrun, & Mitchell, 1998), possibly coupled with an active learning procedure <ref> (McCallum & Nigam, 1998) </ref>. If research in this direction is pushed further, the number of labeled examples available to a learning algorithm might be greatly enlarged at the expense of some incorrectly labeled examples.
Reference: <author> Pfahringer, B. </author> <year> (1995). </year> <title> Practical Uses of the Minimum Description Length Principle in Inductive Learning. </title> <type> Ph.D. thesis, </type> <institution> Technische Universitat Wien. </institution>
Reference: <author> Quinlan, J. R. </author> <year> (1979). </year> <title> Discovering rules by induction from large collections of examples. </title> <editor> In Michie, D. (Ed.), </editor> <booktitle> Expert Systems in the Micro Electronic Age, </booktitle> <pages> pp. 168-201. </pages> <publisher> Edinburgh University Press. </publisher>
Reference-contexts: A Brief History of Windowing Windowing dates back to early versions of the ID3 decision tree learning algorithm, where it was devised as an automated teaching procedure that allowed a preliminary version of ID3 to discover complete and consistent descriptions of various problems in a KRKN chess endgame <ref> (Quinlan, 1979) </ref>. Figure 1 shows the basic windowing algorithm as described in Quinlan's subsequent seminal paper on ID3 (Quinlan, 1983). It starts by picking a random 130 Integrative Windowing sample of a user-settable size InitSize from the total set of Examples.
Reference: <author> Quinlan, J. R. </author> <year> (1983). </year> <title> Learning efficient classification procedures and their application to chess end games. </title> <editor> In Michalski, R. S., Carbonell, J. G., & Mitchell, T. M. (Eds.), </editor> <booktitle> Machine Learning. An Artificial Intelligence Approach, </booktitle> <pages> pp. 463-482. </pages> <publisher> Tioga, </publisher> <address> Palo Alto, CA. </address>
Reference-contexts: We will argue that windowing is more suitable for these algorithms than for divide-and-conquer decision-tree learning <ref> (Quinlan, 1983, 1993) </ref> (section 3. Thereafter, we will introduce integrative windowing, a technique that exploits the fact that rule learning algorithms learn each rule independently. <p> Figure 1 shows the basic windowing algorithm as described in Quinlan's subsequent seminal paper on ID3 <ref> (Quinlan, 1983) </ref>. It starts by picking a random 130 Integrative Windowing sample of a user-settable size InitSize from the total set of Examples. These examples are used for inducing a theory with a given learning algorithm. <p> Despite the discouraging experimental evidence of Wirth and Catlett (1988), Quinlan (1993) implemented a new version of windowing into the C4.5 learning algorithm. It differs from the simple windowing version originally proposed for ID3 <ref> (Quinlan, 1983) </ref> in several ways: * While selecting examples, it takes care to make the class distribution as uniform as possible.
Reference: <author> Quinlan, J. R. </author> <year> (1990). </year> <title> Learning logical definitions from relations. </title> <journal> Machine Learning, </journal> <volume> 5, </volume> <pages> 239-266. </pages>
Reference-contexts: DOS 3 is basically identical to pFoil, a propositional version of Foil <ref> (Quinlan, 1990) </ref> that uses information gain as a search heuristic, but no stopping criterion. Our implementation of windowing closely follows 2. Such a linear growth was already observed by Quinlan (1979) in some early experiments with windowing. 3. Dull Old Separate-and-conquer. 132 Integrative Windowing rule examples perc. <p> The noise-tolerant learning algorithm used was I-RIP, a separate-and-conquer learner halfway between I-REP (Furnkranz & Widmer, 1994; Furnkranz, 1997e) and RIPPER (Cohen, 1995). Like I-REP, it learns single rules by greedily adding one condition at a time (using Foil's information gain heuristic <ref> (Quinlan, 1990) </ref>) until the rule no longer makes incorrect predictions on the growing set, a randomly chosen set of 2=3 of the training examples. <p> Similar approaches could also be imagined for other ILP algorithms. For example, in Foil <ref> (Quinlan, 1990) </ref>, one could systematically vary certain parameters that influence the complexity of the hypothesis space, like the number of new variables that can be introduced in the body of a clause or the maximum length of a clause, in order to define increasingly complex hypothesis spaces. <p> Note that the sub-sampling problem in inductive logic programming is considerably harder than for propositional learning. For example, first-order learning systems often allow an implicit definition of training examples (Muggleton, 1995) or learn from positive examples only by making some form of closed-world assumption <ref> (Quinlan, 1990) </ref>, which prevents the straight-forward use of sub-sampling. Furthermore, the examples in training sets for ILP algorithms can depend on each other.
Reference: <author> Quinlan, J. R. </author> <year> (1993). </year> <title> C4.5: Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference-contexts: It usually does not pay any attention to the examples that do not pass the test. The selection of a condition in a divide-and-conquer decision tree learning algorithm, on the other hand, tries to optimize for all outcomes of the test. For example, the decision tree learner C4.5 <ref> (Quinlan, 1993) </ref> selects a condition by minimizing the average entropy over all its outcomes, 6 while the original version of the rule learner CN2 (Clark & Niblett, 1989) selects the test that minimizes the entropy for all examples that pass the selected test. <p> Actually, C4.5 maximizes information gain, which is computed by subtracting the average entropy from an information value that is constant for all considered tests. 134 Integrative Windowing More evidence for the sensitivity of ID3 to changes in the example distribution also comes from some experiments with C4.5 <ref> (Quinlan, 1993) </ref>, where it turned out that changing windowing in a way that makes the class distribution in the initial window as uniform as possible produces better results. 7 As discussed above, we believe that this sensitivity is caused by the need of decision tree algorithms to optimize the class distribution <p> This finding contradicts the heuristic that is currently employed in C4.5, namely to add at least half of the total misclassified examples. However, this heuristic was formed in order to make windowing more effective in noisy domains <ref> (Quinlan, 1993) </ref>, a goal that in our opinion cannot be achieved with merely using a noise-tolerant learner inside the windowing loop, for reasons discussed in the next section. 145 F urnkranz 5. Windowing and Noise So far we have only dealt with noise-free domains. <p> For the former problem, techniques for combining the results of multiple runs of windowing with different random seeds, as implemented in C4.5 <ref> (Quinlan, 1993) </ref>, are quite similar to bagging and boosting techniques and could produce similar results.
Reference: <author> Sebag, M., & Rouveirol, C. </author> <year> (1997). </year> <title> Tractable induction and classification in first order logic via stochastic matching. </title> <booktitle> In Proceedings of the 15th International Joint Conference on Artificial Intelligence (IJCAI'97), </booktitle> <pages> pp. 888-893, </pages> <address> Nagoya, Japan. </address>
Reference: <author> Seung, H. S., Opper, M., & Sompolinsky, H. </author> <year> (1992). </year> <title> Query by committee. </title> <booktitle> In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory (COLT-92), </booktitle> <pages> pp. 287-294. </pages> <editor> F urnkranz Toivonen, H. </editor> <year> (1996). </year> <title> Sampling large databases for association rules. </title> <booktitle> In Proceedings of the 22nd Conference on Very Large Data Bases (VLDB-96), </booktitle> <pages> pp. 134-145, </pages> <address> Mumbai, India. </address>
Reference: <author> Turney, P. </author> <year> (1996). </year> <title> How to shift bias: Lessons from the Baldwin effect. </title> <journal> Evolutionary Computation, </journal> <volume> 4 (3), </volume> <pages> 271-295. </pages>
Reference: <author> Utgoff, P. E. </author> <year> (1986). </year> <title> Shift of bias for inductive concept learning. </title> <editor> In Michalski, R., Carbonell, J., & Mitchell, T. (Eds.), </editor> <booktitle> Machine Learning: An Artificial Intelligence Approach, </booktitle> <volume> Vol. II, </volume> <pages> pp. 107-148. </pages> <publisher> Morgan Kaufmann, </publisher> <address> Los Altos, CA. </address>
Reference: <author> Wirth, J., & Catlett, J. </author> <year> (1988). </year> <title> Experiments on the costs and benefits of windowing in ID3. </title> <editor> In Laird, J. (Ed.), </editor> <booktitle> Proceedings of the 5th International Conference on Machine Learning (ML-88), </booktitle> <pages> pp. 87-99, </pages> <address> Ann Arbor, MI. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Nevertheless, windowing has not played a major role in machine learning research. One reason for this certainly is the rapid development of computer hardware, which made the motivation for windowing seem less compelling. However, a good deal of this lack of interest can be attributed to an empirical study <ref> (Wirth & Catlett, 1988) </ref>, in which the authors studied windowing with ID3 in various domains and concluded that it cannot be recommended as a procedure for improving efficiency.
Reference: <author> Yang, Y. </author> <year> (1996). </year> <title> Sampling strategies and learning efficiency in text categorization. </title> <editor> In Hearst, M., & Hirsh, H. (Eds.), </editor> <booktitle> Proceedings of the AAAI Spring Symposium on Machine Learning in Information Access, </booktitle> <pages> pp. 88-95. </pages> <note> AAAI Press. Technical Report SS-96-05. </note>
References-found: 55

