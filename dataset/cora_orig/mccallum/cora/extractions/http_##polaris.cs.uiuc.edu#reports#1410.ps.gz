URL: http://polaris.cs.uiuc.edu/reports/1410.ps.gz
Refering-URL: http://polaris.cs.uiuc.edu/polaris/rep2.html
Root-URL: http://www.cs.uiuc.edu
Title: Privatization and Distribution of Arrays A Preliminary Proposal  
Author: Peng Tu 
Abstract: In today's high performance NUMA (Non-Uniform Memory Architecture) multiprocessors with memory hierarchy or distributed memory, the partition and distribution of data associated with parallel computations affect the amount of parallelism that can be exploited and the amount of data movement in the system. The objective of this research is to study and evaluate compile time data management techniques to enhance parallelism and to improve locality of memory reference for large scientific programs written in Fortran. Our first step is to reduce the amount of shared data through privatization. Priva-tization is a technique that allocates a separate copy of a shared variable in the private storage of each processor such that each processor can access a distinct instance of the variable. Privatization can enhance inherent parallelism of a program by eliminating memory-related anti- and output dependences. It can also improve the locality of references since accessing a private variable is inherently local and communication free. We present our algorithm for array privatization and the result of our experiment on the effectiveness of the algorithm. For the remaining shared data, we introduce a new concept: placement matrix, and show its application in deriving data alignment and data decomposition to reduce communication. We also incorporate the ratio of communication to computation in our evaluation of different data partitions. The work is continuing on heuristics for data distribution and the implementation of the tools.
Abstract-found: 1
Intro-found: 1
Reference: [Bal91] <author> V. Balasundaram. </author> <title> Translating control parallelism to data parallelism. </title> <booktitle> In Proc. 5th SIAM Conf. on Parallel Processing for Scientific Computing, </booktitle> <year> 1991. </year>
Reference-contexts: Otherwise, if data are not distributed in such a way that A (I; J) is located with C (I; J), communication from the owner of A (I; J) to the owner of C (I; J) would be necessary. Array privatization together with a owner stores rule <ref> [Bal91] </ref> may result in a more efficient program. By allocating a private variable to each processor, the value of A (J) is computed locally, load is evenly distributed. Communication happens only for storing A (J) to its owner.
Reference: [BCFH89] <author> M. Burke, R. Cytron, J. Ferrante, and W. Hsieh. </author> <title> Automatic generation of nested, fork-join parallelism. </title> <journal> Journal of Supercomputing, </journal> <pages> pages 71-88, </pages> <year> 1989. </year>
Reference-contexts: Privatization is a technique that allows each concurrent process to allocate a variable in private storage such that each process accesses a distinct instance of the variable. Privatization of scalars has been used for many years in parallelizing compilers and is well understood <ref> [BCFH89] </ref>. In this section, we will focus on techniques for the privatization of arrays. By providing distinct instance of a variable to each processor, privatization can eliminate memory related dependences and enhance parallelism. It reduces communication since access to a private variable is local to the processor.
Reference: [Che89] <author> Ding-Kai Chen. </author> <title> MAXPAR: An execution driven simulator for studying parallel systems. </title> <type> MS thesis, </type> <institution> Univ. of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <month> October </month> <year> 1989. </year> <type> CSRD Report 917. </type>
Reference: [CK88] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference: [CKPK90] <author> George Cybenko, Lyle Kipp, Lynn Pointer, and David Kuck. </author> <title> Supercomputer Performance Evaluation and the Perfect Benchmarks. </title> <booktitle> In Proceedings of ICS, </booktitle> <address> Amsterdam, Netherlands, </address> <pages> pages 162-174, </pages> <month> March </month> <year> 1990. </year>
Reference-contexts: A read shadow for a private variable is created locally for a loop iteration, different iterations have different shadows for a private variable, hence the anti-dependences are broken across iterations. Optimal execution time is computed by ignoring all the read shadow variables. Six programs in the Perfect Benchmark Club <ref> [CKPK90] </ref> were instrumented. The loop level speedup results are reported in the table below. The results without privatization show that memory related dependences can severely limit parallelism in programs. After array privatization, we get speedups within for five cases within a factor of two of the optimal speedups.
Reference: [EHLP91] <author> R. Eigenmann, J. Hoeflinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <booktitle> In Proc. 4-th Workshop on Programming Languages and Compilers for Parallel Computing. </booktitle> <publisher> Pitman/MIT Press, </publisher> <month> August </month> <year> 1991. </year>
Reference-contexts: It reduces communication since access to a private variable is local to the processor. Previous studies on the effectiveness of automatic program parallelization show that array privatization is one of the most effective transformations for the exploitation of parallelism <ref> [EHLP91] </ref>. Also, privatization can improve load balancing under some translation systems targeted at distributed memory machines.
Reference: [Fea88] <author> P. Feautrier. </author> <title> Array expansion. </title> <booktitle> In Proc. 1988 ACM Int'l Conf. on Supercomputing, </booktitle> <month> July </month> <year> 1988. </year>
Reference-contexts: If C (fl; J) is not distributed with A (J), communication will be necessary from the owner of A (J) to the owners of C (fl; J). 4 These problems can be relieved by array expansion [PW86] <ref> [Fea88] </ref>. For the loop above, this means expanding A into a two dimensional array as shown next: S0: DO I = 1, N S2: A (I,J) = B (I,J) + 1 S4: END DO In this case, array expansion resolves the anti-dependence, exposes more parallelism and improves load balance.
Reference: [FO84] <author> G. C. Fox and S. W. Otto. </author> <title> Algorithms for concurrent processors. </title> <journal> Phys. Today, </journal> <volume> 37 </volume> <pages> 50-59, </pages> <month> May </month> <year> 1984. </year>
Reference-contexts: They can be partitioned and distributed communication free in those loops. Previous studies for efficient solution of partial differential equations on multiple processor systems noted that the efficiency of parallel algorithm is not determined by the amount of communication but the ratio of communication to computation <ref> [FO84] </ref>. As pointed out by Reed etal:[RAP87]: the stencil type, partition shape and machine organization all affect the performance of resulting parallel computation. In our studies, we try to apply the ratio of communication to computation to a broad class of problems.
Reference: [GB92] <author> M. Gupta and P. Banerjee. </author> <title> Demonstration of automatic data partitioning techniques for parallelizing compilers on multicomputers. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 3(2) </volume> <pages> 179-193, </pages> <month> March </month> <year> 1992. </year> <month> 20 </month>
Reference: [HKT91] <author> S. Hiranandani, K. Kennedy, and Ch.-W. Tseng. </author> <title> Compiler support for machine--independent parallel programming in Fortran D. </title> <institution> Technical Report Rice COMP TR91-149, Department of Computer Science, Rice University, </institution> <month> January </month> <year> 1991. </year>
Reference: [Kum88] <author> M. Kumar. </author> <title> Measuring parallelism in computation-intensive science/engineering applications. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(9) </volume> <pages> 5-40, </pages> <year> 1988. </year>
Reference-contexts: Speedups are computed for each program with array privatization and without array priva-tization assuming loop level parallelism. An upper bound of the optimal loop level speedup for a program is also computed by ignoring all the anti-dependences. In all the calculations, we use a strategy introduced by Kumar <ref> [Kum88] </ref> to measure a program's execution time on an ideal machine where only the arithmetic operations consume time and an unlimited number of processors are available. A program is instrumented such that run-time reference information about memory locations is used to detect the flow dependences and anti-dependences [Che89][PP92].
Reference: [LC91] <author> J. Li and M. Chen. </author> <title> The data alignment phase in compiling programs for distributed-memory machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 13 </volume> <pages> 213-221, </pages> <year> 1991. </year>
Reference-contexts: Most regular alignments are determined by three parameters: orientation, offset, and stride. Alignment constraints can be derived from subscript expression. Finding optimum alignment is difficult. The orientation problem has been formulated as the problem of component affinity graph and shown to be NP-complete <ref> [LC91] </ref>. As different computations require different alignments, it may not be feasible to keep a fixed alignment in the entire program. Dynamic alignment may provide a more efficient program. The communication pattern can be classified in various ways. We identify two classes, uniform communication and non-uniform communication.
Reference: [Pad89] <author> David A. Padua. </author> <title> The Delta Program Manipulation system | Preliminary design. </title> <type> CSRD Report 808, </type> <institution> University of Illinois at Urbana-Champaign, Center for Supercomp. R&D, </institution> <month> June </month> <year> 1989. </year>
Reference-contexts: We have implemented the generalized algorithm and it is used in the next section to evaluate effectiveness. 2.2 Effectiveness Evaluation To evaluate the effectiveness of the algorithm, we implemented the algorithm using the Delta Program Manipulation System <ref> [Pad89] </ref>. The evaluations are made by comparing a program's optimal loop level parallel execution times with and without array privatization. Speedups are computed for each program with array privatization and without array priva-tization assuming loop level parallelism.
Reference: [PP92] <author> Paul Petersen and David Padua. </author> <title> Machine-Independent Evaluation of Paralleliz-ing Compilers. In Advanced Compilation Techniques for Novel Architectures, </title> <month> January </month> <year> 1992. </year>
Reference: [PW86] <author> D. Padua and M. Wolfe. </author> <title> Advanced compiler optimizations for supercomputers. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: If C (fl; J) is not distributed with A (J), communication will be necessary from the owner of A (J) to the owners of C (fl; J). 4 These problems can be relieved by array expansion <ref> [PW86] </ref> [Fea88]. For the loop above, this means expanding A into a two dimensional array as shown next: S0: DO I = 1, N S2: A (I,J) = B (I,J) + 1 S4: END DO In this case, array expansion resolves the anti-dependence, exposes more parallelism and improves load balance.
Reference: [RAP87] <author> Daniel A. Reed, Loyce M. Adams, and Merrell L. Patrick. </author> <title> Stencils and problem partitionings: Their influence on the performance of multiple processor systems. </title> <journal> IEEE Trans. on Computers, </journal> <volume> 36(7) </volume> <pages> 845-858, </pages> <month> July </month> <year> 1987. </year>
Reference: [RP89] <author> A. Rogers and K. Pingali. </author> <title> Process decomposition through locality of reference. </title> <booktitle> In Proc. the SIGPLAN '89 Conference on Program Language Design and Implementation, </booktitle> <month> June </month> <year> 1989. </year>
Reference: [RS91] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compiler-time techniques for data distribution in distributed memory machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4) </volume> <pages> 472-482, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We end this section with an example shows that the placement matrix's application in deriving communication free data decompositions. This work is parallel to the work of Ramanujam and Sadayappan <ref> [RS91] </ref>. We solve the problem here with the same framework discussed above. DO I = 1, 1000 X (I,J) = Y (I,J-1) + Y (I,J) END DO In this loop, we have two references to Y which are conflict.
Reference: [ZBG88] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> Superb: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 21 </month>
Reference-contexts: Since computation and data are closely related, many compilers for distributed memory machines use owner computes rule to determine which processor shall execute a particular piece of a shared section <ref> [ZBG88] </ref> [CK88][RP89]. According to the data distribution specified by user or compiler, each data element is assigned to a owner processor which is the one that stores that data element in its local memory. The owner processor of a data item executes all the instructions that modify its value.
References-found: 19

