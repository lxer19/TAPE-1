URL: http://www.cs.cmu.edu/afs/cs/project/link/www/papers/ps/tr95-125.ps
Refering-URL: 
Root-URL: 
Title: A ROBUST PARSING ALGORITHM FOR LINK GRAMMARS  
Author: Dennis Grinberg John Lafferty Daniel Sleator 
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Date: August 1995  
Pubnum: CMU-CS-95-125  
Abstract: Correspondence regarding this paper should be sent to Dennis Grinberg at the above address, or by e-mail to dennis@cs.cmu.edu. Research supported in part by NSF and ARPA under grant IRI-9314969 and the ATR Telecommunications Research Laboratories. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the NSF or the U.S. government. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Godfrey, E. Holliman, and J. McDaniel. </author> <title> Switchboard: Telephone speech corpus for research development. </title> <booktitle> In Proc. ICASSP-92, </booktitle> <pages> pages I-517-520, </pages> <year> 1992. </year>
Reference-contexts: Memoization together with pruning techniques enable the algorithm to run efficiently, with theoretic time complexity of O (n 3 ) for an input of n words. We have implemented these ideas and tested them by parsing the Switchboard corpus of conversational English <ref> [1] </ref>. This corpus is comprised of approximately three million words of text, corresponding to more than 150 hours of transcribed speech collected from telephone conversations restricted to 70 different topics.
Reference: [2] <author> A. Lavie and M. Tomita. </author> <title> GLR fl : An efficient noise-skipping parsing algorithm for context free grammars. </title> <booktitle> In Proceedings of the Third International Workshop on Parsing Technologies, </booktitle> <pages> pages 123-134, </pages> <year> 1993. </year>
Reference-contexts: In 1974, Lyon [4] proposed a dynamic programming algorithm for finding the least number of mutations, insertions, and deletions of terminal symbols necessary to parse a sentence. Recently, Lee et al. [3] extended this work by allowing errors in non-terminals. In <ref> [2] </ref> Lavie and Tomita describe a modification of the Generalized LR Parser. Their GLR fl algorithm is designed to determine the maximal subsets of the input that are parsable by skipping words. <p> The first is given in terms of the edit distance of a generalized parse and it is analogous to least-errors recognition. The second definition introduces the concept of a null link. Using this definition our approach bears similarity to the work in <ref> [2] </ref>. While the edit distance is perhaps a more general and intuitive concept, it does not lead us to efficient parsing algorithms. In Section 4 we give the details of the robust parsing algorithm using a cost function defined in terms of null links.
Reference: [3] <author> K. J. Lee, C. J. Kweon, J. Seo, and G. C. Kim. </author> <title> A robust parser based on syntactic information. </title> <booktitle> In Proceedings of the 7th Conference of the European Chapter of the Association for Computational Linguistics, </booktitle> <year> 1995. </year>
Reference-contexts: There has been work by a number of researchers on least-errors recognition for context-free grammars. In 1974, Lyon [4] proposed a dynamic programming algorithm for finding the least number of mutations, insertions, and deletions of terminal symbols necessary to parse a sentence. Recently, Lee et al. <ref> [3] </ref> extended this work by allowing errors in non-terminals. In [2] Lavie and Tomita describe a modification of the Generalized LR Parser. Their GLR fl algorithm is designed to determine the maximal subsets of the input that are parsable by skipping words.
Reference: [4] <author> G. Lyon. </author> <title> Syntax-directed least-errors analysis for context-free languages: A practical approach. </title> <journal> Communications of the ACM, </journal> <volume> 17(1) </volume> <pages> 3-14, </pages> <month> Jan </month> <year> 1974. </year>
Reference-contexts: There has been work by a number of researchers on least-errors recognition for context-free grammars. In 1974, Lyon <ref> [4] </ref> proposed a dynamic programming algorithm for finding the least number of mutations, insertions, and deletions of terminal symbols necessary to parse a sentence. Recently, Lee et al. [3] extended this work by allowing errors in non-terminals. In [2] Lavie and Tomita describe a modification of the Generalized LR Parser.
Reference: [5] <author> D. D. K. Sleator and D. Temperley. </author> <title> Parsing English with a link grammar. </title> <type> Technical Report CMU-CS-91-196, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> 5000 Forbes Avenue, Pittsburgh, PA 15213, </address> <year> 1991. </year> <month> 16 </month>
Reference-contexts: 1 Introduction In this paper we present a robust parsing algorithm for the link grammar formalism introduced in <ref> [5] </ref>. Using a simple extension of the original formalism we develop efficient parsing and pruning algorithms for extracting structure from unrestricted natural language. Our approach to robust parsing is purely algorithmic; no modification to the underlying grammar is necessary. <p> In Section 4 we give the details of the robust parsing algorithm using a cost function defined in terms of null links. In Section 5 we explain how the pruning techniques of <ref> [5] </ref> can be extended to accommodate null links. Finally, in Section 6 we present the results of our experiments with the robust parser on the Switchboard corpus. 2 Link Grammar Concepts and Notation In this section we briefly summarize the relevant concepts and notation of link grammar. <p> After pruning, the parsing algorithm itself is carried out. In Sections 4 and 5 we give the details on the parsing and pruning algorithms that are needed to explain the robust algorithm. In <ref> [5] </ref> the relationship between link grammar and other grammatical formalisms is discussed. 3 The Cost of a Linkage The approach that we take to robust parsing uses the notion of cost. <p> The case where l = nil is similar. To explain the robust algorithm it will be helpful to repeat a fragment of the pseudocode representing the original algorithm as given in <ref> [5] </ref>. The function Count takes as input indices of two words L and R, where the words are numbered from 0 to N 1, and pair of connector lists l and r. <p> As with pruning, power pruning is carried out by making alternating left-to-right and right-to-left passes through the sentence; the details are given in <ref> [5] </ref>. The power pruning algorithm eliminates some disjuncts by observing that in order for a linkage to be connected, it is necessary that any two connectors between non-neighboring words cannot both be the last connectors of their respective lists. This condition is relaxed when parsing with null links. <p> Thus, it is a simple matter to modify the power pruning algorithm to allow for the possibility of null links. In the case where the sentence contains a conjunction, a variation of the power pruning algorithm enables pruning of disjuncts before fat connectors are built <ref> [5] </ref>. This variation uses the notion of a deletable region. A substring of words is said to be a deletable region if its removal would result in a grammatical sentence. For example, in the sentence the dog and cat ran, both dog and and and cat are deletable regions.
References-found: 5

