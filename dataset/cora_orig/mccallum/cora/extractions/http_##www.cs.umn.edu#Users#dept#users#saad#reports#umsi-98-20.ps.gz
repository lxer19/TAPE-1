URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/umsi-98-20.ps.gz
Refering-URL: http://www.cs.umn.edu/Users/dept/users/saad/reports/
Root-URL: http://www.cs.umn.edu
Title: RESTARTING TECHNIQUES FOR PRECONDITIONED SYMMETRIC EIGENVALUE METHODS  
Author: ANDREAS STATHOPOULOS AND YOUSEF SAAD 
Keyword: Key words. Davidson, Jacobi-Davidson, Lanczos, Conjugate Gradient methods, implicit restarting, deflation, eigenvalue, preconditioning  
Note: AMS subject classifications. 65F15  
Abstract: The (Jacobi-)Davidson method, which is a popular preconditioned variant of the Arnoldi method for solving large eigenvalue problems, is often used with restarting. This has significant performance shortcomings, since important components of the invariant subspace may be discarded. One way of saving more information at restart is through "thick" restarting, a technique which involves keeping more Ritz vectors than needed. This technique and especially its dynamic version have proved very efficient for symmetric cases. A different restarting strategy for the preconditioned Davidson method has been proposed in [12], motivated by the similarity between the spaces built by the Davidson and Conjugate Gradient methods. For the latter one, a three term recurrence implicitly maintains all required information. In this paper, we consider the effects of preconditioning on the dynamic thick restarting strategy, and we analyze both theoretically and experimentally the strategy based on Conjugate Gradient. Our analysis shows that, in some sense, the two schemes are complementary, and that their combination provides an even more powerful technique. We also describe a way to implement this scheme without additional orthogonalizations or matrix multiplications. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Calvetti, L. Reichel, and D. Sorensen, </author> <title> An implicitly restarted Lanczos method for large symmetric eigenvalue problems, </title> <journal> Electronic Trans. Numer. Anal., </journal> <volume> 2 (1994), </volume> <pages> pp. 1-21. </pages>
Reference-contexts: Even in the non-preconditioned Lanczos method where a three-term recurrence is known, orthogonality problems and spurious solutions prevent the application of the method for a large number of steps. For these reasons, many restarting variants of the Lanczos and Davidson methods are used in practice <ref> [4, 15, 18, 1, 7] </ref>. 2 Department of Computer Science, College of William and Mary, Box 8795, Williamsburg, Vir-ginia 23187-8795, (andreas@cs.wm.edu). 3 Department of Computer Science, University of Minnesota, 4-192 EE/CSci Bldg., Minneapolis, Minnesota, 55455-0154 (saad@cs.umn.edu.) fl Work supported by NSF grants DMR-9217287 and ASC 95-04038, and the Minnesota Super computer <p> This has significant performance shortcomings, since important components of the invariant subspace may be discarded, and the Rayleigh-Ritz procedure does not minimize over the whole Krylov subspace. An efficient solution to this problem has been given by the Implicitly Restarted Lanc-zos method (IRL) <ref> [18, 1, 8] </ref>. IRL provides an implicit way of applying a polynomial filter during restarting and thus removing unwanted spectral information. Implicit restarting provides also an elegant formulation of most previously proposed restarting schemes.
Reference: [2] <author> E. Chow and Y. Saad, </author> <title> Approximate inverse preconditioners via sparse-sparse iteration, </title> <note> SIAM J. Sci. Comput., (To appear). </note>
Reference-contexts: Dynamic thick restarting and preconditioning. The heuristic for dynamic thick restarting relies on the gap ratios, which govern the convergence of the non-preconditioned Lanczos process. Nevertheless, the dynamic scheme performs equally well when used with preconditioning. In Table 1, diagonal and approximate inverse <ref> [2] </ref> preconditioning have been used to demonstrate the effectiveness of the method. In Table 1, we observe that although dynamic is better than one-sided thick restarting, the improvements diminish with better preconditioners (or easier problems).
Reference: [3] <author> M. Crouzeix, B. Philippe, and M. Sadkane, </author> <title> The Davidson method, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 15 (1994), </volume> <pages> pp. 62-76. </pages>
Reference-contexts: However, as size and ill conditioning of the matrix increases, preconditioning becomes necessary to compensate for the loss of efficiency and robustness of iterative methods. The Davidson and its generalization the Jacobi-Davidson method <ref> [5, 11, 3, 17] </ref> are popular preconditioned variants of the Arnoldi method. Instead of extracting the solutions from a generated Krylov space, these methods gradually build a different space by orthogonalizing the preconditioned residual at every step against the existing basis vectors.
Reference: [4] <author> J. Cullum and R. A. Willoughby, </author> <title> Lanczos algorithms for large symmetric eigenvalue computations, </title> <booktitle> vol. 2: Programs of Progress in Scientific Computing; v. </booktitle> <volume> 4, </volume> <publisher> Birkhauser, </publisher> <address> Boston, </address> <year> 1985. </year>
Reference-contexts: Even in the non-preconditioned Lanczos method where a three-term recurrence is known, orthogonality problems and spurious solutions prevent the application of the method for a large number of steps. For these reasons, many restarting variants of the Lanczos and Davidson methods are used in practice <ref> [4, 15, 18, 1, 7] </ref>. 2 Department of Computer Science, College of William and Mary, Box 8795, Williamsburg, Vir-ginia 23187-8795, (andreas@cs.wm.edu). 3 Department of Computer Science, University of Minnesota, 4-192 EE/CSci Bldg., Minneapolis, Minnesota, 55455-0154 (saad@cs.umn.edu.) fl Work supported by NSF grants DMR-9217287 and ASC 95-04038, and the Minnesota Super computer
Reference: [5] <author> E. R. Davidson, </author> <title> The iterative calculation of a few of the lowest eigenvalues and corresponding eigenvectors of large real-symmetric matrices, </title> <journal> J. Comput. Phys., </journal> <note> 17 (1975), p. 87. </note>
Reference-contexts: However, as size and ill conditioning of the matrix increases, preconditioning becomes necessary to compensate for the loss of efficiency and robustness of iterative methods. The Davidson and its generalization the Jacobi-Davidson method <ref> [5, 11, 3, 17] </ref> are popular preconditioned variants of the Arnoldi method. Instead of extracting the solutions from a generated Krylov space, these methods gradually build a different space by orthogonalizing the preconditioned residual at every step against the existing basis vectors.
Reference: [6] <author> I. S. Duff, R. G. Grimes, and J. G. Lewis, </author> <title> Sparse matrix test problems, </title> <journal> ACM Trans. Math. Soft., </journal> <year> (1989), </year> <pages> pp. 1-14. </pages>
Reference-contexts: A few representative results using this strategy on matrices from the Harwell-Boeing collection <ref> [6] </ref> appear in Table 1. For all matrices, we solve for five lowest eigenpairs using a maximum basis size of 20, and we use the Davidson code that was described in [19] together with the extensions proposed in [20].
Reference: [7] <author> D. R. Fokkema, G. L. G. Sleijpen, and H. A. Van der Vorst, </author> <title> Jacobi-Davidson style QR and QZ algorithms for the partial reduction of matrix pencils, </title> <type> Tech. Rep. 941, </type> <institution> Department of Mathematics, University of Utrecht, </institution> <year> 1996. </year> <note> To appear in SISC. </note>
Reference-contexts: Even in the non-preconditioned Lanczos method where a three-term recurrence is known, orthogonality problems and spurious solutions prevent the application of the method for a large number of steps. For these reasons, many restarting variants of the Lanczos and Davidson methods are used in practice <ref> [4, 15, 18, 1, 7] </ref>. 2 Department of Computer Science, College of William and Mary, Box 8795, Williamsburg, Vir-ginia 23187-8795, (andreas@cs.wm.edu). 3 Department of Computer Science, University of Minnesota, 4-192 EE/CSci Bldg., Minneapolis, Minnesota, 55455-0154 (saad@cs.umn.edu.) fl Work supported by NSF grants DMR-9217287 and ASC 95-04038, and the Minnesota Super computer <p> Therefore, this space cannot be replicated by an appropriate CG or PCG method. Second, the Jacobi-Davidson equation involves projections in both sides of the matrix and applying the preconditioned CG is not obvious <ref> [7] </ref>. If we assume that the Ritz value in the residual is kept constant, equal to , and that the preconditioner is also constant (M I) 1 , the operator becomes M C = (M I) 1 (A I) and a similar argument as in theorem 4.2 can be made.
Reference: [8] <author> R. B. Lehoucq, </author> <title> Analysis and implementation of an implicitly restarted Arnoldi iteration, </title> <type> PhD thesis, </type> <institution> Department of Computational and Applied Mathematics, Rice University, </institution> <year> 1995. </year> <month> TR95-13. </month>
Reference-contexts: This has significant performance shortcomings, since important components of the invariant subspace may be discarded, and the Rayleigh-Ritz procedure does not minimize over the whole Krylov subspace. An efficient solution to this problem has been given by the Implicitly Restarted Lanc-zos method (IRL) <ref> [18, 1, 8] </ref>. IRL provides an implicit way of applying a polynomial filter during restarting and thus removing unwanted spectral information. Implicit restarting provides also an elegant formulation of most previously proposed restarting schemes.
Reference: [9] <author> K. Meerbergen, </author> <title> Robust methods for the calculation of rightmost eigenvalues of nonsymmetric problems, </title> <type> PhD thesis, </type> <institution> Department of Computer Science, K. U. Leuven, Heverlee, Belgium, </institution> <year> 1996. </year>
Reference-contexts: The algorithm described by these assumptions is the restarted preconditioned Arnoldi method by Meerbergen <ref> [9] </ref>, which uses the Arnoldi method to compute a Krylov space of M C . Then, it uses this space to perform a Rayleigh-Ritz projection with A, and obtain a new .
Reference: [10] <author> R. B. Morgan, </author> <title> On restarting the Arnoldi method for large nonsymmetric eigenvalue problems, </title> <institution> Math. Comput., </institution> <year> (1996). </year>
Reference-contexts: Recently, it was shown that without preconditioning, and if the Ritz values are used as shifts in the polynomial filter, the Davidson method builds the same search space as IRL <ref> [21, 10] </ref>. To reduce the adverse effects of restarting, we can use the above framework to save more information at every restart. Recently, we have investigated the idea of "thick restarting" [21], which implements this principle by keeping more Ritz vectors than needed.
Reference: [11] <author> R. B. Morgan and D. S. Scott, </author> <title> Generalizations of Davidson's method for computing eigen-values of sparse symmetric matrices, </title> <journal> SIAM J. Sci. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 817-825. </pages>
Reference-contexts: However, as size and ill conditioning of the matrix increases, preconditioning becomes necessary to compensate for the loss of efficiency and robustness of iterative methods. The Davidson and its generalization the Jacobi-Davidson method <ref> [5, 11, 3, 17] </ref> are popular preconditioned variants of the Arnoldi method. Instead of extracting the solutions from a generated Krylov space, these methods gradually build a different space by orthogonalizing the preconditioned residual at every step against the existing basis vectors. <p> Enddo 12. Set V p = fx i ; the p lowest Ritz vectors g; l p &lt; m; and restart 13. Enddo The preconditioning is performed by solving the equation at step 9, with M (s;j) approximating (A 0 I) in some sense. Originally, Morgan and Scott <ref> [11] </ref> proposed to solve approximately with some preconditioner the Generalized Davidson correction equation: (A I) ffi = r 0 :(1) In [17], Sleijpen et al. show that for stability, robustness, as well as efficiency, the operator M (s;j) should have a range orthogonal to x 0 .
Reference: [12] <author> C. W. Murray, S. C. Racine, and E. R. Davidson, </author> <title> Improved algorithms for the lowest eigenvalues and associated eigenvectors of large matrices, </title> <journal> J. Comput. Phys., </journal> <volume> 103 (1992), </volume> <pages> pp. 382-389. </pages> <note> RESTARTING STRATEGIES 19 </note>
Reference-contexts: For symmetric, non-preconditioned cases, a dynamic thick restarting scheme that keeps Ritz vectors on both sides of the spectrum has proved extremely efficient. With preconditioning, although it is still efficient, a less expensive scheme might provide similar benefits. For the Davidson method, Murray et al. <ref> [12] </ref>, and Van Lenthe et al. [22] proposed restarting with the Ritz vector from the previous iteration along with the current one. The motivation stems from the observation that the Davidson and the Conjugate Gradient (CG) methods are equivalent when solving for the eigenvector of a converged eigenvalue. <p> This is often justified in computational quantum chemistry problems, because of the availability of good eigenvalue estimates and the relatively fast convergence of the methods. Murray et al. <ref> [12] </ref>, suggested a more interesting variant of this idea. <p> SAAD because for symmetric matrices the Rayleigh Ritz and Galerkin processes are equivalent. In general, the two spaces are not the same but they are close if the Ritz value does not vary significantly between steps. The similarities also extend between the preconditioned Davidson and preconditioned CG (PCG) <ref> [12] </ref> methods. A theoretical justification of these arguments is presented in the following section. In Figure 1, we show a typical convergence history of the the residual norm for three different restarting strategies. <p> In that case, the operator can be considered constant and the previous theory applies. 5. Overcoming weaknesses. The results of the previous section provide an explanation for the experiments in section 3 and the original observations in <ref> [12] </ref>, i.e., CG-based restarting is justified when looking for an extreme eigenpair. However, the error bound in lemma 4.5 may be large for interior eigenvalues, even when the distance of the shift from the Ritz value is small. <p> Residual convergence curves for four eigenpairs of BCSSTK05, for the three different restarting schemes with diagonal preconditioning. matrix vector multiplication to recompute w (s1) after orthogonalization, but this could prove expensive. There is a simple, elegant, and inexpensive way to face the above problems, which is not mentioned in <ref> [12] </ref>. Instead of keeping long dimension vectors and orthogonaliz-ing in full dimension, we could perform all operations and maintain the extra storage in the coefficient space.
Reference: [13] <author> B. N. Parlett, </author> <title> The Symmetric Eigenvalue Problem, </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1980. </year>
Reference-contexts: Lemma 4.6. Let x 0 a unit-norm vector with Rayleigh quotient 0 . Let also the gap between the first and the second eigenvalue of A be fl = 2 1 . Then: sin 2 (x 0 ; u 1 ) &lt; fl Proof. See <ref> [13] </ref> Lemma (11-9-8). Lemma 4.7. Let the assumptions and notations of Lemma 4.5 hold, and let fl = 2 1 .
Reference: [14] <author> Y. Saad, </author> <title> Computation of eigenvalues of large Hermitian matrices by partitioning techniques, </title> <type> tech. rep., </type> <institution> INPG- University of Grenoble, Grenoble, France, </institution> <year> 1974. </year> <note> Dissertation (French). </note>
Reference-contexts: Only the last of the vectors involved in the three-term recurrence is a Ritz vector. A similar observation was made by Saad in his thesis at Grenoble <ref> [14] </ref>. Therein, he identifies a three term recurrence leading to a specific Ritz vector, if the corresponding Ritz value is known.
Reference: [15] <author> Y. Saad, </author> <title> Chebyshev acceleration techniques for solving nonsymmetric eigenvalue problems, </title> <journal> Math. Comp., </journal> <volume> 42 (1984), </volume> <pages> pp. </pages> <month> 567-588. </month> <title> [16] , Numerical methods for large eigenvalue problems, </title> <publisher> Manchester University Press, </publisher> <year> 1993. </year>
Reference-contexts: Even in the non-preconditioned Lanczos method where a three-term recurrence is known, orthogonality problems and spurious solutions prevent the application of the method for a large number of steps. For these reasons, many restarting variants of the Lanczos and Davidson methods are used in practice <ref> [4, 15, 18, 1, 7] </ref>. 2 Department of Computer Science, College of William and Mary, Box 8795, Williamsburg, Vir-ginia 23187-8795, (andreas@cs.wm.edu). 3 Department of Computer Science, University of Minnesota, 4-192 EE/CSci Bldg., Minneapolis, Minnesota, 55455-0154 (saad@cs.umn.edu.) fl Work supported by NSF grants DMR-9217287 and ASC 95-04038, and the Minnesota Super computer
Reference: [17] <author> G. L. G. Sleijpen and H. A. V. der Vorst, </author> <title> A Jacobi-Davidson iteration method for linear eigenvalue problems, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <month> 17 </month> <year> (1996). </year>
Reference-contexts: However, as size and ill conditioning of the matrix increases, preconditioning becomes necessary to compensate for the loss of efficiency and robustness of iterative methods. The Davidson and its generalization the Jacobi-Davidson method <ref> [5, 11, 3, 17] </ref> are popular preconditioned variants of the Arnoldi method. Instead of extracting the solutions from a generated Krylov space, these methods gradually build a different space by orthogonalizing the preconditioned residual at every step against the existing basis vectors. <p> Enddo The preconditioning is performed by solving the equation at step 9, with M (s;j) approximating (A 0 I) in some sense. Originally, Morgan and Scott [11] proposed to solve approximately with some preconditioner the Generalized Davidson correction equation: (A I) ffi = r 0 :(1) In <ref> [17] </ref>, Sleijpen et al. show that for stability, robustness, as well as efficiency, the operator M (s;j) should have a range orthogonal to x 0 .
Reference: [18] <author> D. C. Sorensen, </author> <title> Implicit application of polynomial filters in a K-step Arnoldi method, </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13 (1992), </volume> <pages> pp. 357-385. </pages>
Reference-contexts: Even in the non-preconditioned Lanczos method where a three-term recurrence is known, orthogonality problems and spurious solutions prevent the application of the method for a large number of steps. For these reasons, many restarting variants of the Lanczos and Davidson methods are used in practice <ref> [4, 15, 18, 1, 7] </ref>. 2 Department of Computer Science, College of William and Mary, Box 8795, Williamsburg, Vir-ginia 23187-8795, (andreas@cs.wm.edu). 3 Department of Computer Science, University of Minnesota, 4-192 EE/CSci Bldg., Minneapolis, Minnesota, 55455-0154 (saad@cs.umn.edu.) fl Work supported by NSF grants DMR-9217287 and ASC 95-04038, and the Minnesota Super computer <p> This has significant performance shortcomings, since important components of the invariant subspace may be discarded, and the Rayleigh-Ritz procedure does not minimize over the whole Krylov subspace. An efficient solution to this problem has been given by the Implicitly Restarted Lanc-zos method (IRL) <ref> [18, 1, 8] </ref>. IRL provides an implicit way of applying a polynomial filter during restarting and thus removing unwanted spectral information. Implicit restarting provides also an elegant formulation of most previously proposed restarting schemes.
Reference: [19] <author> A. Stathopoulos and C. Fischer, </author> <title> A Davidson program for finding a few selected extreme eigenpairs of a large, sparse, real, symmetric matrix, </title> <journal> Computer Physics Communications, </journal> <volume> 79 (1994), </volume> <pages> pp. 268-290. </pages>
Reference-contexts: A few representative results using this strategy on matrices from the Harwell-Boeing collection [6] appear in Table 1. For all matrices, we solve for five lowest eigenpairs using a maximum basis size of 20, and we use the Davidson code that was described in <ref> [19] </ref> together with the extensions proposed in [20]. The number of matrix vector multiplications decreases significantly when thick restarting of double the number of required eigenpairs is used, and it improves even further with the dynamic thick restarting scheme. 2.3. Dynamic thick restarting and preconditioning.
Reference: [20] <author> A. Stathopoulos, Y. Saad, and C. Fischer, </author> <title> Robust preconditioning of large, sparse, symmetric eigenvalue problems, </title> <journal> Journal of Computational and Applied Mathematics, </journal> <volume> 64 (1995), </volume> <pages> pp. 197-215. </pages>
Reference-contexts: For all matrices, we solve for five lowest eigenpairs using a maximum basis size of 20, and we use the Davidson code that was described in [19] together with the extensions proposed in <ref> [20] </ref>. The number of matrix vector multiplications decreases significantly when thick restarting of double the number of required eigenpairs is used, and it improves even further with the dynamic thick restarting scheme. 2.3. Dynamic thick restarting and preconditioning.
Reference: [21] <author> A. Stathopoulos, K. Wu, and Y. Saad, </author> <title> Dynamic thick restarting of the davidson, and the implicitly restarted arnoldi methods, </title> <journal> SIAM Journal on Scientific Computing, </journal> <volume> 19 (1998), </volume> <pages> pp. 227-245. </pages>
Reference-contexts: Recently, it was shown that without preconditioning, and if the Ritz values are used as shifts in the polynomial filter, the Davidson method builds the same search space as IRL <ref> [21, 10] </ref>. To reduce the adverse effects of restarting, we can use the above framework to save more information at every restart. Recently, we have investigated the idea of "thick restarting" [21], which implements this principle by keeping more Ritz vectors than needed. <p> To reduce the adverse effects of restarting, we can use the above framework to save more information at every restart. Recently, we have investigated the idea of "thick restarting" <ref> [21] </ref>, which implements this principle by keeping more Ritz vectors than needed. The question to be addressed is which, and how many Ritz vectors to retain at restart. <p> Especially at restart, it can retain any number of approximate eigenvectors without the need of the implicit restarting framework. In <ref> [21] </ref> we called "thick restarting" a technique in iterative methods which restarts with more Ritz vectors than needed. We showed that in the absence of preconditioning and if exact shifts are used in the IRL, the IRL and the Davidson methods build the same search space. <p> Reducing the amount of discarded information at every restart improves the convergence of iterative methods. On the other hand, when keeping too many vectors, the Lanczos process can not effectively build additional basis vectors, and the or-thogonalization process becomes a bottleneck. In <ref> [21] </ref> it was shown that if some Ritz 4 A. STATHOPOULOS AND Y. SAAD Table 1 Comparison of thick (TR (L)) and dynamic thick restarting (Dyn) with original Davidson (TR (5)) on some symmetric Harwell-Boeing matrices, with diagonal and approximate inverse pre-conditioners.
Reference: [22] <author> J. Van Lenthe and P. Pulay, </author> <title> A space-saving modification of Davidson's eigenvector algorithm, </title> <journal> J. Comput. Chem., </journal> <note> 11 (1990), p. 1164. </note>
Reference-contexts: With preconditioning, although it is still efficient, a less expensive scheme might provide similar benefits. For the Davidson method, Murray et al. [12], and Van Lenthe et al. <ref> [22] </ref> proposed restarting with the Ritz vector from the previous iteration along with the current one. The motivation stems from the observation that the Davidson and the Conjugate Gradient (CG) methods are equivalent when solving for the eigenvector of a converged eigenvalue. <p> The difference in the behavior of the two methods is the non-linearity of the eigenvalue problem, i.e., both the eigenvalue and eigenvector are unknown. However, if the eigenvalue is known, the Lanczos Ritz vectors can be obtained by the CG process <ref> [22] </ref>. In [22], Van Lenthe et al. also proposed to use the CG method for finding an eigenvector, even when the eigenvalue is not known. This is often justified in computational quantum chemistry problems, because of the availability of good eigenvalue estimates and the relatively fast convergence of the methods. <p> The difference in the behavior of the two methods is the non-linearity of the eigenvalue problem, i.e., both the eigenvalue and eigenvector are unknown. However, if the eigenvalue is known, the Lanczos Ritz vectors can be obtained by the CG process <ref> [22] </ref>. In [22], Van Lenthe et al. also proposed to use the CG method for finding an eigenvector, even when the eigenvalue is not known. This is often justified in computational quantum chemistry problems, because of the availability of good eigenvalue estimates and the relatively fast convergence of the methods.
References-found: 21

