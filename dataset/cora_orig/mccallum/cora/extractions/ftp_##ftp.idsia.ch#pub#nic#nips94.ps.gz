URL: ftp://ftp.idsia.ch/pub/nic/nips94.ps.gz
Refering-URL: http://www.cnl.salk.edu/~schraudo/pubs.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: nici@salk.edu terry@salk.edu  
Title: Plasticity-Mediated Competitive Learning  
Author: Nicol N. Schraudolph Terrence J. Sejnowski and 
Address: San Diego, CA 92186-5800  La Jolla, CA 92093-0114  
Affiliation: Computational Neurobiology Laboratory The Salk Institute for Biological Studies  Computer Science Engineering Department University of California, San Diego  
Abstract: Differentiation between the nodes of a competitive learning network is conventionally achieved through competition on the basis of neural activity. Simple inhibitory mechanisms are limited to sparse representations, while decorrelation and factorization schemes that support distributed representations are computation-ally unattractive. By letting neural plasticity mediate the competitive interaction instead, we obtain diffuse, nonadaptive alternatives for fully distributed representations. We use this technique to simplify and improve our binary information gain optimization algorithm for feature extraction (Schraudolph and Sejnowski, 1993); the same approach could be used to improve other learning algorithms.
Abstract-found: 1
Intro-found: 1
Reference: <author> Barlow, H. B. and F oldiak, P. </author> <year> (1989). </year> <title> Adaptation and decorrelation in the cortex. </title> <editor> In Durbin, R. M., Miall, C., and Mitchison, G. J., editors, </editor> <booktitle> The Computing Neuron, chapter 4, </booktitle> <pages> pages 54-72. </pages> <publisher> Addison-Wesley, Wokingham. </publisher>
Reference: <author> Bell, A. J. and Sejnowski, T. J. </author> <year> (1995). </year> <title> A non-linear information maximisation algorithm that performs blind separation. </title> <booktitle> In Advances in Neural Information Processing Systems, volume 7, </booktitle> <address> Denver 1994. </address>
Reference: <author> Guyon, I., Poujaud, I., Personnaz, L., Dreyfus, G., Denker, J., and Le Cun, Y. </author> <year> (1989). </year> <title> Comparing different neural network architectures for classifying handwritten digits. </title> <booktitle> In Proceedings of the International Joint Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 127-132. </pages> <publisher> IEEE. </publisher>
Reference-contexts: the digits, even though the network was never given any class labels (Figure 3). 1 The interaction is excitatory rather than inhibitory since a node's plasticity is inversely correlated with the magnitude of its net input. algorithm (Equations 4 & 6) on a set of 1200 handwritten digits due to <ref> (Guyon et al., 1989) </ref>.
Reference: <author> Leen, T. K. </author> <year> (1991). </year> <title> Dynamics of learning in linear feature-discovery networks. </title> <journal> Network, </journal> <volume> 2 </volume> <pages> 85-105. </pages>
Reference-contexts: Right, same network using plasticity-mediated competition (Equations 4 & 6) succeeds. is often approximated by lateral anti-Hebbian connections whose adaptation must occur on a faster time scale than that of the feedforward weights (Equation 3) for reasons of stability <ref> (Leen, 1991) </ref>. In practice this means that learning is slowed significantly. In addition, decorrelation can be inappropriate when nonlinear objectives are optimized in our case, two prominent binary features may well be correlated.
Reference: <author> Schmidhuber, J. </author> <year> (1992). </year> <title> Learning factorial codes by predictability minimization. </title> <journal> Neural Computation, </journal> <volume> 4(6) </volume> <pages> 863-879. </pages>
Reference: <author> Schraudolph, N. N. and Sejnowski, T. J. </author> <year> (1993). </year> <title> Unsupervised discrimination of clustered data via optimization of binary information gain. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems, </booktitle> <volume> volume 5, </volume> <pages> pages 499-506, </pages> <address> Denver 1992. </address> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo. </address>
Reference-contexts: Finally, we establish a close relationship between the plasticity and the entropy of a logistic node that provides an intuitive interpretation of plasticity-mediated competitive learning in this context. 2 BINARY INFORMATION GAIN OPTIMIZATION In <ref> (Schraudolph and Sejnowski, 1993) </ref>, we proposed an unsupervised learning rule that uses logistic nodes to seek out binary features in its input. The output z = f (y), where f (y) = 1 of each node is interpreted stochastically as the probability that a given feature is present.
Reference: <author> Sejnowski, T. J. </author> <year> (1977). </year> <title> Storing covariance with nonlinearly interacting neurons. </title> <journal> Journal of Mathematical Biology, </journal> <volume> 4 </volume> <pages> 303-321. </pages>
Reference-contexts: In the simplest case, ^y is an empirical average hyi of past activity, computed either over batches of input data or by means of an exponential trace; this amounts to a nonlinear version of the covariance rule <ref> (Sejnowski, 1977) </ref>. Using just the average as prediction introduces a strong preference for splitting the data into two equal-sized clusters. While such a bias is appropriate in the initial phase of learning, it fails to take the nonlinear nature of f into account.
References-found: 7

