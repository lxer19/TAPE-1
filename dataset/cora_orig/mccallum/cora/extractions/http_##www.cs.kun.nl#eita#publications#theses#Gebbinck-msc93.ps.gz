URL: http://www.cs.kun.nl/eita/publications/theses/Gebbinck-msc93.ps.gz
Refering-URL: http://www.cs.kun.nl/eita/publications/
Root-URL: 
Title: Ultrasonic Tissue Characterization Using Neural Networks  
Author: Maurice S. klein Gebbinck 
Date: November 4, 1992  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Oosterveld, B.J., </author> <title> On the Quantitative Analysis of Ultrasound Signals with Applications to Diffuse Liver Disease, </title> <institution> University of Nijmegen, Nijmegen, </institution> <type> Ph.D. project report, </type> <year> 1990. </year>
Reference-contexts: The philosophy behind this approach is that some diseases alter the structure and properties of certain tissues, resulting in a different response to the exposure with ultrasound. Uptil now the classification of tissues based on these features is done using discriminant analysis. With this method good results are obtained <ref> [1] </ref>, but neural networks might perform even better, since neural networks are not limited to linear or quadratic separation functions as discriminant analysis is. Just like in other fields, researchers of UI investigate the applicability of neural networks as well [2, 3, 4, 5]. <p> THE DATA SET ultrasonic tissue characterization in this thesis. The selection was based on the results of earlier research, partially described in <ref> [1] </ref>. In the remainder of this section these parameters, ordered in decreasing discriminating ability, are briefly reviewed. Mean amplitude This parameter has already been explained in section 2.5.1. <p> Clearly the second and third solution are imaginary and thus useless, but the first might satisfy. Unfortunately the term y 2 y is not positive for a y from <ref> [0; 1] </ref>, which is the domain of interest, so the square root of this term is imaginary. This disqualifies the first solution also. The only possible line of action to compute F 1 (y) is that of numerical approximation.
Reference: [2] <author> Nikoonahad, M. and Liu, </author> <title> D.C., Medical Ultrasound Imaging using Neural Networks, </title> <journal> Electronics Letters 26, </journal> <pages> 545-546, </pages> <year> 1990. </year>
Reference-contexts: With this method good results are obtained [1], but neural networks might perform even better, since neural networks are not limited to linear or quadratic separation functions as discriminant analysis is. Just like in other fields, researchers of UI investigate the applicability of neural networks as well <ref> [2, 3, 4, 5] </ref>. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields [6, 7].
Reference: [3] <author> Parikh, J.A. and DaPonte, J.S., </author> <title> Application of Neural Networks to Pattern Recognition Problems in Remote Sensing and Medical Imagery, </title> <booktitle> Applications of Artificial Neural Networks SPIE 1294, </booktitle> <pages> 146-160, </pages> <year> 1990. </year>
Reference-contexts: With this method good results are obtained [1], but neural networks might perform even better, since neural networks are not limited to linear or quadratic separation functions as discriminant analysis is. Just like in other fields, researchers of UI investigate the applicability of neural networks as well <ref> [2, 3, 4, 5] </ref>. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields [6, 7].
Reference: [4] <author> Silverman, R.H. and Noetzel, </author> <title> A.S., Image Processing and Pattern Recognition in Ultrasonograms by Backpropagation, </title> <booktitle> Neural Networks 3, </booktitle> <pages> 593-603, </pages> <year> 1990. </year>
Reference-contexts: With this method good results are obtained [1], but neural networks might perform even better, since neural networks are not limited to linear or quadratic separation functions as discriminant analysis is. Just like in other fields, researchers of UI investigate the applicability of neural networks as well <ref> [2, 3, 4, 5] </ref>. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields [6, 7].
Reference: [5] <author> Ostrem, J.S., Valdes, A.D. and Edmonds, </author> <title> P.D., Application of Neural Nets to Ultrasound Tissue Characterization, </title> <booktitle> Ultrasonic Imaging 13, </booktitle> <pages> 298-299, </pages> <year> 1991. </year>
Reference-contexts: With this method good results are obtained [1], but neural networks might perform even better, since neural networks are not limited to linear or quadratic separation functions as discriminant analysis is. Just like in other fields, researchers of UI investigate the applicability of neural networks as well <ref> [2, 3, 4, 5] </ref>. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields [6, 7]. <p> Just like in other fields, researchers of UI investigate the applicability of neural networks as well [2, 3, 4, 5]. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI <ref> [5] </ref> and in other comparable fields [6, 7]. In this light, this graduation-assignment is to investigate the use of neural networks for ultrasonic tissue characterization of diffuse liver diseases.
Reference: [6] <author> Refenes, A.N. and Bilge, U., </author> <title> Self-Organizing Feature Maps in Pre-Processing Datasets for Decision Support in Histopathology, </title> <booktitle> NSC/BME-90E, Proc. Int. North Sea Conf. in Biomedical Engineering, </booktitle> <year> 1990. </year>
Reference-contexts: Just like in other fields, researchers of UI investigate the applicability of neural networks as well [2, 3, 4, 5]. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields <ref> [6, 7] </ref>. In this light, this graduation-assignment is to investigate the use of neural networks for ultrasonic tissue characterization of diffuse liver diseases. The results are to be compared with the results obtained with discriminant analysis, in order to see which of the methods prevails.
Reference: [7] <author> Refenes, </author> <title> A.N., </title> <editor> Jain, N. and Alsulaiman, M.M., </editor> <title> An Integrated Neural Network System for Histological Image Understanding, </title> <booktitle> Proc. Int. Symp. SPIE, </booktitle> <year> 1990. </year>
Reference-contexts: Just like in other fields, researchers of UI investigate the applicability of neural networks as well [2, 3, 4, 5]. For tissue characterization neural networks seem to be useful, 4 CHAPTER 1. INTRODUCTION both in the field of UI [5] and in other comparable fields <ref> [6, 7] </ref>. In this light, this graduation-assignment is to investigate the use of neural networks for ultrasonic tissue characterization of diffuse liver diseases. The results are to be compared with the results obtained with discriminant analysis, in order to see which of the methods prevails.
Reference: [8] <institution> Leidse Onderwijs Instellingen, Basiskennis Echografie. Vak 902 , Neder-landse Vereniging voor Ultrageluid in de Geneeskunde en de Biologie, </institution> <year> 1992. </year>
Reference-contexts: First of all, it is explained what ultrasound is and how it can be generated. Next the interactions of ultrasound with the medium it travels through are dealt with. Finally it is shown how the acquired data can be transformed into an image. See <ref> [8] </ref> for more detailed information on this subject. 2.1 The transducer Sound with frequencies so high that humans can not hear it, is called ultrasound. Usually this implies a frequency of 20 kHz or more. Unlike most other waves, sound waves are longitudinal.
Reference: [9] <institution> Verhoeven, J.T.M., Een Snelle Digitale Signaal Verwerker voor het Ul-trasoon Biopsie Apparaat, Technical University of Eindhoven, Eindhoven, graduation project report, </institution> <year> 1986. </year>
Reference-contexts: Using this equation the amount of attenuation can be determined experimentally, so the RF-signal can be corrected for it. Much more on this subject can be found in <ref> [9] </ref>. 2.3.4 Diffraction When you examine the beam of ultrasound produced by a transducer regardless of the tissue it travels through, you find that it isn't constant of intensity nor of shape. <p> Because these two factors are dependent on the tissue, the attenuation is a good parameter for the characterization of tissues. The frequency dependent attenuation can be described by the attenuation-coefficient ff (f ). A method for determining this coefficient can be found in <ref> [9] </ref>. Usually it is the sum of a number of terms of which the higher order terms are negligible small at relatively low frequencies. A good approximation therefore is in other words a linear fit through the origin. In this thesis the parameter will be referred to as fi.
Reference: [10] <author> Fabel, R., </author> <title> Laterale Diffraktie-Korrektie van Ultrageluidsscans, </title> <institution> Technical University of Eindhoven, Eindhoven, graduation project report, </institution> <year> 1988. </year>
Reference-contexts: The RF-signal can be corrected for the diffraction effects. A description of this method can be found in <ref> [10] </ref>. 2.4. CONSTRUCTION OF IMAGES 9 unfocussed beam, the thin solid line the beam with medium focus and the dotted line the beam with short focus. 2.4 Construction of images Many different techniques exist to graphically represent echo signals.
Reference: [11] <author> Goodman, J.W., </author> <title> Some fundamental properties of speckle, </title> <journal> J Optical Society of America 66, </journal> <pages> 1145-1150, </pages> <year> 1976. </year>
Reference-contexts: Most people consider these echoes as noise that should be removed, but it is possible to characterize the tissue by it. 2.5 Speckle Speckle is a phenomenon that was first described in connection with lasers. A good explanation of this can be found in several articles by Goodman <ref> [11, 12] </ref>. 12 CHAPTER 2. ULTRASOUND IMAGING In ultrasound imaging speckle is caused by scatterers. When an ultrasonic wave hits a scatterer, the ultrasound is scattered is in all directions. Some of the ultrasound is scattered back to the transducer.
Reference: [12] <author> Goodman, J.W., </author> <title> Statistical Optics, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1975. </year> <note> 119 120 BIBLIOGRAPHY </note>
Reference-contexts: Most people consider these echoes as noise that should be removed, but it is possible to characterize the tissue by it. 2.5 Speckle Speckle is a phenomenon that was first described in connection with lasers. A good explanation of this can be found in several articles by Goodman <ref> [11, 12] </ref>. 12 CHAPTER 2. ULTRASOUND IMAGING In ultrasound imaging speckle is caused by scatterers. When an ultrasonic wave hits a scatterer, the ultrasound is scattered is in all directions. Some of the ultrasound is scattered back to the transducer.
Reference: [13] <author> Wagner, R.F., Smith, S.W., Sandrik, J.M. and Lopez, H., </author> <title> Statistics of Speckle in Ultrasound B-Scans, </title> <journal> IEEE Transactions on Sonics and Ultrasonics 30, </journal> <pages> 156-163, </pages> <year> 1983. </year>
Reference-contexts: These statistics are dependent on the imaging system and the density of the scatterers. If the density is above a certain level speckle only depends on the imaging system. This situation is often referred to as fully developed speckle. For fully developed speckle a model can be constructed <ref> [13] </ref>. In sections 2.5.1 to 2.5.4 theoretical and practical results [14, 15] are compared. 2.5.1 Mean amplitude Theoretically the probability distribution function for the magnitude of the amplitude can be given by p (A) = 2 e 2 2 (A &gt; 0) (2.4) with 2 the backscattered signal power.
Reference: [14] <author> Thijssen, J.M. and Oosterveld, B.J., </author> <title> Texture in Tissue Echograms. Speckle or Information? , J Ultrasound Medicine 9, </title> <type> 215-229, </type> <year> 1990. </year>
Reference-contexts: If the density is above a certain level speckle only depends on the imaging system. This situation is often referred to as fully developed speckle. For fully developed speckle a model can be constructed [13]. In sections 2.5.1 to 2.5.4 theoretical and practical results <ref> [14, 15] </ref> are compared. 2.5.1 Mean amplitude Theoretically the probability distribution function for the magnitude of the amplitude can be given by p (A) = 2 e 2 2 (A &gt; 0) (2.4) with 2 the backscattered signal power. This probability distribution function is known as the Rayleigh distribution function.
Reference: [15] <author> Oosterveld, B.J., Thijssen, J.M. and Verhoef, </author> <title> W.A., Texture of B-Mode Echograms: 3-D Simulations and Experiments of the Effects of Diffraction and Scatterer Density, </title> <booktitle> Ultrasonic Imaging 7, </booktitle> <pages> 142-160, </pages> <year> 1985. </year>
Reference-contexts: If the density is above a certain level speckle only depends on the imaging system. This situation is often referred to as fully developed speckle. For fully developed speckle a model can be constructed [13]. In sections 2.5.1 to 2.5.4 theoretical and practical results <ref> [14, 15] </ref> are compared. 2.5.1 Mean amplitude Theoretically the probability distribution function for the magnitude of the amplitude can be given by p (A) = 2 e 2 2 (A &gt; 0) (2.4) with 2 the backscattered signal power. This probability distribution function is known as the Rayleigh distribution function.
Reference: [16] <author> Holmstrom, L. and Koistinen, P., </author> <title> Using Additive Noise in BackPropagation Training, </title> <journal> IEEE Transactions on Neural Networks 3, </journal> <pages> 24-38, </pages> <year> 1992. </year>
Reference-contexts: This may seem to be a flaw of the method, but is simply the result of a much larger number of cases. 3.4.2 A kernel based method Another method for the generation of data is described in <ref> [16, 17] </ref>. It uses a probability density K, the kernel , and a smoothing parameter h to approximate the distribution function of the data f by its n-point kernel estimate f n;h;K . <p> The characteristics of the generated data are controlled by the choice of K and h. For a normal density f with variance 2 , which is the case in our problem, the optimal choice <ref> [16] </ref> for K turns out to be the Bartlett kernel (see figure 3.11), defined by K (x) = 4 1 x 2 (3.8) in combination with a value for the smoothing parameter h given by h = 1:8218n 1 First the standard deviation and smoothing parameter for a parameter of the
Reference: [17] <author> Duin, R.P.W., </author> <title> On the Choice of Smoothing Parameters for Parzen Estimators of Probability Density Functions, </title> <journal> IEEE Transactions on Computers C-25, </journal> <pages> 1175-1179, </pages> <year> 1976. </year>
Reference-contexts: This may seem to be a flaw of the method, but is simply the result of a much larger number of cases. 3.4.2 A kernel based method Another method for the generation of data is described in <ref> [16, 17] </ref>. It uses a probability density K, the kernel , and a smoothing parameter h to approximate the distribution function of the data f by its n-point kernel estimate f n;h;K .
Reference: [18] <institution> Schikhof, W., Analyse en Calculus 2 , University of Nijmegen, Nijmegen, reader, </institution> <year> 1984. </year>
Reference-contexts: This disqualifies the first solution also. The only possible line of action to compute F 1 (y) is that of numerical approximation. In this thesis Newton's method <ref> [18] </ref> is used because of its fast convergence.
Reference: [19] <author> Albert, A and Harris, E.K., </author> <title> Multivariate Interpretation of Clinical Laboratory Data, </title> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1987. </year>
Reference-contexts: In the first part of this chapter the method is described without discussing the complete theory behind it, for more details see <ref> [19] </ref>. In the second part the results obtained with discriminant analysis are given. 4.1 Principles of the method Discriminant analysis can be performed using two methods, a parametric and a non-parametric method. <p> It can be proven that the percentage of correctly classified patients using the cross-validation method is at most equal to the percentage obtained with the resubstitution method <ref> [19, p. 124] </ref>. The results on the first problems solved with discriminant analysis are used in two ways. Of course the results are needed for comparing discriminant analysis with neural networks, but they are also used to get a feeling for the different options in discriminant analysis. <p> This observation is in agreement with the theory which states that the percentage of correctly classified patients using the cross-validation method is at most equal to the percentage obtained with the resub-stitution method <ref> [19, p. 124] </ref>. The deterioration of the percentage of correctly classified patients is much stronger for the small classes than for class A, something that can be explained very easily.
Reference: [20] <institution> SAS Institute Inc., </institution> <note> SAS/STAT User's Guide Version 6 Volume 1 , SAS Institute Inc., </note> <institution> Cary (NC), </institution> <year> 1989. </year>
Reference-contexts: The posterior probability, which can be calculated for an arbitrary patient, determines to which class a patient is assigned: the patient under observation is classified into the class with the highest posterior probability. Some software packets <ref> [20, p. 677-771] </ref> that can perform discriminant analysis use a slightly different, but related method. <p> This type of discriminant analysis is often called quadratic discriminant analysis. 4.2 Results The results that are presented in the remainder of this chapter have been obtained using a statistical analysis system called SAS/STAT 1 . More information about the procedure used (DISCRIM ) can be found in <ref> [20] </ref>. 4.2.1 Measuring the performance The performance of discriminant analysis is measured using so called confusion or specificity/sensitivity matrices, displaying the classification percentages of the data set under observation.
Reference: [21] <author> Dayhoff, </author> <title> J.E., Neural Network Architectures. An Introduction, </title> <publisher> Van Nos-trand Reinhold, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: In the first part of this chapter the theory behind back-propagation is explained. In the second part the results obtained with this method are presented. A good introduction to this subject can be found in <ref> [21] </ref>, while a more mathematical approach is taken in [22]. 5.1 Network architecture In the early days of neural networks research a well-known model was the perceptron. Because the perceptron consisted of a single layer of adaptable weights, only linear separable problems could be solved. <p> How feature mapping works is explained in the first part of this chapter, while in the second part the results of the application of feature mapping to our data are presented. More on this subject can be found in <ref> [21, 22] </ref>. 6.1 Network architecture In 1982 Kohonen presented a new type of neural network called self-organizing feature map, which is very suitable for classification problems. <p> An example of the topology preserving property, taken from <ref> [21] </ref>, can be found in figures 6.2 to 6.4, in which the feature map of figure 6.1 is trained with a set of points uniformly drawn from [0:0; 1:0] fi [0:0; 1:0]. <p> Neighbourhood radius The size of the neighbourhood is a parameter that does not have to be varied. It is common practice to adjust the initial neighbourhood radius (d 0 ) to a half or a third of the width of the feature map <ref> [21, p. 168] </ref>; we have chosen for half the width. During the training process the neighbourhood radius is automatically decreased using equation 6.5. Initial weights If one uses the basic feature mapping algorithm, there exist basically two methods for the initialization of the weights.
Reference: [22] <author> Hertz, J., Krogh, A. and Palmer, R.G., </author> <title> Introduction to the Theory of Neural Computation, </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Redwood City, </address> <year> 1991. </year>
Reference-contexts: In the first part of this chapter the theory behind back-propagation is explained. In the second part the results obtained with this method are presented. A good introduction to this subject can be found in [21], while a more mathematical approach is taken in <ref> [22] </ref>. 5.1 Network architecture In the early days of neural networks research a well-known model was the perceptron. Because the perceptron consisted of a single layer of adaptable weights, only linear separable problems could be solved. <p> Equations 5.6 and 5.7 suggest that updating of the weights is done after all the patterns have been presented to the network. Of course this strategy, called learning by epoch, can be applied, but in <ref> [22, p. 119] </ref> can be found that updating after each pattern, learning by pattern, can be more effective. The reason for this is that with learning by pattern, the patterns can be presented in random order, making the path through the weight-space stochastic. <p> Furthermore we are not interested in the classification results of the network on the training set, because on this set 100% correct classification can easily be achieved if only the size of the network is large enough <ref> [22, p. 142] </ref>. To reduce the number of points in the figures to 20 without risking that one of the selected points is non-representative for the performance curve for each point the average of 25 successive points is taken. <p> The network has to stay a feed-forward network. Although there is no restriction on the number of hidden layers, in <ref> [22, p. 142] </ref> can be found that a network with three or more hidden layers offers no additional computational power over a network with only two hidden layers. <p> In view of the fact that, the more layers a network consists of, the less well it can be parallelized, it seems to be sensible to confine the architecture to one or two hidden layers. As mentioned before, also in <ref> [22, p. 142] </ref> can be found that a function can be approximated to any given accuracy, if only the size of the network is large enough. For a classification problem this means that a correct classification of the training set up to 100% can be achieved. <p> But just as well could as a result of the momentum term a local minimum be avoided where otherwise the network would get stuck in. The constant that determines the magnitude of the momentum term (ff) must lie between 0 and 1 and is usually chosen relatively high <ref> [22, p. 123] </ref>. As an initial value for ff we therefore choose 0.8. Initial weights When a neural network is initialized, random values are assigned to its weights. <p> This difference is called the 5.4. RESULTS 57 grain of the training process. Since in <ref> [22, p. 119] </ref> can be found that learning by pattern in combination with a random order of presenting the patterns is useful for avoiding local minima, only this fine learn grain is considered. 5.4.3 Discrimination between A and B In this section the results of back-propagation applied to the problem of <p> How feature mapping works is explained in the first part of this chapter, while in the second part the results of the application of feature mapping to our data are presented. More on this subject can be found in <ref> [21, 22] </ref>. 6.1 Network architecture In 1982 Kohonen presented a new type of neural network called self-organizing feature map, which is very suitable for classification problems. <p> The unused neurons do not have any influence on the network performance, but do 88 CHAPTER 6. FEATURE MAPPING slow down the training process. In 1988 Desieno <ref> [22, 24] </ref> therefore proposed a variation in the basic algorithm to cancel this disadvantage. Desieno provided every neuron of the output layer with a conscience, making it for frequently winning neurons difficult to win again. <p> Learning grain Just like with back-propagation, updating of the weights can take place after presentation of a single pattern | learning by pattern | or all patterns | learning by epoch | of the training set. In <ref> [22, p. 222] </ref> can be found that 1 Of course, a three-dimensional feature space implies a cube, two-dimensions a rectangle and one-dimension a line segment. 2 Lower dimensions imply successively sphere, circle and line segment. 3 This property is acquired by the way in which we have scaled the data, see
Reference: [23] <author> Nowlan, S.J. and Hinton, G.E., </author> <title> Soft Weight-Sharing, </title> <institution> University of Toronto, Toronto, </institution> <type> technical report, </type> <year> 1991. </year>
Reference-contexts: One of these methods that seems to be promising is soft weight-sharing. This technique tries to distribute the weights using several Gaussian distributions, each with a different mean and standard deviation. A well-readable explanation of this can be found in <ref> [23] </ref>. 5.4 Results To make the data suitable for back-propagation, the representation of the classes has to be slightly altered. Instead of a simple number back-propagation needs a target pattern that represents the class of the input pattern.
Reference: [24] <author> Bierman, S. and Hooft, H. van, </author> <title> A Kohonen Neural Networks Simulator , University of Nijmegen, </title> <journal> M.Sc. </journal> <note> project report in preparation. </note>
Reference-contexts: These clusters have emerged because the data itself contains clusters and the feature map is topology preserving, something which has been explained in section 6.2.1. To determine the borders between the clusters a data structure called the U-matrix is used <ref> [24] </ref>, containing information about the feature space distances between adjacent neurons on the map. If a two-dimensional map with D 2 entries is used, the U-matrix will contain (2D 1) 2 entries. <p> The unused neurons do not have any influence on the network performance, but do 88 CHAPTER 6. FEATURE MAPPING slow down the training process. In 1988 Desieno <ref> [22, 24] </ref> therefore proposed a variation in the basic algorithm to cancel this disadvantage. Desieno provided every neuron of the output layer with a conscience, making it for frequently winning neurons difficult to win again. <p> A short users-manual of this simulator can be found in <ref> [24] </ref>, while our application specific alterations of the simulator | koho | are described in appendix A. 6.4.1 Measuring the performance The way in which the performance of a feature map is measured is more or less equal to measuring the performance of a back-propagation network, something that is done deliberately
References-found: 24

