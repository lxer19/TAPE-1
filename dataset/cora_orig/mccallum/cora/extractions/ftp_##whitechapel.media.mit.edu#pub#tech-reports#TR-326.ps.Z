URL: ftp://whitechapel.media.mit.edu/pub/tech-reports/TR-326.ps.Z
Refering-URL: http://www.cs.gatech.edu/classes/cs7322_97_spring/papers/papers.html
Root-URL: 
Title: Probabilistic Visual Learning for Ob ject Detection  
Author: Baback Moghaddam and Alex Pentland 
Address: 20 Ames St., Cambridge, MA 02139  
Affiliation: Vision and Modeling Group, The Media Laboratory Massachusetts Institute of Technology  
Abstract: M.I.T Media Laboratory Perceptual Computing Section Technical Report No. 326 Appears in: The 5th International Conference on Computer Vision, Cambridge, MA, June 1995. Abstract We present an unsupervised technique for visual learning which is based on density estimation in high-dimensional spaces using an eigenspace decomposition. Two types of density estimates are derived for modeling the training data: a multivariate Gaussian (for a unimodal distribution) and a multivariate Mixture-of-Gaussians model (for multimodal distributions). These probability densities are then used to formulate a maximum-likelihood estimation framework for visual search and target detection for automatic object recognition. This learning technique is tested in experiments with modeling and subsequent detection of human faces and non-rigid objects such as hands.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Bichsel, M., and Pentland, A., </author> <title> "Human Face Recognition and the Face Image Set's Topology," CVGIP: </title> <booktitle> Image Understanding, </booktitle> <volume> Vol. 59, No. 2, </volume> <pages> pp. 254-261, </pages> <year> 1994. </year>
Reference-contexts: In fact the training data tends to lie on complex and non-separable low-dimensional manifolds in image space <ref> [1] </ref>. One way to tackle this multimodality is to build a view-based (or object-based) formulation where separate eigenspaces are used for each view [10]. Another approach is to capture the complexity of these manifolds in a universal or parametric eigenspace using splines [9], or local basis functions [2].
Reference: [2] <author> Bregler, C., and Omohundro, </author> <title> S.M., "Surface learning with applications to lip reading," </title> <booktitle> in Advances in Neural Information Processing Systems 6, </booktitle> <editor> eds. J.D. Cowan, G. Tesauro and J. Alspector, </editor> <publisher> Morgan Kauf-man Publishers, </publisher> <address> San Fransisco, </address> <pages> pp. 43-50, </pages> <year> 1994. </year>
Reference-contexts: One way to tackle this multimodality is to build a view-based (or object-based) formulation where separate eigenspaces are used for each view [10]. Another approach is to capture the complexity of these manifolds in a universal or parametric eigenspace using splines [9], or local basis functions <ref> [2] </ref>.
Reference: [3] <author> Cover, M. and Thomas, J.A., </author> <title> Elements of Information Theory, </title> <publisher> John Wiley & Sons, </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: The optimal value of can now be determined by minimizing a suitable cost function J (). From an information-theoretic point of view, this cost function should be the Kullback-Leibler divergence <ref> [3] </ref> between the true density P (xj) and its estimate ^ P (xj) J () = E log ^ P (xj) (9) Using the diagonalized forms of the Mahalanobis distance d (x) and its estimate ^ d (x) and the fact that E [y 2 i ] = i , it
Reference: [4] <author> Dempster, A.P., Laird, N.M., Rubin, </author> <title> D.B., "Maximum likelihood from incomplete data via the EM algorithm," </title> <journal> Journal of the Royal Statistical Society B, </journal> <volume> vol. 39, </volume> <year> 1977. </year>
Reference-contexts: Given a training set fy t g N T t=1 the mixture parameters can be estimated using the ML principle fi fl = argmax " N T Y P (y t jfi) (13) This estimation problem is best solved using the Expectation-Maximization (EM) algorithm <ref> [4] </ref>. The EM algorithm is monotonically convergent in likelihood and is thus guaranteed to find a local maximum in the total likelihood of the training set. Further details of the EM algorithm for estimation of mixture densities can be found in [12].
Reference: [5] <author> Jolliffe, </author> <title> I.T., Principal Component Analysis, </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1986. </year>
Reference-contexts: The first method is based on the assumption of a Gaussian distribution while the second method generalizes to arbitrarily complex distributions using a Mixture-of-Gaussians density model. Before introducing these estimators we briefly review eigenvector decomposition as commonly used in principal component analysis (PCA) <ref> [5] </ref>. 2.1 Principal Component Imagery Given a set of m-by-n images fI t g N T t=1 , we can form a training set of vectors fx t g, where x 2 R N=mn , by lexicographic ordering of the pixel elements of each image I t .
Reference: [6] <author> Kumar, B., Casasent, D., and Murakami, H., </author> <title> "Principal Component Imagery for Statistical Pattern Recognition Correlators," </title> <journal> Optical Engineering, </journal> <volume> vol. 21, no. 1, </volume> <month> Jan/Feb </month> <year> 1982. </year>
Reference: [7] <author> Loeve, </author> <title> M.M., Probability Theory, </title> <publisher> Van Nostrand, Princeton, </publisher> <year> 1955. </year>
Reference-contexts: The basis functions in a Karhunen-Loeve Transform (KLT) <ref> [7] </ref> are obtained by solving the eigenvalue problem fl = T (1) where is the covariance matrix of the data, is the eigenvector matrix of and fl is the corresponding diagonal matrix of eigenvalues.
Reference: [8] <author> Moghaddam, B. and Pentland, A., </author> <title> "Face recognition using view-based and modular eigenspaces," in Automatic Systems for the Identification and Inspection of Humans, </title> <booktitle> SPIE vol. </booktitle> <volume> 2277, </volume> <year> 1994. </year>
Reference-contexts: The top three matches in this case are images of the same person taken a month apart and at different scales. The recognition accuracy (defined as the percent correct rank-one matches) on a database of 155 individuals is 99% <ref> [8] </ref>. We have also extended the normalized eigenface representation into an edge-based domain for facial description. We simply run the normalized facial image through a Canny edge detector to yield an edge-map.
Reference: [9] <author> Murase, H., and Nayar, S. K., </author> <title> "Learning and Recognition of 3D Objects from Appearance" in IEEE 2nd Qualitative Vision Workshop, </title> <address> New York, NY, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: One way to tackle this multimodality is to build a view-based (or object-based) formulation where separate eigenspaces are used for each view [10]. Another approach is to capture the complexity of these manifolds in a universal or parametric eigenspace using splines <ref> [9] </ref>, or local basis functions [2].
Reference: [10] <author> Pentland, A., Moghaddam, B. and Starner, T., </author> <title> "View-based and modular eigenspaces for face recognition," </title> <booktitle> Proc. of IEEE Conf. on Computer Vision & Pattern Recognition, </booktitle> <month> June </month> <year> 1994. </year>
Reference-contexts: In a statistical signal detection framework, the use of eigentemplates has been shown to yield superior performance in comparison with standard matched filtering [6]<ref> [10] </ref>. Pentland et al. [10] used this formulation for a modular eigenspace representation of facial features where the corre sponding residual | referred to as "distance-from-feature-space" (DFFS) | was used for localization and detection. Given an input image, a saliency map was constructed by computing the DFFS at each pixel. <p> In fact the training data tends to lie on complex and non-separable low-dimensional manifolds in image space [1]. One way to tackle this multimodality is to build a view-based (or object-based) formulation where separate eigenspaces are used for each view <ref> [10] </ref>. Another approach is to capture the complexity of these manifolds in a universal or parametric eigenspace using splines [9], or local basis functions [2]. <p> In this section we will present several examples from these application domains. 4.1 Faces The eigentemplate approach to the detection of facial features in "mugshots" was proposed in <ref> [10] </ref>, where the DFFS metric was shown to be superior to standard template 3 (a) (b) and (b) the resulting typical detections. detector. matching for target detection.
Reference: [11] <author> Pentland, A., Picard, R., and Sclaroff, S., "Photo-book: </author> <title> Tools for Content-Based Manipulation of Image Databases," in Storage and Retrieval of Image and Video Databases II, </title> <booktitle> SPIE vol. </booktitle> <volume> 2185, </volume> <year> 1994. </year>
Reference-contexts: This example illustrates that the eigenface representation used for recognition is also an effective model-based representation for data compression. The first 8 eigenfaces used for this representation are shown in an image database tool called Photobook <ref> [11] </ref>. Each face in the database was automatically detected and aligned by the face processing system in Figure 6. The normalized faces were then projected onto a 100-dimensional eigenspace.
Reference: [12] <author> Redner, R.A., and Walker, H.F., </author> <title> "Mixture densities, maximum likelihood and the EM algorithm," </title> <journal> SIAM Review, </journal> <volume> vol. 26, no. 2, </volume> <pages> pp. 195-239, </pages> <year> 1984. </year>
Reference-contexts: The EM algorithm is monotonically convergent in likelihood and is thus guaranteed to find a local maximum in the total likelihood of the training set. Further details of the EM algorithm for estimation of mixture densities can be found in <ref> [12] </ref>.
Reference: [13] <author> Sung, K., and Poggio, T., </author> <title> "Example-based Learning for View-based Human Face Detection," </title> <journal> A.I. </journal> <volume> Memo No. 1521, </volume> <booktitle> Artificial Intelligence Laboratory, </booktitle> <publisher> MIT, </publisher> <year> 1994. </year>
Reference-contexts: This can be viewed as a probabilistic approach to learning using positive as well as negative examples. The use of negative examples has been shown to be critically important in building robust face detection systems <ref> [13] </ref>. Acknowledgements The FERET face database was provided by the US Army Research Laboratory. This research was partially funded by British Telecom.
Reference: [14] <author> Turk, M., and Pentland, A., </author> <title> "Eigenfaces for Recognition," </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> Vol. 3, No. 1, </volume> <year> 1991. </year>
Reference-contexts: In particular, the eigenspace formulation leads to a powerful alternative to standard detection techniques such as template matching or normalized correlation. The reconstruction error (or residual) of the eigenspace decomposition (referred to as the "distance-from-face-space" in the context of the work with "eigenfaces" <ref> [14] </ref>) is an effective indicator of similarity. The residual error is easily computed using the projection coefficients and the original signal energy.
References-found: 14

