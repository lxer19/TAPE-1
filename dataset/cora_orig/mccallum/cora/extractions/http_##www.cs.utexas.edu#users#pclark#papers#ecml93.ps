URL: http://www.cs.utexas.edu/users/pclark/papers/ecml93.ps
Refering-URL: http://www.cs.utexas.edu/users/pclark/papers/ecml93.abs.html
Root-URL: 
Email: pete@ai.iit.nrc.ca stan@csi.uottawa.ca  
Title: Learning Domain Theories using Abstract Background Knowledge  
Author: Peter Clark Stan Matwin 
Address: Ottawa, Canada University of Ottawa, Ottawa, Canada  
Affiliation: Knowledge Systems Laboratory Ottawa Machine Learning Group National Research Council Computer Science  
Web: http://www.cs.utexas.edu/users/pclark/papers/ecml93.ps  
Note: In: Proc. Sixth European Conference on Machine Learning (ECML-93), pp360-365, Ed: Pavel Prazdil, Berlin: Springer-Verlag (1993)  
Abstract: Substantial machine learning research has addressed the task of learning new knowledge given a (possibly incomplete or incorrect) domain theory, but leaves open the question of where such domain theories originate. In this paper we address the problem of constructing a domain theory from more general, abstract knowledge which may be available. The basis of our method is to first assume a structure for the target domain theory, and second to view background knowledge as constraints on components of that structure. This enables a focusing of search during learning, and also produces a domain theory which is explainable with respect to the background knowledge. We evaluate an instance of this methodology applied to the domain of economics, where background knowledge is represented as a qualitative model.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <booktitle> In First International Conference on Algorithmic Learning Theory, </booktitle> <pages> pages 369-381, </pages> <address> Tokyo, Japan, </address> <year> 1990. </year> <journal> Japanese Society for Artificial Intellligence. </journal>
Reference-contexts: 1 Introduction It is now well recognised that to learn all but the simplest domain theories from examples, background knowledge is required to constrain search. While several recent learning systems use background knowledge to extend the theory language (e.g. by introducing new terms <ref> [1] </ref>), the use of background knowledge to constrain search in a domain-specific fashion is still a relatively unexplored area. This paper presents and evaluates a simple methodology for doing this. An extended version of this paper is available as [2].
Reference: 2. <author> Peter Clark and Stan Matwin. </author> <title> Learning domain theories using abstract background knowledge. </title> <type> Tech. Report TR-92-35, </type> <institution> Dept CS, Univ. </institution> <address> Ottawa, </address> <month> May </month> <year> 1992. </year>
Reference-contexts: This paper presents and evaluates a simple methodology for doing this. An extended version of this paper is available as <ref> [2] </ref>. We define a domain theory to be a system of knowledge for solving some specific target task, and background knowledge more generally to refer to arbitrary available knowledge. We thus view an idealised domain theory as task-specific, coherent and non-redundant (avoiding details irrelevant to the task). <p> The QM dramatically reduces the size of the rule space from 90,000 rules to 1666 rules <ref> [2] </ref>. We hope that these `explainable' rules will be adequate for constructing a predictive domain theory.
Reference: 3. <author> Bruce W. Porter, Ray Bareiss, and Robert C. Holte. </author> <title> Concept learning and heuristic classification in weak-theory domains. </title> <journal> Artificial Intelligence, </journal> <volume> 45 </volume> <pages> 229-263, </pages> <year> 1990. </year>
Reference-contexts: For example, in the economics application considered later, background knowledge is expressed using qualitative terms while the raw data is numeric, and no well-defined mapping exists between the two. This language gap problem is common in AI (e.g. <ref> [3] </ref>). Applying the methodology, we assume a `two-layer' structure for the target domain theory, in which the top layer uses the abstract terminology of the background knowledge and the bottom layer relates this terminology to the basic facts known about examples.
Reference: 4. <author> Ranan B. Banerji. </author> <title> Learning theoretical terms. </title> <editor> In Stephen Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <year> 1992. </year>
Reference-contexts: The T i can be described as ill-defined `theoretical' terms, and the F i as `observational' terms <ref> [4] </ref>, the two-layer structure distinguishing between these two vocabularies of background knowledge and observation. We call a clause of type (1) a rule, and a clause of type (2) a definition.
Reference: 5. <author> Gerald DeJong. </author> <title> Explanation-based learning with plausible inferencing. </title> <booktitle> In Proc. 4th European Machine Learning Conference (EWSL-89), </booktitle> <pages> pages 1-10, </pages> <year> 1989. </year>
Reference-contexts: For example, the rule "if interest rates high then GNP will decrease." has a plausible explanation: high interest rates reduce companies' profits, reducing future investment and eventually reducing productivity and GNP. We capture this naive knowledge in the form of a qualitative model (QM), in a similar way to <ref> [5] </ref>. The QM expresses the believed relations between the 10 parameters P and an additional 8 unmeasurable parameters Q (the unboxed items in Figure 2).
Reference: 6. <author> Kenneth D. Forbus. </author> <title> Qualitative process theory. </title> <journal> AI Journal, </journal> <volume> 24 </volume> <pages> 85-168, </pages> <year> 1984. </year>
Reference-contexts: Each parameter has an associated numeric value (for a given country and year), but in the model we use just two qualitative values, high or low. As in Qualitative Process Theory <ref> [6] </ref>, we label the arcs Q+ to denote a positive influence and Q a negative influence.
Reference: 7. <author> James C. Spohrer and Christopher K. Riesbeck. </author> <title> Reasoning-driven memory modification in the economics domain. </title> <type> Technical Report YALEU/DCS/RR-308, </type> <institution> Yale University, </institution> <month> May </month> <year> 1984. </year>
Reference-contexts: The model we use is depicted in Figure 2, constructed manually by the authors in the style of Charniak's economic model <ref> [7] </ref>. A rule extraction algorithm is used to extract plausible rules from the model: a rule corresponds to a subgraph which has exactly one node reachable from every other node.
Reference: 8. <author> Peter Clark and Robin Boswell. </author> <title> Rule induction with CN2: Some recent improvements. </title> <editor> In Yves Kodratoff, editor, </editor> <booktitle> Machine Learning - EWSL-91, </booktitle> <pages> pages 151-163, </pages> <address> Berlin, 1991. </address> <publisher> Springer-Verlag. </publisher>
Reference-contexts: Given these definitions, induce rules RSet using the training data. This is done using a greedy set covering algorithm, and performing a standard general-to-specific beam search for a good rule at each iteration of the covering algorithm (the same algorithm used in CN2 for propositional learning <ref> [8] </ref>). The space searched is the QM-constrained space of rules (Section 2.2). 3. Keeping RSet fixed, use a hill-climbing algorithm to search for an improved DSet, by trying alternative definitions for individual terms according to eqn (3).
References-found: 8

