URL: http://www.iscs.nus.sg/~plong/papers/more_theorems.ps
Refering-URL: 
Root-URL: 
Email: Peter.Bartlett@anu.edu.au  plong@iscs.nus.edu.sg  
Title: Prediction, Learning, Uniform Convergence, and Scale-sensitive Dimensions  
Author: Peter L. Bartlett Philip M. Long 
Address: Canberra, 0200 Australia  Singapore 119260, Republic of Singapore  
Affiliation: Department of Systems Engineering RSISE, Australian National University  ISCS Department National University of Singapore  
Abstract: We present a new general-purpose algorithm for learning classes of [0; 1]-valued functions in a generalization of the prediction model, and prove a general upper bound on the expected absolute error of this algorithm in terms of a scale-sensitive generalization of the Vapnik dimension proposed by Alon, Ben-David, Cesa-Bianchi and Haussler. We give lower bounds implying that our upper bounds cannot be improved by more than a constant factor in general. We apply this result, together with techniques due to Haussler and to Benedek and Itai, to obtain new upper bounds on packing numbers in terms of this scale-sensitive notion of dimension. Using a different technique, we obtain new bounds on packing numbers in terms of Kearns and Schapire's fat-shattering function. We show how to apply both packing bounds to obtain improved general bounds on the sample complexity of agnostic learning. For each * &gt; 0, we establish weaker sufficient and stronger necessary conditions for a class of [0; 1]-valued functions to be agnostically learnable to within *, and to be an *-uniform Glivenko-Cantelli class. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> N. Alon, S. Ben-David, N. Cesa-Bianchi, and D. Haussler. </author> <title> Scale-sensitive dimensions, uniform convergence, and learnability. </title> <booktitle> In Proceedings of the 1993 IEEE Symposium on Foundations of Computer Science. </booktitle> <publisher> IEEE Press, </publisher> <year> 1993. </year>
Reference-contexts: 1 Introduction In the prediction model studied in this paper, a <ref> [0; 1] </ref>-valued function f chosen from some known class F is hidden from the learner, the learner is given examples of f evaluated at m 1 elements of the domain of f that were chosen independently at random according to an arbitrary, unknown distribution, another random point x is chosen, and <p> We give bounds on the expected error of this strategy in terms of fatV, the scale-sensitive generalization of the Vapnik dimension introduced by Alon, Ben-David, Cesa-Bianchi and Haussler <ref> [1] </ref> (which is similar to a notion introduced by Kearns and Schapire [14]), and show that no algorithm can improve on these bounds in general by more than a constant factor. <p> We apply the above prediction bound, together with ideas due to Haussler [11] and Benedek and Itai [5], to obtain new bounds on packing numbers in terms of fatV. In agnostic learning [10, 15], a distribution on X fi <ref> [0; 1] </ref> is unknown, and the learner, given examples drawn according to this distribution, tries to find a function h from X to [0; 1] so that, with probability at least 1 ffi, the expectation of jh (x) yj is at most * larger than the minimum of this expectation over <p> In agnostic learning [10, 15], a distribution on X fi <ref> [0; 1] </ref> is unknown, and the learner, given examples drawn according to this distribution, tries to find a function h from X to [0; 1] so that, with probability at least 1 ffi, the expectation of jh (x) yj is at most * larger than the minimum of this expectation over functions in some touchstone class F . <p> This improves on the bound of O 1 fatV F (*=384) * log 2 fatV F (*=384) * 1 that is a straightforward consequence of the results of <ref> [1] </ref> (see [4]). Next, using a different technique, we obtain a new packing bound in terms of Kearns and Schapire's fat-shattering function. This leads to a bound 2 of O 1 * 1 on the sample complexity of (*; ffi)-agnostic learning F . <p> This leads to a bound 2 of O 1 * 1 on the sample complexity of (*; ffi)-agnostic learning F . This improves on the dependence on fat F of the bound O 1 * 1 that follows from the packing bound of <ref> [1] </ref> (see [4]). <p> A standard technique converts this to an *-agnostic learning algorithm. 2 Definitions 2.1 Definitions for the prediction model For a set X, a prediction strategy is a mapping from ([ m (X fi <ref> [0; 1] </ref>) m ) fi X to [0; 1]. Let P X be the set of all prediction strategies, and let D X be the set of all probability distributions on X. For each set F of functions from X to [0; 1], and each positive integer m, define 3 L <p> A standard technique converts this to an *-agnostic learning algorithm. 2 Definitions 2.1 Definitions for the prediction model For a set X, a prediction strategy is a mapping from ([ m (X fi <ref> [0; 1] </ref>) m ) fi X to [0; 1]. Let P X be the set of all prediction strategies, and let D X be the set of all probability distributions on X. For each set F of functions from X to [0; 1], and each positive integer m, define 3 L (F; m) as L (F; m) = <p> strategy is a mapping from ([ m (X fi <ref> [0; 1] </ref>) m ) fi X to [0; 1]. Let P X be the set of all prediction strategies, and let D X be the set of all probability distributions on X. For each set F of functions from X to [0; 1], and each positive integer m, define 3 L (F; m) as L (F; m) = inf sup X m where the supremum is over all D in D X and f in F . <p> That is, L (F; m) is the worst-case expected error of the best prediction strategy. This is a generalization of the f0; 1g prediction model of [13] to <ref> [0; 1] </ref>-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from [ n2N (X fi [0; 1]) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. <p> This is a generalization of the f0; 1g prediction model of [13] to [0; 1]-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from <ref> [ n2N (X fi [0; 1] </ref>) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. <p> This is a generalization of the f0; 1g prediction model of [13] to <ref> [0; 1] </ref>-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from [ n2N (X fi [0; 1]) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. If h is a [0; 1]-valued function defined on X, and P is a probability distribution over X fi [0; 1], define the error of h with respect to P as <p> This is a generalization of the f0; 1g prediction model of [13] to <ref> [0; 1] </ref>-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from [ n2N (X fi [0; 1]) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. If h is a [0; 1]-valued function defined on X, and P is a probability distribution over X fi [0; 1], define the error of h with respect to P as er P (h) = <p> <ref> [0; 1] </ref>-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from [ n2N (X fi [0; 1]) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. If h is a [0; 1]-valued function defined on X, and P is a probability distribution over X fi [0; 1], define the error of h with respect to P as er P (h) = Xfi [0;1] Suppose F is a class of [0; 1]-valued functions defined on X, 0 &lt; *; ffi &lt; 1 <p> set X to be a mapping from [ n2N (X fi <ref> [0; 1] </ref>) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. If h is a [0; 1]-valued function defined on X, and P is a probability distribution over X fi [0; 1], define the error of h with respect to P as er P (h) = Xfi [0;1] Suppose F is a class of [0; 1]-valued functions defined on X, 0 &lt; *; ffi &lt; 1 and m 2 N. <p> If h is a <ref> [0; 1] </ref>-valued function defined on X, and P is a probability distribution over X fi [0; 1], define the error of h with respect to P as er P (h) = Xfi [0;1] Suppose F is a class of [0; 1]-valued functions defined on X, 0 &lt; *; ffi &lt; 1 and m 2 N. We say a learner A (*; ffi)-learns in the agnostic sense with respect to F from m examples if, for all distributions P on X fi [0; 1], m f2F For * &gt; 0, the <p> Xfi [0;1] Suppose F is a class of <ref> [0; 1] </ref>-valued functions defined on X, 0 &lt; *; ffi &lt; 1 and m 2 N. We say a learner A (*; ffi)-learns in the agnostic sense with respect to F from m examples if, for all distributions P on X fi [0; 1], m f2F For * &gt; 0, the function class F is *-agnostically learnable if there is a function m 0 : (0; 1) ! N such that, for all 0 &lt; ffi &lt; 1, there is a learner A which (*; ffi)-learns in the agnostic sense with respect to <p> The reader may assume that X is countable, but significantly weaker assumptions, like those of Pollard's [17] Appendix C, suffice. 2 2.3 Definition of *-uniform GC-classes For *; ffi &gt; 0, a set X and a set F of functions from X to <ref> [0; 1] </ref>, if D X is the set of all probability distributions over X, define m GC;F (*; ffi) = min n : 8m n; 8D 2 D X ; ( fi fi fi m i=1 ! Z f (u) dD (u) fi fi &gt; * ffi : If the minimum <p> That is, define Q ff (u) = ffbu=ffc. Let Q ff (<ref> [0; 1] </ref>) = fQ ff (u) : u 2 [0; 1]g. <p> Finally, for a set F of such functions, define Q ff (F ) = fQ ff (f ) : f 2 F g: 2.6 Definitions relating to fat For m 2 N, S <ref> [0; 1] </ref> m , and fl &gt; 0, we say S fl-fatly shatters a sequence (i 1 ; r 1 ); :::; (i d ; r d ) of elements of f1; : : : ; mg fi [0; 1] if for all (b 1 ; :::; b d ) 2 <p> F g: 2.6 Definitions relating to fat For m 2 N, S <ref> [0; 1] </ref> m , and fl &gt; 0, we say S fl-fatly shatters a sequence (i 1 ; r 1 ); :::; (i d ; r d ) of elements of f1; : : : ; mg fi [0; 1] if for all (b 1 ; :::; b d ) 2 f0; 1g d there is a v 2 S such that for all j 2 f1; :::; dg, v i j r j fl if b j = 0. <p> We then define fat S (fl) to be the length of the longest sequence fl-fatly shattered by S. For a set F of functions from X to <ref> [0; 1] </ref>, and a finite sequence ~ = (x 1 ; :::; x n ) of elements of X, define the restriction of F to ~ to be F j ~ = f (f (x 1 ); :::; f (x n )) : f 2 F g: We define fat F <p> We define fat F (fl) to be the maximum, over all finite sequences ~ of elements of X, of fat F j ~ (fl). (This was called the fat-shattering function in [4], and was defined by Kearns and Schapire [14].) 3 2.7 Definitions relating to fatV For each r 2 <ref> [0; 1] </ref> and * &gt; 0, define r;* : [0; 1] ! f0; ?; 1g by r;* (y) = &gt; &lt; 1 if y r + * 0 if y r *. For a function f from X to [0; 1], define r;* (f ) : X ! f0; ?; 1g <p> over all finite sequences ~ of elements of X, of fat F j ~ (fl). (This was called the fat-shattering function in [4], and was defined by Kearns and Schapire [14].) 3 2.7 Definitions relating to fatV For each r 2 <ref> [0; 1] </ref> and * &gt; 0, define r;* : [0; 1] ! f0; ?; 1g by r;* (y) = &gt; &lt; 1 if y r + * 0 if y r *. For a function f from X to [0; 1], define r;* (f ) : X ! f0; ?; 1g by ( r;* (f ))(x) = r;* (f (x)); <p> [14].) 3 2.7 Definitions relating to fatV For each r 2 <ref> [0; 1] </ref> and * &gt; 0, define r;* : [0; 1] ! f0; ?; 1g by r;* (y) = &gt; &lt; 1 if y r + * 0 if y r *. For a function f from X to [0; 1], define r;* (f ) : X ! f0; ?; 1g by ( r;* (f ))(x) = r;* (f (x)); and for a set F of such functions, define r;* (F ) = f r;* (f ) : f 2 F g: We say x 1 ; : : :; <p> ))(x) = r;* (f (x)); and for a set F of such functions, define r;* (F ) = f r;* (f ) : f 2 F g: We say x 1 ; : : :; x d in X are fl-fatly Vapnik-shattered by F if there is an r 2 <ref> [0; 1] </ref> such that f0; 1g d f ( r;fl (f (x 1 )); :::; r;fl (f (x d ))) : f 2 F g: Define fatV F (fl) to be the length of the longest sequence fl-fatly Vapnik-shattered by F . (This dimension was first studied in [1].) Notice that <p> r 2 [0; 1] such that f0; 1g d f ( r;fl (f (x 1 )); :::; r;fl (f (x d ))) : f 2 F g: Define fatV F (fl) to be the length of the longest sequence fl-fatly Vapnik-shattered by F . (This dimension was first studied in <ref> [1] </ref>.) Notice that fat F (fl) and fatV F (fl) are both non-increasing functions of fl. 3 Prediction of [0; 1]-valued functions and fatV This section describes our general-purpose prediction strategy and shows that it is nearly optimal. <p> (x d ))) : f 2 F g: Define fatV F (fl) to be the length of the longest sequence fl-fatly Vapnik-shattered by F . (This dimension was first studied in [1].) Notice that fat F (fl) and fatV F (fl) are both non-increasing functions of fl. 3 Prediction of <ref> [0; 1] </ref>-valued functions and fatV This section describes our general-purpose prediction strategy and shows that it is nearly optimal. The first theorem of the paper gives the bound for the worst-case expected error incurred by this strategy. Theorem 1 Choose a set F of functions from X to [0; 1], fl <p> Prediction of <ref> [0; 1] </ref>-valued functions and fatV This section describes our general-purpose prediction strategy and shows that it is nearly optimal. The first theorem of the paper gives the bound for the worst-case expected error incurred by this strategy. Theorem 1 Choose a set F of functions from X to [0; 1], fl &gt; 0, and a positive integer m. Then L (F; m) 2fatV F (fl) m Fix a set X. Theorem 1 is proved by considering an algorithm that generates its prediction using binary search (details are given below). <p> Proof (of Theorem 1): Let d = fatV F (fl). Consider the strategy A defined as follows. For each r 2 <ref> [0; 1] </ref>, define B r to be the strategy for learning r;fl (F ) described in Lemma 4. <p> fl; u + fl], it is the case that j^y yj jl uj + fl, we have E (jA (z) f (x m )j) fl Pr (E 1 ) + j2N Applying Lemma 4, and the fact that VCdim ( r;fl (F )) fatV F (fl) for all r 2 <ref> [0; 1] </ref>, we get E (jA (z) f (x m )j) fl + j2N fl + 2d=m: This completes the proof. The following theorem shows that Theorem 1 cannot be improved in general by more than a constant factor, and that the constant on the fl term is best possible. <p> The proof uses techniques due to Ehrenfeucht, Haussler, Kearns and Valiant [8], Haussler, Littlestone and Warmuth [13], and Simon [18]. Theorem 5 There exists c such that for all sufficiently small fl 0, and all sufficiently large d; m 2 N, there is an X, and F <ref> [0; 1] </ref> X such that fatV F (fl) = d and L (F; m) max c fatV F (fl) m Proof: Consider the class F of all functions f from N to [0; 1] such that jf 1 ([2fl; 1])j d. Clearly, fatV F (fl) = d. <p> sufficiently small fl 0, and all sufficiently large d; m 2 N, there is an X, and F <ref> [0; 1] </ref> X such that fatV F (fl) = d and L (F; m) max c fatV F (fl) m Proof: Consider the class F of all functions f from N to [0; 1] such that jf 1 ([2fl; 1])j d. Clearly, fatV F (fl) = d. We begin by proving the first term. Consider the distribution D on f1; :::; dg where D (1) = 1 (d 1)=m and D (2) = ::: = D (d) = 1=m. <p> The following corollary shows that finiteness of fatV F at a scale just below the desired prediction error is sufficient, and that no larger scale will suffice in general. Corollary 6 Suppose * &gt; 0. For a set F of functions from X to <ref> [0; 1] </ref>, if there is an ff &gt; 0 with fatV F (* ff) &lt; 1, then for sufficiently large m, L (F; m) &lt; *. <p> Proposition 7 For any non-increasing function from (0; 1=2] to N [ f0; 1g, there is a function class F : N ! <ref> [0; 1] </ref> that satisfies fatV F (fl) = (fl) for all fl in (0; 1=2]. 8 Proof: Let fA d;n : d 2 N [ f1g; n 2 Ng be a partition of N, with jA d;n j = d for d; n 2 N, and A 1;n countably infinite for <p> One uses fatV, and is proved using Theorem 1, together with techniques from [11, 5]. The second bound uses fat, and is proved through a refinement of a proof in <ref> [1] </ref>. For a set X, and F [0; 1] X , define m L (*; F ) = minfm 2 N : L (F; m) *g: The following bound on m L (*; F ) follows immediately from Theorem 1. <p> One uses fatV, and is proved using Theorem 1, together with techniques from [11, 5]. The second bound uses fat, and is proved through a refinement of a proof in [1]. For a set X, and F <ref> [0; 1] </ref> X , define m L (*; F ) = minfm 2 N : L (F; m) *g: The following bound on m L (*; F ) follows immediately from Theorem 1. Lemma 8 Choose X, F [0; 1] X , and ff; * &gt; 0. <p> For a set X, and F <ref> [0; 1] </ref> X , define m L (*; F ) = minfm 2 N : L (F; m) *g: The following bound on m L (*; F ) follows immediately from Theorem 1. Lemma 8 Choose X, F [0; 1] X , and ff; * &gt; 0. Assume fatV F (* ff) 1. Then m L (*; F ) 2fatV F (* ff)=ff: For m 2 N; x = (x 1 ; :::; x m ) 2 X m , and f : X ! [0; 1], define sam <p> Choose X, F <ref> [0; 1] </ref> X , and ff; * &gt; 0. Assume fatV F (* ff) 1. Then m L (*; F ) 2fatV F (* ff)=ff: For m 2 N; x = (x 1 ; :::; x m ) 2 X m , and f : X ! [0; 1], define sam (x; f ) = ((x 1 ; f (x 1 )); :::; (x m ; f (x m ))): We will also make use of the following, which is implicit in the work of Haussler, et al [13]. Lemma 9 Choose X, F [0; 1] X . <p> : X ! <ref> [0; 1] </ref>, define sam (x; f ) = ((x 1 ; f (x 1 )); :::; (x m ; f (x m ))): We will also make use of the following, which is implicit in the work of Haussler, et al [13]. Lemma 9 Choose X, F [0; 1] X . There is a learner A such that for all f 2 F , for any distribution D on X, for all m 2 N, j (A (sam (x; f )))(u) f (u)j dD (u) dD m1 (x) is no more than L (F; m). <p> Let B = Q 1=b (<ref> [0; 1] </ref>). Choose m 2 N, and let S B m . Set d = fatV S (*=2 ff). Then if d 1, M (*; S) 2ff Proof: For each v 2 S, define f v : f1; :::; mg ! [0; 1] by f v (i) = v i , and define F = ff v : v 2 Sg. Let D be the uniform distribution on f1; :::; mg. <p> Next, we give a new bound on M (*; S) in terms of fat S . Its proof is based on that of a corresponding lemma in <ref> [1] </ref> which dealt with the ` 1 norm. Lemma 11 Choose * &gt; 0. Choose b 2 N, b &gt; 4=*. Let B = Q 1=b ([0; 1]). Choose m 2 N, and let S B m . <p> Since t (2) 1, by induction, for all k, t (2b 3k ) 2 k , and therefore t 2b 3 (blog 2 yc+1) However, as argued in <ref> [1] </ref>, there are only y sets of at most d elements of f1; : : :; mg fi B. <p> We start with the latter. Theorem 12 Choose X, and a set F of functions from X to <ref> [0; 1] </ref>. <p> The following is a restatement of Theorem 8 of Chapter II of [17]. Lemma 14 ([17]) Suppose X and U are sets, D is a probability distribution on X, and : X fi U ! <ref> [0; 1] </ref> and : X fi U ! [0; 1] are functions for which (; u 1 ) and (; u 2 ) are independent random variables for all u 1 and u 2 in U . <p> The following is a restatement of Theorem 8 of Chapter II of [17]. Lemma 14 ([17]) Suppose X and U are sets, D is a probability distribution on X, and : X fi U ! <ref> [0; 1] </ref> and : X fi U ! [0; 1] are functions for which (; u 1 ) and (; u 2 ) are independent random variables for all u 1 and u 2 in U . <p> Then for all * &gt; 0, Dfx : sup (x; u) &gt; *g fi u These are applied in the following. Lemma 15 Choose a set X, and a set F <ref> [0; 1] </ref> X . Choose * &gt; 0, 0 &lt; ff &lt; * and m 2 N. <p> Lemma 16 Choose X, F <ref> [0; 1] </ref> X . Let D be a probability distribution over X. Choose 0 &lt; ff; * &lt; 1 with ff &lt; *=2. <p> Choose f 2 F . Suppose g : X ! <ref> [0; 1] </ref> had 1 m X (jg (x i ) f (x i )j + jg (y i ) f (y i )j) *=2 ff; and that fi fi fi 1 m X u i (f (x i ) f (y i )) fi fi &gt; *: Then fi fi fi <p> Then u 1 (v 1 v m+1 ); :::; u m (v m v 2m ) form a sequence of independent <ref> [1; 1] </ref> random variables with zero mean. Applying Hoeffding's inequality, we get U m (u 1 ; :::; u m ) : fi fi m i=1 ! fi fi fi ) Combining this with jT j sup N (*=2 ff; F j ); (8), and (7) completes the proof. <p> Applying Hoeffding's inequality, we get U m (u 1 ; :::; u m ) : fi fi m i=1 ! fi fi fi ) Combining this with jT j sup N (*=2 ff; F j ); (8), and (7) completes the proof. Lemma 17 Choose S <ref> [0; 1] </ref> m , * &gt; 0, ff &lt; *=2. Then N (*; S) N (* ff; Q ff (S)): Proof: Choose v; w 2 [0; 1] m . ` 1 (v; Q ff (w)) = m i=1 jv i ffbw i =ffcj = m i=1 ` 1 (v; w) ff: <p> Lemma 17 Choose S <ref> [0; 1] </ref> m , * &gt; 0, ff &lt; *=2. Then N (*; S) N (* ff; Q ff (S)): Proof: Choose v; w 2 [0; 1] m . ` 1 (v; Q ff (w)) = m i=1 jv i ffbw i =ffcj = m i=1 ` 1 (v; w) ff: Thus ` 1 (v; w) ` 1 (v; Q ff (w)) + ff. <p> This lemma is implicit in the analysis of Natarajan [16]. 4 Lemma 19 Suppose that X is a set, F is a class of functions that map from X to <ref> [0; 1] </ref>, x = (x 1 ; : : : ; x m ) 2 X m , and z = ((x 1 ; y 1 ); : : :; (x m ; y m )) 2 (X fi [0; 1]) m . <p> F is a class of functions that map from X to <ref> [0; 1] </ref>, x = (x 1 ; : : : ; x m ) 2 X m , and z = ((x 1 ; y 1 ); : : :; (x m ; y m )) 2 (X fi [0; 1]) m . <p> * &gt; 0, N 4 It is also possible to relate the fat-shattering functions of these classes directly using Sauer's lemma (see [9]), but the proof is not as simple and the result slightly weaker. 15 Theorem 20 Choose a set X, a set F of functions from X to <ref> [0; 1] </ref>, and *; ffi &gt; 0. If there exists &gt; 0 such that for all * &gt; 0, fat F ((1=4 )*) is finite, then there is a learner A that (*; ffi)-learns in the agnostic sense with respect to F from O 1 * 1 examples. <p> Proof: Fix fi &gt; 0, a small positive constant. The algorithm we will consider takes a sample ((x 1 ; y 1 ); : : : ; (x m ; y m )) 2 (X fi <ref> [0; 1] </ref>) m and chooses a function f 0 2 F that has 1 m X jf 0 (x i ) y i j &lt; inf 1 m X jf (x i ) y i j + fi: Fix any distribution P on X fi [0; 1]. <p> m )) 2 (X fi <ref> [0; 1] </ref>) m and chooses a function f 0 2 F that has 1 m X jf 0 (x i ) y i j &lt; inf 1 m X jf (x i ) y i j + fi: Fix any distribution P on X fi [0; 1]. Let f fl 2 F satisfy er P (f fl ) inf g2F er P (g) + fi. <p> *-agnostic learning, which saves a factor of two in the scale at which the dimension must be finite over that described in the previous section, sometimes at the expense of a small increase in the sample complexity. 16 Theorem 21 Choose X, a set F of functions from X to <ref> [0; 1] </ref>, and *; ffi &gt; 0. <p> Proof: Fix k 2 N, let ff = 1=d1=(*)e, and let fl = ff=13. Consider a mapping Q from (X fi <ref> [0; 1] </ref>) k fi X k to [0; 1], defined as follows. Fix a function that maps from X 2k to the set of finite subsets of [0; 1] 2k such that, for any x 2 X 2k , (x) is a minimal (* 9fl)-cover of F j x , and <p> Proof: Fix k 2 N, let ff = 1=d1=(*)e, and let fl = ff=13. Consider a mapping Q from (X fi <ref> [0; 1] </ref>) k fi X k to [0; 1], defined as follows. Fix a function that maps from X 2k to the set of finite subsets of [0; 1] 2k such that, for any x 2 X 2k , (x) is a minimal (* 9fl)-cover of F j x , and (x) is invariant under permutations of the <p> Proof: Fix k 2 N, let ff = 1=d1=(*)e, and let fl = ff=13. Consider a mapping Q from (X fi <ref> [0; 1] </ref>) k fi X k to [0; 1], defined as follows. Fix a function that maps from X 2k to the set of finite subsets of [0; 1] 2k such that, for any x 2 X 2k , (x) is a minimal (* 9fl)-cover of F j x , and (x) is invariant under permutations of the components of x. <p> Then let x = (x 1 ; : : : ; x 2k ) 2 X 2k , and for (y 1 ; : : :; y k ) 2 <ref> [0; 1] </ref> k let Q ((x 1 ; y 1 ); : : :; (x k ; y k ); x k+1 ; : : : ; x 2k ) = t 0 2k , where t 0 = (t 0 1 ; : : : ; t 0 satisfies 1 <p> x k+1 ; : : : ; x 2k ) = t 0 2k , where t 0 = (t 0 1 ; : : : ; t 0 satisfies 1 k X jt 0 s2 (x) k i=1 We will first show that, for any distribution on X fi <ref> [0; 1] </ref>, Q predicts the value y 2k associated with x 2k almost as well as the best function in F , taking expectations over random sequences. <p> We use this property to construct a learner that returns a hypothesis that has error within * of the best in F , with high probability. Fix a distribution P on X fi <ref> [0; 1] </ref>. Suppose ((x 1 ; y 1 ); : : :; (x 2k ; y 2k )) 2 (X fi [0; 1]) 2k is a random sequence chosen according to P . <p> Fix a distribution P on X fi <ref> [0; 1] </ref>. Suppose ((x 1 ; y 1 ); : : :; (x 2k ; y 2k )) 2 (X fi [0; 1]) 2k is a random sequence chosen according to P . Let x = (x 1 ; : : : ; x 2k ) and y = (y 1 ; : : : ; y 2k ). <p> In that case, with probability at least 1 fl, ` 1 (f fl ; y) er P (f fl ) + fl, which implies ` 1 (t fl ; y) * 8fl + er P (f fl ). For two vectors a; b 2 <ref> [0; 1] </ref> 2k , define ` first 1 (a; b) = k i=1 ` last 1 (a; b) = k i=k+1 17 Now, as in the proof of Lemma 16, let U be the uniform distribution over f1; 1g. <p> We also provide examples showing that these necessary conditions are not sufficient conditions, and that they cannot be improved. First, we prove the necessity condition for *-uniform GC classes. The proof is based on that of the analogous result for fatV which was proved in <ref> [1] </ref> and follows from this new result since fatV F fat F for all F . It improves on the result in [1] by a factor of 2 the scale at which fat F 's finiteness is necessary for F to be an *-uniform GC class. <p> First, we prove the necessity condition for *-uniform GC classes. The proof is based on that of the analogous result for fatV which was proved in <ref> [1] </ref> and follows from this new result since fatV F fat F for all F . It improves on the result in [1] by a factor of 2 the scale at which fat F 's finiteness is necessary for F to be an *-uniform GC class. Theorem 22 Choose X, F [0; 1] X , and 0 &lt; * &lt; 1. <p> It improves on the result in [1] by a factor of 2 the scale at which fat F 's finiteness is necessary for F to be an *-uniform GC class. Theorem 22 Choose X, F <ref> [0; 1] </ref> X , and 0 &lt; * &lt; 1. Then if there exists ff &gt; 0 such that fat F (*=2 + ff) = 1, then F is not an *-uniform GC class. Proof: Choose 0 &lt; * &lt; 1. Assume for contradiction that there exist X, F [0; 1] <p> F <ref> [0; 1] </ref> X , and 0 &lt; * &lt; 1. Then if there exists ff &gt; 0 such that fat F (*=2 + ff) = 1, then F is not an *-uniform GC class. Proof: Choose 0 &lt; * &lt; 1. Assume for contradiction that there exist X, F [0; 1] X , and ff &gt; 0 such that fat F (*=2 + ff) = 1 but that F is an *-uniform GC class. Let m = m GC;F (*; 1=2). <p> Proof: Fix 0 &lt; * &lt; 1=2 and let F be the class of all functions f from N to <ref> [0; 1] </ref> satisfying f (i) 2 f1=2 + (*=2 + 1=(i + 3)); 1=2 (*=2 + 1=(i + 3))g: Clearly, for all ff &gt; 0, fat F (*=2 + ff) is finite. <p> Next, we turn to proving a necessary condition for *-agnostic learnability. The following variant of fat F , due to Simon [18], will be useful. For X, F <ref> [0; 1] </ref> X , and fl &gt; 0, we say F strongly fl-fatly shatters a sequence (x 1 ; l 1 ; u 1 ); :::; (x d ; l d ; u d ) of elements of X fi [0; 1] 2 if u i l i + 2fl for <p> For X, F <ref> [0; 1] </ref> X , and fl &gt; 0, we say F strongly fl-fatly shatters a sequence (x 1 ; l 1 ; u 1 ); :::; (x d ; l d ; u d ) of elements of X fi [0; 1] 2 if u i l i + 2fl for i = 1; : : :; d and, for all (b 1 ; :::; b d ) 2 f0; 1g d , there is an f 2 F such that f (x i ) = u i , b j <p> The following lemma, whose proof closely follows parts of that of a related result in [18], as well as Theorem 5, will be useful. Lemma 25 Choose X, F <ref> [0; 1] </ref> X , * &gt; 0. Then if there exists ff &gt; 0 such that sfat F (* + ff) is infinite, then F is not *-agnostically learnable. <p> Proof: Assume for contradiction that F is *-agnostically learnable, but that there exists ff &gt; 0 such that sfat F (* + ff) is infinite. Fix such an ff &gt; 0. Let m 2 N, and a learner A be such that for all distributions P on X fi <ref> [0; 1] </ref>, P m z : j (A (z))(x) yj dP (x; y) ( inf jf (x) yj) + * 1=2: Choose d 2 N such that d &gt; ff Choose a sequence (x 1 ; l 1 ; u 1 ); : : :; (x d ; l d ; <p> For each b 2 f0; 1g d , let P b be a distribution over X fi <ref> [0; 1] </ref> obtained by choosing the first component uniformly from x 1 ; :::; x d , and evaluating f b at the first component to get the second. <p> Theorem 26 Choose X, F <ref> [0; 1] </ref> X , and * &gt; 0. Then if there exists ff &gt; 0 such that fat F (* + ff) is infinite, then F is not *-agnostically learnable. Proof: Fix ff &gt; 0 such that fat F (* + ff) is infinite. <p> By Lemma 9 of [2], this implies sfat Q ff=3 (F ) (* + 2ff=3) is infinite, and then Lemma 25 implies Q ff=3 (F ) is not (*+ff=3)-agnostically learnable. But then F is not *-agnostically learnable, since for every f 2 F and distribution P on X fi <ref> [0; 1] </ref>, er P (f ) er P (Q ff=3 (f )) + ff=3, so a learner that *-agnostically learns F can (* + ff=3)-agnostically learn Q ff=3 (F ). Next, we show that the converse of Theorem 26 is not true. <p> Proof: As in the proof of Theorem 23, fix 0 &lt; * &lt; 1=4 and let F be the class of all functions f from N to <ref> [0; 1] </ref> satisfying f (i) 2 f1=2 + (* + 1=(i + 3)); 1=2 (* + 1=(i + 3))g: Clearly, for all ff &gt; 0, fat F (* + ff) is finite. Choose d 2 N. <p> Choose d 2 N. For b 2 f1; 1g d , choose f b such that for each i 2 f1; ::; dg, f (i) = 1=2 + b i (* + 1=(i + 3)). Define a distribution P b over f1; :::; dg fi <ref> [0; 1] </ref> by choosing the first component uniformly from f1; :::; dg, and evaluating f b at the first component to get the second.
Reference: [2] <author> M. Anthony and P. Bartlett. </author> <title> Function learning from interpolation. </title> <booktitle> In Computational Learning Theory: </booktitle> <address> EUROCOLT'95, </address> <year> 1995. </year>
Reference-contexts: Proof: Fix ff &gt; 0 such that fat F (* + ff) is infinite. Then fat Q ff=3 (F ) (* + 2ff=3) is infinite. By Lemma 9 of <ref> [2] </ref>, this implies sfat Q ff=3 (F ) (* + 2ff=3) is infinite, and then Lemma 25 implies Q ff=3 (F ) is not (*+ff=3)-agnostically learnable.
Reference: [3] <author> M. Anthony, N. Biggs, and J. Shawe-Taylor. </author> <title> The learnability of formal concepts. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 246-257, </pages> <year> 1990. </year>
Reference-contexts: Therefore, if some T is an (* ff)-cover of Q ff (S), then T is an *-cover of S, completing the proof. Next, we write down a lemma calculating a useful inverse. The lemma is proved using the by now standard technique from <ref> [3] </ref>. <p> y 3 ), then m y 4 y 2 ln 2y 2 y 3 y 1 implies 1 y 4 m y 4 y 2 ln 1 y 1 Solving for m, we get m y 4 y 2 fly 3 m + ln 1 y 1 Applying the fact <ref> [3] </ref> that for all x; fl &gt; 0, ln x + ln fl flx with x = y 3 m, we get m y 4 y 2 ln (y 3 m) + ln ffi : Solving for ffi completes the proof.
Reference: [4] <author> P. L. Bartlett, P. M. Long, and R. C. Williamson. </author> <title> Fat-shattering and the learnability of real-valued functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(3) </volume> <pages> 434-452, </pages> <year> 1996. </year>
Reference-contexts: This improves on the bound of O 1 fatV F (*=384) * log 2 fatV F (*=384) * 1 that is a straightforward consequence of the results of [1] (see <ref> [4] </ref>). Next, using a different technique, we obtain a new packing bound in terms of Kearns and Schapire's fat-shattering function. This leads to a bound 2 of O 1 * 1 on the sample complexity of (*; ffi)-agnostic learning F . <p> This leads to a bound 2 of O 1 * 1 on the sample complexity of (*; ffi)-agnostic learning F . This improves on the dependence on fat F of the bound O 1 * 1 that follows from the packing bound of [1] (see <ref> [4] </ref>). <p> F j ~ = f (f (x 1 ); :::; f (x n )) : f 2 F g: We define fat F (fl) to be the maximum, over all finite sequences ~ of elements of X, of fat F j ~ (fl). (This was called the fat-shattering function in <ref> [4] </ref>, and was defined by Kearns and Schapire [14].) 3 2.7 Definitions relating to fatV For each r 2 [0; 1] and * &gt; 0, define r;* : [0; 1] ! f0; ?; 1g by r;* (y) = &gt; &lt; 1 if y r + * 0 if y r *.
Reference: [5] <author> G. M. Benedek and A. Itai. </author> <title> Learnability with respect to fixed distributions. </title> <journal> Theoretical Computer Science, </journal> <volume> 86 </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: We apply the above prediction bound, together with ideas due to Haussler [11] and Benedek and Itai <ref> [5] </ref>, to obtain new bounds on packing numbers in terms of fatV. <p> One uses fatV, and is proved using Theorem 1, together with techniques from <ref> [11, 5] </ref>. The second bound uses fat, and is proved through a refinement of a proof in [1]. <p> We apply these in the following. In addition to Theorem 1, the proof uses ideas due to Haussler [11] and Benedek and Itai <ref> [5] </ref>. 9 Lemma 10 Choose 0 &lt; * &lt; 1, b 2 N and 0 &lt; ff &lt; *=4. Let B = Q 1=b ([0; 1]). Choose m 2 N, and let S B m . Set d = fatV S (*=2 ff).
Reference: [6] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: Thus, no such subset can have fat S (*=22=b) at most d. Taking the contrapositive completes the proof of the first inequality in the lemma. The second inequality is obtained by bounding y using Sauer's lemma (see, for example, <ref> [6] </ref>). 5 Sample complexity bounds In this section, we apply the bounds of the previous section to upper bound the sample size necessary for agnostic learnability, and for uniformly good estimates of the expectations of a set of random variables. We start with the latter.
Reference: [7] <author> K.L. Buescher and P.R. Kumar. </author> <title> Learning stochastic functions by smooth simultaneous estimation. </title> <booktitle> In Proceedings of the 5th Annual ACM Workshop on Computational Learning Theory, </booktitle> <pages> pages 272-279. </pages> <publisher> ACM Press, </publisher> <year> 1992. </year>
Reference-contexts: Clearly, the algorithm needs to see O *d 1 log 2 1 + ff 2 log ffiff examples, completing the proof of (11). The bound (12) can be proved analogously using Lemma 10 in place of Lemma 11. Buescher and Kumar proposed a related algorithm in <ref> [7] </ref>. Their algorithm (the "canonical estima tor") splits a sequence of labelled examples into two parts. Let ~ be the sequence of points from X in the first part of the sample.
Reference: [8] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82 </volume> <pages> 247-261, </pages> <year> 1989. </year>
Reference-contexts: The following theorem shows that Theorem 1 cannot be improved in general by more than a constant factor, and that the constant on the fl term is best possible. The proof uses techniques due to Ehrenfeucht, Haussler, Kearns and Valiant <ref> [8] </ref>, Haussler, Littlestone and Warmuth [13], and Simon [18].
Reference: [9] <author> L. Gurvits and P. Koiran. </author> <title> Approximation and learning of convex superpositions. </title> <booktitle> In Computational Learning Theory: </booktitle> <address> EUROCOLT'95, </address> <year> 1995. </year>
Reference-contexts: Define the loss function class L F = f (x; y) 7! jf (x) yj : f 2 F g : Then for any * &gt; 0, N 4 It is also possible to relate the fat-shattering functions of these classes directly using Sauer's lemma (see <ref> [9] </ref>), but the proof is not as simple and the result slightly weaker. 15 Theorem 20 Choose a set X, a set F of functions from X to [0; 1], and *; ffi &gt; 0.
Reference: [10] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year> <title> 24 the finiteness of fat and fatV at certain scales and learnability and uniform convergence. A point on one of the number lines corresponding to fat or fatV at position fl on the line represents the statement "fat F (fl) (respectively fatV F (fl)) is finite". The ellipses on the right have the obvious interpretation. An arrow indicates an implication, a crossed out arrow indicates that no such implication exists. </title> <type> 25 </type>
Reference-contexts: We apply the above prediction bound, together with ideas due to Haussler [11] and Benedek and Itai [5], to obtain new bounds on packing numbers in terms of fatV. In agnostic learning <ref> [10, 15] </ref>, a distribution on X fi [0; 1] is unknown, and the learner, given examples drawn according to this distribution, tries to find a function h from X to [0; 1] so that, with probability at least 1 ffi, the expectation of jh (x) yj is at most * larger <p> We combine our new packing bound with the techniques of another paper of Haussler <ref> [10] </ref> to prove an upper bound 1 of O 1 fatV F (*=5) * 1 + log ffi on the sample complexity of (*; ffi)-agnostic learning F .
Reference: [11] <author> D. Haussler. </author> <title> Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik--Chervonenkis dimension. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 69(2) </volume> <pages> 217-232, </pages> <year> 1995. </year>
Reference-contexts: A packing number for a class of functions measures, in a certain sense, the largest number of significantly different behaviors functions in the class can have on a set of points of a given size. We apply the above prediction bound, together with ideas due to Haussler <ref> [11] </ref> and Benedek and Itai [5], to obtain new bounds on packing numbers in terms of fatV. <p> One uses fatV, and is proved using Theorem 1, together with techniques from <ref> [11, 5] </ref>. The second bound uses fat, and is proved through a refinement of a proof in [1]. <p> We apply these in the following. In addition to Theorem 1, the proof uses ideas due to Haussler <ref> [11] </ref> and Benedek and Itai [5]. 9 Lemma 10 Choose 0 &lt; * &lt; 1, b 2 N and 0 &lt; ff &lt; *=4. Let B = Q 1=b ([0; 1]). Choose m 2 N, and let S B m . Set d = fatV S (*=2 ff).
Reference: [12] <author> D. Haussler, M. Kearns, N. Littlestone, and M. K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <journal> Inform. Comput., </journal> <volume> 95(2) </volume> <pages> 129-161, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: : ; x 2k1 ; fi); we have P 2k1 ((x 1 ; y 1 ); : : : ; (x 2k1 ; y 2k1 )) : er P (h) inf er P (f ) &gt; * fl &lt; 1 fl=*: To complete the proof, we use a technique from <ref> [12] </ref> to convert this prediction strategy to an agnostic learning algorithm.
Reference: [13] <author> D. Haussler, N. Littlestone, and M. K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <journal> Information and Computation, </journal> <volume> 115(2) </volume> <pages> 129-161, </pages> <year> 1994. </year>
Reference-contexts: The learner is penalized by j^y f (x)j. This can be viewed as a model of on-line learning, and is the straightforward generalization of the prediction model of Haussler, Littlestone and Warmuth <ref> [13] </ref> to real-valued functions. In this paper, we begin by introducing a new general-purpose prediction strategy that uses a binary search to divide the problem of real-valued prediction into a number of binary-valued prediction problems. <p> That is, L (F; m) is the worst-case expected error of the best prediction strategy. This is a generalization of the f0; 1g prediction model of <ref> [13] </ref> to [0; 1]-valued functions. 2.2 Definitions for the agnostic learning model Define a learner for a set X to be a mapping from [ n2N (X fi [0; 1]) n to [0; 1] X , i.e. to take a sequence of labelled examples, and output a hypothesis. <p> 1g is maxfd : 9x 1 ; :::; x d 2 X; f0; 1g d f (g (x 1 ); :::; g (x d )) : g 2 Ggg: First, we will make use of the following well-known lemma, whose application is usually referred to as the "permutation trick" (see <ref> [13] </ref>). It formalizes the idea that, when m points are chosen independently at random, then any permutation of a certain sequence of points is equally likely to have been chosen. Lemma 2 Choose m 2 N, a distribution D on X, and a random variable defined on X m . <p> The following theorem shows that Theorem 1 cannot be improved in general by more than a constant factor, and that the constant on the fl term is best possible. The proof uses techniques due to Ehrenfeucht, Haussler, Kearns and Valiant [8], Haussler, Littlestone and Warmuth <ref> [13] </ref>, and Simon [18]. <p> Then the expectation of j^y m f b (u m )j is at least 1=2 times the probability that u m 62 fu 1 ; :::; u m1 g. This probability has been shown to be (d=m) <ref> [13] </ref>, and therefore, the expectation of j^y m f b (u m )j over the random choice of the u i 's and b is (d=m), which implies there exists b such that for that fixed b, the expectation of j^y m f b (u m )j only over the random <p> 2 X m , and f : X ! [0; 1], define sam (x; f ) = ((x 1 ; f (x 1 )); :::; (x m ; f (x m ))): We will also make use of the following, which is implicit in the work of Haussler, et al <ref> [13] </ref>. Lemma 9 Choose X, F [0; 1] X .
Reference: [14] <author> M. J. Kearns and R. E. Schapire. </author> <title> Efficient distribution-free learning of probabilistic concepts. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 464-497, </pages> <year> 1994. </year>
Reference-contexts: We give bounds on the expected error of this strategy in terms of fatV, the scale-sensitive generalization of the Vapnik dimension introduced by Alon, Ben-David, Cesa-Bianchi and Haussler [1] (which is similar to a notion introduced by Kearns and Schapire <ref> [14] </ref>), and show that no algorithm can improve on these bounds in general by more than a constant factor. <p> ); :::; f (x n )) : f 2 F g: We define fat F (fl) to be the maximum, over all finite sequences ~ of elements of X, of fat F j ~ (fl). (This was called the fat-shattering function in [4], and was defined by Kearns and Schapire <ref> [14] </ref>.) 3 2.7 Definitions relating to fatV For each r 2 [0; 1] and * &gt; 0, define r;* : [0; 1] ! f0; ?; 1g by r;* (y) = &gt; &lt; 1 if y r + * 0 if y r *.
Reference: [15] <author> M. J. Kearns, R. E. Schapire, and L. M. Sellie. </author> <title> Toward efficient agnostic learning. </title> <journal> Machine Learning, </journal> <volume> 17 </volume> <pages> 115-141, </pages> <year> 1994. </year>
Reference-contexts: We apply the above prediction bound, together with ideas due to Haussler [11] and Benedek and Itai [5], to obtain new bounds on packing numbers in terms of fatV. In agnostic learning <ref> [10, 15] </ref>, a distribution on X fi [0; 1] is unknown, and the learner, given examples drawn according to this distribution, tries to find a function h from X to [0; 1] so that, with probability at least 1 ffi, the expectation of jh (x) yj is at most * larger
Reference: [16] <author> B.K. Natarajan. </author> <title> Occam's razor for functions. </title> <booktitle> In Proceedings of the 1993 ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 370-376. </pages> <publisher> ACM Press, </publisher> <year> 1993. </year>
Reference-contexts: In this case, we need to show that a class of associated loss functions is an *-uniform GC class, and to do this we relate covering numbers of the loss function class to covering numbers of the function class. This lemma is implicit in the analysis of Natarajan <ref> [16] </ref>. 4 Lemma 19 Suppose that X is a set, F is a class of functions that map from X to [0; 1], x = (x 1 ; : : : ; x m ) 2 X m , and z = ((x 1 ; y 1 ); : : :;
Reference: [17] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer, </publisher> <address> New York, </address> <year> 1984. </year>
Reference-contexts: The reader may assume that X is countable, but significantly weaker assumptions, like those of Pollard's <ref> [17] </ref> Appendix C, suffice. 2 2.3 Definition of *-uniform GC-classes For *; ffi &gt; 0, a set X and a set F of functions from X to [0; 1], if D X is the set of all probability distributions over X, define m GC;F (*; ffi) = min n : 8m <p> The first is Hoeffding's inequality (see <ref> [17] </ref>, Appendix B). Lemma 13 Choose a &lt; b, X. Let D be a probability distribution on X, and let f 1 ; :::; f m be independent random variables taking values in [a; b]. <p> The following is a restatement of Theorem 8 of Chapter II of <ref> [17] </ref>. Lemma 14 ([17]) Suppose X and U are sets, D is a probability distribution on X, and : X fi U ! [0; 1] and : X fi U ! [0; 1] are functions for which (; u 1 ) and (; u 2 ) are independent random variables for
Reference: [18] <author> H. U. Simon. </author> <title> Bounds on the number of examples needed for learning functions. In Computational Learning Theory: </title> <publisher> EUROCOLT'93. Oxford University Press, </publisher> <year> 1994. </year> <month> 26 </month>
Reference-contexts: The following theorem shows that Theorem 1 cannot be improved in general by more than a constant factor, and that the constant on the fl term is best possible. The proof uses techniques due to Ehrenfeucht, Haussler, Kearns and Valiant [8], Haussler, Littlestone and Warmuth [13], and Simon <ref> [18] </ref>. <p> Next, we turn to proving a necessary condition for *-agnostic learnability. The following variant of fat F , due to Simon <ref> [18] </ref>, will be useful. <p> We then define sfat F (fl) to be the length of the longest sequence that is strongly fl-fatly shattered by F , or 1 is there is no longest sequence. The following lemma, whose proof closely follows parts of that of a related result in <ref> [18] </ref>, as well as Theorem 5, will be useful. Lemma 25 Choose X, F [0; 1] X , * &gt; 0. Then if there exists ff &gt; 0 such that sfat F (* + ff) is infinite, then F is not *-agnostically learnable.
References-found: 18

