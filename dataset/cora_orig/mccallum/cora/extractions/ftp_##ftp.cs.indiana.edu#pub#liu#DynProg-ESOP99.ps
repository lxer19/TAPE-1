URL: ftp://ftp.cs.indiana.edu/pub/liu/DynProg-ESOP99.ps
Refering-URL: http://www.cs.indiana.edu/hyplan/liu.html
Root-URL: http://www.cs.indiana.edu
Email: fliu,stollerg@cs.indiana.edu  
Title: Dynamic Programming via Static  
Author: Incrementalization Yanhong A. Liu and Scott D. Stoller 
Address: Bloomington, IN 47405  
Affiliation: Computer Science Department, Indiana University,  
Abstract: Dynamic programming is an important algorithm design technique. It is used for solving problems whose solutions involve recursively solving subproblems that share subsubproblems. While a straightforward recursive program solves common subsubproblems repeatedly and often takes exponential time, a dynamic programming algorithm solves every subsubproblem just once, saves the result, reuses it when the sub-subproblem is encountered again, and takes polynomial time. This paper describes a systematic method for transforming programs written as straightforward recursions into programs that use dynamic programming. The method extends the original program to cache all possibly computed values, incrementalizes the extended program with respect to an input increment to use and maintain all cached results, prunes out cached results that are not used in the incremental computation, and uses the resulting incremental program to form an optimized new program. In-crementalization statically exploits semantics of both control structures and data structures and maintains as invariants equalities characterizing cached results. The principle underlying incrementalization is general for achieving drastic program speedups. Compared with previous methods that perform memoization or tabulation, the method based on incremen-talization is more powerful and systematic. It has been implemented and applied to numerous problems and succeeded on all of them. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> M. Abadi, B. Lampson, and J.-J. Levy. </author> <title> Analysis and caching of dependencies. </title> <booktitle> In Proceedings of the 1996 ACM SIGPLAN International Conference on Functional Programming. ACM, </booktitle> <address> New York, </address> <month> May </month> <year> 1996. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 2. <author> A. V. Aho, J. E. Hopcroft, and J. D. Ullman. </author> <title> The Design and Analysis of Computer Algorithms. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, Mass., </address> <year> 1974. </year>
Reference-contexts: 1 Introduction Dynamic programming is an important technique for designing efficient algorithms <ref> [2, 46, 14] </ref>. It is used for problems whose solutions involve recursively solving subproblems that overlap. While a straightforward recursive program solves common subproblems repeatedly, a dynamic programming algorithm solves every subproblem just once, saves the result in a table, and reuses the result when the subproblem is encountered again. <p> Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in <ref> [2, 46, 14] </ref>. Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions [46, 14]. <p> n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence [14] p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees <ref> [2] </ref> p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O (nfl (2flsize +1) n ) O (n 3 fl size) Fig. <p> m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees <ref> [2] </ref> p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O (nfl (2flsize +1) n ) O (n 3 fl size) Fig. 3. Summary of Examples. 8 Related work and conclusion Dynamic programming was first formulated by Bellman [4] and has been studied extensively since [51].
Reference: 3. <author> F. L. Bauer, B. Moller, H. Partsch, and P. Pepper. </author> <title> Formal program construction by transformations|Computer-aided, </title> <journal> intuition-guided programming. IEEE Trans. Softw. Eng., </journal> <volume> 15(2) </volume> <pages> 165-180, </pages> <month> Feb. </month> <year> 1989. </year>
Reference: 4. <author> R. E. Bellman. </author> <title> Dynamic Programming. </title> <publisher> Princeton University Press, </publisher> <address> Princeton, New Jersey, </address> <year> 1957. </year>
Reference-contexts: This can reduce the time complexity from exponential to polynomial. The technique is generally applicable to all problems whose efficient solutions involve memoizing results of subproblems <ref> [4, 5] </ref>. Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. <p> Summary of Examples. 8 Related work and conclusion Dynamic programming was first formulated by Bellman <ref> [4] </ref> and has been studied extensively since [51]. Bird [5], de Moor [16], and others have studied it in the context of program transformation. While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation.
Reference: 5. <author> R. S. Bird. </author> <title> Tabulation techniques for recursive programs. </title> <journal> ACM Comput. Surv., </journal> <volume> 12(4) </volume> <pages> 403-417, </pages> <month> Dec. </month> <year> 1980. </year>
Reference-contexts: This can reduce the time complexity from exponential to polynomial. The technique is generally applicable to all problems whose efficient solutions involve memoizing results of subproblems <ref> [4, 5] </ref>. Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. <p> This can reduce the time complexity from exponential to polynomial. The technique is generally applicable to all problems whose efficient solutions involve memoizing results of subproblems [4, 5]. Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation <ref> [5] </ref>. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1]. <p> Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> Summary of Examples. 8 Related work and conclusion Dynamic programming was first formulated by Bellman [4] and has been studied extensively since [51]. Bird <ref> [5] </ref>, de Moor [16], and others have studied it in the context of program transformation. While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation.
Reference: 6. <author> R. S. Bird. </author> <title> The promotion and accumulation strategies in transformational pro-gramming. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 6(4) </volume> <pages> 487-504, </pages> <month> Oct. </month> <year> 1984. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. <p> running time optimized prog's running time Fibonacci function [39] O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence [14] p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence <ref> [6] </ref> p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O
Reference: 7. <author> R. S. Bird and O. de Moor. </author> <title> From dynamic programming to greedy algorithms. </title> <editor> In B. Moller, H. Partsch, and S. Schuman, editors, </editor> <booktitle> Formal Program Development, volume 755 of Lecture Notes in Computer Science, </booktitle> <pages> pages 43-61. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1993. </year>
Reference-contexts: If so, we can use time and space analysis [29] to determine whether it is worthwhile to use and maintain such information. Many dynamic programming algorithms can be further improved by exploiting additional properties of the given problems <ref> [7] </ref>, e.g., greedy properties. Our method is not specially aimed at discovering such properties. Nevertheless, it can maintain such properties once they are added.
Reference: 8. <author> E. A. Boiten. </author> <title> Improving recursive functions by inverting the order of evaluation. </title> <journal> Sci. Comput. Program., </journal> <volume> 18(2) </volume> <pages> 139-179, </pages> <month> Apr. </month> <year> 1992. </year>
Reference: 9. <author> R. M. Burstall and J. Darlington. </author> <title> A transformation system for developing recursive programs. </title> <journal> J. ACM, </journal> <volume> 24(1) </volume> <pages> 44-67, </pages> <month> Jan. </month> <year> 1977. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET.
Reference: 10. <author> W.-N. Chin. </author> <title> Towards an automated tupling strategy. </title> <booktitle> In Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation, </booktitle> <pages> pages 119-132. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1993. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. <p> Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. In particular, tupling [40, 41] aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems <ref> [10] </ref> and to work on more general forms [12]. It is also extended to store lists of values [42], but such lists are generated in a fixed way, which is not the most appropriate way for many programs.
Reference: 11. <author> W.-N. Chin and M. Hagiya. </author> <title> A bounds inference method for vector-based memoization. </title> <booktitle> In ICFP 1997 [23], </booktitle> <pages> pages 176-187. </pages>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> A special form of tupling can eliminate multiple data traversals for many functions [21]. A method specialized for introducing arrays was proposed for tabulation <ref> [11] </ref>, but as our method has shown, array is not 2 Matrix-chain multiplication, optimal binary search trees, optimal polygon triangulation, and other problems not in Fig. 3 have similar control structures for recursive calls.
Reference: 12. <author> W.-N. Chin and S.-C. Khoo. </author> <title> Tupling functions with multiple recursion parameters. </title> <editor> In P. Cousot, M. Falaschi, G. File, and A. Rauzy, editors, </editor> <booktitle> Proceedings of the 3rd International Workshop on Static Analysis, volume 724 of Lecture Notes in Computer Science, </booktitle> <pages> pages 124-140. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Sept. </month> <year> 1993. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. <p> In particular, tupling [40, 41] aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms <ref> [12] </ref>. It is also extended to store lists of values [42], but such lists are generated in a fixed way, which is not the most appropriate way for many programs. A special form of tupling can eliminate multiple data traversals for many functions [21].
Reference: 13. <author> N. H. Cohen. </author> <title> Eliminating redundant recursive calls. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 5(3) </volume> <pages> 265-299, </pages> <month> July </month> <year> 1983. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations.
Reference: 14. <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> The MIT Press/McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Dynamic programming is an important technique for designing efficient algorithms <ref> [2, 46, 14] </ref>. It is used for problems whose solutions involve recursively solving subproblems that overlap. While a straightforward recursive program solves common subproblems repeatedly, a dynamic programming algorithm solves every subproblem just once, saves the result in a table, and reuses the result when the subproblem is encountered again. <p> This can reduce the time complexity from exponential to polynomial. The technique is generally applicable to all problems whose efficient solutions involve memoizing results of subproblems [4, 5]. Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming <ref> [14] </ref>: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1]. <p> Second, strategies for table filling and lookup can be specialized to be efficient for particular problems. However, tabulation has two drawbacks. First, it usually requires a thorough understanding of the problem and a complete manual rewrite of the program <ref> [14] </ref>. Second, to statically ensure that all values possibly needed are computed and stored, a table that is larger than necessary is often used; it may also include solutions to subproblems not actually needed in the original computation. <p> Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in <ref> [2, 46, 14] </ref>. Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions [46, 14]. <p> The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions <ref> [46, 14] </ref>. For example, the matrix-chain-multiplication problem [14, pages 302-314] computes the minimum number of scalar multiplications needed by any parenthesization in multiplying a chain of n matrices, where matrix i has dimensions p i1 fi p i . <p> Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions [46, 14]. For example, the matrix-chain-multiplication problem <ref> [14, pages 302-314] </ref> computes the minimum number of scalar multiplications needed by any parenthesization in multiplying a chain of n matrices, where matrix i has dimensions p i1 fi p i . <p> scalar multiplications for multiplying matrices i through j and can be defined as: for i j, m (i; j) = 0 if i = j min ikj1 fm (i; k) + m (k + 1; j) + p i1 fl p k fl p j g otherwise The longest-common-subsequence problem <ref> [14, pages 314-320] </ref> computes the length c (n; m) of the longest common subsequence of two sequences hx 1 ; x 2 ; :::; x n i and hy 1 ; y 2 ; :::; y m i, where c (i; j) can be defined as: for i; j 0, c <p> c (i; j) can be defined as: for i; j 0, c (i; j) = &lt; 0 if i = 0 or j = 0 max (c (i; j 1); c (i 1; j)) otherwise Both of these examples are literally copied from the textbook by Cormen, Leis-erson, and Rivest <ref> [14] </ref>. These recursive functions can be written straightforwardly in the following first-order, call-by-value functional programming language. <p> For the longest-common-subsequence example, only a linear list is needed, whereas in standard textbooks, a quadratic two-dimensional array is used, and an additional optimization is needed to reduce it to a one-dimensional array <ref> [14] </ref>. For the matrix-chain-multiplication example, our optimized program uses a list of lists that forms a triangle shape, rather than a two-dimensional array of square shape. It's nontrivial to see that recursive data structures gives the same asymptotic speedup as arrays for these examples. <p> Many dynamic programming algorithms can be further improved by exploiting additional properties of the given problems [7], e.g., greedy properties. Our method is not specially aimed at discovering such properties. Nevertheless, it can maintain such properties once they are added. For example, for the paragraph-formatting problem <ref> [14, 17] </ref>, we can derive a quadratic-time algorithm that uses dynamic programming; if the original program has a simple extra conditional that follows from a greedy property, our derived dynamic programming program uses it as well and takes linear time with a factor of line width. <p> Performance measurements confirmed drastic speedups. Examples multiple cache arg aux info original program's running time optimized prog's running time Fibonacci function [39] O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence <ref> [14] </ref> p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl <p> Performance measurements confirmed drastic speedups. Examples multiple cache arg aux info original program's running time optimized prog's running time Fibonacci function [39] O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence <ref> [14] </ref> p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] <p> time Fibonacci function [39] O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence <ref> [14] </ref> p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O (nfl (2flsize +1) n ) <p> binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence <ref> [14] </ref> p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O (nfl (2flsize +1) n ) O (n 3 fl size) Fig. 3. <p> <ref> [14] </ref> p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n ) O (n fl weight) context-free-grammar parsing [2] p p a O (nfl (2flsize +1) n ) O (n 3 fl size) Fig. 3.
Reference: 15. <author> S. Curtis. </author> <title> Dynamic programming: A different perspective. </title> <editor> In R. Bird and L. Meertens, editors, </editor> <booktitle> Algorithmic Languages and Calculi, </booktitle> <pages> pages 1-23. </pages> <publisher> Chapman & Hall, </publisher> <address> London, U.K., </address> <year> 1997. </year>
Reference: 16. <author> O. de Moor. </author> <title> A generic program for sequential decision processes. </title> <editor> In M. Hermenegildo and D. S. Swierstra, editors, </editor> <booktitle> Programming Languages: Implementations, Logics, and Programs, volume 982 of Lecture Notes in Computer Science, </booktitle> <pages> pages 1-23. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1995. </year>
Reference-contexts: Summary of Examples. 8 Related work and conclusion Dynamic programming was first formulated by Bellman [4] and has been studied extensively since [51]. Bird [5], de Moor <ref> [16] </ref>, and others have studied it in the context of program transformation. While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation.
Reference: 17. <author> O. de Moor and J. Gibbons. </author> <title> Bridging the algorithm gap: A linear-time functional program for paragraph formatting. </title> <type> Technical Report CMS-TR-97-03, </type> <institution> School of Computing and Mathematical Sciences, Oxford Brookes University, </institution> <month> July </month> <year> 1997. </year>
Reference-contexts: Many dynamic programming algorithms can be further improved by exploiting additional properties of the given problems [7], e.g., greedy properties. Our method is not specially aimed at discovering such properties. Nevertheless, it can maintain such properties once they are added. For example, for the paragraph-formatting problem <ref> [14, 17] </ref>, we can derive a quadratic-time algorithm that uses dynamic programming; if the original program has a simple extra conditional that follows from a greedy property, our derived dynamic programming program uses it as well and takes linear time with a factor of line width. <p> The third column shows whether the incremental program computes values not necessarily computed by the original program. Paragraph formatting 2 <ref> [17] </ref> includes a conditional that reflects a greedy property. The "a" in the third column for the last two examples shows that cached values are stored in arrays. Performance measurements confirmed drastic speedups.
Reference: 18. <author> J. Field and T. Teitelbaum. </author> <title> Incremental reduction in the lambda calculus. </title> <booktitle> In Proceedings of the 1990 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 307-322. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> June </month> <year> 1990. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 19. <author> D. P. Friedman, D. S. Wise, and M. Wand. </author> <title> Recursive programming through table look-up. </title> <booktitle> In Proceedings of the 1976 ACM Symposium on Symbolic and Algebraic Computation, </booktitle> <pages> pages 85-89. </pages> <publisher> ACM, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 20. <author> Y. Futamura and K. Nogi. </author> <title> Generalized partial evaluation. </title> <editor> In B. Bjorner, A. P. Ershov, and N. D. Jones, editors, </editor> <booktitle> Partial Evaluation and Mixed Computation, </booktitle> <pages> pages 133-151. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1988. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET.
Reference: 21. <author> Z. Hu, H. Iwasaki, M. Takeichi, and A. Takano. </author> <title> Tupling calculation eliminates multiple data traversals. </title> <booktitle> In ICFP 1997 [23], </booktitle> <pages> pages 164-175. </pages>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. <p> It is also extended to store lists of values [42], but such lists are generated in a fixed way, which is not the most appropriate way for many programs. A special form of tupling can eliminate multiple data traversals for many functions <ref> [21] </ref>. A method specialized for introducing arrays was proposed for tabulation [11], but as our method has shown, array is not 2 Matrix-chain multiplication, optimal binary search trees, optimal polygon triangulation, and other problems not in Fig. 3 have similar control structures for recursive calls.
Reference: 22. <author> J. Hughes. </author> <title> Lazy memo-functions. </title> <booktitle> In Proceedings of the 2nd Conference on Functional Programming Languages and Computer Architecture, volume 201 of Lecture Notes in Computer Science, </booktitle> <pages> pages 129-146. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> Sept. </month> <year> 1985. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 23. <institution> Proceedings of the 1997 ACM SIGPLAN International Conference on Functional Programming. ACM, </institution> <address> New York, </address> <month> June </month> <year> 1997. </year>
Reference: 24. <author> R. M. Keller and M. R. Sleep. </author> <title> Applicative caching. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 8(1) </volume> <pages> 88-108, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 25. <author> H. Khoshnevisan. </author> <title> Efficient memo-table management strategies. </title> <journal> Acta Informatica, </journal> <volume> 28(1) </volume> <pages> 43-81, </pages> <year> 1990. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 26. <author> Y. A. Liu. CACHET: </author> <title> An interactive, incremental-attribution-based program transformation system for deriving incremental programs. </title> <booktitle> In Proceedings of the 10th Knowledge-Based Software Engineering Conference, </booktitle> <pages> pages 19-26. </pages> <publisher> IEEE CS Press, Los Alamitos, </publisher> <address> Calif., </address> <month> Nov. </month> <year> 1995. </year>
Reference-contexts: How to systematically discover and use these additional properties is a subject for future study. 7 Implementation and experimentation results All three steps have been implemented in a prototype system, CACHET. The incrementalization step as currently implemented is semi-automatic <ref> [26] </ref> and is being automated. The implementation uses the Synthesizer Generator [47]. Fig. 3 summarizes some of the examples derived (most of them semi-automa-tically and some automatically) and compares their asymptotic running times. 2 The second column shows whether more than one cache argument is needed in an incremental program.
Reference: 27. <author> Y. A. Liu. </author> <title> Principled strength reduction. </title> <editor> In R. Bird and L. Meertens, editors, </editor> <booktitle> Algorithmic Languages and Calculi, </booktitle> <pages> pages 357-381. </pages> <publisher> Chapman & Hall, </publisher> <address> London, U.K., </address> <year> 1997. </year>
Reference-contexts: Our method is based on static analyses and transformations studied previously by others [52, 9, 48, 6, 36, 20, 49, 41] and ourselves <ref> [33, 32, 31, 27, 32] </ref> and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. <p> Such situations become evident when doing incrementalization and can be taken care of easily. This will be described in a future paper. Although we present the optimizations for a functional language, the underlying principle is general and has been applied to programs that use loops and arrays <ref> [27, 30] </ref>. Some values computed in a hoisted program might not be computed by the original program and are therefore called auxiliary information [31]. <p> Overall, being able to incrementalize complicated recursion in a systematic way is a more drastic improvement complementing previous methods for incre-mentalizing loops <ref> [38, 27] </ref>. Our new method based on static incrementalization is general and fully automatable. Based on our existing implementation, we believe that a complete system will perform incrementalization efficiently.
Reference: 28. <author> Y. A. Liu. </author> <title> Dependence analysis for recursive data. </title> <booktitle> In Proceedings of the IEEE 1998 International Conference on Computer Languages, </booktitle> <pages> pages 206-215. </pages> <publisher> IEEE CS Press, Los Alamitos, </publisher> <address> Calif., </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Pruning. Pruning requires a dependence analysis that can precisely describe substructures of recursive trees [32]. We use an analysis method based on regular tree grammars <ref> [28] </ref>. We have implemented a simplified version that uses set constraints to efficiently produce precise analysis results. Pruning can save space, as well as time, and reduce code size. For example, in program c 0 , only the third component of r is useful. <p> This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning <ref> [28] </ref>. Finite differencing [38, 37] is based on the same underlying principle as incremental computation. Paige has explicitly asked whether finite differencing can be generalized to handle dynamic programming [36]; it is clear that he perceived an important connection.
Reference: 29. <author> Y. A. Liu and G. Gomez. </author> <title> Automatic accurate time-bound analysis for high-level languages. </title> <booktitle> In Proceedings of the ACM SIGPLAN 1998 Workshop on Languages, Compilers, and Tools for Embedded Systems, volume 1474 of Lecture Notes in Computer Science, </booktitle> <pages> pages 31-40. </pages> <publisher> Springer-Verlag, </publisher> <month> June </month> <year> 1998. </year>
Reference-contexts: We can determine statically whether such information is cached in the final program. If so, we can use time and space analysis <ref> [29] </ref> to determine whether it is worthwhile to use and maintain such information. Many dynamic programming algorithms can be further improved by exploiting additional properties of the given problems [7], e.g., greedy properties. Our method is not specially aimed at discovering such properties.
Reference: 30. <author> Y. A. Liu and S. D. Stoller. </author> <title> Loop optimization for aggregate array computations. </title> <booktitle> In Proceedings of the IEEE 1998 International Conference on Computer Languages, </booktitle> <pages> pages 262-271. </pages> <publisher> IEEE CS Press, Los Alamitos, </publisher> <address> Calif., </address> <month> May </month> <year> 1998. </year>
Reference-contexts: Such situations become evident when doing incrementalization and can be taken care of easily. This will be described in a future paper. Although we present the optimizations for a functional language, the underlying principle is general and has been applied to programs that use loops and arrays <ref> [27, 30] </ref>. Some values computed in a hoisted program might not be computed by the original program and are therefore called auxiliary information [31].
Reference: 31. <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Discovering auxiliary information for incremental computation. </title> <booktitle> In Conference Record of the 23rd Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 157-170. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1996. </year>
Reference-contexts: Our method is based on static analyses and transformations studied previously by others [52, 9, 48, 6, 36, 20, 49, 41] and ourselves <ref> [33, 32, 31, 27, 32] </ref> and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. <p> Fig. 1 gives programs for the examples above. Invariants about an input are not part of a program but are written explicitly to be used by the transformations. These examples do not use data constructors, but our previous papers contain a number of examples that use them <ref> [33, 32, 31] </ref> and our method handles them. These straightforward programs repeatedly solve common subproblems and take exponential time. We transform them into dynamic programming algorithms that perform efficient caching and take polynomial time. We use an asymptotic cost model for measuring time complexity. <p> In particular, it is based on a general approach for program optimization|incrementalization. Although our static incrementalization allows only one incremental version for each original function, it is still powerful enough to incrementalize all examples in <ref> [33, 32, 31] </ref>, including various list manipulations, matrix computations, attribute evaluation, and graph problems. We believe that our method can perform dynamic programming for all problems whose solutions involve recursively solving subproblems that overlap, but a formal justification awaits more rigorous study. <p> Although we present the optimizations for a functional language, the underlying principle is general and has been applied to programs that use loops and arrays [27, 30]. Some values computed in a hoisted program might not be computed by the original program and are therefore called auxiliary information <ref> [31] </ref>. Both incrementalization and pruning produce programs that are as least as fast as the given program, but caching auxiliary information may result in a slower program on certain inputs. We can determine statically whether such information is cached in the final program. <p> Our work is based on the general principle of incrementalization <ref> [38, 31] </ref> and consists of precise program analyses and transformations. In particular, tupling [40, 41] aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms [12]. <p> Yet, it is nontrivial for an automated system to handle all of them uniformly. essential for the speedup of many programs; their arrays are complicated to derive and often consume more space than necessary. Compared with our previous work for incrementalizing functional programs <ref> [33, 32, 31] </ref>, this work contains drastic improvements. First, our previous work address the systematic derivation of an incremental program f 0 given both program f and operation . <p> Finally, based on the idea of cache-and-prune that was proposed earlier [32], the method in this paper uses hoisting to extend the set of intermediate results [32] to include a kind of auxiliary information <ref> [31] </ref> that is sufficient for dynamic programming. This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning [28]. Finite differencing [38, 37] is based on the same underlying principle as incremental computation. <p> idea of cache-and-prune that was proposed earlier [32], the method in this paper uses hoisting to extend the set of intermediate results [32] to include a kind of auxiliary information <ref> [31] </ref> that is sufficient for dynamic programming. This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning [28]. Finite differencing [38, 37] is based on the same underlying principle as incremental computation.
Reference: 32. <author> Y. A. Liu, S. D. Stoller, and T. Teitelbaum. </author> <title> Static caching for incremental com putation. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 20(3) </volume> <pages> 546-585, </pages> <month> May </month> <year> 1998. </year>
Reference-contexts: Our method is based on static analyses and transformations studied previously by others [52, 9, 48, 6, 36, 20, 49, 41] and ourselves <ref> [33, 32, 31, 27, 32] </ref> and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. <p> Fig. 1 gives programs for the examples above. Invariants about an input are not part of a program but are written explicitly to be used by the transformations. These examples do not use data constructors, but our previous papers contain a number of examples that use them <ref> [33, 32, 31] </ref> and our method handles them. These straightforward programs repeatedly solve common subproblems and take exponential time. We transform them into dynamic programming algorithms that perform efficient caching and take polynomial time. We use an asymptotic cost model for measuring time complexity. <p> The first problem is discussed in Section 6. Extension transformation. For each hoisted function definition f (v 1 ; :::; v n ) , e, we construct a function definition f (v 1 ; :::; v n ) , Ext [[e]] (1) where Ext [[e]], defined in <ref> [32] </ref>, extends an expression e to return a nested tuple that contains the values of all function calls made in computing e, i.e., it examines subexpressions of e in applicative order, introduces bindings that name the results of function calls, builds up tuples of these values together with the values of <p> Finally, we form an optimized program that computes f 0 by using the base cases in ^ f 0 and by repeatedly using the incremental version ^ f 0 0 . Pruning. Pruning requires a dependence analysis that can precisely describe substructures of recursive trees <ref> [32] </ref>. We use an analysis method based on regular tree grammars [28]. We have implemented a simplified version that uses set constraints to efficiently produce precise analysis results. Pruning can save space, as well as time, and reduce code size. <p> In particular, it is based on a general approach for program optimization|incrementalization. Although our static incrementalization allows only one incremental version for each original function, it is still powerful enough to incrementalize all examples in <ref> [33, 32, 31] </ref>, including various list manipulations, matrix computations, attribute evaluation, and graph problems. We believe that our method can perform dynamic programming for all problems whose solutions involve recursively solving subproblems that overlap, but a formal justification awaits more rigorous study. <p> Yet, it is nontrivial for an automated system to handle all of them uniformly. essential for the speedup of many programs; their arrays are complicated to derive and often consume more space than necessary. Compared with our previous work for incrementalizing functional programs <ref> [33, 32, 31] </ref>, this work contains drastic improvements. First, our previous work address the systematic derivation of an incremental program f 0 given both program f and operation . <p> Finally, based on the idea of cache-and-prune that was proposed earlier <ref> [32] </ref>, the method in this paper uses hoisting to extend the set of intermediate results [32] to include a kind of auxiliary information [31] that is sufficient for dynamic programming. This method is simpler than our previous general method for discovering auxiliary information [31]. <p> Finally, based on the idea of cache-and-prune that was proposed earlier <ref> [32] </ref>, the method in this paper uses hoisting to extend the set of intermediate results [32] to include a kind of auxiliary information [31] that is sufficient for dynamic programming. This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning [28].
Reference: 33. <author> Y. A. Liu and T. Teitelbaum. </author> <title> Systematic derivation of incremental programs. </title> <journal> Sci. Comput. Program., </journal> <volume> 24(1) </volume> <pages> 1-39, </pages> <month> Feb. </month> <year> 1995. </year>
Reference-contexts: Our method is based on static analyses and transformations studied previously by others [52, 9, 48, 6, 36, 20, 49, 41] and ourselves <ref> [33, 32, 31, 27, 32] </ref> and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. <p> Fig. 1 gives programs for the examples above. Invariants about an input are not part of a program but are written explicitly to be used by the transformations. These examples do not use data constructors, but our previous papers contain a number of examples that use them <ref> [33, 32, 31] </ref> and our method handles them. These straightforward programs repeatedly solve common subproblems and take exponential time. We transform them into dynamic programming algorithms that perform efficient caching and take polynomial time. We use an asymptotic cost model for measuring time complexity. <p> In particular, it is based on a general approach for program optimization|incrementalization. Although our static incrementalization allows only one incremental version for each original function, it is still powerful enough to incrementalize all examples in <ref> [33, 32, 31] </ref>, including various list manipulations, matrix computations, attribute evaluation, and graph problems. We believe that our method can perform dynamic programming for all problems whose solutions involve recursively solving subproblems that overlap, but a formal justification awaits more rigorous study. <p> Yet, it is nontrivial for an automated system to handle all of them uniformly. essential for the speedup of many programs; their arrays are complicated to derive and often consume more space than necessary. Compared with our previous work for incrementalizing functional programs <ref> [33, 32, 31] </ref>, this work contains drastic improvements. First, our previous work address the systematic derivation of an incremental program f 0 given both program f and operation .
Reference: 34. <author> D. Michie. </author> <title> "memo" functions and machine learning. </title> <journal> Nature, </journal> <volume> 218 </volume> <pages> 19-22, </pages> <month> Apr. </month> <year> 1968. </year>
Reference-contexts: This can reduce the time complexity from exponential to polynomial. The technique is generally applicable to all problems whose efficient solutions involve memoizing results of subproblems [4, 5]. Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization <ref> [34] </ref> and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1]. <p> Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 35. <author> D. J. Mostow and D. Cohen. </author> <title> Automating program speedup by deciding what to cache. </title> <booktitle> In Proceedings of the 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 165-172. </pages> <publisher> Morgan Kaufmann Publishers, </publisher> <address> San Francisco, Calif., </address> <month> Aug. </month> <year> 1985. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 36. <author> R. Paige. </author> <title> Programming with invariants. </title> <journal> IEEE Software, </journal> <pages> pages 56-69, </pages> <month> Jan. </month> <year> 1986. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. <p> Additionally, we now use a more precise and efficient dependence analysis for pruning [28]. Finite differencing [38, 37] is based on the same underlying principle as incremental computation. Paige has explicitly asked whether finite differencing can be generalized to handle dynamic programming <ref> [36] </ref>; it is clear that he perceived an important connection. However, finite differencing has been formulated for set-based languages, while straightforward solutions to dynamic programming problems are usually formulated as recursive functions, so it was difficult to actually establish the connection.
Reference: 37. <author> R. Paige. </author> <title> Symbolic finite differencing|Part I. </title> <booktitle> In Proceedings of the 3rd European Symposium on Programming, volume 432 of Lecture Notes in Computer Science, </booktitle> <pages> pages 36-56. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <month> May </month> <year> 1990. </year>
Reference-contexts: This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning [28]. Finite differencing <ref> [38, 37] </ref> is based on the same underlying principle as incremental computation. Paige has explicitly asked whether finite differencing can be generalized to handle dynamic programming [36]; it is clear that he perceived an important connection.
Reference: 38. <author> R. Paige and S. Koenig. </author> <title> Finite differencing of computable expressions. </title> <journal> ACM Trans. Program. Lang. Syst., </journal> <volume> 4(3) </volume> <pages> 402-454, </pages> <month> July </month> <year> 1982. </year>
Reference-contexts: Our work is based on the general principle of incrementalization <ref> [38, 31] </ref> and consists of precise program analyses and transformations. In particular, tupling [40, 41] aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms [12]. <p> This method is simpler than our previous general method for discovering auxiliary information [31]. Additionally, we now use a more precise and efficient dependence analysis for pruning [28]. Finite differencing <ref> [38, 37] </ref> is based on the same underlying principle as incremental computation. Paige has explicitly asked whether finite differencing can be generalized to handle dynamic programming [36]; it is clear that he perceived an important connection. <p> Overall, being able to incrementalize complicated recursion in a systematic way is a more drastic improvement complementing previous methods for incre-mentalizing loops <ref> [38, 27] </ref>. Our new method based on static incrementalization is general and fully automatable. Based on our existing implementation, we believe that a complete system will perform incrementalization efficiently.
Reference: 39. <author> H. A. Partsch. </author> <title> Specification and Transformation of Programs|A Formal Approach to Software Development. </title> <publisher> Springer-Verlag, </publisher> <address> Berlin, </address> <year> 1990. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>. <p> Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> The "a" in the third column for the last two examples shows that cached values are stored in arrays. Performance measurements confirmed drastic speedups. Examples multiple cache arg aux info original program's running time optimized prog's running time Fibonacci function <ref> [39] </ref> O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence [14] p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary <p> The "a" in the third column for the last two examples shows that cached values are stored in arrays. Performance measurements confirmed drastic speedups. Examples multiple cache arg aux info original program's running time optimized prog's running time Fibonacci function <ref> [39] </ref> O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence [14] p matrix-chain multiplication [14] p string editing distance [46] O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph
Reference: 40. <author> A. Pettorossi. </author> <title> A powerful strategy for deriving efficient programs by transformation. </title> <booktitle> In Conference Record of the 1984 ACM Symposium on LISP and Functional Programming. ACM, </booktitle> <address> New York, </address> <month> Aug. </month> <year> 1984. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. <p> Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. In particular, tupling <ref> [40, 41] </ref> aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms [12].
Reference: 41. <author> A. Pettorossi and M. Proietti. </author> <title> Rules and strategies for transforming functional and logic programs. </title> <journal> ACM Comput. Surv., </journal> <volume> 28(2) </volume> <pages> 360-414, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. <p> Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. In particular, tupling <ref> [40, 41] </ref> aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms [12].
Reference: 42. <author> A. Pettorossi and M. Proietti. </author> <title> Program derivation via list introduction. </title> <editor> In R. Bird and L. Meertens, editors, </editor> <title> Algorithmic Languages and Calculi. </title> <publisher> Chapman & Hall, </publisher> <address> London, U.K., </address> <year> 1997. </year>
Reference-contexts: Tabulation determines what shape of table is needed to store the values of all possibly needed subcomputations, introduces appropriate data structures for the table, and computes the table entries in a bottom-up fashion so that the solution to a superproblem is computed using available solutions to subproblems <ref> [5, 13, 40, 39, 10, 12, 41, 42, 21, 11] </ref>. This overcomes both disadvantages of memoization. First, table filling and lookup are compiled into the resulting program so no separate mechanism is needed for the execution. <p> While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation. Previous methods for this problem either apply to specific subclasses of problems <ref> [13, 40, 10, 12, 42, 21] </ref> or give general frameworks and strategies rather than precise algorithms [52, 9, 5, 48, 6, 3, 39, 49, 8, 16, 41, 15]. Our work is based on the general principle of incrementalization [38, 31] and consists of precise program analyses and transformations. <p> In particular, tupling [40, 41] aims to compute multiple values together in an efficient way. It is improved to be automatic on subclasses of problems [10] and to work on more general forms [12]. It is also extended to store lists of values <ref> [42] </ref>, but such lists are generated in a fixed way, which is not the most appropriate way for many programs. A special form of tupling can eliminate multiple data traversals for many functions [21].
Reference: 43. <author> W. Pugh. </author> <title> An improved cache replacement strategy for function caching. </title> <booktitle> In Proceedings of the 1988 ACM Conference on LISP and Functional Programming, </booktitle> <pages> pages 269-276. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 44. <author> W. Pugh. </author> <title> The Omega Test: A fast and practical integer programming algorithm for dependence analysis. </title> <journal> Commun. ACM, </journal> <volume> 31(8), </volume> <month> Aug. </month> <year> 1992. </year>
Reference-contexts: The simplification here, as well as the manipulations below, can be done automatically using Omega <ref> [44] </ref>. Represent the arguments of recursive calls so that the differences between them and x are explicit. For function c, S c is already in this form, and for function m, S m is rewritten as fhi; j li; hi + l; ji j 1 l j ig. <p> n to eliminate variables in Inv and can 1 In previous papers, we defined f 0 0 slightly differently: if f 0 (x) = r and f 0 (x y) = r 0 , then f 0 0 (x; y; r) = r 0 . be done automatically using Omega <ref> [44] </ref>. Resulting equations relating x 00 1 ; :::; x 00 are used also to duplicate other invariants deduced. If a resulting invariant still uses a variable other than x 00 1 ; :::; x 00 n , discard it.
Reference: 45. <author> W. Pugh and T. Teitelbaum. </author> <title> Incremental computation via function caching. </title> <booktitle> In Conference Record of the 16th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 315-328. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Given a straightforward recursion, there are two traditional ways to achieve the effect of dynamic programming [14]: memoization [34] and tabulation [5]. Memoization uses a mechanism that is separate from the original program to save the result of each function call or reduction <ref> [34, 19, 22, 35, 24, 43, 45, 39, 25, 18, 1] </ref>.
Reference: 46. <author> P. W. Purdom and C. A. Brown. </author> <title> The Analysis of Algorithms. </title> <publisher> Holt, Rinehart and Winston, </publisher> <year> 1985. </year>
Reference-contexts: 1 Introduction Dynamic programming is an important technique for designing efficient algorithms <ref> [2, 46, 14] </ref>. It is used for problems whose solutions involve recursively solving subproblems that overlap. While a straightforward recursive program solves common subproblems repeatedly, a dynamic programming algorithm solves every subproblem just once, saves the result in a table, and reuses the result when the subproblem is encountered again. <p> Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET. The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in <ref> [2, 46, 14] </ref>. Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions [46, 14]. <p> The system has been used to optimize many programs written as straightforward recursions, including all dynamic programming problems found in [2, 46, 14]. Performance measurements confirm drastic asymptotic speedups. 2 Formulating the problem Straightforward solutions to many combinatorics and optimization problems can be written as simple recursions <ref> [46, 14] </ref>. For example, the matrix-chain-multiplication problem [14, pages 302-314] computes the minimum number of scalar multiplications needed by any parenthesization in multiplying a chain of n matrices, where matrix i has dimensions p i1 fi p i . <p> Examples multiple cache arg aux info original program's running time optimized prog's running time Fibonacci function [39] O (2 n ) O (n) binomial coefficients [39] O (2 n ) O (n fl k) longest common subsequence [14] p matrix-chain multiplication [14] p string editing distance <ref> [46] </ref> O (3 n+m ) O (n fl m) dag path sequence [6] p optimal polygon triangulation [14] p optimal binary search trees [2] p paragraph formatting [14] p paragraph formatting 2 p O (n fl 2 n ) O (n fl width) 0-1 knapsack [14] p a O (2 n
Reference: 47. <author> T. Reps and T. Teitelbaum. </author> <title> The Synthesizer Generator: A System for Constructing Language-Based Editors. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1988. </year>
Reference-contexts: The incrementalization step as currently implemented is semi-automatic [26] and is being automated. The implementation uses the Synthesizer Generator <ref> [47] </ref>. Fig. 3 summarizes some of the examples derived (most of them semi-automa-tically and some automatically) and compares their asymptotic running times. 2 The second column shows whether more than one cache argument is needed in an incremental program.
Reference: 48. <author> W. L. Scherlis. </author> <title> Program improvement by internal specialization. </title> <booktitle> In Conference Record of the 8th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <pages> pages 41-49. </pages> <publisher> ACM, </publisher> <address> New York, </address> <month> Jan. </month> <year> 1981. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET.
Reference: 49. <author> D. R. Smith. KIDS: </author> <title> A semiautomatic program development system. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> 16(9) </volume> <pages> 1024-1043, </pages> <month> Sept. </month> <year> 1990. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET.
Reference: 50. <author> D. R. Smith. </author> <title> Structure and design of problem reduction generators. </title> <editor> In B. Moller, editor, </editor> <booktitle> Constructing Programs from Specifications, </booktitle> <pages> pages 91-124. </pages> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1991. </year>
Reference-contexts: Bird [5], de Moor [16], and others have studied it in the context of program transformation. While some works address the derivation of recursive equations, notably the work by Smith <ref> [50] </ref>, our work addresses the derivation of efficient programs that use tabulation.
Reference: 51. <author> M. Sniedovich. </author> <title> Dynamic Programming. </title> <publisher> Marcel Dekker, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: Summary of Examples. 8 Related work and conclusion Dynamic programming was first formulated by Bellman [4] and has been studied extensively since <ref> [51] </ref>. Bird [5], de Moor [16], and others have studied it in the context of program transformation. While some works address the derivation of recursive equations, notably the work by Smith [50], our work addresses the derivation of efficient programs that use tabulation.
Reference: 52. <author> B. Wegbreit. </author> <title> Goal-directed program transformation. </title> <journal> IEEE Trans. Softw. Eng., </journal> <volume> SE-2(2):69-80, </volume> <month> June </month> <year> 1976. </year>
Reference-contexts: Second, it stores only values that are necessary for the optimization; it also shows exactly when and where subproblems not in the original computation are necessarily included. Our method is based on static analyses and transformations studied previously by others <ref> [52, 9, 48, 6, 36, 20, 49, 41] </ref> and ourselves [33, 32, 31, 27, 32] and improves them. Yet, all three steps are simple, automatable, and efficient and have been implemented in a prototype system, CACHET.
References-found: 52

