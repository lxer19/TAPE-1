URL: http://www.almaden.ibm.com/cs/quest/papers/vldb94.ps
Refering-URL: http://www.almaden.ibm.com/cs/quest/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Fast Algorithms for Mining Association Rules  
Author: Rakesh Agrawal Ramakrishnan Srikant 
Address: 650 Harry Road, San Jose, CA 95120  
Affiliation: IBM Almaden Research Center  
Abstract: We consider the problem of discovering association rules between items in a large database of sales transactions. We present two new algorithms for solving this problem that are fundamentally different from the known algorithms. Empirical evaluation shows that these algorithms outperform the known algorithms by factors ranging from three for small problems to more than an order of magnitude for large problems. We also show how the best features of the two proposed algorithms can be combined into a hybrid algorithm, called AprioriHybrid. Scale-up experiments show that AprioriHybrid scales linearly with the number of transactions. AprioriHybrid also has excellent scale-up properties with respect to the transaction size and the number of items in the database. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, C. Faloutsos, and A. Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In Proc. of the Fourth International Conference on Foundations of Data Organization and Algorithms, </booktitle> <address> Chicago, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: We call the number of items in an itemset its size, and call an itemset of size k a k-itemset. Items within an itemset are kept in lexicographic order. We use the notation c <ref> [1] </ref> c [2] . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. <p> Items within an itemset are kept in lexicographic order. We use the notation c <ref> [1] </ref> c [2] . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. If c = X Y and Y is an m-itemset, we also call Y an m-extension of X. <p> Items within an itemset are kept in lexicographic order. We use the notation c <ref> [1] </ref> c [2] . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. If c = X Y and Y is an m-itemset, we also call Y an m-extension of X. Associated with each itemset is a count field to store the support for this itemset. <p> In Quest, we are exploring the various aspects of the database mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include 12 the enhancement of the database capability with clas-sification queries [2] and similarity queries over time sequences <ref> [1] </ref>. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgment We wish to thank Mike Carey for his insightful comments and suggestions.
Reference: [2] <author> R. Agrawal, S. Ghosh, T. Imielinski, B. Iyer, and A. Swami. </author> <title> An interval classifier for database mining applications. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 560-573, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: We call the number of items in an itemset its size, and call an itemset of size k a k-itemset. Items within an itemset are kept in lexicographic order. We use the notation c [1] c <ref> [2] </ref> . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. <p> Items within an itemset are kept in lexicographic order. We use the notation c [1] c <ref> [2] </ref> . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. If c = X Y and Y is an m-itemset, we also call Y an m-extension of X. <p> Items within an itemset are kept in lexicographic order. We use the notation c [1] c <ref> [2] </ref> . . . c [k] to represent a k-itemset c consisting of items c [1]; c [2]; . . .c [k], where c [1] &lt; c [2] &lt; . . . &lt; c [k]. If c = X Y and Y is an m-itemset, we also call Y an m-extension of X. Associated with each itemset is a count field to store the support for this itemset. <p> In Quest, we are exploring the various aspects of the database mining problem. Besides the problem of discovering association rules, some other problems that we have looked into include 12 the enhancement of the database capability with clas-sification queries <ref> [2] </ref> and similarity queries over time sequences [1]. We believe that database mining is an important new application area for databases, combining commercial interest with intriguing research questions. Acknowledgment We wish to thank Mike Carey for his insightful comments and suggestions.
Reference: [3] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Database mining: A performance perspective. </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> 5(6) </volume> <pages> 914-925, </pages> <month> December </month> <year> 1993. </year> <note> Special Issue on Learning and Discovery in Knowledge-Based Databases. </note>
Reference-contexts: Experiments show that the Apriori-Hybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining <ref> [3] </ref> [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10].
Reference: [4] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. of the ACM SIGMOD Conference on Management of Data, </booktitle> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Successful organizations view such databases as important pieces of the marketing infrastructure. They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies [6]. The problem of mining association rules over basket data was introduced in <ref> [4] </ref>. An example of such a rule might be that 98% of customers that purchase fl Visiting from the Department of Computer Science, University of Wisconsin, Madison. <p> Other applications include catalog design, add-on sales, store layout, and customer segmentation based on buying patterns. The databases involved in these applications are very large. It is imperative, therefore, to have fast algorithms for this task. The following is a formal statement of the problem <ref> [4] </ref>: Let I = fi 1 ; i 2 ; . . . ; i m g be a set of literals, called items. Let D be a set of transactions, where each transaction T is a set of items such that T I. <p> The rule X =) Y has support s in the transaction set D if s% of transactions in D contain X [ Y . Our rules are somewhat more general than in <ref> [4] </ref> in that we allow a consequent to have more than one item. <p> Our discussion is neutral with respect to the representation of D. For example, D could be a data file, a relational table, or the result of a relational expression. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in <ref> [4] </ref>. Another algorithm for this task, called the SETM algorithm, has been proposed in [13]. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algorithms. We present experimental results showing 1 that the proposed algorithms always outperform the earlier algorithms. <p> these issues in this paper, except to point out that these are necessary features of a rule discovery system that may use our algorithms as the engine of the discovery process. 1.1 Problem Decomposition and Paper Organization The problem of discovering all association rules can be decomposed into two subproblems <ref> [4] </ref>: 1. Find all sets of items (itemsets) that have transaction support above minimum support. The support for an itemset is the number of transactions that contain the itemset. Itemsets with minimum support are called large itemsets, and all others small itemsets. <p> Due to lack of space, we do not discuss this subproblem further, but refer the reader to [5] for a fast algorithm. In Section 3, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS <ref> [4] </ref> and SETM [13] algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. We also describe how the Apriori and AprioriTid algorithms can be combined into a hybrid algorithm, AprioriHybrid, and demonstrate the scaleup properties of this algorithm. <p> At the end of the pass, we determine which of the candidate itemsets are actually large, and they become the seed for the next pass. This process continues until no new large itemsets are found. The Apriori and AprioriTid algorithms we propose differ fundamentally from the AIS <ref> [4] </ref> and SETM [13] algorithms in terms of which candidate itemsets are counted in a pass and in the way that those candidates are generated. In both the AIS and SETM algorithms, candidate itemsets are generated on-the-fly during the pass as data is being read. <p> The data resided in the AIX file system and was stored on a 2GB SCSI 3.5" drive, with measured sequential throughput of about 2 MB/second. We first give an overview of the AIS <ref> [4] </ref> and SETM [13] algorithms against which we compare the performance of the Apriori and AprioriTid algorithms. We then describe the synthetic datasets used in the performance evaluation and show the performance results. <p> The candidates generated from a transaction are added to the set of candidate itemsets maintained for the pass, or the counts of the corresponding entries are increased if they were created by an earlier transaction. See <ref> [4] </ref> for further details of the AIS algorithm. 3.2 The SETM Algorithm The SETM algorithm [13] was motivated by the desire to use SQL to compute large itemsets. Like AIS, the SETM algorithm also generates candidates on-the-fly based on transactions read from the database. <p> We compared these algorithms to the previously known algorithms, the AIS <ref> [4] </ref> and SETM [13] algorithms. We presented experimental results, showing that the proposed algorithms always outperform AIS and SETM. The performance gap increased with the problem size, and ranged from a factor of three for small problems to more than an order of magnitude for large problems.
Reference: [5] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules in large databases. </title> <type> Research Report RJ 9839, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, California, </address> <month> June </month> <year> 1994. </year>
Reference-contexts: We need to consider all subsets of l to generate rules with multiple consequents. Due to lack of space, we do not discuss this subproblem further, but refer the reader to <ref> [5] </ref> for a fast algorithm. In Section 3, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS [4] and SETM [13] algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. <p> Each member of this set has two fields: i) itemset and ii) support count. Set of candidate k-itemsets when the TIDs C k of the generating transactions are kept associated with the candidates. given transaction t. Section 2.1.2 describes the subset function used for this purpose. See <ref> [5] </ref> for a discussion of buffer management. 1) L 1 = flarge 1-itemsetsg; 2) for ( k = 2; L k1 6= ;; k++ ) do begin 3) C k = apriori-gen (L k1 ); // New candidates 4) forall transactions t 2 D do begin 5) C t = subset <p> However, for small values for k, each entry may be larger than the corresponding transaction because an entry in C k includes all candidate k-itemsets contained in the transaction. In Section 2.2.1, we give the data structures used to implement the algorithm. See <ref> [5] </ref> for a proof of correctness and a discussion of buffer management. 1) L 1 = flarge 1-itemsetsg; 2) C 1 = database D; 3) for ( k = 2; L k1 6= ;; k++ ) do begin 4) C k = apriori-gen (L k1 ); // New candidates 5) C <p> The algorithms presented in this paper have been implemented on several data repositories, including the AIX file system, DB2/MVS, and DB2/6000. We have also tested these algorithms against real customer data, the details of which can be found in <ref> [5] </ref>. In the future, we plan to extend this work along the following dimensions: * Multiple taxonomies (is-a hierarchies) over items are often available. An example of such a hierarchy is that a dish washer is a kitchen appliance is a heavy electric appliance, etc.
Reference: [6] <author> D. S. Associates. </author> <title> The new direct marketing. Business One Irwin, </title> <publisher> Illinois, </publisher> <year> 1990. </year>
Reference-contexts: Successful organizations view such databases as important pieces of the marketing infrastructure. They are interested in instituting information-driven marketing processes, managed by database technology, that enable marketers to develop and implement customized marketing programs and strategies <ref> [6] </ref>. The problem of mining association rules over basket data was introduced in [4]. An example of such a rule might be that 98% of customers that purchase fl Visiting from the Department of Computer Science, University of Wisconsin, Madison.
Reference: [7] <author> R. Brachman et al. </author> <title> Integrated support for data archeology. </title> <booktitle> In AAAI-93 Workshop on Knowledge Discovery in Databases, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: There has been work on quantifying the "usefulness" or "interestingness" of a rule [20]. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in <ref> [7] </ref> [14].
Reference: [8] <author> L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone. </author> <title> Classification and Regression Trees. </title> <publisher> Wadsworth, </publisher> <address> Belmont, </address> <year> 1984. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules <ref> [8] </ref> [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
Reference: [9] <author> P. Cheeseman et al. </author> <title> Autoclass: A bayesian classification system. </title> <booktitle> In 5th Int'l Conf. on Machine Learning. </booktitle> <publisher> Morgan Kaufman, </publisher> <month> June </month> <year> 1988. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering <ref> [9] </ref> [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large.
Reference: [10] <author> D. H. Fisher. </author> <title> Knowledge acquisition via incremental conceptual clustering. </title> <journal> Machine Learning, </journal> <volume> 2(2), </volume> <year> 1987. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] <ref> [10] </ref>. The closest work in the machine learning literature is the KID3 algorithm presented in [20]. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large.
Reference: [11] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Knowledge discovery in databases: An attribute oriented approach. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 547-559, </pages> <address> Vancouver, British Columbia, Canada, </address> <year> 1992. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] <ref> [11] </ref> [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
Reference: [12] <author> M. Holsheimer and A. Siebes. </author> <title> Data mining: The search for knowledge in databases. </title> <type> Technical Report CS-R9406, </type> <institution> CWI, Netherlands, </institution> <year> 1994. </year>
Reference-contexts: Experiments show that the Apriori-Hybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [3] <ref> [12] </ref>, also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10].
Reference: [13] <author> M. Houtsma and A. Swami. </author> <title> Set-oriented mining of association rules. </title> <type> Research Report RJ 9567, </type> <institution> IBM Almaden Research Center, </institution> <address> San Jose, Cali-fornia, </address> <month> October </month> <year> 1993. </year>
Reference-contexts: For example, D could be a data file, a relational table, or the result of a relational expression. An algorithm for finding all association rules, henceforth referred to as the AIS algorithm, was presented in [4]. Another algorithm for this task, called the SETM algorithm, has been proposed in <ref> [13] </ref>. In this paper, we present two new algorithms, Apriori and AprioriTid, that differ fundamentally from these algorithms. We present experimental results showing 1 that the proposed algorithms always outperform the earlier algorithms. <p> Due to lack of space, we do not discuss this subproblem further, but refer the reader to [5] for a fast algorithm. In Section 3, we show the relative performance of the proposed Apriori and AprioriTid algorithms against the AIS [4] and SETM <ref> [13] </ref> algorithms. To make the paper self-contained, we include an overview of the AIS and SETM algorithms in this section. We also describe how the Apriori and AprioriTid algorithms can be combined into a hybrid algorithm, AprioriHybrid, and demonstrate the scaleup properties of this algorithm. <p> This process continues until no new large itemsets are found. The Apriori and AprioriTid algorithms we propose differ fundamentally from the AIS [4] and SETM <ref> [13] </ref> algorithms in terms of which candidate itemsets are counted in a pass and in the way that those candidates are generated. In both the AIS and SETM algorithms, candidate itemsets are generated on-the-fly during the pass as data is being read. <p> The data resided in the AIX file system and was stored on a 2GB SCSI 3.5" drive, with measured sequential throughput of about 2 MB/second. We first give an overview of the AIS [4] and SETM <ref> [13] </ref> algorithms against which we compare the performance of the Apriori and AprioriTid algorithms. We then describe the synthetic datasets used in the performance evaluation and show the performance results. <p> See [4] for further details of the AIS algorithm. 3.2 The SETM Algorithm The SETM algorithm <ref> [13] </ref> was motivated by the desire to use SQL to compute large itemsets. Like AIS, the SETM algorithm also generates candidates on-the-fly based on transactions read from the database. It thus generates and counts every candidate itemset that the AIS algorithm generates. <p> In fact, it needs to visit every member of L k only once in the TID order, and the candidate generation can be performed using the relational merge-join operation <ref> [13] </ref>. The disadvantage of this approach is mainly due to the size of candidate sets C k . For each candidate itemset, the candidate set now has as many entries as the number of transactions in which the candidate itemset is present. <p> The largest dataset in the scaleup experiments for SETM in <ref> [13] </ref> was still small enough that C k could fit in memory; hence they did not encounter this jump in execution time. Note that for the same minimum support, the support count for candidate itemsets increases linearly with the number of transactions. <p> We compared these algorithms to the previously known algorithms, the AIS [4] and SETM <ref> [13] </ref> algorithms. We presented experimental results, showing that the proposed algorithms always outperform AIS and SETM. The performance gap increased with the problem size, and ranged from a factor of three for small problems to more than an order of magnitude for large problems.
Reference: [14] <author> R. Krishnamurthy and T. Imielinski. </author> <title> Practitioner problems in need of database research: Research directions in knowledge discovery. </title> <journal> SIG-MOD RECORD, </journal> <volume> 20(3) </volume> <pages> 76-78, </pages> <month> September </month> <year> 1991. </year>
Reference-contexts: There has been work on quantifying the "usefulness" or "interestingness" of a rule [20]. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [7] <ref> [14] </ref>.
Reference: [15] <author> P. Langley, H. Simon, G. Bradshaw, and J. Zytkow. </author> <title> Scientific Discovery: Computational Explorations of the Creative Process. </title> <publisher> MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data <ref> [15] </ref>, and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
Reference: [16] <author> H. Mannila and K.-J. Raiha. </author> <title> Dependency inference. </title> <booktitle> In Proc. of the VLDB Conference, </booktitle> <pages> pages 155-158, </pages> <address> Brighton, England, </address> <year> 1987. </year>
Reference-contexts: If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data <ref> [16] </ref>. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [16] consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature. <p> Related work in the database literature is the work on inferring functional dependencies from data <ref> [16] </ref>. Functional dependencies are rules requiring strict satisfaction. Consequently, having determined a dependency X ! A, the algorithms in [16] consider any other dependency of the form X + Y ! A redundant and do not generate it. The association rules we consider are probabilistic in nature.
Reference: [17] <author> H. Mannila, H. Toivonen, and A. I. Verkamo. </author> <title> Efficient algorithms for discovering association rules. </title> <booktitle> In KDD-94: AAAI Workshop on Knowledge Discovery in Databases, </booktitle> <month> July </month> <year> 1994. </year>
Reference-contexts: q.item k2 , p.item k1 &lt; q.item k1 ; Next, in the prune step, we delete all itemsets c 2 C k such that some (k 1)-subset of c is not in L k1 : 1 Concurrent to our work, the following two-step candidate generation procedure has been proposed in <ref> [17] </ref>: C 0 C k = fX 2 C 0 k jX contains k members of L k1 g These two steps are similar to our join and prune steps respectively.
Reference: [18] <author> S. Muggleton and C. Feng. </author> <title> Efficient induction of logic programs. </title> <editor> In S. Muggleton, editor, </editor> <booktitle> Inductive Logic Programming. </booktitle> <publisher> Academic Press, </publisher> <year> 1992. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions <ref> [18] </ref>, fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
Reference: [19] <author> J. Pearl. </author> <title> Probabilistic reasoning in intelligent systems: Networks of plausible inference, </title> <year> 1992. </year>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules <ref> [19] </ref>, learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
Reference: [20] <author> G. Piatestsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In G. Piatestsky-Shapiro, editor, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in <ref> [20] </ref>. If used for finding all association rules, this algorithm will make as many passes over the data as the number of combinations of items in the antecedent, which is exponentially large. Related work in the database literature is the work on inferring functional dependencies from data [16]. <p> Similarly, the presence of rules X ! Y and Y ! Z does not necessarily mean that X ! Z holds because the latter may not have minimum confidence. There has been work on quantifying the "usefulness" or "interestingness" of a rule <ref> [20] </ref>. What is useful or interesting is often application-dependent. The need for a human in the loop and providing tools to allow human guidance of the rule discovery process has been articulated, for example, in [7] [14].
Reference: [21] <author> G. Piatestsky-Shapiro, </author> <title> editor. Knowledge Discovery in Databases. </title> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Experiments show that the Apriori-Hybrid has excellent scale-up properties, opening up the feasibility of mining association rules over very large databases. The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases <ref> [21] </ref>. Related, but not directly applicable, work includes the induction of classification rules [8] [11] [22], discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10].
Reference: [22] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufman, </publisher> <year> 1993. </year> <month> 13 </month>
Reference-contexts: The problem of finding association rules falls within the purview of database mining [3] [12], also called knowledge discovery in databases [21]. Related, but not directly applicable, work includes the induction of classification rules [8] [11] <ref> [22] </ref>, discovery of causal rules [19], learning of logical definitions [18], fitting of functions to data [15], and clustering [9] [10]. The closest work in the machine learning literature is the KID3 algorithm presented in [20].
References-found: 22

