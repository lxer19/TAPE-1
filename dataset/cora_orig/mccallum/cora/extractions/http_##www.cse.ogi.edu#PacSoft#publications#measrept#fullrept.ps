URL: http://www.cse.ogi.edu/PacSoft/publications/measrept/fullrept.ps
Refering-URL: http://www.cse.ogi.edu/PacSoft/publications/measrept/meas_rept.html
Root-URL: http://www.cse.ogi.edu
Author: Walter J. Ellis and Alexei Kotov 
Date: February 28, 1995  
Affiliation: Oregon Graduate Institute Pacific Software Research Center  
Note: CONTRACT NO. F19628-93-C-0069  
Abstract: Measurement Final Report SDRR Proof of Concept Experiment 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Tom DeMarco. </author> <title> Controlling Software Projects. </title> <publisher> Yourdon Press, </publisher> <year> 1982. </year>
Reference-contexts: This was perceived as a success of the measurement program in the group. 3.2.4 Resources Required for Measurement The effort expended in measurement tasks was 6% of the total project effort (1229 person-hours). This figure agrees with industrial experience <ref> [1] </ref>. 3.3 Infrastructure and Data Collection Details 3.3.1 Basic Infrastructure The basic element of the measurement infrastructure was a measurable SDRR development process. This implicitly placed requirements on the project processes that they be well-defined and sufficiently delineated so that measurements are possible.
Reference: [2] <author> Robin Milner, Mads Tofle, and Robert Harper. </author> <title> The Definition of Standard ML. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1990 </year>
Reference-contexts: The size is measured in Source Lines of Code (SLOC) of Standard ML (metalanguage) <ref> [2] </ref>, the primary development language. Effort distribution on the project level is in Figure E-1. Effort expended on reusable tool suite development is detailed in Figure E-5. Average development defect rate per thousand of lines of source code for the reusable tool suite was 3.4.
Reference: [3] <author> Richard Kieburtz. </author> <title> Results of the SDRR Validation Experiment. </title> <booktitle> In [8]. </booktitle>
Reference-contexts: The technology validation experiment demonstrated the benefits of SDRR by comparing it to a template-based technology, a previously-proven productivity enhancer. Some results of the validation experiment are presented in Section 4.7. More detailed information on the experiment planning and results can be found in the Experiment document <ref> [3] </ref>. The benchmarks of the performance of the generated code are presented in the Performance document [4]. The project-level application of measurements, including developing the infrastructure, is described in Section 3. Section 5 contains a list of references. <p> We now know the complexity of such an application and we can predict the effort required to do similar applications. 1.4.2.3 Technology Validation The technology validation experiment demonstrated 2.7 times productivity improvement using the SDRR technology compared to using templates. For details on the technology validation experiment refer to <ref> [3] </ref>. Brief analysis of the benefits of the use of this technology is presented in Section 5.1. 1.4.2.4 Technology Transition PacSoft is in an early stage of transitioning the technology. The first stage of transition is maximum exposure. <p> This report presents the answers to these questions. Through the use of metrics, PacSoft was able to quantify the value of the new technology (see <ref> [3] </ref>). The project productivity and quality was successfully quantified (Sections 4.4 and 4.2). Measurements made the process more visible and provided a basis for early decisions. This added significant value to the project. <p> These experiments were comparisons of SDRR application generator technology to template technology. Matched experiments were run for SDRR and templates. The effort expended in preparation of the experiment was 9.7 person-months (1553 person-hours). Detailed information on the experiment planning and results can be found in the Experiment document <ref> [3] </ref>.
Reference: [4] <author> Dino Oliva. </author> <title> Baselined Performance Measurements of Unoptimized Generated Code. </title> <booktitle> In [8]. </booktitle>
Reference-contexts: Some results of the validation experiment are presented in Section 4.7. More detailed information on the experiment planning and results can be found in the Experiment document [3]. The benchmarks of the performance of the generated code are presented in the Performance document <ref> [4] </ref>. The project-level application of measurements, including developing the infrastructure, is described in Section 3. Section 5 contains a list of references. The Appendix contains a verification cross-reference index matching the final report accomplishments to the original plan document [9]. <p> The effort expended in preparation of the experiment was 9.7 person-months (1553 person-hours). Detailed information on the experiment planning and results can be found in the Experiment document [3]. The benchmarks of the performance of the generated code are presented in the Performance document <ref> [4] </ref>. 4.7.1 Results and Brief Analysis Results of the experiment analysis show that the use of SDRR technology leads to 2.7 times improvement over templates technology in low-level design, coding, and testing stages of software development. These 3 stages comprise about 50% of a typical software development project.
Reference: [5] <author> Alex Kotov. </author> <title> Application of a New Functional Metric in a Research Environment. </title> <type> Technical report, </type> <institution> Department of Computer Science and Engineering, Oregon Graduate Institute, </institution> <year> 1994. </year>
Reference-contexts: The capabilities metric and project experience with this metric is described in greater detail in <ref> [5] </ref>. Capabilities were defined as the units of functionality of software. Each tool in the pipeline had an associated list of capabilities that should be achieved. The capabilities were further categorized as critical or noncritical.
Reference: [6] <author> James Hook, Jeffrey Bell. </author> <title> Design of the SDRR Pipeline. </title>
Reference-contexts: Furthermore, the author of the tool, who was initially an active participant of the project, left OGI, so the support for the tool could not have been easily obtained (technical aspects are described in detail in the document <ref> [6] </ref>. The decision was made to develop another tool internally to replace Schism. This change added 4 capabilities to the list of the system capabilities. The effort required to build the replacement tool was 430 person-hours. The impact in size was 4027 SLOC. Also, 332 SLOC were removed. <p> Schedule of the tool development is in Figure P-3. 4.5.2 DomainSpecific Tools The attributes of the domainspecific tools are in the following figures: The size of the domainspecific tools is 6000 SLOC of CRML (superset of Standard ML). The units of measure are detailed in the Design Document <ref> [6] </ref>. The effort expended on MTV-G is 2025 person hours. Average development defect rate per thousand of lines of source code for MTV-G was 6.5. There were no product failures discovered during 3 months of operation in the validation experiment. The PSF of the MSL compiler is in Figure D-6.
Reference: [7] <author> Laura McKinney. </author> <title> Tool Survey. Software Design for Reliability and Reuse Project. Phase I. </title> <booktitle> In [8]. </booktitle>
Reference-contexts: In the beginning of the project, PacSoft paid special attention to create the measurement infrastructure. The tool set used for measurement consisted of both commercial tools and "freeware" tools available at no charge <ref> [7] </ref>. When commercial or freeware tools were not available (e.g. code counter for Standard ML) or couldn't satisfy the project's requirements, custom tools were built. The main collection and analysis tool for the metrics data was Microsoft Excel for Macintosh, version 4.0.
Reference: [8] <institution> Pacific Software Research Center. </institution> <note> SDRR Project Phase I Final Scientific and Technical Report, February 1995. Pacific Software Research Center Measurement Final Report 15 </note>
References-found: 8

