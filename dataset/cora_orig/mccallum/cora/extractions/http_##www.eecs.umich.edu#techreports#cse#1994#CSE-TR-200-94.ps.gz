URL: http://www.eecs.umich.edu/techreports/cse/1994/CSE-TR-200-94.ps.gz
Refering-URL: http://www.eecs.umich.edu/home/techreports/cse94.html
Root-URL: http://www.eecs.umich.edu
Title: Unix I/O Performance in Workstations and Mainframes 1 Unix I/O Performance in Workstations and Mainframes  
Author: Peter M. Chen David A. Patterson Peter M. Chen David A. Patterson 
Keyword: Input/Output, I/O, performance, evaluation, disk, fi le cache, main memory bus, workstation, minisupercomputer, mainframe, benchmark, Unix, Sprite, Alpha AXP/3000, Convex C2, DECStation  
Note: 5000, HP 730, IBM 3090, RS/6000, SPARCStation 1, SPARCStation 10.  
Address: Berkeley  
Affiliation: Computer Science and Engineering Division Electrical Engineering and Computer Science Department University of Michigan  Computer Science and Engineering Division Electrical Engineering and Computer Science Department University of California at  
Abstract: Rapid advances in processor performance have shifted the performance bottleneck to I/O systems. The relatively slow rate of improvement in I/O is due in part to a lack of quantitative performance analysis of software and hardware alternatives. Using a new self-scaling I/O benchmark, we provide such an evaluation for 11 hardware configurations using 9 variations of the Unix operating system. In contrast to processor performance comparisons, where factors of 2 are considered large, we find differences of factors of 10 to 100 in I/O systems. The principal performance culprits are the policies of different Unix operating systems; some policies on writes to the fi le cache will cause processors to run at magnetic disk speeds instead of at main memory speeds. These results suggest a greater emphasis be placed on I/O performance when making operating system policy decisions. 
Abstract-found: 1
Intro-found: 1
Reference: [Baker91] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout. </author> <title> Measurements of a Distributed File System. </title> <booktitle> In Proceedings of the 13th ACM Symposium on Operating Systems Principles , pages 198212, </booktitle> <month> October </month> <year> 1991. </year>
Reference-contexts: Unix systems use a file cache a buffer in main memoryto reduce accesses to disks. <ref> [Baker91] </ref> found that in one Unix workstation environment file caches fulfill 60% of the data read requests, depending on the size of fi le cache. [ McKusick84] found that accesses to fi le meta data had even higher success rates. <p> Chen & David A. Patterson Unix I/O Performance in Workstations and Mainframes 6 for the file cache. HP/UX version 9, in contrast, supports dynamically varying file cache size, leading to an 10-fold larger file cache for the HP 730. <ref> [Baker91] </ref> demonstrated that Unix workloads benefi t from larger file caches, so this fi xed-size policy surely hurts I/O performance. <p> The short lives of fi les means that fi les will be deleted or overwritten and so their data need not be written to disk. Baker found that this 30-second window captures 65% to 80% of the lifetimes for all files <ref> [Baker91] </ref>. Their paper claims this percentage represents only small files and so almost all data is written through.
Reference: [Baker92] <author> Mary Baker, Satoshi Asami, Etienne Deprit, John Ousterhout, and Margo Seltzer. </author> <title> NonVolatile Memory for Fast Reliable File Systems. </title> <booktitle> In Fifth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS-V) pages 1022, </booktitle> <month> October </month> <year> 1992. </year>
Reference-contexts: According to another study , a small, nonvolatile fi le cache can reduce write traf fic by 50% <ref> [Baker92] </ref>.
Reference: [Chen93a] <author> Peter M. Chen and David A. Patterson. </author> <title> A New Approach to I/O Performance Evaluation Self-Scaling I/O Benchmarks, Predicted I/O Performance (conference version). </title> <booktitle> In Proceedings of the 1993 ACM SIGMETRICS Conference on Measurement and Modeling of Computer Systems , pages 112, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Multiple runs determine performance for varying balances of reads and writes, with each section picking the appropriate mix. Readers with more questions about the experimental method should see Section 5 or read <ref> [Chen93a] </ref>. We conclude this paper with a summary of results, a recommendation that more emphasis in operat ing systems be given to I/O performance, and a short section on future directions. 2 Disk Subsystem Performance A comprehensive evaluation of disk performance is problematic. <p> Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance [ Chen93b]. This led to a new approach to I/O performance analysis <ref> [Chen93a] </ref>. The fi rst step is a self-scaling benchmark that automatically and dynamically adjusts aspects of its workload according to the performance characteristics of the system being measured. <p> Because of the very different performance for file cache and disk accesses, the benchmark automatically picks two values for number of bytes accessed. (If there were a third region of very different performance, it would add that value to be explored; see <ref> [Chen93a] </ref> for more details on picking parameters.) Table 2 shows the nominal values selected for the machines and operating systems in this paper. The parameters of Table 2 can be considered workloads that use the resources of the machine ef fec-tively. <p> Thus to use the same average access sizes in Figures 2, 3, and 5 we adjusted performance in the self-scaling benchmark plot by the ratio of the performance at 32 KB to the performance at the nominal access size (see Table 3 in the Appendix). <ref> [Chen93a] </ref> has shown that this technique yields accurate performance estimates, within 10% for most workloads. This paper is a perfect example of the use of predicted performance. We came into this paper without preconceived notions about what the results should be, much less what values to set the nominal parameters.
Reference: [Chen93b] <author> Peter M. Chen and David A. Patterson. </author> <title> Storage PerformanceMetrics and Benchmarks. </title> <booktitle> Proceedings of the IEEE , 81(8):11511165, </booktitle> <month> August </month> <year> 1993. </year>
Reference-contexts: Current I/O benchmarks suffer from several chronic problems: they quickly become obsolete, they do not stress the I/O system, and they do not help in understanding I/O system performance <ref> [ Chen93b] </ref>. This led to a new approach to I/O performance analysis [Chen93a]. The fi rst step is a self-scaling benchmark that automatically and dynamically adjusts aspects of its workload according to the performance characteristics of the system being measured.
Reference: [Hartman93] <author> John H. Hartman and John K. Ousterhout. </author> <title> Letter to the Editor. </title> <journal> Operating Systems Review 27(1):79, </journal> <month> January </month> <year> 1993. </year>
Reference-contexts: In a correction to that claim, Hartman and Ousterhout reported that 36% to 63% of the bytes written do not survive a 30 second window; this number jumps to 60% to 95% in a 1000 second window <ref> [Hartman93] </ref>. According to another study , a small, nonvolatile fi le cache can reduce write traf fic by 50% [Baker92].
Reference: [Hennessy90] <author> John. L. Hennessy and David A. Patterson. </author> <title> Computer Architecture: </title> <publisher> A Quantitative Approach . Morgan Kaufmann Publishers, Inc., </publisher> <year> 1990. </year>
Reference-contexts: There is often some confusion about the defi ni-tion and implications of alternative write strategies for caches. To lessen that confusion, we fi rst review write policies of processor caches. (Readers interested in more about caches can consult textbooks such as <ref> [Hennessy90] </ref>.) The simplest write policy is write-through ; writes update the cache and the next level of the memory hierarchy behind the cache.
Reference: [Kay92] <author> Jonathan Kay and Joseph Pasquale. </author> <title> A Performance Analysis of TCP/IP and UDP/IP Networking Software for the DECStation 5000. </title> <type> Technical Report Sequoia Technical Report 92/18, </type> <institution> University of California at Berkeley, </institution> <month> December </month> <year> 1992. </year>
Reference-contexts: Chen & David A. Patterson Unix I/O Performance in Workstations and Mainframes 5 ware. Sprite does fewer copies when reading data from the fi le cache than does Ultrix <ref> [ Kay92] </ref>, hence its higher performance. 4 The Impact of Operating System Policies on File Cache Performance Given that Unix systems have common ancestors, we expected that the operating system policies towards I/O to be the same on all machines.
Reference: [Leffler89] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD Unix Operating System . Addison-Wesley Publishing Company, </title> <year> 1989. </year>
Reference: [Lyon90] <author> Bob Lyon and Russel Sandberg. </author> <title> Breaking Through the NFS Performance Barrier. </title> <type> Technical report, </type> <institution> Legato Systems, Inc., </institution> <year> 1990. </year>
Reference-contexts: Put another way , for all but the most heavily read-oriented workloads, the SP ARCStation client operates at disk speed while the HP client runs at main memory speed. Adding nonvolatile RAM such as Legato Systems Prestoserve board, would increase NFS performance somewhat <ref> [Lyon90] </ref>. However, writing through to NVRAM on a server is still limited by network speeds.
Reference: [McKusick84] <author> Marshall Kirk McKusick, William N. Joy, Samuel J. Leffler, and Robert S. Fabry. </author> <title> A Fast File System for UNIX. </title> <journal> ACM Transactions on Computer Systems , 2(3):181197, </journal> <month> August </month> <year> 1984. </year>
Reference-contexts: Unix systems use a file cache a buffer in main memoryto reduce accesses to disks. [Baker91] found that in one Unix workstation environment file caches fulfill 60% of the data read requests, depending on the size of fi le cache. <ref> [ McKusick84] </ref> found that accesses to fi le meta data had even higher success rates. Since main memory is much faster than disks, fi le caches yield a substantial performance improvement, and are found in every Unix operating system.
Reference: [Nelson88] <author> Michael N. Nelson, Brent B. Welch, and John K. Ousterhout. </author> <title> Caching in the Sprite Network File System. </title> <journal> ACM Transactions on Computer Systems , 6(1):134154, </journal> <month> February </month> <year> 1988. </year>
Reference-contexts: Using a shared bus multiprocessor as a rough analogy to our workstations on the local area network, DUX offers write-cancellation with cache coherency while NFS does write through without write buffers. Readers interested in more details of file cache policies should see <ref> [Nelson88] </ref>.
Reference: [Ousterhout90] <author> John K. Ousterhout. </author> <title> Why arent operating systems getting faster as fast as hardware? In Proceedings USENIX Summer Conference , pages 247256, </title> <month> June </month> <year> 1990. </year>
Reference: [Rau79] <author> B. Ramakrishna Rau. </author> <title> Program Behavior and the Performance of Interleaved Memories. </title> <journal> IEEE Transactions on Computers , C-28(3):191199, </journal> <month> March </month> <year> 1979. </year>
Reference-contexts: It models locality using the LRU stack model, with the average access depth in the LRU stack set at half this parameter <ref> [Rau79] </ref>. The benchmark varies this parameter to reveal file cache size (Figure 4). 2. Percentage of reads : The percentage of writes is 100% minus this percentage.
Reference: [Sandberg85] <author> Russel Sandberg, David Goldbert, Steve Kleiman, Dan Walsh, and Bob Lyon. </author> <title> Design and Implementation of the Sun Network Filesystem. </title> <booktitle> In Proceedings of the Summer 1985 Usenix Conference </booktitle>
Reference-contexts: This brings us to a policy decision: how to ensure that no one accesses stale data. The NFS solution, used by SunOS, is to make all client writes go to the server s disk when the file is closed <ref> [Sandberg85] </ref>. Taking an outsider s perspective, it seems inconsistent that a 30 second delay would be satisfactory for writes to local disk but not for writes to the server disk. Hence HP/UX offers an alternative network protocol, called DUX, which allows client-level caching of writes.
Reference: [Stern94] <author> Hal Stern. </author> <title> Virtual Amnesia. </title> <booktitle> Advanced Systems (previously SunWorld) , pages 1720, </booktitle> <month> February </month> <year> 1994. </year>
Reference-contexts: More recent systems allow the barrier between fi le cache and program memory to vary , allowing fi le caches to grow to be virtually the full size of main memory if warranted by the workload <ref> [ Tanenbaum85, Stern94] </ref>. File servers, for example, will surely use much more of their main memory for fi le cache than will most client workstations. Our benchmark varies the amount of data and measures performance to determine the maximum fi le cache size for each system.
Reference: [Tanenbaum85] <author> Andrew S. Tanenbaum and Robbert Van Renesse. </author> <title> Distributed Operating Systems. </title> <editor> Peter M. Chen & David A. </editor> <title> Patterson Unix I/O Performance in Workstations and Mainframes 15 Computing Surveys , 17(4):419470, </title> <month> December </month> <year> 1985. </year>
Reference-contexts: More recent systems allow the barrier between fi le cache and program memory to vary , allowing fi le caches to grow to be virtually the full size of main memory if warranted by the workload <ref> [ Tanenbaum85, Stern94] </ref>. File servers, for example, will surely use much more of their main memory for fi le cache than will most client workstations. Our benchmark varies the amount of data and measures performance to determine the maximum fi le cache size for each system.
Reference: [TPC90] <editor> TPC Benchmark B Standard Specification. </editor> <title> Technical report, Transaction Processing Performance Council, </title> <month> August </month> <year> 1990. </year>
Reference-contexts: By doing so, the benchmark automatically scales across a wide range of current and future systems.This scaling is more general than the scaling found in TPC-B <ref> [TPC90] </ref> and LADDIS [ Wittle93], for scaling here varies more than the load on the system. This first step aids in understanding system performance by reporting how performance varies according to each of fi ve workload parameters; these parameters determine the fi rst-order performance effects in I/O systems.
Reference: [Wittle93] <author> Mark Wittle and Bruce E. Keith. LADDIS: </author> <title> The next Generation in NSF File Server Benchmarking. </title> <booktitle> In Proceedings of the Summer 1993 USENIX Technical Conference </booktitle>
Reference-contexts: By doing so, the benchmark automatically scales across a wide range of current and future systems.This scaling is more general than the scaling found in TPC-B [TPC90] and LADDIS <ref> [ Wittle93] </ref>, for scaling here varies more than the load on the system. This first step aids in understanding system performance by reporting how performance varies according to each of fi ve workload parameters; these parameters determine the fi rst-order performance effects in I/O systems.
References-found: 18

