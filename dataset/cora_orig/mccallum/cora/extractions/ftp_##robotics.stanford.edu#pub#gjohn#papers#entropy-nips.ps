URL: ftp://robotics.stanford.edu/pub/gjohn/papers/entropy-nips.ps
Refering-URL: http://www.robotics.stanford.edu/~gjohn/pubs.html
Root-URL: http://www.robotics.stanford.edu
Email: gjohn@cs.Stanford.EDU  
Title: Robust Soft-Entropy Neural Network Trees  
Author: George H. John 
Keyword: Neural Net Trees with Pruning and Iterative RE-filtering).  
Address: Stanford, CA 94305  
Affiliation: Computer Science Department Stanford University  
Note: Submitted to Advances in Neural Information Processing Systems 7  
Abstract: We present a new method for the induction of tree-structured recursive partitioning classifiers that use a neural network as the partitioning function at each node in the tree. Our technique is appropriate for pattern recognition tasks with many continuous inputs and a single multivalued nominal output. This paper presents two main contributions: 1) a novel objective function called soft entropy, which is used to train each neural net to give the optimal partitioning of the data, and 2) a novel but simple method for removing outliers called iterative re-filtering, which boosts performance on many datasets. These two ideas are presented in the context of a single learning system called SENNT-PIRE (Soft Entropy 
Abstract-found: 1
Intro-found: 1
Reference: <author> Aha, D. W. </author> <year> (1991), </year> <title> `Instance-based learning algorithms', </title> <booktitle> Machine Learning 6(1), </booktitle> <pages> 37-66. </pages>
Reference-contexts: and thus we suspect backward pattern selection to give greater performance than forward selection; however, this hypothesis has not been tested. 4 EXPERIMENTAL RESULTS We ran SENNT, SENNT-PIRE, OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets <ref> (Thrun et al. 1991, Murphy & Aha 1994) </ref>, and results appear in Table 1. Note that our trees require continuous inputs, so a local encoding was used for all nominal input values. We give results for the unpruned, pruned, and retrained versions of the SENNT algorithm.
Reference: <author> Breiman, L., Friedman, J., Olshen, R. & Stone, C. </author> <year> (1984), </year> <title> Classification and Regression Trees, </title> <publisher> Chapman & Hall, </publisher> <address> New York. </address>
Reference-contexts: Note that the approach here is more general than only defining a soft version of entropy - any splitting criterion which depends only on the statistics p; p i ; n; n i (e.g., GINI <ref> (Breiman et al. 1984) </ref>, C-SEP (Fayyad & Irani 1992)) can be similarly softened by taking lef t (x) and right (x) to be continuous in [0; 1] and using the above equations for these statistics. 2.2 THE DERIVATIVE To use steepest descent or any other function optimizer requiring first derivative information, <p> In general, unless time is a very limited resource, the best results are achieved by starting big and then shrinking <ref> (Breiman et al. 1984) </ref>. This should 2 A more sensible strategy will be implemented shortly. Table 1: Test set accuracy and concept size (number of nodes) results on several datasets.
Reference: <author> Brent, R. P. </author> <year> (1991), </year> <title> `Fast training algorithms for neural networks', </title> <journal> IEEE Transactions on Neural Networks 2(3), </journal> <pages> 346-354. </pages>
Reference: <author> Brodley, C. E. & Utgoff, P. E. </author> <year> (1992), </year> <title> Multivariate versus univariate decision trees, </title> <type> Tech--nical Report COINS TR 92-8, </type> <institution> Department of Computer Science, University of Mas-sachusetts, </institution> <address> Amherst, MA, </address> <month> 01003. </month>
Reference: <author> Fahlman, S. E. & Lebiere, C. </author> <year> (1990), </year> <title> The cascade-correlation learning architecture, </title> <editor> in D. Touretsky, ed., </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 2, </volume> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (1) apply to pattern selection as well, and thus we suspect backward pattern selection to give greater performance than forward selection; however, this hypothesis has not been tested. 4 EXPERIMENTAL RESULTS We ran SENNT, SENNT-PIRE, OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation <ref> (Fahlman & Lebiere 1990) </ref> on several datasets (Thrun et al. 1991, Murphy & Aha 1994), and results appear in Table 1. Note that our trees require continuous inputs, so a local encoding was used for all nominal input values. <p> That smaller trees were generated by SENNT suggests that the cyclical optimization in OC1 might still be getting stuck in local minima. From these results we cannot claim that soft entropy outperforms entropy, since totally different splits seem to have been found. We ran Cascade Correlation <ref> (Fahlman & Lebiere 1990) </ref> ten times on each dataset with the default parameter settings. Results were competitive with the other algorithms, but worse than the reported performance of Cascade Correlation in (Thrun et al. 1991). <p> Other related work has been discussed in the text where appropriate. We are currently pursuing the use of soft entropy, rather than MSE, as the objective function for the early nodes in Cascaded neural networks <ref> (Fahlman & Lebiere 1990, Littmann & Ritter 1993) </ref> which add neurons as training progresses.
Reference: <author> Fayyad, U. M. & Irani, K. B. </author> <year> (1992), </year> <title> The attribute selection problem in decision tree generation, </title> <booktitle> in `AAAI-92: Proceedings of the Tenth National Conference on Artificial Intelligence', American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 104-110. </pages>
Reference-contexts: Note that the approach here is more general than only defining a soft version of entropy - any splitting criterion which depends only on the statistics p; p i ; n; n i (e.g., GINI (Breiman et al. 1984), C-SEP <ref> (Fayyad & Irani 1992) </ref>) can be similarly softened by taking lef t (x) and right (x) to be continuous in [0; 1] and using the above equations for these statistics. 2.2 THE DERIVATIVE To use steepest descent or any other function optimizer requiring first derivative information, we must first differentiate the
Reference: <author> Friedman, J. H. </author> <year> (1977), </year> <title> `A recursive partitioning decision rule for nonparametric classification', </title> <journal> IEEE Transactions on Computers pp. </journal> <pages> 404-408. </pages>
Reference-contexts: 1 INTRODUCTION Recursive partitioning classifiers <ref> (Friedman 1977, Breiman, Friedman, Olshen & Stone 1984) </ref>, or decision trees, are an important nonparametric function representation. Their wide and successful use in fielded applications and their simple intuitive appeal make decision tree learning algorithms an important area of study. <p> We would expect entropy to perform better in this role than MSE. In this example the entropy-minimizing split also maximizes the Kolmogoroff-Smirnoff distance <ref> (Friedman 1977) </ref>. The entropy criterion is described in Morgan & Messenger (1973). Soft entropy is quite similar, but is to be preferred. Figure 2 shows several candidate splits of a simple dataset.
Reference: <author> Guyon, I., Matic, N. & Vapnik, V. </author> <year> (1994), </year> <title> Discovering informative patterns and data cleaning, </title> <booktitle> in `AAAI-94: Proceedings of the Twelfth National Conference on Artificial Intelligence', American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press. To Appear. </publisher>
Reference: <author> Hubel, P. J. </author> <year> (1977), </year> <title> Robust Statistical Procedures, </title> <institution> Society for Industrial and Applied Mathematics, </institution> <address> Pittsburgh, PA. </address>
Reference: <author> John, G., Kohavi, R. & Pfleger, K. </author> <year> (1994), </year> <title> Irrelevant features and the subset selection problem, </title> <editor> in H. Hirsh & W. Cohen, eds, </editor> <booktitle> `Machine Learning: Proceedings of the Eleventh International Conference', </booktitle> <publisher> Morgan Kaufmann. To Appear. </publisher>
Reference-contexts: Other methods (Matan 1994, Aha 1991) actively accept or reject training patterns from a temporal sequence presented to them. The difference between our method and theirs is somewhat akin to the difference between forward and backward feature subset selection <ref> (John, Kohavi & Pfleger 1994) </ref> or forward and backward (construction/pruning) search methods over neural net or decision tree architectures. In general, unless time is a very limited resource, the best results are achieved by starting big and then shrinking (Breiman et al. 1984).
Reference: <author> Littmann, E. & Ritter, H. </author> <year> (1993), </year> <title> Generalization abilities of cascade network architectures, </title> <editor> in S. J. Hanson, J. Cowan & C. L. Giles, eds, </editor> <booktitle> `Advances in Neural Information Processing Systems', </booktitle> <volume> Vol. 5, </volume> <publisher> Morgan Kaufmann, </publisher> <pages> pp. 188-195. </pages>
Reference: <author> Matan, O. </author> <year> (1994), </year> <title> On-site learning, or learning "on the job", </title> <note> Submitted to these proceedings. </note>
Reference-contexts: The selection of patterns for training is an ubiquitous problem in pattern recognition. Many methods are passive, accepting data from a training sample in some random order. Other methods <ref> (Matan 1994, Aha 1991) </ref> actively accept or reject training patterns from a temporal sequence presented to them.
Reference: <author> Morgan, J. N. & Messenger, R. C. </author> <year> (1973), </year> <title> THAID: a sequential analysis program for the analysis of nominal scale dependent variables, </title> <institution> University of Michigan. </institution>
Reference-contexts: Previous objective functions for neural net training (e.g., mean squared error) were found deficient, so we derived a new objective function called soft entropy based on the entropy criterion used in early and recent decision tree methods <ref> (Morgan & Messenger 1973, Quinlan 1986) </ref>. Since optimizing performance on an unseen test set is the real problem we're trying to solve, many authors have proposed pruning the decision tree in an attempt to address the bias/variance tradeoff. Pruning involves removing an entire subtree and replacing it with a leaf.
Reference: <author> Murphy, P. M. & Aha, D. W. </author> <year> (1994), </year> <note> `UCI repository of machine learning databases', Available by anonymous ftp to ics.uci.edu in the pub/machine-learning-databases directory. </note>
Reference: <author> Murthy, S. K., Salzberg, S. & Kasif, S. </author> <year> (1993a), </year> <note> `OC1', Software available by ftp to blaze.cs.jhu.edu. </note>
Reference: <author> Murthy, S., Kasif, S., Salzberg, S. & Beigel, R. </author> <year> (1993b), </year> <title> OC1: Randomized induction of oblique decision trees, </title> <booktitle> in `AAAI-93: Proceedings of the Eleventh National Conference on Artificial Intelligence', American Association for Artificial Intelligence, </booktitle> <publisher> AAAI Press / The MIT Press, </publisher> <pages> pp. 322-327. </pages>
Reference-contexts: 94.2 (5) 90.3 (5) Cascor 97.4 (5.9) 97.9 (1.2) 96.1 (1) 98.2 (1) 91.4 (1) apply to pattern selection as well, and thus we suspect backward pattern selection to give greater performance than forward selection; however, this hypothesis has not been tested. 4 EXPERIMENTAL RESULTS We ran SENNT, SENNT-PIRE, OC1 <ref> (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a) </ref>, and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets (Thrun et al. 1991, Murphy & Aha 1994), and results appear in Table 1. <p> In Monks3 the algorithm performed poorly becuase the pruning step actually threw out more real patterns than noisy ones! This supports Hubel's position. A related algorithm is OC1 <ref> (Murthy et al. 1993b, Murthy et al. 1993a) </ref>, which is quite similar to the cycliclal optimization algorithm for finding hyperplane splits used in CART, except that OC1 adds an attempt to jump out of local minima by moving in a random direction in weight space.
Reference: <author> Quinlan, J. R. </author> <year> (1986), </year> <title> `Induction of decision trees', </title> <booktitle> Machine Learning 1, </booktitle> <pages> 81-106. </pages>
Reference: <author> Sankar, A. & Mammone, R. J. </author> <year> (1991), </year> <title> Speaker independent vowel recognition using neural tree networks, </title> <booktitle> in `IJCNN-91-SEATTLE: International Joint Conference on Neural Networks', </booktitle> <publisher> IEEE Press, </publisher> <address> Seattle, WA, </address> <pages> pp. II: 809-14. </pages>
Reference-contexts: Previous work in neural trees has generally used off-the-shelf training algorithms for neural networks, thus minimizing a mean squared error criterion <ref> (Sankar & Mammone 1991, Brodley & Utgoff 1992) </ref>. Soft entropy is to be preferred over MSE for several reasons. Figure 1 shows a dataset and the hyperplanes (neural nets with no hidden units) minimizing MSE and soft entropy.
Reference: <author> Thrun, S. B. et al. </author> <year> (1991), </year> <title> The monk's problems a performance comparison of different learning algorithms, </title> <type> Technical Report CMU-CS-91-197, </type> <institution> CMU School of Computer Science. </institution>
Reference-contexts: and thus we suspect backward pattern selection to give greater performance than forward selection; however, this hypothesis has not been tested. 4 EXPERIMENTAL RESULTS We ran SENNT, SENNT-PIRE, OC1 (Murthy, Kasif, Salzberg & Beigel 1993b, Murthy, Salzberg & Kasif 1993a), and Cascade Correlation (Fahlman & Lebiere 1990) on several datasets <ref> (Thrun et al. 1991, Murphy & Aha 1994) </ref>, and results appear in Table 1. Note that our trees require continuous inputs, so a local encoding was used for all nominal input values. We give results for the unpruned, pruned, and retrained versions of the SENNT algorithm. <p> We ran Cascade Correlation (Fahlman & Lebiere 1990) ten times on each dataset with the default parameter settings. Results were competitive with the other algorithms, but worse than the reported performance of Cascade Correlation in <ref> (Thrun et al. 1991) </ref>. We did not use the parameter values reported there because it is unclear to what extent those parameters were determined by training on the test set, that is, using the test set performance to select good values for the parameters.
References-found: 19

