URL: ftp://ftp.cs.wisc.edu/wwt/icpp94_cachier.ps
Refering-URL: http://www.cs.wisc.edu/~chilimbi/index1.html
Root-URL: 
Email: [chilimbi, larus]@cs.wisc.edu  
Title: Cachier: A Tool for Automatically Inserting CICO Annotations  
Author: Trishul M. Chilimbi and James R. Larus 
Keyword: Shared-memory, parallel programming performance models, parallel programming tools, cache-coherence, directory protocols.  
Address: 1210 West Dayton Street Madison, WI 53706  
Affiliation: Computer Sciences Department University of WisconsinMadison  
Abstract: Shared memory in a parallel computer provides programmers with the valuable abstraction of a shared address spacethrough which any part of a computation can access any datum. Although uniform access simplifies programming, it also hides communication, which can lead to inefficient programs. The check-in, check-out (CICO) performance model for cache-coherent, shared-memory parallel computers helps a programmer identify the communication underlying memory references and account for its cost. CICO consists of annotations that a programmer can use to elucidate communication and a model that attributes costs to these annotations. The annotations can also serve as directives to a memory system to improve program performance. Inserting CICO annotations requires reasoning about the dynamic cache behavior of a program, which is not always easy. This paper describes Cachier, a tool that automatically inserts CICO annotations into shared-memory programs. A novel feature of this tool is its use of both dynamic information, obtained from a program execution trace, as well as static information, obtained from program analysis. We measured several benchmarks annotated by Cachier by running them on a simulation of the Dir 1 SW cache coherence protocol [10], which supports these directives. The results show that programs annotated by Cachier perform significantly better than both programs without CICO annotations and programs that were annotated by hand. This work is supported in part by NSF NYI Award CCR-9357779, NSF Grants CCR-9101035 and MIP-9225097, and a Univ. of Wisconsin Graduate School Grant. Our Thinking Machines CM-5 was purchased through NSF Institutional Infrastructure Grant No. CDA-9024618 with matching funding from the Univ. of Wisconsin Graduate School. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita V. Adve, Vikram S. Adve, Mark D. Hill, and Mary K. Vernon, </author> <title> Comparison of Hardware and Software Cache Coherence Schemes, </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <month> (June, </month> <year> 1991), </year> <pages> pp. 298-308. </pages>
Reference-contexts: Even for programs amenable to static analysis, dynamic information supplements the static information since static analysis alone can produce overly conservative estimates of sharing <ref> [1] </ref>. The trace file contains information about a cache miss, including its type, the address being accessed, the program counter at that point, the node making the access, and the epoch in which the access occurred (see Figure 3).
Reference: [2] <author> David Callahan, Ken Kennedy, and Allan Porterfield, </author> <title> Software Prefetching, </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> (April, </month> <year> 1991), </year> <pages> pp. 40-52. </pages>
Reference: [3] <author> David Chaiken, John Kubiatowics, and Anant Agarwal, </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme, </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> (April, </month> <year> 1991), </year> <pages> pp. 224-234. </pages>
Reference-contexts: Shared memory, on the other hand, offers a simpler programming model since shared data can be transparently accessed by any processor. Typically, scalable shared memory systems use a message-passing hardware base augmented by special hardware or software that implements a cache-coherence protocolfor example, Stanford DASH [14], MIT Alewife <ref> [3] </ref>, or Wis-consin Dir 1 SW [10][18]. A read to or write from a shared memory location will cause interprocessor communication in some cases, depending whether the referenced data was cached locally or is stored remotely. This communication can seriously impair a programs performance.
Reference: [4] <author> J. Cheong, and A.V. Veidenbaum, </author> <title> A Cache Coherence Scheme with Fast Selective Invalidation, </title> <booktitle> Proceedings of the 15th Annual International Symposium on Computer Architecture, </booktitle> <month> (June, </month> <year> 1988), </year> <pages> pp. 299-307. </pages>
Reference: [5] <author> Ron Cytron, Steve Karlovsky, and Kevin P. McAuliffe, </author> <title> Automatic Management of Programmable Caches, </title> <booktitle> Proceedings of the 1988 International Conference on Parallel Processing (Vol. 2 Software), </booktitle> <month> (Aug., </month> <year> 1988), </year> <pages> pp. 229-238. </pages>
Reference: [6] <author> Perry A. Emrath, Sanjoy Ghosh, and David A. Padua, </author> <title> Event Synchronization Analysis for Debugging Parallel Programs, </title> <booktitle> Supercomputing 89, </booktitle> <month> (Nov., </month> <year> 1989), </year> <pages> pp. 580-588. </pages>
Reference-contexts: Techniques for race detection in the context of debugging programs have either used dynamic information from a programs execution trace or static information from an analysis of the program text [17]. A few techniques have used dynamic information as well as static information <ref> [6] </ref>. However the static information supplements the dynamic information by ruling out races in certain parts of the program, thereby precluding the need to trace those parts. The 10 actual race detection uses dynamic information.
Reference: [7] <author> J. A. Fisher, and S. M. Freudenberger, </author> <title> Predicting Conditional Branch Directions from Previous Runs of a Program, </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> (Sept., </month> <year> 1992), </year> <pages> pp 85-95. </pages>
Reference-contexts: Secondly, it appears that even dynamic applications are not all that dynamic as far as memory access patterns are concerned. Moreover, other measurements show that program behavior is typically independent of the input data set <ref> [7] </ref>. 5 Restructuring with CICO This section illustrates how the CICO annotations inserted by Cachier can be used to restructure a program. We do this using the same matrix multiply example from the previous section.
Reference: [8] <author> Dennis Gannon, William Jalby, and K. Gallivan, </author> <title> Strategies for Cache and Local Memory Management by Global Program Transformation Journal of Parallel and Distributed Computing, </title> <journal> (Vol. </journal> <volume> 5, </volume> <year> 1988), </year> <pages> pp. 587-616. </pages>
Reference: [9] <author> E. Gornish, E. Granston, and A. Veidenbaum, </author> <title> Compiler Directed Data Prefetching in Multiprocessors with Memory Hierarchies, </title> <booktitle> International Conference on Supercomputing, </booktitle> <year> 1990. </year>
Reference: [10] <author> Mark D. Hill, James R. Larus, Steven R. Reinhardt, and David A. Wood, </author> <title> Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors, </title> <journal> ACM Transactions on Computer Systems, </journal> <month> (Nov., </month> <year> 1993), </year> <pages> pp. 300-318. </pages>
Reference-contexts: Finally, Section 7 discusses related work. 2 Programming Performance Model A shared-memory programming performance model should aid a programmer in writing efficient programs by exposing the communication underlying shared-memory references and by providing a measure of the cost of this communication. This section briey describes the CICO model <ref> [10] </ref> [13] with the help of an example. 2.1 CICO Model The CICO model consists of check-out, check-in, and prefetch annotations that a programmer adds to a program and a cost model that uses these annotations to compute a programs shared-memory communication cost. <p> To illustrate the CICO model, we consider a simple example of Jacobi relaxation code on a matrix of size N x N. This example is from Hill et al. <ref> [10] </ref>. Assume there are P 2 processors (where N is a multiple of P), each of which has been assigned a block of the matrix consisting of N/P rows, L ip to U ip and N/P columns, L jp to U jp . <p> It uses a technique called virtual prototyping, by which it only simulates those features of the parallel architecture that are not present in the native hardware. We use it to simulate Dir 1 SW <ref> [10] </ref>, [21], which is a cache-coherence directory protocol that has support for programs written within the CICO model. <p> The simulated computer consists of 32 processor nodes, each containing a processor, shared-memory module, cache, and network interface. The cache is 256 KB, 4-way set-associative with a cache block size of 32 bytes. We used WWT to simulate a directory-based Dir 1 SW cache-coherence protocol <ref> [10] </ref> [21]. For this evaluation we use five benchmarks: Barnes, Ocean, Mp3d (from the SPLASH Benchmark suite [19]), Matrix Multiply, and Tomcatv (a parallel version of the SPEC Benchmark). Barnes performs a gravitational N-body simulation using the Barnes-Hut algorithm. We simulated it for a data set of size 1024 bodies.
Reference: [11] <institution> Kendall Square Research, Kendall Square Research Technical Summary, </institution> <year> 1992. </year>
Reference-contexts: Even in this role as memory system directives, the annotations do not affect a programs semantics. Most parallel computers provide memory system directives similar to CICO directives. Perhaps the most common is a prefetch instruction. The Kendall Square KSR-1 <ref> [11] </ref> provides a post-store instruction that broadcasts read-only copies of a cache block to all other nodes that have it allocated but are in the invalid state. This operation is similar, though not identical, to a check-in.
Reference: [12] <author> Monica S. Lam, Edward E. Rothberg, and Michael E. </author> <title> Wolfe The Cache Performance and Optimizations of Blocked Algorithms, </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> (April, </month> <year> 1991), </year> <pages> pp. 63-74. </pages>
Reference: [13] <author> James R. Larus, Satish Chandra, and David A. Wood, CICO: </author> <title> A Practical Shared-Memory Programming Performance Model, Workshop on Portability and Performance for Parallel Processing, </title> <month> (July, </month> <year> 1993), </year> <note> To appear: </note> <author> Ferrante & Hey eds., </author> <title> Portability and Performance for Parallel Processors. </title>
Reference-contexts: Although it is often easier to write shared-memory programs, it may be more difficult to write a fast program. To write efficient programs, a shared-memory programmer must be aware of the cost of memory references. The check-in, check-out (CICO) shared-memory programming performance model proposed by Larus et al. <ref> [13] </ref> is a first step in this direction. CICO exposes the communication underlying memory references in cache-coherent shared-memory computers. CICO consists of annotations that a programmer can use to capture the communication underlying shared-memory references and a cost model that attributes a cost to this communication. <p> Finally, Section 7 discusses related work. 2 Programming Performance Model A shared-memory programming performance model should aid a programmer in writing efficient programs by exposing the communication underlying shared-memory references and by providing a measure of the cost of this communication. This section briey describes the CICO model [10] <ref> [13] </ref> with the help of an example. 2.1 CICO Model The CICO model consists of check-out, check-in, and prefetch annotations that a programmer adds to a program and a cost model that uses these annotations to compute a programs shared-memory communication cost.
Reference: [14] <author> Daniel Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica Lam, </author> <title> The Stanford DASH Multiprocessor, </title> <booktitle> IEEE Computer, </booktitle> <month> (March, </month> <year> 1992), </year> <pages> pp. 63-79. </pages>
Reference-contexts: Shared memory, on the other hand, offers a simpler programming model since shared data can be transparently accessed by any processor. Typically, scalable shared memory systems use a message-passing hardware base augmented by special hardware or software that implements a cache-coherence protocolfor example, Stanford DASH <ref> [14] </ref>, MIT Alewife [3], or Wis-consin Dir 1 SW [10][18]. A read to or write from a shared memory location will cause interprocessor communication in some cases, depending whether the referenced data was cached locally or is stored remotely. This communication can seriously impair a programs performance.
Reference: [15] <author> Sang Lyul Min, and Jean-Loup Baer, </author> <title> A Timestamp-based Cache Coherence Scheme, </title> <booktitle> Proceedings of the 1989 International Conference on Parallel Processing (Vol. 1 Architecture), </booktitle> <month> (Aug., </month> <year> 1989), </year> <pages> pp 23-32. </pages>
Reference: [16] <author> Todd C. Mowry, Monica S. Lam, and Anoop Gupta, </author> <title> Design and Evaluation of a Compiler Algorithm for Prefetching, </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <month> (Sept., </month> <year> 1992), </year> <pages> pp. 62-73. </pages>
Reference: [17] <author> Robert H. Netzer. </author> <title> Race Condition Detection for Debugging Shared-Memory Parallel Programs. </title> <type> Ph.D. thesis, </type> <institution> University of Wisconsin-Madison, </institution> <year> 1991. </year>
Reference-contexts: This results in a reduction in message traffic as well as communication latency. Techniques for race detection in the context of debugging programs have either used dynamic information from a programs execution trace or static information from an analysis of the program text <ref> [17] </ref>. A few techniques have used dynamic information as well as static information [6]. However the static information supplements the dynamic information by ruling out races in certain parts of the program, thereby precluding the need to trace those parts. The 10 actual race detection uses dynamic information.
Reference: [18] <author> Steven K. Reinhardt, Mark D. Hill, James R. Larus, Alvin R. Lebeck, James C. Lewis, and David A. Wood, </author> <title> The Wisconsin Wind Tunnel: Virtual Prototyping of Parallel Computers, </title> <booktitle> Proceedings of the 1993 ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <month> (May </month> <year> 1993), </year> <pages> pp. 4860. </pages>
Reference-contexts: WWT Filter Cachier Unannotated Target Program Annotated Target Program Fig 1. Overview of the Cachier Framework trace filetarget program trace execute read Synchronization Barrier Synchronization Barrier Synchronization Barrier Epoch Fig. 2 Program Model 4 3.2 Wisconsin Wind Tunnel (WWT) The Wisconsin Wind Tunnel (WWT) <ref> [18] </ref> is an accurate parallel architecture simulator that runs on a Thinking Machines CM-5 computer [20]. It uses a technique called virtual prototyping, by which it only simulates those features of the parallel architecture that are not present in the native hardware. <p> The hand CICO was carefully done over a period of a few weeks with the aid of existing profiling tools by individuals with a detailed understanding of the problem and cache-coherence protocol. Cachier produced the automatic CICO version. All simulations were run on the Wisconsin Wind Tunnel (WWT) <ref> [18] </ref>. The simulated computer consists of 32 processor nodes, each containing a processor, shared-memory module, cache, and network interface. The cache is 256 KB, 4-way set-associative with a cache block size of 32 bytes. We used WWT to simulate a directory-based Dir 1 SW cache-coherence protocol [10] [21].
Reference: [19] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta, </author> <title> SPLASH: Stanford Parallel Applications for Shared Memory, Computer Architecture News, </title> <month> (March, </month> <year> 1992), </year> <pages> pp. 5-44. </pages>
Reference-contexts: Section 4 explains the need for both types of program information, as well as Cachiers techniques for inserting the CICO annotations. This section describes the computing environment in which Cachier operates. 3.1 Target Program Model We studied programs from the Stanford Splash shared-memory benchmark suite <ref> [19] </ref>. These programs use barrier synchronizations as their primary synchronization mechanism. The programs also use locks. However, a very small fraction of the programs total computation is performed within lock-unlock intervals, so we ignore locks and concentrate on the program model shown in Figure 2. <p> The cache is 256 KB, 4-way set-associative with a cache block size of 32 bytes. We used WWT to simulate a directory-based Dir 1 SW cache-coherence protocol [10] [21]. For this evaluation we use five benchmarks: Barnes, Ocean, Mp3d (from the SPLASH Benchmark suite <ref> [19] </ref>), Matrix Multiply, and Tomcatv (a parallel version of the SPEC Benchmark). Barnes performs a gravitational N-body simulation using the Barnes-Hut algorithm. We simulated it for a data set of size 1024 bodies. Ocean performs a cuboidal ocean basin simulation using Gauss-Seidel with Successive Over Relaxation. <p> In Ocean, 88% of loads read shared data and 68% of the stores write shared data, whereas for Mp3d, the corresponding numbers are 71% (shared reads) and 80% (shared writes) respectively <ref> [19] </ref>. On the other hand, in Barnes, where the performance improvement is not as large, the degree of sharing is much lower25.5% of the loads are shared data reads and only 1.3% of the stores are shared data writes [19]. <p> corresponding numbers are 71% (shared reads) and 80% (shared writes) respectively <ref> [19] </ref>. On the other hand, in Barnes, where the performance improvement is not as large, the degree of sharing is much lower25.5% of the loads are shared data reads and only 1.3% of the stores are shared data writes [19]. Moreover, Cachier-annotated versions of the programs consistently outperformed the hand-annotated versions, which shows that inserting annotations by hand is not an easy task, especially for programs with dynamic memory access patterns.
Reference: [20] <author> Thinking Machines Corporation. </author> <title> The Connection Machine CM-5 Technical Summary, </title> <year> 1991. </year>
Reference-contexts: Overview of the Cachier Framework trace filetarget program trace execute read Synchronization Barrier Synchronization Barrier Synchronization Barrier Epoch Fig. 2 Program Model 4 3.2 Wisconsin Wind Tunnel (WWT) The Wisconsin Wind Tunnel (WWT) [18] is an accurate parallel architecture simulator that runs on a Thinking Machines CM-5 computer <ref> [20] </ref>. It uses a technique called virtual prototyping, by which it only simulates those features of the parallel architecture that are not present in the native hardware.
Reference: [21] <author> David A. Wood, Satish Chandra, Babak Falsafi, Mark D. Hill, James R. Larus, Alvin L. Lebeck, James C. Lewis, Shubhendu S. Mukherjee, Subbarao Palacharla, and Steven K. Reinhardt, </author> <title> Mechanisms for Cooperative Shared Memory, </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <month> (May </month> <year> 1993), </year> <pages> pp. 156-168. </pages>
Reference-contexts: A programmer can use the information from Cachier to eliminate some of these events. We measured the usefulness of CICO annotations as memory-system directives by running several benchmarks on a simulator of the Dir 1 SW memory system protocol <ref> [21] </ref>. The CICO annotations inserted by Cachier outperformed both the program without any annotations as well as a hand-annotated version. Interestingly, Cachier performed better on programs with complex, dynamic memory access, which caused programmers the greatest trouble. The rest of this paper is organized as follows. <p> It uses a technique called virtual prototyping, by which it only simulates those features of the parallel architecture that are not present in the native hardware. We use it to simulate Dir 1 SW [10], <ref> [21] </ref>, which is a cache-coherence directory protocol that has support for programs written within the CICO model. <p> The simulated computer consists of 32 processor nodes, each containing a processor, shared-memory module, cache, and network interface. The cache is 256 KB, 4-way set-associative with a cache block size of 32 bytes. We used WWT to simulate a directory-based Dir 1 SW cache-coherence protocol [10] <ref> [21] </ref>. For this evaluation we use five benchmarks: Barnes, Ocean, Mp3d (from the SPLASH Benchmark suite [19]), Matrix Multiply, and Tomcatv (a parallel version of the SPEC Benchmark). Barnes performs a gravitational N-body simulation using the Barnes-Hut algorithm. We simulated it for a data set of size 1024 bodies.
References-found: 21

