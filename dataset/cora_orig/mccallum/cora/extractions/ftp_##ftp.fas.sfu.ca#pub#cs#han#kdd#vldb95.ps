URL: ftp://ftp.fas.sfu.ca/pub/cs/han/kdd/vldb95.ps
Refering-URL: http://fas.sfu.ca/cs/research/groups/DB/sections/publication/kdd/kdd.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: fhan,yongjiang@cs.sfu.ca  
Title: Discovery of Multiple-Level Association Rules from Large Databases  
Author: Jiawei Han and Yongjian Fu 
Address: Canada V5A 1S6  
Affiliation: School of Computing Science Simon Fraser University British Columbia,  
Abstract: Previous studies on mining association rules find rules at single concept level, however, mining association rules at multiple concept levels may lead to the discovery of more specific and concrete knowledge from data. In this study, a top-down progressive deepening method is developed for mining multiple-level association rules from large transaction databases by extension of some existing association rule mining techniques. A group of variant algorithms are proposed based on the ways of sharing intermediate results, with the relative performance tested on different kinds of data. Relaxation of the rule conditions for finding "level-crossing" association rules is also discussed in the paper.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> R. Agrawal, T. Imielinski, and A. Swami. </author> <title> Mining association rules between sets of items in large databases. </title> <booktitle> In Proc. 1993 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <pages> pp. 207-216, </pages> <address> Washington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data <ref> [1, 2, 3, 12] </ref>. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels. <p> To confine the association rules discovered to be strong ones, that is, the patterns which occur relatively frequently and the rules which demonstrate relatively strong implication relationships, the concepts of minimum support and minimum confidence have been introduced <ref> [1, 2] </ref>. Informally, the support of a pattern A in a set of transactions S is the probability that a transaction in S contains pattern A; and the confidence of A ! B in S is the probability that pattern B occurs in S if pattern A occurs in S. <p> To simplify our discussion, an abstract example which simulates the real life example of Example 2.1 is analyzed as follows. Example 3.1 As stated above, the taxonomy information for each (grouped) item in Example 2.1 is en coded as a sequence of digits in the transaction table T <ref> [1] </ref> (Table 4). For example, the item `2% Foremost milk' is encoded as `112' in which the first digit, `1', represents `milk' at level-1, the second, `1', for `2% (milk)' at level-2, and the third, `2', for the brand `Foremost' at level-3. <p> TID Items T 1 f111, 121, 211, 221g T 3 f112, 122, 221, 411g T 5 f111, 122, 211, 221, 413g T 7 f323, 411, 524, 713g Table 4: Encoded transaction table: T <ref> [1] </ref> The derivation of the large item sets at level 1 proceeds as follows. Let the minimum support be 4 transactions (i.e., minsup [1] = 4). (Notice since the total number of transactions is fixed, the support is expressed in an absolute value rather than a relative percentage for simplicity). <p> 211, 221g T 3 f112, 122, 221, 411g T 5 f111, 122, 211, 221, 413g T 7 f323, 411, 524, 713g Table 4: Encoded transaction table: T <ref> [1] </ref> The derivation of the large item sets at level 1 proceeds as follows. Let the minimum support be 4 transactions (i.e., minsup [1] = 4). (Notice since the total number of transactions is fixed, the support is expressed in an absolute value rather than a relative percentage for simplicity). The level-1 large 1-itemset table L [1; 1] can be derived by scanning T [1], registering support of each generalized item, such as 1flfl, <p> Let the minimum support be 4 transactions (i.e., minsup [1] = 4). (Notice since the total number of transactions is fixed, the support is expressed in an absolute value rather than a relative percentage for simplicity). The level-1 large 1-itemset table L <ref> [1; 1] </ref> can be derived by scanning T [1], registering support of each generalized item, such as 1flfl, : : : , 4flfl, if a transaction contains such an item (i.e., the item in the transaction belongs to the generalized item 1flfl, : : : , 4flfl, respectively), and filtering out <p> Let the minimum support be 4 transactions (i.e., minsup <ref> [1] </ref> = 4). (Notice since the total number of transactions is fixed, the support is expressed in an absolute value rather than a relative percentage for simplicity). The level-1 large 1-itemset table L [1; 1] can be derived by scanning T [1], registering support of each generalized item, such as 1flfl, : : : , 4flfl, if a transaction contains such an item (i.e., the item in the transaction belongs to the generalized item 1flfl, : : : , 4flfl, respectively), and filtering out those whose accumulated support count is lower than <p> L <ref> [1; 1] </ref> is then used to filter out (1) any item which is not large in a transaction, and (2) the transactions in T [1] which contain only small items. This results in a filtered transaction table T [2] of Figure 2. <p> L [1; 1] is then used to filter out (1) any item which is not large in a transaction, and (2) the transactions in T <ref> [1] </ref> which contain only small items. This results in a filtered transaction table T [2] of Figure 2. <p> Algorithm 3.1 (ML T2L1) Find multiple-level large item sets for mining strong ML association rules in a transaction database. Input: (1) T <ref> [1] </ref>, a hierarchy-information-encoded and task-relevant set of transaction database, in the format of hT ID; Itemseti, in which each item in the Itemset contains encoded concept hierarchy information, and (2) the minimum support threshold (minsup [l]) for each concept level l. Output: Multiple-level large item sets. <p> Starting at level 1, derive for each level l, the large k-items sets, L [l; k], for each k, and the large item set, LL [l] (for all k's), as follows (in the syntax similar to C and Pascal, which should be self-explanatory). (1) for (l := 1; L <ref> [l; 1] </ref> 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T [2] := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; <p> the large item set, LL [l] (for all k's), as follows (in the syntax similar to C and Pascal, which should be self-explanatory). (1) for (l := 1; L <ref> [l; 1] </ref> 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T [2] := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) <p> k's), as follows (in the syntax similar to C and Pascal, which should be self-explanatory). (1) for (l := 1; L [l; 1] 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T <ref> [1] </ref>; l); (4) T [2] := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate <p> should be self-explanatory). (1) for (l := 1; L [l; 1] 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T <ref> [1] </ref>; l); (4) T [2] := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k 1]); (9) foreach transaction t 2 T [2] <p> self-explanatory). (1) for (l := 1; L [l; 1] 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T [2] := get f iltered t table (T [1]; L <ref> [1; 1] </ref>); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k 1]); (9) foreach transaction t 2 T [2] do f <p> <ref> [l; 1] </ref> 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T [2] := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k 1]); (9) foreach transaction t 2 T [2] do f (10) C t := get subsets (C <p> According to Algorithm 3.1, the discovery of large support items at each level l proceeds as follows. 1. At level 1, the large 1-itemsets L <ref> [l; 1] </ref> is derived from T [1] by "get large 1 itemsets (T [1]; l)". At any other level l, L [l; 1] is derived from T [2] by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] <p> According to Algorithm 3.1, the discovery of large support items at each level l proceeds as follows. 1. At level 1, the large 1-itemsets L [l; 1] is derived from T <ref> [1] </ref> by "get large 1 itemsets (T [1]; l)". <p> According to Algorithm 3.1, the discovery of large support items at each level l proceeds as follows. 1. At level 1, the large 1-itemsets L [l; 1] is derived from T <ref> [1] </ref> by "get large 1 itemsets (T [1]; l)". <p> According to Algorithm 3.1, the discovery of large support items at each level l proceeds as follows. 1. At level 1, the large 1-itemsets L <ref> [l; 1] </ref> is derived from T [1] by "get large 1 itemsets (T [1]; l)". At any other level l, L [l; 1] is derived from T [2] by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. <p> At any other level l, L [l; 1] is derived from T [2] by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L <ref> [l 1; 1] </ref> will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. <p> any other level l, L <ref> [l; 1] </ref> is derived from T [2] by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. This is implemented by scanning the items of each transaction t in T [1] (or T [2]), incrementing the support count of an item i in the itemset if i's count has not been incremented by t. <p> This is implemented by scanning the items of each transaction t in T <ref> [1] </ref> (or T [2]), incrementing the support count of an item i in the itemset if i's count has not been incremented by t. After scanning the transaction table, filter out those items whose support is smaller than minsup [l]. 2. <p> After scanning the transaction table, filter out those items whose support is smaller than minsup [l]. 2. The filtered transaction table T [2] is derived by "get f iltered t table (T <ref> [1] </ref>; L [1; 1])", which uses L [1,1] as a filter to filter out (1) any item which is not large at level 1, and (2) the transactions which contain no large items. 3. <p> After scanning the transaction table, filter out those items whose support is smaller than minsup [l]. 2. The filtered transaction table T [2] is derived by "get f iltered t table (T [1]; L <ref> [1; 1] </ref>)", which uses L [1,1] as a filter to filter out (1) any item which is not large at level 1, and (2) the transactions which contain no large items. 3. <p> Algorithm ML T2L1 inherits several important optimization techniques developed in previous studies at finding association rules <ref> [1, 2] </ref>. For example, get candidate set of the large k-itemsets from the known large (k 1)-itemsets follows apriori-gen of Algorithm Apriori [2]. Function get subsets (C k ; t) is implemented by a hashing technique from [2]. <p> Generalization is first performed on a given item description relation to derive a generalized item table in which each tuple contains a set of item identifiers (such as bar codes) and is encoded with concept hierarchy information. 2. The transaction table T is transformed into T <ref> [1] </ref> with each item in the itemset replaced by its cor responding encoded hierarchy information. 3. A filtered transaction T [2] which filters out small items at the top level of T [1] using the large 1-itemsets L [1,1] is derived and used in the derivation of large k-items for any <p> The transaction table T is transformed into T <ref> [1] </ref> with each item in the itemset replaced by its cor responding encoded hierarchy information. 3. A filtered transaction T [2] which filters out small items at the top level of T [1] using the large 1-itemsets L [1,1] is derived and used in the derivation of large k-items for any k (k &gt; 1) at level-1 and for any k (k 1) for level l (l &gt; 1). 4. <p> From level l to level (l + 1), only large items at L <ref> [l; 1] </ref> are checked against T [2] for L [l + 1; 1]. <p> From level l to level (l + 1), only large items at L [l; 1] are checked against T [2] for L <ref> [l + 1; 1] </ref>. <p> From level l to level (l + 1), only large items at L [l; 1] are checked against T [2] for L [l + 1; 1]. Notice that in the processing, T <ref> [1] </ref> needs to be scanned twice, whereas T [2] needs to be scanned p times where p = l k l 1, and k l is the maximum k such that the k-itemset table is nonempty at level l. 4 Variations of the Algorithm for po tential performance improvement Potential performance <p> of Algorithm ML T2L1 are considered by exploration of the sharing of data structures and intermediate results and maximally generation of results at each database scan, etc. which leads to the following variations of the algorithm: (1) ML T1LA: using only one encoded transaction table (thus T1) and generating L <ref> [l; 1] </ref> for all the levels at one database scan (thus LA), (2) ML TML1: using multiple encoded transaction tables and generating L [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T [1] and T [2]) and generating L [l; 1] for all <p> each database scan, etc. which leads to the following variations of the algorithm: (1) ML T1LA: using only one encoded transaction table (thus T1) and generating L <ref> [l; 1] </ref> for all the levels at one database scan (thus LA), (2) ML TML1: using multiple encoded transaction tables and generating L [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T [1] and T [2]) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], <p> only one encoded transaction table (thus T1) and generating L [l; 1] for all the levels at one database scan (thus LA), (2) ML TML1: using multiple encoded transaction tables and generating L [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T <ref> [1] </ref> and T [2]) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], that is, no filtered encoded transaction table T [2] will be generated in the processing. <p> and generating L <ref> [l; 1] </ref> for all the levels at one database scan (thus LA), (2) ML TML1: using multiple encoded transaction tables and generating L [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T [1] and T [2]) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], that is, no filtered encoded transaction table T [2] will be generated in the processing. <p> [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T <ref> [1] </ref> and T [2]) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], that is, no filtered encoded transaction table T [2] will be generated in the processing. At the first scan of T [1], large 1-itemsets L [l; 1] for every level l can be generated in parallel, because the scan of an item i in each transaction t may increase the <p> generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T <ref> [1] </ref>, that is, no filtered encoded transaction table T [2] will be generated in the processing. At the first scan of T [1], large 1-itemsets L [l; 1] for every level l can be generated in parallel, because the scan of an item i in each transaction t may increase the count of the item in every L [l; 1] if its has not been incremented by t. <p> At the first scan of T [1], large 1-itemsets L <ref> [l; 1] </ref> for every level l can be generated in parallel, because the scan of an item i in each transaction t may increase the count of the item in every L [l; 1] if its has not been incremented by t. <p> At the first scan of T [1], large 1-itemsets L <ref> [l; 1] </ref> for every level l can be generated in parallel, because the scan of an item i in each transaction t may increase the count of the item in every L [l; 1] if its has not been incremented by t. After the scanning of T [1], each item in L [l; 1] whose parent (if l &gt; 1) is not a large item in the higher level large 1-itemsets or whose support is lower than minsup [l] will be removed from <p> After the scanning of T <ref> [1] </ref>, each item in L [l; 1] whose parent (if l &gt; 1) is not a large item in the higher level large 1-itemsets or whose support is lower than minsup [l] will be removed from L [l; 1]. <p> for every level l can be generated in parallel, because the scan of an item i in each transaction t may increase the count of the item in every L <ref> [l; 1] </ref> if its has not been incremented by t. After the scanning of T [1], each item in L [l; 1] whose parent (if l &gt; 1) is not a large item in the higher level large 1-itemsets or whose support is lower than minsup [l] will be removed from L [l; 1]. <p> After the scanning of T [1], each item in L <ref> [l; 1] </ref> whose parent (if l &gt; 1) is not a large item in the higher level large 1-itemsets or whose support is lower than minsup [l] will be removed from L [l; 1]. After the generation of large 1-itemsets for each level l, the candidate set for large 2-itemsets for each level l can be generated by the apriori-gen algorithm [2]. <p> The get subsets function will be processed against the candidate sets at all the levels at the same time by scanning T <ref> [1] </ref> once, which calculates the support for each candidate itemset and generates large 2-itemsets L [l; 2]. Similar processes can be processed for step-by step generation of large k-item-sets L [l; k] for k &gt; 2. This algorithm avoids the generation of a new encoded transaction table. <p> Similar processes can be processed for step-by step generation of large k-item-sets L [l; k] for k &gt; 2. This algorithm avoids the generation of a new encoded transaction table. Moreover, it needs to scan T <ref> [1] </ref> once for generation of each large k-itemset table. Since the total number of scanning of T [1] will be k times for the largest k-itemsets, it is a potentially efficient algorithm. However, T [1] may consist of many small items which could be wasteful to be scanned or examined. <p> This algorithm avoids the generation of a new encoded transaction table. Moreover, it needs to scan T <ref> [1] </ref> once for generation of each large k-itemset table. Since the total number of scanning of T [1] will be k times for the largest k-itemsets, it is a potentially efficient algorithm. However, T [1] may consist of many small items which could be wasteful to be scanned or examined. Also, it needs a large space to keep all C [l] which may cause some page swapping. <p> Moreover, it needs to scan T <ref> [1] </ref> once for generation of each large k-itemset table. Since the total number of scanning of T [1] will be k times for the largest k-itemsets, it is a potentially efficient algorithm. However, T [1] may consist of many small items which could be wasteful to be scanned or examined. Also, it needs a large space to keep all C [l] which may cause some page swapping. <p> Example 4.1 The execution of the same task as Example 3.1 using Algorithm ML T1LA will generate the same large item sets L [l; k] for all the l's and k's but in difference sequences (without generating and using T [2]). It first generates large 1-itemsets L <ref> [l; 1] </ref> for all the l's from T [1]. Then it generates the candidate sets Page 6 from L [l; 1], and then derives large 2-itemsets L [l; 2] by passing the candidate sets through T [1] to obtain the support count and filter those smaller than minsup [l]. <p> It first generates large 1-itemsets L [l; 1] for all the l's from T <ref> [1] </ref>. Then it generates the candidate sets Page 6 from L [l; 1], and then derives large 2-itemsets L [l; 2] by passing the candidate sets through T [1] to obtain the support count and filter those smaller than minsup [l]. <p> It first generates large 1-itemsets L <ref> [l; 1] </ref> for all the l's from T [1]. Then it generates the candidate sets Page 6 from L [l; 1], and then derives large 2-itemsets L [l; 2] by passing the candidate sets through T [1] to obtain the support count and filter those smaller than minsup [l]. <p> It first generates large 1-itemsets L [l; 1] for all the l's from T <ref> [1] </ref>. Then it generates the candidate sets Page 6 from L [l; 1], and then derives large 2-itemsets L [l; 2] by passing the candidate sets through T [1] to obtain the support count and filter those smaller than minsup [l]. This process repeats to find k-itemsets for larger k until all the large k-itemsets have been derived. 2 4.2 Algorithm ML TML1 The second variation is to generate multiple encoded transaction tables T [1], T [2], : : <p> candidate sets through T <ref> [1] </ref> to obtain the support count and filter those smaller than minsup [l]. This process repeats to find k-itemsets for larger k until all the large k-itemsets have been derived. 2 4.2 Algorithm ML TML1 The second variation is to generate multiple encoded transaction tables T [1], T [2], : : : , T [max l + 1], where max l is the maximal level number to be examined in the processing. Similar to Algorithm ML T2L1, the first scan of T [1] generates the large 1-itemsets L [1; 1] which then serves as a filter to <p> Algorithm ML TML1 The second variation is to generate multiple encoded transaction tables T <ref> [1] </ref>, T [2], : : : , T [max l + 1], where max l is the maximal level number to be examined in the processing. Similar to Algorithm ML T2L1, the first scan of T [1] generates the large 1-itemsets L [1; 1] which then serves as a filter to filter out from T [1] any small items or transactions containing only small items. T [2] is resulted from this filtering process and is used in the generation of large k-itemsets at level 1. <p> Similar to Algorithm ML T2L1, the first scan of T [1] generates the large 1-itemsets L <ref> [1; 1] </ref> which then serves as a filter to filter out from T [1] any small items or transactions containing only small items. T [2] is resulted from this filtering process and is used in the generation of large k-itemsets at level 1. <p> Similar to Algorithm ML T2L1, the first scan of T <ref> [1] </ref> generates the large 1-itemsets L [1; 1] which then serves as a filter to filter out from T [1] any small items or transactions containing only small items. T [2] is resulted from this filtering process and is used in the generation of large k-itemsets at level 1. Different from Algorithm ML T2L1, T [2] is not repeatedly used in the processing of the lower levels. <p> Instead, a new table T [l + 1] is generated at the processing of each level l, for l &gt; 1. This is done by scanning T [l] to generate the large 1-itemsets L <ref> [l; 1] </ref> which serves as a filter to filter out from T [l] any small items or transactions containing only small items and results in T [l+1] which will be used for the generation of large k-itemsets (for k &gt; 1) at level l and table T [l + 2] at <p> Notice that as an optimization, for each level l &gt; 1, T [l] and L <ref> [l; 1] </ref> can be generated in parallel (i.e., at the same scan). The algorithm derives a new filtered transaction table, T [l + 1], at the processing of each level l. <p> It first generates the large 1-itemsets L <ref> [1; 1] </ref> for level 1. <p> It first generates the large 1-itemsets L [1; 1] for level 1. Then for each level l (initially l = 1), it generates the filtered transaction table T [l + 1] and the level-(l + 1) large 1-itemsets L <ref> [l + 1; 1] </ref> by scanning T [l] using L [l; 1], and then generates the candidate 2-itemsets from L [l; 1], calculates the supports using T [l + 1], filters those with support less than minsup [l], and derives L [l; 2]. <p> It first generates the large 1-itemsets L [1; 1] for level 1. Then for each level l (initially l = 1), it generates the filtered transaction table T [l + 1] and the level-(l + 1) large 1-itemsets L [l + 1; 1] by scanning T [l] using L <ref> [l; 1] </ref>, and then generates the candidate 2-itemsets from L [l; 1], calculates the supports using T [l + 1], filters those with support less than minsup [l], and derives L [l; 2]. <p> Then for each level l (initially l = 1), it generates the filtered transaction table T [l + 1] and the level-(l + 1) large 1-itemsets L [l + 1; 1] by scanning T [l] using L <ref> [l; 1] </ref>, and then generates the candidate 2-itemsets from L [l; 1], calculates the supports using T [l + 1], filters those with support less than minsup [l], and derives L [l; 2]. <p> The process repeats for the derivation of L [l; 3], : : : , L [l; k]. 2 4.3 Algorithm ML T2LA The third variation uses the same two encoded transaction tables T <ref> [1] </ref> and T [2] as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T <p> [l; 3], : : : , L [l; k]. 2 4.3 Algorithm ML T2LA The third variation uses the same two encoded transaction tables T <ref> [1] </ref> and T [2] as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the <p> The scan of T [1] first generates large 1-itemsets L <ref> [1; 1] </ref>. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the count of every L [l; 1] <p> The third variation uses the same two encoded transaction tables T <ref> [1] </ref> and T [2] as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing <p> The scan of T [1] first generates large 1-itemsets L <ref> [1; 1] </ref>. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and <p> The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L <ref> [l; 1] </ref> for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and the items whose parent is small from L [l; 1] at the end of the scan of T [1]. <p> Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L <ref> [l; 1] </ref> for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and the items whose parent is small from L [l; 1] at the end of the scan of T [1]. <p> [2] and all the large 1-itemset tables for all the remaining levels, i.e., L <ref> [l; 1] </ref> for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and the items whose parent is small from L [l; 1] at the end of the scan of T [1]. <p> remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and the items whose parent is small from L [l; 1] at the end of the scan of T <ref> [1] </ref>. Then the candidate set for the large 2-itemsets at each level l can be generated by the apriori-gen algorithm [2], and the get subsets routine will extract the candidate sets for all the level l (l 1) at the same time by scanning T [2] once. <p> Similar processes proceed step-by-step which generates large k-item-sets L [l; k] for k &gt; 2 using the same T [2]. This algorithm avoids the generation of a group of new filtered transaction tables. It scans T <ref> [1] </ref> twice to generate T [2] and the large 1-itemset tables for all the levels. <p> Since k-itemsets generation for k &gt; 1 is performed on T [2] which may consist of much less items than T <ref> [1] </ref>, the algorithm could be a potentially efficient one. Example 4.3 The execution of the same task as Example 3.1 using Algorithm ML T2LA will generate the same large itemsets L [l; k] for all the l's and k's. It first generates large 1-itemsets L [l; 1] from T [1], then <p> Example 4.3 The execution of the same task as Example 3.1 using Algorithm ML T2LA will generate the same large itemsets L [l; k] for all the l's and k's. It first generates large 1-itemsets L <ref> [l; 1] </ref> from T [1], then T [2] and all the large 1-itemsets L [2; 1], : : : , L [max l; 1], where max l is the maximum level to be explored. Then it generates the candidate sets from L [l; 1], and derives large 2-itemsets L [l; 2] <p> T <ref> [1] </ref>, the algorithm could be a potentially efficient one. Example 4.3 The execution of the same task as Example 3.1 using Algorithm ML T2LA will generate the same large itemsets L [l; k] for all the l's and k's. It first generates large 1-itemsets L [l; 1] from T [1], then T [2] and all the large 1-itemsets L [2; 1], : : : , L [max l; 1], where max l is the maximum level to be explored. <p> It first generates large 1-itemsets L [l; 1] from T [1], then T [2] and all the large 1-itemsets L <ref> [2; 1] </ref>, : : : , L [max l; 1], where max l is the maximum level to be explored. <p> It first generates large 1-itemsets L [l; 1] from T [1], then T [2] and all the large 1-itemsets L [2; 1], : : : , L <ref> [max l; 1] </ref>, where max l is the maximum level to be explored. <p> It first generates large 1-itemsets L <ref> [l; 1] </ref> from T [1], then T [2] and all the large 1-itemsets L [2; 1], : : : , L [max l; 1], where max l is the maximum level to be explored. Then it generates the candidate sets from L [l; 1], and derives large 2-itemsets L [l; 2] by testing the candidate sets against T [2] to obtain the support count and filter those with count smaller than minsup [l]. <p> Database S T # of transactions Size (MBytes) DB1 2 5 100,000 2.7MB Table 5: Transaction databases Each transaction database is converted into an encoded transaction table, denoted as T <ref> [1] </ref>, according to the information about the generalized items in the item description (hierarchy) table. The maximal level of the concept hierarchy in the item table is set to 4. The number of the top level nodes keeps increasing until the total number of items reaches 1000. <p> On the other hand, M L T 1LA is the worst since it consults a large T <ref> [1] </ref> at every level. M L T M L1 the worst among the four algorithms under the setting: a different test database T 5, the same item set I1, and with the minimum support thresholds: (20; 8; 2; 1). <p> Since M L T 2LA scans T <ref> [1] </ref> twice and needs to maintain all large itemsets L [l; k] at the same time, it is outperformed by M L T 2L1 when the thresholds are big enough so that a substantial amount of T [1] is cut and the maximal length of large itemsets at each level is <p> Since M L T 2LA scans T <ref> [1] </ref> twice and needs to maintain all large itemsets L [l; k] at the same time, it is outperformed by M L T 2L1 when the thresholds are big enough so that a substantial amount of T [1] is cut and the maximal length of large itemsets at each level is small. Moreover, one may observe the significant performance degradation from thre4 to thre5. <p> Let minimum support at each level be: minsup = 4 at level-1, and minsup = 3 at levels 2 and 3. The derivation of the large itemsets at level 1 proceeds in the same way as in Example 3.1, which generates the same large itemsets tables L <ref> [1; 1] </ref> and L [1; 2] at level 1 and the same filtered transaction table T [2], as shown in Figure 2. The derivation of level-2 large itemsets generates the same large 1-itemsets L [2; 1] as shown in Figure 9. <p> The derivation of the large itemsets at level 1 proceeds in the same way as in Example 3.1, which generates the same large itemsets tables L [1; 1] and L <ref> [1; 2] </ref> at level 1 and the same filtered transaction table T [2], as shown in Figure 2. The derivation of level-2 large itemsets generates the same large 1-itemsets L [2; 1] as shown in Figure 9. <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% milk"), or f11fl, 2flflg (for potential associations like "2% milk ! bread"). <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% milk"), or f11fl, 2flflg (for potential associations like "2% milk ! bread"). <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L [2; 1] as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L <ref> [1; 1] </ref> as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% milk"), or f11fl, 2flflg (for potential associations like "2% milk ! bread"). <p> Similarly, the itemsets f111, 11flg and f111, 1flflg have the same support as f111g in L <ref> [3, 1] </ref> and are thus not included in L [3,2]. Since the large k-itemset (for k &gt; 1) tables do not explicitly include the pairs of items with their own ancestors, attention should be paid to include them at the generation of association rules.
Reference: [2] <author> R. Agrawal and R. Srikant. </author> <title> Fast algorithms for mining association rules. </title> <booktitle> In Proc. 1994 Int. Conf. Very Large Data Bases, </booktitle> <pages> pp. 487-499, </pages> <address> Santiago, Chile, </address> <month> Sept. </month> <year> 1994. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data <ref> [1, 2, 3, 12] </ref>. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels. <p> This requires progressively deepening the knowledge mining process for finding refined knowledge from data. The necessity for mining multiple level association rules or using taxonomy information at mining association rules has also been observed by other researchers, e.g., <ref> [2] </ref>. To confine the association rules discovered to be strong ones, that is, the patterns which occur relatively frequently and the rules which demonstrate relatively strong implication relationships, the concepts of minimum support and minimum confidence have been introduced [1, 2]. <p> To confine the association rules discovered to be strong ones, that is, the patterns which occur relatively frequently and the rules which demonstrate relatively strong implication relationships, the concepts of minimum support and minimum confidence have been introduced <ref> [1, 2] </ref>. Informally, the support of a pattern A in a set of transactions S is the probability that a transaction in S contains pattern A; and the confidence of A ! B in S is the probability that pattern B occurs in S if pattern A occurs in S. <p> For example, the item `2% Foremost milk' is encoded as `112' in which the first digit, `1', represents `milk' at level-1, the second, `1', for `2% (milk)' at level-2, and the third, `2', for the brand `Foremost' at level-3. Similar to <ref> [2] </ref>, repeated items (i.e., items with the same encoding) at any level will be treated as one item in one transaction. <p> L [1; 1] is then used to filter out (1) any item which is not large in a transaction, and (2) the transactions in T [1] which contain only small items. This results in a filtered transaction table T <ref> [2] </ref> of Figure 2. Moreover, since there are only two entries in L [1,1], the level-1 large-2 itemset table L [1,2] may contain only 1 candidate item f1flfl, 2flflg, which is supported by 4 transactions in T [2]. <p> This results in a filtered transaction table T <ref> [2] </ref> of Figure 2. Moreover, since there are only two entries in L [1,1], the level-1 large-2 itemset table L [1,2] may contain only 1 candidate item f1flfl, 2flflg, which is supported by 4 transactions in T [2]. Level-1 minsup = 4 Level-1 large 1-itemsets: L [1,1] Itemset Support f1flflg 5 f2flflg 5 Level-1 large 2-itemsets: L [1,2] Itemset Support f1flfl, 2flflg 4 Filtered transaction table: T [2] TID Items T 1 f111, 121, 211, 221g T 3 f112, 122, 221g T 5 f111, 122, 211, 221g action <p> itemset table L [1,2] may contain only 1 candidate item f1flfl, 2flflg, which is supported by 4 transactions in T <ref> [2] </ref>. Level-1 minsup = 4 Level-1 large 1-itemsets: L [1,1] Itemset Support f1flflg 5 f2flflg 5 Level-1 large 2-itemsets: L [1,2] Itemset Support f1flfl, 2flflg 4 Filtered transaction table: T [2] TID Items T 1 f111, 121, 211, 221g T 3 f112, 122, 221g T 5 f111, 122, 211, 221g action table: T [2] According to the definition of ML-association rules, Page 4 Level-2 minsup = 3 Level-2 large 1-itemsets: L [2,1] Itemset Support f11flg 5 f21flg 4 Level-2 large 2-itemsets: <p> = 4 Level-1 large 1-itemsets: L [1,1] Itemset Support f1flflg 5 f2flflg 5 Level-1 large 2-itemsets: L [1,2] Itemset Support f1flfl, 2flflg 4 Filtered transaction table: T <ref> [2] </ref> TID Items T 1 f111, 121, 211, 221g T 3 f112, 122, 221g T 5 f111, 122, 211, 221g action table: T [2] According to the definition of ML-association rules, Page 4 Level-2 minsup = 3 Level-2 large 1-itemsets: L [2,1] Itemset Support f11flg 5 f21flg 4 Level-2 large 2-itemsets: L [2,2] Itemset Support f11fl, 12flg 4 f11fl, 22flg 4 f21fl, 22flg 3 Level-2 large 3-itemsets: L [2,3] Itemset Support f11fl, 12fl, 22flg <p> Let minsup <ref> [2] </ref> = 3. The level-2 large 1-itemsets L [2,1] can be derived from the filtered transaction table T [2] by accumulating the support count and removing those whose support is smaller than the minimum support, which results in L [2,1] of Figure 3. <p> Let minsup <ref> [2] </ref> = 3. The level-2 large 1-itemsets L [2,1] can be derived from the filtered transaction table T [2] by accumulating the support count and removing those whose support is smaller than the minimum support, which results in L [2,1] of Figure 3. Similarly, the large 2-itemset table L [2,2] is formed by the combinations of the entries in L [2,1], together with the support derived from T [2], <p> <ref> [2] </ref> by accumulating the support count and removing those whose support is smaller than the minimum support, which results in L [2,1] of Figure 3. Similarly, the large 2-itemset table L [2,2] is formed by the combinations of the entries in L [2,1], together with the support derived from T [2], filtered using the corresponding threshold. The large 3-itemset table L [2,3] is formed by the combinations of the entries in L [2,2] (which has only one possibility f11fl, 12fl, 22flg), and a similar process. <p> the syntax similar to C and Pascal, which should be self-explanatory). (1) for (l := 1; L [l; 1] 6= ; and l &lt; max level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T <ref> [2] </ref> := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k <p> level; l++) do f (2) if l = 1 then f (3) L [l; 1] := get large 1 itemsets (T [1]; l); (4) T <ref> [2] </ref> := get f iltered t table (T [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T [2]; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k 1]); (9) foreach transaction t 2 T [2] do f (10) C t := get subsets (C k ; t); (11) foreach candidate c 2 <p> [1]; L [1; 1]); (5) g (6) else L [l; 1] := get large 1 itemsets (T <ref> [2] </ref>; l); (7) for (k := 2; L [l; k 1] 6= ;; k++) do f (8) C k := get candidate set (L [l; k 1]); (9) foreach transaction t 2 T [2] do f (10) C t := get subsets (C k ; t); (11) foreach candidate c 2 C t do c.support++; (12) g (13) L [l; k] := fc 2 C k jc:support minsup [l]g (14) g S (16) g 2 Explanation of Algorithm 3.1. <p> At level 1, the large 1-itemsets L [l; 1] is derived from T [1] by "get large 1 itemsets (T [1]; l)". At any other level l, L [l; 1] is derived from T <ref> [2] </ref> by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. <p> At level 1, the large 1-itemsets L [l; 1] is derived from T [1] by "get large 1 itemsets (T [1]; l)". At any other level l, L [l; 1] is derived from T <ref> [2] </ref> by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. This is implemented by scanning the items of each transaction t in T [1] (or T [2]), <p> At any other level l, L [l; 1] is derived from T <ref> [2] </ref> by "get large 1 itemsets (T [2]; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. This is implemented by scanning the items of each transaction t in T [1] (or T [2]), incrementing the support count of an item i in the itemset if i's count has not been incremented by t. <p> <ref> [2] </ref>; l)", and notice that when l &gt; 2, only the item in L [l 1; 1] will be considered when examining T [2] in the derivation of the large 1-itemsets L [l; 1]. This is implemented by scanning the items of each transaction t in T [1] (or T [2]), incrementing the support count of an item i in the itemset if i's count has not been incremented by t. After scanning the transaction table, filter out those items whose support is smaller than minsup [l]. 2. The filtered transaction table T [2] is derived by "get f iltered t <p> transaction t in T [1] (or T <ref> [2] </ref>), incrementing the support count of an item i in the itemset if i's count has not been incremented by t. After scanning the transaction table, filter out those items whose support is smaller than minsup [l]. 2. The filtered transaction table T [2] is derived by "get f iltered t table (T [1]; L [1; 1])", which uses L [1,1] as a filter to filter out (1) any item which is not large at level 1, and (2) the transactions which contain no large items. 3. <p> The large k (for k &gt; 1) item set table at level l is derived in two steps: (a) Compute the candidate set from L [l; k1], as done in the apriori candidate generation algorithm <ref> [2] </ref>, apriori-gen, i.e., it first generates a set C k in which each item set consists of k items, derived by joining two (k 1) items Page 5 in L [l; k] which share (k 2) items, and then removes a k-itemset c from C k if there exists a c's <p> derived by joining two (k 1) items Page 5 in L [l; k] which share (k 2) items, and then removes a k-itemset c from C k if there exists a c's (k 1) subset which is not in L [l; k 1]. (b) For each transaction t in T <ref> [2] </ref>, for each of t's k-item subset c, increment c's support count if c is in the candidate set C k . Then collect into L [l; k] each c (together with its support) if its support is no less than minsup [l]. 4. <p> This is performed as follows <ref> [2] </ref>. For every large itemset r, if a is a nonempty subset of r, the rule "a! r a" is inserted into rule set [l] if support (r)=support (a) minconf [l], where min-conf [l] is the minimum confidence at level l. <p> Algorithm ML T2L1 inherits several important optimization techniques developed in previous studies at finding association rules <ref> [1, 2] </ref>. For example, get candidate set of the large k-itemsets from the known large (k 1)-itemsets follows apriori-gen of Algorithm Apriori [2]. Function get subsets (C k ; t) is implemented by a hashing technique from [2]. <p> Algorithm ML T2L1 inherits several important optimization techniques developed in previous studies at finding association rules [1, 2]. For example, get candidate set of the large k-itemsets from the known large (k 1)-itemsets follows apriori-gen of Algorithm Apriori <ref> [2] </ref>. Function get subsets (C k ; t) is implemented by a hashing technique from [2]. Moreover, to accomplish the new task of mining multiple-level association rules, some interesting optimization techniques have been developed, as illustrated below. 1. <p> For example, get candidate set of the large k-itemsets from the known large (k 1)-itemsets follows apriori-gen of Algorithm Apriori <ref> [2] </ref>. Function get subsets (C k ; t) is implemented by a hashing technique from [2]. Moreover, to accomplish the new task of mining multiple-level association rules, some interesting optimization techniques have been developed, as illustrated below. 1. <p> The transaction table T is transformed into T [1] with each item in the itemset replaced by its cor responding encoded hierarchy information. 3. A filtered transaction T <ref> [2] </ref> which filters out small items at the top level of T [1] using the large 1-itemsets L [1,1] is derived and used in the derivation of large k-items for any k (k &gt; 1) at level-1 and for any k (k 1) for level l (l &gt; 1). 4. <p> From level l to level (l + 1), only large items at L [l; 1] are checked against T <ref> [2] </ref> for L [l + 1; 1]. Notice that in the processing, T [1] needs to be scanned twice, whereas T [2] needs to be scanned p times where p = l k l 1, and k l is the maximum k such that the k-itemset table is nonempty at level <p> From level l to level (l + 1), only large items at L [l; 1] are checked against T <ref> [2] </ref> for L [l + 1; 1]. Notice that in the processing, T [1] needs to be scanned twice, whereas T [2] needs to be scanned p times where p = l k l 1, and k l is the maximum k such that the k-itemset table is nonempty at level l. 4 Variations of the Algorithm for po tential performance improvement Potential performance improvements of Algorithm ML T2L1 are considered by <p> transaction table (thus T1) and generating L [l; 1] for all the levels at one database scan (thus LA), (2) ML TML1: using multiple encoded transaction tables and generating L [l; 1] for one corresponding concept level, and (3) ML T2LA: using two encoded transaction tables (T [1] and T <ref> [2] </ref>) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], that is, no filtered encoded transaction table T [2] will be generated in the processing. <p> ML T2LA: using two encoded transaction tables (T [1] and T <ref> [2] </ref>) and generating L [l; 1] for all the levels at one database scan. 4.1 Algorithm ML T1LA The first variation is to use only one encoded transaction table T [1], that is, no filtered encoded transaction table T [2] will be generated in the processing. <p> After the generation of large 1-itemsets for each level l, the candidate set for large 2-itemsets for each level l can be generated by the apriori-gen algorithm <ref> [2] </ref>. The get subsets function will be processed against the candidate sets at all the levels at the same time by scanning T [1] once, which calculates the support for each candidate itemset and generates large 2-itemsets L [l; 2]. <p> The get subsets function will be processed against the candidate sets at all the levels at the same time by scanning T [1] once, which calculates the support for each candidate itemset and generates large 2-itemsets L <ref> [l; 2] </ref>. Similar processes can be processed for step-by step generation of large k-item-sets L [l; k] for k &gt; 2. This algorithm avoids the generation of a new encoded transaction table. Moreover, it needs to scan T [1] once for generation of each large k-itemset table. <p> Example 4.1 The execution of the same task as Example 3.1 using Algorithm ML T1LA will generate the same large item sets L [l; k] for all the l's and k's but in difference sequences (without generating and using T <ref> [2] </ref>). It first generates large 1-itemsets L [l; 1] for all the l's from T [1]. <p> It first generates large 1-itemsets L [l; 1] for all the l's from T [1]. Then it generates the candidate sets Page 6 from L [l; 1], and then derives large 2-itemsets L <ref> [l; 2] </ref> by passing the candidate sets through T [1] to obtain the support count and filter those smaller than minsup [l]. <p> This process repeats to find k-itemsets for larger k until all the large k-itemsets have been derived. 2 4.2 Algorithm ML TML1 The second variation is to generate multiple encoded transaction tables T [1], T <ref> [2] </ref>, : : : , T [max l + 1], where max l is the maximal level number to be examined in the processing. <p> Similar to Algorithm ML T2L1, the first scan of T [1] generates the large 1-itemsets L [1; 1] which then serves as a filter to filter out from T [1] any small items or transactions containing only small items. T <ref> [2] </ref> is resulted from this filtering process and is used in the generation of large k-itemsets at level 1. Different from Algorithm ML T2L1, T [2] is not repeatedly used in the processing of the lower levels. <p> T <ref> [2] </ref> is resulted from this filtering process and is used in the generation of large k-itemsets at level 1. Different from Algorithm ML T2L1, T [2] is not repeatedly used in the processing of the lower levels. Instead, a new table T [l + 1] is generated at the processing of each level l, for l &gt; 1. <p> Example 4.2 The execution of the same task as Example 3.1 using Algorithm ML TML1 will generate the same large itemsets L [l; k] for all the l's and k's but in difference sequences, with the generation and help of the filtered transaction tables T <ref> [2] </ref>, : : : , T [max l + 1], where max l is the maximum level explored in the algorithm. It first generates the large 1-itemsets L [1; 1] for level 1. <p> 1] and the level-(l + 1) large 1-itemsets L [l + 1; 1] by scanning T [l] using L [l; 1], and then generates the candidate 2-itemsets from L [l; 1], calculates the supports using T [l + 1], filters those with support less than minsup [l], and derives L <ref> [l; 2] </ref>. <p> The process repeats for the derivation of L [l; 3], : : : , L [l; k]. 2 4.3 Algorithm ML T2LA The third variation uses the same two encoded transaction tables T [1] and T <ref> [2] </ref> as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all <p> and T <ref> [2] </ref> as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. The scan of T [1] first generates large 1-itemsets L [1; 1]. Then one more scan of T [1] using L [1; 1] will generate a filtered transaction table T [2] and all the large 1-itemset tables for all the remaining levels, i.e., L [l; 1] for 1 &lt; l max l by incrementing the count of every L [l; 1] at the scan of each transaction and removing small items and the items whose parent is small from L [l; <p> Then the candidate set for the large 2-itemsets at each level l can be generated by the apriori-gen algorithm <ref> [2] </ref>, and the get subsets routine will extract the candidate sets for all the level l (l 1) at the same time by scanning T [2] once. This will calculate the support for each candidate itemset and generate large 2-item-sets L [l; 2] for l 1. <p> Then the candidate set for the large 2-itemsets at each level l can be generated by the apriori-gen algorithm <ref> [2] </ref>, and the get subsets routine will extract the candidate sets for all the level l (l 1) at the same time by scanning T [2] once. This will calculate the support for each candidate itemset and generate large 2-item-sets L [l; 2] for l 1. Similar processes proceed step-by-step which generates large k-item-sets L [l; k] for k &gt; 2 using the same T [2]. <p> This will calculate the support for each candidate itemset and generate large 2-item-sets L <ref> [l; 2] </ref> for l 1. Similar processes proceed step-by-step which generates large k-item-sets L [l; k] for k &gt; 2 using the same T [2]. This algorithm avoids the generation of a group of new filtered transaction tables. <p> l (l 1) at the same time by scanning T <ref> [2] </ref> once. This will calculate the support for each candidate itemset and generate large 2-item-sets L [l; 2] for l 1. Similar processes proceed step-by-step which generates large k-item-sets L [l; k] for k &gt; 2 using the same T [2]. This algorithm avoids the generation of a group of new filtered transaction tables. It scans T [1] twice to generate T [2] and the large 1-itemset tables for all the levels. Then it scans T [2] once for the generation of each large k-itemset, and thus scans T [2] in <p> Similar processes proceed step-by-step which generates large k-item-sets L [l; k] for k &gt; 2 using the same T <ref> [2] </ref>. This algorithm avoids the generation of a group of new filtered transaction tables. It scans T [1] twice to generate T [2] and the large 1-itemset tables for all the levels. Then it scans T [2] once for the generation of each large k-itemset, and thus scans T [2] in total k 1 times for the generation of all the k-itemsets, where k is the largest such k-itemsets available. <p> generates large k-item-sets L [l; k] for k &gt; 2 using the same T <ref> [2] </ref>. This algorithm avoids the generation of a group of new filtered transaction tables. It scans T [1] twice to generate T [2] and the large 1-itemset tables for all the levels. Then it scans T [2] once for the generation of each large k-itemset, and thus scans T [2] in total k 1 times for the generation of all the k-itemsets, where k is the largest such k-itemsets available. Since k-itemsets generation for k &gt; 1 is performed on T [2] which may consist of much <p> T <ref> [2] </ref>. This algorithm avoids the generation of a group of new filtered transaction tables. It scans T [1] twice to generate T [2] and the large 1-itemset tables for all the levels. Then it scans T [2] once for the generation of each large k-itemset, and thus scans T [2] in total k 1 times for the generation of all the k-itemsets, where k is the largest such k-itemsets available. Since k-itemsets generation for k &gt; 1 is performed on T [2] which may consist of much less items than T [1], the algorithm could be a potentially efficient one. <p> Then it scans T <ref> [2] </ref> once for the generation of each large k-itemset, and thus scans T [2] in total k 1 times for the generation of all the k-itemsets, where k is the largest such k-itemsets available. Since k-itemsets generation for k &gt; 1 is performed on T [2] which may consist of much less items than T [1], the algorithm could be a potentially efficient one. Example 4.3 The execution of the same task as Example 3.1 using Algorithm ML T2LA will generate the same large itemsets L [l; k] for all the l's and k's. <p> Example 4.3 The execution of the same task as Example 3.1 using Algorithm ML T2LA will generate the same large itemsets L [l; k] for all the l's and k's. It first generates large 1-itemsets L [l; 1] from T [1], then T <ref> [2] </ref> and all the large 1-itemsets L [2; 1], : : : , L [max l; 1], where max l is the maximum level to be explored. <p> It first generates large 1-itemsets L [l; 1] from T [1], then T [2] and all the large 1-itemsets L <ref> [2; 1] </ref>, : : : , L [max l; 1], where max l is the maximum level to be explored. <p> Then it generates the candidate sets from L [l; 1], and derives large 2-itemsets L <ref> [l; 2] </ref> by testing the candidate sets against T [2] to obtain the support count and filter those with count smaller than minsup [l]. <p> Then it generates the candidate sets from L [l; 1], and derives large 2-itemsets L [l; 2] by testing the candidate sets against T <ref> [2] </ref> to obtain the support count and filter those with count smaller than minsup [l]. <p> Page 7 The testbed consists of a set of synthetic transac-tion databases generated using a randomized item set generation algorithm similar to that described in <ref> [2] </ref>. The following are the basic parameters of the generated synthetic transaction databases: (1) the total number of items, I, is 1000; (2) the total number of transactions is 100,000; and (3) 2000 potentially large itemsets are generated and put into the transactions based on some distribution. <p> This can be explained as follows. Since the first threshold filters out many small 1-itemsets at level 1 which results in a much smaller filtered transaction table T <ref> [2] </ref>, but the later filter is not so strong and parallel derivation of L [l; k] without derivation of T [3] and T [4] is more beneficial, thus leads M L T 2LA to be the best algorithm. <p> This is because the first threshold filters out few small 1-itemsets at level 1 which results in almost the same sized transaction table T <ref> [2] </ref>. The generation of multiple filtered transaction tables is largely wasted, which leads the worst performance of M L T M L1. <p> This is because the first threshold filters out relatively more 1-itemsets at level 1 which results in small transaction table T <ref> [2] </ref>. Thus the generation of multiple filtered transaction tables is relatively beneficial. <p> The derivation of the large itemsets at level 1 proceeds in the same way as in Example 3.1, which generates the same large itemsets tables L [1; 1] and L <ref> [1; 2] </ref> at level 1 and the same filtered transaction table T [2], as shown in Figure 2. The derivation of level-2 large itemsets generates the same large 1-itemsets L [2; 1] as shown in Figure 9. <p> The derivation of the large itemsets at level 1 proceeds in the same way as in Example 3.1, which generates the same large itemsets tables L [1; 1] and L [1; 2] at level 1 and the same filtered transaction table T <ref> [2] </ref>, as shown in Figure 2. The derivation of level-2 large itemsets generates the same large 1-itemsets L [2; 1] as shown in Figure 9. <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% milk"), or f11fl, 2flflg (for potential associations like "2% milk ! bread"). <p> The derivation of level-2 large itemsets generates the same large 1-itemsets L <ref> [2; 1] </ref> as shown in Figure 9. However, the candidate items are not confined to pairing only those in L [2; 1] because the items in L [2; 1] can be paired with those in L [1; 1] as well, such as f11fl, 1flflg (for potential associations like "milk ! 2% milk"), or f11fl, 2flflg (for potential associations like "2% milk ! bread"). <p> These candidate large 2-itemsets will be checked against T <ref> [2] </ref> to find large items (for the level-mixed nodes, the minimum support at a lower level, i.e., minsup [2], can be used as a default). Such a process generates the large 2-itemsets table L [2,2] as shown in Figure 9. <p> These candidate large 2-itemsets will be checked against T <ref> [2] </ref> to find large items (for the level-mixed nodes, the minimum support at a lower level, i.e., minsup [2], can be used as a default). Such a process generates the large 2-itemsets table L [2,2] as shown in Figure 9. <p> Similarly, the level 2 large 3-itemsets L <ref> [2; 3] </ref> can be computed, with the results shown in Figure 9. Also, the entries which pair with their own ancestors are not listed here since it is contained implicitly in their corresponding 2-itemsets. For example, hf11fl, 12flg, 4i in L [2,2] implies hf11fl, 12fl, 1flflg, 4i in L [2,3]. <p> The large 2-itemset table includes more itemsets since these items can be paired with higher level large items, which leads to the large 2-itemsets L <ref> [3, 2] </ref> and large 3-itemsets L [3, 3] as shown in Figure 10. Similarly, the itemsets f111, 11flg and f111, 1flflg have the same support as f111g in L [3, 1] and are thus not included in L [3,2].
Reference: [3] <author> R. Agrawal and R. Srikant. </author> <title> Mining sequential patterns. </title> <booktitle> In Proc. 1995 Int. Conf. Data Engineering, </booktitle> <address> Taipei, Taiwan, </address> <month> March </month> <year> 1995. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data <ref> [1, 2, 3, 12] </ref>. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels. <p> The process repeats for the derivation of L <ref> [l; 3] </ref>, : : : , L [l; k]. 2 4.3 Algorithm ML T2LA The third variation uses the same two encoded transaction tables T [1] and T [2] as in Algorithm ML T2L1 but it integrates some optimization techniques considered in the algorithm ML T1LA. <p> This can be explained as follows. Since the first threshold filters out many small 1-itemsets at level 1 which results in a much smaller filtered transaction table T [2], but the later filter is not so strong and parallel derivation of L [l; k] without derivation of T <ref> [3] </ref> and T [4] is more beneficial, thus leads M L T 2LA to be the best algorithm. On the other hand, M L T 1LA is the worst since it consults a large T [1] at every level. <p> Similarly, the level 2 large 3-itemsets L <ref> [2; 3] </ref> can be computed, with the results shown in Figure 9. Also, the entries which pair with their own ancestors are not listed here since it is contained implicitly in their corresponding 2-itemsets. For example, hf11fl, 12flg, 4i in L [2,2] implies hf11fl, 12fl, 1flflg, 4i in L [2,3]. <p> The large 2-itemset table includes more itemsets since these items can be paired with higher level large items, which leads to the large 2-itemsets L <ref> [3, 2] </ref> and large 3-itemsets L [3, 3] as shown in Figure 10. Similarly, the itemsets f111, 11flg and f111, 1flflg have the same support as f111g in L [3, 1] and are thus not included in L [3,2]. <p> The large 2-itemset table includes more itemsets since these items can be paired with higher level large items, which leads to the large 2-itemsets L [3, 2] and large 3-itemsets L <ref> [3, 3] </ref> as shown in Figure 10. Similarly, the itemsets f111, 11flg and f111, 1flflg have the same support as f111g in L [3, 1] and are thus not included in L [3,2]. <p> Similarly, the itemsets f111, 11flg and f111, 1flflg have the same support as f111g in L <ref> [3, 1] </ref> and are thus not included in L [3,2]. Since the large k-itemset (for k &gt; 1) tables do not explicitly include the pairs of items with their own ancestors, attention should be paid to include them at the generation of association rules. <p> Extension of methods for mining single-level knowledge rules to multiple-level ones poses many new issues for further investigation. For example, with the recent developments on mining single-level sequential patterns <ref> [3] </ref> and metaquery guided data mining [16], mining multiple-level sequential patterns and meta-query guided mining of multiple-level association rules are two interesting topics for future study.
Reference: [4] <author> A. Borgida and R. J. Brachman. </author> <title> Loading data into description reasoners. </title> <booktitle> In Proc. 1993 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <pages> pp. 217-226, </pages> <address> Wash-ington, D.C., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: In order to remove uninteresting rules generated in knowledge mining processes, researchers have proposed some measurements to quantify the "usefulness" or "interestingness" of a rule [13] or suggested to "put a human in the loop" and provide tools to allow human guidance <ref> [4] </ref>. Nevertheless, automatic generation of relatively focused, informative association rules will be obviously more efficient than first generating a large mixture of interesting and uninteresting rules. <p> Since the first threshold filters out many small 1-itemsets at level 1 which results in a much smaller filtered transaction table T [2], but the later filter is not so strong and parallel derivation of L [l; k] without derivation of T [3] and T <ref> [4] </ref> is more beneficial, thus leads M L T 2LA to be the best algorithm. On the other hand, M L T 1LA is the worst since it consults a large T [1] at every level.
Reference: [5] <author> W. W. Chu and K. Chiang. </author> <title> Abstraction of high level concepts from numerical values in databases. </title> <booktitle> In AAAI'94 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pp. 133-144, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: An expert or a user may provide mapping rules at the schema level (i.e., meta-rules) to indicate meaningful or desired mappings, such as "fcontent, brand, categoryg fcontent, categoryg category", etc. Concept hierarchies may not exist for numerical valued attributes but can be automatically generated according to data distribution statistics <ref> [8, 5] </ref>. For example, a hierarchy for the price range of sales items can be generated based on the distribution of price values. Moreover, a given concept hierarchy for numerical or nonnumerical data can be dynamically adjusted based on data distribution [8].
Reference: [6] <author> D. Fisher. </author> <title> Improving inference through conceptual clustering. </title> <booktitle> In Proc. 1987 AAAI Conf., </booktitle> <pages> pp. 461-465, </pages> <address> Seattle, Washington, </address> <month> July </month> <year> 1987. </year>
Reference-contexts: Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules [11], clustering <ref> [6] </ref>, etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels.
Reference: [7] <author> J. Han, Y. Cai, and N. Cercone. </author> <title> Data-driven discovery of quantitative rules in relational databases. </title> <journal> IEEE Trans. Knowledge and Data Engineering, </journal> <volume> 5 </volume> <pages> 29-40, </pages> <year> 1993. </year>
Reference-contexts: Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases [1, 2, 3, 9, 12, 14]. Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules <ref> [7, 15] </ref>, causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level. <p> In many applications, the taxonomy information is either stored implicitly in the database, such as "Wonder wheat bread is a wheat bread which is in turn a bread", or computed elsewhere <ref> [7] </ref>. Thus, data items can be easily generalized to multiple concept levels. However, direct application of the existing association rule mining methods to mining multiple-level associations may lead to some undesirable results as presented below. <p> Let the query be to find multiple-level strong associations in the database for the purchase patterns related to the foods which can only be stored for less than three weeks. The query can be expressed as follows in an SQL-like data mining language <ref> [7] </ref>. discover association rules from sales transactions T, sales item I where T.bar code = I.bar code and I.category = "food" and I.storage period &lt; 21 with interested attributes category, content, brand The query is first transformed into a standard SQL query which retrieves all the data items within the bar
Reference: [8] <author> J. Han and Y. Fu. </author> <title> Dynamic generation and refinement of concept hierarchies for knowledge discovery in databases. </title> <booktitle> In AAAI'94 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pp. 157-168, </pages> <address> Seattle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: An expert or a user may provide mapping rules at the schema level (i.e., meta-rules) to indicate meaningful or desired mappings, such as "fcontent, brand, categoryg fcontent, categoryg category", etc. Concept hierarchies may not exist for numerical valued attributes but can be automatically generated according to data distribution statistics <ref> [8, 5] </ref>. For example, a hierarchy for the price range of sales items can be generated based on the distribution of price values. Moreover, a given concept hierarchy for numerical or nonnumerical data can be dynamically adjusted based on data distribution [8]. <p> For example, a hierarchy for the price range of sales items can be generated based on the distribution of price values. Moreover, a given concept hierarchy for numerical or nonnumerical data can be dynamically adjusted based on data distribution <ref> [8] </ref>. For example, if there are many distinct country names in the attribute "place made", countries can be grouped into continents, such as Asia, Europe, South America, etc. <p> Moreover, if most fresh food products are from B.C. and Northwest America, the geographic hierarchy can be automatically adjusted to reflect this distribution when studying fresh food products <ref> [8] </ref>. 6.2 Generation of flexible association rules Our study has been confined to mining association relationships level-by-level in a fixed hierarchy. However, it is often necessary or desirable to find flexible association rules not confined to a strict, pre-arranged concept hierarchies.
Reference: [9] <author> M. Klemettinen, H. Mannila, P. Ronkainen, H. Toivo-nen, and A. I. Verkamo. </author> <title> Finding interesting rules from large sets of discovered association rules. </title> <booktitle> In Proc. 3rd Int'l Conf. on Information and Knowledge Management, </booktitle> <pages> pp. 401-408, </pages> <address> Gaithersburg, Maryland, </address> <month> Nov. </month> <year> 1994. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> However, mining association rules at high concept levels may often lead to the rules corresponding to prior knowledge and expectations <ref> [9] </ref>, such as "milk ! bread", or lead to some uninteresting attribute combinations, such as "toy ! milk".
Reference: [10] <author> H. Mannila and K-J. Raiha. </author> <title> Dependency inference. </title> <booktitle> In Proc. 1987 Int. Conf. Very Large Data Bases, </booktitle> <pages> pp. 155-158, </pages> <address> Brighton, England, </address> <month> Sept. </month> <year> 1987. </year>
Reference-contexts: Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases [1, 2, 3, 9, 12, 14]. Studies on mining association rules have evolved from techniques for discovery of functional dependencies <ref> [10] </ref>, strong rules [14], classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level.
Reference: [11] <author> R. S. Michalski and G. </author> <title> Tecuci. Machine Learning, A Multistrategy Approach, </title> <journal> Vol. </journal> <volume> 4. </volume> <publisher> Morgan Kaufmann, </publisher> <year> 1994. </year>
Reference-contexts: Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules <ref> [11] </ref>, clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels.
Reference: [12] <author> J.S. Park, M.S. Chen, and P.S. Yu. </author> <title> An effective hash-based algorithm for mining association rules. </title> <booktitle> In Proc. 1995 ACM-SIGMOD Int. Conf. Management of Data, </booktitle> <address> San Jose, CA, </address> <month> May </month> <year> 1995. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data <ref> [1, 2, 3, 12] </ref>. However, previous work has been focused on mining association rules at a single concept level. There are applications which need to find associations at multiple concept levels.
Reference: [13] <author> G. Piatesky-Shapiro and C. J. Matheus. </author> <title> The interestingness of deviations. </title> <booktitle> In AAAI'94 Workshop on Knowledge Discovery in Databases, </booktitle> <pages> pp. 25-36, </pages> <address> Seat-tle, WA, </address> <month> July </month> <year> 1994. </year>
Reference-contexts: In order to remove uninteresting rules generated in knowledge mining processes, researchers have proposed some measurements to quantify the "usefulness" or "interestingness" of a rule <ref> [13] </ref> or suggested to "put a human in the loop" and provide tools to allow human guidance [4]. Nevertheless, automatic generation of relatively focused, informative association rules will be obviously more efficient than first generating a large mixture of interesting and uninteresting rules.
Reference: [14] <author> G. Piatetsky-Shapiro. </author> <title> Discovery, analysis, and presentation of strong rules. </title> <editor> In G. Piatetsky-Shapiro and W. J. Frawley, editors, </editor> <booktitle> Knowledge Discovery in Databases, </booktitle> <pages> pp. 229-238. </pages> <publisher> AAAI/MIT Press, </publisher> <year> 1991. </year>
Reference-contexts: Proceedings of the 21st VLDB Conference Zurich, Swizerland, 1995 covery of interesting association relationships among huge amounts of data will help marketing, decision making, and business management. Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases <ref> [1, 2, 3, 9, 12, 14] </ref>. <p> Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases [1, 2, 3, 9, 12, 14]. Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules <ref> [14] </ref>, classification rules [7, 15], causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level.
Reference: [15] <author> J. R. Quinlan. C4.5: </author> <title> Programs for Machine Learning. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1992. </year>
Reference-contexts: Therefore, mining association rules from large data sets has been a focused topic in recent research into knowledge discovery in databases [1, 2, 3, 9, 12, 14]. Studies on mining association rules have evolved from techniques for discovery of functional dependencies [10], strong rules [14], classification rules <ref> [7, 15] </ref>, causal rules [11], clustering [6], etc. to disk-based, efficient methods for mining association rules in large sets of transaction data [1, 2, 3, 12]. However, previous work has been focused on mining association rules at a single concept level.
Reference: [16] <author> W. Shen, K. Ong, B. Mitbander, and C. Zaniolo. </author> <title> Metaqueries for data mining. In U.M. </title> <editor> Fayyad, G. Piatetsky-Shapiro, P. Smyth, and R. Uthurusamy, editors, </editor> <booktitle> Advances in Knowledge Discovery and Data Mining. </booktitle> <publisher> AAAI/MIT Press, </publisher> <year> 1995. </year> <pages> Page 12 </pages>
Reference-contexts: Extension of methods for mining single-level knowledge rules to multiple-level ones poses many new issues for further investigation. For example, with the recent developments on mining single-level sequential patterns [3] and metaquery guided data mining <ref> [16] </ref>, mining multiple-level sequential patterns and meta-query guided mining of multiple-level association rules are two interesting topics for future study.
References-found: 16

