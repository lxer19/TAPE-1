URL: http://polaris.cs.uiuc.edu/reports/1377.ps
Refering-URL: http://polaris.cs.uiuc.edu/tech_reports.html
Root-URL: http://www.cs.uiuc.edu
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> D. Callahan, K. Kennedy, and A. K. Porterfield, </author> <title> "Software prefetching," </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 40-52, </pages> <year> 1991. </year>
Reference-contexts: Exploiting temporal locality reduces memory latency for subsequent local accesses to a particular word; exploiting spatial locality reduces latency for accesses to adjacent words. Data prefetching <ref> [1, 2] </ref> has been used successfully to enhance uniprocessor cache performance, by exploiting additional locality and further lowering cache miss ratios. In cache coherent multiprocessors, sharing misses occur in addition to uniprocessor nonsharing misses. <p> The use of prefetching in multiprocessors allows latency to be hidden for both sharing and subsequent nonsharing accesses. Many different prefetching architectures and algorithms have been described in the literature. This dissertation focuses on software-initiated nonbinding prefetching into cache <ref> [1, 2] </ref>. In these schemes, explicit prefetching instructions are inserted into application codes at the source or machine language level. Source-level control of prefetching provides more semantic information about program structure 2 for use in eliminating ineffective prefetching. <p> Software-initiated prefetching schemes have the disadvantage of introducing instruction overhead for prefetching (and related) instructions, this overhead can be a significant performance issue <ref> [1, 2] </ref>. <p> Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Prefetches are lifted out of loops and moved lexically backward in an application code, which results in large, block prefetches. The idea of using software pipelined data prefetching is proposed in <ref> [1] </ref>. In this scheme, source-level prefetch instructions are inserted into loops in such a manner that individual variables or array elements are prefetched (with a prefetch distance of) one loop 132 iteration before they are used. <p> Multiprocessing may increase cache pollution since it introduces sharing 135 accesses; however, cache pollution and the necessity to consider prefetch buffers may decrease with increasing cache size. Instruction overhead is an important performance issue in software-initiated prefetching schemes <ref> [1, 2] </ref>. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms [2], or code optimization [93, 94]. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels <ref> [1] </ref>, and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [2] <author> T. C. Mowry, M. S. Lam, and A. Gupta, </author> <title> "Design and evaluation of a compiler algorithm for prefetching," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 62-73, </pages> <year> 1992. </year>
Reference-contexts: Exploiting temporal locality reduces memory latency for subsequent local accesses to a particular word; exploiting spatial locality reduces latency for accesses to adjacent words. Data prefetching <ref> [1, 2] </ref> has been used successfully to enhance uniprocessor cache performance, by exploiting additional locality and further lowering cache miss ratios. In cache coherent multiprocessors, sharing misses occur in addition to uniprocessor nonsharing misses. <p> The use of prefetching in multiprocessors allows latency to be hidden for both sharing and subsequent nonsharing accesses. Many different prefetching architectures and algorithms have been described in the literature. This dissertation focuses on software-initiated nonbinding prefetching into cache <ref> [1, 2] </ref>. In these schemes, explicit prefetching instructions are inserted into application codes at the source or machine language level. Source-level control of prefetching provides more semantic information about program structure 2 for use in eliminating ineffective prefetching. <p> Software-initiated prefetching schemes have the disadvantage of introducing instruction overhead for prefetching (and related) instructions, this overhead can be a significant performance issue <ref> [1, 2] </ref>. <p> (DOALL) loops, vectorizable loops containing regular array section references, serial loops, and vector statements containing array section references [36]. 4.1.1 Prefetching algorithms Prefetching is performed by using either a blocked vector algorithm that specifically targets vector statements and vectorizable loops, or a software pipelined algorithm that extends techniques described in <ref> [2] </ref> to support serial, parallel, and vectorizable loops. The blocked vector algorithm offers the advantages of simplicity of implementation and low instruction overhead, whereas the software pipelined algorithm allows longer memory latencies to be hidden more effectively, especially for nonvectorizable loops. <p> Round-robin scheduling and processor self-scheduling of parallel loop iterations are both supported by the algorithm. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of <ref> [2] </ref>. Selective prefetching [2] is not currently implemented, which significantly reduces the complexity (but as will be shown in Section 4.3, not the effectiveness) of the algorithm. <p> Round-robin scheduling and processor self-scheduling of parallel loop iterations are both supported by the algorithm. The algorithm uses loop splitting and software pipelining transformations, estimates loop execution times, computes prefetch distances, and schedules prefetching in a manner similar to that of <ref> [2] </ref>. Selective prefetching [2] is not currently implemented, which significantly reduces the complexity (but as will be shown in Section 4.3, not the effectiveness) of the algorithm. An example of software pipelined prefetching for a simple serial loop and a prefetch distance of D iterations is given in Figure 4.2 (a). <p> The prefetch distance, D, is chosen individually for each loop based on a specified memory latency as well as on compiler-generated execution time estimates for loop bodies <ref> [2] </ref>. Code expansion caused by peeling and unrolling multiple levels of loops is handled in two ways. First, loop prolog and epilog transformations can be suppressed if prefetch distances for particular loops fall below specified thresholds. <p> The memory system provides a one-cycle cache hit latency; round trip network and main memory access delay result in a base 100-cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 17, 59] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks are composed of 2x2 switches which are of the output queue variety and perform cut-through routing [60]. <p> This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> Prefetching is effective in cache sizes appropriate to the working set sizes of each application when compared to earlier results for a larger cache size. Prefetching offers comparable performance in both larger and smaller caches, despite the fact that selective prefetching <ref> [2] </ref> was not used. Forwarding performed worse in smaller caches because of increased undesirable replacement in destination processors' caches. Hybrid prefetching and forwarding was successful in countering undesirable replacement, which improved conflict miss ratios, decreased memory access delay, and even decreased underutilization time through prefetching in FLO52 and ARC2D. <p> Figure 5.17 shows extensions of the transformations in Figure 5.15 to address this concern. The first part of Figure 5.17 considers the case of normalized single loops. In this case, loop prolog and epilog transformations, similar to those used in software pipelined prefetching (Subsection 4.1.1, <ref> [2] </ref>), are used to inhibit unnecessary forwarding. For D &gt; 0, the prolog eliminates unnecessary forwarding for the first D iterations, since these loop index values fall below the lower bound of the read loop (Figure 5.15). <p> and forwarding strategy fine-tunes the selection of eligible writes made in the initial phase of the forwarding algorithm to target writes for which forwarding will be most effective. 5.3.2.7 Selective forwarding Whereas locality and reuse information for a single nest of parallel loops is sufficient to implement software-initiated selective prefetching <ref> [2] </ref>, intertask analysis that considers more than one loop nest is required to implement software-initiated selective forwarding. Granston [78] describes compiler analysis techniques to determine reuse caused by intertask locality. <p> The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in [90]. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes <ref> [2, 78, 82, 83, 88] </ref>. The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses <ref> [2] </ref>, or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. The analysis described in [78], however, determines reuse caused by intertask locality and uses this analysis to support local memory management and prefetching. <p> The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses <ref> [2] </ref>, or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. The analysis described in [78], however, determines reuse caused by intertask locality and uses this analysis to support local memory management and prefetching. <p> The benefits of this approach are 121 demonstrated for various computational kernels such as matrix multiplication and LU decomposition. Mowry <ref> [2, 18] </ref> uses a variant of this type of analysis to implement his selective prefetching scheme, applying prefetching only where the compiler algorithm indicates that cache misses are likely. <p> Hybrid prefetching and forwarding also decreased instruction overhead and prefetch miss ratios compared to prefetching alone. Hybrid prefetching and forwarding implements a selective prefetching and forwarding scheme analogous to the selective prefetching scheme proposed in <ref> [2] </ref>. The threshold value in the hybrid scheme can be adjusted to account for architectural parameters, such as cache size, as well as application code 127 characteristics. Selective prefetching is useful to decrease instruction overhead and to reduce unnecessary prefetching in larger caches [2]. <p> to the selective prefetching scheme proposed in <ref> [2] </ref>. The threshold value in the hybrid scheme can be adjusted to account for architectural parameters, such as cache size, as well as application code 127 characteristics. Selective prefetching is useful to decrease instruction overhead and to reduce unnecessary prefetching in larger caches [2]. Selective forwarding is useful to decrease undesirable replacement in smaller caches. Conditions that favor forwarding, and thus higher threshold values, include low overlap between communication and computation, increased communication, and high prefetching instruction overhead. <p> Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Klaiber and Levy [96] extend this concept to multiple iteration prefetch distances, which are determined for each loop using compiler analysis, and use machine-language-level prefetch instructions. Mowry <ref> [2] </ref> further extends software pipelined prefetching by introducing new compiler analysis and transformation techniques. Compiler analysis is used to predict cache misses and to avoid unnecessary prefetching. Prefetch distances are chosen according to compiler estimates of loop execution times. <p> The modeled system has 16 processors with caches connected to interleaved main memory modules via multistage interconnection networks (MINs). Software-initiated prefetching into processor cache is performed by manually inserting prefetching instructions into SPLASH codes according to the strategy described in <ref> [2] </ref>. Hardware-initiated prefetching is performed as in [72]. The results for software-initiated prefetching are variable and do not approach best-case performance for most of the codes studied. Prefetching results for vector prefetching in the Cedar multiprocessor are presented in [16]. <p> Multiprocessing may increase cache pollution since it introduces sharing 135 accesses; however, cache pollution and the necessity to consider prefetch buffers may decrease with increasing cache size. Instruction overhead is an important performance issue in software-initiated prefetching schemes <ref> [1, 2] </ref>. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms [2], or code optimization [93, 94]. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> Instruction overhead is an important performance issue in software-initiated prefetching schemes [1, 2]. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms <ref> [2] </ref>, or code optimization [93, 94]. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. The use of more sophisticated software algorithms can result in higher instruction overhead, but this overhead may be offset by the benefits of latency reduction and the ability to prefetch for initial loop iterations <ref> [2] </ref>. Hardware-initiated prefetching schemes can be used to eliminate software prefetch generation overhead at the expense of complex prefetching hardware [71, 72]. <p> Instruction overhead is also reduced through the use of hybrid prefetching and forwarding (Section 5.1). The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops <ref> [2, 57, 96] </ref>. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94].
Reference: [3] <author> D. M. Tullsen and S. J. Eggers, </author> <title> "Limitations of cache prefetching on a bus-based multiprocessor," </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 278-288, </pages> <year> 1993. </year>
Reference-contexts: Caches can hide memory latency for sharing accesses only by exploiting spatial locality; exploiting this type of locality (e.g., by increasing the cache block size) may lead to undesirable false sharing <ref> [3] </ref>. Data prefetching has the potential to hide memory latency for both sharing and nonsharing (local) accesses; however, data forwarding [4] may be a more effective technique than prefetching for reducing the latency of sharing accesses. <p> The memory system provides a one-cycle cache hit latency; round trip network and main memory access delay result in a base 100-cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 17, 59] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks are composed of 2x2 switches which are of the output queue variety and perform cut-through routing [60]. <p> A large cache size (compared to the working set sizes of the applications studied) is used to highlight communication effects while de-emphasizing cache conflict behavior. Single word cache blocks are used to minimize false sharing effects, particularly those caused or aggravated by prefetching <ref> [3] </ref>. Processor caches are lockup-free [62] to allow multiple outstanding prefetches (regular reads use blocking loads). The cache coherence scheme 60 uses a three-state, directory-based invalidation protocol. <p> Figures 4.5 (b) through 4.9 (b) depict the miss ratio observed by the memory subsystem for each code and scheme, broken down into the aforementioned processor misses, prefetch-in-progress misses, and prefetch misses. Cache misses are divided into the above categories according to the classification scheme used in <ref> [3] </ref>. This scheme classifies each event that occurs during simulation of program execution; thus, the construction of a cache simulator that collects statistics classifying events in this way is straightforward. Events are partitioned as follows: 1. Processor misses. These misses directly affect this miss latency observed by each processor. <p> The high percentage of cluster forwarding in DYFESM is caused by the high frequency of single-cluster parallel loops in this code. The availability of sufficient bandwidth is important to the performance of prefetching <ref> [3, 57] </ref> and forwarding. <p> This result was caused in part by the ample memory system bandwidth provided by the modeled system. Prefetching was effective in attacking both conflict (FLO52, ARC2D) and invalidation misses. Prefetching was found to be less effective in attacking invalidation misses in <ref> [3] </ref>; however, false sharing was shown to be a contributor to this effect. <p> Increasing the cache block size, however, would most likely lead to false sharing in the other three codes studied. False sharing has been noted as prominent, especially in numerical applications <ref> [3, 64, 65, 66] </ref>. <p> False sharing has been noted as prominent, especially in numerical applications [3, 64, 65, 66]. The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing <ref> [3] </ref>. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access [3, 18, 57, 59, 82]. <p> The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing [3]. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access <ref> [3, 18, 57, 59, 82] </ref>. The experimental results presented in Chapter 4 suggest, barring the case of broadcast forwarding, that multistage, packet-switched networks provide ample bandwidth to support prefetching and forwarding. <p> Since the network and memory request statistics presented in Subsection 4.3.3 show that utilization and contention are generally low, lower-cost interconnection architectures that provide less bandwidth than the simulated networks may offer attractive cost/performance trade-offs. Results in <ref> [3, 59] </ref> indicate that a single bus may not provide adequate bandwidth, but that split buses and interleaved memories may perform almost as well as a pipelined memory system [59]. <p> This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence <ref> [3, 16, 17, 59, 97] </ref> or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. <p> This process results in bursts of prefetching that have a significant impact on system performance, despite the short round-trip latency of the memory system. The blocked vector and software pipelined algorithms described in Subsection 4.1.1 attempt to balance prefetching and computation to decrease burst prefetching. Tullsen and Eggers <ref> [3] </ref> study the potential performance of data prefetching in bus-based multiprocessors. Prefetching is inserted into traces of SPLASH benchmarks in an optimistic fashion, and the resulting traces are simulated while varying the degree of bus contention. <p> The low bandwidth of the bus (compared to the multistage networks described in Subsection 4.2.1) is found to be a major inhibitor of prefetching performance. The use of a larger cache block size in <ref> [3] </ref> causes false sharing and concomitant performance degradation. The authors propose program restructuring to reduce false sharing, but this restructuring is shown to have limited effectiveness. Reducing the cache block size can be a more effective remedy for the false sharing problem. <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes <ref> [3, 17, 59] </ref>. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [4] <author> J. B. Andrews, C. J. Beckmann, and D. K. Poulsen, </author> <title> "Notification and multicast networks for synchronization and coherence," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 15, no. 4, </volume> <pages> pp. 332-350, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Data prefetching has the potential to hide memory latency for both sharing and nonsharing (local) accesses; however, data forwarding <ref> [4] </ref> may be a more effective technique than prefetching for reducing the latency of sharing accesses. In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. <p> Directory-based schemes implement receiver-initiated forwarding without requiring that sending processors explicitly specify the processors to receive forwarded data <ref> [4] </ref>. One study proposes the use of data forwarding to optimize data communication caused by cross-iteration data dependences within DOACROSS loops [21]. <p> Little data exists on how data forwarding schemes can effectively be used to reduce memory latency for sharing accesses <ref> [4] </ref>. This section describes and compares various forwarding schemes (Table 6.2). The schemes are partitioned into pure forwarding schemes and notification schemes, which integrate forwarding and synchronization mechanisms. Pure forwarding is sender-initiated and uses explicit forwarding instructions to send data to other processors. Notification, like data prefetching, is receiver-initiated. <p> Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> One scheme implements binding forwarding into local memory [21]. Several of the schemes use directories to record the processors to which data must be forwarded. Most directory-based schemes use the same directories used for cache coherence, while one scheme employs special directories used only for forwarding <ref> [4] </ref>. The action of data forwarding is similar, but not identical, to the update action in an update coherence protocol; thus, data forwarding in a shared memory architecture is similar to an adaptive update/invalidate cache coherence protocol [70, 73]. <p> The Dash Update Write instruction [20] updates the value of a specified word in all caches holding a copy of that word, by using the directory to determine which caches must receive the update. 138 Notification schemes use forwarding read primitives <ref> [4] </ref> to implement forwarding in conjunction with producer-consumer synchronization. Combining forwarding with synchronization offers the potential to reduce the overhead of both operations. <p> These schemes provide an extra directory bit that indicates whether each directory entry is being used for forwarding reads or for normal coherence transactions. Architectures to support forwarding in shared memory multiprocessors with multistage interconnection networks are described in <ref> [4] </ref>. The schemes implement directory-based, receiver-initiated forwarding and notification by using forwarding read and write operations. Notification is also used to combine synchronization and data forwarding for sharing accesses. Data are presented that demonstrate the effectiveness of forwarding and notification for various synchronization and data sharing tasks. <p> In the forwarding schemes surveyed, forwarding to multiple processors is accomplished by using Multicube broadcast [101], MIN-based broadcast [100], unrestricted MIN-based multicast <ref> [4] </ref>, or multiple point-to-point messages [20]. In MIN-based systems, broadcast or multiple point-to-point messages can result in significant network contention [4]; this consideration applies to other interconnect topologies as well. <p> In the forwarding schemes surveyed, forwarding to multiple processors is accomplished by using Multicube broadcast [101], MIN-based broadcast [100], unrestricted MIN-based multicast <ref> [4] </ref>, or multiple point-to-point messages [20]. In MIN-based systems, broadcast or multiple point-to-point messages can result in significant network contention [4]; this consideration applies to other interconnect topologies as well. Multicast forwarding is attractive, since it has the potential to reduce the memory traffic caused by forwarding requests and the overhead of generating these requests. 6.3 Execution-Driven Simulation Many execution-driven simulation and tracing systems have been described in the literature.
Reference: [5] <author> R. Alverson, D. Callahan, D. Cummings, B. Koblenz, A. K. Porterfield, and B. Smith, </author> <title> "The Tera computer system," </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 1-6, </pages> <year> 1990. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading <ref> [5, 6, 7, 8] </ref>, dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [6] <author> A. Gupta, J. Hennessy, K. Gharachorloo, T. Mowry, and W.-D. Weber, </author> <title> "Comparative evaluation of latency reducing and tolerating techniques," </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 254-263, </pages> <year> 1991. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading <ref> [5, 6, 7, 8] </ref>, dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [7] <author> J. T. Kuehn and B. J. Smith, </author> <title> "The Horizon supercomputer system: </title> <booktitle> architecture and software," Proceedings of Supercomputing, </booktitle> <pages> pp. 28-34, </pages> <year> 1988. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading <ref> [5, 6, 7, 8] </ref>, dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [8] <author> B. J. Smith, </author> <title> "A pipelined, shared resource MIMD computer," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 6-8, </pages> <year> 1978. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading <ref> [5, 6, 7, 8] </ref>, dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [9] <author> D. W. Anderson, F. J. Sparacio, and R. M. Tomasulo, </author> <title> "The IBM System/360 model 91: </title> <journal> machine philosophy and instruction-handling," IBM Journal of Research and Development, </journal> <volume> vol. 11, no. 1, </volume> <pages> pp. 8-24, </pages> <month> January </month> <year> 1967. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead <ref> [9, 10, 11, 12] </ref>, and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [10] <author> K. Gharachorloo, A. Gupta, and J. Hennessy, </author> <title> "Hiding memory latency using dynamic scheduling in shared-memory multiprocessors," </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 22-33, </pages> <year> 1992. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead <ref> [9, 10, 11, 12] </ref>, and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [11] <author> R. L. Lee, P.-C. Yew, and D. H. Lawrie, </author> <title> "Data prefetching in shared memory multiprocessors," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 28-31, </pages> <year> 1987. </year> <month> 150 </month>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead <ref> [9, 10, 11, 12] </ref>, and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [12] <author> R. M. Tomasulo, </author> <title> "An efficient hardware algorithm for exploiting multiple arithmetic units," </title> <journal> IBM Journal, </journal> <volume> vol. 11, </volume> <pages> pp. 25-33, </pages> <year> 1967. </year>
Reference-contexts: In addition to data prefetching and data forwarding, a number of architectural approaches to reducing global memory latency not considered further in this dissertation have previously been described in the literature. These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead <ref> [9, 10, 11, 12] </ref>, and decoupled access-execute architectures [13, 14, 15]. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [13] <author> A. R. Pleszkun and E. S. Davidson, </author> <title> "Structured memory access architecture," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 461-471, </pages> <year> 1983. </year>
Reference-contexts: These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures <ref> [13, 14, 15] </ref>. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [14] <author> J. E. Smith, </author> <title> "Decoupled access/execute computer architectures," </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> vol. 2, no. 4, </volume> <pages> pp. 289-308, </pages> <month> November </month> <year> 1984. </year>
Reference-contexts: These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures <ref> [13, 14, 15] </ref>. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [15] <author> J. E. Smith, </author> <title> "Dynamic instruction scheduling and the Astronautics ZS-1," </title> <journal> IEEE Computer, </journal> <volume> vol. 22, no. 7, </volume> <pages> pp. 21-35, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: These approaches include multithreading [5, 6, 7, 8], dynamic instruction scheduling and instruction lookahead [9, 10, 11, 12], and decoupled access-execute architectures <ref> [13, 14, 15] </ref>. 1.1 Data Prefetching and Data Forwarding Data prefetching has been proposed as a mechanism for hiding memory latency and exploiting temporal and spatial locality.
Reference: [16] <author> D. Kuck, E. Davidson, D. Lawrie, A. Sameh, C.-Q. Zhu, A. Veidenbaum, J. Konicek, P.-C. Yew, K. Gallivan, W. Jalby, H. Wijshoff, R. Bramley, U. M. Yang, P. Emrath, D. Padua, R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, T. Murphy, J. Andrews, and S. Turner, </author> <title> "The Cedar system and an initial performance study," </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 213-223, </pages> <year> 1993. </year>
Reference-contexts: Software-initiated prefetching schemes have the disadvantage of introducing instruction overhead for prefetching (and related) instructions, this overhead can be a significant performance issue [1, 2]. Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [16, 17] </ref>, few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching [18] and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism. Data prefetching may not be the most effective technique for handling certain types of interprocessor communication. <p> The blocked vector prefetching algorithm stripmines work into blocks of N elements and issues prefetches for each block immediately before the computation for that block. This algorithm is inspired by techniques employed in the Cedar multiprocessor for parallelizing vector operations and fetching vectors from global memory <ref> [16] </ref>. The algorithm works by exploiting spatial locality in vectorizable references, which hides memory latency particularly for the latter accesses in each block. <p> Barrier synchronization primitives are provided for use at the beginning or end of parallel loops, and queue-based locks are supported for use by particular applications. The loop scheduling and synchronization mechanisms are modeled after those used in the Cedar multiprocessor <ref> [16, 61] </ref>. Each processor has a 32K word, four-way set-associative, write-back, write-allocate cache with single word cache blocks, write buffers, and LRU replacement. A large cache size (compared to the working set sizes of the applications studied) is used to highlight communication effects while de-emphasizing cache conflict behavior. <p> Several dynamic scheduling techniques for reducing load imbalance, particularly for memory access delay, have been described in the literature; these techniques include multithreading, dynamic instruction scheduling, and instruction lookahead. The Alliant FX/80 [61] and the Cedar multiprocessor <ref> [16] </ref> employ hardware mechanisms for processor self-scheduling of parallel loop iterations to facilitate dynamic scheduling without excessive runtime overhead. Compiler analysis and transformations can be used to reduce the necessity for dynamic scheduling. <p> Several machines already support these types of operations, including the Stanford Dash multiprocessor [20], the Kendall Square KSR1 [91], and the Cedar multiprocessor <ref> [16] </ref>. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Deliver instructions. A prefetch instruction is similar to a regular load, except that it is nonblocking and is dropped on exceptions or page faults [18, 59]. <p> This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence <ref> [3, 16, 17, 59, 97] </ref> or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. <p> Hardware-initiated prefetching is performed as in [72]. The results for software-initiated prefetching are variable and do not approach best-case performance for most of the codes studied. Prefetching results for vector prefetching in the Cedar multiprocessor are presented in <ref> [16] </ref>. The prefetching algorithm used is similar to the vector prefetching scheme described in Subsection 4.1.1, except that N = vector length (no blocking) is used. Prefetched data are stored in preload buffers that are emptied upon first use of the data.
Reference: [17] <author> T. C. Mowry and A. Gupta, </author> <title> "Tolerating latency through software-controlled prefetching in shared-memory multiprocessors," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 12, no. 2, </volume> <pages> pp. 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Software-initiated prefetching schemes have the disadvantage of introducing instruction overhead for prefetching (and related) instructions, this overhead can be a significant performance issue [1, 2]. Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors <ref> [16, 17] </ref>, few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching [18] and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism. Data prefetching may not be the most effective technique for handling certain types of interprocessor communication. <p> The memory system provides a one-cycle cache hit latency; round trip network and main memory access delay result in a base 100-cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 17, 59] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks are composed of 2x2 switches which are of the output queue variety and perform cut-through routing [60]. <p> This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> Write buffering requires that state information for outstanding writes be saved to ensure correct access ordering. One easy way to add support for prefetching is to view prefetches as similar to writes and to utilize existing write buffers to make prefetches appear nonblocking <ref> [17] </ref>. Cache support for multiple outstanding prefetches may be simplified as compared to the support required for the general lockup-free case or the case of multiple outstanding writes [18]. <p> Previous studies indicate that modest buffer sizes (e.g., a single write buffer with 16 entries) should be sufficient to achieve reasonable performance under prefetching <ref> [17, 18] </ref>. Data forwarding using Forwarding Write operations offers the additional benefit of easing write buffer size requirements. Since Forwarding Write operations replace regular writes, write buffer sizes should not have to be increased significantly to support forwarding. <p> This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence <ref> [3, 16, 17, 59, 97] </ref> or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. <p> These schemes may generate unnecessary or ineffective prefetching, because they do not have the structural information available to a compiler and because they lack the ability to perform global optimization and analysis to support more effective prefetching. Mowry and Gupta <ref> [17] </ref> study software-initiated nonbinding prefetching for the Dash distributed shared memory multiprocessor. The Dash architecture and the entire hierarchy of L1, L2, and Remote Access Caches (RACs) are modeled in considerable detail, but simulations assume one processor per cluster and data are prefetched into RAC in most experiments. <p> Instruction overhead is an important performance issue in software-initiated prefetching schemes [1, 2]. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms [2], or code optimization [93, 94]. The overhead associated with software-initiated prefetching when simple algorithms are used is low <ref> [17] </ref>. The use of more sophisticated software algorithms can result in higher instruction overhead, but this overhead may be offset by the benefits of latency reduction and the ability to prefetch for initial loop iterations [2]. <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes <ref> [3, 17, 59] </ref>. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [18] <author> T. C. Mowry, </author> <title> "Tolerating latency through software-controlled data prefetching," </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> March </month> <year> 1994. </year>
Reference-contexts: Although data prefetching has been shown to be effective in reducing memory latency in shared memory multiprocessors [16, 17], few multiprocessor studies have considered the implementation of compiler algorithms for multiprocessor prefetching <ref> [18] </ref> and the performance impact of these algorithms on large, numerical applications with loop-level and vector parallelism. Data prefetching may not be the most effective technique for handling certain types of interprocessor communication. Data forwarding is a technique for integrating fine-grained message passing capabilities into a shared memory architecture. <p> This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> The usefulness of these techniques for enhancing locality, parallelism, and performance is well known [82, 83, 86, 87, 88]. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in <ref> [18] </ref>. The following paragraphs summarize the uses of some of the transformations listed above and their effects on cache behavior [79, 82, 89]. Loop splitting is used to increase opportunities for parallelization. <p> The benefits of this approach are 121 demonstrated for various computational kernels such as matrix multiplication and LU decomposition. Mowry <ref> [2, 18] </ref> uses a variant of this type of analysis to implement his selective prefetching scheme, applying prefetching only where the compiler algorithm indicates that cache misses are likely. <p> The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Deliver instructions. A prefetch instruction is similar to a regular load, except that it is nonblocking and is dropped on exceptions or page faults <ref> [18, 59] </ref>. A Forwarding Write is similar to a regular write, except that it has different coherence semantics, as discussed in Subsection 5.5.2. <p> In the broadest sense, these capabilities are provided by using lockup-free caches [62]; however, general lockup-free capabilities are not necessarily required <ref> [18, 59] </ref>. 5.5.2.1 Support for prefetching Cache design to support software-initiated prefetching has been discussed in detail in [18, 59]. Support for prefetching requires the ability to issue multiple prefetches that are nonblocking with respect to processors. Regular loads still block as in a traditional single-issue architecture. <p> In the broadest sense, these capabilities are provided by using lockup-free caches [62]; however, general lockup-free capabilities are not necessarily required <ref> [18, 59] </ref>. 5.5.2.1 Support for prefetching Cache design to support software-initiated prefetching has been discussed in detail in [18, 59]. Support for prefetching requires the ability to issue multiple prefetches that are nonblocking with respect to processors. Regular loads still block as in a traditional single-issue architecture. <p> Cache support for multiple outstanding prefetches may be simplified as compared to the support required for the general lockup-free case or the case of multiple outstanding writes <ref> [18] </ref>. As long as caches can accept incoming prefetched data while handling processor requests, it is only necessary to save address information for outstanding prefetches. <p> While prefetch issue buffers may offer a potential design simplification, using a single write buffer may offer similar performance <ref> [18] </ref>. 123 Cache operation is similar whether using separate prefetch issue buffers or a single write buffer [18, 59]. Each issued prefetch first checks to see whether the requested data are available in cache. If not, the prefetch is compared with other prefetches in the appropriate buffer. <p> While prefetch issue buffers may offer a potential design simplification, using a single write buffer may offer similar performance [18]. 123 Cache operation is similar whether using separate prefetch issue buffers or a single write buffer <ref> [18, 59] </ref>. Each issued prefetch first checks to see whether the requested data are available in cache. If not, the prefetch is compared with other prefetches in the appropriate buffer. If a match is found, the new prefetch is canceled. <p> If a match is found, the new prefetch is canceled. Regular loads are blocking and have higher priority than prefetches [59]. Unlike writes, prefetches can optionally be dropped if issue or write buffers are full; exercising this option may improve performance compared to blocking on full buffers <ref> [18] </ref>. 5.5.2.2 Support for forwarding Software-initiated nonbinding forwarding into processor caches also requires that caches accept incoming forwarded data while handling processor requests. From the standpoint of the issuing cache, Forwarding Writes are similar to regular writes and thus can use existing cache mechanisms and write buffers. <p> A Forwarding Write combines a write and a Deliver operation. Destination processors for forwarding, specified by the bit vector operand of a Forwarding Write instruction, receive copies of each forwarded cache block in the read-shared state. Read-exclusive forwarding, analogous to read-exclusive prefetching in <ref> [18] </ref>, is also possible; this type of forwarding was not experimentally evaluated. <p> Previous studies indicate that modest buffer sizes (e.g., a single write buffer with 16 entries) should be sufficient to achieve reasonable performance under prefetching <ref> [17, 18] </ref>. Data forwarding using Forwarding Write operations offers the additional benefit of easing write buffer size requirements. Since Forwarding Write operations replace regular writes, write buffer sizes should not have to be increased significantly to support forwarding. <p> The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing [3]. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access <ref> [3, 18, 57, 59, 82] </ref>. The experimental results presented in Chapter 4 suggest, barring the case of broadcast forwarding, that multistage, packet-switched networks provide ample bandwidth to support prefetching and forwarding.
Reference: [19] <author> W.-D. Weber and A. Gupta, </author> <title> "Analysis of cache invalidation patterns in multiprocessors," </title> <booktitle> Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 243-256, </pages> <year> 1989. </year>
Reference-contexts: Forwarding can potentially hide all the memory latency of sharing accesses since data can be sent to destination processors' caches immediately after they are produced; prefetching can only approach this level of latency tolerance. Forwarding for migratory <ref> [19] </ref> or producer-consumer sharing accesses can be more efficient than prefetching in systems with hardware cache coherence, since less coherence traffic may be required. While many data forwarding mechanisms have been proposed, many of these mechanisms have been intended primarily for use in optimizing synchronization operations. <p> Marking eligible write accesses can be accomplished given compiler analysis that determines reference types. Weber and Gupta suggest classifying references as read-only, migratory, synchronization, mostly-read, or frequently read/written <ref> [19] </ref>. A similar classification scheme is proposed in [73]. Migratory references are primary candidates for forwarding because of their sequential sharing patterns. Forwarding may also be beneficial for mostly-read references to reduce the latency of initial read accesses after writes. <p> The stream of writes and reads generated by each processor is monitored during profiling, and references are classified according to the definitions used in [73]. Reference types are stored on a per-array basis, as opposed to a per-element basis. Migratory, mostly-read, and frequently read/written <ref> [19] </ref> reference types are all considered to be potential candidates for forwarding. Tests for the eligibility of writes for forwarding are applied only to the last writes of particular array elements in each task.
Reference: [20] <author> D. E. Lenoski, J. Laudon, K. Gharachorloo, W.-D. Weber, A. Gupta, J. Hennessy, M. Horowitz, and M. S. Lam, </author> <title> "The Stanford Dash multiprocessor," </title> <journal> IEEE Computer, </journal> <volume> vol. 25, no. 3, </volume> <pages> pp. 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: This dissertation considers the use of data forwarding for optimizing sharing accesses. The forwarding mechanisms considered are software-initiated nonbinding mechanisms that perform sender-initiated forwarding of data to other processors' caches. One example of this type of mechanism is the Dash Deliver instruction <ref> [20] </ref>, an operation that transmits a copy of a cache block to one or more explicitly specified cluster caches. 3 Other types of forwarding mechanisms have been proposed. Directory-based schemes implement receiver-initiated forwarding without requiring that sending processors explicitly specify the processors to receive forwarded data [4]. <p> A Forwarding Write is a single instruction that combines a write and a sender-initiated forwarding operation, where the semantics of the forwarding operation are similar to those of the Dash Deliver instruction <ref> [20] </ref>. The operand size is a single cache block and destination processors are specified via a bit vector; the specified cache block is forwarded to the caches of each specified processor in the read-shared state. <p> The destination processors for each such written array element are stored as bit vectors, similar to the bit vectors used in the Dash Deliver instruction <ref> [20] </ref>. Writes are eligible for forwarding if they have downstream reads on 57 other processors. <p> Several machines already support these types of operations, including the Stanford Dash multiprocessor <ref> [20] </ref>, the Kendall Square KSR1 [91], and the Cedar multiprocessor [16]. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Deliver instructions. <p> Regular loads still block as in a traditional single-issue architecture. Many traditional designs for write-back, write-allocate caches use write buffering and allow multiple outstanding writes for the purposes of supporting relaxed consistency and reducing write latency <ref> [20] </ref>. Write buffering requires that state information for outstanding writes be saved to ensure correct access ordering. One easy way to add support for prefetching is to view prefetches as similar to writes and to utilize existing write buffers to make prefetches appear nonblocking [17]. <p> Unlike prefetches and regular writes, Forwarding Writes cannot be dropped from full write buffers. The Forwarding Write operation has different coherence semantics than a regular write, although the semantics of Forwarding Write are somewhat similar to those of the Dash Deliver instruction <ref> [20, 92] </ref>. A Forwarding Write combines a write and a Deliver operation. Destination processors for forwarding, specified by the bit vector operand of a Forwarding Write instruction, receive copies of each forwarded cache block in the read-shared state. <p> Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> Receiver-initiated forwarding mechanisms are more closely related to adaptive coherence schemes; they provide mechanisms for processors to add themselves to directories to enable them to receive future updates of particular cache blocks. Several sender-initiated forwarding schemes have been proposed. The Deliver instruction <ref> [20] </ref> is used in the Dash multiprocessor to implement pure forwarding. This instruction forwards the specified cache block to one or more RACs. The block to be forwarded must already have been acquired by the issuing processor, by using normal memory requests, before a Deliver can be issued. <p> The Notify primitive [101] performs broadcast forwarding by using the broadcast invalidation mechanism in the cache coherence protocol. None of these schemes requires any directory information to accomplish forwarding. The Dash Update Write instruction <ref> [20] </ref> updates the value of a specified word in all caches holding a copy of that word, by using the directory to determine which caches must receive the update. 138 Notification schemes use forwarding read primitives [4] to implement forwarding in conjunction with producer-consumer synchronization. <p> In the forwarding schemes surveyed, forwarding to multiple processors is accomplished by using Multicube broadcast [101], MIN-based broadcast [100], unrestricted MIN-based multicast [4], or multiple point-to-point messages <ref> [20] </ref>. In MIN-based systems, broadcast or multiple point-to-point messages can result in significant network contention [4]; this consideration applies to other interconnect topologies as well.
Reference: [21] <author> H.-M. Su and P.-C. Yew, </author> <title> "Efficient doacross synchronization on distributed shared-memory multiprocessors," </title> <booktitle> Proceedings of Supercomputing, </booktitle> <pages> pp. 842-853, </pages> <year> 1991. </year>
Reference-contexts: Directory-based schemes implement receiver-initiated forwarding without requiring that sending processors explicitly specify the processors to receive forwarded data [4]. One study proposes the use of data forwarding to optimize data communication caused by cross-iteration data dependences within DOACROSS loops <ref> [21] </ref>. <p> reduced and provides forwarding with a potential performance advantage over prefetching. 4.1.3 Forwarding algorithms The forwarding schemes presented in this chapter target the reduction of memory latency for sharing accesses between successive parallel loops, rather than for fine-grained communication and synchronization within a particular parallel loop nest or DOACROSS loop <ref> [21] </ref>. Compiler algorithms for forwarding between parallel loops must perform two tasks: marking of eligible write accesses and determination of destination processors in subsequent loops. Compiler algorithms for data forwarding are discussed in detail in Section 5.3. <p> In the case of round-robin scheduling, the processors accessing particular shared memory locations can be determined given loop index and array reference subscript information. This type of analysis has been applied to generating explicit message passing within a single loop nest for forwarding in DOACROSS loops <ref> [21] </ref>, as well as for compiling languages such as Fortran D for message passing machines [74]. <p> In the latter case, the problem is considerably simplified for a cache coherent, shared memory architecture, because the compiler does not have to generate data distributions; data distributions are implied for each loop based on loop partitioning and scheduling. Although the compiler analyses described in <ref> [21, 74] </ref> are applied to communication within DOACROSS loops or within parallel loop nests, these techniques are extensible to the problem of determining destination processors for forwarding between parallel loops. <p> Forwarded data can also cause 137 other useful data to be cast out of destination caches as the forwarded data arrive. One scheme implements binding forwarding into local memory <ref> [21] </ref>. Several of the schemes use directories to record the processors to which data must be forwarded. Most directory-based schemes use the same directories used for cache coherence, while one scheme employs special directories used only for forwarding [4]. <p> The schemes implement directory-based, receiver-initiated forwarding and notification by using forwarding read and write operations. Notification is also used to combine synchronization and data forwarding for sharing accesses. Data are presented that demonstrate the effectiveness of forwarding and notification for various synchronization and data sharing tasks. Su and Yew <ref> [21] </ref> propose a forwarding scheme and compiler algorithms for use in distributed shared memory architectures to optimize communication caused by cross-iteration data dependences in DOACROSS loops. This scheme implements sender-initiated binding forwarding into local memory and assumes statically (i.e., round-robin) scheduled parallel loops. <p> This binding forwarding scheme performs 139 static message-passing by using explicit Get and Put operations. Destinations for forwarded data are bound at compile time. Tagging or indirection can be used to implement dynamic message binding <ref> [21] </ref>. Dynamic binding allows more efficient use of local memory at the expense of more complex runtime support. Binding forwarding can be used in conjunction with software coherence techniques, in a manner similar to the binding prefetching approach taken in [57].
Reference: [22] <author> P. Konas, D. K. Poulsen, C. J. Beckmann, J. D. Bruner, and P.-C. Yew, </author> <title> "Chief: a simulation environment for studying parallel systems," </title> <journal> International Journal of Computer Simulation, </journal> <note> to appear, </note> <year> 1994. </year>
Reference-contexts: Forwarding is applied and evaluated by using a profile-based approach. A compiler algorithm for data forwarding is presented that analyzes communication between loops. A hybrid prefetching and forwarding scheme that combines the best features of both approaches is also presented and evaluated. EPG-sim <ref> [22, 23] </ref>, a system of execution-driven simulation tools for studying parallel architectures, algorithms, and applications, was developed as a prerequisite for this work. EPG-sim performs execution-driven simulation and critical path simulation within a single, integrated environment. <p> Chapter 6 includes a summary of related work on data prefetching, data forwarding, and execution-driven simulation, and Chapter 7 presents conclusions and future work. 8 2 THE EPG-SIM EXECUTION-DRIVEN SIMULATION SYSTEM EPG-sim <ref> [22, 23] </ref>, a comprehensive set of execution-driven simulation tools for studying parallel systems, was developed to enable the characterization of application code parallelism and performance and the development and study of prefetching and forwarding schemes. This chapter describes the functionality, specific capabilities, implementation, and performance of EPG-sim. <p> These event generators are coupled with parallel discrete-event simulators to implement EDS. ETG can be performed during EDS to produce traces that have the same physical order as their simulation time order. The overall structure of an EPG-sim EDS is shown in Chief <ref> [22, 47, 48] </ref> parallel discrete-event simulators are used to construct system models and to perform simulation. The Chief system is portable, which allows simulations to be executed on a variety of uniprocessor and parallel host machines.
Reference: [23] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "Execution-driven tools for parallel simulation of parallel architectures and applications," </title> <booktitle> Proceedings of Supercomputing, </booktitle> <pages> pp. 860-869, </pages> <year> 1993. </year> <month> 151 </month>
Reference-contexts: Forwarding is applied and evaluated by using a profile-based approach. A compiler algorithm for data forwarding is presented that analyzes communication between loops. A hybrid prefetching and forwarding scheme that combines the best features of both approaches is also presented and evaluated. EPG-sim <ref> [22, 23] </ref>, a system of execution-driven simulation tools for studying parallel architectures, algorithms, and applications, was developed as a prerequisite for this work. EPG-sim performs execution-driven simulation and critical path simulation within a single, integrated environment. <p> Chapter 6 includes a summary of related work on data prefetching, data forwarding, and execution-driven simulation, and Chapter 7 presents conclusions and future work. 8 2 THE EPG-SIM EXECUTION-DRIVEN SIMULATION SYSTEM EPG-sim <ref> [22, 23] </ref>, a comprehensive set of execution-driven simulation tools for studying parallel systems, was developed to enable the characterization of application code parallelism and performance and the development and study of prefetching and forwarding schemes. This chapter describes the functionality, specific capabilities, implementation, and performance of EPG-sim.
Reference: [24] <author> M. Berry, D.-K. Chen, P. Koss, D. Kuck, L. Pointer, S. Lo, Y. Pang, R. Roloff, A. Sameh, E. Clementi, S. Chin, D. Schneider, G. Fox, P. Messina, D. Walker, C. Hsiung, J. Schwarzmeier, K. Lue, S. Orszag, F. Seidl, O. Johnson, G. Swanson, R. Goodrum, and J. Martin, </author> <title> "The Perfect Club benchmarks: effective performance evaluation of supercomputers," </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> vol. 3, no. 3, </volume> <pages> pp. 5-40, </pages> <month> Fall </month> <year> 1989. </year>
Reference-contexts: Software and hardware support for prefetching and forwarding is also discussed. 1.3 Application Codes The experiments described in this dissertation utilize large, numerical application codes with loop-level and vector parallelism. The application codes used are selected from the Perfect Benchmarks <ref> [24] </ref>, a suite of scientific, numerical benchmarks that represent a variety of scientific disciplines and numerical methods [25]. Critical path simulations utilize the original, serial Perfect codes. Multiprocessor execution-driven simulations utilize optimized, parallel versions of the codes, described in Subsection 4.2.3.
Reference: [25] <author> M. Berry, G. Cybenko, and J. Larson, </author> <title> "Scientific benchmark characterizations," </title> <journal> Parallel Computing, </journal> <volume> vol. 17, no. 6, </volume> <pages> pp. 1173-1194, </pages> <month> December </month> <year> 1991. </year>
Reference-contexts: The application codes used are selected from the Perfect Benchmarks [24], a suite of scientific, numerical benchmarks that represent a variety of scientific disciplines and numerical methods <ref> [25] </ref>. Critical path simulations utilize the original, serial Perfect codes. Multiprocessor execution-driven simulations utilize optimized, parallel versions of the codes, described in Subsection 4.2.3. Each serial and parallel code was individually modified to reduce simulation time while preserving parallelism and memory reference behavior characteristics.
Reference: [26] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "The Perfect benchmark suite some imperfections," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <year> 1994. </year>
Reference-contexts: Each serial and parallel code was individually modified to reduce simulation time while preserving parallelism and memory reference behavior characteristics. These reductions were accomplished by reducing numbers of iterations or time steps where possible, rather than by reducing data set sizes. The details of these modifications are described in <ref> [26] </ref>. The Perfect codes used and their sizes, in lines of source code, are summarized in Table 1.1 [27]. QCD is a quantum chromodynamics benchmark that performs lattice gauge theory simulation by using a pseudo heat-bath algorithm and Monte Carlo techniques.
Reference: [27] <author> L. Kipp, </author> <title> "Perfect Benchmarks documentation, </title> <type> suite 1," </type> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <month> March </month> <year> 1990. </year>
Reference-contexts: These reductions were accomplished by reducing numbers of iterations or time steps where possible, rather than by reducing data set sizes. The details of these modifications are described in [26]. The Perfect codes used and their sizes, in lines of source code, are summarized in Table 1.1 <ref> [27] </ref>. QCD is a quantum chromodynamics benchmark that performs lattice gauge theory simulation by using a pseudo heat-bath algorithm and Monte Carlo techniques. MDG is a molecular dynamics benchmark that simulates several hundred water molecules at room temperature.
Reference: [28] <author> J. B. Andrews and K. A. Gallivan, </author> <title> "Analysis of a Cedar implementation of TRFD," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1312, </type> <month> August </month> <year> 1993. </year>
Reference-contexts: In each transformation, each processor 7 uses all of V and VT and a partition of S <ref> [28] </ref>. TRFD is highly parallel but does not use particularly long vectors. FLO52 and ARC2D contain large amounts of loop-level parallelism and are highly vectorizable. QCD and DYFESM exhibit less loop-level parallelism and short vector lengths than TRFD, FLO52, or ARC2D. <p> CPS is extremely efficient compared to EDS and, as will be shown in Chapter 3, produces useful performance results. TDS and EDS for TRFD-p, a parallel version of TRFD <ref> [28] </ref>, were performed to compare their accuracy and performance. In both cases, events were generated while considering only the explicit parallelism in TRFD-p, and events for scalar memory references were suppressed to simulate register usage. <p> Three versions of TRFD were studied: TRFD, the original Perfect code; TRFD-p, a functionally equivalent parallel version of TRFD <ref> [28] </ref>; and TRFD-s, a serial version of TRFD-p.
Reference: [29] <author> C. B. Stunkel, B. Janssens, and W. K. Fuchs, </author> <title> "Address tracing for parallel machines," </title> <journal> IEEE Computer, </journal> <volume> vol. 24, no. 1, </volume> <pages> pp. 31-38, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: The resulting instrumented codes execute on a host machine using actual input data. Execution causes the generation of events that reflect the behavior of the original applications as if executing on a hypothetical modeled machine. Other event generation techniques include hardware monitoring, interrupt-based methods, pure simulation, and microcode-based methods <ref> [29] </ref>; these techniques can be used to study multiprogramming and operating system behavior in addition to application behavior. 9 This chapter considers execution-driven event generation techniques based on application code instrumentation and tools that result from the use of these techniques. Execution-driven techniques have important uses in studying parallel systems. <p> Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in <ref> [29] </ref>. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus [55], PEET [114], and RPPT [115].
Reference: [30] <author> H. Davis, S. R. Goldschmidt, and J. Hennessy, </author> <title> "Multiprocessor simulation and tracing using Tango," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-99-106, </address> <year> 1991. </year>
Reference-contexts: This feedback can be used to alter the ordering, timing, or latency of subsequently generated events. In a simulated parallel code, the path of execution and ordering and latency of events may depend on the ordering and timing of preceding events <ref> [30] </ref>. In TDS, these dependences cannot be taken into account, which limits the accuracy of these simulations [31, 32]. EDS correctly models processor interactions. <p> ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango <ref> [30, 113] </ref>, Proteus [55], PEET [114], and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts. Machine-language-level instrumentation is usually employed, performed either during compilation or in a compiler postprocessing step. <p> The ability to trade off simulation cost versus accuracy is an important feature in versatile EDS systems <ref> [30, 55] </ref>. This trade-off applies to both processor and system modeling. Existing systems employing machine-language-level instrumentation techniques are limited in their ability to model varying processor architectures.
Reference: [31] <author> P. Bitar, </author> <title> "A critique of trace-driven simulation for shared-memory multiprocessors," </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture Workshop on Cache and Interconnect Architectures in Multiprocessors, </booktitle> <year> 1989. </year>
Reference-contexts: In a simulated parallel code, the path of execution and ordering and latency of events may depend on the ordering and timing of preceding events [30]. In TDS, these dependences cannot be taken into account, which limits the accuracy of these simulations <ref> [31, 32] </ref>. EDS correctly models processor interactions.
Reference: [32] <author> S. R. Goldschmidt and J. L. Hennessy, </author> <title> "The accuracy of trace-driven simulations of multiprocessors," </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 146-157, </pages> <year> 1993. </year>
Reference-contexts: In a simulated parallel code, the path of execution and ordering and latency of events may depend on the ordering and timing of preceding events [30]. In TDS, these dependences cannot be taken into account, which limits the accuracy of these simulations <ref> [31, 32] </ref>. EDS correctly models processor interactions. <p> TDS uses near-optimal processor scheduling, performed at ETG time, whereas EDS uses self-scheduling at execution time. While the difference in cache hit ratio is small, and the difference in speedup is only 1.3%, TRFD-p uses no constructs that typically cause larger accuracy discrepancies <ref> [32] </ref>. The execution time required to generate events for EDS is the same as that required for ETG (line 1 in Table 2.2), since the same instrumented code is executed in both cases.
Reference: [33] <author> C. D. Polychronopoulos, M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten, </author> <title> "Parafrase-2: an environment for parallelizing, partitioning, synchronizing and scheduling programs on multiprocessors," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-39-48, </address> <year> 1989. </year>
Reference-contexts: The following sections describe the various components of EPG-sim in greater detail. 12 2.2.1 Source-level instrumentation EPG-sim uses source-level instrumentation to generate events for CPS, ETG, and EDS. Source-level event generators are produced by using application code instrumentation tools that are based on the Parafrase-2 compiler <ref> [33, 34, 35] </ref>, developed at CSRD. Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran [36] input are both supported, which allows both serial and parallel codes to be instrumented.
Reference: [34] <author> M. B. Girkar, M. R. Haghighat, C. L. Lee, B. P. Leung, and D. A. Schouten, </author> <title> "Parafrase-2 programmer's manual," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <month> August </month> <year> 1991. </year> <month> 152 </month>
Reference-contexts: The following sections describe the various components of EPG-sim in greater detail. 12 2.2.1 Source-level instrumentation EPG-sim uses source-level instrumentation to generate events for CPS, ETG, and EDS. Source-level event generators are produced by using application code instrumentation tools that are based on the Parafrase-2 compiler <ref> [33, 34, 35] </ref>, developed at CSRD. Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran [36] input are both supported, which allows both serial and parallel codes to be instrumented.
Reference: [35] <author> B. P. Leung, </author> <title> "Issues on the design of parallelizing compilers," M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1012, </type> <month> June </month> <year> 1990. </year>
Reference-contexts: The following sections describe the various components of EPG-sim in greater detail. 12 2.2.1 Source-level instrumentation EPG-sim uses source-level instrumentation to generate events for CPS, ETG, and EDS. Source-level event generators are produced by using application code instrumentation tools that are based on the Parafrase-2 compiler <ref> [33, 34, 35] </ref>, developed at CSRD. Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran [36] input are both supported, which allows both serial and parallel codes to be instrumented.
Reference: [36] <author> D. Padua, J. Hoeflinger, G. Jaxon, and R. Eigenmann, </author> <title> "The Cedar Fortran project," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1262, </type> <month> October </month> <year> 1992. </year>
Reference-contexts: Source-level event generators are produced by using application code instrumentation tools that are based on the Parafrase-2 compiler [33, 34, 35], developed at CSRD. Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran <ref> [36] </ref> input are both supported, which allows both serial and parallel codes to be instrumented. Parafrase-2 features include multiple parsers and source code generators, data and control dependence analyses [37, 38], interprocedural and symbolic analyses, and a suite of transformation passes. <p> The experimental results are summarized and discussed in Section 4.4, and conclusions are presented in Section 4.5. 4.1 Prefetching and Forwarding Schemes This section describes prefetching algorithms, forwarding architectures, and forwarding algorithms for use in multiprocessors that execute parallel codes such as Cedar Fortran <ref> [36] </ref> codes. The pertinent language features for purposes of these descriptions are parallel (DOALL) loops, vectorizable loops containing regular array section references, serial loops, and vector statements containing array section references [36]. 4.1.1 Prefetching algorithms Prefetching is performed by using either a blocked vector algorithm that specifically targets vector statements and <p> prefetching algorithms, forwarding architectures, and forwarding algorithms for use in multiprocessors that execute parallel codes such as Cedar Fortran <ref> [36] </ref> codes. The pertinent language features for purposes of these descriptions are parallel (DOALL) loops, vectorizable loops containing regular array section references, serial loops, and vector statements containing array section references [36]. 4.1.1 Prefetching algorithms Prefetching is performed by using either a blocked vector algorithm that specifically targets vector statements and vectorizable loops, or a software pipelined algorithm that extends techniques described in [2] to support serial, parallel, and vectorizable loops. <p> Experimental results are acquired through EPG-sim behavioral EDS driven by parallel application codes. Simulations model a shared memory architecture with directory-based cache coherence and are driven by parallel versions of Perfect codes that have been optimized for the modeled architecture. These parallel Cedar Fortran <ref> [36] </ref> codes have prefetching and forwarding applied via algorithms implemented in Parafrase-2. 4.2.1 System model The modeled system is a 32-processor, cache coherent, distributed shared memory architecture with processor/cache/memory nodes connected via multistage, packet-switched networks (Figure 4.3). <p> Processor-private variables either are 61 allocated on each processor's private stack or are explicitly scalar-expanded. 4.2.3 Application codes The application codes studied in this work are parallel, Cedar Fortran versions of Perfect codes <ref> [36] </ref>. These codes express parallelism by using parallel loops and vector statements. The particular application codes studied are listed in Table 4.1. <p> The codes were originally parallelized using an optimizing compiler, then further hand-optimized to exploit available parallelism, to increase locality, and to reduce memory latency <ref> [36] </ref>. 4.3 Experimental Results This section presents the experimental results of applying prefetching and forwarding to the various parallel Perfect codes. The following specific schemes were evaluated: Scheme Meaning BASE No prefetching or forwarding; the base performance of each application. <p> For example, all the code in a serial code section, a serial loop, or a single iteration of an XDOALL or CDOALL loop <ref> [36] </ref> executes on a single processor. Code in CDOALL loops nested inside SDOALL loops can execute on different processors; otherwise, nested code inside loop iterations executes on the same processors as those that execute the loop iterations themselves.
Reference: [37] <author> U. Banerjee, </author> <title> Dependence Analysis for Supercomputing. </title> <address> Boston, MA: </address> <publisher> Kluwer Academic Publishers, </publisher> <year> 1988. </year>
Reference-contexts: Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran [36] input are both supported, which allows both serial and parallel codes to be instrumented. Parafrase-2 features include multiple parsers and source code generators, data and control dependence analyses <ref> [37, 38] </ref>, interprocedural and symbolic analyses, and a suite of transformation passes. The EPG (1) source code instrumentation tools [39] are implemented as a set of passes within Parafrase-2 [40]. These tools perform instrumentation and constitute the front-end for EPG-sim. <p> COMMON variable defined in both the current and called subprograms or if the write reference is passed as a parameter representing an array pointer or array base address. 5.3.2.5 Determining destination processors In general, the problem of determining destination processors for forwarding is related to that of data dependence analysis <ref> [37] </ref>. As an example, consider the code shown in Figure 5.14. A write for an array A with K dimensions is nested inside X loops. A read for this same array A is nested inside Y downstream loops. Neither set of loops is required to be perfectly nested.
Reference: [38] <author> J. Ferranti, K. J. Ottenstein, and J. D. Warren, </author> <title> "The program dependence graph and its uses in optimization," </title> <journal> ACM Transactions on Programming Languages and Systems, </journal> <volume> vol. 9, no. 3, </volume> <pages> pp. 319-349, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: Parafrase-2 is a portable, multilingual, source-to-source parallelizing and restructuring compiler with multiple pass construction. Fortran-77 and Cedar Fortran [36] input are both supported, which allows both serial and parallel codes to be instrumented. Parafrase-2 features include multiple parsers and source code generators, data and control dependence analyses <ref> [37, 38] </ref>, interprocedural and symbolic analyses, and a suite of transformation passes. The EPG (1) source code instrumentation tools [39] are implemented as a set of passes within Parafrase-2 [40]. These tools perform instrumentation and constitute the front-end for EPG-sim.
Reference: [39] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "EPG source code instrumentation tools user manual," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <year> 1994. </year>
Reference-contexts: Parafrase-2 features include multiple parsers and source code generators, data and control dependence analyses [37, 38], interprocedural and symbolic analyses, and a suite of transformation passes. The EPG (1) source code instrumentation tools <ref> [39] </ref> are implemented as a set of passes within Parafrase-2 [40]. These tools perform instrumentation and constitute the front-end for EPG-sim. EPG handles source codes in a robust manner, with little or no user intervention, and produces efficient instrumented codes. <p> The ability to measure interprocedural synchronization overhead is a useful extension of previous CPS work. 34 3 PARALLELISM AND COMMUNICATION IN SCIENTIFIC BENCHMARKS This chapter describes the results of numerous types of EPG-sim CPS <ref> [39, 56] </ref> experiments involving Perfect codes. <p> This fact is a consequence of the semantics of the particular CPS options used to obtain the data <ref> [39] </ref>. 40 Table 3.4 Perfect Code Optimistic Parallelization Results, Scalars Ignored parallel serial execution execution code time time speedup (cycles) (cycles) QCD 1351145 29345559 21.72 TRACK 2046731 118108037 57.71 OCEAN 43907 76829120 1749.81 DYFESM 550593 50435459 91.60 ARC2D 82218 736835249 896.20 TRFD 752 12638412 16806.40 SPEC77 291563 708826704 2431.13 3.3 Effect <p> The results of the experiments are summarized in Table 3.6. The figure shows N G , normalized parallel execution time (as in Table 3.5), for L G =100 cycles and various prefetching and forwarding schemes. The various schemes are defined in <ref> [39, 40] </ref> and are described here in order of decreasing aggressiveness (i.e., degree of latency tolerance): Scheme Description Scheme 1 Prefetching or forwarding operations may be issued as soon as data dependences are satisfied.
Reference: [40] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "EPG source code instrumentation tools internal documentation," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <year> 1994. </year>
Reference-contexts: Parafrase-2 features include multiple parsers and source code generators, data and control dependence analyses [37, 38], interprocedural and symbolic analyses, and a suite of transformation passes. The EPG (1) source code instrumentation tools [39] are implemented as a set of passes within Parafrase-2 <ref> [40] </ref>. These tools perform instrumentation and constitute the front-end for EPG-sim. EPG handles source codes in a robust manner, with little or no user intervention, and produces efficient instrumented codes. Events of interest are identified statically by EPG or by instrumenting to identify dynamic instances of events. <p> The results of the experiments are summarized in Table 3.6. The figure shows N G , normalized parallel execution time (as in Table 3.5), for L G =100 cycles and various prefetching and forwarding schemes. The various schemes are defined in <ref> [39, 40] </ref> and are described here in order of decreasing aggressiveness (i.e., degree of latency tolerance): Scheme Description Scheme 1 Prefetching or forwarding operations may be issued as soon as data dependences are satisfied.
Reference: [41] <author> A. V. Aho, R. Sethi, and J. D. Ullman, </author> <booktitle> Compilers: Principles, Techniques and Tools. </booktitle> <address> Reading, MA: </address> <publisher> Addison-Wesley, </publisher> <year> 1986. </year>
Reference-contexts: Events may be associated with the value of simulated time at which they occur, or with the static task in the source code (a particular subroutine, loop, basic block <ref> [41] </ref>, or statement) in which they occur. Histogramming allows profiles to be produced that show the behavior or parallelism of an instrumented code as a function of simulation time or of static tasks. <p> The only type of synchronization that occurs is barrier synchronization at the ends of parallel loops. 5.3.2.2 Scanning rules The first two steps in the forwarding algorithm implement an interprocedural DEF-USE analysis <ref> [41, 75] </ref> that is performed on a per-task basis given the parallel execution model defined above. This analysis requires a set of rules for scanning an application code to locate particular references. <p> Serial tasks end at the start of the next parallel loop. Parallel tasks end at the end of parallel loop iterations or at the start of a CDOALL loop nested inside an SDOALL loop. Scanning does not follow back edges of explicit or natural loops <ref> [41] </ref>, since code in serial loop iterations executes on a single processor and since task boundaries occur at the ends of parallel loops. Control flow is dealt with by using branch profiling to predict likely branch outcomes; scanning follows the most likely branch path.
Reference: [42] <author> C. D. Polychronopoulos, </author> <title> "Toward auto-scheduling compilers," </title> <journal> Journal of Supercomputing, </journal> <volume> vol. 2, no. 3, </volume> <pages> pp. 297-330, </pages> <month> May </month> <year> 1988. </year>
Reference-contexts: The processor model uses the notion of a task grain size, particularly when optimistically parallelizing codes. Task grain sizes include: subroutines, loop iterations (i.e., a DOALL/DOACROSS model of execution, that may or may not allow high-level spreading <ref> [42] </ref> between loops), basic blocks, statements, and individual operations. Tasks at the specified grain size are allowed to execute in parallel, completely or partially overlapping with the execution of other tasks, subject to their dynamic dependences or synchronization and resource availability. Operations within a particular task must 14 execute serially.
Reference: [43] <author> D.-K. Chen, H.-M. Su, and P.-C Yew, </author> <title> "The impact of synchronization and granularity on parallel systems," </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 239-249, </pages> <year> 1990. </year>
Reference-contexts: The processor scheduler is implemented as a runtime library routine that can be used to implement arbitrary scheduling policies. Scheduling strategies include round-robin, earliest available processor, and near-optimal <ref> [43] </ref>, which attempts to maximize processor utilization. Intelligent instrumentation is used to make event generation more efficient. Instrumentation is guided by the wealth of information contained in the Parafrase-2 intermediate representation, which allows events to be easily identified and processed. <p> The minimum execution time of an optimistically parallelized code is the critical (longest) path length through its dynamic dependence graph [44, 45]. Variations on the techniques described here are also used to implement ETG and EDS. The techniques used are based on those used in MaxPar <ref> [43, 46] </ref>. Minimum parallel execution time is measured by instrumenting to compute the earliest time at which each task can execute, given complete dynamic, interprocedural knowledge of data and control dependences, task grain size and associated serialization constraints, processor scheduling, resource constraints, and other parameters. <p> Since CPS executes on uniprocessor hosts, instrumented statements physically execute in serial order (although their simulated execution order will be different). This physical execution order ensures that shadow variables are set and used in an order that is consistent with the dependences in the original program <ref> [43, 46] </ref>. As an example, consider the addition operation C = A + B. For this operation, the instrumentation shown in Figure 2.3 (a) is generated by EPG. <p> Finally, Section 3.6 presents conclusions. 3.1 Parallelism and Memory Reference Behavior Table 3.1 shows the minimum parallel execution time for optimistic parallelization of various Perfect codes when using unconstrained numbers of processors. The serial execution time and maximum speedup (or inherent parallelism <ref> [43] </ref>), of each code is also given, followed by the number of dynamic tasks instantiated, the number of dynamic subprogram calls made, and the speedup for 512 processors while using near-optimal processor scheduling. <p> MaxPar <ref> [43, 46] </ref> is a CPS tool that measures the potential performance, parallelism, and behavior of optimistically parallelized codes, given various architectural parameters.
Reference: [44] <author> P. M. Petersen and D. Padua, </author> <title> "Dynamic dependence analysis: a novel method for data dependence evaluation," </title> <booktitle> 5th Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <year> 1992. </year>
Reference-contexts: The minimum execution time of an optimistically parallelized code is the critical (longest) path length through its dynamic dependence graph <ref> [44, 45] </ref>. Variations on the techniques described here are also used to implement ETG and EDS. The techniques used are based on those used in MaxPar [43, 46]. <p> The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism [106, 107, 108, 109] and perform dynamic dependence analysis <ref> [44, 45, 110] </ref>. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in [29].
Reference: [45] <author> P. M. Petersen, </author> <title> "Evaluation of programs and parallelizing compilers using dynamic analysis techniques," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1273, </type> <year> 1993. </year>
Reference-contexts: The minimum execution time of an optimistically parallelized code is the critical (longest) path length through its dynamic dependence graph <ref> [44, 45] </ref>. Variations on the techniques described here are also used to implement ETG and EDS. The techniques used are based on those used in MaxPar [43, 46]. <p> The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism [106, 107, 108, 109] and perform dynamic dependence analysis <ref> [44, 45, 110] </ref>. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in [29].
Reference: [46] <author> D.-K. Chen, </author> <title> "MaxPar: an execution driven simulator for studying parallel systems," M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 917, </type> <month> October </month> <year> 1989. </year> <month> 153 </month>
Reference-contexts: The minimum execution time of an optimistically parallelized code is the critical (longest) path length through its dynamic dependence graph [44, 45]. Variations on the techniques described here are also used to implement ETG and EDS. The techniques used are based on those used in MaxPar <ref> [43, 46] </ref>. Minimum parallel execution time is measured by instrumenting to compute the earliest time at which each task can execute, given complete dynamic, interprocedural knowledge of data and control dependences, task grain size and associated serialization constraints, processor scheduling, resource constraints, and other parameters. <p> Since CPS executes on uniprocessor hosts, instrumented statements physically execute in serial order (although their simulated execution order will be different). This physical execution order ensures that shadow variables are set and used in an order that is consistent with the dependences in the original program <ref> [43, 46] </ref>. As an example, consider the addition operation C = A + B. For this operation, the instrumentation shown in Figure 2.3 (a) is generated by EPG. <p> MaxPar <ref> [43, 46] </ref> is a CPS tool that measures the potential performance, parallelism, and behavior of optimistically parallelized codes, given various architectural parameters.
Reference: [47] <author> J. Bruner, </author> <title> "Parsim user interface reference manual," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1002, </type> <month> September </month> <year> 1990. </year>
Reference-contexts: These event generators are coupled with parallel discrete-event simulators to implement EDS. ETG can be performed during EDS to produce traces that have the same physical order as their simulation time order. The overall structure of an EPG-sim EDS is shown in Chief <ref> [22, 47, 48] </ref> parallel discrete-event simulators are used to construct system models and to perform simulation. The Chief system is portable, which allows simulations to be executed on a variety of uniprocessor and parallel host machines.
Reference: [48] <author> J. Bruner, H. Cheong, A. Veidenbaum, and P.-C. Yew, </author> <title> "Chief: a parallel simulation environment for parallel systems," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1050, </type> <month> November </month> <year> 1990. </year>
Reference-contexts: These event generators are coupled with parallel discrete-event simulators to implement EDS. ETG can be performed during EDS to produce traces that have the same physical order as their simulation time order. The overall structure of an EPG-sim EDS is shown in Chief <ref> [22, 47, 48] </ref> parallel discrete-event simulators are used to construct system models and to perform simulation. The Chief system is portable, which allows simulations to be executed on a variety of uniprocessor and parallel host machines.
Reference: [49] <author> P. Konas and P.-C. Yew, </author> <title> "Synchronous parallel discrete-event simulation on shared-memory multiprocessors," </title> <booktitle> 6th Workshop on Parallel and Distributed Simulations, </booktitle> <address> Newport Beach, CA, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: A Chief simulator consists of a simulation model and a kernel library that drives the simulation. Multiple simulation paradigms are supported through the use of interchangeable kernel libraries. Paradigms currently supported use standard parallel discrete-event simulation, Chandy-Misra, or Time-Warp techniques <ref> [49] </ref>. Since Chief simulation kernels support parallel discrete-event simulation, EDS in EPG-sim is supported on uniprocessor or parallel hosts. Because of the features of the execution-driven event generators used, EDS in EPG-sim can be driven by serial, optimistically parallelized, or parallel codes.
Reference: [50] <author> C. J. Beckmann, "CARL: </author> <title> an architecture simulation language," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1066, </type> <month> December </month> <year> 1990. </year>
Reference-contexts: This flexibility allows parallel simulations of optimistically parallelized codes to be executed on parallel hosts. Chief simulation models are described and constructed by using a hardware description language, CARL <ref> [50] </ref>. CARL, an extension of C, supports hierarchical, component-based modeling of parallel systems at a wide range of levels of detail, from high-level abstract models to detailed behavioral register-transfer-level models.
Reference: [51] <author> W. Blume and R. Eigenmann, </author> <title> "Performance analysis of parallelizing compilers on the Perfect Benchmarks (R) programs," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 3, no. 6, </volume> <pages> pp. 643-656, </pages> <month> November </month> <year> 1992. </year>
Reference-contexts: The parallelism that can be identified (for example, at loop iteration grain) when using CPS is much higher than that which can be identified even by state-of-research compiler technology or hand optimization <ref> [51, 52, 53] </ref>. This optimism in CPS occurs for two major reasons. First, most real systems schedule parallel loop iterations in serial order. <p> Fixing the number of processors shows that some codes require large numbers of processors in order to exploit this parallelism effectively. Even with a fixed number of processors, CPS is able to identify considerably more parallelism than current parallelizing compilers <ref> [51, 52, 53] </ref>. The number of dynamic subprogram calls indicates the amount of interprocedural activity in each code. 36 number of task instantiations parallel execution time (cycles) 0 200000 0 1000000 2000000 3000000 4000000 against parallel execution time. Similar histograms for the remaining Perfect codes appear in Appendix A. <p> When induction-related data dependences are ignored, the data for TRFD corresponds to the data in Table 3.1. The results for TRFD-s show that restructuring reveals significant additional parallelism that could not be identified in TRFD. Much of the additional parallelism is created by eliminating induction variables <ref> [51, 52] </ref> and scalar data dependences. The results for TRFD-p show the effect of exploiting only the explicit parallelism in TRFD-s. <p> Ignoring scalar dependences has the effect of breaking all scalar inductions and ignoring all dependences that could be removed through the use of scalar expansion. The importance of removing complex inductions has been noted as being important to the parallel performance of TRFD <ref> [51, 52] </ref>. Comparison of the data in Table 3.3 indicates the effects of nontrivial inductions and shows the need for scalar expansion in TRFD. This effect is not observed in the data for TRFD-p, since these experiments considered only explicit parallelism and therefore no dynamic dependence analysis was performed.
Reference: [52] <author> W. Blume, </author> <title> "Success and limitations in automatic parallelization of Perfect Benchmarks programs," M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1249, </type> <month> July </month> <year> 1992. </year>
Reference-contexts: The parallelism that can be identified (for example, at loop iteration grain) when using CPS is much higher than that which can be identified even by state-of-research compiler technology or hand optimization <ref> [51, 52, 53] </ref>. This optimism in CPS occurs for two major reasons. First, most real systems schedule parallel loop iterations in serial order. <p> Fixing the number of processors shows that some codes require large numbers of processors in order to exploit this parallelism effectively. Even with a fixed number of processors, CPS is able to identify considerably more parallelism than current parallelizing compilers <ref> [51, 52, 53] </ref>. The number of dynamic subprogram calls indicates the amount of interprocedural activity in each code. 36 number of task instantiations parallel execution time (cycles) 0 200000 0 1000000 2000000 3000000 4000000 against parallel execution time. Similar histograms for the remaining Perfect codes appear in Appendix A. <p> When induction-related data dependences are ignored, the data for TRFD corresponds to the data in Table 3.1. The results for TRFD-s show that restructuring reveals significant additional parallelism that could not be identified in TRFD. Much of the additional parallelism is created by eliminating induction variables <ref> [51, 52] </ref> and scalar data dependences. The results for TRFD-p show the effect of exploiting only the explicit parallelism in TRFD-s. <p> Ignoring scalar dependences has the effect of breaking all scalar inductions and ignoring all dependences that could be removed through the use of scalar expansion. The importance of removing complex inductions has been noted as being important to the parallel performance of TRFD <ref> [51, 52] </ref>. Comparison of the data in Table 3.3 indicates the effects of nontrivial inductions and shows the need for scalar expansion in TRFD. This effect is not observed in the data for TRFD-p, since these experiments considered only explicit parallelism and therefore no dynamic dependence analysis was performed.
Reference: [53] <author> R. Eigenmann, J. Hoeflinger, G. Jaxon, Z. Li, and D. Padua, </author> <title> "Restructuring Fortran programs for Cedar," </title> <journal> Concurrency: Practice and Experience, </journal> <volume> vol. 5, no. 7, </volume> <pages> pp. 553-573, </pages> <month> October </month> <year> 1993. </year>
Reference-contexts: The parallelism that can be identified (for example, at loop iteration grain) when using CPS is much higher than that which can be identified even by state-of-research compiler technology or hand optimization <ref> [51, 52, 53] </ref>. This optimism in CPS occurs for two major reasons. First, most real systems schedule parallel loop iterations in serial order. <p> Fixing the number of processors shows that some codes require large numbers of processors in order to exploit this parallelism effectively. Even with a fixed number of processors, CPS is able to identify considerably more parallelism than current parallelizing compilers <ref> [51, 52, 53] </ref>. The number of dynamic subprogram calls indicates the amount of interprocedural activity in each code. 36 number of task instantiations parallel execution time (cycles) 0 200000 0 1000000 2000000 3000000 4000000 against parallel execution time. Similar histograms for the remaining Perfect codes appear in Appendix A.
Reference: [54] <author> D.-K. Chen and P.-C. Yew, </author> <title> "An empirical study of DOACROSS loops," </title> <booktitle> Proceedings of Supercomputing, </booktitle> <pages> pp. 620-632, </pages> <year> 1991. </year>
Reference-contexts: Second, loops are parallelized in CPS based on accurate, dynamic dependence analysis. Static compiler analysis cannot compete with the accuracy of dynamic dependence analysis and therefore makes conservative assumptions, which inhibits the parallelization of many loops that may actually be parallel in the dynamic sense <ref> [54] </ref>. Even hand optimization cannot parallelize loops whose dependences are functions of execution-time values. CPS cannot easily model memory latency effects caused by memory contention, network contention, or cache coherence activity.
Reference: [55] <author> E. Brewer, C. Dellarocas, A. Colbrook, and W. Weihl, "Proteus: </author> <title> a high performance parallel architecture simulator," </title> <institution> Laboratory for Computer Science, MIT, </institution> <address> Boston, MA, </address> <note> Technical Report MIT/LCS/TR-516, </note> <month> September </month> <year> 1991. </year>
Reference-contexts: Some EDS performance is sacrificed in EPG-sim, when compared with other EDS 32 Table 2.4 FLO52 Hit Ratio Versus Cache Block Size cache block size (bytes) code 8 16 32 64 128 256 FLO52 63.67 80.95 89.41 93.70 95.77 96.33 systems (e.g., Proteus <ref> [55] </ref>), in order to allow architecture-independent processor modeling and the integration of CPS and EDS. 2.4 Conclusions Execution-driven techniques instrument application codes to generate events for simulation or tracing. These techniques are more accurate and versatile than trace-driven techniques and are preferable for studying parallel systems. <p> The cache model maintains exact cache and directory state information, computes cache and memory access delays, and collects behavioral statistics. Network delays are computed by using an analytical delay model for indirect, multistage networks presented in [60] and used in the Proteus simulator <ref> [55] </ref>. The model has been validated in [55, 60] and found to provide reasonably accurate delays for a variety of network load conditions. The Cedar multiprocessor has a clustered architecture; thus, Cedar Fortran expresses parallelism by using several different types of parallel loops. <p> Network delays are computed by using an analytical delay model for indirect, multistage networks presented in [60] and used in the Proteus simulator [55]. The model has been validated in <ref> [55, 60] </ref> and found to provide reasonably accurate delays for a variety of network load conditions. The Cedar multiprocessor has a clustered architecture; thus, Cedar Fortran expresses parallelism by using several different types of parallel loops. <p> ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus <ref> [55] </ref>, PEET [114], and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts. Machine-language-level instrumentation is usually employed, performed either during compilation or in a compiler postprocessing step. <p> The ability to trade off simulation cost versus accuracy is an important feature in versatile EDS systems <ref> [30, 55] </ref>. This trade-off applies to both processor and system modeling. Existing systems employing machine-language-level instrumentation techniques are limited in their ability to model varying processor architectures.
Reference: [56] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "EPG-sim critical path simulation tools tutorial," </title> <institution> Center for Supercomputing Research and Development, University of Illinois, Urbana, IL, CSRD Report, </institution> <year> 1994. </year>
Reference-contexts: The ability to measure interprocedural synchronization overhead is a useful extension of previous CPS work. 34 3 PARALLELISM AND COMMUNICATION IN SCIENTIFIC BENCHMARKS This chapter describes the results of numerous types of EPG-sim CPS <ref> [39, 56] </ref> experiments involving Perfect codes.
Reference: [57] <author> E. H. Gornish, E. D. Granston, and A. V. Veidenbaum, </author> <title> "Compiler-directed data prefetching in multiprocessors with memory hierarchies," </title> <booktitle> Proceedings of the International Conference on Supercomputing, </booktitle> <pages> pp. 354-368, </pages> <year> 1990. </year>
Reference-contexts: Scheme 2 Prefetching or forwarding operations may be issued as soon as data and control dependences are satisfied. This idea is based on the prefetching algorithm in <ref> [57] </ref> and results in prefetching or forwarding only when data are certain to be used. <p> The TRACK results show that adding the control dependence constraint of Scheme 2 caused a significant performance degradation. Static control dependences and static data dependences were found to inhibit prefetching equally in <ref> [57] </ref>. The results for Scheme 2 show that dynamic control dependences have little impact on the performance of prefetching and forwarding for most codes. <p> The high percentage of cluster forwarding in DYFESM is caused by the high frequency of single-cluster parallel loops in this code. The availability of sufficient bandwidth is important to the performance of prefetching <ref> [3, 57] </ref> and forwarding. <p> The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing [3]. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access <ref> [3, 18, 57, 59, 82] </ref>. The experimental results presented in Chapter 4 suggest, barring the case of broadcast forwarding, that multistage, packet-switched networks provide ample bandwidth to support prefetching and forwarding. <p> This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence <ref> [57] </ref>. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. Prefetch buffers are extra caches used to store prefetched data. <p> Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence <ref> [57] </ref>. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. Prefetch buffers are extra caches used to store prefetched data. <p> In binding prefetching, prefetched data are not kept coherent with other caches and memory. Data prefetched in this way must be invalidated locally if there is any possibility of the data becoming stale. Prefetching into preload buffers is always binding [93]. In <ref> [57] </ref>, binding prefetch into local memory is complemented with a prefetching algorithm designed to ensure that all prefetched data will be used, which avoids unnecessary prefetching and accesses to stale prefetched data. Different schemes control prefetching using hardware or software techniques. <p> The use of machine-language-level prefetching techniques can facilitate more efficient prefetching as a result of increased opportunities for low-level code optimization. Various software-initiated prefetching techniques have been proposed. In <ref> [57] </ref>, compiler analysis is used to insert source-level prefetch instructions. The goal of the compiler algorithm is to determine the earliest point at which a prefetch can occur while honoring the constraint that the prefetched data be guaranteed to be used. <p> Prefetching is somewhat effective for the codes studied, particularly for smaller cache block sizes, and results in speedups of up to a factor of four on eight processors. 134 A prefetching algorithm for a software cache coherent, shared memory multiprocessor is described in <ref> [57] </ref>. The modeled system has 32 processors with prefetch buffers connected to interleaved main memory modules via multistage networks. The proposed algorithm attempts to move prefetches back as far as possible while still guaranteeing correct use of data. <p> Instruction overhead is also reduced through the use of hybrid prefetching and forwarding (Section 5.1). The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops <ref> [2, 57, 96] </ref>. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. <p> Dynamic binding allows more efficient use of local memory at the expense of more complex runtime support. Binding forwarding can be used in conjunction with software coherence techniques, in a manner similar to the binding prefetching approach taken in <ref> [57] </ref>. Rather than focusing on statement-level communication within a single DOACROSS loop, the forwarding scheme presented in Subsection 4.1.3 focuses on the more prevalent loop-level communication between successive parallel loops.
Reference: [58] <author> D. K. Poulsen and P.-C. Yew, </author> <title> "Data prefetching and data forwarding in shared memory multiprocessors," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> II-276-280, </address> <year> 1994. </year>
Reference-contexts: expansion; the sensitivity of performance to various types of memory access delays; and the effects of varying parallel task grain sizes. 51 4 PREFETCHING AND FORWARDING ARCHITECTURES AND ALGORITHMS This chapter presents two different multiprocessor software-initiated data prefetching algorithms and a data forwarding scheme for cache coherent, shared memory multiprocessors <ref> [58] </ref>. The effectiveness of these schemes for reducing memory latency caused by interprocessor communication is evaluated via EDS of a modeled parallel system that executes large, parallel, numerical application codes. Blocked vector prefetching is shown to offer an attractive cost/performance trade-off for the codes studied.
Reference: [59] <author> T.-F. Chen, </author> <title> "Data prefetching for high-performance processors," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science and Engineering, University of Washington, Seattle, WA, </institution> <type> Technical Report 93-07-01, </type> <month> July </month> <year> 1993. </year>
Reference-contexts: The memory system provides a one-cycle cache hit latency; round trip network and main memory access delay result in a base 100-cycle cache miss latency under light network loads (this latency was chosen largely to facilitate comparison with the results of other prefetching studies <ref> [2, 3, 17, 59] </ref>). Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks are composed of 2x2 switches which are of the output queue variety and perform cut-through routing [60]. <p> This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Deliver instructions. A prefetch instruction is similar to a regular load, except that it is nonblocking and is dropped on exceptions or page faults <ref> [18, 59] </ref>. A Forwarding Write is similar to a regular write, except that it has different coherence semantics, as discussed in Subsection 5.5.2. <p> In the broadest sense, these capabilities are provided by using lockup-free caches [62]; however, general lockup-free capabilities are not necessarily required <ref> [18, 59] </ref>. 5.5.2.1 Support for prefetching Cache design to support software-initiated prefetching has been discussed in detail in [18, 59]. Support for prefetching requires the ability to issue multiple prefetches that are nonblocking with respect to processors. Regular loads still block as in a traditional single-issue architecture. <p> In the broadest sense, these capabilities are provided by using lockup-free caches [62]; however, general lockup-free capabilities are not necessarily required <ref> [18, 59] </ref>. 5.5.2.1 Support for prefetching Cache design to support software-initiated prefetching has been discussed in detail in [18, 59]. Support for prefetching requires the ability to issue multiple prefetches that are nonblocking with respect to processors. Regular loads still block as in a traditional single-issue architecture. <p> While prefetch issue buffers may offer a potential design simplification, using a single write buffer may offer similar performance [18]. 123 Cache operation is similar whether using separate prefetch issue buffers or a single write buffer <ref> [18, 59] </ref>. Each issued prefetch first checks to see whether the requested data are available in cache. If not, the prefetch is compared with other prefetches in the appropriate buffer. If a match is found, the new prefetch is canceled. <p> Each issued prefetch first checks to see whether the requested data are available in cache. If not, the prefetch is compared with other prefetches in the appropriate buffer. If a match is found, the new prefetch is canceled. Regular loads are blocking and have higher priority than prefetches <ref> [59] </ref>. Unlike writes, prefetches can optionally be dropped if issue or write buffers are full; exercising this option may improve performance compared to blocking on full buffers [18]. 5.5.2.2 Support for forwarding Software-initiated nonbinding forwarding into processor caches also requires that caches accept incoming forwarded data while handling processor requests. <p> The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing [3]. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access <ref> [3, 18, 57, 59, 82] </ref>. The experimental results presented in Chapter 4 suggest, barring the case of broadcast forwarding, that multistage, packet-switched networks provide ample bandwidth to support prefetching and forwarding. <p> Since the network and memory request statistics presented in Subsection 4.3.3 show that utilization and contention are generally low, lower-cost interconnection architectures that provide less bandwidth than the simulated networks may offer attractive cost/performance trade-offs. Results in <ref> [3, 59] </ref> indicate that a single bus may not provide adequate bandwidth, but that split buses and interleaved memories may perform almost as well as a pipelined memory system [59]. <p> Results in [3, 59] indicate that a single bus may not provide adequate bandwidth, but that split buses and interleaved memories may perform almost as well as a pipelined memory system <ref> [59] </ref>. <p> This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence <ref> [3, 16, 17, 59, 97] </ref> or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. <p> Machine-language-level techniques are used in [93, 94] to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. Hardware-initiated prefetching <ref> [59, 71, 72, 97] </ref> uses extra hardware to initiate prefetching. These schemes provide support for strided prefetching and frequently have the dynamic ability to predict strides for use in future prefetching operations. Hardware-initiated prefetching schemes do not depend on compiler technology but can be expensive to implement. <p> Software-initiated prefetching is inserted by hand for three SPLASH benchmarks [98] by using knowledge of the algorithms. Prefetching is found 133 to result in substantial performance benefits with low instruction overhead. The low overhead of prefetching is a result of the prefetching strategies used. Chen <ref> [59] </ref> describes a comparison between hardware-initiated and software-initiated prefetching schemes for a hardware cache coherent, shared memory architecture. The modeled system has 16 processors with caches connected to interleaved main memory modules via multistage interconnection networks (MINs). <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes <ref> [3, 17, 59] </ref>. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [60] <author> C. P. Kruskal and M. Snir, </author> <title> "The performance of multistage interconnection networks for multiprocessors," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-32, no. 12, </volume> <pages> pp. 1091-1098, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: Uniform memory access (UMA) to main memory is assumed to simplify simulations. The multistage networks are composed of 2x2 switches which are of the output queue variety and perform cut-through routing <ref> [60] </ref>. Each processor is capable of in-order single instruction issue and has the ability to process vector, prefetching, and forwarding instructions. Basic arithmetic and logical operations take one cycle each; vector results are retired at the rate of one per cycle. <p> The cache model maintains exact cache and directory state information, computes cache and memory access delays, and collects behavioral statistics. Network delays are computed by using an analytical delay model for indirect, multistage networks presented in <ref> [60] </ref> and used in the Proteus simulator [55]. The model has been validated in [55, 60] and found to provide reasonably accurate delays for a variety of network load conditions. The Cedar multiprocessor has a clustered architecture; thus, Cedar Fortran expresses parallelism by using several different types of parallel loops. <p> Network delays are computed by using an analytical delay model for indirect, multistage networks presented in [60] and used in the Proteus simulator [55]. The model has been validated in <ref> [55, 60] </ref> and found to provide reasonably accurate delays for a variety of network load conditions. The Cedar multiprocessor has a clustered architecture; thus, Cedar Fortran expresses parallelism by using several different types of parallel loops.
Reference: [61] <institution> FX/Series Architecture Manual, Littleton, MA: Alliant Computer Systems Corporation, </institution> <year> 1986. </year>
Reference-contexts: Barrier synchronization primitives are provided for use at the beginning or end of parallel loops, and queue-based locks are supported for use by particular applications. The loop scheduling and synchronization mechanisms are modeled after those used in the Cedar multiprocessor <ref> [16, 61] </ref>. Each processor has a 32K word, four-way set-associative, write-back, write-allocate cache with single word cache blocks, write buffers, and LRU replacement. A large cache size (compared to the working set sizes of the applications studied) is used to highlight communication effects while de-emphasizing cache conflict behavior. <p> Several dynamic scheduling techniques for reducing load imbalance, particularly for memory access delay, have been described in the literature; these techniques include multithreading, dynamic instruction scheduling, and instruction lookahead. The Alliant FX/80 <ref> [61] </ref> and the Cedar multiprocessor [16] employ hardware mechanisms for processor self-scheduling of parallel loop iterations to facilitate dynamic scheduling without excessive runtime overhead. Compiler analysis and transformations can be used to reduce the necessity for dynamic scheduling.
Reference: [62] <author> D. Kroft, </author> <title> "Lockup-free instruction fetch/prefetch cache organization," </title> <booktitle> Proceedings of the 8th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 81-87, </pages> <year> 1981. </year>
Reference-contexts: A large cache size (compared to the working set sizes of the applications studied) is used to highlight communication effects while de-emphasizing cache conflict behavior. Single word cache blocks are used to minimize false sharing effects, particularly those caused or aggravated by prefetching [3]. Processor caches are lockup-free <ref> [62] </ref> to allow multiple outstanding prefetches (regular reads use blocking loads). The cache coherence scheme 60 uses a three-state, directory-based invalidation protocol. <p> In the broadest sense, these capabilities are provided by using lockup-free caches <ref> [62] </ref>; however, general lockup-free capabilities are not necessarily required [18, 59]. 5.5.2.1 Support for prefetching Cache design to support software-initiated prefetching has been discussed in detail in [18, 59]. Support for prefetching requires the ability to issue multiple prefetches that are nonblocking with respect to processors.
Reference: [63] <author> A. Gupta, W.-D. Weber, and T. Mowry, </author> <title> "Reducing memory and traffic requirements for scalable directory-based cache coherence schemes," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> I-312-321, </address> <year> 1990. </year>
Reference-contexts: Processor caches are lockup-free [62] to allow multiple outstanding prefetches (regular reads use blocking loads). The cache coherence scheme 60 uses a three-state, directory-based invalidation protocol. Directories are distributed across the main memory modules, are full-mapped, and are organized as pointer caches <ref> [63] </ref> to reduce their size. 4.2.2 Simulation environment Experimental results are acquired by using EPG-sim behavioral EDS for various parallel Perfect codes. Events simulated include global and private memory accesses, parallel loop setup and scheduling operations, and synchronization operations.
Reference: [64] <author> M. Dubois, J. Skeppstedt, L. Ricciulli, K. Ramamurthy, and P. Stenstrom, </author> <title> "The detection and elimination of useless misses in multiprocessors," </title> <booktitle> Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 88-97, </pages> <year> 1993. </year>
Reference-contexts: The scheme is approximate, but it leads to a straightforward online implementation in a cache simulator and it provides useful information at low cost. Several other researchers have addressed the problem of accurately classifying cache misses <ref> [64, 65, 66] </ref>; a discussion of these accuracy issues is beyond the scope of this work. <p> Increasing the cache block size, however, would most likely lead to false sharing in the other three codes studied. False sharing has been noted as prominent, especially in numerical applications <ref> [3, 64, 65, 66] </ref>.
Reference: [65] <author> S. J. Eggers and T. E. Jeremiassen, </author> <title> "Eliminating false sharing," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. </pages> <address> I-377-381, </address> <year> 1991. </year>
Reference-contexts: The scheme is approximate, but it leads to a straightforward online implementation in a cache simulator and it provides useful information at low cost. Several other researchers have addressed the problem of accurately classifying cache misses <ref> [64, 65, 66] </ref>; a discussion of these accuracy issues is beyond the scope of this work. <p> Increasing the cache block size, however, would most likely lead to false sharing in the other three codes studied. False sharing has been noted as prominent, especially in numerical applications <ref> [3, 64, 65, 66] </ref>.
Reference: [66] <author> J. Torrellas, M. J. Lam, and J. L. Hennessy, </author> <title> "Shared data placement optimizations to reduce multiprocessor cache misses," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 266-270, </pages> <year> 1990. </year>
Reference-contexts: The scheme is approximate, but it leads to a straightforward online implementation in a cache simulator and it provides useful information at low cost. Several other researchers have addressed the problem of accurately classifying cache misses <ref> [64, 65, 66] </ref>; a discussion of these accuracy issues is beyond the scope of this work. <p> Increasing the cache block size, however, would most likely lead to false sharing in the other three codes studied. False sharing has been noted as prominent, especially in numerical applications <ref> [3, 64, 65, 66] </ref>.
Reference: [67] <author> C. J. Beckmann, </author> <title> "Hardware and software for functional and fine grain parallelism," </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical and Computer Engineering, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1346, </type> <year> 1993. </year>
Reference-contexts: This section explores the performance trade-offs that result from the use of processor self-scheduling in place of round-robin scheduling. Round-robin scheduling is static, in that the scheduling of parallel loop iterations to processors is predictable at compile time. Processor self-scheduling is a form of dynamic, demand-based scheduling 80 <ref> [67, 68, 69] </ref>. Dynamic scheduling allows an executing parallel code to achieve a reasonable load balance among processors and to maximize processor utilization in spite of conditions that may not be predictable at compile time.
Reference: [68] <author> C. J. Beckmann and C. D. Polychronopoulos, </author> <title> "The effect of barrier synchronization and scheduling overhead on parallel loops," </title> <booktitle> Proceedings of the International Conference on Parallel Processing, </booktitle> <pages> pp. 200-204, </pages> <year> 1989. </year>
Reference-contexts: This section explores the performance trade-offs that result from the use of processor self-scheduling in place of round-robin scheduling. Round-robin scheduling is static, in that the scheduling of parallel loop iterations to processors is predictable at compile time. Processor self-scheduling is a form of dynamic, demand-based scheduling 80 <ref> [67, 68, 69] </ref>. Dynamic scheduling allows an executing parallel code to achieve a reasonable load balance among processors and to maximize processor utilization in spite of conditions that may not be predictable at compile time.
Reference: [69] <author> C. D. Polychronopoulos and D. J. Kuck, </author> <title> "Guided self-scheduling: a practical scheduling scheme for parallel supercomputers," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-36, no. 12, </volume> <pages> pp. 1425-1439, </pages> <month> December </month> <year> 1987. </year>
Reference-contexts: This section explores the performance trade-offs that result from the use of processor self-scheduling in place of round-robin scheduling. Round-robin scheduling is static, in that the scheduling of parallel loop iterations to processors is predictable at compile time. Processor self-scheduling is a form of dynamic, demand-based scheduling 80 <ref> [67, 68, 69] </ref>. Dynamic scheduling allows an executing parallel code to achieve a reasonable load balance among processors and to maximize processor utilization in spite of conditions that may not be predictable at compile time.
Reference: [70] <author> F. Mounes-Toussi and D. J. Lilja, </author> <title> "Using compile-time analysis to adapt the cache coherence enforcement strategy to the data sharing characteristics," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <note> to appear, 1994. 155 </note>
Reference-contexts: In this section, a hybrid prefetching and forwarding scheme is described that attempts to adapt to the sharing characteristics of different references 87 and different application codes. This hybrid scheme was inspired by techniques employed in compiler-assisted adaptive cache coherence schemes <ref> [70] </ref> that use cost functions to decide whether updating or invalidating is most appropriate for particular references, based on their sharing patterns. Cost functions are used in [70] to choose updating or invalidating for particular references, based on the cost of global traffic required to maintain coherence in each case. <p> This hybrid scheme was inspired by techniques employed in compiler-assisted adaptive cache coherence schemes <ref> [70] </ref> that use cost functions to decide whether updating or invalidating is most appropriate for particular references, based on their sharing patterns. Cost functions are used in [70] to choose updating or invalidating for particular references, based on the cost of global traffic required to maintain coherence in each case. <p> Migratory references are primary candidates for forwarding because of their sequential sharing patterns. Forwarding may also be beneficial for mostly-read references to reduce the latency of initial read accesses after writes. Determination of reference types can be performed by using data dependence analysis <ref> [70] </ref> or profiling techniques [73]. A cost function can be used to determine whether particular accesses should be forwarded, in a manner analogous to how communication cost functions are used in [70] to choose updating or invalidating for particular accesses based on the relative cost of the corresponding coherence actions. 105 <p> Determination of reference types can be performed by using data dependence analysis <ref> [70] </ref> or profiling techniques [73]. A cost function can be used to determine whether particular accesses should be forwarded, in a manner analogous to how communication cost functions are used in [70] to choose updating or invalidating for particular accesses based on the relative cost of the corresponding coherence actions. 105 For round-robin scheduling, the problem of determining destination processors for forwarding can be addressed by using data dependence analysis techniques. <p> The action of data forwarding is similar, but not identical, to the update action in an update coherence protocol; thus, data forwarding in a shared memory architecture is similar to an adaptive update/invalidate cache coherence protocol <ref> [70, 73] </ref>. Whereas an update protocol only sends data to processors that previously held a copy of a cache block, sender-initiated forwarding allows updates to be sent unilaterally to destination processors, which permits more efficient sequential data sharing.
Reference: [71] <author> J.-L. Baer and T.-F. Chen, </author> <title> "An effective on-chip preloading scheme to reduce data access penalty," </title> <booktitle> Proceedings of Supercomputing, </booktitle> <pages> pp. 176-186, </pages> <year> 1991. </year>
Reference-contexts: This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Machine-language-level techniques are used in [93, 94] to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. Hardware-initiated prefetching <ref> [59, 71, 72, 97] </ref> uses extra hardware to initiate prefetching. These schemes provide support for strided prefetching and frequently have the dynamic ability to predict strides for use in future prefetching operations. Hardware-initiated prefetching schemes do not depend on compiler technology but can be expensive to implement. <p> Hardware-initiated prefetching schemes can be used to eliminate software prefetch generation overhead at the expense of complex prefetching hardware <ref> [71, 72] </ref>. In the results presented in Section 4.3, instruction overhead is reduced by using blocked vector prefetching instead of software pipelined prefetching, by restricting code expansion when using software pipelined prefetching, and by using forwarding, particularly with Forwarding Write operations. <p> Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks <ref> [71, 94] </ref>. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes. <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes <ref> [71, 95, 97] </ref> or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [72] <author> T.-F. Chen and J.-L. Baer, </author> <title> "Reducing memory latency via non-blocking and prefetching caches," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 51-61, </pages> <year> 1992. </year>
Reference-contexts: This approach to cache size analysis is common to many simulation studies of data prefetching that have appeared in the literature Using a single reduced cache size to compare the performance of the various applications would unfairly penalize some applications while exaggerating the performance of others <ref> [2, 17, 18, 59, 71, 72] </ref>. A 32K word cache size was used in the experiments described in Chapter 4. Figure 5.7 shows the effect of decreasing the cache size on conflict miss ratio and execution time. <p> Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Machine-language-level techniques are used in [93, 94] to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. Hardware-initiated prefetching <ref> [59, 71, 72, 97] </ref> uses extra hardware to initiate prefetching. These schemes provide support for strided prefetching and frequently have the dynamic ability to predict strides for use in future prefetching operations. Hardware-initiated prefetching schemes do not depend on compiler technology but can be expensive to implement. <p> The modeled system has 16 processors with caches connected to interleaved main memory modules via multistage interconnection networks (MINs). Software-initiated prefetching into processor cache is performed by manually inserting prefetching instructions into SPLASH codes according to the strategy described in [2]. Hardware-initiated prefetching is performed as in <ref> [72] </ref>. The results for software-initiated prefetching are variable and do not approach best-case performance for most of the codes studied. Prefetching results for vector prefetching in the Cedar multiprocessor are presented in [16]. <p> Hardware-initiated prefetching schemes can be used to eliminate software prefetch generation overhead at the expense of complex prefetching hardware <ref> [71, 72] </ref>. In the results presented in Section 4.3, instruction overhead is reduced by using blocked vector prefetching instead of software pipelined prefetching, by restricting code expansion when using software pipelined prefetching, and by using forwarding, particularly with Forwarding Write operations. <p> The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies <ref> [72, 93, 95] </ref>. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59].
Reference: [73] <author> J. K. Bennett, J. B. Carter, and W. Zwaenepoel, </author> <title> "Adaptive software cache management for distributed shared memory architectures," </title> <booktitle> Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 125-134, </pages> <year> 1990. </year>
Reference-contexts: Marking eligible write accesses can be accomplished given compiler analysis that determines reference types. Weber and Gupta suggest classifying references as read-only, migratory, synchronization, mostly-read, or frequently read/written [19]. A similar classification scheme is proposed in <ref> [73] </ref>. Migratory references are primary candidates for forwarding because of their sequential sharing patterns. Forwarding may also be beneficial for mostly-read references to reduce the latency of initial read accesses after writes. Determination of reference types can be performed by using data dependence analysis [70] or profiling techniques [73]. <p> proposed in <ref> [73] </ref>. Migratory references are primary candidates for forwarding because of their sequential sharing patterns. Forwarding may also be beneficial for mostly-read references to reduce the latency of initial read accesses after writes. Determination of reference types can be performed by using data dependence analysis [70] or profiling techniques [73]. <p> The stream of writes and reads generated by each processor is monitored during profiling, and references are classified according to the definitions used in <ref> [73] </ref>. Reference types are stored on a per-array basis, as opposed to a per-element basis. Migratory, mostly-read, and frequently read/written [19] reference types are all considered to be potential candidates for forwarding. <p> The action of data forwarding is similar, but not identical, to the update action in an update coherence protocol; thus, data forwarding in a shared memory architecture is similar to an adaptive update/invalidate cache coherence protocol <ref> [70, 73] </ref>. Whereas an update protocol only sends data to processors that previously held a copy of a cache block, sender-initiated forwarding allows updates to be sent unilaterally to destination processors, which permits more efficient sequential data sharing.
Reference: [74] <author> S. Hiranandani, K. Kennedy, and C.-W. Tseng, </author> <title> "Compiling Fortran D for MIMD distributed-memory machines," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, no. 8, </volume> <pages> pp. 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: This type of analysis has been applied to generating explicit message passing within a single loop nest for forwarding in DOACROSS loops [21], as well as for compiling languages such as Fortran D for message passing machines <ref> [74] </ref>. In the latter case, the problem is considerably simplified for a cache coherent, shared memory architecture, because the compiler does not have to generate data distributions; data distributions are implied for each loop based on loop partitioning and scheduling. <p> In the latter case, the problem is considerably simplified for a cache coherent, shared memory architecture, because the compiler does not have to generate data distributions; data distributions are implied for each loop based on loop partitioning and scheduling. Although the compiler analyses described in <ref> [21, 74] </ref> are applied to communication within DOACROSS loops or within parallel loop nests, these techniques are extensible to the problem of determining destination processors for forwarding between parallel loops.
Reference: [75] <author> D. A. Schouten, </author> <title> "An overview of interprocedural analysis techniques for high performance parallelizing compilers," M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1005, </type> <year> 1990. </year>
Reference-contexts: The only type of synchronization that occurs is barrier synchronization at the ends of parallel loops. 5.3.2.2 Scanning rules The first two steps in the forwarding algorithm implement an interprocedural DEF-USE analysis <ref> [41, 75] </ref> that is performed on a per-task basis given the parallel execution model defined above. This analysis requires a set of rules for scanning an application code to locate particular references.
Reference: [76] <author> W. Pugh, </author> <title> "A practical algorithm for exact array dependence analysis," </title> <journal> Communications of the ACM, </journal> <volume> vol. 35, no. 8, </volume> <pages> pp. 102-114, </pages> <month> August </month> <year> 1992. </year>
Reference-contexts: Multiple destination processors for a single write reference can also result if the compiler algorithm has several likely downstream reads from which to choose. In the general case, this set of simultaneous equations can be solved by using integer programming methods such as the Omega test <ref> [76] </ref>; however, many common cases yield closed-form solutions that can easily be determined. The following paragraphs enumerate some of these common cases and their solutions. Round-robin processor scheduling is assumed throughout this discussion.
Reference: [77] <author> Z. Li, P.-C. Yew, and C.-Q. Zhu, </author> <title> "An efficient data dependence analysis for parallelizing compilers," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 1, </volume> <pages> pp. 26-34, </pages> <month> January </month> <year> 1990. </year>
Reference-contexts: Forwarding_Write (A (i), p) = ... 115 determines the processor scheduling, and the solutions described above can be applied to this innermost loop. Arbitrarily complex subscript expressions can appear, including coupled subscripts <ref> [77] </ref> among outer loop index variables, as long as the innermost parallel loop index variable and its corresponding subscript expression appear in the form of one of the preceding solutions. 5.3.2.6 Hybrid prefetching and forwarding The compiler algorithm outlined in this section is extensible to support a compiler-based hybrid prefetching and
Reference: [78] <author> E. D. Granston, </author> <title> "Reducing memory access delays in large-scale shared-memory multiprocessors," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 1257, </type> <month> August </month> <year> 1992. </year>
Reference-contexts: Granston <ref> [78] </ref> describes compiler analysis techniques to determine reuse caused by intertask locality. <p> The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in [90]. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes <ref> [2, 78, 82, 83, 88] </ref>. The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. The analysis described in <ref> [78] </ref>, however, determines reuse caused by intertask locality and uses this analysis to support local memory management and prefetching. As described in Subsection 5.3.2, this type of analysis can also be used to implement software-initiated selective forwarding.
Reference: [79] <author> D. Gannon, W. Jalby, and K. Gallivan, </author> <title> "Strategies for cache and local memory management by global program transformation," </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 5, no. 5, </volume> <pages> pp. 587-616, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. <p> The effects of locality optimizations on prefetching are discussed in [18]. The following paragraphs summarize the uses of some of the transformations listed above and their effects on cache behavior <ref> [79, 82, 89] </ref>. Loop splitting is used to increase opportunities for parallelization. Statements in a loop are partitioned into separate loops, according to the strongly connected components of the data dependence graph. This breaks noncyclic dependences and typically reduces the amount of memory used by each loop after splitting. <p> Loop blocking has the desirable effect of forming inner loops with fixed-size amounts of work even if the total amount of work is not known at compile time. The combination of loop blocking or stripmining with loop interchange is particularly effective in enhancing locality and parallelism <ref> [79, 82] </ref>. Blocking may also be useful around pairs of successive parallel loops to balance communication and computation for data forwarding. 120 Transformations that affect loop partitioning and scheduling can significantly enhance intertask locality and thereby reduce the need for data forwarding.
Reference: [80] <author> D. B. Loveman, </author> <title> "Program improvement by source-to-source transformation," </title> <journal> Journal of the ACM, </journal> <volume> vol. 24, no. 1, </volume> <pages> pp. 121-145, </pages> <month> January </month> <year> 1977. </year>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85].
Reference: [81] <author> D. A. Padua and M. J. Wolfe, </author> <title> "Advanced compiler optimizations for supercomputers," </title> <journal> Communications of the ACM, </journal> <volume> vol. 29, no. 12, </volume> <pages> pp. 1184-1201, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85].
Reference: [82] <author> A. K. Porterfield, </author> <title> "Software methods for improvement of cache performance on supercomputer applications," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, Rice University, Technical Report Rice COMP TR88-93, </institution> <month> May </month> <year> 1989. </year> <month> 156 </month>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. <p> More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known <ref> [82, 83, 86, 87, 88] </ref>. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in [18]. <p> The effects of locality optimizations on prefetching are discussed in [18]. The following paragraphs summarize the uses of some of the transformations listed above and their effects on cache behavior <ref> [79, 82, 89] </ref>. Loop splitting is used to increase opportunities for parallelization. Statements in a loop are partitioned into separate loops, according to the strongly connected components of the data dependence graph. This breaks noncyclic dependences and typically reduces the amount of memory used by each loop after splitting. <p> Loop blocking has the desirable effect of forming inner loops with fixed-size amounts of work even if the total amount of work is not known at compile time. The combination of loop blocking or stripmining with loop interchange is particularly effective in enhancing locality and parallelism <ref> [79, 82] </ref>. Blocking may also be useful around pairs of successive parallel loops to balance communication and computation for data forwarding. 120 Transformations that affect loop partitioning and scheduling can significantly enhance intertask locality and thereby reduce the need for data forwarding. <p> The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in [90]. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes <ref> [2, 78, 82, 83, 88] </ref>. The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The results of this type of analysis are used to guide compiler transformations <ref> [82, 83, 88] </ref>, to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The use of a smaller cache size may be more effective than restructuring applications to reduce false sharing [3]. 126 5.5.3 Memory system and network bandwidth One of the key requirements of an architecture to support prefetching and forwarding is sufficient bandwidth for interprocessor communication and global memory access <ref> [3, 18, 57, 59, 82] </ref>. The experimental results presented in Chapter 4 suggest, barring the case of broadcast forwarding, that multistage, packet-switched networks provide ample bandwidth to support prefetching and forwarding.
Reference: [83] <author> M. E. Wolf, </author> <title> "Improving locality and parallelism in nested loops," </title> <type> Ph.D. dissertation, </type> <institution> Departments of Electrical Engineering and Computer Science, Stanford University, </institution> <type> Technical Report CSL-TR-92-538, </type> <month> August </month> <year> 1992. </year>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. <p> More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations <ref> [83, 85] </ref>. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known [82, 83, 86, 87, 88]. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. <p> More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known <ref> [82, 83, 86, 87, 88] </ref>. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in [18]. <p> The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in [90]. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes <ref> [2, 78, 82, 83, 88] </ref>. The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The results of this type of analysis are used to guide compiler transformations <ref> [82, 83, 88] </ref>, to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> As described in Subsection 5.3.2, this type of analysis can also be used to implement software-initiated selective forwarding. Wolf has described an overall compiler strategy for locality enhancement based on compiler reuse analysis <ref> [83, 88] </ref>. This strategy uses automated compiler techniques to develop a representation of the locality and reuse in an application code and then uses this representation to drive locality-enhancing transformations, including unimodular transformations and loop tiling.
Reference: [84] <author> M. J. Wolfe, </author> <title> "Optimizing compilers for supercomputers," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> CSRD Report 329, </type> <month> October </month> <year> 1982. </year>
Reference-contexts: is useful to guide transformation and locality enhancement strategies, to predict potential cache misses, and to generate useful prefetching and forwarding instructions. 5.4.1 Traditional locality-enhancing transformations Existing compiler transformations for optimizing cache behavior and communication include loop splitting (or distribution), loop fusion, loop interchange, loop blocking (or tiling), and stripmining <ref> [79, 80, 81, 82, 83, 84] </ref>. More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. <p> Loop skewing and related transformations are also useful in that they enable other transformations, such as loop fusion and loop interchange <ref> [84] </ref>.
Reference: [85] <author> U. Banerjee, </author> <title> "Unimodular transformations of double loops," </title> <booktitle> Proceedings of the 3rd Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year>
Reference-contexts: More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations <ref> [83, 85] </ref>. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known [82, 83, 86, 87, 88]. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling.
Reference: [86] <author> K. Gallivan, W. Jalby, U. Meier, and A. Sameh, </author> <title> "The impact of hierarchical memory systems on linear algebra algorithm design," </title> <journal> International Journal of Supercomputer Applications, </journal> <volume> vol. 2, no. 1, </volume> <pages> pp. 12-48, </pages> <month> Spring </month> <year> 1988. </year>
Reference-contexts: More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known <ref> [82, 83, 86, 87, 88] </ref>. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in [18].
Reference: [87] <author> M. S. Lam and M. E. Wolf, </author> <title> "Cache performance of blocked algorithms," </title> <booktitle> Proceedings of the 4th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 63-74, </pages> <year> 1991. </year>
Reference-contexts: More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known <ref> [82, 83, 86, 87, 88] </ref>. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in [18].
Reference: [88] <author> M. E. Wolf and M. S. Lam, </author> <title> "A data locality optimizing algorithm," </title> <booktitle> Proceedings of the ACM SIGPLAN Conference on Programming Language Design and Implementation, </booktitle> <pages> pp. 30-44, </pages> <year> 1991. </year>
Reference-contexts: More recently, loop interchange, loop skewing, loop reversal, and loop permutation transformations have been viewed as special cases of the more general class of unimodular transformations [83, 85]. The usefulness of these techniques for enhancing locality, parallelism, and performance is well known <ref> [82, 83, 86, 87, 88] </ref>. Experimental results in Subsection 4.3.5 suggest that these transformations, particularly stripmining, are effective in improving cache locality under processor self-scheduling. The effects of locality optimizations on prefetching are discussed in [18]. <p> The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in [90]. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes <ref> [2, 78, 82, 83, 88] </ref>. The results of this type of analysis are used to guide compiler transformations [82, 83, 88], to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> The results of this type of analysis are used to guide compiler transformations <ref> [82, 83, 88] </ref>, to predict cache misses [2], or to generate more effective prefetching [2] or forwarding. Most of these analysis techniques focus on a single nest of loops. <p> As described in Subsection 5.3.2, this type of analysis can also be used to implement software-initiated selective forwarding. Wolf has described an overall compiler strategy for locality enhancement based on compiler reuse analysis <ref> [83, 88] </ref>. This strategy uses automated compiler techniques to develop a representation of the locality and reuse in an application code and then uses this representation to drive locality-enhancing transformations, including unimodular transformations and loop tiling.
Reference: [89] <author> W. Abu-Sufah, </author> <title> "Improving the performance of virtual memory computers," </title> <type> Ph.D. dissertation, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <type> Technical Report UIUCDCS-R-78-945, </type> <month> November </month> <year> 1978. </year>
Reference-contexts: The effects of locality optimizations on prefetching are discussed in [18]. The following paragraphs summarize the uses of some of the transformations listed above and their effects on cache behavior <ref> [79, 82, 89] </ref>. Loop splitting is used to increase opportunities for parallelization. Statements in a loop are partitioned into separate loops, according to the strongly connected components of the data dependence graph. This breaks noncyclic dependences and typically reduces the amount of memory used by each loop after splitting.
Reference: [90] <author> E. T. Kalns and L. M. Ni, </author> <title> "Processor mapping techniques toward efficient data redistribution," </title> <booktitle> Proceedings of the 8th Annual International Parallel Processing Symposium, </booktitle> <pages> pp. 469-476, </pages> <year> 1994. </year>
Reference-contexts: Loop skewing and related transformations are also useful in that they enable other transformations, such as loop fusion and loop interchange [84]. The analogous problem of loop partitioning to minimize interprocessor communication between successive parallel loops for a message passing machine is considered in <ref> [90] </ref>. 5.4.2 Locality and reuse analysis Several researchers have addressed the necessity for compiler analysis that determines the reference locality and data reuse patterns of application codes [2, 78, 82, 83, 88].
Reference: [91] <institution> KSR1 Principles of Operation, Kendall Square Research Corporation, </institution> <year> 1991. </year>
Reference-contexts: Several machines already support these types of operations, including the Stanford Dash multiprocessor [20], the Kendall Square KSR1 <ref> [91] </ref>, and the Cedar multiprocessor [16]. The prefetch and Forwarding Write operations assumed in this work most closely resemble the Dash Prefetch and Deliver instructions. A prefetch instruction is similar to a regular load, except that it is nonblocking and is dropped on exceptions or page faults [18, 59].
Reference: [92] <author> D. E. Lenoski, </author> <title> "The design and analysis of Dash: a scalable directory-based multiprocessor," </title> <type> Ph.D. dissertation, </type> <institution> Departments of Electrical Engineering and Computer Science, Stanford University, </institution> <type> Technical Report CSL-TR-92-507, </type> <month> February </month> <year> 1992. </year>
Reference-contexts: Unlike prefetches and regular writes, Forwarding Writes cannot be dropped from full write buffers. The Forwarding Write operation has different coherence semantics than a regular write, although the semantics of Forwarding Write are somewhat similar to those of the Dash Deliver instruction <ref> [20, 92] </ref>. A Forwarding Write combines a write and a Deliver operation. Destination processors for forwarding, specified by the bit vector operand of a Forwarding Write instruction, receive copies of each forwarded cache block in the read-shared state.
Reference: [93] <author> W. Y. Chen, R. A. Bringmann, S. A. Mahlke, R. E. Hank, and J. E. Sicolo, </author> <title> "An efficient architecture for loop based data preloading," </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 92-101, </pages> <year> 1992. </year>
Reference-contexts: Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers <ref> [93] </ref>. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. Prefetch buffers are extra caches used to store prefetched data. <p> Preload buffers are similar to local memory, except that these buffers are organized and used as first-in, first-out queues. This simplified hardware organization can result in faster access times <ref> [93] </ref>. Preload buffers are used when the order in which prefetched data are to be used is predictable; prefetching into cache is used 130 Table 6.1 Summary of Prefetching Schemes This table summarizes various prefetching schemes described in the literature. <p> In binding prefetching, prefetched data are not kept coherent with other caches and memory. Data prefetched in this way must be invalidated locally if there is any possibility of the data becoming stale. Prefetching into preload buffers is always binding <ref> [93] </ref>. In [57], binding prefetch into local memory is complemented with a prefetching algorithm designed to ensure that all prefetched data will be used, which avoids unnecessary prefetching and accesses to stale prefetched data. Different schemes control prefetching using hardware or software techniques. <p> Prefetch distances are chosen according to compiler estimates of loop execution times. Loop splitting is used to prefetch for initial loop iterations and to avoid prefetching for final iterations. Machine-language-level prefetch instructions are used to evaluate these techniques. Machine-language-level techniques are used in <ref> [93, 94] </ref> to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. <p> Instruction overhead is an important performance issue in software-initiated prefetching schemes [1, 2]. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms [2], or code optimization <ref> [93, 94] </ref>. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies <ref> [72, 93, 95] </ref>. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59].
Reference: [94] <author> W. Y. Chen, S. A. Mahlke, P. P. Chang, and W. W. Hwu, </author> <title> "Data access microarchitectures for superscalar processors with compiler-assisted data prefetching," </title> <booktitle> Proceedings of the 24th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 69-73, </pages> <year> 1991. </year>
Reference-contexts: Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Prefetch buffers are extra caches used to store prefetched data. These buffers are kept coherent with other caches and memory, are typically small-sized and fully associative, and may have slower access times than processor caches because of their hardware complexity <ref> [94] </ref>. Preload buffers are similar to local memory, except that these buffers are organized and used as first-in, first-out queues. This simplified hardware organization can result in faster access times [93]. <p> Prefetch and preload buffers keep prefetched data separate from other data in processor caches, which reduces cache pollution due to prefetching <ref> [94] </ref>. The majority of the schemes use nonbinding prefetching. In nonbinding prefetching into cache, prefetched data can still be cast out due to cache replacement before the data are used. <p> Prefetch distances are chosen according to compiler estimates of loop execution times. Loop splitting is used to prefetch for initial loop iterations and to avoid prefetching for final iterations. Machine-language-level prefetch instructions are used to evaluate these techniques. Machine-language-level techniques are used in <ref> [93, 94] </ref> to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. <p> The authors propose program restructuring to reduce false sharing, but this restructuring is shown to have limited effectiveness. Reducing the cache block size can be a more effective remedy for the false sharing problem. Trade-offs between different prefetching architectures are considered in <ref> [94] </ref>. This work discusses the relative benefits of prefetching into cache or prefetch buffers, for small, on-chip caches and prefetch buffers and a uniprocessor architecture. The study concludes that using a small prefetch buffer is more effective than using a larger on-chip cache or a cache with greater associativity. <p> Instruction overhead is an important performance issue in software-initiated prefetching schemes [1, 2]. This overhead can be reduced through the use of additional hardware [96], more complex software algorithms [2], or code optimization <ref> [93, 94] </ref>. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks <ref> [71, 94] </ref>. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [95] <author> J. W. C. Fu, J. H. Patal, and B. L. Janssens, </author> <title> "Stride directed prefetching in scalar processors," </title> <booktitle> Proceedings of the 25th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 102-110, </pages> <year> 1992. </year> <month> 157 </month>
Reference-contexts: Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks [99] were also used in several studies <ref> [72, 93, 95] </ref>. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59]. <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes <ref> [71, 95, 97] </ref> or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [96] <author> A. C. Klaiber and H. M. Levy, </author> <title> "An architecture for software-controlled data prefetching," </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 43-53, </pages> <year> 1991. </year>
Reference-contexts: Few prefetching studies, however, have considered the application of multiprocessor software-initiated prefetching algorithms to large, numerical applications with loop-level and vector parallelism. This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures <ref> [1, 2, 71, 72, 93, 94, 95, 96] </ref>, while a few studies have considered prefetching for multiprocessors with hardware cache coherence [3, 16, 17, 59, 97] or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. <p> Prefetched data are stored in processor caches, local memory [57], prefetch buffers <ref> [96] </ref>, or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. Prefetch buffers are extra caches used to store prefetched data. <p> In this scheme, source-level prefetch instructions are inserted into loops in such a manner that individual variables or array elements are prefetched (with a prefetch distance of) one loop 132 iteration before they are used. Klaiber and Levy <ref> [96] </ref> extend this concept to multiple iteration prefetch distances, which are determined for each loop using compiler analysis, and use machine-language-level prefetch instructions. Mowry [2] further extends software pipelined prefetching by introducing new compiler analysis and transformation techniques. <p> Instruction overhead is an important performance issue in software-initiated prefetching schemes [1, 2]. This overhead can be reduced through the use of additional hardware <ref> [96] </ref>, more complex software algorithms [2], or code optimization [93, 94]. The overhead associated with software-initiated prefetching when simple algorithms are used is low [17]. <p> Instruction overhead is also reduced through the use of hybrid prefetching and forwarding (Section 5.1). The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops <ref> [2, 57, 96] </ref>. Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94].
Reference: [97] <author> J. W. C. Fu and J. H. Patel, </author> <title> "Data prefetching in multiprocessor vector cache memories," </title> <booktitle> Proceedings of the 18th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 54-63, </pages> <year> 1991. </year>
Reference-contexts: This section describes and compares these schemes (Table 6.1). Much of this work has considered prefetching for uniprocessor architectures [1, 2, 71, 72, 93, 94, 95, 96], while a few studies have considered prefetching for multiprocessors with hardware cache coherence <ref> [3, 16, 17, 59, 97] </ref> or software coherence [57]. Prefetched data are stored in processor caches, local memory [57], prefetch buffers [96], or preload buffers [93]. Cache is distinguished from local memory in that local memory is not kept coherent with other caches and memory. <p> Machine-language-level techniques are used in [93, 94] to insert and optimize prefetching. Code motion and loop unrolling are used to place prefetches sufficiently before loads to hide memory latency. Profiling is used to determine which accesses are likely cache misses and therefore candidates for prefetching. Hardware-initiated prefetching <ref> [59, 71, 72, 97] </ref> uses extra hardware to initiate prefetching. These schemes provide support for strided prefetching and frequently have the dynamic ability to predict strides for use in future prefetching operations. Hardware-initiated prefetching schemes do not depend on compiler technology but can be expensive to implement. <p> For this reason, and because of dissimilarities in the Cedar architecture, direct comparisons of the results are not meaningful. Prefetching does result in substantial performance improvements for the various Perfect codes. Hardware-initiated prefetching for a vector multiprocessor is explored in <ref> [97] </ref>. Simulations are driven by Perfect codes, but the modeled system consists of a small number of processors connected by a single bus with snooping caches and a relatively short global memory latency. <p> Larger kernels or SPEC benchmarks [99] were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes <ref> [71, 95, 97] </ref> or SPLASH codes [3, 17, 59]. Of these benchmarks, only Perfect and SPLASH benchmarks are representative of whole, parallel, numerical application codes.
Reference: [98] <author> J. P. Singh, W.-D. Weber, and A. Gupta, </author> <title> "SPLASH: Stanford parallel applications for shared-memory," </title> <institution> Computer Systems Laboratory, Stanford University, </institution> <type> Technical Report CSL-TR-91-469, </type> <month> April </month> <year> 1991. </year>
Reference-contexts: The Dash architecture and the entire hierarchy of L1, L2, and Remote Access Caches (RACs) are modeled in considerable detail, but simulations assume one processor per cluster and data are prefetched into RAC in most experiments. Software-initiated prefetching is inserted by hand for three SPLASH benchmarks <ref> [98] </ref> by using knowledge of the algorithms. Prefetching is found 133 to result in substantial performance benefits with low instruction overhead. The low overhead of prefetching is a result of the prefetching strategies used.
Reference: [99] <author> SPEC, </author> <title> The SPEC Benchmark Report. </title> <address> Fremont, CA: </address> <publisher> Waterside Associates, </publisher> <year> 1990. </year>
Reference-contexts: Instruction overhead is also reduced through the use of hybrid prefetching and forwarding (Section 5.1). The surveyed studies differ in the workloads used to drive simulations (Table 6.1). Some studies used only numerical subroutines, kernels, or individual Livermore loops [2, 57, 96]. Larger kernels or SPEC benchmarks <ref> [99] </ref> were also used in several studies [72, 93, 95]. One study used the RiCEPS compiler evaluation benchmarks, which contain some numerical kernels [1], and two studies used non-numerical benchmarks [71, 94]. Several studies used Perfect codes [71, 95, 97] or SPLASH codes [3, 17, 59].
Reference: [100] <author> W. Berke, </author> <title> "A cache technique for synchronization variables in highly parallel, shared memory systems," </title> <institution> Courant Institute of Mathematical Sciences, </institution> <address> New York University, New York, NY, </address> <note> Ultracomputer Note 151, </note> <month> December </month> <year> 1988. </year>
Reference-contexts: Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> This instruction forwards the specified cache block to one or more RACs. The block to be forwarded must already have been acquired by the issuing processor, by using normal memory requests, before a Deliver can be issued. The UpdateAll primitive <ref> [100] </ref> performs broadcast in a MIN-based system to implement forwarding; caches not holding a copy of the forwarded block ignore the broadcast. The Notify primitive [101] performs broadcast forwarding by using the broadcast invalidation mechanism in the cache coherence protocol. <p> Rather than focusing on statement-level communication within a single DOACROSS loop, the forwarding scheme presented in Subsection 4.1.3 focuses on the more prevalent loop-level communication between successive parallel loops. In the forwarding schemes surveyed, forwarding to multiple processors is accomplished by using Multicube broadcast [101], MIN-based broadcast <ref> [100] </ref>, unrestricted MIN-based multicast [4], or multiple point-to-point messages [20]. In MIN-based systems, broadcast or multiple point-to-point messages can result in significant network contention [4]; this consideration applies to other interconnect topologies as well.
Reference: [101] <author> J. R. Goodman, M. K. Vernon, and P. J. Woest, </author> <title> "Efficient synchronization primitives for large-scale cache-coherent multiprocessors," </title> <booktitle> Proceedings of the 3rd International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 64-75, </pages> <year> 1989. </year>
Reference-contexts: Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> The UpdateAll primitive [100] performs broadcast in a MIN-based system to implement forwarding; caches not holding a copy of the forwarded block ignore the broadcast. The Notify primitive <ref> [101] </ref> performs broadcast forwarding by using the broadcast invalidation mechanism in the cache coherence protocol. None of these schemes requires any directory information to accomplish forwarding. <p> Rather than focusing on statement-level communication within a single DOACROSS loop, the forwarding scheme presented in Subsection 4.1.3 focuses on the more prevalent loop-level communication between successive parallel loops. In the forwarding schemes surveyed, forwarding to multiple processors is accomplished by using Multicube broadcast <ref> [101] </ref>, MIN-based broadcast [100], unrestricted MIN-based multicast [4], or multiple point-to-point messages [20]. In MIN-based systems, broadcast or multiple point-to-point messages can result in significant network contention [4]; this consideration applies to other interconnect topologies as well.
Reference: [102] <author> M. D. Hill, J. R. Larus, S. K. Reinhardt, and D. A. Wood, </author> <title> "Cooperative shared memory: software and hardware for scalable multiprocessors," </title> <booktitle> Proceedings of the 5th International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 262-273, </pages> <year> 1992. </year>
Reference-contexts: Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> One benefit of this type of mechanism is that sending and receiving processors are not required to have knowledge of each other's identities, thereby relaxing processor scheduling constraints and facilitating the use of dynamic processor scheduling. Read-Update [103] and Cooperative Prefetch <ref> [102] </ref> are examples of forwarding read primitives. These schemes provide an extra directory bit that indicates whether each directory entry is being used for forwarding reads or for normal coherence transactions. Architectures to support forwarding in shared memory multiprocessors with multistage interconnection networks are described in [4].
Reference: [103] <author> J. Lee and U. Ramachandran, </author> <title> "Architectural primitives for a scalable shared memory multiprocessor," </title> <booktitle> Proceedings of the 3rd Annual ACM Symposium on Parallel Algorithms and Architectures, </booktitle> <pages> pp. 103-114, </pages> <year> 1991. </year>
Reference-contexts: Notification, like data prefetching, is receiver-initiated. In notification schemes, processors request that data be forwarded to them; data are implicitly forwarded to requesting processors when new values are produced. Most existing schemes implement nonbinding forwarding into processor cache <ref> [4, 20, 100, 101, 102, 103] </ref>. In nonbinding forwarding, as in nonbinding prefetching, data forwarded into caches can still be cast out as a result of cache replacement or coherence activity. <p> One benefit of this type of mechanism is that sending and receiving processors are not required to have knowledge of each other's identities, thereby relaxing processor scheduling constraints and facilitating the use of dynamic processor scheduling. Read-Update <ref> [103] </ref> and Cooperative Prefetch [102] are examples of forwarding read primitives. These schemes provide an extra directory bit that indicates whether each directory entry is being used for forwarding reads or for normal coherence transactions.
Reference: [104] <author> M. Kumar, </author> <title> "Effect of storage allocation/reclamation methods on parallelism and storage requirements," </title> <booktitle> Proceedings of the 14th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 197-205, </pages> <year> 1987. </year>
Reference-contexts: MaxPar [43, 46] is a CPS tool that measures the potential performance, parallelism, and behavior of optimistically parallelized codes, given various architectural parameters. The approach used in MaxPar 140 was first described in <ref> [104, 105] </ref> and has subsequently been used in a number of CPS tools that measure program parallelism [106, 107, 108, 109] and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models.
Reference: [105] <author> M. Kumar, </author> <title> "Measuring parallelism in computation-intensive scientific/engineering applications," </title> <journal> IEEE Transactions on Computers, </journal> <volume> vol. C-37, no. 9, </volume> <pages> pp. 1088-1098, </pages> <month> September </month> <year> 1988. </year>
Reference-contexts: MaxPar [43, 46] is a CPS tool that measures the potential performance, parallelism, and behavior of optimistically parallelized codes, given various architectural parameters. The approach used in MaxPar 140 was first described in <ref> [104, 105] </ref> and has subsequently been used in a number of CPS tools that measure program parallelism [106, 107, 108, 109] and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models.
Reference: [106] <author> W.-C. Chang, </author> <title> "An empirical study on the effectiveness of branch bypassing," M.S. </title> <type> thesis, </type> <institution> Department of Computer Science, University of Illinois, Urbana, IL, </institution> <year> 1991. </year>
Reference-contexts: The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism <ref> [106, 107, 108, 109] </ref> and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively.
Reference: [107] <author> J. R. Larus, </author> <title> "Loop-level parallelism in numeric and symbolic programs," </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 4, no. 7, </volume> <pages> pp. 812-826, </pages> <month> July </month> <year> 1993. </year>
Reference-contexts: The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism <ref> [106, 107, 108, 109] </ref> and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively.
Reference: [108] <author> J. R. Larus, </author> <title> "Parallelism in numeric and symbolic programs," </title> <booktitle> Proceedings of the 3rd Annual Workshop on Programming Languages and Compilers for Parallel Computing, </booktitle> <year> 1990. </year> <month> 158 </month>
Reference-contexts: The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism <ref> [106, 107, 108, 109] </ref> and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively.
Reference: [109] <author> L. Rauchwerger, P. K. Dubey, and R. Nair, </author> <title> "Measuring limits of parallelism and characterizing its vulnerability to resource constraints," </title> <booktitle> Proceedings of the 26th Annual International Symposium on Microarchitecture, </booktitle> <pages> pp. 105-117, </pages> <year> 1993. </year>
Reference-contexts: The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism <ref> [106, 107, 108, 109] </ref> and perform dynamic dependence analysis [44, 45, 110]. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively.
Reference: [110] <author> T. M. Austin and G. S. Sohi, </author> <title> "Dynamic dependency analysis of ordinary programs," </title> <booktitle> Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pp. 342-351, </pages> <year> 1992. </year>
Reference-contexts: The approach used in MaxPar 140 was first described in [104, 105] and has subsequently been used in a number of CPS tools that measure program parallelism [106, 107, 108, 109] and perform dynamic dependence analysis <ref> [44, 45, 110] </ref>. Previous CPS tools that employ source-level instrumentation have used somewhat simplistic processor models. ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in [29].
Reference: [111] <author> S. Eggers, D. Keppel, E. Koldinger, and H. Levy, </author> <title> "Techniques for efficient inline tracing on a shared-memory multiprocessor," </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 37-47, </pages> <year> 1990. </year>
Reference-contexts: ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace <ref> [111] </ref> and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus [55], PEET [114], and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts.
Reference: [112] <author> C. B. Stunkel and W. K. Fuchs, "TRAPEDS: </author> <title> producing traces for multicomputers via execution-driven simulation," </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 70-78, </pages> <year> 1989. </year>
Reference-contexts: ETG tools instrument serial or parallel codes to generate traces when executed on uniprocessor or parallel hosts, respectively. ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS <ref> [112] </ref>. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus [55], PEET [114], and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts.
Reference: [113] <author> S. R. Goldschmidt and H. Davis, </author> <title> "Tango introduction and tutorial," </title> <institution> Computer Systems Laboratory, Stanford University, </institution> <type> Technical Report, </type> <month> February </month> <year> 1991. </year>
Reference-contexts: ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango <ref> [30, 113] </ref>, Proteus [55], PEET [114], and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts. Machine-language-level instrumentation is usually employed, performed either during compilation or in a compiler postprocessing step.
Reference: [114] <author> D. Grunwald, G. J. Nutt, A. M. Sloane, D. Wagner, and B. Zorn, </author> <title> "A testbed for studying parallel programs and parallel execution architectures," </title> <institution> Department of Computer Science, University of Colorado, Boulder, CO, </institution> <type> Technical Report, </type> <month> April 28, </month> <year> 1992. </year>
Reference-contexts: ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus [55], PEET <ref> [114] </ref>, and RPPT [115]. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts. Machine-language-level instrumentation is usually employed, performed either during compilation or in a compiler postprocessing step. <p> The use of these techniques results in processor simulation models that are efficient but fairly architecture-specific (similar to that of the host architecture). Processor models are also somewhat dependent on the particular compiler being used, since the compiler often makes architecture-specific optimization decisions <ref> [114] </ref>. In PEET, an abstract tracing tool attempts to remove architectural-specific considerations, such as the number of processors, synchronization mechanisms, and 141 scheduling and other operating and runtime system policies, but the instruction-level processor model is still tied strongly to the host architecture.
Reference: [115] <author> R. C. Covington, S. Madala, V. Mehta, J. R. Jump, and J. B. Sinclair, </author> <title> "The Rice Parallel Processing Testbed," </title> <booktitle> Proceedings of the ACM Sigmetrics Conference on Measurement and Modeling of Computer Systems, </booktitle> <pages> pp. 4-11, </pages> <year> 1988. </year>
Reference-contexts: ETG techniques for serial codes are described in [29]. Tools for tracing parallel codes on parallel hosts include MPtrace [111] and TRAPEDS [112]. These tracing tools use machine-language-level instrumentation techniques. Examples of EDS systems include Tango [30, 113], Proteus [55], PEET [114], and RPPT <ref> [115] </ref>. These systems use serial or parallel codes to drive simulations; most perform EDS for parallel codes on uniprocessor hosts. Machine-language-level instrumentation is usually employed, performed either during compilation or in a compiler postprocessing step. Events can be output as traces (ETG) or event generation can be coupled with EDS.
Reference: [116] <author> S. R. Goldschmidt, </author> <title> "Simulation of multiprocessors: accuracy and performance," </title> <type> Ph.D. dissertation, </type> <institution> Department of Electrical Engineering, Stanford University, </institution> <month> June </month> <year> 1993. </year>
Reference-contexts: Events can be output as traces (ETG) or event generation can be coupled with EDS. Several simulators exist that perform EDS for parallel codes on parallel hosts. A parallel version of Tango exists that allows EDS to use general system models <ref> [116] </ref>. A parallel EDS system for distributed-memory hosts is described in [117]. The WWT system [118] performs parallel EDS on a CM5 host, but does not have the ability to model memory or interconnect contention. Some CPS tools and most general EDS tools use machine-language-level instrumentation and event generation techniques.
Reference: [117] <author> J. M. Lin and S. G. Abraham, </author> <title> "SIMPLE: an execution-driven multiprocessor simulator," </title> <institution> Department of Electrical Engineering and Computer Science, University of Michigan, </institution> <type> Technical Report, </type> <year> 1991. </year>
Reference-contexts: Several simulators exist that perform EDS for parallel codes on parallel hosts. A parallel version of Tango exists that allows EDS to use general system models [116]. A parallel EDS system for distributed-memory hosts is described in <ref> [117] </ref>. The WWT system [118] performs parallel EDS on a CM5 host, but does not have the ability to model memory or interconnect contention. Some CPS tools and most general EDS tools use machine-language-level instrumentation and event generation techniques.

References-found: 117

