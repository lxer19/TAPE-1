URL: http://www.math.tau.ac.il/~mansour/papers/97colt.ps.gz
Refering-URL: 
Root-URL: 
Email: fmansour,marianog@math.tau.ac.il  
Title: Learning with Maximum-Entropy Distributions  
Author: Yishay Mansour Mariano Schain 
Address: Tel-Aviv University  
Affiliation: Computer Science Dept.  
Abstract: We are interested in distributions which are derived as a maximum entropy distribution given a set of constraints. More specifically, we are interested in the case where the constraints are the expectation of individual and pairs of attributes. For such a given maximum entropy distribution we develop an efficient learning algorithm for read-once DNF. We also show how to extend our results to monotone read-k DNF, following the techniques of [HM91]
Abstract-found: 1
Intro-found: 1
Reference: [AK91] <author> Dana Angluin and Michael Kharitonov. </author> <booktitle> When won't membership queries help? In Proceedings of STOC '91, </booktitle> <pages> pages 44-454. </pages> <publisher> ACM, </publisher> <year> 1991. </year>
Reference-contexts: A much more complicated and non-intuitive result is showing that if DNF is learnable with membership queries, in the PAC model, then DNF can be learned also without membership queries <ref> [AK91] </ref>. Researchers were aware that the need to learn a concept class with respect to an arbitrary distribution is one of the main sources of intractability in the PAC model.
Reference: [BI88] <author> Gyora M. Benedek and Alon Itai. </author> <title> Learn-ability by fixed distributions. </title> <booktitle> In Workshop on Computational Learning Theory, </booktitle> <pages> pages 80-90. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: Research was done on the learnability with respect to a fixed distribution, and a model, similar to the PAC model, was suggested, where the learning algorithm may depend on the underline distribution <ref> [BI88, Nat87] </ref>. They also show examples where this allows to learn classes which where not learnable before. When looking for learning with respect to a fixed distribution, rather than an arbitrary one, it is most important to chose "natural" distributions. <p> The output of the learning algorithm is a hypothesis h 2 C. The error of h with respect to the distribution D and the target concept c is Error D (h; c) def Similar to <ref> [BI88] </ref>, we say that an algorithm PAC-learns the concept class C over distribution D if for any c 2 C given *; ffi &gt; 0 the algorithm outputs a hypothesis h 2 C, such that P r (Error D (h; c) *) ffi: We say that such a learning algorithm is
Reference: [CT91] <author> Thomas M. Cover and Joy A. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> John Wiley and Sons Inc., </publisher> <year> 1991. </year>
Reference-contexts: First let us deviate and discuss maximum entropy distributions. The entropy is a functional over probability distributions, intuitively it measures the uncertainty of an observer about the outcome of an experiment (see, e.g. <ref> [CT91] </ref>). Maximizing the entropy may be viewed in many cases as trying to adapt the "most permissive" distribution that obeys certain constraints. We view our setting as being given a set of constraints on the distri-bution, each constraint is an expectation of a predicate of the attributes.
Reference: [HKLW90] <author> David Haussler, Michael Kearns, Nick Lit-tlestone, and Manfred K. Warmuth. </author> <title> Equivalence of models for polynomial learnability. </title> <booktitle> Information and Computation, </booktitle> <year> 1990. </year>
Reference-contexts: In addition, due to the fact that the distribution may be arbitrary, it was rather simple to show equivalence fl This research was supported in part by a grant from the Israel Science Foundation. in the difficulty of related concept classes, such as general DNF and monotone read-once DNF <ref> [HKLW90] </ref>. A much more complicated and non-intuitive result is showing that if DNF is learnable with membership queries, in the PAC model, then DNF can be learned also without membership queries [AK91]. <p> Unfortunately, from a computational point of view, it is NP-complete to decide if there is any distribution that satisfies the set of pair-wise constraints [KM93, KN95]. As stated before, learning the read-once DNF concept class over arbitrary distribution is equivalent to DNF <ref> [HKLW90] </ref>. Constraining the underlying probability distribution to uniform or product distribution enables to learn read-once DNF [KLV94]. However, the algorithms for learning the read-once DNF over uniform or product distributions depends heavily on the fact that the attributes are independent.
Reference: [HM91] <author> Thomas Hancock and Yishay Mansour. </author> <title> Learning Monotone k DNF Formulas on Product Distributions. </title> <booktitle> In Proceedings of COLT '91, </booktitle> <publisher> pages ?-?. Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: Our algorithm receives the parameters of the maximum entropy distribution, and based on them and random examples from the target concept, it finds a read-once DNF that approximates well the target read-once DNF. We extend the rusults to monotone read-k DNF. A previous result by <ref> [HM91] </ref> describes an algorithm that efficiently approximates monotone read-k DNF over product distributions. We follow thier techniques and prove that a simmilar algorithm works for Maximum Entropy distributions as well. Our algorithm requires only statistical queries, for this reason we use the Statistical Query model [Kea93] to describe the algorithm. <p> In the previous sections we showed how a result about the learnability of read once DNF over product distributions can be extended to ff-bounded MEM dis tributions. We now continue and briefly describe how other results can be extended in a simmilar way. 8 Learning Monotone k DNF In <ref> [HM91] </ref> it was shown how to efficiently learn over product distributions the class of monotone DNF where each variable appears in no more than k terms (denoted k-DNF). <p> It was shown that knowing an approximation of the set of minimal blocking sets for each variable, enables a good approxi mation of the k-DNF. 8.1 Computing using blocking sets We first present (as in <ref> [HM91] </ref>) a function that computes a monotone function f () given its blocking sets. <p> THEN RETURN 1 ELSE RETURN 0. Lemma 19 <ref> [HM91] </ref> Let f (x 1 ; : : : ; x n ) be a monotone function, and B i , 1 i n, is a maximal collection of minimal blocking sets of the variable x i . <p> This will allow us to eficiently identify the blocking sets. This generalizes the result of <ref> [HM91] </ref>. Lemma 20 Let D be an ff-bounded MEM distribution and be a monotone k DNF such that T 0 is a term of size l and T 1 T s are the terms sharing literals with T 0 . <p> Then, P r D (compute (x; B) 6= g (x)) *=2; Proof: Similar to <ref> [HM91] </ref>, since every blocking set is an approximate blocking set, B i includes all the blocking sets of variable x i . This implies that if compute (x; B) = 1 then g (x) = 1. <p> By Corollary 12, the contribution of all g's terms with length more than is at most kn (1+e 3ff ) = *=2, which completes the proof. 8.4 The algorithm The resulting algorithm for identifying *-blocking sets is identical to the one of <ref> [HM91] </ref>. Let = ln 2kn Estimate p P rob [g = 0]. IF p &lt; *=2 THEN DONE. FOR each variable x i , Let B i = ;.
Reference: [Kea93] <author> Michael Kearns. </author> <title> Efficient noise-tolerant learning from statistical queries. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 392 - 352, </pages> <year> 1993. </year>
Reference-contexts: A previous result by [HM91] describes an algorithm that efficiently approximates monotone read-k DNF over product distributions. We follow thier techniques and prove that a simmilar algorithm works for Maximum Entropy distributions as well. Our algorithm requires only statistical queries, for this reason we use the Statistical Query model <ref> [Kea93] </ref> to describe the algorithm. We use a variant of the statistical query model where we have the ability to query on the expectation of a real valued function with a bounded range, instead of only boolean predicates. <p> Let b i = 0 if ` i = x i and b i = 1 if ` i = x i , then g i b i g i0 and g i b i g i0 _ g i1 . 2.4 Learning using statistical queries The Statistical-Query learning model <ref> [Kea93] </ref> uses an expectation oracle STAT that has as input a function SQ (x; b) which is a function of x 2 X and the value of the target concept b = c (x). <p> It can be shown <ref> [Kea93] </ref> that if a concept class C is efficiently learnable using statistical queries over distribution D, then C is efficiently PAC-learnable over the distribution D (even in the presence of random classification noise).
Reference: [KM93] <author> D. Koller and N. Megiddo. </author> <title> Constructing small sample spaces satisfying given constraints. </title> <booktitle> In Proceedings of the 25th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 268-277, </pages> <year> 1993. </year>
Reference-contexts: One of the basic results is the form of the maximum entropy distribution, given a set of constraints. Unfortunately, from a computational point of view, it is NP-complete to decide if there is any distribution that satisfies the set of pair-wise constraints <ref> [KM93, KN95] </ref>. As stated before, learning the read-once DNF concept class over arbitrary distribution is equivalent to DNF [HKLW90]. Constraining the underlying probability distribution to uniform or product distribution enables to learn read-once DNF [KLV94]. <p> As stated in the introduction, the related computational problem is hard since testing the consistency of the given expectations (existence of any distribution sat isfying those constraints) is known to be N P -Complete <ref> [KN95, KM93] </ref>. 4 Properties of MEM distributions In this section we derive a few properties of MEM distributions. One basic property which will be very important for us is relating the probabilities when a variable is fixed to one or zero.
Reference: [KN95] <author> Joe Kilian and Moni Naor. </author> <title> On the complexity of statistical reasoning. </title> <booktitle> In Proceedings of the 3rd Israel Symposium on Theory of Computing and Systems, </booktitle> <pages> pages 209-217, </pages> <year> 1995. </year>
Reference-contexts: One of the basic results is the form of the maximum entropy distribution, given a set of constraints. Unfortunately, from a computational point of view, it is NP-complete to decide if there is any distribution that satisfies the set of pair-wise constraints <ref> [KM93, KN95] </ref>. As stated before, learning the read-once DNF concept class over arbitrary distribution is equivalent to DNF [HKLW90]. Constraining the underlying probability distribution to uniform or product distribution enables to learn read-once DNF [KLV94]. <p> As stated in the introduction, the related computational problem is hard since testing the consistency of the given expectations (existence of any distribution sat isfying those constraints) is known to be N P -Complete <ref> [KN95, KM93] </ref>. 4 Properties of MEM distributions In this section we derive a few properties of MEM distributions. One basic property which will be very important for us is relating the probabilities when a variable is fixed to one or zero.
Reference: [KV88] <author> Michael Kearns and Leslie G. Valiant. </author> <title> Learning boolean formulae or finite au tomata is as hard as factoring. </title> <type> Technical Report TR 14-88, </type> <institution> Harvard University Aiken Computation Laboratory, </institution> <year> 1988. </year>
Reference-contexts: Only few algorithmic techniques where developed, and many simple concept classes have been proven to be computationally intractable. To mention two intractability results in the PAC model, efficient learning 3-term DNF [PV88] (for learning using a hypothesis which is a 3 term DNF) and efficient learning polynomial size formula <ref> [KV88] </ref> (for learning using any hypothesis).
Reference: [KLV94] <author> M. Kearns, M. Li and L. Valiant. </author> <title> Learn ing boolean formulas. </title> <journal> Journal of the ACM, </journal> <volume> 41(6) </volume> <pages> 1298-1328, </pages> <year> 1994. </year>
Reference-contexts: When looking for learning with respect to a fixed distribution, rather than an arbitrary one, it is most important to chose "natural" distributions. The most common distributions are the uniform distribution and product distributions. For example with respect to such distributions one can learn read-once DNF <ref> [KLV94] </ref>, although with respect to an arbitrary distribution it is equivalent to learning general DNF, which is a major open problem in computational learning theory. This was the starting point of our research, we were interested in widening the set of "natural" distributions that are considered. <p> As stated before, learning the read-once DNF concept class over arbitrary distribution is equivalent to DNF [HKLW90]. Constraining the underlying probability distribution to uniform or product distribution enables to learn read-once DNF <ref> [KLV94] </ref>. However, the algorithms for learning the read-once DNF over uniform or product distributions depends heavily on the fact that the attributes are independent. This is why it is interesting to develop an efficient learning algorithm for read once DNF under maximum entropy distributions. <p> The algorithm itself is very similar to the one for learning read-once DNF under a uniform/product distribution <ref> [KLV94] </ref>. We first identify the variables on which the target function depends, and then test every two such variables if they appear in the same term. We defer the proofs (which are the statistical query algorithm) to the Appendix. The following lemma shows that we can compute fl i .
Reference: [Nat87] <author> B. K. Natarajan. </author> <title> On learning boolean func tions. </title> <booktitle> In Proceedings of the Nineteenth An nual ACM Symposium on Theory of Com puting, </booktitle> <pages> pages 296-304, </pages> <address> New York, New York, </address> <month> May </month> <year> 1987. </year>
Reference-contexts: Research was done on the learnability with respect to a fixed distribution, and a model, similar to the PAC model, was suggested, where the learning algorithm may depend on the underline distribution <ref> [BI88, Nat87] </ref>. They also show examples where this allows to learn classes which where not learnable before. When looking for learning with respect to a fixed distribution, rather than an arbitrary one, it is most important to chose "natural" distributions.
Reference: [Pap91] <author> Athanasios Papoulis. </author> <title> Probability, Random Variables, and Stochastic Processes, Chap ter 15. </title> <publisher> McGraw-Hill, </publisher> <address> third edition, </address> <year> 1991. </year>
Reference-contexts: We first examine the form of the Maximum Entropy distribution, satisfying a given set of constraints. This problem has been widely studied, here we briefly present the basic results. (The interested reader may look at <ref> [Pap91, SWG85] </ref>.) We want to determine the probabil ity distribution D (x) subject to the constraints that the expected value of the function f i (x) is i , i.e. <p> In our special case, we are given n expectations, E (x i ) = i , and n 2 corre lations, E (x i x j ) = ij . One can show <ref> [Pap91] </ref> that the distribution suggested by the maximum entropy method (denoted MEM) has the form D (x) = Ae x t Mxw t x Where M is a symmetric n fi n matrix and w 2 &lt; n is a vector (we denote by x t the vector x transposed).
Reference: [PV88] <author> Leonard Pitt and Leslie G. Valiant. </author> <title> Compu tational limitations on learning from exam ples. </title> <journal> Journal of the ACM, </journal> <volume> 35(4) </volume> <pages> 965-984, </pages> <year> 1988. </year>
Reference-contexts: Only few algorithmic techniques where developed, and many simple concept classes have been proven to be computationally intractable. To mention two intractability results in the PAC model, efficient learning 3-term DNF <ref> [PV88] </ref> (for learning using a hypothesis which is a 3 term DNF) and efficient learning polynomial size formula [KV88] (for learning using any hypothesis).
Reference: [SWG85] <author> C.Ray Smith and Jr. W.T. Grandy, </author> <title> editors. Maximum-Entropy and Bayesian Methods in Inverse Problems. </title> <publisher> D.Reidel Publishing Company, </publisher> <year> 1985. </year>
Reference-contexts: There is a vast literature about the subject of maximum entropy distributions, a subject of special interest in physics <ref> [SWG85] </ref>. One of the basic results is the form of the maximum entropy distribution, given a set of constraints. Unfortunately, from a computational point of view, it is NP-complete to decide if there is any distribution that satisfies the set of pair-wise constraints [KM93, KN95]. <p> We first examine the form of the Maximum Entropy distribution, satisfying a given set of constraints. This problem has been widely studied, here we briefly present the basic results. (The interested reader may look at <ref> [Pap91, SWG85] </ref>.) We want to determine the probabil ity distribution D (x) subject to the constraints that the expected value of the function f i (x) is i , i.e.
Reference: [Val84] <author> Leslie G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11):1134 1142, </volume> <month> November </month> <year> 1984. </year>
Reference-contexts: 1 Introduction The PAC learning model <ref> [Val84] </ref> is the most basic model in computational learning theory. Its introduction brought forward a simple set of assumptions and raised many challenging problems. Initially, the main goal was a computational one, to develop new algorithms within this framework and show the learnability of different concept classes.
References-found: 15

