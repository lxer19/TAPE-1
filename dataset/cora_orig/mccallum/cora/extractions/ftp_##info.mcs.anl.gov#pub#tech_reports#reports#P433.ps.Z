URL: ftp://info.mcs.anl.gov/pub/tech_reports/reports/P433.ps.Z
Refering-URL: http://www.mcs.anl.gov/publications/abstracts/abstracts94.htm
Root-URL: http://www.mcs.anl.gov
Title: A Compilation System That Integrates High Performance Fortran and Fortran M  
Author: Ian Foster Bhaven Avalani Ming Xu Alok Choudhary 
Address: Argonne, IL 60439 Syracuse, NY 13244  
Affiliation: Mathematics and Computer Science CIS/CSE Argonne National Laboratory Syracuse University  
Abstract: Task parallelism and data parallelism are often seen as mutually exclusive approaches to parallel programming. Yet there are important classes of application, for example in multidisciplinary simulation and command and control, that would benefit from an integration of the two approaches. In this paper, we describe a programming system that we are developing to explore this sort of integration. This system builds on previous work on task-parallel and data-parallel Fortran compilers to provide an environment in which the task-parallel language Fortran M can be used to coordinate data-parallel High Performance Fortran tasks. We use an image-processing problem to illustrate the issues that arise when building an integrated compilation system of this sort. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Z. Bozkus, Alok Choudhary, Geoffrey C. Fox, T. Haupt, and S. Ranka, </author> <title> Fortran 90D/HPF compiler for distributed memory MIMD computers: Design, implementation, and performance eesults. </title> <booktitle> In Proc. Supercomputing '93, IEEE, </booktitle> <month> November </month> <year> 1993, </year> <institution> Portland, Oregon. </institution>
Reference-contexts: The Syra-cuse HPF compiler translates HPF programs into Fortran 77 plus calls to a set of collective communication library routines <ref> [1] </ref>. It exploits only the parallelism expressed in HPF's explicitly data-parallel constructs such as forall, array constructs, the independent do assertion, and intrinsic functions. It does not attempt to parallelize other constructs. The foundation of the compiler lies in recognizing commonly occurring communication and computation patterns.
Reference: [2] <author> K. M. Chandy, I. Foster, K. Kennedy, C. Koelbel, and C.-W. Tseng, </author> <title> Integrated support for task and data parallelism, </title> <journal> Intl. J. Supercomputer Applications, </journal> <note> 8(2), 1994 (in press). </note>
Reference-contexts: This issue is the main focus of this paper. The language design issues are concerned with how task- and data-parallel constructs are harmonized. In this paper, we adopt the approach proposed by Chandy et al. <ref> [2] </ref>, namely, the use of distinct task- and data-parallel languages, with the task-parallel language used to coordinate data-parallel computations. An alternative approach that we will pursue in future work is the definition of a single integrated language. The organization of this paper is as follows. <p> We incorporate this conversion in the interface routines generated by the MPCALL preprocessor. Currently, the conversion is simple because the Syracuse HPF uses simple data layouts. In the future, it could be more complex <ref> [2] </ref>. Data-Parallel Channel Communication. Our compilation system currently assumes that two communicating HPF computations execute on the same number of (virtual) processors and use different decompositions for the variables that they communicate.
Reference: [3] <author> B. Chapman, P. Mehrotra, J. van Rosendale, and H. Zima, </author> <title> A software architecture for multidisciplinary applications: Integrating task and data parallelism, </title> <type> Technical Report 94-18, </type> <institution> ICASE, MS 132C, NASA Langley Research Center, </institution> <address> Hampton, VA 23681, </address> <year> 1994. </year>
Reference-contexts: An appropriate mix of task and data parallelism is then generated automatically by the compiler. HPF/FM permits more general and dynamic forms of task parallelism but also requires more pro grammer intervention. Chapman et al. <ref> [3] </ref> have recently proposed the addition of "spawn" and "shared data abstraction" constructs to data-parallel languages to support multi disciplinary programming. The shared data abstrac 7 tion, a form of monitor, is used to control interactions between tasks created using spawn.
Reference: [4] <author> G. Cheng, G. Fox, and K. Mills, </author> <title> Integrating multiple programming paradigms on Connection Machine CM5 in a dataflow-based software environment, </title> <type> Technical Report, </type> <institution> NPAC, Syracuse University, Syracuse, </institution> <address> N.Y., </address> <year> 1993. </year>
Reference-contexts: In a parallel context, most work has focused on task-parallel coordination frameworks for data-parallel components. For example, Cheng et al. propose the use of the AVS dataflow visualization system to implement multidisciplinary applications, in which some components may be data-parallel programs <ref> [4] </ref>. This provides an elegant graphical programming model but is less expressive than HPF/FM. For example, cyclic communication structures are not easily expressed. Similarly, Quinn et al. describe work on iWARP in which a configuration language is used to connect Dataparallel C computations [8].
Reference: [5] <author> I. Foster and K. M. Chandy. </author> <title> Fortran M: A language for modular parallel programming. </title> <journal> J. </journal> <note> Parallel and Distributed Computing (to appear), and Preprint MCS-P327-0992, </note> <institution> Argonne National Laboratory, </institution> <year> 1992. </year>
Reference-contexts: Both paradigms are supported by a number of parallel languages; in this paper, we use High Performance Fortran (HPF) [9] and Fortran M (FM) <ref> [5] </ref> as prototypical examples. While task and data parallelism are often considered mutually exclusive approaches to parallel programming, many applications can in fact benefit from fl To appear in: Proc. 1994 Scalable High Performance Computing Conf., IEEE Computer Science Press. both forms of parallelism.
Reference: [6] <author> I. Foster, C. Kesselman, R. Olson, and S. Tuecke, </author> <title> Nexus: An Interoperability toolkit for parallel and distributed computer systems, </title> <type> Technical Report ANL/MCS-TM-189, </type> <institution> Argonne National Laboratory, </institution> <year> 1994. </year>
Reference-contexts: In the HPF/FM system, FM processes are used to encapsulate data-parallel HPF computations, and virtual computers are used to control the allocation of computational resources to HPF computations. The Argonne FM compiler compiles FM to Fortran 77 plus calls to a thread-management and message-passing library called Nexus <ref> [6] </ref>. Nexus message-passing functions are implemented in terms of TCP/IP, shared-memory operations, MPI, NX, or PVM, depending on the target architecture.
Reference: [7] <author> I. Foster and M. Xu. </author> <title> Libraries for parallel paradigm integration. In Toward Teraflop Computing and New Grand Challenge Applications, </title> <editor> R. Kalia and P. Vashishta (eds.), </editor> <publisher> Nova Science Publishers, </publisher> <year> 1994. </year>
Reference-contexts: Invocation of an HPF computation is indicated by a special HPFCALL statement, which specifies the name of the routine to execute and its arguments. For example: HPFCALL my hpf proc (a1,...,an) Arguments passed from FM to HPF can be either replicated or distributed over the HPF virtual processors <ref> [7] </ref>. The number of processors is determined by the FM virtual computer in which the call is made. <p> The HPFCALL statement can be imple-mented in terms of an existing syntactic extension, MPCALL. This statement, which allows invocation of arbitrary message-passing programs, is implemented via a preprocessor that generates an interface routine that invokes the HPF program <ref> [7] </ref>. Data Sharing. An FM program can pass FM data structures as arguments to an HPF procedure; modified versions of these data structures may be returned upon completion of the HPF computation. <p> We have adopted the latter approach and have developed a message-passing compatibility library written in FM that implements the Express calls generated by the HPF compiler as operations on channels set up when the HPF code is invoked from FM <ref> [7] </ref>. The merit of this layered approach is that it re FFT (HPF) Image Stream 1 Image Stream 2 (operator) Convolution of Image 1 & Operator FM process FM process FM process FM channels (HPF) quires no changes to either run-time system.
Reference: [8] <author> P. Hatcher and M. Quinn. </author> <title> Data-parallel Programming on MIMD Computers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1991. </year>
Reference-contexts: This provides an elegant graphical programming model but is less expressive than HPF/FM. For example, cyclic communication structures are not easily expressed. Similarly, Quinn et al. describe work on iWARP in which a configuration language is used to connect Dataparallel C computations <ref> [8] </ref>. The Data-parallel C programs use specialized versions of C I/O libraries for communication. Process and communication structures are static, and all communication passes via a central communication server. Subhlok et al. describe a compile-time approach for exploiting task and data parallelism on the iWarp mesh-connected multicomputer [12].
Reference: [9] <author> High Performance Fortran Forum. </author> <title> High Performance Fortran language specification, version 1.0. </title> <type> Technical Report CRPC-TR92225, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, Texas, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Both paradigms are supported by a number of parallel languages; in this paper, we use High Performance Fortran (HPF) <ref> [9] </ref> and Fortran M (FM) [5] as prototypical examples. While task and data parallelism are often considered mutually exclusive approaches to parallel programming, many applications can in fact benefit from fl To appear in: Proc. 1994 Scalable High Performance Computing Conf., IEEE Computer Science Press. both forms of parallelism.
Reference: [10] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiling Fortran D for MIMD distributed-memory machines. </title> <journal> CACM, </journal> <volume> 35(8) </volume> <pages> 66-80, </pages> <month> August </month> <year> 1992. </year>
Reference: [11] <author> J. del Rosario and A. Choudhary, </author> <title> High performance I/O for parallel computers: Problems and prospects. </title> <booktitle> IEEE Computer, </booktitle> <month> March </month> <year> 1994. </year>
Reference-contexts: In the general case, however, processors may use data distributions, in which case data redistributions are required as part of communication. This situation is similar to the parallel I/O problem (where in-memory and disk files may be laid out in different ways), and similar solutions can probably be employed <ref> [11] </ref>. Runtime Systems. The code generated by the HPF and FM compilers includes calls to the HPF and FM run-time systems, which provide collective communication and task/message management routines, respectively.
Reference: [12] <author> J. Subhlok, J. Stichnoth, D. O'Hallaron, and T. Gross. </author> <title> Exploiting task and data parallelism on a multicomputer. </title> <booktitle> In Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle> <address> San Diego, Calif., </address> <month> May </month> <year> 1993. </year>
Reference-contexts: The Data-parallel C programs use specialized versions of C I/O libraries for communication. Process and communication structures are static, and all communication passes via a central communication server. Subhlok et al. describe a compile-time approach for exploiting task and data parallelism on the iWarp mesh-connected multicomputer <ref> [12] </ref>. The input program incorporates HPF-like data parallel constructs and directives indicating data dependencies and parallel sections. An appropriate mix of task and data parallelism is then generated automatically by the compiler. HPF/FM permits more general and dynamic forms of task parallelism but also requires more pro grammer intervention.
Reference: [13] <author> P. Zave, </author> <title> A compositional approach to multiparadigm programming, </title> <journal> IEEE Software, </journal> <pages> 15-25, </pages> <year> 1989. </year> <month> 8 </month>
Reference-contexts: Hence, we can expect it to be faster when N is small and P is large, the situation that we in fact see in practice. 6 Related work Multiparadigm programming has been investigated by many researchers in a sequential context (for example, see <ref> [13] </ref>). In a parallel context, most work has focused on task-parallel coordination frameworks for data-parallel components. For example, Cheng et al. propose the use of the AVS dataflow visualization system to implement multidisciplinary applications, in which some components may be data-parallel programs [4].
References-found: 13

