URL: ftp://ftp.wins.uva.nl/pub/computer-systems/aut-sys/reports/KroDam92b.ps.gz
Refering-URL: http://www.fwi.uva.nl/research/neuro/publications/publications.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: krose@fwi.uva.nl  
Title: LEARNING TO AVOID COLLISIONS: A REINFORCEMENT LEARNING PARADIGM FOR MOBILE ROBOT NAVIGATION  
Author: Ben J.A. Krose and Joris W.M.van Dam 
Keyword: self-adapting systems, learning systems, neural nets, vehicles.  
Address: Kruislaan 403, NL-1098 SJ Amsterdam, The Netherlands  
Affiliation: Faculty of Mathematics and Computer Science, University of Amsterdam  
Abstract: The paper describes a self-learning control system for a mobile robot. Based on sensor information the control system has to provide a steering signal in such a way that collisions are avoided. Since in our case no `examples' are available, the system learns on the basis of an external reinforcement signal which is negative in case of a collision and zero otherwise. We describe the adaptive algorithm which is used for a discrete coding of the state space, and the adaptive algorithm for learning the correct mapping from the input (state) vector to the output (steering) signal. 
Abstract-found: 1
Intro-found: 1
Reference: <editor> Albada, G.D. van, J.M. Lagerberg and B.J.A. Krose. </editor> <booktitle> Software architecture and simulation tools for autonomous mobile robots, Proc. of Euriscon '91 Corfu, </booktitle> <address> Greece, </address> <publisher> Kluwer Press (in press). </publisher>
Reference: <author> Barto, A.G., R.S. Sutton and C.W. </author> <title> Anderson (1983). Neuronlike adaptive elements that can solve difficult learning control problems. </title> <journal> IEEE Trans. on Systems, Man and Cybernetics 13 834-846. </journal>
Reference-contexts: Not always there are correct output values known for each input: sometimes the only information available is a binary evaluation of the current state of the system (reinforcement signal). In this case a reinforcement learning procedure must be used <ref> (Barto, Sutton and Anderson, 1983) </ref>. In our application we studied a reinforcement learning method for a mobile robot.
Reference: <author> Kohonen, T. </author> <year> (1984). </year> <title> Self-Organization and Associative Memory. </title> <publisher> Springer Verlag. </publisher>
Reference: <author> Krose, B.J.A. and E. </author> <month> Dondorp </month> <year> (1989). </year> <title> A Sensor Simulation System for Mobile Robots. </title>
Reference: <editor> In: T. Kanade, F.C.A. </editor> <publisher> Groen and L.O. </publisher>
Reference: <institution> Hertzberger (Ed.)Intelligent Autonomous Systems 2 pp. </institution> <month> 641-649. </month>
Reference: <author> Krose, </author> <title> B.J.A and J.W.M.van Dam (1992). Adaptive state space quantisation for reinforcement learning of collision-free navigation IEEE International Conference on Intelligent Robots and System Kuperstein, </title> <editor> M. </editor> <year> (1987). </year> <title> Adaptive visual-motor coordination in multi-joint robots using parallel architecture. </title> <booktitle> Proceedings of IEEE International Conference on Robotics and Automation pp. </booktitle> <pages> 1595-1602. </pages>
Reference-contexts: Instead of having a single neu-ron active for each state, the sensor readings can be mapped directly into motor commands, which also has shown to result in a collision free navigation <ref> (Verschure, Krose and Pfeifer, 1992) </ref>. However, this can only be done for linear control rules. A second approach which is studied is to have a discretisation where also the number of regions can be adapted. <p> To limit the size of the network, neurons can be merged with neighbouring neurons when their action probability distributions are similar. This approach has resulted in a control system which has about the same performance as the system described in this paper, but needs less neurons <ref> (Krose and Dam, 1992) </ref>. The controller described in this paper results in a reactive behaviour of the robot, aimed at avoiding collisions with obstacles. If also a goal directed behaviour has to be learned, basically the same approach can be used.
Reference: <editor> Michie, D and R.A. </editor> <title> Chambers (1968). Boxes: An experiment in adaptive control. In: </title> <publisher> E. </publisher>
Reference: <editor> Dale and D. Michie (Ed.) </editor> <booktitle> Machine Intelligence 2 Oliver and Boyd pp. </booktitle> <pages> 137-152. </pages>
Reference: <author> Moody, J and C. </author> <month> Darken </month> <year> (1989). </year> <title> Fast learning in networks of locally-tuned processing units. </title> <booktitle> Neural Computation 1 281-294. </booktitle>
Reference-contexts: In most "neuro-computational" approaches, a function u = F ( ~ d) is approximated by a weighted summation of basis functions x i ( ~ d). These can be sigmoid functions, such as in the standard multilayer feed-forward networks (Rumelhart and Mc-Clelland, 1986), radial basis functions (RBF's) <ref> (Moody and Darken, 1989) </ref>, or non-overlapping functions which discretise the input-space and assign an output signal explicitly to each region in the state-space (Michie and Chambers, 1968; Barto, Sutton and Anderson, 1983).
Reference: <author> Ritter, H.J., T.M.Martinetz and K.J. </author> <title> Schulten (1989). Topology conserving maps for learning visuo-motor coordination. Neural Networks 2 159-168. </title>
Reference: <author> Rosen, B.E., J.M.Goodwin and J.J.Vidal. </author> <title> (1990) Adaptive range coding. </title> <booktitle> In: </booktitle> <address> D. </address>
Reference-contexts: Self-organising schemes for quantisation of the input space have been presented <ref> (Rosen,1990) </ref> and seem to be able to overcome this problem. 1 In the first section of this paper we describe how such a self-organising algorithm can be used to quantise the input space of our neural controller.
Reference: <editor> Touretzky (Ed.) </editor> <booktitle> Neural Information Processing Systems 3. </booktitle> <publisher> Morgan Kauffman. </publisher>
Reference: <editor> Rumelhart, D.E. </editor> <booktitle> and J.L.McClelland (1986). Parallel Distributed Processing. </booktitle> <publisher> MIT Press. </publisher>
Reference-contexts: In most "neuro-computational" approaches, a function u = F ( ~ d) is approximated by a weighted summation of basis functions x i ( ~ d). These can be sigmoid functions, such as in the standard multilayer feed-forward networks <ref> (Rumelhart and Mc-Clelland, 1986) </ref>, radial basis functions (RBF's) (Moody and Darken, 1989), or non-overlapping functions which discretise the input-space and assign an output signal explicitly to each region in the state-space (Michie and Chambers, 1968; Barto, Sutton and Anderson, 1983).
Reference: <author> Smagt, P.P van der, and B.J.A. </author> <month> Krose </month> <year> (1991). </year> <title> A real-time learning neural robot controller. </title> <booktitle> In: Proceedings of the 1991 Int. Conf. on Artificial Neural Networks Finland pp. </booktitle> <pages> 351-356. </pages>
Reference-contexts: These controllers are often based on some sort of neural network, which can be trained by presenting learning examples consisting of input and corresponding output pairs. Learning samples are either provided by an external teacher (Touretzky and Pomerleau, 1989) (supervised learning) or are generated by the system itself <ref> (Smagt and Krose, 1991) </ref> (self-supervised learning). Not always there are correct output values known for each input: sometimes the only information available is a binary evaluation of the current state of the system (reinforcement signal). In this case a reinforcement learning procedure must be used (Barto, Sutton and Anderson, 1983).
Reference: <author> Sutton, </author> <title> R.S. (1984) Temporal credit assignment in reinforcement learning Ph.D. </title> <institution> dissertation at University of Massachusets. </institution>
Reference: <author> Sutton, </author> <title> R.S. (1990) Integrated architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> Proc. of the Seventh Int. Conf. on Machine Learning. </booktitle>
Reference: <author> Touretzky, D.S. and D.A. </author> <title> Pomerleau (1989) What's hidden in the hidden layers? Byte, </title> <note> August pp. 227-233. </note>
Reference-contexts: These controllers are often based on some sort of neural network, which can be trained by presenting learning examples consisting of input and corresponding output pairs. Learning samples are either provided by an external teacher <ref> (Touretzky and Pomerleau, 1989) </ref> (supervised learning) or are generated by the system itself (Smagt and Krose, 1991) (self-supervised learning). Not always there are correct output values known for each input: sometimes the only information available is a binary evaluation of the current state of the system (reinforcement signal).
Reference: <author> Verschure, P.F.M.J., B.J.A. Krose and R. </author> <title> Pfeifer (1992) Distributed Adaptive Control: the self organization of structured behavior. </title> <booktitle> Robotics and Autonomous Systems 9 (2). </booktitle> <pages> 5 </pages>
Reference-contexts: Instead of having a single neu-ron active for each state, the sensor readings can be mapped directly into motor commands, which also has shown to result in a collision free navigation <ref> (Verschure, Krose and Pfeifer, 1992) </ref>. However, this can only be done for linear control rules. A second approach which is studied is to have a discretisation where also the number of regions can be adapted.
References-found: 19

