URL: http://www.cs.cmu.edu/afs/cs/project/pdl/ftp/SPFS/Compcon95.ps
Refering-URL: http://www.cs.cmu.edu/~danner/
Root-URL: 
Title: Abstract  
Abstract: To meet the bandwidth needs of modern computer systems, parallel storage systems are evolving beyond RAID levels 1 through 5. The Parallel Data Lab at Carnegie Mellon University has constructed three Scotch parallel storage testbeds to explore and evaluate five directions in RAID evolution: first, the development of new RAID architectures to reduce the cost/performance penalty of maintaining redundant data; second, an extensible software framework for rapid prototyping of new architectures; third, mechanisms to reduce the complexity of and automate error-handling in RAID subsystems; fourth, a file system extension that allows serial programs to exploit parallel storage; and lastly, a parallel file system that extends the RAID advantages to distributed, parallel computing environments. This paper describes these five RAID evolutions and the testbeds in which they are being implemented and evaluated. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Patterson, D., Gibson, G., Katz, R. </author> <title> A Case for Redundant Arrays of Inexpensive Disks (RAID), </title> <booktitle> Proc. ACM Conf. on Management of Data, </booktitle> <year> 1988, </year> <pages> pp. 109-116. </pages>
Reference-contexts: The introduction of parallel processing, coupled with the unrelenting pace of microprocessor performance improvements, has converted many traditionally compute-constrained tasks to ones dominated by I/O. Redundant Arrays of Inexpensive Disks (RAID), as defined by Patterson, Gibson, and Katz <ref> [1] </ref>, has emerged as the most promising technology for meeting these needs. Consequently, the market for RAID systems is undergoing rapid growth, exceeding three billion dollars in 1994 and expected to surpass 13 billion dollars by 1997 [2]. However, RAID storage is not without limitations.
Reference: [2] <author> DISK/TREND, Inc. </author> <year> 1994. </year> <title> 1994 DISK/TREND Report: Disk Drive Arrays. 1925 Landings Drive, </title> <institution> Mountain View, Calif., SUM-3. </institution>
Reference-contexts: Consequently, the market for RAID systems is undergoing rapid growth, exceeding three billion dollars in 1994 and expected to surpass 13 billion dollars by 1997 <ref> [2] </ref>. However, RAID storage is not without limitations. First, there are cost and performance penalties for maintaining a redundant encoding of stored data. Overcoming these penalties continues to spur the development of new variations of RAID architectures.
Reference: [3] <author> Chen, P. M., Lee, E. K, Gibson, G. A., Katz, R. H., Patter-son, D. A.RAID: </author> <title> High-Performance, Reliable Secondary Storage, </title> <journal> ACM Computing Surveys, </journal> <volume> 26(2) </volume> <pages> 145-185, </pages> <year> 1994. </year>
Reference-contexts: For the sake of brevity, we describe only parity declustering, our most mature RAID architecture. Additional architectures and the work of others is described in a broad survey of RAID research by Chen, Lee, Gibson, Katz, and Patterson <ref> [3] </ref>. 3.1 Architecture Example: Parity Declustering Fault tolerance and high concurrency make RAID level 5 an attractive storage architecture for transaction processing environments. However, RAID level 5 disk arrays typically experience a 60-80% load increase in the presence of a failed drive.
Reference: [4] <author> Holland, M., Gibson, G. </author> <title> Parity Declustering for Continuous Operation in Redundant Disk Arrays, </title> <booktitle> Proc. Int. Conf. on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pp. 23-25, </pages> <year> 1992. </year>
Reference-contexts: Parity declustering is a variant of RAID level 5 that reduces the performance degradation of on-line failure recovery <ref> [4] </ref>. The key idea behind parity declustering is that a parity unit protects fewer than N-1 data units, where N is the number of disks in the array.
Reference: [5] <author> Holland, M.C, Stodolsky, D., Gibson, G. </author> <title> Parity Declus-tering and Declustered P+Q in RAIDframe, </title> <institution> School of Computer Science, Carnegie Mellon University, </institution> <type> Tech Report. </type> <note> Under preparation. </note>
Reference-contexts: We have implemented parity declustering in the Scotch-2 parallel storage testbed. Figure 4 shows the time measured for the reconstruction of the first 200 MB of a disk in a 15-disk declustered array under three workload intensities <ref> [5] </ref>.
Reference: [6] <author> Storage Technology Corporation. </author> <title> Iceberg 9200 Storage System: Introduction, STK Part Number 307406101, Storage Technology Corporation, </title> <type> Corporate Technical Publications, </type> <address> 2270 South 88th Street, Louisville, CO 80028 </address>
Reference: [7] <author> Courtright, W. V. II, Gibson, G. </author> <title> Backward Error Recovery in Redundant Disk Arrays, </title> <booktitle> Proc. 1994 Computer Measurement Group Conf., </booktitle> <pages> pp. 63-74, </pages> <year> 1994 </year>
Reference-contexts: Further extensions are underway. 3.3 Automated Error Recovery Error handling is one of the major sources of complexity in the implementation of a RAID controller <ref> [7] </ref>. In a non-fault-tolerant system, many errors are handled by discarding all operations in progress and reporting the error for host software to handle.
Reference: [8] <author> Patterson, R. H., Gibson, G. </author> <title> Exposing I/O Concurrency with Informed Prefetching, </title> <booktitle> Proc. 3rd Int. Conf. on Parallel & Distributed Information Systems, </booktitle> <pages> pp. 7-16, </pages> <year> 1994. </year>
Reference-contexts: Second, resource decisions, notably buffer-cache management, can be improved by foreknowledge. Third, deep prefetching yields deep disk queues that allow disk scheduling to improve access throughput. Transparent Informed Prefetching (TIP) is a system we have developed to exploit access-pattern information for read-intensive workloads <ref> [8] </ref>. Applications are annotated to generate hints that disclose future accesses. The application passes these hints to the buffer cache manager through the file system interface, which then issues prefetch accesses that efficiently utilize the parallel storage system and available system memory. <p> Furthermore, because applications can provide hints without knowing the details of the underlying system configuration, they obtain performance optimizations portable to any machine incorporating a TIP system. TIP has been implemented in the Scotch-1 direct-attach storage testbed and measured for compilation, text search, and visualization applications <ref> [8] </ref>. Figure 8 shows the our experience with the 3-D scientific data visualization package, XDataSlice. Originally an in-core rendering tool, we modified XDataSlice to handle datasets too large for memory, in this case 112 MB, by staging data directly from blocked disk files.
Reference: [9] <author> Howard, J. H., Kazar, M. L, et. al. </author> <title> Scale and Performance in a Distributed File System, </title> <journal> ACM Trans. on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 51-81, </pages> <year> 1988. </year>
Reference: [10] <editor> Satyanarayanan, M, Kistler, J. J., Kumar, et. al., </editor> <title> Coda: a Highly available File System for a Distributed Workstation Environment, </title> <journal> IEEE Trans. on Computers, </journal> <volume> 39(4): </volume> <pages> 447-459, </pages> <year> 1990. </year>
Reference: [11] <author> Hartman, J.H, Ousterhout, J.K. </author> <title> The Zebra Striped Network File, </title> <booktitle> Proc. 14th ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 29-43, </pages> <year> 1994. </year>
Reference: [12] <author> Cabrera, L., Long, D. D. E. Swift: </author> <title> Using Distributed Disk Striping to Provide High I/O Data Rates, </title> <journal> Computing Systems, </journal> <volume> 4(4) </volume> <pages> 405-436, </pages> <note> 1991 [13] del Rosario, </note> <author> J.M. Choudhary, </author> <title> A.N. High-performance I/O for Massively Parallel Computers: Problems and Prospects, </title> <journal> Computer, </journal> 27(3):59-68, 1994. 
Reference: [14] <author> Geist, A., Beguelin, A., Dongarra, J., et. al., </author> <title> PVM: Parallel Virtual Machine, A Users Guide and Tutorial for Network Parallel Computing. </title> <publisher> MIT Press, </publisher> <year> 1994, </year> <note> ISBN 0-262-57108-0 </note>
Reference: [15] <author> Bershad, B. N., Zekauskas, M. J., Sawdon, W. A. </author> <title> The Midway Distributed Shared Memory System, </title> <booktitle> Proc. 1993 IEEE Compcon Conf., </booktitle> <pages> pp. 528-537, </pages> <year> 1993. </year>
Reference-contexts: SPFSs sharing model is close to a DSM model called entry consistency <ref> [15] </ref>, illustrated in Figure 10. Expunge /* Sequentially consistent file system*/ file_handle fh; int my_start = 2000 * process_number (); loop forever fs_read (fh,...); computation fs_read (fh,...) computation ... BARRIER; /* write a disjoint section */ fs_write (fh,...); computation fs_write (fh,...); computation ....
Reference: [16] <author> Zosel, M. E. </author> <title> High Performance FORTRAN: An Overview, </title> <booktitle> Proc. 1993 IEEE Compcon Conf., </booktitle> <pages> pp. 132-6, </pages> <year> 1993. </year>
Reference: [17] <author> Carter, J. B., Bennett, J., K., Zwaenepoel, W. </author> <title> Implementation and Performance of Munin, </title> <booktitle> Proc. 13th ACM Symp. on Operating Systems Principles, </booktitle> <pages> pp. 152-164, </pages> <year> 1991. </year>
Reference: [18] <author> Kung, H.T., Sansom, R., Schlick, S., et. al., </author> <title> Network-based Multicomputers: an Emerging Parallel Architecture, </title> <booktitle> Supercomputing91, </booktitle> <pages> pp. 664-673, </pages> <year> 1991. </year>
Reference-contexts: We call a network of workstations used in parallel a multi-computer <ref> [18] </ref>. The multicomputer environment provides new challenges for a distributed file system. The bandwidth and storage capacity requirements are similar to that of a supercomputing environment, but multiple clients concurrently writing a single file are now commonplace.
Reference: [19] <author> Corbett, P.F, Feitelson, D.G. </author> <title> Design and Implementation of the VESTA Parallel File System, </title> <booktitle> Proc. Scalable High-Performance Computing Conf., </booktitle> <pages> pp. 63-70, </pages> <year> 1994. </year>
Reference-contexts: The bandwidth and storage capacity requirements are similar to that of a supercomputing environment, but multiple clients concurrently writing a single file are now commonplace. The sharing, fault-tolerance, and scaling challenges of a multi-computer environment are being by the development of parallel file systems <ref> [19] </ref>. We are developing the Scotch Parallel File System (SPFS) for the multicomputer environment shown in Figure 2. It supports concurrent-read and -write sharing within a parallel application and provides scalable bandwidth and customizable availability by striping over independent servers on a file-by-file basis.
Reference: [20] <author> Cormen, T. H., Kotz, D., </author> <title> Integrating Theory and Practice in Parallel File Systems, </title> <booktitle> Proc. DAGS/PC Symp., </booktitle> <pages> pp. 64-74, </pages> <year> 1993. </year>
Reference-contexts: Also, at the applications discretion, redundancy computations can be selectively disabled and enabled to minimize the performance cost of short bursts of rapid changes. This idea, the deferred computation of parity, is called a paritypoint by Cormen and Kotz in their requirements for out-of-core algorithms <ref> [20] </ref>. SPFS is intended to complement rather than replace parallel programming tools such as PVM or DSM by providing high-bandwidth file storage. We expect the generic synchronization needs of applications to be meet by mechanisms provided by these tools.
Reference: [21] <author> Adve, S. V. Hill, M. D. </author> <title> A Unified Formalization of Four Shared-Memory Models, </title> <journal> IEEE Trans. on Parallel and Distributed Systems, </journal> <volume> 4(6) </volume> <pages> 613-624, </pages> <year> 1993. </year>
Reference-contexts: Therefore, SPFS does not provide synchronization primitives such as barriers or locks. However, because SPFS does anticipate file sharing within a parallel application and because it aggressively defers and prefetches, SPFS implements a form of weakly consistent shared memory <ref> [21] </ref>. SPFS exports two primitives, propagate and expunge, to provide weakly-consistent sharing. Sometime after writing a portion of a shared file, an SPFS client must explicitly propagate that portion to make sure it is visible to other SPFS clients.
References-found: 20

