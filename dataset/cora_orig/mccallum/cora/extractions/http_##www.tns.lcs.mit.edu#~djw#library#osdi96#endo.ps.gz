URL: http://www.tns.lcs.mit.edu/~djw/library/osdi96/endo.ps.gz
Refering-URL: http://www.tns.lcs.mit.edu/~djw/library/osdi96/index.html
Root-URL: 
Email: Email: office@usenix.org  
Title: Using Latency to Evaluate Interactive System Performance  
Phone: 1. Phone: 510 528-8649 2. FAX: 510 548-5738 3.  4.  
Author: Yasuhiro Endo, Zheng Wang, J. Bradley Chen, and Margo Seltzer 
Affiliation: Harvard University  
Web: WWW URL: http://www.usenix.org  
Date: October 1996  
Note: The following paper was originally published in the Proceedings of the USENIX 2nd Symposium on Operating Systems Design and Implementation Seattle, Washington,  For more information about USENIX Association contact:  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> Business Applications Performance Corporation, </author> <title> SYSmark for Windows NT, Press Release by IDEAS International, </title> <address> Santa Clara, CA, </address> <month> March </month> <year> 1995. </year>
Reference: [2] <author> Brian N. Bershad, Richard P. Draves, and Alessan-dro Forin, </author> <title> Using Microbenchmarks to Evaluate System Performance. </title> <booktitle> Proceedings of the Third Workshop on Workstation Operating Systems, IEEE, </booktitle> <address> Key Biscayne, Florida, </address> <month> April </month> <year> 1992, </year> <pages> pages 148153. </pages>
Reference-contexts: They can be used to measure latency (e.g., 7.7 ms per IPC) or throughput (e.g.,129,000 local IPCs per second). Some examples of current popular microbenchmark suites are Winbench [14], the Bytemarks [3], and lmbench [7]. The shortcomings of microbenchmarks for general performance analysis are well documented <ref> [2] </ref>. The fundamental problem is that the computation they perform is not useful in and of itself and, as such, they fail to accurately reect system behavior for any realistic situation.
Reference: [3] <author> Ben Smith, Ultrafast Ultrasparcs, Byte Magazine, </author> <month> January </month> <year> 1996, </year> <note> page 139. Additional information on the Bytemarks suite is available on the Internet: http://www.byte.com/bmark/bdoc.htm. </note>
Reference-contexts: Microbench-marks are commonly used to measure the performance of relatively simple, specific events [7,10]. They can be used to measure latency (e.g., 7.7 ms per IPC) or throughput (e.g.,129,000 local IPCs per second). Some examples of current popular microbenchmark suites are Winbench [14], the Bytemarks <ref> [3] </ref>, and lmbench [7]. The shortcomings of microbenchmarks for general performance analysis are well documented [2]. The fundamental problem is that the computation they perform is not useful in and of itself and, as such, they fail to accurately reect system behavior for any realistic situation.
Reference: [4] <author> J. Bradley Chen, Yasuhiro Endo, Kee Chan, David Mazieres, Antonio Dias, Margo Seltzer, and Michael D. Smith, </author> <title> The Measured Performance of Personal Computer Operating Systems, </title> <journal> ACM Transactions on Computer Systems 14, </journal> <volume> 1, </volume> <month> February </month> <year> 1996, </year> <pages> pages 340. </pages>
Reference-contexts: Our goal is to measure latency for a more general class of events. A prior study compared Windows NT, Windows for Workgroups 3.11, and NetBSD to explore how differences in system structure affect overall performance <ref> [4] </ref>. The study used microbenchmarks to measure the latency of simple events and application workloads. Although some of the application workloads were based on interactive applications, the applications were driven by an uninterrupted input stream, and the results were reported in terms of throughput. <p> On the key stroke test, Windows 95 shows substantially worse performance than NT 4.0. This is a reection of segment register loads (not shown) and other overhead associated with 16-bit windows code <ref> [4] </ref>, which persist in Windows 95. The mouse click results are even more striking. The Windows 95 measurements are off the scale, because the system busy-waits between mouse down and mouse up events; therefore our measurement indicates the length of time the user took to perform the mouse click. <p> The difference in the latency between the two versions of Windows NT is explained by the differences in system architecture. In NT 3.51, the Win32 API is implemented by a user-level server. The negative performance effects of this server-based architecture were demonstrated in prior research <ref> [4] </ref>. In NT 4.0, some components of the Win32 API server are rumored to have been moved into the kernel. The improved locality from this change is reected in reduced TLB misses for NT 4.0 compared to NT 3.51.
Reference: [5] <author> Intel Corporation, </author> <title> Pentium Processor Family Developers Manual. Volume 3: Architecture and Programming Manual, </title> <publisher> Intel Corporation, </publisher> <year> 1995. </year>
Reference-contexts: the traditional Windows GUI, while NT 4.0 uses a GUI similar to that of Windows 95) and the movement of some Win32 components into the kernel. 2.2 The Pentium Counters The Intel Pentium processor has several built-in hardware counters, including one 64-bit cycle counter and two 40-bit configurable event counters <ref> [5] </ref>. The counters can be configured to count any one of a number of different hardware events (e.g., TLB misses, interrupts, or segment register loads). The Pentium counters make it possible to obtain accurate counts of a broad range of processor events. <p> The improved locality from this change is reected in reduced TLB misses for NT 4.0 compared to NT 3.51. A lower TLB miss rate implies fewer protection domain crossings in Pentium processors, which ush the TLB on each crossing <ref> [5] </ref>. Using 20 cycles per miss as a lower bound on TLB miss handling latency 5 , the extra TLB misses that occur for NT 3.51 (both bracketed numbers in the second graph report the total elapsed time for the benchmark run in seconds.
Reference: [6] <author> C. J. Lindblad and D. L. Tennenhouse, </author> <title> The VuSys-tem: A Programming System for Compute-Intensive Multimedia, </title> <journal> To appear in IEEE Journal of Selected Areas in Communication, </journal> <year> 1996. </year>
Reference-contexts: For multimedia applications and applications that use visual feedback (such as rubber-banding in a drawing program), latency requirements may be smaller. The limitations of the human visual system suggest that 10 ms provides a lower bound on the latencies that can be perceived for continuous media operations <ref> [6] </ref>. integrate over the histogram presenting a cumulative latency graph. This provides the quantitative data indicating how events of a particular duration contribute to the overall time required to complete a task.
Reference: [7] <author> Larry McVoy, Lmbench: </author> <title> Portable tools for performance analysis, </title> <booktitle> Proceedings of the 1996 USENIX Technical Conference, </booktitle> <month> January </month> <year> 1996, </year> <pages> pages 179 294. </pages>
Reference-contexts: Microbench-marks are commonly used to measure the performance of relatively simple, specific events [7,10]. They can be used to measure latency (e.g., 7.7 ms per IPC) or throughput (e.g.,129,000 local IPCs per second). Some examples of current popular microbenchmark suites are Winbench [14], the Bytemarks [3], and lmbench <ref> [7] </ref>. The shortcomings of microbenchmarks for general performance analysis are well documented [2]. The fundamental problem is that the computation they perform is not useful in and of itself and, as such, they fail to accurately reect system behavior for any realistic situation.
Reference: [8] <author> Jeffrey C. Mogul, </author> <title> SPECmarks are leading us astray, </title> <booktitle> Proceedings of the Third Workshop on Workstation Operating Systems, IEEE, </booktitle> <address> Key Bis-cayne, Florida, </address> <month> April </month> <year> 1992, </year> <pages> pages 160161. </pages>
Reference-contexts: To meet these requirements, the SPEC benchmarks are distributed in source form. The central problem with benchmarks such as the SPEC suites is that, although they may represent a realistic batch load, they fail to model the behavior of an interactive user <ref> [8] </ref>. Benchmarks such as Winstone, BAPCo SYSmark NT and BAPCo SYSmark 32 sacrifice portability in order to use popular interactive applications. Winstone is specific to PC-compatible hardware running Windows 95 or Windows NT.
Reference: [9] <author> James OToole, Scott Nettles, and David Gifford, </author> <title> Concurrent Compacting Garbage Collection, </title> <booktitle> The Proceedings of the Fourteenth ACM Symposium on Operating System Principles, </booktitle> <month> December </month> <year> 1993, </year> <pages> pages 161174. </pages>
Reference-contexts: Second, our techniques for visualizing latency were inuenced by the work of OToole et al. <ref> [9] </ref> on reducing the pause times for garbage collection. 2 Methodology Our methodology must provide the ability to measure the latency of individual events that occur while executing realistic interactive workloads.
Reference: [10] <author> John K. Ousterhout, </author> <title> Why Operating Systems Arent Getting Faster As Fast As Hardware. </title> <booktitle> Proceedings of the Summer 1991 USENIX Conference, </booktitle> <month> June </month> <year> 1991, </year> <pages> pages 247256. </pages>
Reference: [11] <author> Mark Shand, </author> <title> Measuring Unix Kernel Performance with Reprogammable Hardware, </title> <institution> Digital Paris Research Lab, </institution> <note> Research Report #19, </note> <month> August </month> <year> 1992. </year>
Reference-contexts: There was no attempt to measure interactive performance directly. Because of the requirement that the workloads run on a Unix system, the study included no popular PC applications. Two prior studies inuenced our work. Shand used a free-running counter in a tight loop to measure the latency of interrupts <ref> [11] </ref>. This methodology is similar to ours in that it measures the computation of interest by 1. TPC-A and TPC-B were declared obsolete by the Transaction Processing Council in June of 1995. detecting lost time.
Reference: [12] <author> Ben Shneiderman, </author> <title> Designing the User Interface, </title> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: Although the system may require a few tens of milliseconds to respond to each keystroke, such small waits will be unnoticeable, as even the best typists require approximately 120 ms per keystroke <ref> [12] </ref>. Distinguishing between wait time and think time is non-trivial, and the quantity and distribu-tion of wait time is what the user perceives as an applications responsiveness. Our measurement methodology must help us recognize the wait time that is likely to irritate users. <p> Events that complete in 0.1 seconds or less are believed to have imperceptible latency and do not contribute to user dissatisfaction 3 , whereas events in the 2-4 second range invariably irritate users <ref> [12] </ref>. Events that fall between these ranges may be acceptable but can correspond to perceptibly worse performance than events under 0.1 seconds. Our intuition is that a Operation under Windows NT 4.0.
Reference: [13] <author> Jeff Reilly, </author> <title> SPEC Discusses the History and Reasoning behind SPEC 95, </title> <journal> SPEC Newsletter, </journal> <volume> 7(3) </volume> <pages> 1-3, </pages> <month> September </month> <year> 1995. </year>
Reference: [14] <author> M. L. VanName and B. Catchings, </author> <title> Reaching New Heights in Benchmark Testing, </title> <journal> PC Magazine, </journal> <month> 13 December </month> <year> 1994, </year> <pages> pages 327-332. </pages> <note> Further information on Ziff-David benchmarks is available on the Internet: http://www.zdnet.com/zdbop/. </note>
Reference-contexts: Microbench-marks are commonly used to measure the performance of relatively simple, specific events [7,10]. They can be used to measure latency (e.g., 7.7 ms per IPC) or throughput (e.g.,129,000 local IPCs per second). Some examples of current popular microbenchmark suites are Winbench <ref> [14] </ref>, the Bytemarks [3], and lmbench [7]. The shortcomings of microbenchmarks for general performance analysis are well documented [2]. The fundamental problem is that the computation they perform is not useful in and of itself and, as such, they fail to accurately reect system behavior for any realistic situation.
References-found: 14

