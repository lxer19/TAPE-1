URL: ftp://ftp.cs.columbia.edu/pub/CAVE/papers/noguchi/noguchi-nayar-micro_icpr-94.ps.gz
Refering-URL: http://www.cs.columbia.edu/CAVE/video-demos.html
Root-URL: http://www.cs.columbia.edu
Title: Microscopic Shape from Focus Using Active Illumination  
Author: Minori Noguchi and Shree K. Nayar 
Address: New York, NY 10027  
Affiliation: Department of Computer Science Columbia University,  
Abstract: Shape from focus relies on surface texture for the computation of depth. In many real-world applications, surfaces can be smoothly shaded and lacking in detectable texture. In such cases, shape from focus generates inaccurate and sparse depth maps. This paper presents a novel extension to the original shape from focus method. A strong texture is forced on imaged surfaces by the use of active illumination. The exact pattern of the projected illumination is determined through a careful Fourier analysis of all the optical effects involved in focus analysis. When the focus operator used is a 2-d Laplacian, the optimal illumination pattern is found to be a checkerboard whose pitch is the same size as the distance between adjacent elements in the discrete Laplacian kernel. This analysis also reveals the exact number of images required for accurate shape recovery. These results are experimentally verified using an optical microscope. Surfaces lacking in texture, such as, three-dimensional structures on silicon substrates and solder joints on circuit boards were used in the experiments. The results show that the derived illumination pattern is in fact optimal, facilitating accurate shape recovery of complex and pertinent industrial samples. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. Born and E. Wolf, </author> <booktitle> Principles of Optics, </booktitle> <address> Lon-don:Pergamon, </address> <year> 1965. </year>
Reference-contexts: Since the lens aperture of the imaging system is of finite radius, it does not capture the higher frequencies radiated by the surface. This effect places a limit on the optical resolution of the imaging system, which is characterized by the optical transfer function (see <ref> [1] </ref> for details): O (u; v) = O (u; v; a; d 0 ) (7) d 0 ) 2 (fl sin fl) , u 2 + v 2 2a 0 , u 2 + v 2 &gt; 2a where fl = cos 1 ( d 0 a u 2 +v 2 <p> It is clear from the above expression that only spatial frequencies below the limit 2a d 0 will be imaged by the optical system. 3.3 Defocusing The defocus function is described in detail in previous work (see <ref> [1] </ref> [7], for example). Let ff be the distance between the focused image of a surface point and its defocused image formed on the sensor plane. <p> K represents the radiance, or brightness, of the surface point. In Fourier domain, the above defocus function is given by: H (u; v) = H (u; v; ff; a; d) = ad u 2 + v 2 2aff p (9) where J 1 is the first-order Bessel function <ref> [1] </ref>. 3.4 Image Sensing We assume the image sensor to be a typical CCD TV camera. Such a sensor can be modeled as a rectangular array of rectangular sensing elements (pixels). The quantum efficiency [8] of each pixel is assumed to be uniform over the area of the pixel.
Reference: [2] <author> R. N. Bracewell, </author> <title> The Fourier Transform and Its Applications, </title> <publisher> McGraw-Hill, </publisher> <year> 1965. </year>
Reference-contexts: The basic building block of the pattern is a rectangular illuminated patch, or cell, with uniform intensity: i c (x; y) = i c (x; y; b x ; b y ) = 2 ( b x 1 y) (1) where, 2 () is the two-dimensional Rectangular function <ref> [2] </ref>. The unknown parameters of this illumination cell are b x and b y , the length and width of the cell. <p> assumed to be repeated on a two-dimensional grid defined as: i g (x; y) = i g (x; y; t x ; t y ) = 2 III ( 1 t x x + 1 2 ( 1 t y y)) where, 2 III () is the 2-dimensional Shah function <ref> [2] </ref>, and t x and t y determine the periods of the grid in the x and y directions. Note that this grid is not rectangular but has vertical and horizontal symmetry on the x y plane.
Reference: [3] <author> T. Darrell and K. Wohn, </author> <title> Pyramid Based Depth from Focus, </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 504-509, </pages> <year> 1988. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past <ref> [3] </ref> [4] [6] [9] [10] [11] [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture.
Reference: [4] <author> J. Ens and P. Lawrence, </author> <title> A Matrix Based Method For Determining Depth From Focus, </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 600-606, </pages> <year> 1991. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] <ref> [4] </ref> [6] [9] [10] [11] [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture.
Reference: [5] <author> J. Goodman, </author> <title> Introduction to Fourier Optics, </title> <publisher> McGraw-Hill, </publisher> <address> San Fransisco, </address> <year> 1968. </year>
Reference: [6] <author> P. Grossmann, </author> <title> Depth from Focus, </title> <journal> Pattern Recognition Letters, </journal> <volume> Vol. 5, </volume> <pages> pp. 63-69, </pages> <year> 1987. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] <ref> [6] </ref> [9] [10] [11] [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture.
Reference: [7] <author> B.K.P. Horn, </author> <title> Focusing, </title> <journal> MIT Artificial Intelligence Laboratory, </journal> <volume> Memo No. 160, </volume> <month> May, </month> <year> 1968. </year>
Reference-contexts: It is clear from the above expression that only spatial frequencies below the limit 2a d 0 will be imaged by the optical system. 3.3 Defocusing The defocus function is described in detail in previous work (see [1] <ref> [7] </ref>, for example). Let ff be the distance between the focused image of a surface point and its defocused image formed on the sensor plane. The light energy radiated by the surface point and collected by the imaging optics is uniformly distributed over a circular patch on the sensor plane.
Reference: [8] <author> B. K. P. Horn, </author> <title> Robot Vision, </title> <publisher> MIT Press, </publisher> <year> 1986. </year>
Reference-contexts: Such a sensor can be modeled as a rectangular array of rectangular sensing elements (pixels). The quantum efficiency <ref> [8] </ref> of each pixel is assumed to be uniform over the area of the pixel. Let m (x; y) be the continuous image formed on the sensor plane. The finite pixel area has the effect of averaging the continuous image m (x; y).
Reference: [9] <author> R.A. Jarvis, </author> <title> Focus optimization criteria for computer image processing, </title> <journal> Microscope, </journal> <volume> Vol. 24, No. 2, </volume> <pages> pp. 163-180, </pages> <year> 1976. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] <ref> [9] </ref> [10] [11] [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [10] <author> A. Krishnan and N. Ahuja, </author> <title> Range Estimation From Focus Using a Non-frontal Imaging Camera, </title> <booktitle> Proc. AAAI Conference, </booktitle> <address> pp.830-835, </address> <month> July, </month> <year> 1993. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] <ref> [10] </ref> [11] [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [11] <author> E. Krotkov, </author> <title> Focusing, </title> <journal> International Journal of Computer Vision, </journal> <volume> Vol. 1, </volume> <pages> pp. 223-237, </pages> <year> 1987. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] <ref> [11] </ref> [12] [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [12] <author> G. Ligthart and F. Groen, </author> <title> A comparison of different autofo-cus algorithms, </title> <booktitle> Proc. of International Conference on Pattern Recognition, </booktitle> <pages> pp. 597-600, </pages> <year> 1982. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] <ref> [12] </ref> [13] [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [13] <author> S. K. Nayar and Y. Nakagawa, </author> <title> Shape from Focus: An Effective Approach for Rough Surfaces, </title> , <booktitle> Proc. IEEE Intl. Conf. on Robotics and Automation, </booktitle> <pages> pp. 218-225, </pages> <year> 1990. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] <ref> [13] </ref> [15] [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations. <p> Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations. It is not a viable technique for smoothly shaded surfaces. In <ref> [13] </ref>, a shape from focus system was developed for microscopic objects. The high magnification of a microscope results in images that capture brightness variations caused by the micro-structure of the surface. Most surfaces that appear smooth and non-textured to the naked eye produce highly textured images under a microscope. <p> Most surfaces that appear smooth and non-textured to the naked eye produce highly textured images under a microscope. Examples of such surfaces are paper, plastics, ceramics, etc. Microscopic shape from focus <ref> [13] </ref> was therefore demonstrated to be an effective approach, offering solutions to a variety to challenging shape inspection problems. However, there exist surfaces that are smooth at the micro-structure level and consequently do not produce sufficient texture even under a microscope. <p> The analysis is carried further to determine the minimum number of images required for accurate depth estimation. The resulting illumination filter was fabricated using micro-lithography and incorporated into the shape from focus system <ref> [13] </ref>. The illumination pattern is projected onto the sample via the same optics used to image the sample. <p> Accurate and detailed depth maps of structures on silicon substrates and solder joints are reconstructed. The optimal illumination is shown to produce higher accuracy than other sub-optimal illuminations. 2 Shape from Focus We begin with a brief overview of the microscope-based shape from focus system described in <ref> [13] </ref>. Fig.1 shows a schematic diagram of the various components of the system. The sample of interest (object) is placed on the translational stage of the microscope. The sample is magnified using the objective lens of the microscope and imaged by a standard CCD TV camera. In the original implementation [13], <p> <ref> [13] </ref>. Fig.1 shows a schematic diagram of the various components of the system. The sample of interest (object) is placed on the translational stage of the microscope. The sample is magnified using the objective lens of the microscope and imaged by a standard CCD TV camera. In the original implementation [13], the sample is illuminated using the bright-field illumination of the microscope where light rays emitted by a source are projected onto the sample through the same objective lens used to image the sample. This results in uniform illumination over the entire field of view. <p> The peak is localized by Gaussian interpolation of the three largest focus measures. This system was tested on a variety of complex samples, such as, via-hole fillings on ceramic substrates <ref> [13] </ref>. Such samples have substantial texture, enabling the recovery of very precise depth maps. Here, we are concerned with surfaces that do not produce sufficient texture for focus analysis. To overcome this problem, we propose the projection of an illumination pattern on the imaged surface. <p> Note that the illumination pattern (i c fl i g ) is projected through the same optics that is used for image formation. Consequently, this pattern is also subject to the limits imposed by the optical transfer function o and 1 In <ref> [13] </ref>, a modified version of the Laplacian operator was used to overcome the problem of zero-crossings caused by the unknown surface texture. Here, the texture is determined by the illumination pattern and hence the Laplacian is adequate. 3 the defocus function h. <p> (p x ; p y ) is the pixel size, and (' x ; ' y ) is the illumination phase shift with respect to the image sensing grid. 5 Implementation The optimal illumination filter shown in Fig. 2 (b) was incorporated into the shape from focus system developed in <ref> [13] </ref>. In addition, cross-polarized filters were fixed in front of the illumination filter and the image sensor to filter out strong specular reflections from the sample. A set of sample images are obtained by automatically moving the microscope stage in increments of z = z i z i1 . <p> The discrete stage position z j that yields the maximum focus measure value at an image point, can be used as an approximation of the depth of the corresponding surface point. A more accurate depth estimate z is obtained by applying Gaussian interpolation <ref> [13] </ref> to the three focus measures corresponding to z j1 ; z j ; and z j+1 . It was shown in [13] that at least 3 focus measures are needed for depth estimation by Gaussian interpolation. <p> A more accurate depth estimate z is obtained by applying Gaussian interpolation <ref> [13] </ref> to the three focus measures corresponding to z j1 ; z j ; and z j+1 . It was shown in [13] that at least 3 focus measures are needed for depth estimation by Gaussian interpolation. In the active illumination system proposed here, the fundamental frequency of the surface texture is determined by the illumination filter used and is simply q t x t y ) 2 . <p> Silicon wafer inspection is of great relevance in a variety of chip manufacturing processes. The surface of the wafer is very smooth, resulting in images (see Fig. 3 (a)) that are more or less textureless. This renders the original shape from focus system <ref> [13] </ref> ineffective (see Fig. 3 (d)). A total of 16 images were taken for this sample. The optimal illumination (Fig. 3 (b)) produces very accurate shape information that is superior to that produced by sub-optimal filters (Fig. 3 (c)).
Reference: [14] <author> M. Noguchi and S. K. Nayar, </author> <title> Recovering Microscopic Shapes Using Focus Analysis and Active Illumination, </title> <type> Technical Report, </type> <institution> CUCS-024-94, </institution> <month> February, </month> <year> 1994. </year>
Reference-contexts: Using these values in eqs.(37) and (38) we get the maximum allowable stage displacement z 0:5 m. Experiments reported in <ref> [14] </ref> illustrate that z = 0:5 m does in fact produce the best results. Further decreasing z does not significantly improve the accuracy of the depth maps. Figs. 3 and 4 illustrate the advantage of using the optimal illumination over sub-optimal and uniform illumination.
Reference: [15] <author> A. Pentland, </author> <title> A New Sense for Depth of Field, </title> <journal> IEEE Transactions on Pattern Analysis and Machine Intelligence, </journal> <volume> Vol. 9, No. 4, </volume> <pages> pp. 523-531, </pages> <month> July </month> <year> 1987. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] [13] <ref> [15] </ref> [17] [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations. <p> Minori Noguchi is a visiting researcher from the Production Engineering Research Laboratory, Hitachi Ltd., 292 Yoshida-cho, Totsuka-ku, Yokohama 244, Japan. of great industrial import. For such samples, shape from focus has remained inadequate. In <ref> [15] </ref>, it was shown that textures can be forced on non-textured surfaces by using active illumination. An illumination pattern was projected on the scene and the resulting texture was used to estimate depth from defocus. In this case, the illumination pattern was selected in an arbitrary fashion.
Reference: [16] <author> A. Pentland, </author> <title> Simple Range Cameras Based on Focal Error, </title> <booktitle> Proc. Intl. Simp. on Advanced Image-acquisition Technology, </booktitle> <month> November </month> <year> 1991. </year>
Reference: [17] <author> M. Subbarao, </author> <title> Efficient Depth Recovery through Inverse Optics, Machine Vision for Inspection and Measurement, edited by H. </title> <publisher> Freeman, Academic Press, </publisher> <year> 1989. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] [13] [15] <ref> [17] </ref> [18] [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [18] <author> M. Subbarao and G. Surya, </author> <title> Depth from Defocus: A Spatial Domain Approach, </title> <type> Technical Report 92.120.45, </type> <institution> SUNY, Stony Brook, </institution> <month> December 92. </month>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] [13] [15] [17] <ref> [18] </ref> [19] [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [19] <author> J.M. Tenenbaum, </author> <title> Accommodation in Computer Vision, </title> <type> Ph.D. Thesis, </type> <institution> Stanford University, </institution> <year> 1970. </year>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] [13] [15] [17] [18] <ref> [19] </ref> [20]. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
Reference: [20] <author> Y. Xiong and S. A. Shafer, </author> <title> Depth from Focusing and Defocus-ing, </title> <booktitle> Proc. CVPR, </booktitle> <pages> pp. 68-73, </pages> <year> 1993. </year> <month> 6 </month>
Reference-contexts: As a result it is computational efficient, requiring only the use of simple focus measure operators. Several focus operators and depth estimation algorithms based on these operators have been suggested in the past [3] [4] [6] [9] [10] [11] [12] [13] [15] [17] [18] [19] <ref> [20] </ref>. An inherent weakness of focus-based methods, however, is that they require the imaged scene to have significant texture. Since defocusing is equivalent to low-pass filtering, depth from focus (or defocus) is effective only if the scene has high-frequency brightness variations.
References-found: 20

