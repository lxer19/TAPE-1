URL: ftp://ftp.cse.cuhk.edu.hk/pub/techreports/95/tr-95-15.ps.gz
Refering-URL: ftp://ftp.cs.cuhk.hk/pub/techreports/README.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Efficient Connectionist Representations of Syntactic Parse Trees for Grammatical Inference  
Author: HO, Kei Shiu Edward CHAN, Lai Wan 
Address: Shatin, N.T., Hong Kong  
Affiliation: Department of Computer Science and Engineering The Chinese University of Hong Kong  
Abstract: In this paper, syntactic parsing is tackled in the context of connectionism. We present the Confluent Preorder Parser (CPP) in which syntactic parsing is achieved via a holistic transformation from the sentence representation to the desired parse tree representation. To achieve the generalization ability, these connectionist representations should be structural-preserving, that is, parse trees which look alike (structurally) in the symbolic forms should have their corresponding representations close to each other in the representational space. In this respect, the traditional convention to encode the parse tree as a hierarchical data structure is inappropriate. We show that by linearizing the parse tree via preorder traversal and applying confluent inference during training, the representations obtained are capable of preserving the structural characteristics of the parse trees they encode. Simulation results show that CPP has an excellent generalization performance of more than 90%. In addition, it is capable of correcting erroneous sentences and resolving syntactic ambiguities. We analyze CPP in two different ways. Firstly, hierarchical clustering is used to examine the parse tree representations developed. Then, an analysis is presented which elucidates the operations of CPP as governed by a finite state automata and holistic parsing is interpreted as a series of decision makings during the process. Based on this formalism, the syntactic parsing mechanism, the generalization and the error recovery capability of CPP can all be explained in terms of state transitions. In addition, we can observe a theoretical limit on the computational power of CPP in learning infinite grammars. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. V. Aho, R. Sethi, and J. D. Ullman. </author> <booktitle> Compilers: Principles, Techniques, and Tools. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Reading:Mass, </address> <year> 1986. </year>
Reference-contexts: In CPP, robustness in processing is achieved without an extra cost neither is it necessary to teach CPP to parse negative training examples (that is, erroneous or ambiguous sentences), nor an explicit mechanism of any kind has to be installed (e.g. error production <ref> [1, 17] </ref>). 2 Structural-Preserving Representations As mentioned in Section 1, distributed representations have to be developed for the sentences and the parse trees. In CPP, these representations are "learned" via training a neural network, rather than "designed" or "calculated" as in the Holographic Reduced Representations HRR [20].
Reference: [2] <author> J. Allen. </author> <title> Natural Language Understanding. </title> <publisher> The Benjamin/Cummings Publishing Company, Inc., </publisher> <address> second edition, </address> <year> 1995. </year> <month> 20 </month>
Reference-contexts: During parsing, the internal structure of the sentence is analyzed fl email: ho052@cs.cuhk.hk y email: lwchan@cs.cuhk.hk 1 step-by-step according to a predefined grammar, with the parse tree being built up incrementally at the same time (see Figure 2). Representative examples include the pushdown automata [13] and the chart parser <ref> [2] </ref>. 1.2 Hybrid Parsers Despite their popularity, symbolic parsing techniques have been criticized of their inadequacy in handling natural languages. In particular, the "crisp" nature of the symbolic rules simply fails to demonstrate the type of generative capability and flexibility (that is, robustness) as found in actual language usage [15].
Reference: [3] <author> G. Berg. </author> <title> Learning recursive phrase structure : Combining the strengths of PDP and X-bar syntax. </title> <type> Technical Report TR91-5, </type> <institution> University at Albany, </institution> <year> 1991. </year>
Reference-contexts: In much the same way, the representations encoding two structurally similar parse trees should also be close to each other in the Euclidean space. In this respect, the traditional convention that a parse tree should be represented as a hierarchical data structure (see <ref> [3, 11, 24] </ref>) has been found inappropriate. The structure of the parse tree is only implicitly reflected by the order and manner in which different components and terminals of it are combined together to give the fixed length representation.
Reference: [4] <author> D. S. Blank, L.A. Meeden, and J. B. Marshall. </author> <title> Exploring the symbolic/subsymbolic continuum: A case study of RAAM. </title> <editor> In J. Dinsmore, editor, </editor> <booktitle> The Symbolic and Connectionist Paradigms: Closing the Gap, </booktitle> <pages> pages 113-148. </pages> <publisher> Lawrence Erlbaum Associates, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: The Confluent Preorder Parser (CPP) will be presented which exemplifies the Holistic Parsing Paradigm. Instead of a step-by-step algorithmic approach, syntactic parsing is accomplished via a Holistic Transformation <ref> [4, 5] </ref>. This transformation maps the connectionist representation encoding the input sentence to the connectionist representation encoding the target parse tree (see Figure 3). Effectively, the parsing mechanism is encapsulated in a "black box".
Reference: [5] <author> D. J. Chalmers. </author> <title> Syntactic transformations of distributed representations. </title> <editor> In N. Sharkey, editor, </editor> <booktitle> Connectionist Natural Language Processing, </booktitle> <pages> pages 46-55. </pages> <publisher> Intellect Books, </publisher> <year> 1992. </year>
Reference-contexts: The Confluent Preorder Parser (CPP) will be presented which exemplifies the Holistic Parsing Paradigm. Instead of a step-by-step algorithmic approach, syntactic parsing is accomplished via a Holistic Transformation <ref> [4, 5] </ref>. This transformation maps the connectionist representation encoding the input sentence to the connectionist representation encoding the target parse tree (see Figure 3). Effectively, the parsing mechanism is encapsulated in a "black box".
Reference: [6] <author> L. Chrisman. </author> <title> Learning recursive distributed representations for holistic computation. </title> <journal> Connection Science, </journal> <volume> 3 </volume> <pages> 345-366, </pages> <year> 1991. </year>
Reference-contexts: When the preorder traversal sequences are encoded using the SRAAM network, their similarity will be manifested in the coding developed, thus preserving the structural characteristics of their corresponding parse trees. This is confirmed by the experimental results shown in Section 5. To further promote the generalization performance, confluent inference <ref> [6] </ref> is applied in training. This technique, instead of performing a mapping from one representational space to another one, develops the same SRAAM representation for the input sentence and the parse tree. <p> It combines SRAAM 1 (the Sentence Encoder) and SRAAM 2 (the Preorder Traversal Encoder) in Figure 5 to form a dual-ported SRAAM <ref> [6] </ref>, with one single hidden layer shared between the two SRAAMs. The equivalence mapping from the preorder traversal sequence to the parse tree is done separately. Given a set of training sentences, we first generate the preorder traversals for their corresponding parse trees. <p> The training sentences as sequences are then encoded by the Sentence Encoder on the left half of CPP (see Figure 7 (a)), while the preorder traversals are encoded by the Preorder Traversal Encoder on the right half (see Figure 7 (b)). As explained in Section 2, confluent inference <ref> [6] </ref> is applied.
Reference: [7] <author> J. L. Elman. </author> <title> Finding structure in time. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 179-211, </pages> <year> 1990. </year>
Reference-contexts: Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by [8, 23, 31], and the "prediction problem" attempted by <ref> [7, 18, 26] </ref>. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13]. In contrast to other approaches (such as SPEC [19] and the Neural Network Pushdown Automata [28]), CPP is not equipped with a stack.
Reference: [8] <author> C. Giles, C. Miller, D. Chen, H. Chen, G. Sun, and Y. Lee. </author> <title> Learning and extracting finite state automata with second-order recurrent neural networks. </title> <journal> Neural Computation, </journal> <volume> 4(3) </volume> <pages> 393-405, </pages> <year> 1992. </year>
Reference-contexts: This has the advantage that little knowledge has to be assumed for the actual, detailed parsing algorithm. Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by <ref> [8, 23, 31] </ref>, and the "prediction problem" attempted by [7, 18, 26]. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13].
Reference: [9] <author> K. S. Ho and L. W. Chan. </author> <title> Confluent Preorder Parsing. </title> <type> Technical Report CS-TR-95-03, </type> <institution> Department of Computer Science, The Chinese University of Hong Kong, </institution> <year> 1995. </year>
Reference-contexts: To better understand this error recovery capability of CPP, two factors which may affect its efficiency have been evaluated, namely, the "Location of the Error" and the "Length of the Erroneous Sentence". See [10] and <ref> [9] </ref> for a detailed discussion. 3 We should account for the case when S E is corrected by bringing its two swapped elements back to their original order.
Reference: [10] <author> K. S. Ho and L. W. Chan. </author> <title> Syntactic parsing using RAAM. </title> <booktitle> In Proc. World Congress on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 485-488, </pages> <year> 1995. </year>
Reference-contexts: To better understand this error recovery capability of CPP, two factors which may affect its efficiency have been evaluated, namely, the "Location of the Error" and the "Length of the Erroneous Sentence". See <ref> [10] </ref> and [9] for a detailed discussion. 3 We should account for the case when S E is corrected by bringing its two swapped elements back to their original order.
Reference: [11] <author> K. S. Ho and L.W. Chan. </author> <title> Representing sentence structures in neural networks. </title> <editor> In M. W. Kim and S. Y Lee, editors, </editor> <booktitle> Proc. International Conference in Neural Information Processing Systems, </booktitle> <volume> volume 3, </volume> <pages> pages 1462-1467, </pages> <year> 1994. </year>
Reference-contexts: This is superior when compared with previous holistic parsing approaches. For example, in the Backpropagation Parsing Network <ref> [11] </ref>, the generalization performance is 40% only and for the parser designed by Reily [24], the generalization performance is as low as 0%. * CPP is also capable of parsing erroneous sentences and resolving syntactic ambiguities. <p> In much the same way, the representations encoding two structurally similar parse trees should also be close to each other in the Euclidean space. In this respect, the traditional convention that a parse tree should be represented as a hierarchical data structure (see <ref> [3, 11, 24] </ref>) has been found inappropriate. The structure of the parse tree is only implicitly reflected by the order and manner in which different components and terminals of it are combined together to give the fixed length representation. <p> They were trained separately on parsing the same set of 80 complete sentences used in Section 4.1 : * Prototype P1 has the same architecture as the Backpropagation Parsing Network (BPN) proposed in <ref> [11] </ref>. Linearization is not adopted. The parse trees as hierarchical data structures are encoded by a RAAM network, while the sentences as sequences are represented using a SRAAM network.
Reference: [12] <author> K. S. Ho, W. S. Luk, and L. W. Chan. </author> <title> Parallel implementation of the Confluent Preorder Parser on the DECmpp. </title> <note> in preparation. </note>
Reference-contexts: Another is a parallel version of CPP on the DECmpp computer which has an SIMD architecture. Tens of folds in speedup is achieved. The parallel 7 CPP takes less than 30 seconds to finish one epoch and totally, it needs about 2,000 cycles before training converges. See <ref> [12] </ref> for a detailed discussion of this parallel CPP. 4 Experiment 4.1 Generalization Performance of CPP To evaluate the performance of CPP, we use the following context-free grammar as in [22] : Sentence Noun Phrase Verb Phrase Prepositional Phrase Adjectival Phrase np ! D ap np ! D N pp !
Reference: [13] <author> J. E. Hopcroft and J. D. Ullman. </author> <title> Introduction to Automata Theory, Languages, and Computation. </title> <publisher> Addison-Wesley, </publisher> <address> Reading:Mass, </address> <year> 1979. </year>
Reference-contexts: During parsing, the internal structure of the sentence is analyzed fl email: ho052@cs.cuhk.hk y email: lwchan@cs.cuhk.hk 1 step-by-step according to a predefined grammar, with the parse tree being built up incrementally at the same time (see Figure 2). Representative examples include the pushdown automata <ref> [13] </ref> and the chart parser [2]. 1.2 Hybrid Parsers Despite their popularity, symbolic parsing techniques have been criticized of their inadequacy in handling natural languages. <p> Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required <ref> [13] </ref>. In contrast to other approaches (such as SPEC [19] and the Neural Network Pushdown Automata [28]), CPP is not equipped with a stack. Secondly, in the classification problem, a single bit suffices to represent whether a string can be derived by the language or not. <p> For CPP2, there are 106 final states, 77 intermediate states and one starting state, giving a total of 184 states (see Table 6). These states, together with the possible transitions between them, effectively define a finite state automata <ref> [13] </ref> which encodes the grammatical knowledge/regularity necessary for the parsing function. Syntactic parsing by CPP can therefore be re-formulated as follows.
Reference: [14] <author> R. A. Johnson and D. W. Wichern. </author> <title> Applied Multivariate Statistical Analysis. </title> <publisher> Prentice-Hall, Inc., </publisher> <address> third edition, </address> <year> 1992. </year>
Reference-contexts: Because the representations developed are of very high dimension (over 200), techniques must be applied to project them onto a lower-dimensional space for easy visualization. Two methods have been used. Firstly, hierarchical clustering <ref> [14] </ref> is applied to the final representations of the 80 parse trees obtained in training. For Prototype P1, local clustering does occur occassionally (see Figure 9). However, no regular and systematic grouping can be observed in the global sense, regardless of whether prefix or 11 suffix is considered.
Reference: [15] <author> S. C. Kwasny and K. A. Faisal. </author> <title> Symbolic parsing via subsymbolic rules. </title> <editor> In J. Dinsmore, editor, </editor> <booktitle> The Symbolic and Connectionist Paradigms: Closing the Gap, </booktitle> <pages> pages 209-236. </pages> <publisher> Lawrence Erlbaum, </publisher> <address> Hillsdale, NJ, </address> <year> 1992. </year>
Reference-contexts: In particular, the "crisp" nature of the symbolic rules simply fails to demonstrate the type of generative capability and flexibility (that is, robustness) as found in actual language usage <ref> [15] </ref>. Moreover, the formulation of the parsing algorithm requires a thorough knowledge of the characteristics of the target grammar. Unfortunately, parsing (particularly when used in natural language understanding) is among those problems of which the solution is not well defined, let alone talking about the details of its mechanism. <p> Many of the deeply-entrenched assumptions may have little evidential background and the linguistic theory approaches are originally intended for explaining the competence of human language usage only, rather than for the purpose of efficient algorithmic realization (see also <ref> [15] </ref>). Compared with the symbolic AI methods, connectionist approaches are inherently robust. In addition, they use inductive learning to acquire knowledge from examples and can generalize naturally to unseen sentences. <p> Nodes leading to a consistent parse are connected via excitatory links, while nodes corresponding to mutually-exclusive parses are connected via inhibitory links. The overall interpretation of the sentence is obtained via parallel constraint satisfaction. 2 The Connectionist Deterministic Parser (CDP) <ref> [15] </ref> consists of a symbolic component which is a deterministic parser, and a subsymbolic component which is a feedforward network. <p> In some sense, they are merely partial or complete neural network re-implementation of some symbolic algorithm. The inductive learning power of neural networks simply has not been fully utilized. What we target for is a system that exhibits a style of Extensional Programming as discussed in <ref> [15] </ref> with access to the input and the output of the parsing process only, but with minimal knowledge of the actual algorithm, the system can induce the transformation/mapping from the sentence to the parse tree which effects syntactic parsing. 1.3 Holistic Parsing by the Confluent Preorder Parser In this paper, syntactic
Reference: [16] <author> S. C. Kwasny and B. L. </author> <title> Kalman. Tail-recursive distributed representations and simple recurrent networks. </title> <type> Technical Report WUCS-93-52, </type> <institution> Department of Computer Science, Washington University, </institution> <year> 1993. </year>
Reference-contexts: In view of this, the parse tree is no longer represented as a hierarchical data structure in CPP. Instead, it is linearized 1 by preorder traversal to form a sequence and encoded using a SRAAM network 1 Linearization has first been applied by Kwasny and Kalman <ref> [16] </ref> to facilitate the encoding of trees using a SRAAM network. Their approach and ours differ in the way of expressing the empty branches. In [16], null pointers (as the special terminal `*') are explicitly indicated for the leaves and those internal nodes with empty branch (es). <p> linearized 1 by preorder traversal to form a sequence and encoded using a SRAAM network 1 Linearization has first been applied by Kwasny and Kalman <ref> [16] </ref> to facilitate the encoding of trees using a SRAAM network. Their approach and ours differ in the way of expressing the empty branches. In [16], null pointers (as the special terminal `*') are explicitly indicated for the leaves and those internal nodes with empty branch (es). These `*'s 4 (Sequential Recursive Auto-Associative Memory [22]).
Reference: [17] <author> J. R. Levine, T. Mason, and D. Brown. </author> <title> lex & yacc. </title> <publisher> O'Reilly & Associates, Inc., </publisher> <address> second edition, </address> <year> 1992. </year>
Reference-contexts: In CPP, robustness in processing is achieved without an extra cost neither is it necessary to teach CPP to parse negative training examples (that is, erroneous or ambiguous sentences), nor an explicit mechanism of any kind has to be installed (e.g. error production <ref> [1, 17] </ref>). 2 Structural-Preserving Representations As mentioned in Section 1, distributed representations have to be developed for the sentences and the parse trees. In CPP, these representations are "learned" via training a neural network, rather than "designed" or "calculated" as in the Holographic Reduced Representations HRR [20].
Reference: [18] <author> A. Maskara and A. Noetzel. </author> <title> Forced simple recurrent neural networks and grammatical inference. </title> <booktitle> In Proc. the Fifteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 420-425, </pages> <address> Hillsdale, NJ, 1993. </address> <publisher> Lawrence Erlbaum Associates. </publisher>
Reference-contexts: Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by [8, 23, 31], and the "prediction problem" attempted by <ref> [7, 18, 26] </ref>. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13]. In contrast to other approaches (such as SPEC [19] and the Neural Network Pushdown Automata [28]), CPP is not equipped with a stack.
Reference: [19] <author> R. Miikkulainen. </author> <title> Subsymbolic case-role analysis of sentences with embedded clauses. </title> <type> Technical Report AI93-202, </type> <institution> Department of Computer Sciences, the University of Texas at Austin, </institution> <year> 1993. </year>
Reference-contexts: Effectively, the rigid rules are replaced by the feedforward network which has a more flexible decision boundary. Robustness in processing is thus achieved. In SPEC (Subsymbolic Parser for Embedded Clauses) <ref> [19] </ref>, a complex sentence is broken down into its constituent clauses by the Segmenter (which is a feedforward network). Each clause is parsed by the Parser (which is a Simple Recurrent Network) to generate the corresponding case-role representation. <p> Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13]. In contrast to other approaches (such as SPEC <ref> [19] </ref> and the Neural Network Pushdown Automata [28]), CPP is not equipped with a stack. Secondly, in the classification problem, a single bit suffices to represent whether a string can be derived by the language or not.
Reference: [20] <author> T. A. </author> <title> Plate. Distributed Representations and Nested Compositional Structure. </title> <type> PhD thesis, </type> <institution> Graduate Department of Computer Science, University of Toronto, </institution> <year> 1994. </year>
Reference-contexts: In CPP, these representations are "learned" via training a neural network, rather than "designed" or "calculated" as in the Holographic Reduced Representations HRR <ref> [20] </ref>. <p> extrapolating between a novel pattern and the training patterns, the representations developed should be Structural-Preserving their surface forms should reflect systematically the structural characteristics of the data structures they encode, such as the constituent components that are present as well as their relationship and alignment in the structure (see also <ref> [20] </ref>). Precisely, the coding corresponding to two similar sentences should have a short Euclidean distance in the connectionist representational space. In much the same way, the representations encoding two structurally similar parse trees should also be close to each other in the Euclidean space.
Reference: [21] <author> J. Pollack and D. Waltz. </author> <title> Massively parallel parsing: A strongly interactive model of natural language interpretation. </title> <journal> Cognitive Science, </journal> <volume> 9 </volume> <pages> 51-74, </pages> <year> 1985. </year>
Reference-contexts: To exploit these advantages, efforts have been paid to integrate the connectionist techniques into the symbolic processing framework, giving rise to various "hybrid" parsers. For example, in the "Massively Parallel Parsing" system <ref> [21] </ref>, a chart parser is used to generate all possible parses of the input sentence first. They are then translated into a connectionist network in which nodes are used to represent alternative syntactic classes and meanings of the words as well as the pragmatic constraints.
Reference: [22] <author> J. B. Pollack. </author> <title> Recursive distributed representations. </title> <journal> Artificial Intelligence, </journal> <volume> 46 </volume> <pages> 77-105, </pages> <year> 1990. </year>
Reference-contexts: Each clause is parsed by the Parser (which is a Simple Recurrent Network) to generate the corresponding case-role representation. To deal with center-embeddings, information of the superordinate clause is held in the Stack (which is a RAAM network <ref> [22] </ref>) when the Parser has to switch to a subordinate clause. When the processing of the subordinate clause is done, this information is recovered/popped from the Stack and the parsing of the superordinate clause continues. <p> Thus to encode these data structures using fixed-length coding, distributed representations must be used and they have to be developed using functional composition techniques [29] (such as RAAM <ref> [22] </ref>). Needless to say, the overall performance of the parser will be intimately related to these representations. Unfortunately, encoding a recursive symbolic data structure, especially a hierarchical one (such as the parse tree), is by all means "hard" for neural networks. <p> Their approach and ours differ in the way of expressing the empty branches. In [16], null pointers (as the special terminal `*') are explicitly indicated for the leaves and those internal nodes with empty branch (es). These `*'s 4 (Sequential Recursive Auto-Associative Memory <ref> [22] </ref>). For example, the parse tree in Figure 1 gives rise to the sequence h s np D N vp V np np D N pp P np D N i. CPP thus produces the preorder traversal sequence first, from which the target parse tree can be reconstructed 2 . <p> As explained in Section 2, confluent inference [6] is applied. In addition to the standard training method of SRAAM <ref> [22] </ref> (Figure 7 (a) and 7 (b)), an extra error term is backpropagated during training from the output layer of the Preorder Traversal Encoder to the input layer of the Sentence Encoder, and also from the output layer of the Sentence Encoder to the input layer of the Preorder Traversal Encoder, <p> See [12] for a detailed discussion of this parallel CPP. 4 Experiment 4.1 Generalization Performance of CPP To evaluate the performance of CPP, we use the following context-free grammar as in <ref> [22] </ref> : Sentence Noun Phrase Verb Phrase Prepositional Phrase Adjectival Phrase np ! D ap np ! D N pp ! P np s ! np V vp ! V pp ap ! A N Each syntactically well-formed sentence (and its parse tree) is generated using s as the root.
Reference: [23] <author> J. B. Pollack. </author> <title> The induction of dynamical recognizers. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 227-252, </pages> <year> 1991. </year>
Reference-contexts: This has the advantage that little knowledge has to be assumed for the actual, detailed parsing algorithm. Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by <ref> [8, 23, 31] </ref>, and the "prediction problem" attempted by [7, 18, 26]. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13].
Reference: [24] <author> R. Reily. </author> <title> Connectionist techniques for on-line parsing. </title> <journal> Network, </journal> <volume> 3 </volume> <pages> 37-45, </pages> <year> 1990. </year> <month> 21 </month>
Reference-contexts: This is superior when compared with previous holistic parsing approaches. For example, in the Backpropagation Parsing Network [11], the generalization performance is 40% only and for the parser designed by Reily <ref> [24] </ref>, the generalization performance is as low as 0%. * CPP is also capable of parsing erroneous sentences and resolving syntactic ambiguities. This characteristic is especially appealing in natural language processing when incomplete or ungrammatical sentences have to be dealt with frequently. <p> In much the same way, the representations encoding two structurally similar parse trees should also be close to each other in the Euclidean space. In this respect, the traditional convention that a parse tree should be represented as a hierarchical data structure (see <ref> [3, 11, 24] </ref>) has been found inappropriate. The structure of the parse tree is only implicitly reflected by the order and manner in which different components and terminals of it are combined together to give the fixed length representation.
Reference: [25] <author> D. E. Rumelhart, G. E. Hinton, and R.J. Williams. </author> <title> Learning internal representations through error propagation. </title> <editor> In D. E. Rumelhart, J. L. McClelland, and The PDP Research Group, editors, </editor> <booktitle> Parallel distributed processing: Experiments in the microstructure of cognition, </booktitle> <volume> volume 1, </volume> <pages> pages 318-362. </pages> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: For CPP2, in addition to complete sentences, the network is also trained to parse phrases to produce the corresponding sub-trees (see Section 3.2). In either case, the network is trained using the back-propagation algorithm <ref> [25] </ref> with fixed rate and momentum until all sentences in the training set can be successfully parsed. Generalization capability is then measured by evaluating its performance on the testing set.
Reference: [26] <author> D. Servan-Schreiber, A. Cleeremans, and J. L. McCleeland. </author> <title> Graded state machines: The representation of temporal contingencies in simple recurrent networks. </title> <journal> Machine Learning, </journal> <volume> 7 </volume> <pages> 161-193, </pages> <year> 1991. </year>
Reference-contexts: Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by [8, 23, 31], and the "prediction problem" attempted by <ref> [7, 18, 26] </ref>. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13]. In contrast to other approaches (such as SPEC [19] and the Neural Network Pushdown Automata [28]), CPP is not equipped with a stack.
Reference: [27] <author> A. Stolcke and D. Wu. </author> <title> Tree matching with recursive distributed representations. </title> <type> Technical Report TR-92-025, </type> <institution> International Computer Science Institute, Berkeley, </institution> <year> 1992. </year>
Reference-contexts: The representations of the terminals (including the internal nodes) are shown in Table 1. Orthogonal binary strings are used to minimize the mutual interference between the coding. The error injection method proposed in <ref> [27] </ref> is adopted an extra bit is attached as the leftmost bit of each coding which is initialized to 0 for a terminal while for a non-terminal (a SRAAM coding), its value is forced to 1 during training. Two CPPs were evaluated.
Reference: [28] <author> G. Z. Sun, C. L. Giles, H. H. Chen, and Y. C. Lee. </author> <title> The neural network pushdown automaton: Model, stack and learning simulations. </title> <type> Technical Report UMIACS-TR-93-77 & CS-TR-3118, </type> <institution> University of Maryland, </institution> <year> 1993. </year>
Reference-contexts: Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13]. In contrast to other approaches (such as SPEC [19] and the Neural Network Pushdown Automata <ref> [28] </ref>), CPP is not equipped with a stack. Secondly, in the classification problem, a single bit suffices to represent whether a string can be derived by the language or not. Similarly, for the prediction problem, it is only necessary to devise a unique representation for each terminal in the alphabet.
Reference: [29] <author> T. van Gelder. </author> <title> Compositionality: a connectionist variation on a classical theme. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 355-384, </pages> <year> 1990. </year>
Reference-contexts: They are structured, recursive and possibly infinite (since there is no prescribed maximum length for the sentences or maximum height for the parse trees). Thus to encode these data structures using fixed-length coding, distributed representations must be used and they have to be developed using functional composition techniques <ref> [29] </ref> (such as RAAM [22]). Needless to say, the overall performance of the parser will be intimately related to these representations. Unfortunately, encoding a recursive symbolic data structure, especially a hierarchical one (such as the parse tree), is by all means "hard" for neural networks.
Reference: [30] <author> T. van Gelder. </author> <title> Making conceptual space. </title> <editor> In S. Davis, editor, </editor> <booktitle> Connectionism: theory and practice, </booktitle> <pages> pages 179-194. </pages> <publisher> Oxford University Press, </publisher> <address> New York, </address> <year> 1992. </year>
Reference-contexts: In this way, a nested structure is observed in which the further the sub-division goes, the finer details of the structure of the parse trees that count. A representational space resembling the Conceptual Space as put forth by van Gelder in <ref> [30] </ref> has thus been successfully realized by CPP, which is not achievable by Prototype P1 (c.f. Figure 9). 6 Holistic Parsing as a Step-by-Step Rational Inference Holistic parsing as exemplified by CPP effectively encapsulates its underlying parsing mechanism in a "black box". <p> The resulting representational space exhibits a nested, hierachical and regular structure resembling the idea of Conceptual Space as proposed by van Gelder <ref> [30] </ref>. To have a better understanding of the underlying parsing mechanism of CPP, an analysis has been presented in which a finite state automata is constructed upon successful training of the network.
Reference: [31] <author> R. Watrous and G. Kuhn. </author> <title> Induction of finite state languages using second-order recurrent networks. </title> <editor> In J. Moody, S. Hanson, and R. Lippmann, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 4, </booktitle> <pages> pages 309-316, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This has the advantage that little knowledge has to be assumed for the actual, detailed parsing algorithm. Compared with other grammatical inference approaches, CPP is unique in several ways : * Syntactic parsing as addressed by CPP is significantly more difficult than the "classification prob lem" tackled by <ref> [8, 23, 31] </ref>, and the "prediction problem" attempted by [7, 18, 26]. Firstly, CPP aims at tackling context-free grammars instead of regular languages. In general, to 3 parse context-free languages, external memory is required [13].
Reference: [32] <author> S. Wermter. </author> <title> Hybrid Connectionist Natural Language Processing. </title> <publisher> Chapman & Hall, </publisher> <address> London, </address> <year> 1995. </year> <month> 22 </month>
Reference-contexts: When the processing of the subordinate clause is done, this information is recovered/popped from the Stack and the parsing of the superordinate clause continues. In the SCAN architecture (Symbolic/Connectionist Approach for Natural language phrases) <ref> [32] </ref>, a Plausibility Network (which is basically a feedforward network) is trained to evaluate the "likelihood" that a syntactic relationship can hold between two nouns (or noun groups) by using the words' semantic features. A chart parser then exploits this decision/information to resolve syntactic ambiguity.
References-found: 32

