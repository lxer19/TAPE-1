URL: http://ai.eecs.umich.edu/people/douglasp/pubs/book.ps
Refering-URL: http://ai.eecs.umich.edu/people/douglasp/pubs/book.html
Root-URL: 
Title: Correcting Imperfect Domain Theories: A Knowledge-Level Analysis  
Author: Scott B. Huffman Douglas J. Pearson John E. Laird 
Note: To appear in Machine Learning: Induction, Analogy and Discovery, edited by Susan Chip man and Alan Meyrowitz, Kluwer Academic Press, 1992. Sponsored in part by NASA and the Office of Naval Research under contract NCC 2-517, and by a University of Michigan Research Partnership Fellowship.  
Date: March 8, 1996  
Address: 1101 Beal Ave. Ann Arbor, Michigan 48109-2122  
Affiliation: Artificial Intelligence Laboratory The University of Michigan  
Abstract: Explanation-Based Learning [Mitchell et al., 1986; DeJong and Mooney, 1986] has shown promise as a powerful analytical learning technique. However, EBL is severely hampered by the requirement of a complete and correct domain theory for successful learning to occur. Clearly, in non-trivial domains, developing such a domain theory is a nearly impossible task. Therefore, much research has been devoted to understanding how an imperfect domain theory can be corrected and extended during system performance. In this paper, we present a characterization of this problem, and use it to analyze past research in the area. Past characterizations of the problem (e.g, [Mitchell et al., 1986; Rajamoney and DeJong, 1987]) have viewed the types of performance errors caused by a faulty domain theory as primary. In contrast, we focus primarily on the types of knowledge deficiencies present in the theory, and from these derive the types of performance errors that can result. Correcting the theory can be viewed as a search through the space of possible domain theories, with a variety of knowledge sources that can be used to guide the search. We examine the types of knowledge used by a variety of past systems for this purpose. The hope is that this analysis will indicate the need for a "universal weak method" of domain theory correction, in which different sources of knowledge for theory correction can be freely and flexibly combined. 
Abstract-found: 1
Intro-found: 1
Reference: [Ali, 1989] <author> Kamal M. Ali. </author> <title> Augmenting domain theory for explanation-based learning. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 40-42, </pages> <year> 1989. </year>
Reference: [Anderson, 1986] <author> J. R. Anderson. </author> <title> Knowledge compilation: The general learning mechanism. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, </booktitle> <volume> Volume II, </volume> <pages> pages 289-310. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: By analyzing the dependency structure of the explanation, the learning system can construct a generalized, operational rule which directly recognizes the training example as an instance of the goal concept. EBL has taken a variety of slightly different forms, such as knowledge compilation <ref> [Anderson, 1986] </ref>, chunking [Laird et al., 1986], operationalization [Mostow, 1981], and schema construction [Mooney, 1990], and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots [Mitchell, 1990; Laird and Rosenbloom, 1990].
Reference: [Bennett, 1987] <author> Scott W. Bennett. </author> <title> Approximation in mathematical domains. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 239-241, </pages> <month> August </month> <year> 1987. </year>
Reference: [Bennett, 1990] <author> Scott W. Bennett. </author> <title> Reducing real-world failures of approximate explanation-based rules. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 226-234, </pages> <year> 1990. </year>
Reference-contexts: The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state. This category includes typical planning tasks, such as planning a robot's actions (e.g., <ref> [Gupta, 1987; Laird et al., 1989; Bennett, 1990] </ref>), and standard concept recognition tasks (e.g., [Ourston and Mooney, 1990]), where the sequence of inferences needed to recognize an example of the concept is considered the "plan" that is generated. <p> Thus the analysis task is a more constrained version of the generation task. In addition, generation systems may require knowledge of how to actually carry out the operations in the external world. Many systems (e.g., <ref> [Bennett, 1990; Chien, 1989] </ref>) perform both types of problems. In the remainder of this paper we will cast our discussion from the point of view of generation tasks, but the analysis is equally applicable to analysis tasks.
Reference: [Bergadano and Giordana, 1988] <author> Francesco Bergadano and Attilio Giordana. </author> <title> A knowledge intensive approach to concept induction. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 305-317, </pages> <year> 1988. </year>
Reference: [Bhatnager and Mostow, 1990] <author> Neeraj Bhatnager and Jack Mostow. </author> <title> Adaptive search by explanation-based learning of heuristic censors. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 895-901, </pages> <year> 1990. </year>
Reference-contexts: Bennett [1987] deals with intractability in mathematical reasoning by approximating mathematical formulas. Doyle [1986] presents a system given a set of domain theories at multiple abstraction levels, which reasons with the most abstract first, and falls back on more and more concrete theories as failures arise. FAILSAFE-2 <ref> [Bhatnager and Mostow, 1990] </ref> learns overgeneral search heuristics during planning by assuming the most recent operator applied is to blame for search failures, but later specializes these heuristics when all search paths are eliminated.
Reference: [Carbonell and Gil, 1987] <author> Jaime G. Carbonell and Yolanda Gil. </author> <title> Learning by experimentation. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 256-265, </pages> <year> 1987. </year>
Reference-contexts: This means that some earlier operator either was expected to achieve the precondition but did not, or that some earlier operator was expected to preserve the precondition but clobbered it <ref> [Carbonell and Gil, 1987] </ref>. (b) Postconditions of the operator just executed are not achieved. The operator does not cause all of the effects that the planner predicted. Carbonell and Gil [1987] break this down further into two cases: either all postconditions are unachieved, or only some subset are unachieved.
Reference: [Chien, 1989] <author> Steve A. Chien. </author> <title> Using and refining simplifications: Explanation-based learning of plans in intractable domains. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 590-595, </pages> <year> 1989. </year>
Reference-contexts: The first type, what we will call analysis tasks, involve explaining or understanding some observed example. This category includes plan recognition [Mooney, 1990], in which the system tries to explain the actions of characters in a narrative, and apprenticeship learning systems (e.g., <ref> [Wilkins, 1988; Chien, 1989; VanLehn, 1987] </ref>), in which the system observes an expert carrying out some 1 task which it must learn to perform. The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state. <p> Thus the analysis task is a more constrained version of the generation task. In addition, generation systems may require knowledge of how to actually carry out the operations in the external world. Many systems (e.g., <ref> [Bennett, 1990; Chien, 1989] </ref>) perform both types of problems. In the remainder of this paper we will cast our discussion from the point of view of generation tasks, but the analysis is equally applicable to analysis tasks.
Reference: [Danyluk, 1987] <author> Andrea Pohoreckyj Danyluk. </author> <title> The use of explanations in similarity-based learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 274-276, </pages> <year> 1987. </year>
Reference: [Danyluk, 1989] <author> Andrea Pohoreckyj Danyluk. </author> <title> Finding new rules for incomplete theories: Explicit biases for induction with contextual information. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 34-36, </pages> <year> 1989. </year>
Reference: [Davies and Russell, 1987] <author> Todd R. Davies and Stuart J. Russell. </author> <title> A logical approach to reasoning by analogy. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 264-270, </pages> <year> 1987. </year> <month> 20 </month>
Reference-contexts: Genest et al. [1990] also use abduction to complete partial explanations, but do not extend the domain theory using the abductive conclusions. Determinations <ref> [Davies and Russell, 1987] </ref> are a type of internal knowledge that provide capabilities similar to abduction. A determination makes explicit the functional dependencies between attributes within the domain.
Reference: [DeJong and Mooney, 1986] <author> Gerald F. DeJong and Raymond J. Mooney. </author> <title> Explanation-based learning: An alternative view. </title> <journal> Machine Learning, </journal> <volume> 1(2) </volume> <pages> 145-176, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction Much recent research in machine learning has centered around analytical learning techniques. In particular, Explanation-Based Learning (EBL) <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref> has emerged as an important approach to using prior domain knowledge to improve performance.
Reference: [Doyle, 1986] <author> Richard J. Doyle. </author> <title> Constructing and refining causal explanations from an inconsistent domain theory. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 538-544, </pages> <year> 1986. </year>
Reference: [Ellman, 1988] <author> Thomas Ellman. </author> <title> Approximate theory formation: An explanation-based approach. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 570-574, </pages> <month> August </month> <year> 1988. </year>
Reference: [Firby, 1987] <author> R. James Firby. </author> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 202-206, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: Martin and Firby [1991] have begun developing a system that learns to correct execution-time failures by being told. The approach is a combination of Martin's DMAP parser [Martin, 1989] and Firby's RAPs system <ref> [Firby, 1987] </ref>. Upon an execution failure, the system takes advice in natural language indicating how to correctly perform the task. Comprehending the language input is aided by specific expectations which are set up by the failure.
Reference: [Flann and Dietterich, 1989] <author> Nicholas S. Flann and Thomas G. Dietterich. </author> <title> A study of explanation-based methods for inductive learning. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 187-226, </pages> <year> 1989. </year>
Reference: [Flann, 1990] <author> Nicholas S. Flann. </author> <title> Applying abstraction and simplification to learn in intractable domains. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 277-285, </pages> <year> 1990. </year>
Reference: [Genest et al., 1990] <author> Jean Genest, Stan Matwin, and Boris Plante. </author> <title> Explanation-based learning with incomplete theories: A three-step approach. </title> <booktitle> In Proceedings of the International Conference on Machine Learning, </booktitle> <pages> pages 286-294, </pages> <year> 1990. </year>
Reference: [Gil, 1991a] <author> Yolanda Gil. </author> <title> Acquiring domain knowledge for planning by experimentation. </title> <type> Thesis Proposal, </type> <institution> Carnegie Mellon University, School of Computer Science, </institution> <year> 1991. </year>
Reference: [Gil, 1991b] <author> Yolanda Gil. </author> <title> A domain-independent framework for effective experimentation in planning. </title> <booktitle> In Proceedings of the International Machine Learning Workshop, </booktitle> <pages> pages 13-17, </pages> <year> 1991. </year>
Reference: [Ginsberg and Smith, 1987] <author> Matthew L. Ginsberg and David E. Smith. </author> <title> Reasoning about action I: A possible worlds approach. </title> <editor> In Frank M. Brown, editor, </editor> <booktitle> The Frame Problem in Artificial Intelligence: Proceedings of the 1987 Workshop, </booktitle> <pages> pages 233-258. </pages> <publisher> Morgan Kauf-mann, </publisher> <year> 1987. </year>
Reference-contexts: This is due to the frame problem: the preconditions and effects of actions are extremely difficult to describe fully except in limited domains. For example, knowing the preconditions of actions in the real world is extremely difficult; this is known as the qualification problem <ref> [Ginsberg and Smith, 1987] </ref>. Exceptions can be generated almost ad infinitum. Consider an operator for going through a door. The most obvious precondition is that the door is opened.
Reference: [Gupta, 1987] <author> Ajay Gupta. </author> <title> Explanation-based failure recovery. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 606-610, </pages> <year> 1987. </year>
Reference-contexts: The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state. This category includes typical planning tasks, such as planning a robot's actions (e.g., <ref> [Gupta, 1987; Laird et al., 1989; Bennett, 1990] </ref>), and standard concept recognition tasks (e.g., [Ourston and Mooney, 1990]), where the sequence of inferences needed to recognize an example of the concept is considered the "plan" that is generated.
Reference: [Hall, 1986] <author> Robert J. Hall. </author> <title> Learning by failing to explain. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 568-572, </pages> <year> 1986. </year>
Reference-contexts: For analysis tasks, this corresponds to being unable to construct a complete explanation of the observed training example. Several systems attempt to construct a parse of training examples by working both bottom-up and top-down <ref> [VanLehn, 1987; Hall, 1986] </ref>. A failure to complete the parse represents an incomplete plan failure. (b) Multiple Inconsistent Plans Rajamoney and DeJong [1988] examine cases in which it is known that only one plan should be formed, but multiple plans are found. <p> Often these systems will form a maximal partial explanation using its incomplete domain theory. The parts of the example which are left unexplained can then be focussed on, to generate a set of hypotheses for the lack of knowledge within the domain theory <ref> [Pazzani, 1988; VanLehn, 1987; Hall, 1986] </ref>. Empirical techniques are often used to discriminate these hypotheses (discussed further below). In addition to the domain theory used to perform the task, the system may have other knowledge which it can use to analyze and understand its failure.
Reference: [Hammond, 1986] <author> Kristian J. Hammond. </author> <title> Learning to anticipate and avoid planning problems through the explanation of failures. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 556-560, </pages> <year> 1986. </year> <month> 21 </month>
Reference-contexts: Case-based approaches rely on storing complete past cases, and altering these cases to apply to new situations. CHEF <ref> [Hammond, 1986] </ref> uses a complete causal theory to explain and repair execution-time errors, but then stores the repaired plan in its case library, indexed under the type of error repaired.
Reference: [Holland, 1986] <author> John H. Holland. </author> <title> Escaping brittleness: The possibilities of general-purpose learning algorithms applied to parallel rule-based systems. </title> <editor> In R. S. Michalski, J. G. Car-bonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach, Volume II. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1986. </year>
Reference-contexts: This is a ubiquitous problem in machine learning. It is faced, for example, by concept learning programs, which must determine which combinations of a set of features determine category membership. It is also faced by reinforcement learning systems (e.g., <ref> [Sutton, 1990; Holland, 1986; Rumelhart and McClelland, 1986] </ref>) that learn which actions will achieve which effects in the world, by receiving positive feedback from the environment when some goal is met.
Reference: [Knoblock, 1990] <author> Craig A. Knoblock. </author> <title> Learning abstraction hierarchies for problem solving. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 923-928, </pages> <year> 1990. </year>
Reference: [Knoblock, 1991] <author> Craig A. Knoblock. </author> <title> Search reduction in hierarchical problem solving. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence, </booktitle> <pages> pages 686-691, </pages> <month> July </month> <year> 1991. </year>
Reference: [Kodratoff and Tecuci, 1987] <editor> Yves Kodratoff and Gheorghe Tecuci. DISCIPLE-1: </editor> <title> Interactive apprentice system in weak theory fields. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 271-273, </pages> <month> August </month> <year> 1987. </year>
Reference-contexts: The system uses some other knowledge or bias to generate hypotheses about how to fix the domain theory, and then these are validated or invalidated by the teacher. Examples of systems using this kind of passive teaching include <ref> [Kodratoff and Tecuci, 1987; Widmer, 1989] </ref>. A more interesting use of knowledge from a teacher are cases where the teacher can actively guide the domain theory repair process. Two main categories of teaching have been used. First, the teacher may provide examples to the system.
Reference: [Laird and Newell, 1983] <author> John E. Laird and Allen Newell. </author> <title> A universal weak method: Summary of results. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 771-773, </pages> <year> 1983. </year>
Reference-contexts: The ability to flexibly draw upon a combination of knowledge sources should greatly enhance a system's ability to improve its domain theory. Thus, our long-term goal can be viewed as the development of a "universal weak method" <ref> [Laird and Newell, 1983] </ref> for domain theory refinement. In this paper, we will present a framework that lays out the space of possible approaches to extending and correcting domain theories.
Reference: [Laird and Rosenbloom, 1990] <author> John E. Laird and Paul S. Rosenbloom. </author> <title> Integrating execution, planning, and learning in Soar for external environments. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1022-1029, </pages> <address> Boston, Mass., 1990. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: EBL has taken a variety of slightly different forms, such as knowledge compilation [Anderson, 1986], chunking [Laird et al., 1986], operationalization [Mostow, 1981], and schema construction [Mooney, 1990], and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots <ref> [Mitchell, 1990; Laird and Rosenbloom, 1990] </ref>. This technique is severely hampered, however, by the requirement of a complete and correct domain theory. Even for simple domains, a perfect domain theory is extremely difficult to construct. <p> This may result in overgeneralizations in some cases. Another example of using information given by a teacher to correct an imperfect domain theory is the Robo-Soar system <ref> [Laird and Rosenbloom, 1990; Laird et al., 1990] </ref>.
Reference: [Laird et al., 1986] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Chunking in Soar: The anatomy of a general learning mechanism. </title> <journal> Maching Learning, </journal> <volume> 1(1) </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: By analyzing the dependency structure of the explanation, the learning system can construct a generalized, operational rule which directly recognizes the training example as an instance of the goal concept. EBL has taken a variety of slightly different forms, such as knowledge compilation [Anderson, 1986], chunking <ref> [Laird et al., 1986] </ref>, operationalization [Mostow, 1981], and schema construction [Mooney, 1990], and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots [Mitchell, 1990; Laird and Rosenbloom, 1990].
Reference: [Laird et al., 1987] <author> John E. Laird, Allen Newell, and Paul S. Rosenbloom. </author> <title> Soar: An architecture for general intelligence. </title> <journal> Artificial Intelligence, </journal> <volume> 33(1) </volume> <pages> 1-64, </pages> <year> 1987. </year>
Reference-contexts: EBL systems that have learned search control (e.g., PRODIGY [Minton et al., 1989] and Soar <ref> [Laird et al., 1987] </ref>) did not extend or correct domain theories by doing so. However, if resource constraints can affect the correctness of the solution, then the solution may not be independent of the search process.
Reference: [Laird et al., 1989] <author> John E. Laird, Eric S. Yager, Christopher M. Tuck, and Michael Hucka. </author> <title> Learning in tele-autonomous systems using Soar. </title> <booktitle> In Proceedings of the NASA Conference on Space Telerobotics, </booktitle> <year> 1989. </year>
Reference-contexts: The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state. This category includes typical planning tasks, such as planning a robot's actions (e.g., <ref> [Gupta, 1987; Laird et al., 1989; Bennett, 1990] </ref>), and standard concept recognition tasks (e.g., [Ourston and Mooney, 1990]), where the sequence of inferences needed to recognize an example of the concept is considered the "plan" that is generated.
Reference: [Laird et al., 1990] <author> John E. Laird, Michael Hucka, Eric S. Yager, and Christopher M. Tuck. </author> <title> Correcting and extending domain knowledge using outside guidance. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 235-241, </pages> <year> 1990. </year>
Reference-contexts: This may result in overgeneralizations in some cases. Another example of using information given by a teacher to correct an imperfect domain theory is the Robo-Soar system <ref> [Laird and Rosenbloom, 1990; Laird et al., 1990] </ref>.
Reference: [Mahadevan, 1989] <author> Sridhar Mahadevan. </author> <title> Using determinations in EBL: A solution to the incomplete theory problem. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 320-325, </pages> <year> 1989. </year>
Reference: [Martin and Firby, 1991] <author> Charles E. Martin and R. James Firby. </author> <title> Generating natural language expectations from a reactive execution system. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 811-815, </pages> <month> August </month> <year> 1991. </year>
Reference: [Martin, 1989] <author> Charles E. Martin. </author> <title> Pragmatic interpretation and ambiguity. </title> <booktitle> In Proceedings of the Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 474-481, </pages> <month> August </month> <year> 1989. </year> <month> 22 </month>
Reference-contexts: However, advice was not used to correct imperfections in the tractable domain theory (the one used to actually play the game). Martin and Firby [1991] have begun developing a system that learns to correct execution-time failures by being told. The approach is a combination of Martin's DMAP parser <ref> [Martin, 1989] </ref> and Firby's RAPs system [Firby, 1987]. Upon an execution failure, the system takes advice in natural language indicating how to correctly perform the task. Comprehending the language input is aided by specific expectations which are set up by the failure.
Reference: [Minton et al., 1989] <author> Steven Minton, Jaime G. Carbonell, Craig A. Knoblock, Daniel R. Kuokka, Oren Etzioni, and Yolanda Gil. </author> <title> Explanation-based learning: A problem-solving perspective. </title> <journal> Artificial Intelligence, </journal> <volume> 40 </volume> <pages> 63-118, </pages> <year> 1989. </year>
Reference-contexts: If a system has no search control, or incorrect search control, it will have to explore more of the search space to solve the problem, but 2 the lack of search control will not preclude solving the problem. EBL systems that have learned search control (e.g., PRODIGY <ref> [Minton et al., 1989] </ref> and Soar [Laird et al., 1987]) did not extend or correct domain theories by doing so. However, if resource constraints can affect the correctness of the solution, then the solution may not be independent of the search process. <p> Note that correcting a domain theory in response to failures is different from learning search heuristics from search failures, which can be done using standard EBL methods and does not alter the domain theory <ref> [Mostow and Bhatnager, 1987; Minton et al., 1989] </ref>. 3.1 Types of Domain Theory Imperfections We have cast the problem facing an EBL system as that of finding a sequence of operators. Therefore, the domain theory consists of the system's knowledge of the preconditions and postconditions of its operators.
Reference: [Mitchell et al., 1986] <author> Tom M. Mitchell, R. M. Keller, and S. T. Kedar-Cabelli. </author> <title> Explanation-based generalization: A unifying view. </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <year> 1986. </year>
Reference-contexts: 1 Introduction Much recent research in machine learning has centered around analytical learning techniques. In particular, Explanation-Based Learning (EBL) <ref> [Mitchell et al., 1986; DeJong and Mooney, 1986] </ref> has emerged as an important approach to using prior domain knowledge to improve performance.
Reference: [Mitchell, 1982] <author> Tom M. Mitchell. </author> <title> Generalization as search. </title> <journal> Artificial Intelligence, </journal> <volume> 18 </volume> <pages> 203-226, </pages> <year> 1982. </year>
Reference-contexts: Rather, the major source of corrective knowledge here is a set of randomly chosen examples. The systems which rely on this type of knowledge typically make use of inductive techniques (such as version spaces <ref> [Mitchell, 1982] </ref> or information-theory approaches [Quinlan, 1986]) to determine a set of changes to the domain theory that will be consistent with all (or most) examples.
Reference: [Mitchell, 1990] <author> Tom M. Mitchell. </author> <title> Becoming increasingly reactive. </title> <booktitle> In Proceedings of the Eighth National Conference on Artificial Intelligence, </booktitle> <pages> pages 1051-1058, </pages> <address> Boston, Mass., 1990. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: EBL has taken a variety of slightly different forms, such as knowledge compilation [Anderson, 1986], chunking [Laird et al., 1986], operationalization [Mostow, 1981], and schema construction [Mooney, 1990], and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots <ref> [Mitchell, 1990; Laird and Rosenbloom, 1990] </ref>. This technique is severely hampered, however, by the requirement of a complete and correct domain theory. Even for simple domains, a perfect domain theory is extremely difficult to construct.
Reference: [Mooney, 1990] <author> Raymond J. Mooney. </author> <title> Learning plan schemata from observation: Explanation-based learning for plan recognition. </title> <journal> Cognitive Science, </journal> <volume> 14 </volume> <pages> 483-509, </pages> <year> 1990. </year>
Reference-contexts: EBL has taken a variety of slightly different forms, such as knowledge compilation [Anderson, 1986], chunking [Laird et al., 1986], operationalization [Mostow, 1981], and schema construction <ref> [Mooney, 1990] </ref>, and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots [Mitchell, 1990; Laird and Rosenbloom, 1990]. This technique is severely hampered, however, by the requirement of a complete and correct domain theory. <p> The first type, what we will call analysis tasks, involve explaining or understanding some observed example. This category includes plan recognition <ref> [Mooney, 1990] </ref>, in which the system tries to explain the actions of characters in a narrative, and apprenticeship learning systems (e.g., [Wilkins, 1988; Chien, 1989; VanLehn, 1987]), in which the system observes an expert carrying out some 1 task which it must learn to perform.
Reference: [Mostow and Bhatnager, 1987] <author> Jack Mostow and Neeraj Bhatnager. </author> <title> Failsafe a floor planner that uses EBG to learn from its failures. </title> <booktitle> In Proceedings of IJCAI-87, </booktitle> <pages> pages 249-255, </pages> <year> 1987. </year>
Reference-contexts: Note that correcting a domain theory in response to failures is different from learning search heuristics from search failures, which can be done using standard EBL methods and does not alter the domain theory <ref> [Mostow and Bhatnager, 1987; Minton et al., 1989] </ref>. 3.1 Types of Domain Theory Imperfections We have cast the problem facing an EBL system as that of finding a sequence of operators. Therefore, the domain theory consists of the system's knowledge of the preconditions and postconditions of its operators.
Reference: [Mostow, 1981] <author> David J. Mostow. </author> <title> Mechanical transformation of task heurisitics into operational procedures. </title> <type> PhD thesis, </type> <institution> Carnegie-Mellon University, Department of Computer Science, </institution> <month> April </month> <year> 1981. </year>
Reference-contexts: EBL has taken a variety of slightly different forms, such as knowledge compilation [Anderson, 1986], chunking [Laird et al., 1986], operationalization <ref> [Mostow, 1981] </ref>, and schema construction [Mooney, 1990], and has proven useful in domains from recognizing simple concepts to learning reactive plans for mobile robots [Mitchell, 1990; Laird and Rosenbloom, 1990]. This technique is severely hampered, however, by the requirement of a complete and correct domain theory.
Reference: [Mostow, 1983] <author> D. J. Mostow. </author> <title> Learning by being told: Machine transformation of advice into a heuristic search procedure. </title> <editor> In R. S. Michalski, J. G. Carbonell, and T. M. Mitchell, editors, </editor> <booktitle> Machine Learning: An artificial intelligence approach. </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1983. </year>
Reference: [Oblinger and DeJong, 1991] <author> Daniel Oblinger and Gerald DeJong. </author> <title> An alternative to deduction. </title> <booktitle> In Proceedings of the Thirteenth Annual Conference of the Cognitive Science Society, </booktitle> <pages> pages 837-841, </pages> <year> 1991. </year>
Reference: [Ourston and Mooney, 1990] <author> Dirk Ourston and Raymond J. Mooney. </author> <title> Changing the rules: A comprehensive approach to theory refinement. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 815-820, </pages> <year> 1990. </year>
Reference-contexts: This category includes typical planning tasks, such as planning a robot's actions (e.g., [Gupta, 1987; Laird et al., 1989; Bennett, 1990]), and standard concept recognition tasks (e.g., <ref> [Ourston and Mooney, 1990] </ref>), where the sequence of inferences needed to recognize an example of the concept is considered the "plan" that is generated. These two types of tasks are similar, in that they require similar types of knowledge to be performed.
Reference: [Pazzani et al., 1987] <author> Michael Pazzani, Michael Dyer, and Margot Flowers. </author> <title> Using prior learning to facilitate the learning of new causal theories. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 277-279, </pages> <year> 1987. </year>
Reference-contexts: If there are multiple conditions that imply darkness, we may have to use additional knowledge to choose between them. Thus abduction can be viewed as a way of limiting the possible theory revisions being considered. OCCAM <ref> [Pazzani, 1989; Pazzani et al., 1987] </ref> uses an abductive method to fill in missing knowledge in its domain theory. When some aspect of an observation cannot be explained, OCCAM looks for a domain theory rule that contains that aspect as its result.
Reference: [Pazzani, 1988] <author> Michael J. Pazzani. </author> <title> Integrated learning with incorrect and incomplete theories. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 291-297, </pages> <year> 1988. </year>
Reference-contexts: Often these systems will form a maximal partial explanation using its incomplete domain theory. The parts of the example which are left unexplained can then be focussed on, to generate a set of hypotheses for the lack of knowledge within the domain theory <ref> [Pazzani, 1988; VanLehn, 1987; Hall, 1986] </ref>. Empirical techniques are often used to discriminate these hypotheses (discussed further below). In addition to the domain theory used to perform the task, the system may have other knowledge which it can use to analyze and understand its failure. <p> This technique thus learns a "missing operator" anew rule that the system didn't have before. The approach is similar to VanLehn's technique of intersecting partial explanations, but here there are no assumptions about the form of the examples. Thus the rules formed are sometimes faulty. OCCAM <ref> [Pazzani, 1988] </ref> also uses an empirical approach to learn missing domain theory rules. Part of the knowledge OCCAM uses to build explanations is a very general set of rules describing causal relationships. These rules derive an abstract "explanation pattern" that is then verified by more specific domain knowledge.
Reference: [Pazzani, 1989] <author> Michael Pazzani. </author> <title> Detecting and correcting errors of omission after explanation-based learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 713-718, </pages> <year> 1989. </year>
Reference-contexts: If there are multiple conditions that imply darkness, we may have to use additional knowledge to choose between them. Thus abduction can be viewed as a way of limiting the possible theory revisions being considered. OCCAM <ref> [Pazzani, 1989; Pazzani et al., 1987] </ref> uses an abductive method to fill in missing knowledge in its domain theory. When some aspect of an observation cannot be explained, OCCAM looks for a domain theory rule that contains that aspect as its result.
Reference: [Quinlan, 1986] <author> J. R. Quinlan. </author> <title> Induction of decision trees. </title> <journal> Machine Learning, </journal> <volume> 1(1) </volume> <pages> 81-106, </pages> <year> 1986. </year>
Reference-contexts: Rather, the major source of corrective knowledge here is a set of randomly chosen examples. The systems which rely on this type of knowledge typically make use of inductive techniques (such as version spaces [Mitchell, 1982] or information-theory approaches <ref> [Quinlan, 1986] </ref>) to determine a set of changes to the domain theory that will be consistent with all (or most) examples. <p> Since the discriminating knowledge that can be gleaned from each example is less concentrated than for experimentation approaches, more examples are required for these systems to correct a faulty domain theory. Pure inductive systems, such as ID3 <ref> [Quinlan, 1986] </ref>, can be viewed as learning an entire domain theory from scratch. This is the degenerate case of an "incomplete domain theory." In this paper, the focus is on correcting an already existing domain theory that has proven faulty.
Reference: [Rajamoney and DeJong, 1987] <author> Shankar Rajamoney and Gerald DeJong. </author> <title> The classification, detection and handling of imperfect theory problems. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 205-207, </pages> <year> 1987. </year>
Reference: [Rajamoney and DeJong, 1988] <author> Shankar A. Rajamoney and Gerald F. DeJong. </author> <title> Active explanation reduction: An approach to the multiple explanations problem. </title> <booktitle> In Proceedings of the International Machine Learning Conference, </booktitle> <pages> pages 242-255, </pages> <year> 1988. </year>
Reference: [Redmond, 1989] <author> Michael Redmond. </author> <title> Combining case-based reasoning, explanation-based learning and learning from instruction. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 20-22, </pages> <year> 1989. </year>
Reference: [Roy and Mostow, 1988] <author> Subrata Roy and Jack Mostow. </author> <title> Parsing to learn fine grained rules. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 547-551, </pages> <year> 1988. </year>
Reference: [Rumelhart and McClelland, 1986] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This is a ubiquitous problem in machine learning. It is faced, for example, by concept learning programs, which must determine which combinations of a set of features determine category membership. It is also faced by reinforcement learning systems (e.g., <ref> [Sutton, 1990; Holland, 1986; Rumelhart and McClelland, 1986] </ref>) that learn which actions will achieve which effects in the world, by receiving positive feedback from the environment when some goal is met.
Reference: [Sacerdoti, 1974] <author> Earl D. Sacerdoti. </author> <title> Planning in a hierarchy of abstraction spaces. </title> <journal> Artificial Intelligence, </journal> <volume> 5(2) </volume> <pages> 115-135, </pages> <year> 1974. </year>
Reference-contexts: Flann [1990] approximates a domain theory by assuming a limited number of objects will be present. Other work on approximating a domain theory to make it tractable to reason with has appeared under a different guise, namely abstraction planning. ABSTRIPS <ref> [Sacerdoti, 1974] </ref> is the classic example, showing that search could be greatly reduced by abstracting out preconditions of operators during planning.
Reference: [Sutton, 1990] <author> Richard S. Sutton. </author> <title> Integrated architectures for learning, planning and reacting based on approximating dynamic programming. </title> <booktitle> In Proceedings of the Seventh International Conference on Machine Learning, </booktitle> <pages> pages 216-224, </pages> <year> 1990. </year>
Reference-contexts: This is a ubiquitous problem in machine learning. It is faced, for example, by concept learning programs, which must determine which combinations of a set of features determine category membership. It is also faced by reinforcement learning systems (e.g., <ref> [Sutton, 1990; Holland, 1986; Rumelhart and McClelland, 1986] </ref>) that learn which actions will achieve which effects in the world, by receiving positive feedback from the environment when some goal is met.
Reference: [Tadepalli, 1989] <author> Prasad Tadepalli. </author> <title> Lazy explanation-based learning: A solution to the intractable theory problem. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 694-700, </pages> <year> 1989. </year>
Reference: [Towell et al., 1990] <author> Geoffrey G. Towell, Jude W. Shavlik, and Michiel O. Noordewier. </author> <title> Refinement of approximate domain theories by knowledge-based neural networks. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 861-866, </pages> <year> 1990. </year>
Reference: [Unruh and Rosenbloom, 1989] <author> Amy Unruh and Paul S. Rosenbloom. </author> <title> Abstraction in problem solving and learning. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1989. </year>
Reference: [VanLehn, 1987] <author> Kurt VanLehn. </author> <title> Learning one subprocedure per lesson. </title> <journal> Artificial Intelligence, </journal> <volume> 31(1) </volume> <pages> 1-40, </pages> <year> 1987. </year>
Reference-contexts: The first type, what we will call analysis tasks, involve explaining or understanding some observed example. This category includes plan recognition [Mooney, 1990], in which the system tries to explain the actions of characters in a narrative, and apprenticeship learning systems (e.g., <ref> [Wilkins, 1988; Chien, 1989; VanLehn, 1987] </ref>), in which the system observes an expert carrying out some 1 task which it must learn to perform. The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state. <p> For analysis tasks, this corresponds to being unable to construct a complete explanation of the observed training example. Several systems attempt to construct a parse of training examples by working both bottom-up and top-down <ref> [VanLehn, 1987; Hall, 1986] </ref>. A failure to complete the parse represents an incomplete plan failure. (b) Multiple Inconsistent Plans Rajamoney and DeJong [1988] examine cases in which it is known that only one plan should be formed, but multiple plans are found. <p> Often these systems will form a maximal partial explanation using its incomplete domain theory. The parts of the example which are left unexplained can then be focussed on, to generate a set of hypotheses for the lack of knowledge within the domain theory <ref> [Pazzani, 1988; VanLehn, 1987; Hall, 1986] </ref>. Empirical techniques are often used to discriminate these hypotheses (discussed further below). In addition to the domain theory used to perform the task, the system may have other knowledge which it can use to analyze and understand its failure. <p> In some systems, assumptions about the structure of the examples provides a strong bias that allows the system to solve the credit assignment and revision problems. One such system is VanLehn's SIERRA <ref> [VanLehn, 1987] </ref>. SIERRA learns to perform multi-column subtraction by explaining examples of worked out problems given by a teacher. SIERRA receives its examples organized into groups called lessons.
Reference: [Widmer, 1989] <author> Gerhard Widmer. </author> <title> A tight integration of deductive and inductive learning. </title> <booktitle> In Proceedings of the International Workshop on Machine Learning, </booktitle> <pages> pages 11-13, </pages> <year> 1989. </year>
Reference-contexts: The system uses some other knowledge or bias to generate hypotheses about how to fix the domain theory, and then these are validated or invalidated by the teacher. Examples of systems using this kind of passive teaching include <ref> [Kodratoff and Tecuci, 1987; Widmer, 1989] </ref>. A more interesting use of knowledge from a teacher are cases where the teacher can actively guide the domain theory repair process. Two main categories of teaching have been used. First, the teacher may provide examples to the system.
Reference: [Wilkins, 1988] <author> David C. Wilkins. </author> <title> Knowledge base refinement using apprenticeship learning techniques. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 646-651, </pages> <year> 1988. </year>
Reference-contexts: The first type, what we will call analysis tasks, involve explaining or understanding some observed example. This category includes plan recognition [Mooney, 1990], in which the system tries to explain the actions of characters in a narrative, and apprenticeship learning systems (e.g., <ref> [Wilkins, 1988; Chien, 1989; VanLehn, 1987] </ref>), in which the system observes an expert carrying out some 1 task which it must learn to perform. The second type, what we will call generation tasks, involve constructing (as opposed to observing) a plan to reach some goal from an initial state.
Reference: [Winston et al., 1983] <author> P. H. Winston, T. O. Binford, B. Katz, and M. Lowry. </author> <title> Learning physical descriptions from functional descriptions, examples, and precedents. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence, </booktitle> <pages> pages 433-439, </pages> <year> 1983. </year> <month> 24 </month>
Reference-contexts: Given this characterization of the problem, the knowledge that comprises the system's domain theory becomes clear. Simply, the system's knowledge of operators their preconditions and effects makes up the domain theory. In pure inference tasks, such as the cups domain <ref> [Winston et al., 1983] </ref>, each inference can be viewed as an operator. Note that this simple definition of the domain theory is a result of how we have cast the problem.
References-found: 65

