URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/93-01.ps.Z
Refering-URL: ftp://ftp.cs.buffalo.edu/pub/tech-reports/README.html
Root-URL: 
Email: email: sarnath@cs.buffalo.edu  
Phone: Phone (716)  
Title: Doubly logarithmic time parallel sorting  
Author: R.Sarnath 
Keyword: Parallel comparison sorting, Doubly logarithmic time, Constant time maximum  
Address: 226, Bell Hall  Amherst NY 14260  645-3180  
Affiliation: Dept. of Computer Science  SUNY at Buffalo  
Abstract: Recently, attempts have been made to separate the problem of parallel sorting from that of list ranking, in order to get around the well known (log n= log log n) lower bound. These approaches have been of two kinds chain sorting and padded sorting. Here we present nearly optimal, comparison based padded sorting algorithms that run in average case time O( 1 * 2 + * log log n) using n 1+* processors, and O(n 1+* ) space, on an Common CRCW PRAM.From these results, algorithms for chain sorting within the same time and processor bounds can be easily obtained. Using a similar approach, we also give an O(1) average case time, comparison based algorithm for finding the largest of n items using a linear number of processors. The algorithm for finding the maximum, runs on a Common CRCW PRAM using only n 3=4 cells of shared memory. Finally, we obtain randomised algorithms for these problems that run on Common/Tolerant CRCW PRAMs, and also satisfy the above time and processor bounds. As a consequence of our results on sorting, we can also show that the problem of sorting arbitrary input sequences can be reduced to that of sorting integer inputs, within the above mentioned time and processor bounds. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Ajtai M., Komlos J. and Szemeredi E., </author> <title> An O(n log n) sorting network. </title> <journal> Combinatorica, </journal> <volume> 3 </volume> <pages> 1-19, </pages> <year> 1983. </year>
Reference-contexts: Considerable attention has been paid to the problem of parallel sorting on shared memory machines. On the EREW PRAM, Cole's parallel mergesort [8] acheived fi (log n) worst case time using n processors. Leighton's modification [11] to the AKS sorting network <ref> [1] </ref>, acheived the same time and processor bounds. Rajashekaran and Reif [13] give a randomized fi (log n= log log n) time, n log * n processor algorithm for general sorting. <p> Thus, sorting in time polynomial in log log n using polynomially many processors is impossible. Recently, attempts have been made to separate the list ranking problem from sorting, thus accomplishing a "chain-sort" [10]. Among other results, Hagerup [10] has succeeded in sorting n integers in the range <ref> [1; n] </ref> into a linked list in O (1) expected time using O (n log n= log log n) processors and in O (log log n log ? n= log log log n) expected time using an optimal number of processors.
Reference: [2] <author> Alon N. and Azar Y., </author> <title> The average case complexity of deterministic and randomized parallel comparison sorting algorithms. </title> <journal> SIAM J Comput., </journal> <volume> 17, </volume> <year> 1988, </year> <pages> 1178-1192. </pages>
Reference-contexts: This can be done in O (1) time using n processors on a Common CRCW PRAM in the obvious way. There is a well known result due to Alon and Azar <ref> [2] </ref> on sorting using the Parallel Comparison Tree (PCT) model.
Reference: [3] <author> Alon N. and Megiddo N., </author> <title> Parallel linear programming in fixed dimension almost surely in constant time. </title> <booktitle> Proc. 31st annual IEEE FOCS, </booktitle> <pages> pp 574-582, </pages> <year> 1990. </year>
Reference-contexts: He gives an O (1) expected depth parallel decision tree for selection using the idea of multi-point sampling, but does not discuss any PRAM algorithms. An algorithm that finds the maximum in O (1) expected time on a Priority CRCW PRAM has been given by Alon and Megiddo in <ref> [3] </ref>.
Reference: [4] <author> Beame P. and Hastad J., </author> <title> Optimal bounds for decision problems on the CRCW PRAM. </title> <journal> J of the ACM, </journal> <volume> 36(3) </volume> <pages> 643-670, </pages> <month> July </month> <year> 1989. </year>
Reference-contexts: Very fast parallel algorithms have been developed for several problems. We characterise very fast algorithms as those that run in time polynomial in log log n using a polynomial number of processors. Some examples of these can be found in [5,6,12,15]. It has been shown by Beame and Hastad <ref> [4] </ref> that sorting n items using polynomially many processors requires (log n= log log n) time. Thus, sorting in time polynomial in log log n using polynomially many processors is impossible. Recently, attempts have been made to separate the list ranking problem from sorting, thus accomplishing a "chain-sort" [10]. <p> The only obvious bound we can place on the size of the final array, is that it cannot be less than n + polylog (n), since we can then compress and eliminate the duplicates to get a conventional sort, thus beating the lower bound in <ref> [4] </ref>. It 11 would also be an interesting open question to find a deterministic padded sorting algorithm, that has worst case running time better than (log n= log log n).
Reference: [5] <author> Berkman O., Breslauer D., Galil Z., Schieber B., Vishkin U., </author> <title> Highly parallelizable problems (extended abstract). </title> <booktitle> Proc. 21st annual ACM STOC, </booktitle> <pages> pp 309, </pages> <year> 1989. </year>
Reference: [6] <author> Borodin A. and Hopcroft J.E., </author> <title> Routing merging and sorting on parallel models of computation, </title> <journal> J of Comput. Syst. Sci., </journal> <volume> 30, </volume> <year> 1985, </year> <pages> pp 130-145. </pages>
Reference-contexts: Proof: For each partition S, do the following: Divide the array into n *=2 blocks. Chain together the blocks that contain an element of S, and count them. The chaining can be done in time O (log log n) using n *=2 processors <ref> [6] </ref>. We can now assign n *=2 processors to each such block and continue the process. This process terminates when the block size drops to n *=2 i.e., in no more than 2=* steps, each step requiring time O (log log n).
Reference: [7] <author> Chlebus B.S., Diks K., Hagerup T., and Radzik T., </author> <title> New Simulations between CRCW PRAMs, </title> <booktitle> in Proc 7th Int'l Conf. on Fundamentals of Computation Theory, Springer lecture notes in computer science, </booktitle> <volume> vol 380, </volume> <pages> pp 95-104. </pages>
Reference-contexts: The randomised algorithms run on Tolerant/Common CRCW PRAMs. We shall adopt the definitions of these models, as given by Chlebus et al. <ref> [7] </ref> Throughout the rest of the paper, we shall assume that all the elements are distinct. This does not result in any loss of generality in the analysis or implementation of our algorithms.
Reference: [8] <author> Cole R., </author> <title> Parallel merge sort. </title> <booktitle> Proc. 27th annual IEEE FOCS, </booktitle> <pages> pp 511-516, </pages> <year> 1986. </year>
Reference-contexts: 1 Introduction The problem of sorting a sequence of numbers is well known. Considerable attention has been paid to the problem of parallel sorting on shared memory machines. On the EREW PRAM, Cole's parallel mergesort <ref> [8] </ref> acheived fi (log n) worst case time using n processors. Leighton's modification [11] to the AKS sorting network [1], acheived the same time and processor bounds. Rajashekaran and Reif [13] give a randomized fi (log n= log log n) time, n log * n processor algorithm for general sorting. <p> If we sort these pivots and the partitions separately, we can then put them together to get a sorted array. The sorting of pivots is done by recursive calls. We can sort the small partitions, using the algorithm in <ref> [8] </ref>, in O (log log n) time. However, we must also ensure that the size of the input to the recursive calls is no more than O ( p n), so that the recursion depth is no more than O (log log n). <p> array of size n; A fl is the output produced; we divide A into p n contiguous, equal sized blocks; every cell of A need not contain an element, i.e, A has n cells, and atmost n elements *) IF n &lt; ffi, THEN sort A using the algorithm in <ref> [8] </ref> ELSE, * Step 1: Pick the leftmost element of each block to be a pivot; partition the elements of A using these (atmost) p n pivots. Let p j denote the pivot picked from the j-th block. <p> However, this is detected within our claimed resource bounds, as explained in section 3.1. If this occurs, we abort the exercise, and sort the input using the algorithm in <ref> [8] </ref>. Since the probability of this event is very small, our average case claim holds. We shall now show how all these steps can be carried out in O (1) time using n 2 processors. To begin with, we make the following observation about the recursive steps in our algorithm. <p> Since there are no more than 2n 2 partitions being created in the entire course of the algorithm, the combined failure probability is O (n c log e=2+2 ). In case of failure, we can resort to the algorithm in <ref> [8] </ref>, to sort in O (log n) time, giving us an average case time of O (log log n). 2 8 4 An n processor algorithm Algorithm Efficient-sort Divide the array into n *=2 blocks, each of size n 1*=2 .
Reference: [9] <author> Cole R. and Vishkin U., </author> <title> Approximate and exact parallel scheduling, with applications to list, tree and graph problems. </title> <booktitle> Proc. 27th annual IEEE FOCS, </booktitle> <pages> pp 478-491, </pages> <year> 1986. </year>
Reference: [10] <author> Hagerup T., </author> <title> Fast parallel space allocation, estimation and integer sorting, </title> <type> tech rep MPI-I-91-106, </type> <institution> Max Planck Institut fur Informatik, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: Thus, sorting in time polynomial in log log n using polynomially many processors is impossible. Recently, attempts have been made to separate the list ranking problem from sorting, thus accomplishing a "chain-sort" <ref> [10] </ref>. Among other results, Hagerup [10] has succeeded in sorting n integers in the range [1; n] into a linked list in O (1) expected time using O (n log n= log log n) processors and in O (log log n log ? n= log log log n) expected time using <p> Thus, sorting in time polynomial in log log n using polynomially many processors is impossible. Recently, attempts have been made to separate the list ranking problem from sorting, thus accomplishing a "chain-sort" <ref> [10] </ref>. Among other results, Hagerup [10] has succeeded in sorting n integers in the range [1; n] into a linked list in O (1) expected time using O (n log n= log log n) processors and in O (log log n log ? n= log log log n) expected time using an optimal number of processors.
Reference: [11] <author> Leighton T., </author> <title> Tight bounds on the complexity of parallel sorting. </title> <journal> IEEE trans. on computers, </journal> <volume> c-34:344-354, </volume> <month> April </month> <year> 1985. </year>
Reference-contexts: 1 Introduction The problem of sorting a sequence of numbers is well known. Considerable attention has been paid to the problem of parallel sorting on shared memory machines. On the EREW PRAM, Cole's parallel mergesort [8] acheived fi (log n) worst case time using n processors. Leighton's modification <ref> [11] </ref> to the AKS sorting network [1], acheived the same time and processor bounds. Rajashekaran and Reif [13] give a randomized fi (log n= log log n) time, n log * n processor algorithm for general sorting.
Reference: [12] <author> Mackenzie P. and Stout Q., </author> <title> Ultra-fast expected time parallel algorithms. </title> <booktitle> Proc. of symposium on discrete algorithms, </booktitle> <pages> 414-424, </pages> <year> 1991. </year>
Reference-contexts: Another attempt to sidestep this lowerbound is by doing a padded sort sorting into a larger array <ref> [12] </ref>. <p> Since many problems rely simply on the relative ordering information and not the actual rank, a padded sort may be as useful as a conventional sort. For instance, padded sorting is useful in a situation where the ordering is primarily needed for fast searching. The results in <ref> [12] </ref> are for uniformly distributed inputs, i.e, any item in the specified range is equally likely to appear in the input array. Their ideas for padded sorting have also found applications in solving some problems in computational geometry.
Reference: [13] <author> Rajasekaran S. and Reif J.H., </author> <title> Optimal and sublogarithmic time randomized parallel sorting algorithms. </title> <journal> SIAM J of computing, </journal> <volume> 18(3) </volume> <pages> 594-607, </pages> <month> June </month> <year> 1989. </year> <month> 12 </month>
Reference-contexts: On the EREW PRAM, Cole's parallel mergesort [8] acheived fi (log n) worst case time using n processors. Leighton's modification [11] to the AKS sorting network [1], acheived the same time and processor bounds. Rajashekaran and Reif <ref> [13] </ref> give a randomized fi (log n= log log n) time, n log * n processor algorithm for general sorting. They also have a randomized algorithm for integer sorting which runs in expected time fi (log n) and uses n= log n processors.
Reference: [14] <author> Reischuk R., </author> <title> Probabilistic parallel algorithms for sorting and selection, </title> <journal> SIAM J of comput, </journal> <volume> vol 14, no. 2, </volume> <month> May </month> <year> 1985. </year>
Reference-contexts: Their ideas for padded sorting have also found applications in solving some problems in computational geometry. They have acheived an expected time bound of O (log log n) using n= log log n processors. These algorithms however, are not comparison based. Reischuk <ref> [14] </ref> has some results on Parallel Decision Trees for selection. He gives an O (1) expected depth parallel decision tree for selection using the idea of multi-point sampling, but does not discuss any PRAM algorithms.
Reference: [15] <author> Shiloach Y. and Vishkin U., </author> <title> Finding the maximum, merging, and sorting in a parallel computation model, </title> <journal> J. Algorithms, </journal> <volume> vol. 2, </volume> <year> 1981, </year> <pages> pp 88. </pages>
Reference-contexts: execution of the while loop of the algorithm can be carried out in O (1) time on a Common CRCW PRAM using n 3=4 cells of shared memory. 3 Proof: Finding the maximum of p n elememts can be done in O (1) time using n processors as described in <ref> [15] </ref>. To pick the pivots, we do the following: Divide each block into n 1=4 equal sized sub-blocks. Using a single concurrent write operation, we determine which of these sub-blocks contains a candidate. <p> Using a single concurrent write operation, we determine which of these sub-blocks contains a candidate. We can then find the leftmost candidate of the leftmost such sub-block in 2 applications of the algorithm in <ref> [15] </ref>. Note that this is possible since we have p n processors, and only n 1=4 sub-blocks with n 1=4 items per sub-block. 2 We prove next, that in atmost 6 executions of the while loop, we would have found the maximum with high probability (HP). <p> We choose n 1=2 i+1 pivots to partition each subproblem; this implies that we have to compute the minimum of n 1=2 i+1 differences for each element. This minimum can be computed in O (1) time using n 1=2 i processors, using the algorithm in <ref> [15] </ref>. This implies that we can carry out steps 1 and 6 2 for each subproblem in O (1) time using n 1=2 i1 processors. <p> Since r is atleast *=2, this operation takes no more than 2=* steps. 9 In order to partition the array, note that we can find the minimum of n r quantities using n 2r processors on a Common CRCW PRAM in O (1) time. <ref> [15] </ref> 2 Lemma 13 Step 2 of algorithm efficient sort can be carried out using n 1+* processors in O ( 1 * log log n) time. Proof: For each partition S, do the following: Divide the array into n *=2 blocks.
Reference: [16] <author> Vishkin U. and Wigderson A., </author> <title> Trade-offs between depth and width in parallel computation, </title> <journal> SIAM J comput, </journal> <volume> vol 14, </volume> <pages> no.2, </pages> <month> May </month> <year> 1985, </year> <pages> 303-314. 13 </pages>
Reference-contexts: The algorithm uses n 3=4 cells of shared memory, whereas it is well known that any algorithm, for computing the maximum of n items, which has a worst case running time of T , requires at least n=T cells of shared memory <ref> [16] </ref>. While carrying out the average case analysis, we assume that all permutations of the input elements are equally likely.
References-found: 16

