URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/papers/paper8-1.ps
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.755/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Email: Email: ibrahim@cogs.susx.ac.uk  
Title: Evolution of Learning Rules for Supervised Tasks I: Simple Learning Problems  
Author: Ibrahim KUSCU 
Keyword: Evolution, Genetic Algorithms, Supervised Learning  
Date: November 10, 1995  
Address: Brighton BN1 9QH  
Affiliation: Cognitive and Computing Sciences University of Sussex  
Abstract: Initial experiments with a genetic based encoding schema are presented as a potentially powerful tool to discover learning rules by means of evolution. Several simple supervised learning tasks are tested. The results indicate the potential of the encoding schema to discover learning rules for more complex and larger learning problems. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D.H. Ackley and M.S. Litman. </author> <title> Interactions between learning and evolution. </title> <editor> In Langton et al, editor, </editor> <booktitle> Artificial Life II. </booktitle> <publisher> Addison-Wesley, </publisher> <year> 1992. </year>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] [11] [19] <ref> [1] </ref> [17]. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [2] <author> R.K. Belew, J. McInerney, and N.N. Schraudolph. </author> <title> Evolving networks: using the genetic algorithms with connectionist learning. </title> <editor> In Langton et al, editor, </editor> <booktitle> Artificial Life II. </booktitle> <address> Santa Fe Institute, </address> <year> 1992. </year>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning <ref> [2] </ref> [16] [11] [19] [1] [17]. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [3] <author> D.J. Chalmers. </author> <title> Evolution of learning: an experiment in genetic connectionism. </title> <editor> In Touretzky et al, editor, </editor> <title> Connectionist Models. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: 1 Introduction Evolution and learning address two different levels of adaptation processes: one taking place during the life time of an organism and the other taking place during the evolutionary history of population of organisms <ref> [3] </ref> [18]. There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] [11] [19] [1] [17]. <p> Evolution is used as a mean of creating an initial state upon which learning process is applied for increased performance. However, the nature of learning during an evolutionary process has not been investigated. 1 An interesting experiment was carried out by Chalmers <ref> [3] </ref>. His primary aim was to observe how learning might evolve during the process of evolution. Rather than looking at the interaction between evolution and learning, the major focus of his study was evolution of learning mechanisms themselves. <p> * *I1*) - *I1*)))) + - * 4. (((((*I2* * *I1*) + ((*I1* * *I2*) - (*I1* - *I2*))) ((1 * *I1*) - (*I2* + *I1*))) + (1 - *I2*)) - + - 3.2 Eight Linearly Separable Tasks The other experiments included the eight different supervised tasks used by Chalmers <ref> [3] </ref>. All of these tasks were linearly separable mappings. As shown in the following table they have five input units and one output unit. For each task twelve patterns are used.
Reference: [4] <author> Ali Dasdan and Kemal Oflazer. </author> <title> Genetic syntesis of unsupervised learning algorithms. </title> <type> Technical report, </type> <institution> Dept. of Coputer Engineering and Information Science, Bilkent University, </institution> <year> 1994. </year>
Reference-contexts: A similar experiment is applied in the area of unsupervised learning to find rules for the Self-Organising Map <ref> [4] </ref>. They have supported the idea that genetic algorithms can be used to search for optimal learning algorithms by providing evidence for the existence of several learning algorithms that produced an organisation similar to that of Kohonen Algorithm.
Reference: [5] <author> A. DeJong, Kenneth. </author> <title> Genetic algorithms: a 10 years perspective. </title> <booktitle> In International Conference on Genetic Algorithms and their applications. </booktitle> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1985. </year>
Reference-contexts: GA works directly on the representation of the problem encoded on genome. Thus, the search space within which the possible solution lies is directly influenced by the representation. It is widely recognised that fixed length character strings pose severe limitations for the solution of particular problems <ref> [5] </ref> [6] Several representation schema which improve the capabilities of Genetic Algorithms have been presented by Smith [21] Koza [13] and Harvey [8][9][10]. Smith has provided an earlier example of using variable length strings in representing the genome.
Reference: [6] <author> A. DeJong, Kenneth. </author> <title> Learning with genetic algorithms: an overview. </title> <journal> Machine Learning, </journal> <volume> 3(2) </volume> <pages> 121-138, </pages> <year> 1988. </year>
Reference-contexts: GA works directly on the representation of the problem encoded on genome. Thus, the search space within which the possible solution lies is directly influenced by the representation. It is widely recognised that fixed length character strings pose severe limitations for the solution of particular problems [5] <ref> [6] </ref> Several representation schema which improve the capabilities of Genetic Algorithms have been presented by Smith [21] Koza [13] and Harvey [8][9][10]. Smith has provided an earlier example of using variable length strings in representing the genome.
Reference: [7] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Massacheusettes, </address> <year> 1989. </year>
Reference-contexts: to achieve, potentially an equal chance of producing negative and positive values when generating the expressions in the initial population. 2.2 Genetic Algorithms The Schema Theorem developed by Holland [12] based on genetic search has been proven to be useful in many applications involving large, complex and deceptive search spaces <ref> [7] </ref>. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions. Using Genetic Algorithms (GA) the model is implemented in LISP. The top level structure of the system exhibit the following: 1. Initialize the population of expressions 2.
Reference: [8] <author> I. Harvey. </author> <title> Adding species adaptation genetic algorithms:a basis for a conit-inuing saga. </title> <type> Technical Report CSRP-221, </type> <institution> University of Sussex, COGS, </institution> <year> 1992a. </year>
Reference: [9] <author> I. Harvey. </author> <title> Evolutionary robotics and saga: The case for hill crowling and tournement selection. </title> <type> Technical Report CSRP-222, </type> <institution> University of Sussex, COGS, </institution> <year> 1992b. </year>
Reference: [10] <author> I. Harvey. </author> <title> The saga cross:the mechanics of recombination for species with variable-length genotypes. </title> <type> Technical Report CSRP-223, </type> <institution> University of Sus-sex, COGS, </institution> <year> 1992c. </year>
Reference: [11] <author> G.E. Hinton and S. J. Nowlan. </author> <title> How learning can guide evolution. </title> <journal> Complex systems, </journal> <volume> 1 </volume> <pages> 495-502, </pages> <year> 1987. </year>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] <ref> [11] </ref> [19] [1] [17]. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [12] <author> J. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, USA, </address> <year> 1975. </year>
Reference-contexts: This is to achieve, potentially an equal chance of producing negative and positive values when generating the expressions in the initial population. 2.2 Genetic Algorithms The Schema Theorem developed by Holland <ref> [12] </ref> based on genetic search has been proven to be useful in many applications involving large, complex and deceptive search spaces [7]. So genetic search is most likely to allow fast, robust evolution of genotypes encoding for potential learning rules as mathematical expressions.
Reference: [13] <author> J. Koza. </author> <title> Genetic Programming:On the programming of computers by means of natural selection. </title> <publisher> MIT press, </publisher> <year> 1992. </year>
Reference-contexts: It is widely recognised that fixed length character strings pose severe limitations for the solution of particular problems [5] [6] Several representation schema which improve the capabilities of Genetic Algorithms have been presented by Smith [21] Koza <ref> [13] </ref> and Harvey [8][9][10]. Smith has provided an earlier example of using variable length strings in representing the genome. Harvey has shown that adaptation of variable length genome improves conventional Genetic Algorithms by changing their finite search space to an open ended search. <p> Rather than searching for a general learning algorithm (as in the work of Chalmers), the aim is to see whether evolution would produce a specific learning rule for the problem in hand. Although the representation schema is very similar to that of Koza's Genetic Programming <ref> [13] </ref>, introducing prior knowledge into representation of the problem will be minimal, if any at all. In this strategy potential learning rules are encoded as random mathematical expressions at variable lengths. Mathematical expressions are used in simulating the evolution of cooperation in iterated prisoner's dilemma game by Kuscu [14].
Reference: [14] <author> I. Kuscu. </author> <title> Evolution of cooperation, MSc. </title> <type> Thesis, </type> <institution> Dept. of AI, University of Edinburgh, </institution> <year> 1991. </year>
Reference-contexts: In this strategy potential learning rules are encoded as random mathematical expressions at variable lengths. Mathematical expressions are used in simulating the evolution of cooperation in iterated prisoner's dilemma game by Kuscu <ref> [14] </ref>. The expressions are made up of random numbers and random variables. The variables are to be instantiated to input values of training set in a typical supervised learning. By using LISP's "EVAL" statement, the expressions are evaluated to certain numbers.
Reference: [15] <author> Ibrahim Kuscu. </author> <title> Evolution of learning rules for supervised tasks ii: Hard learning problems. </title> <type> Technical Report CSRP-395, Uni. </type> <institution> of Sussex, COGS, </institution> <year> 1995. </year>
Reference-contexts: Although this reduces the scale and the complexity of the search space, it also effectively introduces possible low level solutions to the problem in hand. This form of human intervention makes it less attractive for a learning paradigm. This issue is extensively discussed in <ref> [15] </ref>. In this paper, as part of an ongoing research an encoding schema will be presented. Combined with genetic algorithms it can successfully produce evolution of learning rules.
Reference: [16] <author> J. Maynard Smith. </author> <title> When learning guides evolution. </title> <journal> Nature, </journal> <volume> 329 </volume> <pages> 761-762, </pages> <year> 1987. </year>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning [2] <ref> [16] </ref> [11] [19] [1] [17]. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [17] <author> G.F. Miller and P.M. Todd. </author> <title> Exploring adaptive agency i:theory and methods for simulating the evolution of learning. </title> <editor> In Touretzky et al, editor, </editor> <title> Connectionist Models. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 17 </month>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] [11] [19] [1] <ref> [17] </ref>. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [18] <editor> Melanie Mitchell and Stephanie Forrest. </editor> <title> Genetic algorithms and artificial life. </title> <type> Technical Report 93-11-072, </type> <institution> Santa Fe, </institution> <year> 1993. </year>
Reference-contexts: 1 Introduction Evolution and learning address two different levels of adaptation processes: one taking place during the life time of an organism and the other taking place during the evolutionary history of population of organisms [3] <ref> [18] </ref>. There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] [11] [19] [1] [17].
Reference: [19] <author> S. Nolfi, J.L. Elman, and D. Parisi. </author> <title> Learning and evolution in neural networks. </title> <type> Technical Report CRL 9019, Uni. </type> <institution> of California, </institution> <year> 1990. </year>
Reference-contexts: There are quite a number of research concentrating on the relationship between evolution and learning [2] [16] [11] <ref> [19] </ref> [1] [17]. The nature of interaction between the two has been shown to be complementary : the presence of learning can facilitate the process of evolution and evolutionary methods can significantly speed up the learning process.
Reference: [20] <author> D. Rumelhart, G. Hinton, and R. Williams. </author> <title> Learning internal representations by error propagation. </title> <editor> In D. Rumelhart, J. McClelland, </editor> <title> and the PDP Research Group, editors, Parallel Distributed Processing: Explorations in the Micro-structures of Cognition. Vols I and II. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Mass., </address> <year> 1986a. </year>
Reference-contexts: Since the target outputs are in the range of 0 to 1, the values, once obtained after the evaluation of the expressions, are mapped to values between 0 and 1 by using a squashing function. Several functions have been tested in this mapping including logistic activation function used by <ref> [20] </ref>. <p> *I2*)) (((*I1* + *I1*) - (*I1* + *I4*)) + (*I4* + 0.28141))) (*I1* + *I1*)) - (((*I4* + *I3*) * ((*I3* + *I5*) ((*I5* + *I3*) * (*I2* - *I4*)))) * (*I4* + 0.30423)))) Success: 100 percent 3.3 Parity Problems A difficult task for many learning algorithms are parity problems <ref> [20] </ref> where the required output is 1 if the input pattern contains an odd number of 1's; otherwise it is 0. This is a hard learning problem because very similar patterns (even different with one bit) may require completely different output.
Reference: [21] <author> S.S. Smith. </author> <title> A learning system based on genetic adaptive algorithms, </title> <type> phd. dissertation, </type> <institution> university of pittsburgh, </institution> <year> 1980. </year>
Reference-contexts: It is widely recognised that fixed length character strings pose severe limitations for the solution of particular problems [5] [6] Several representation schema which improve the capabilities of Genetic Algorithms have been presented by Smith <ref> [21] </ref> Koza [13] and Harvey [8][9][10]. Smith has provided an earlier example of using variable length strings in representing the genome. Harvey has shown that adaptation of variable length genome improves conventional Genetic Algorithms by changing their finite search space to an open ended search.
Reference: [22] <author> C. Thornton. </author> <title> Supervised learning of conditional approach: a case study. </title> <type> Technical Report 291, </type> <institution> COGS, University of Sussex, </institution> <year> 1993. </year>
Reference-contexts: problems have been shown to be difficult to solve using conventional learning algorithms (i.e. back-propagation) due to the fact that the rule of learning contained in the target mapping may not refer to particular values of variables but rather it may refer to the possible relations among the input variables <ref> [22] </ref>. Thus the aim of the next experiments is to see whether the encoding strategy presented here would provide solution to hard learning problems under evolution. The next experiments will also involve target mappings with continuous rather than binary values and with more than one output values.
Reference: [23] <author> D. Whitley. </author> <title> The genitor algorithm and why rank based-based allocation of reproductive trials is best. </title> <editor> In J.D. Schaffer, editor, </editor> <booktitle> Proceedings of Third iInternational Conference on Genetic Algorithms, </booktitle> <pages> pages 116-123, </pages> <year> 1989. </year> <month> 18 </month>
Reference-contexts: For the model this means to select those expressions with higher scores (beginning part of the rank) and give them more chance to reproduce. In the model, parent selection technique for reproduction is normalizing by using an exponential function taken from Whitley's <ref> [23] </ref> rank-based selection technique. The function generates integer numbers from 1 to population size. The generation of numbers exhibits characteristics of a non-linear function where there is more tendency to produce smaller numbers (since higher scoring expressions are on top of the rank).
References-found: 23

