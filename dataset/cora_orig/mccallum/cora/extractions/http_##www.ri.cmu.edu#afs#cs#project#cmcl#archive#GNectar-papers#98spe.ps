URL: http://www.ri.cmu.edu/afs/cs/project/cmcl/archive/GNectar-papers/98spe.ps
Refering-URL: http://www.ri.cmu.edu/afs/cs/usr/prs/WWW/papers.html
Root-URL: 
Title: Design, Implementation, and Evaluation of a Single-Copy Protocol Stack  
Author: Peter Steenkiste 
Keyword: zero-copy protocol implementation, TCP/IP, outboard buffering, communication API.  
Note: To Appear in Software Practice and Experience  
Address: Pittsburgh, PA 15213  
Affiliation: School of Computer Science Carnegie Mellon University  
Email: e-mail: steenkiste@cs.cmu.edu  
Phone: Tel: 412 268 3261  
Date: January 20, 1998  
Abstract: Data copying and checksumming are the most expensive operations on hosts performing high-bandwidth network I/O over a high-speed network. Under some conditions, outboard buffering and checksumming can eliminate accesses to the data, thus making communication less expensive and faster. One of the scenarios in which outboard buffering and checksumming pays off is the common case of applications accessing the network using the Berkeley sockets interface and the Internet protocol stack. In this paper we describe the host software for a host interface with outboard buffering and checksumming support. The platform used is DEC Alpha workstations with a Turbochannel I/O bus and running the DEC OSF/1 operating system. Our implementation does not only achieve single copy communication for applications that use sockets, but it also interoperates efficiently with in-kernel applications and other network devices. Measurements show that for large reads and writes the single-copy path through the stack is five to seven times more efficient than the traditional implementation for large writes. We also present a detailed analysis of the measurements using a simple I/O model. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David D. Clark, Van Jacobson, John Romkey, and Howard Salwen. </author> <title> An Analysis of TCP Processing Overhead. </title> <journal> IEEE Communications Magazine, </journal> <volume> 27(6):2329, </volume> <month> June </month> <year> 1989. </year>
Reference-contexts: 1 Introduction For bulk data transfer over high-speed networks, the sending and receiving hosts typically form the bottleneck, and it is important to minimize the communication overhead to achieve high application-level throughput. The communication cost can be broken up in per-packet and per-byte costs. The per-packet cost can be optimized <ref> [1, 2] </ref>, and for large packets, this overhead is amortized over a lot of data. However, the cost of per-byte operations such data copying or checksumming cost is not reduced by increasing the packet size.
Reference: [2] <author> Peter Steenkiste. </author> <title> Analyzing communication latency using the nectar communication processor. </title> <booktitle> In Proceedings of the SIGCOMM '92 Symposium on Communications Architectures and Protocols, pages 199209, </booktitle> <address> Baltimore, </address> <month> August </month> <year> 1992. </year> <note> ACM. </note>
Reference-contexts: 1 Introduction For bulk data transfer over high-speed networks, the sending and receiving hosts typically form the bottleneck, and it is important to minimize the communication overhead to achieve high application-level throughput. The communication cost can be broken up in per-packet and per-byte costs. The per-packet cost can be optimized <ref> [1, 2] </ref>, and for large packets, this overhead is amortized over a lot of data. However, the cost of per-byte operations such data copying or checksumming cost is not reduced by increasing the packet size. <p> The fact that a network device controlled by software increases latency was also observed in the Nectar system <ref> [2] </ref>, the predecessor of Gigabit Nectar. 7 Related work We review related work in the area of host interface design, focusing on single-copy architectures. The Afterburner interface [6], built by HP Research Labs in Bristol, also uses outboard buffering and checksumming to achieve single-copy communication for sockets and IP protocols.
Reference: [3] <author> Peter A. Steenkiste. </author> <title> A systematic approach to host interface design for high-speed networks. </title> <journal> IEEE Computer, </journal> <volume> 26(3):4757, </volume> <month> March </month> <year> 1994. </year>
Reference-contexts: In contrast, most host interfaces in use today copy the data two or three times before it reaches the network. For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support <ref> [3] </ref>, and several projects have proposed or implemented network adapters that include these features [4, 5, 6, 25]. Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12].
Reference: [4] <author> Van Jacobson. </author> <title> Efficient Protocol Implementation. </title> <booktitle> ACM '90 SIGCOMM tutorial, </booktitle> <month> September </month> <year> 1990. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [3], and several projects have proposed or implemented network adapters that include these features <ref> [4, 5, 6, 25] </ref>. Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> The checksum is calculated while the data is copied. The number of data transfers has been reduced to two. This interface corresponds to the WITLESS interface proposed by Van Jacobson <ref> [4, 6] </ref>. Figure 1 (c) shows how the number of data transfers can be further reduced to one by using DMA for the data transfer between host memory and the buffers on the network adapter. This is the minimum number with the socket interface.
Reference: [5] <author> Peter A. Steenkiste, Brian D. Zill, H.T. Kung, Steven J. Schlick, Jim Hughes, Bob Kowalski, and John Mullaney. </author> <title> A host interface architecture for high-speed networks. </title> <booktitle> In Proceedings of the 4th IFIP Conference on High Performance Networks, </booktitle> <pages> pages A3 116, </pages> <address> Liege, Belgium, </address> <month> December </month> <year> 1992. </year> <title> IFIP, </title> <publisher> Elsevier. </publisher>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [3], and several projects have proposed or implemented network adapters that include these features <ref> [4, 5, 6, 25] </ref>. Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> All DMA engines can operate at the same time, using time sharing to access network memory. There is hardware support for the checksum calculation on both the SDMA path (outgoing packets) and MDMA path (incoming packets) <ref> [5] </ref>. The CAB is controlled using a 29K microprocessor, which has its own memory bus, separate from the network memory bus. <p> The 29K microprocessor is responsible for the interaction with the host, the management of buffers in network memory, the operation of the DMA engines, and media access control <ref> [5] </ref>. The microcode managing the SDMA engine is the bottleneck in the host interface. The reason is that the translation of host DMA requests into SDMA commands is subject to a large number of constraints. These constraints are imposed by the Turbochannel (e.g.
Reference: [6] <author> Chris Dalton, Greg Watson, David Banks, Costas Calamvokis, Aled Edwards, and John Lum-ley. </author> <title> Afterburner. </title> <journal> IEEE Network Magazine, </journal> <volume> 7(4):3643, </volume> <month> July </month> <year> 1993. </year>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [3], and several projects have proposed or implemented network adapters that include these features <ref> [4, 5, 6, 25] </ref>. Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> The checksum is calculated while the data is copied. The number of data transfers has been reduced to two. This interface corresponds to the WITLESS interface proposed by Van Jacobson <ref> [4, 6] </ref>. Figure 1 (c) shows how the number of data transfers can be further reduced to one by using DMA for the data transfer between host memory and the buffers on the network adapter. This is the minimum number with the socket interface. <p> The fact that a network device controlled by software increases latency was also observed in the Nectar system [2], the predecessor of Gigabit Nectar. 7 Related work We review related work in the area of host interface design, focusing on single-copy architectures. The Afterburner interface <ref> [6] </ref>, built by HP Research Labs in Bristol, also uses outboard buffering and checksumming to achieve single-copy communication for sockets and IP protocols. Its predecessor, Medusa [24], has a similar architecture.
Reference: [7] <author> Aled Edwards, Greg Watson, John Lumley, David Banks, Costas Calamvokis, and Chris Dalton. </author> <title> User-space protocols deliver high performance to application on a low-cost Gb/s LAN. </title> <booktitle> In Proceedings of the SIGCOMM '94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 1423. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1994. </year>
Reference: [8] <author> Eric Cooper, Peter Steenkiste, Robert Sansom, and Brian Zill. </author> <title> Protocol implementation on the nectar communication processor. </title> <booktitle> In Proceedings of the SIGCOMM '90 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 135143, </pages> <address> Philadelphia, </address> <month> September </month> <year> 1990. </year> <note> ACM. </note>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics <ref> [8, 9, 10, 11] </ref>, or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> With a single-copy stack based on outboard buffering, performance is less sensitive to the alignment and length of the application buffer, but more hardware support is needed. Finally, many groups have implemented APIs with share and move semantics <ref> [8, 9, 10, 11] </ref>. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering.
Reference: [9] <author> Peter Druschel and Larry Peterson. Fbufs: </author> <title> A High-Bandwidth Cross-Domain Transfer Facility. </title> <booktitle> In Proceedings of the Fourteenth Symposium on Operating System Principles, </booktitle> <pages> pages 189202. </pages> <publisher> ACM, </publisher> <month> December </month> <year> 1993. </year>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics <ref> [8, 9, 10, 11] </ref>, or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> Even though the API still has copy semantics, performance will be best if a limited set of buffers are used for communication, e.g. the usage of the API has share semantics (e.g <ref> [9] </ref>). 3.5.2 Synchronization The copy semantics of the socket interfaces requires that the application is only allowed to continue after a copy has been made of the data (transmit), or after the incoming data is available (receive). <p> With a single-copy stack based on outboard buffering, performance is less sensitive to the alignment and length of the application buffer, but more hardware support is needed. Finally, many groups have implemented APIs with share and move semantics <ref> [8, 9, 10, 11] </ref>. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering.
Reference: [10] <author> Peter Druschel, Larry Peterson, and Bruce Davie. </author> <title> Experience with a high-speed network adaptor: A software perspective. </title> <booktitle> In Proceedings of the SIGCOMM '94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 213. </pages> <publisher> ACM, </publisher> <month> August </month> <year> 1994. </year> <month> 22 </month>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics <ref> [8, 9, 10, 11] </ref>, or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> With a single-copy stack based on outboard buffering, performance is less sensitive to the alignment and length of the application buffer, but more hardware support is needed. Finally, many groups have implemented APIs with share and move semantics <ref> [8, 9, 10, 11] </ref>. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering.
Reference: [11] <author> Jose Brustoloni. </author> <title> Exposed buffering and subdatagram flow control for ATM LANs. </title> <booktitle> In Proceedings of the 19th Conference on Local Computer Networks, </booktitle> <pages> pages 324334. </pages> <publisher> IEEE, </publisher> <month> Octo-ber </month> <year> 1994. </year>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics <ref> [8, 9, 10, 11] </ref>, or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> With a single-copy stack based on outboard buffering, performance is less sensitive to the alignment and length of the application buffer, but more hardware support is needed. Finally, many groups have implemented APIs with share and move semantics <ref> [8, 9, 10, 11] </ref>. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering.
Reference: [12] <author> Hsiao keng Chu. </author> <title> Zero-copy tcp in solaris. </title> <booktitle> In Proceedings of the Winter 1996 USENIX Conference. Usenix, </booktitle> <month> January </month> <year> 1996. </year>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way <ref> [12] </ref>. A detailed study of the impact of API semantics on data passing performance can be found in [13]. Many applications use BSD sockets and the Internet protocols for communication, and as a result it is worthwhile to look at how they can be supported efficiently.
Reference: [13] <author> Jose Brustoloni and Peter Steenkiste. </author> <title> The effects buffering semantics on i/o performance. </title> <booktitle> In Proceedings 2nd Symposium on Operating Systems Design and Implementation (OSDI'96), </booktitle> <pages> pages 277291. </pages> <publisher> Usenix, </publisher> <month> October </month> <year> 1996. </year>
Reference-contexts: Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in <ref> [13] </ref>. Many applications use BSD sockets and the Internet protocols for communication, and as a result it is worthwhile to look at how they can be supported efficiently. <p> Note that we expect this difference in the per-byte cost between the two architectures to grow over time, both as a result of further increases in the page size, and as a result of a widening of the gap between CPU speed and memory speed <ref> [13] </ref>. The factor of eight difference in per-byte costs translates into a factor of five or more difference in overall efficiency for larger writes. The reason is that the per-write and per-packets costs were not reduced by moving to a single-copy architecture. <p> It accounts for a third of the increase. These operations could be optimized, and more recent measurements for a different operating system (NetBSD) and faster computer systems show indeed much lower overheads for virtual memory management <ref> [13] </ref>. A second reason is additional book keeping in the single-copy stack: the single-copy stack does not only have to manage mbufs, but it also has to manage data structures that keep track of the buffers in application space so that they can be unpinned appropriately. <p> These techniques make it possible to achieve single-copy communication, although there are usually severe constraints on the buffer alignment and length, e.g. buffers have to be page aligned. If these constraints are not met, the regular path through the stack is used, although this is not a fundamental constraint <ref> [13] </ref>. This approach is for example used on some SGI systems; they also use an outboard checksum unit to eliminate the data access for checksumming. <p> Finally, many groups have implemented APIs with share and move semantics [8, 9, 10, 11]. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering. We are evaluating these techniques in the context of the Credit Net project <ref> [28, 13, 29] </ref>. 8 Conclusion We described a network adapter architecture and implementation that provides outboard buffering and checksumming in support of single-copy communication for applications using BSD sockets and IP protocols.
Reference: [14] <institution> Digital Equipment Corporation. TURBOchannel Overview, </institution> <month> April </month> <year> 1990. </year>
Reference-contexts: We also present a detailed performance study, which allows us to quantify both the speedups and additional overheads associated with our single-copy approach. Our implementation is for a HIPPI network and DEC workstations using the Turbochannel I/O bus <ref> [14] </ref> and running the DEC OSF/1 operating system. The interface was used in the Nectar gigabit testbed. The remainder of the paper is organized as follows. We first describe the host interface architecture and the implementation of a single copy protocol stack. <p> It was built using off-the-shelf components by NSC for DEC workstations with a Turbochannel I/O bus <ref> [14] </ref>. The workstation CAB sits in a separate box that connects to a Turbochannel paddle card through a cable. The paddle card uses the TcIA chip [15] to communicate over the Turbochannel. network memory that stores incoming and outgoing packets (Figure 2).
Reference: [15] <author> Jim Crapuchettes. </author> <title> TURBOchannel Interface ASIC Functional Specification. TRI/ADD Program, </title> <note> DEC, revision 0.6, preliminary edition, </note> <year> 1992. </year>
Reference-contexts: It was built using off-the-shelf components by NSC for DEC workstations with a Turbochannel I/O bus [14]. The workstation CAB sits in a separate box that connects to a Turbochannel paddle card through a cable. The paddle card uses the TcIA chip <ref> [15] </ref> to communicate over the Turbochannel. network memory that stores incoming and outgoing packets (Figure 2).
Reference: [16] <author> Samuel J. Leffler, Marshall Kirk McKusick, Michael J. Karels, and John S. Quarterman. </author> <title> The Design and Implementation of the 4.3BSD UNIX Operating System. </title> <publisher> Addison-Wesley Publishing Company, </publisher> <address> Reading, Massachusetts, </address> <year> 1989. </year>
Reference-contexts: This model is different from that found in Berkeley Unix operating systems, where data is channeled through the system's network buffer pool <ref> [16] </ref>.
Reference: [17] <author> Van Jacobson. pbufs. </author> <type> Personal communication, </type> <month> October </month> <year> 1992. </year>
Reference-contexts: However, this would have required more substantial changes to the code, and the data structure would have ended up being fairly similar to external mbufs. In some ways, our M UIO mbufs are similar to pbufs <ref> [17] </ref>.
Reference: [18] <author> J. Postel. </author> <title> Transmission control protocol. Request for Comments 793, </title> <month> September </month> <year> 1981. </year>
Reference-contexts: The host sets the value of S to the length of all the headers (HIPPI and IP), i.e. the hardware calculates the checksum over the user data, and the host is responsible for the fields in the header (the TCP header and pseudo-header <ref> [18] </ref>). While there are many other choices for S, this selection has the advantage that it works correctly when retransmitting data. Specifically, when retransmit-ting, the host provides a new header, with a new checksum seed.
Reference: [19] <author> Richard F. Rashid, Robert V. Baron, A. Forin, David B. Golub, Michael Jones, Daniel Julin, D. Orr, and R. Sanzi. </author> <title> Mach: A Foundation for Open Systems. </title> <booktitle> In Proceedings of the Second IEEE Workshop on Workstation Operating Systems, </booktitle> <pages> pages 109113, </pages> <month> September </month> <year> 1989. </year>
Reference-contexts: In contrast, when the DMA is to or from an application address space, the driver (or kernel) has to perform VM operations on pages in a different address space. Unfortunately, operating systems do not support this in a uniform way. We briefly describe implementations for the Mach 3.0 microkernel <ref> [19] </ref> and DEC OSF/1, and compare them in terms of complexity and performance. In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port [20] for the target address space. In the case of a Mach 3.0 microkernel [19], this allows the device <p> the Mach 3.0 microkernel <ref> [19] </ref> and DEC OSF/1, and compare them in terms of complexity and performance. In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port [20] for the target address space. In the case of a Mach 3.0 microkernel [19], this allows the device driver, which resides in the Unix server, to perform the address translation 9 and pinning of the pages in the application address space, i.e. DMA support is localized in the device driver.
Reference: [20] <author> Linda R. Walmer and Mary R. Thompson. </author> <title> A Programmer's Guide to the Mach User Environment. </title> <institution> Computer Science Department, Carnegie Mellon University, </institution> <month> November </month> <year> 1989. </year>
Reference-contexts: We briefly describe implementations for the Mach 3.0 microkernel [19] and DEC OSF/1, and compare them in terms of complexity and performance. In Mach, the required VM operations can be performed on a different address by specifying the appropriate Mach port <ref> [20] </ref> for the target address space. In the case of a Mach 3.0 microkernel [19], this allows the device driver, which resides in the Unix server, to perform the address translation 9 and pinning of the pages in the application address space, i.e.
Reference: [21] <author> K.K. Ramakrishnan. </author> <title> Performance Considerations in Designing Network Interfaces. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 11(2):203219, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: This means that when a read or write operation is interrupted (say because of an alarm signal), the process will only be allowed to restart after all outstanding DMA operations have completed. 10 3.5.3 Optimization based on packet size The tradeoff between programmed IO and DMA are well understood (e.g. <ref> [21] </ref>). PIO has typically a higher per-byte overhead while DMA has a higher per-transfer overhead; as a result PIO is typically more efficient for short transfers, and DMA for longer transfers. Since the CAB only supports DMA, this is not an issue.
Reference: [22] <author> D. Borman, R. Braden, and V. Jacobson. </author> <title> TCP Extensions for High Performance. Request for Comments 1323, </title> <month> May </month> <year> 1992. </year>
Reference-contexts: The OSF/1 protocol stack is based on Net2 BSD and also supports TCP window scaling <ref> [22] </ref>. The implementation of the single-copy stack supports user-level and in-kernel applications communicating through Ethernet and user-level applications communicating through the CAB.
Reference: [23] <author> Jonathan Kay and Joseph Pasquale. </author> <title> The Importance of Non-Data Touching Processing Overheads in TCP/IP. </title> <booktitle> In Proceedings of the SIGCOMM '93 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 259268. </pages> <publisher> ACM, </publisher> <month> October </month> <year> 1993. </year>
Reference-contexts: For 512 KByte writes, the difference is 97% versus 75%. This means that further improvements in communication efficiency will have to include optimization of the per-packet and per-write operations. This would also improve the efficiency of smaller writes <ref> [23] </ref>. 6.2 Latency Another performance parameter is the latency for short messages. For 4 byte messages we measured a roundtrip latency using the unmodified TCP/IP stack of 1.0 millisecond over Ethernet and 2.7 milliseconds over HIPPI.
Reference: [24] <author> David Banks and Michael Prudence. </author> <title> A High-Performance Network Interface for a PA-RISC Workstation. </title> <journal> IEEE Journal on Selected Areas in Communication, </journal> <volume> 11(2):191202, </volume> <month> February </month> <year> 1993. </year>
Reference-contexts: The Afterburner interface [6], built by HP Research Labs in Bristol, also uses outboard buffering and checksumming to achieve single-copy communication for sockets and IP protocols. Its predecessor, Medusa <ref> [24] </ref>, has a similar architecture. Afterburner provides the host side of a network adapter and it can be used for different networks by adding a network-specific module, e.g. FDDI or ATM [25, 26].
Reference: [25] <author> Aled Edwards, Greg Watson, John Lumley, David Banks, Costas Calamvokis, and Chris Dalton. </author> <title> User-space protocols deliver high performance to applications on a low-cost gb/s lan. </title> <booktitle> In Proceedings of the SIGCOMM '94 Symposium on Communications Architectures and Protocols, </booktitle> <pages> pages 1423. </pages> <note> ACM, August/September 1995. 23 </note>
Reference-contexts: For Application Programming Interfaces (APIs) with copy semantics (e.g. sockets), the single-copy architecture might require outboard buffering and checksum support [3], and several projects have proposed or implemented network adapters that include these features <ref> [4, 5, 6, 25] </ref>. Alternatively, it is possible to eliminate copying by using APIs with other semantics [8, 9, 10, 11], or by using copy semantics in a restricted way [12]. A detailed study of the impact of API semantics on data passing performance can be found in [13]. <p> Its predecessor, Medusa [24], has a similar architecture. Afterburner provides the host side of a network adapter and it can be used for different networks by adding a network-specific module, e.g. FDDI or ATM <ref> [25, 26] </ref>. While at a high-level the Afterburner and CAB architectures are similar, there are a number of significant differences. First, Afterburner uses programmed IO to transfer data to outboard memory, plus some reformating of outboard data is possible, thus relaxing the constraints on the host software.
Reference: [26] <author> Aled Edwards and Steve Muir. </author> <title> Experience implementing a high-performance tcp in user-space. </title> <booktitle> In Proceedings of the SIGCOMM '95 Symposium on Communications Architectures and Protocols, pages 196205. ACM, </booktitle> <month> August/September </month> <year> 1995. </year>
Reference-contexts: Its predecessor, Medusa [24], has a similar architecture. Afterburner provides the host side of a network adapter and it can be used for different networks by adding a network-specific module, e.g. FDDI or ATM <ref> [25, 26] </ref>. While at a high-level the Afterburner and CAB architectures are similar, there are a number of significant differences. First, Afterburner uses programmed IO to transfer data to outboard memory, plus some reformating of outboard data is possible, thus relaxing the constraints on the host software.
Reference: [27] <author> Peter A. Steenkiste, Michael Hemy, Todd Mummert, and Brian Zill. </author> <title> Architecture and evaluation of a high-speed networking subsystem for distributed-memory systems. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 154163. </pages> <publisher> IEEE, </publisher> <month> May </month> <year> 1994. </year>
Reference-contexts: On receive, the two approaches used are more similar, although the use of programmed IO should result in a simpler implementation for Afterburner. Another example of a single-copy interface is the Gigabit Nectar HIPPI interface for iWarp <ref> [27, ?] </ref>, which has hardware support for protocol processing similar to that of the provided by the workstation CAB. The organization of the protocol stack is similar to what we describe in this paper, although the implementation is in a proprietary runtime system, and not in a Unix environment.
Reference: [28] <author> Corey Kosak, David Eckhardt, Todd Mummert, Peter Steenkiste, and Allan Fisher. </author> <title> Buffer management and flow control in the credit net ATM host interface. </title> <booktitle> In Proceedings of the 20th Conference on Local Computer Networks, </booktitle> <pages> pages 370378, </pages> <address> Minneapolis, </address> <month> October </month> <year> 1995. </year> <note> IEEE. </note>
Reference-contexts: Finally, many groups have implemented APIs with share and move semantics [8, 9, 10, 11]. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering. We are evaluating these techniques in the context of the Credit Net project <ref> [28, 13, 29] </ref>. 8 Conclusion We described a network adapter architecture and implementation that provides outboard buffering and checksumming in support of single-copy communication for applications using BSD sockets and IP protocols.
Reference: [29] <author> Jose Brustoloni and Peter Steenkiste. </author> <title> Copy emulation in checksummed, </title> <journal> multiple-packet communication. </journal> <note> In IEEE INFOCOM'97, page To be presented. IEEE, </note> <month> April </month> <year> 1997. </year> <month> 24 </month>
Reference-contexts: Finally, many groups have implemented APIs with share and move semantics [8, 9, 10, 11]. By changing the semantics of the API it is sometimes possible to achieve single-copy communication without the need for outboard buffering. We are evaluating these techniques in the context of the Credit Net project <ref> [28, 13, 29] </ref>. 8 Conclusion We described a network adapter architecture and implementation that provides outboard buffering and checksumming in support of single-copy communication for applications using BSD sockets and IP protocols.
References-found: 29

