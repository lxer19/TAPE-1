URL: http://www.cs.utah.edu/~cs686/Previous/s96/hybrid.ps
Refering-URL: http://www.cs.utah.edu/~cs686/Previous/s96/
Root-URL: 
Title: Performance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols  
Author: Rohit Chandra, Kourosh Gharachorloo, Vijayaraghavan Soundararajan, and Anoop Gupta 
Address: CA 94305  
Affiliation: Computer Systems Laboratory Stanford University,  
Abstract: Hardware distributed shared memory (DSM) systems efficiently support fine grain sharing of data by maintaining coherence at the level of individual cache lines and providing automatic replication in processor caches. Software DSM systems, on the other hand, amortize high communication costs by maintaining coherence at coarser granularities and replicating data at the level of local main memories. Even though software DSM systems have traditionally been targeted towards loosely coupled environments, some of the techniques are potentially useful in the context of tightly coupled multiprocessors. In particular, communicating data at a coarse grain can sometimes be more efficient than transferring the data as individual cache lines. Furthermore, replication in local memories can accommodate applications with larger working sets as compared to replication in processor caches only. Therefore, combining the two techniques in a hybrid protocol can potentially exploit the benefits of each approach. This paper proposes one such hybrid protocol and evaluates its performance in the context of the FLASH multiprocessor architecture [24]. The hybrid system allows the programmer to optionally identify regions of data shared at a coarse granularity. Coherence for such data is maintained at the grain of the entire region using a software-DSM-style protocol. We evaluate the performance gains of this approach through a detailed simulation study of several parallel applications. Our preliminary results show that the hybrid protocol can eliminate a substantial fraction of remote cache misses through bulk transfer of coarse grain data regions and replication of such data in local memories. The performance gains over hardware cache coherence are modest at low network latencies, but increase substantially at higher network latencies and processor speeds. Finally, we show that similar to cache-only memory architectures, the hybrid protocol is insensitive to data placement issues. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Sarita Adve and Mark Hill. </author> <title> Weak ordering Anew definition. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 2-14, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The FLASH architecture provides a programmable controller at each node that can be used to efficiently support both the base and the software DSM protocol. Similar to most protocols that exploit relaxed memory models, our hybrid protocol requires the programmer to identify all synchronization operations <ref> [1, 12, 17] </ref>. In addition, the programmer has the option of identifying regions of data that exhibit coarse grain sharing by supplying annotations that delimit references to such data in between synchronization points. Data regions identified in this manner are kept coherent using the software DSM protocol. <p> Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols [3, 8, 26] typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models <ref> [1, 12, 17] </ref> to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41]. <p> We assume the hardware DSM component of our protocol uses the release consistency model. Therefore, the programmer is required to identify all synchronization points in a program and to provide sufficient synchronization to ensure that all references to shared data are race-free <ref> [1, 17] </ref>. We extend the above base model to allow the programmer to optionally provide extra usage information about data regions that are shared at a coarse granularity. We loosely define a region as a set of memory locations that are referenced and shared together.
Reference: [2] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubiatow-icz. </author> <month> April: </month> <title> A processor architecture for multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Consequently, designers have attempted to support this programming model across a variety of parallel machines. Traditional tightly coupled multiprocessors support the single address space model directly in hardware and provide for data replication within processor caches (e.g., Alewife <ref> [2] </ref>, Dash [27]). The low latency and high bandwidth networks used in these architectures allow for efficient communication at a fine granularity, typically in the form of fixed size cache lines. <p> Second, higher remote latencies provide extra headroom for tolerating the overheads associated with the software DSM component of a hybrid protocol. Third, several recent designs provide efficient support for invoking general protocol actions at remote nodes by either utilizing processors with fast interrupt capability (e.g., MIT Alewife <ref> [2] </ref>) or providing dedicated processors to handle protocol actions (e.g., MIT *T [31], Stanford FLASH [24]). In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]).
Reference: [3] <author> James Archibald and Jean-Loup Baer. </author> <title> Cache coherence protocols: Evaluation using a multiprocessor simulation model. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 4(4) </volume> <pages> 273-298, </pages> <month> November </month> <year> 1986. </year>
Reference-contexts: Finally, we conclude in Section 7. 2 Background This section provides a brief overview of conventional hardware cache coherence and software DSM protocols. Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols <ref> [3, 8, 26] </ref> typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models [1, 12, 17] to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41].
Reference: [4] <author> Jean-Loup Baer and Tien-Fu Chen. </author> <title> An effective on-chip preloading scheme to reduce data access penalty. </title> <type> Technical Report 91-03-07, </type> <institution> University of Washington, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: Therefore, in our opinion, providing usage annotations in the hybrid protocol provides a much simpler abstraction. As part of our future research, we hope to quantify how well our transparent approach performs relative to the explicit use of messages. Prefetching <ref> [4, 30] </ref> is another approach that may be used to hide the latency of cache misses. With increasing remote latencies, overlapping prefetches with computation becomes more difficult. Therefore, the expected gains from prefetching will mainly arise from pipelining multiple prefetches.
Reference: [5] <author> B. Bershad and M. Zekauskas. Midway: </author> <title> Shared memory parallel programming with entry consistency for distributed memory multiprocessors. </title> <type> Technical Report CMU-CS-91-170, </type> <institution> Carnegie-Mellon University, </institution> <month> September </month> <year> 1991. </year>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy [28], Munin [7], LRC [13, 21], Midway <ref> [5, 6] </ref>). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> Thus, the granularity for communication is typically chosen to be either that of fixed size pages of several kilobytes (e.g., Ivy [28]) or large variable size objects (e.g., Midway <ref> [5, 6] </ref>). The two types of architectures described above are often referred to as hardware distributed shared memory (HW-DSM) and software distributed shared memory (SW-DSM), respectively. One fundamental difference between hardware and software distributed shared memory (DSM) architectures is the granularity at which data is communicated and kept coherent. <p> Data regions identified in this manner are kept coherent using the software DSM protocol. Among conventional software DSM protocols, our protocol most closely resembles Midway <ref> [5, 6] </ref>, with some refinements on the programming model and a different implementation better suited for tightly coupled environments. Our performance evaluation study is based on detailed simulations of four parallel applications. <p> Midway <ref> [5, 6] </ref> is an example of a region-based software DSM system. The Midway protocol is based on an extension of release consistency called entry consistency (EC). Entry consistency extends the release consistency model by requiring the programmer to explicitly associate shared data with synchronization variables. <p> One disadvantage of Midway compared to Munin and LRC is that the programmer is required to provide additional information to associate data with synchronizations. The original Midway proposal <ref> [5] </ref> required all shared data to be explicitly associated with synchronizations. Since annotating every use of every shared data can become burdensome for programmers, a follow-on proposal [6] prescribes supporting more than one consistency model (i.e., entry, release, and processor consistency). <p> The main disadvantage of region-based protocols is that the programmer is required to provide extra information about the usage of data regions in between synchronization points (e.g., by associating data to synchronization as in Midway <ref> [5, 6] </ref>). However, as we will argue in the next section, the information that is required is often naturally known by the programmer. This coupled with the fact that our hybrid protocol requires this information for only the coarse grain regions can greatly reduce the extra programming burden. <p> The next two parts describe the programming model and implementation for the specific region-based approach we have chosen. Our programming model and protocol resemble Midway in several ways. Like Midway, our software protocol is based on a variation of entry consistency <ref> [5, 6] </ref>. In order to reduce overheads, our protocol is streamlined towards providing simple and efficient mechanisms for maintaining coherence, achieving bulk transfer of data, and replicating data in local memories.
Reference: [6] <author> Brian Bershad, Matthew Zekauskas, and Wayne Sawdon. </author> <title> The Midway distributed shared memory system. </title> <booktitle> In Proceedings of COMP-CON'93, </booktitle> <pages> pages 528-537, </pages> <month> February </month> <year> 1993. </year> <month> 14 </month>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy [28], Munin [7], LRC [13, 21], Midway <ref> [5, 6] </ref>). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> Thus, the granularity for communication is typically chosen to be either that of fixed size pages of several kilobytes (e.g., Ivy [28]) or large variable size objects (e.g., Midway <ref> [5, 6] </ref>). The two types of architectures described above are often referred to as hardware distributed shared memory (HW-DSM) and software distributed shared memory (SW-DSM), respectively. One fundamental difference between hardware and software distributed shared memory (DSM) architectures is the granularity at which data is communicated and kept coherent. <p> Data regions identified in this manner are kept coherent using the software DSM protocol. Among conventional software DSM protocols, our protocol most closely resembles Midway <ref> [5, 6] </ref>, with some refinements on the programming model and a different implementation better suited for tightly coupled environments. Our performance evaluation study is based on detailed simulations of four parallel applications. <p> Midway <ref> [5, 6] </ref> is an example of a region-based software DSM system. The Midway protocol is based on an extension of release consistency called entry consistency (EC). Entry consistency extends the release consistency model by requiring the programmer to explicitly associate shared data with synchronization variables. <p> The original Midway proposal [5] required all shared data to be explicitly associated with synchronizations. Since annotating every use of every shared data can become burdensome for programmers, a follow-on proposal <ref> [6] </ref> prescribes supporting more than one consistency model (i.e., entry, release, and processor consistency). This approach allows the programmer to selectively provide usage information for only the subset of data regions that should be kept coherent through entry consistency. <p> The main disadvantage of region-based protocols is that the programmer is required to provide extra information about the usage of data regions in between synchronization points (e.g., by associating data to synchronization as in Midway <ref> [5, 6] </ref>). However, as we will argue in the next section, the information that is required is often naturally known by the programmer. This coupled with the fact that our hybrid protocol requires this information for only the coarse grain regions can greatly reduce the extra programming burden. <p> The next two parts describe the programming model and implementation for the specific region-based approach we have chosen. Our programming model and protocol resemble Midway in several ways. Like Midway, our software protocol is based on a variation of entry consistency <ref> [5, 6] </ref>. In order to reduce overheads, our protocol is streamlined towards providing simple and efficient mechanisms for maintaining coherence, achieving bulk transfer of data, and replicating data in local memories.
Reference: [7] <author> J. B. Carter, J. K. Bennett, and W. Zwaenepoel. </author> <title> Implementation and performance of Munin. </title> <booktitle> In Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 152-164, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy [28], Munin <ref> [7] </ref>, LRC [13, 21], Midway [5, 6]). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> However, the large coherence granularity of pages makes false sharing a major problem in Ivy. Some page-based protocols attempt to alleviate false sharing problems by exploiting a relaxed memory consistency model. For example, Munin <ref> [7] </ref> exploits release consistency (RC) [17] to efficiently support multiple writers to a single page by delaying the propagation of coherence transactions until a release synchronization (e.g., unlock). <p> More importantly, false sharing can be a major problem in page-based designs and techniques to solve it require sophisticated protocol actions such as duplicating and diffing pages <ref> [7] </ref> that can lead to large relative overheads in tightly coupled environments. Region-based software DSM protocols (discussed in the previous section) seem more applicable to tightly coupled environments primarily because their variable size coherence granularity virtually eliminates the problem of false sharing.
Reference: [8] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS directories: A scalable cache coherence scheme. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife <ref> [8] </ref>, FLASH [24]). Finally, many recent systems are providing support for efficient bulk data transfer (e.g., Alewife [23], FLASH [24], or the recently announced Cray T3D). The above features have the effect of enhancing potential gains and reducing relative overheads of a hybrid protocol. <p> Finally, we conclude in Section 7. 2 Background This section provides a brief overview of conventional hardware cache coherence and software DSM protocols. Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols <ref> [3, 8, 26] </ref> typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models [1, 12, 17] to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41].
Reference: [9] <author> Rohit Chandra, Anoop Gupta, and John L. Hennessy. </author> <title> Integrating concurrency and data abstraction in the COOL programming language. </title> <booktitle> IEEE Computer, </booktitle> <month> August </month> <year> 1994. </year> <note> To appear. </note>
Reference-contexts: All except Block Cholesky are versions of SPLASH applications [38] written in the Cool parallel programming language <ref> [9] </ref>. We chose the above set of applications for our preliminary study based on the higher likelihood that they would benefit from the optimizations provided by the hybrid protocol; we plan to extend our performance evaluation to the complete set of SPLASH benchmarks in the near future.
Reference: [10] <author> J. Chase, F. Amador, E. Lazowska, H. Levy, and R. Littlefield. </author> <title> The Amber sytem: Parallel programming on a network of multiprocessors. </title> <booktitle> In Proceedings of the 12th ACM Symposium on Operating Systems Principles, </booktitle> <pages> pages 147-58, </pages> <month> December </month> <year> 1989. </year>
Reference-contexts: The ability to maintain coherence at a variable grain size can alleviate many of the problems (e.g., false sharing) that arise from such a mismatch. One approach is to associate the coherence granularity to the granularity of program data objects (referred to as object-based) <ref> [10, 20, 29] </ref>.
Reference: [11] <author> Czarek Dubnicki and Thomas LeBlanc. </author> <title> Adjustable block size coherent caches. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 170-180, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: In contrast, software DSM protocols depend on coarse grain communication for efficiency and can experience severe performance degradation in the presence of fine grain sharing. However, applications often exhibit both coarse grain and fine grain communication behavior <ref> [11] </ref>. Therefore, it may be beneficial to expand the range of communication granu-larities that are efficiently supported by each protocol. Another fundamental difference between hardware and software DSM protocols is the effective size of memory that is exploited for caching shared data. <p> Furthermore, it is interesting to consider combinations of the two approaches, such as replication of data together with prefetching to hide the latency of local misses. There have been several other proposals for supporting communication or coherence at a variable granularity in tightly coupled multiprocessors. Dubnicki and LeBlanc <ref> [11] </ref> describe a hardware cache coherent system that dynamically adapts the effective granularity of cache lines. Compared to the hybrid protocol, the maximum granularity achieved in practice is at most several cache lines.
Reference: [12] <author> Michel Dubois, Christoph Scheurich, and Faye Briggs. </author> <title> Memory access buffering in multiprocessors. </title> <booktitle> In Proceedings of the 13th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 434-442, </pages> <month> June </month> <year> 1986. </year>
Reference-contexts: The FLASH architecture provides a programmable controller at each node that can be used to efficiently support both the base and the software DSM protocol. Similar to most protocols that exploit relaxed memory models, our hybrid protocol requires the programmer to identify all synchronization operations <ref> [1, 12, 17] </ref>. In addition, the programmer has the option of identifying regions of data that exhibit coarse grain sharing by supplying annotations that delimit references to such data in between synchronization points. Data regions identified in this manner are kept coherent using the software DSM protocol. <p> Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols [3, 8, 26] typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models <ref> [1, 12, 17] </ref> to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41].
Reference: [13] <author> Sandhya Dwarkadas, Pete Keleher, Alan Cox, and Willy Zwaenepoel. </author> <title> Evaluation of release consistent software distributed shared memory on emerging network technology. </title> <booktitle> In Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 144-155, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy [28], Munin [7], LRC <ref> [13, 21] </ref>, Midway [5, 6]). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> The more advanced optimizations in LRC lead to a more complex and higher overhead protocol than Munin. However, simulation results have shown that reduction in the number and size of messages can lead to an overall performance gain <ref> [13, 21] </ref>. 2.2.2 Protocols with Variable Size Granularity The main difficulty with software page-based systems is the mismatch between the fixed size coherence granularity and the inherent grain of sharing and communication in an application.
Reference: [14] <author> S.J. Eggers and R. H. Katz. </author> <title> The effect of sharing on the cache and bus performance of parallel programs. </title> <booktitle> In Proceedings of the Third International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 257-270, </pages> <month> April </month> <year> 1989. </year>
Reference-contexts: In such systems the cache line size can have a significant effect on miss rates and overall performance <ref> [14, 40, 41] </ref>. Even though larger lines can provide potential benefits due to prefetching, too large a line can either result in false sharing [14, 40] or lead to increased communication traffic for applications that exhibit poor spatial locality. <p> In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41]. Even though larger lines can provide potential benefits due to prefetching, too large a line can either result in false sharing <ref> [14, 40] </ref> or lead to increased communication traffic for applications that exhibit poor spatial locality. Compared to using large line sizes, the hybrid protocol proposed in this paper is a more robust and selective technique for exploiting coarse grain communication.
Reference: [15] <author> Michael Feeley and Henry Levy. </author> <title> Distributed shared memory with versioned objects. </title> <booktitle> In Proceedings of OOPSLA'92 Conference on Object-Oriented Programming Systems, Languages, and Applications, </booktitle> <pages> pages 247-262, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: The remaining data regions are kept coherent using a page-based approach with updates, that adheres to either the processor or release consistency semantics. The next section describes some of the similarities between the programming model for Midway and for our hybrid protocol. 1 VDOM <ref> [15] </ref> introduces fragment objects to alleviate some of these problems. 3 Combining Traditional Hardware and Software Coher ence Protocols This section describes the basic design of our hybrid coherence protocol for tightly coupled multiprocessors. The hybrid protocol consists of both a hardware DSM and a software DSM protocol.
Reference: [16] <author> Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> Performance evaluation of memory consistency models for shared-memory multiprocessors. </title> <booktitle> In Fourth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> pages 245-257, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: The latency of cache write operations does not show up in the execution time because we assume a relaxed memory model <ref> [16] </ref>. The next two categories apply to the Hybrid protocol only and together represent the total time taken to perform the BeginRead/EndRead and Be-ginWrite/EndWrite operation; these two categories distinguish the time software protocol operations spend waiting due to contention (ProtCont) from the time they are being serviced (Protocol).
Reference: [17] <author> Kourosh Gharachorloo, Dan Lenoski, James Laudon, Phillip Gibbons, Anoop Gupta, and John Hennessy. </author> <title> Memory consistency and event ordering in scalable shared-memory multiprocessors. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 15-26, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: The FLASH architecture provides a programmable controller at each node that can be used to efficiently support both the base and the software DSM protocol. Similar to most protocols that exploit relaxed memory models, our hybrid protocol requires the programmer to identify all synchronization operations <ref> [1, 12, 17] </ref>. In addition, the programmer has the option of identifying regions of data that exhibit coarse grain sharing by supplying annotations that delimit references to such data in between synchronization points. Data regions identified in this manner are kept coherent using the software DSM protocol. <p> Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols [3, 8, 26] typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models <ref> [1, 12, 17] </ref> to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41]. <p> However, the large coherence granularity of pages makes false sharing a major problem in Ivy. Some page-based protocols attempt to alleviate false sharing problems by exploiting a relaxed memory consistency model. For example, Munin [7] exploits release consistency (RC) <ref> [17] </ref> to efficiently support multiple writers to a single page by delaying the propagation of coherence transactions until a release synchronization (e.g., unlock). To ensure that the page returns to a consistent state, each processor is required to keep track of the changes it makes to its local copy. <p> We assume the hardware DSM component of our protocol uses the release consistency model. Therefore, the programmer is required to identify all synchronization points in a program and to provide sufficient synchronization to ensure that all references to shared data are race-free <ref> [1, 17] </ref>. We extend the above base model to allow the programmer to optionally provide extra usage information about data regions that are shared at a coarse granularity. We loosely define a region as a set of memory locations that are referenced and shared together. <p> In this example, the BeginWrite/EndWrite pair encapsulates the segment of the program in which the array locations are modified at the producer, 2 Providing usage annotations can be considered as an extension to the properly-labeled (PL) programming framework <ref> [17] </ref> used for release consistency. In addition to identifying the synchronization (as required by PL), the programmer is also required to identify all segments of race-free use for a subset of the memory locations.
Reference: [18] <author> Stephen Goldschmidt and John Hennessy. </author> <title> The accuracy of trace-driven simulations of multiprocessors. </title> <booktitle> In ACM Sigmetrics Conference on the Measurement and Modeling of Computer Systems, </booktitle> <month> May </month> <year> 1993. </year>
Reference-contexts: Furthermore, bulk transfer of data can be achieved by using the protocol processors at the source and destination nodes to transfer a series of cache lines from one memory to another in a pipelined fashion. 4.2 Simulation Environment Our performance evaluation is based on simulations using the TangoLite multiprocessor simulator <ref> [18] </ref>. Using FLASH as the base architecture, we model three different protocols | hardware cache coherence (CC), COMA, and our Hybrid protocol; these protocols are described below.
Reference: [19] <author> Eric Hagersten, Anders Landin, and Seif Haridi. </author> <title> DDM a cache-only memory architecture. </title> <journal> IEEE Computer, </journal> <volume> 25(9) </volume> <pages> 44-54, </pages> <year> 1992. </year>
Reference-contexts: While hardware DSM protocols replicate data only in the processor caches, software DSM protocols allow for replication in the local main memory thus providing a larger effective cache size that benefits applications with large working sets. This feature provides similar benefits to cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> by relieving the programmer from the need to control placement and migration of data. The above two observations suggest that merging ideas used in hardware and software DSM protocols can lead to hybrid protocols that potentially combine the benefits of each approach. <p> Compared to using large line sizes, the hybrid protocol proposed in this paper is a more robust and selective technique for exploiting coarse grain communication. Cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> use an alternative hardware coherence protocol that treats a node's local memory as an extension to the cache hierarchy and can therefore reduce remote misses that arise due to capacity or conflict in higher-level processor caches. <p> The hybrid protocol provides gains by reducing the latency of cache misses through bulk transfer and replication. Below, we compare our approach to some other techniques that can be used to reduce miss latency. One possible technique is to use cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref>. As shown by our results, the hybrid protocol achieves gains from replication that are comparable to COMA. One advantage of COMA is that it replicates all shared data while the hybrid protocol is limited to replicating region data only.
Reference: [20] <author> E. Jul, H. Levy, and N. Hutchinson. </author> <title> Fine-grained mobility in the Emerald system. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 6(1) </volume> <pages> 109-133, </pages> <month> February </month> <year> 1988. </year>
Reference-contexts: The ability to maintain coherence at a variable grain size can alleviate many of the problems (e.g., false sharing) that arise from such a mismatch. One approach is to associate the coherence granularity to the granularity of program data objects (referred to as object-based) <ref> [10, 20, 29] </ref>.
Reference: [21] <author> Pete Keleher, Alan Cox, and Willy Zwaenepoel. </author> <title> Lazy release consistency for software distributed shared memory. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 13-21, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy [28], Munin [7], LRC <ref> [13, 21] </ref>, Midway [5, 6]). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> Munin accomplishes this by duplicating a page before modifying it and comparing the dirty page to the original duplicate. These computed changes or diffs are then communicated to other copies at the next release synchronization. 2 Lazy release consistency (LRC) <ref> [21] </ref> is a more recent imple-mentation technique for page-based systems. Instead of eagerly propagating changes to all copies at each release synchronization, LRC delays communication until a processor with a copy attempts to acquire a synchronization variable (e.g., lock). <p> The more advanced optimizations in LRC lead to a more complex and higher overhead protocol than Munin. However, simulation results have shown that reduction in the number and size of messages can lead to an overall performance gain <ref> [13, 21] </ref>. 2.2.2 Protocols with Variable Size Granularity The main difficulty with software page-based systems is the mismatch between the fixed size coherence granularity and the inherent grain of sharing and communication in an application.
Reference: [22] <author> David Kranz, Kirk Johnson, Anant Agarwal, John Kubiatowicz, and Beng-Hong Lim. </author> <title> Integrating message-passing and shared-memory: Early experience. </title> <booktitle> In Proceedings of the Symposium on Principles and Practices of Parallel Programming, </booktitle> <pages> pages 54-63, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: However, as we have shown, these annotations allow the hybrid protocol to exploit bulk transfer in addition to replication and thereby achieve higher performance in some cases. Another alternative approach for achieving gains from bulk transfer is the use of explicit messages in a single address space environment <ref> [22] </ref>. With this approach explicit messages are used selectively to communicate coarse grain data in an application. Even though explicit programmer control over communication may potentially lead to larger gains than possible with our hybrid protocol, the programming effort required may be significant.
Reference: [23] <author> John Kubiatowicz and Anant Agarwal. </author> <title> Anatomy of a message in the Alewife multiprocessor. </title> <booktitle> In Proceedings of the 1993 International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1993. </year>
Reference-contexts: In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]). Finally, many recent systems are providing support for efficient bulk data transfer (e.g., Alewife <ref> [23] </ref>, FLASH [24], or the recently announced Cray T3D). The above features have the effect of enhancing potential gains and reducing relative overheads of a hybrid protocol. This paper proposes a hybrid protocol and evaluates its performance in the context of the Stanford FLASH architecture [24].
Reference: [24] <author> Jeffrey Kuskin, David Ofelt, Mark Heinrich, John Heinlein, Richard Simoni, Kourosh Gharachorloo, John Chapin, David Nakahira, Joel Baxter, Mark Horowitz, Anoop Gupta, Mendel Rosenblum, and John Hennessy. </author> <title> The Stanford FLASH multiprocessor. </title> <booktitle> In Proceedings of the 21st International Symposium on Computer Architecture, </booktitle> <pages> pages 302-313, </pages> <month> April </month> <year> 1994. </year>
Reference-contexts: Third, several recent designs provide efficient support for invoking general protocol actions at remote nodes by either utilizing processors with fast interrupt capability (e.g., MIT Alewife [2]) or providing dedicated processors to handle protocol actions (e.g., MIT *T [31], Stanford FLASH <ref> [24] </ref>). In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]). Finally, many recent systems are providing support for efficient bulk data transfer (e.g., Alewife [23], FLASH [24], or the recently announced Cray T3D). <p> either utilizing processors with fast interrupt capability (e.g., MIT Alewife [2]) or providing dedicated processors to handle protocol actions (e.g., MIT *T [31], Stanford FLASH <ref> [24] </ref>). In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]). Finally, many recent systems are providing support for efficient bulk data transfer (e.g., Alewife [23], FLASH [24], or the recently announced Cray T3D). The above features have the effect of enhancing potential gains and reducing relative overheads of a hybrid protocol. <p> protocol actions (e.g., MIT *T [31], Stanford FLASH <ref> [24] </ref>). In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]). Finally, many recent systems are providing support for efficient bulk data transfer (e.g., Alewife [23], FLASH [24], or the recently announced Cray T3D). The above features have the effect of enhancing potential gains and reducing relative overheads of a hybrid protocol. This paper proposes a hybrid protocol and evaluates its performance in the context of the Stanford FLASH architecture [24]. <p> bulk data transfer (e.g., Alewife [23], FLASH <ref> [24] </ref>, or the recently announced Cray T3D). The above features have the effect of enhancing potential gains and reducing relative overheads of a hybrid protocol. This paper proposes a hybrid protocol and evaluates its performance in the context of the Stanford FLASH architecture [24]. The hybrid protocol consists of a base cache coherence protocol with fixed size cache lines and a software DSM protocol that maintains coherence on variable size data. <p> This section describes the target multiprocessor architecture, simulation environment, and application benchmarks used in this study. The actual performance results are presented in Section 5. 4.1 Simulated Multiprocessor Architecture For our simulation study, we have chosen to model an architecture that resembles the FLASH multiprocessor <ref> [24] </ref>. Figure 3 shows the high-level organization of the FLASH architecture. The unique component in FLASH is the programmable controller that is used to support both a directory-based hardware cache coherence protocol and a message passing protocol.
Reference: [25] <author> Leslie Lamport. </author> <title> How to make a multiprocessor computer that correctly executes multiprocess programs. </title> <journal> IEEE Transactions on Computers, </journal> <volume> C-28(9):690-691, </volume> <month> September </month> <year> 1979. </year>
Reference-contexts: The protection status for each page is manipulated to trigger faults on references to invalid pages and on writes to read shared pages. The fault handlers in turn generate the appropriate coherence operations. Ivy maintains the sequential consistency model <ref> [25] </ref> which allows the system to execute any shared memory program without modifications. However, the large coherence granularity of pages makes false sharing a major problem in Ivy. Some page-based protocols attempt to alleviate false sharing problems by exploiting a relaxed memory consistency model.
Reference: [26] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Anoop Gupta, and John Hennessy. </author> <title> The directory-based cache coherence protocol for the DASH multiprocessor. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 148-159, </pages> <month> May </month> <year> 1990. </year>
Reference-contexts: Finally, we conclude in Section 7. 2 Background This section provides a brief overview of conventional hardware cache coherence and software DSM protocols. Readers familiar with DSM protocols may wish to skip to Section 3. 2.1 Hardware Cache Coherence Protocols Hardware cache coherence protocols <ref> [3, 8, 26] </ref> typically support communication and coherence at the fixed granularity of a cache line, and often exploit relaxed memory models [1, 12, 17] to hide memory latencies. In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41].
Reference: [27] <author> Dan Lenoski, James Laudon, Kourosh Gharachorloo, Wolf-Dietrich Weber, Anoop Gupta, John Hennessy, Mark Horowitz, and Monica S. Lam. </author> <title> The Stanford Dash multiprocessor. </title> <journal> IEEE Computer, </journal> <volume> 25(3) </volume> <pages> 63-79, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: Consequently, designers have attempted to support this programming model across a variety of parallel machines. Traditional tightly coupled multiprocessors support the single address space model directly in hardware and provide for data replication within processor caches (e.g., Alewife [2], Dash <ref> [27] </ref>). The low latency and high bandwidth networks used in these architectures allow for efficient communication at a fine granularity, typically in the form of fixed size cache lines. <p> /* regionID identifies the region corresponding to memory locations */ /* spanned by array A */ /* Producer process */ /* Consumer process*/ BeginWrite (regionID); WAIT (event); /* read/write array A */ BeginRead (regionID); EndWrite (regionID); /* read array A */ SIGNAL (event); EndRead (regionID); cache coherent systems (e.g., DASH <ref> [27] </ref>) that exploit a relaxed memory model. We assume the hardware DSM component of our protocol uses the release consistency model.
Reference: [28] <author> Kai Li and Paul Hudak. </author> <title> Memory coherence in shared virtual memory systems. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 7(4) </volume> <pages> 321-359, </pages> <month> November </month> <year> 1989. </year>
Reference-contexts: At the other extreme, loosely coupled multicomputers, such as workstations connected by a fast local area network, support the single address space model in software and provide caching through replication of shared data in the local main memory of each node (e.g., Ivy <ref> [28] </ref>, Munin [7], LRC [13, 21], Midway [5, 6]). The high software overhead and To appear in the Eighth ACM International Conference on Supercomputing (ICS), July 1994, Manchester, England. <p> Thus, the granularity for communication is typically chosen to be either that of fixed size pages of several kilobytes (e.g., Ivy <ref> [28] </ref>) or large variable size objects (e.g., Midway [5, 6]). The two types of architectures described above are often referred to as hardware distributed shared memory (HW-DSM) and software distributed shared memory (SW-DSM), respectively. <p> Below, we discuss each scheme in more detail. 2.2.1 Protocols with Fixed Size Granularity Fixed size granularity protocols typically maintain coherence at the granularity of physical memory pages (referred to as page-based). Ivy is among the first systems that implemented a page-based DSM protocol <ref> [28] </ref>. Ivy allows pages to be replicated among multiple readers and exploits conventional memory management hardware to maintain coherence among the multiple copies. The protection status for each page is manipulated to trigger faults on references to invalid pages and on writes to read shared pages.
Reference: [29] <author> B. Liskov. </author> <title> Distributed programming in Argus. </title> <journal> Communications of the ACM, </journal> <volume> 31(3) </volume> <pages> 300-312, </pages> <month> March </month> <year> 1988. </year>
Reference-contexts: The ability to maintain coherence at a variable grain size can alleviate many of the problems (e.g., false sharing) that arise from such a mismatch. One approach is to associate the coherence granularity to the granularity of program data objects (referred to as object-based) <ref> [10, 20, 29] </ref>.
Reference: [30] <author> Todd Mowry and Anoop Gupta. </author> <title> Tolerating latency through software-controlled prefetching in shared-memory multiprocessors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 12(2) </volume> <pages> 87-106, </pages> <month> June </month> <year> 1991. </year>
Reference-contexts: Therefore, in our opinion, providing usage annotations in the hybrid protocol provides a much simpler abstraction. As part of our future research, we hope to quantify how well our transparent approach performs relative to the explicit use of messages. Prefetching <ref> [4, 30] </ref> is another approach that may be used to hide the latency of cache misses. With increasing remote latencies, overlapping prefetches with computation becomes more difficult. Therefore, the expected gains from prefetching will mainly arise from pipelining multiple prefetches.
Reference: [31] <author> Rishiyur Nikhil, Gregory Papadopoulos, and Arvind. </author> <title> *T: A multi-threaded massively parallel architecture. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 156-167, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: Third, several recent designs provide efficient support for invoking general protocol actions at remote nodes by either utilizing processors with fast interrupt capability (e.g., MIT Alewife [2]) or providing dedicated processors to handle protocol actions (e.g., MIT *T <ref> [31] </ref>, Stanford FLASH [24]). In fact, these mechanisms are used in many of the designs to support conventional cache coherence protocols partly in software (e.g., Alewife [8], FLASH [24]).
Reference: [32] <institution> Kendall Square Research. </institution> <type> KSR1 Technical Summary. </type> <address> Waltham, MA, </address> <year> 1992. </year>
Reference-contexts: While hardware DSM protocols replicate data only in the processor caches, software DSM protocols allow for replication in the local main memory thus providing a larger effective cache size that benefits applications with large working sets. This feature provides similar benefits to cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> by relieving the programmer from the need to control placement and migration of data. The above two observations suggest that merging ideas used in hardware and software DSM protocols can lead to hybrid protocols that potentially combine the benefits of each approach. <p> Compared to using large line sizes, the hybrid protocol proposed in this paper is a more robust and selective technique for exploiting coarse grain communication. Cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> use an alternative hardware coherence protocol that treats a node's local memory as an extension to the cache hierarchy and can therefore reduce remote misses that arise due to capacity or conflict in higher-level processor caches. <p> The hybrid protocol provides gains by reducing the latency of cache misses through bulk transfer and replication. Below, we compare our approach to some other techniques that can be used to reduce miss latency. One possible technique is to use cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref>. As shown by our results, the hybrid protocol achieves gains from replication that are comparable to COMA. One advantage of COMA is that it replicates all shared data while the hybrid protocol is limited to replicating region data only.
Reference: [33] <author> Martin Rinard. </author> <title> The Design and Implementation of Jade, a high-level Portable Parallel Programming Language. </title> <type> PhD thesis, </type> <institution> Department of Computer Science, Stanford University. </institution> <note> In preparation. </note>
Reference-contexts: The data corresponding to the region is then transferred back to the requester and the requesting processor is allowed to continue once a copy of the region is created in its local memory. 4 We use a dynamic algorithm similar to that used in the Jade runtime system <ref> [33] </ref> to efficiently locate the owner of a region. 3 Since we are assuming programs with sufficient synchronization, no overlaps arise between conflicting Begin/End intervals to the same region (i.e., region references are race-free). 4 A BeginWrite is allowed to complete as soon as the copy of the region is created
Reference: [34] <author> Martin Rinard, Daniel Scales, and Monica Lam. </author> <title> Jade: A high-level, machine-independent language for parallel programming. </title> <journal> IEEE Computer, </journal> <volume> 26(6) </volume> <pages> 28-38, </pages> <year> 1993. </year>
Reference-contexts: Furthermore, debugging environments that check the correctness of annotations as well as appropriate language constructs can make it easier to supply this information. Finally, for some language designs, the correctness of annotations can be checked at runtime with negligible overhead <ref> [34] </ref>. The hybrid protocol provides gains by reducing the latency of cache misses through bulk transfer and replication. Below, we compare our approach to some other techniques that can be used to reduce miss latency. One possible technique is to use cache-only memory architectures (COMA) [19, 32, 39].
Reference: [35] <author> Edward Rothberg and Anoop Gupta. </author> <title> Techniques for improving the performance of sparse matrix factorization on multiprocessor workstations. </title> <booktitle> In Supercomputing '90, </booktitle> <pages> pages 232-243, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: The reason for choosing both Panel and Block is that they represent two distinct ways of parallelizing the Cholesky factorization <ref> [35] </ref>, a widely used kernel in linear algebra. Panel methods are more coarse grain, have simpler algorithmic structure, but generate more communication. Block methods, in contrast, exploit finer grain parallelism, are more complex algorithmically, but generate less communication volume.
Reference: [36] <author> Edward Rothberg and Anoop Gupta. </author> <title> An efficient block-oriented approach to parallel sparse cholesky factorization. </title> <type> Technical Report CSL-TR-92-533, </type> <institution> Computer Systems Lab, Stanford University, </institution> <month> July </month> <year> 1992. </year>
Reference-contexts: Synchronization in the program is through events for the source panel and a lock for the destination panel. We run the application on the input matrix BHSSTK17. In Block Cholesky <ref> [36] </ref> the matrix is represented as a set of rectangular blocks. The basic operation is to update a destination block using two source blocks.
Reference: [37] <author> Harjinder S. Sandhu, Benjamin Gamsa, and Songnian Zhou. </author> <title> The shared regions approach to software cache coherence on multiprocessors. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on the Principles and Practice of Parallel Programming (PPoPP), </booktitle> <pages> pages 229-238, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: We loosely define a region as a set of memory locations that are referenced and shared together. Our notion of a region borrows from the work on Shared Regions <ref> [37] </ref> (discussed in Section 6). A region may correspond directly to a program level data object, it may encompass only parts of a data object, or it may comprise parts of multiple data objects. <p> Dubnicki and LeBlanc [11] describe a hardware cache coherent system that dynamically adapts the effective granularity of cache lines. Compared to the hybrid protocol, the maximum granularity achieved in practice is at most several cache lines. Shared Regions (SR) <ref> [37] </ref> is another approach for providing a variable coherence granularity in systems that lack hardware cache coherence. The annotations required by our protocol are similar to those provided for SR.
Reference: [38] <author> Jaswinder Pal Singh, Wolf-Dietrich Weber, and Anoop Gupta. </author> <title> SPLASH: Stanford parallel applications for shared memory. </title> <journal> Computer Architecture News, </journal> <volume> 20(1) </volume> <pages> 5-44, </pages> <month> March </month> <year> 1992. </year>
Reference-contexts: The same transfer using individual cache line transfers takes 1904 cycles. 4.3 Benchmark Applications We present results for four different parallel applications in this study | Panel Cholesky, Block Cholesky, Ocean, and Water. All except Block Cholesky are versions of SPLASH applications <ref> [38] </ref> written in the Cool parallel programming language [9].
Reference: [39] <author> Per Stenstrom, Truman Joe, and Anoop Gupta. </author> <title> Comparative performance evaluation of cache-coherent NUMA and COMA architectures. </title> <booktitle> In Proceedings of the 19th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 80-91, </pages> <month> May </month> <year> 1992. </year>
Reference-contexts: While hardware DSM protocols replicate data only in the processor caches, software DSM protocols allow for replication in the local main memory thus providing a larger effective cache size that benefits applications with large working sets. This feature provides similar benefits to cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> by relieving the programmer from the need to control placement and migration of data. The above two observations suggest that merging ideas used in hardware and software DSM protocols can lead to hybrid protocols that potentially combine the benefits of each approach. <p> Compared to using large line sizes, the hybrid protocol proposed in this paper is a more robust and selective technique for exploiting coarse grain communication. Cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref> use an alternative hardware coherence protocol that treats a node's local memory as an extension to the cache hierarchy and can therefore reduce remote misses that arise due to capacity or conflict in higher-level processor caches. <p> COMA models a COMA-FLAT architecture <ref> [39] </ref>. Our COMA model is ideal (i.e., aggressive) in many ways. First, the COMA cache (i.e., second level cache) is modeled as an infinite cache (with the same cache line size as the processor cache). <p> The hybrid protocol provides gains by reducing the latency of cache misses through bulk transfer and replication. Below, we compare our approach to some other techniques that can be used to reduce miss latency. One possible technique is to use cache-only memory architectures (COMA) <ref> [19, 32, 39] </ref>. As shown by our results, the hybrid protocol achieves gains from replication that are comparable to COMA. One advantage of COMA is that it replicates all shared data while the hybrid protocol is limited to replicating region data only.
Reference: [40] <author> Josep Torrellas, Monica Lam, and John Hennessy. </author> <title> Shared data placement optimizations to reduce multiprocessor cache miss rates. </title> <booktitle> In Proceedings of the 1990 International Conference on Parallel Processing, pages II: </booktitle> <pages> 266-270, </pages> <month> August </month> <year> 1990. </year>
Reference-contexts: In such systems the cache line size can have a significant effect on miss rates and overall performance <ref> [14, 40, 41] </ref>. Even though larger lines can provide potential benefits due to prefetching, too large a line can either result in false sharing [14, 40] or lead to increased communication traffic for applications that exhibit poor spatial locality. <p> In such systems the cache line size can have a significant effect on miss rates and overall performance [14, 40, 41]. Even though larger lines can provide potential benefits due to prefetching, too large a line can either result in false sharing <ref> [14, 40] </ref> or lead to increased communication traffic for applications that exhibit poor spatial locality. Compared to using large line sizes, the hybrid protocol proposed in this paper is a more robust and selective technique for exploiting coarse grain communication.
Reference: [41] <author> Wolf-Dietrich Weber and Anoop Gupta. </author> <title> Cache invalidation patterns in shared-memory multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(3) </volume> <pages> 794-810, </pages> <year> 1992. </year> <month> 15 </month>
Reference-contexts: In such systems the cache line size can have a significant effect on miss rates and overall performance <ref> [14, 40, 41] </ref>. Even though larger lines can provide potential benefits due to prefetching, too large a line can either result in false sharing [14, 40] or lead to increased communication traffic for applications that exhibit poor spatial locality.
References-found: 41

