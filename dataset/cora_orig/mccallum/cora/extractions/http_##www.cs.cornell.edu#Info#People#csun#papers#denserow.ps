URL: http://www.cs.cornell.edu/Info/People/csun/papers/denserow.ps
Refering-URL: http://www.cs.cornell.edu/Info/People/csun/sun.html
Root-URL: 
Email: csun@cs.cornell.edu  
Title: Dealing with Dense Rows in the Solution of Sparse Linear Least Squares Problems 1  
Author: Chunguang Sun 
Date: May 1995.  
Address: Ithaca, NY 14853-3801  Ithaca, NY,  
Affiliation: Advanced Computing Research Institute Cornell Theory Center Cornell University  Tech.  Advanced Computing Research Institute, Cornell Theory Center, Cornell University,  
Pubnum: Report CTC95TR227,  
Abstract: Sparse linear least squares problems containing a few relatively dense rows occur frequently in practice. Straightforward solution of these problems could cause catastrophic fill and delivers extremely poor performance. This paper studies a scheme for solving such problems efficiently by handling dense rows and sparse rows separately. How a sparse matrix is partitioned into dense rows and sparse rows determines the efficiency of the overall solution process. A new algorithm is proposed to find a partition of a sparse matrix which leads to satisfactory or even optimal performance. Extensive numerical experiments are performed to demonstrate the effectiveness of the proposed scheme. A MATLAB implementation is included. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Bj orck, </author> <title> Stability analysis of the method of seminormal equations for linear least squares problems, </title> <journal> Linear Algebra Appl., </journal> <volume> 88/89 (1987), </volume> <pages> pp. 31-48. </pages>
Reference-contexts: This method is numerically backward stable. In practical applications, the matrix A is often very large and sparse. In order to solve (1) efficiently, the sparsity of A must be exploited. Extensive research has been done on the direct solution of sparse linear least squares problems <ref> [1, 6, 9, 10, 11, 12, 13] </ref>. Typically, a direct method for solving (1) involves the following steps: 1. Ordering: Find a permutation matrix P so that AP has a sparse triangular factor R. 2. Symbolic factorization: Determine the symbolic structure of R. 3.
Reference: [2] <author> T. F. Coleman, </author> <title> private communication. </title> <year> 1995. </year>
Reference-contexts: It is possible to achieve optimality to (10) by solving a sequence of least squares problems involving matrices of the form (11) <ref> [2] </ref>. In our experiments, the LP problems from Netlib are used and they can be obtained by anonymous ftp to ftp.netlib.org (directory: /lp/data). In these problems, M is sparse, and vectors ^c and ^ b are relatively dense.
Reference: [3] <author> J. A. George, </author> <title> Nested dissection of a regular finite element mesh, </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 10 (1973), </volume> <pages> pp. 345-363. </pages>
Reference-contexts: Let L be the Cholesky factor of A T A. Then L T is equal to the triangular factor of A, apart from possible sign differences in the rows. Therefore, the ordering algorithms such as minimum degree ordering [7] and nested dissection ordering <ref> [3] </ref> developed for sparse Cholesky factorization can be applied to A T A to obtain a sparse triangular factor R. Similarly, the symbolic factorization algorithm for sparse Cholesky factorization described in [5] can be applied to A T A to predict the symbolic structure of R prior to numeric factorization.
Reference: [4] <author> J. A. George and M. T. Heath, </author> <title> Solution of sparse linear least squares problems using Givens rotations, Linear Algebra and its Appl., </title> <booktitle> 34 (1980), </booktitle> <pages> pp. 69-83. </pages>
Reference-contexts: Similarly, the symbolic factorization algorithm for sparse Cholesky factorization described in [5] can be applied to A T A to predict the symbolic structure of R prior to numeric factorization. This approach is initially proposed in <ref> [4] </ref> and is now widely used for implementing the symbolic phase of the overall process for solving sparse linear least squares problems. 2 In practice, a sparse linear least squares problem frequently contains relatively dense rows. Therefore, the corresponding triangular factor R becomes nearly or completely full. <p> Experimental results are discussed in x4. Concluding remarks are contained in x5. Finally, a MATLAB implementation of our overall approach is provided in the Appendix. 2 A Solution Updating Algorithm In this section, we briefly describe an algorithm proposed in <ref> [4] </ref> for updating the solution to a sparse linear least squares problem. Let A 1 be an m 1 fi n sparse matrix with full column rank, and A 2 an m 2 fi n matrix. <p> Solve the sparse triangular system Rv = u and set z = P v. We refer the reader to <ref> [4] </ref> for a proof of the correctness of this algorithm. 3 A Partitioning Algorithm In this section, we consider the solution of a sparse linear least squares problem min kAx bk 2 ; (7) where A is an m fi n sparse matrix with full column rank and A may contain
Reference: [5] <author> J. A. George and J. W. H. Liu, </author> <title> Computer Solution of Large Sparse Positive Definite Systems, </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, NJ, </address> <year> 1981. </year> <title> [6] , Householder reflections versus Givens rotations in sparse orthogonal decomposition, Linear Algebra and its Appl., </title> <booktitle> 88/89 (1987), </booktitle> <pages> pp. </pages> <month> 223-238. </month> <title> [7] , The evolution of the minimum degree algorithm, </title> <journal> SIAM Review, </journal> <volume> 31 (1989), </volume> <pages> pp. 1-19. </pages>
Reference-contexts: Therefore, the ordering algorithms such as minimum degree ordering [7] and nested dissection ordering [3] developed for sparse Cholesky factorization can be applied to A T A to obtain a sparse triangular factor R. Similarly, the symbolic factorization algorithm for sparse Cholesky factorization described in <ref> [5] </ref> can be applied to A T A to predict the symbolic structure of R prior to numeric factorization.
Reference: [8] <author> G. Golub, </author> <title> Numerical methods for solving linear least squares problems, </title> <journal> Numer. Math., </journal> <volume> 7 (1965), </volume> <pages> pp. 206-216. </pages>
Reference-contexts: Let A be an m fi n matrix with full column rank. A well-known direct method <ref> [8] </ref> for solving (1) first computes the QR factorization A = Q R # where Q is an m fi m orthogonal matrix and R is an n fi n upper triangular matrix.
Reference: [9] <author> J. W. H. Liu, </author> <title> On general row merging schemes for sparse Givens transformations, </title> <journal> SIAM J. Sci. Stat. Comput., </journal> <volume> 7 (1986), </volume> <pages> pp. 1190-1211. </pages>
Reference-contexts: This method is numerically backward stable. In practical applications, the matrix A is often very large and sparse. In order to solve (1) efficiently, the sparsity of A must be exploited. Extensive research has been done on the direct solution of sparse linear least squares problems <ref> [1, 6, 9, 10, 11, 12, 13] </ref>. Typically, a direct method for solving (1) involves the following steps: 1. Ordering: Find a permutation matrix P so that AP has a sparse triangular factor R. 2. Symbolic factorization: Determine the symbolic structure of R. 3.
Reference: [10] <author> P. Matstoms, </author> <title> Sparse QR Factorization with Applications to Linear Least Squares Problems, </title> <type> PhD thesis, </type> <institution> Linkoping University, Sweden, </institution> <year> 1994. </year>
Reference-contexts: This method is numerically backward stable. In practical applications, the matrix A is often very large and sparse. In order to solve (1) efficiently, the sparsity of A must be exploited. Extensive research has been done on the direct solution of sparse linear least squares problems <ref> [1, 6, 9, 10, 11, 12, 13] </ref>. Typically, a direct method for solving (1) involves the following steps: 1. Ordering: Find a permutation matrix P so that AP has a sparse triangular factor R. 2. Symbolic factorization: Determine the symbolic structure of R. 3.
Reference: [11] <author> C. Sun, </author> <title> Parallel sparse orthogonal factorization on distributed-memory multiprocessors, </title> <type> Tech. </type> <institution> Report CTC93 TR162, Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> Dec. </month> <year> 1993. </year> <note> (Revised, To appear in SIAM Journal on Scientific Computing). [12] , Improved algorithms for multifrontal QR factorization. Work in preparation, </note> <year> 1995. </year> <title> [13] , Parallel solution of sparse linear least squares problems on distributed-memory multiprocessors, </title> <type> Tech. Report CTC95TR212, </type> <institution> Advanced Computing Research Institute, Center for Theory and Simulation in Science and Engineering, Cornell University, </institution> <address> Ithaca, NY, </address> <month> May </month> <year> 1995. </year> <month> 17 </month>
Reference-contexts: This method is numerically backward stable. In practical applications, the matrix A is often very large and sparse. In order to solve (1) efficiently, the sparsity of A must be exploited. Extensive research has been done on the direct solution of sparse linear least squares problems <ref> [1, 6, 9, 10, 11, 12, 13] </ref>. Typically, a direct method for solving (1) involves the following steps: 1. Ordering: Find a permutation matrix P so that AP has a sparse triangular factor R. 2. Symbolic factorization: Determine the symbolic structure of R. 3.
References-found: 9

