URL: ftp://ftp.cc.gatech.edu/pub/people/cga/rl-compare.ps.gz
Refering-URL: http://www.cs.gatech.edu/fac/Chris.Atkeson/publications.html
Root-URL: 
Email: cga@cc.gatech.edu,  carlos@cc.gatech.edu,  
Title: A Comparison of Direct and Model-Based Reinforcement Learning  
Author: Christopher G. Atkeson and Juan Carlos Santamara 
Web: http://www.cc.gatech.edu/fac/Chris.Atkeson  http://www.cc.gatech.edu/ai/students/jcs  
Address: Atlanta, GA 30332-0280  
Affiliation: College of Computing, Georgia Institute of Technology,  
Abstract: This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task: pendulum swing up. We find that in this task model-based approaches support reinforcement learning from smaller amounts of training data and efficient handling of changing goals. 
Abstract-found: 1
Intro-found: 1
Reference: <author> Atkeson, C. G. </author> <year> (1994). </year> <title> Using local trajectory optimizers to speed up global optimization in dynamic programming. </title> <editor> In Cowan, J. D., Tesauro, G., and Alspector, J., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 6, </booktitle> <pages> pages 663-670. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1996a). </year> <title> Locally weighted learning. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference-contexts: We use locally weighted learning to learn a discrete time forward model of the pendulum dynamics ( k+1 ; _ k+1 ) = f ( k ; _ k ; t k ) using all the data available <ref> (Atkeson et al., 1996a) </ref>. Before storing new data we test to see if it can already be predicted by the model. If it can, we don't add that point to the database. <p> The model-based method used locally weighted regression <ref> (Atkeson et al., 1996a) </ref> to build a model and a grid with bilinear interpolation to represent the value function. The direct reinforcement learning implementation did not learn a model, and used a CMAC to represent the Q function, a form of value function. <p> These representations use information from experience to adapt the representational structure. The representations used in this study did not change their structure during learning. Locally weighted learning could adapt architectural parameters such as a distance metric <ref> (Atkeson et al., 1996a) </ref>. Tables and CMACs can adapt their resolution in different parts of the space during learning.
Reference: <author> Atkeson, C. G., Moore, A. W., and Schaal, S. </author> <year> (1996b). </year> <title> Locally weighted learning for control. </title> <journal> Artificial Intelligence Review. </journal> <note> in press. </note>
Reference-contexts: Note 4 that the value function computed by the model-based approach should be related to the ideal Q function by: V (x) = min Q (x; u) (6) Locally weighted regression does a better job in learning a model using supervised learning than tables <ref> (Atkeson et al., 1996b) </ref> and worked better than a CMAC for model learning in our own tests. Value and Q function learning are very different, as, unlike supervised learning, values learned for a particular state change during learning.
Reference: <author> Atkeson, C. G. and Schaal, S. </author> <year> (1997). </year> <title> Learning tasks from a single demonstration. </title> <address> ICRA 97. </address>
Reference-contexts: In our view this indicates a benefit of model-based approaches. There is a large class of problems for which there is underlying simplicity, even in real implementations <ref> (Atkeson and Schaal, 1997) </ref>, and model-based approaches can take advantage of that simplicity. <p> In an actual implementation on a robot, this computation could be run on a dedicated processor and redone each second with the updated model, as is currently done with inverse kinematics and dynamics <ref> (Atkeson and Schaal, 1997) </ref>. Thus, there is no "extra" cost of planning or re-planning, as it is done in parallel with robot execution. <p> The model-based planning process finds good policies for the learned model, not the actual system. It may be the case that a plan based on a learned but still inaccurate model does not perform acceptably. This did not happen in our tests, but has happened in actual implementations <ref> (Atkeson and Schaal, 1997) </ref>. Depending on the characteristics of the controlled system, small modeling errors can lead to large policy errors. A serious challenge for direct reinforcement learning is overcoming inaccurate value or Q functions. <p> Direct reinforcement learning uses exploration to correct errors in the Q function. 3) In the case of model-based learning the planner can compensate for modeling error by building robust plans and by taking into account previous task outcomes in adjusting the plan independently of model updates <ref> (Atkeson and Schaal, 1997) </ref>. There is much more to do both in the practical development of algorithms and in the theoretical analysis of them.
Reference: <author> Baird, L. C. and Klopf, A. H. </author> <year> (1993). </year> <title> Reinforcement learning with high-dimensional, continuous actions. </title> <type> Technical Report WL-TR-93-1147, </type> <institution> Wright Laboratory, Wright-Patterson Air Force Base Ohio. </institution> <address> http://kirk.usafa.af.mil/ baird/papers/index.html. </address>
Reference: <author> Barto, A. G., Bradtke, S. J., and Singh, S. P. </author> <year> (1995). </year> <title> Learning to act using real-time dynamic programming. </title> <journal> Artificial Intelligence, 72(1):81|138. </journal>
Reference: <author> Bersini, H. and Gorrini, V. </author> <year> (1996). </year> <title> Three connectionist implementations of dynamic programming for optimal control: A preliminary comparative analysis. </title> <note> To appear in the proceedings of Nicrosp`96. </note>
Reference: <author> Boone, G. </author> <year> (1997a). </year> <title> Efficient reinforcement learning: Model-based acrobot control. </title> <address> ICRA 97. </address>
Reference-contexts: A number of researchers have focused on this case (Gullapalli, 1990; Jordan and Jacobs, 1990; Williams, 1990; Millington, 1991; Baird and Klopf, 1993; Bradtke, 1993; Bradtke et al., 1994; Doya, 1996). A related paper explores efficient reinforcement learning in the presence of discrete actions <ref> (Boone, 1997a) </ref>. We also assume that the state space is continuous. 1 We will only treat the discrete time case.
Reference: <author> Boone, G. </author> <year> (1997b). </year> <note> Minimum-time control of the acrobot. ICRA 97. </note>
Reference: <author> Bradtke, S. J. </author> <year> (1993). </year> <title> Reinforcement learning applied to linear quadratic regulation. </title> <editor> In Hanson, S. J., Cowan, J. D., and Giles, C. L., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 5, </booktitle> <pages> pages 295-302. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Bradtke, S. J., Ydstie, B. E., and Barto, A. G. </author> <year> (1994). </year> <title> Adaptive linear quadratic control using policy iteration. </title> <type> Technical Report CMPSCI Technical Report 94-49, </type> <institution> Department of Computer Science, University of Massachusetts, </institution> <address> Amherst. ftp://ftp.cs.umass.edu/pub/techrept/techreport/1994, </address> <booktitle> also published in the Proceedings of the 8th Yale Workshop on Adaptive and Learning Systems. </booktitle> <institution> Yale University, </institution> <year> 1994, </year> <pages> pp. 85-96. </pages>
Reference: <author> Craik, K. J. W. </author> <year> (1943). </year> <title> The Nature of Explanation. </title> <publisher> Cambridge University Press, </publisher> <address> Cambridge, UK. </address>
Reference-contexts: out various alternatives, conclude which is the best of them, react to future situations before they arise, utilize the knowledge of past events in dealing with the present and future, and in every way react in a much fuller, safer, and more competent manner to the emergencies which face it. <ref> (Craik, 1943) </ref> 6 Conclusions This paper compares direct reinforcement learning (no explicit model) and model-based reinforcement learning on a simple task.
Reference: <author> Crites, R. H. and Barto, A. G. </author> <year> (1996). </year> <title> Improving elevator performance using reinforcement learning. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cam-bridge, MA. </address>
Reference: <author> Doya, K. </author> <year> (1996). </year> <title> Temporal difference learning in continuous time and space. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA. http://www.hip.atr.co.jp/ doya/papers/nips8.ps.Z. </address>
Reference-contexts: A related paper explores efficient reinforcement learning in the presence of discrete actions (Boone, 1997a). We also assume that the state space is continuous. 1 We will only treat the discrete time case. A treat-ment of the continuous time case has recently been presented <ref> (Doya, 1996) </ref>. 2 Pendulum Swing Up This paper was inspired by the success of Sutton (1996) in using a function approximator (CMAC) in direct reinforcement learning of a double pendulum (acrobot) swing up maneuver.
Reference: <author> Gullapalli, V. </author> <year> (1990). </year> <title> A stochastic reinforcement learning algorithm for learning real-valued functions. </title> <booktitle> Neural Networks, </booktitle> <volume> 3 </volume> <pages> 671-692. </pages>
Reference: <author> Jordan, M. I. and Jacobs, R. A. </author> <year> (1990). </year> <title> Learning to control an unstable system with forward modeling. </title> <editor> In Touretzky, D. S., editor, </editor> <booktitle> Advances in Neural Information Processing Systems 2, </booktitle> <pages> pages 324-331. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address>
Reference: <author> Kaelbling, L. P., Littman, M. L., and Moore, A. W. </author> <year> (1996). </year> <title> Reinforcement learning: A survey. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 4 </volume> <pages> 237-285. </pages>
Reference: <author> Larson, R. E. </author> <year> (1968). </year> <title> State Increment Dynamic Programming. </title> <publisher> Elsevier, </publisher> <address> NY. </address>
Reference-contexts: Once a model or partially correct model is in hand, computing a value function using specialized techniques and representations such as state increment dynamic programming <ref> (Larson, 1968) </ref> or adaptive grid techniques (Atke-son, 1994) may also be easier than learning a Q function. It is important to distinguish Q learning from direct reinforcement learning.
Reference: <author> Millington, P. J. </author> <year> (1991). </year> <title> Associative Reinforcement Learning For Optimal Control. </title> <type> MS thesis, </type> <institution> Massachusetts Institute of Technology, Department of Aeronautics and Astronautics. </institution> <note> also published as Technical Report CSDL-T-1070, </note> <institution> The Charles Start Draper Laboratory, </institution> <address> Cambridge, MA. </address>
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1993). </year> <title> Prioritized sweeping: Reinforcement learning with less data and less real time. </title> <journal> Machine Learning, </journal> <volume> 13 </volume> <pages> 103-130. </pages>
Reference: <author> Moore, A. W. and Atkeson, C. G. </author> <year> (1995). </year> <title> The parti-game algorithm for variable resolution reinforcement learning in multidimensional state spaces. </title> <journal> Machine Learning, </journal> <volume> 21 </volume> <pages> 199-233. </pages>
Reference: <author> Rust, J. </author> <year> (1996). </year> <title> Numerical dynamic programming in economics. </title> <editor> In Amman, H., Kendrick, D., and Rust, J., editors, </editor> <booktitle> Handbook of Computational Economics. </booktitle> <publisher> North-Holland. </publisher> <address> http://thor.econ.wisc.edu. </address>
Reference-contexts: Many researchers have focused on reinforcement learning problems where a finite set of discrete actions are available on each time step (discrete decision processes (DDP) or discrete choice problems). In some cases a continuous set of actions is available (continuous decision processes (CDP) or choice problems) <ref> (Rust, 1996) </ref>. If, in addition, the one step reward function is smooth, the derivatives of the one step reward with respect to the actions exist.
Reference: <author> Santamara, J. C., Sutton, R., and Ram, A. </author> <year> (1996). </year> <title> Experiments with reinforcement learning in problems with continuous states and action spaces. </title> <type> COINS Technical Report 96-88, </type> <institution> Dept. of Computer Science, University of Massachusetts. </institution>
Reference: <author> Spong, M. W. </author> <year> (1995). </year> <title> The swing up control problem for the acrobot. </title> <journal> IEEE Control Systems Magazine, </journal> <volume> 15(1) </volume> <pages> 49-55. </pages>
Reference-contexts: A different acrobot maneuver is to move from a stable hanging down configuration to an unstable inverted vertical configuration in which the robot has zero velocity and is actively balancing itself, a more difficult maneuver which has been extensively studied in control theory <ref> (Spong, 1995) </ref>. We had a great deal of trouble implementing a direct reinforcement learning approach on the double pendulum swingup to vertical problem with continuous actions, so we are focusing on a simpler task, the single pendulum swingup, on which we can directly compare model-based and direct reinforcement learning.
Reference: <author> Standfuss, A. and Eckmiller, R. </author> <year> (1994). </year> <title> To swing up an inverted pendulum using stochastic real-valued reinforcement learning. </title> <booktitle> In 4th International Conference on Artificial Neural Networks (ICANN '94), </booktitle> <pages> pages 655-658, </pages> <address> Sorrento, Italy. </address> <publisher> Springer Verlag. </publisher> <address> http://marvin.nero.uni-bonn.de/veroeffentl-en.html. </address>
Reference-contexts: Note that this task is different from the cart-pole swingup, in which the base of the pendulum is moved horizontally and there is no motor applying torque directly to the pendulum <ref> (Standfuss and Eck-miller, 1994) </ref>.
Reference: <author> Sutton, R. </author> <year> (1996). </year> <title> Generalization in reinforcement learning: Successful examples using sparse coarse coding. </title> <editor> In Touretzky, D. S., Mozer, M. C., and Hasselmo, M. E., editors, </editor> <booktitle> Advances in Neural Information Processing Systems 8. </booktitle> <address> http://envy.cs.umass.edu/People/sutton/publications.html. </address>
Reference-contexts: The eligibilities are updated according to: e tile = 1 if the tile includes (x; u) fle tile otherwise (4) The eligibility decay rate is = 0:5. A major difference between our implementation and <ref> (Sutton, 1996) </ref> is that our trials are terminated after a fixed time. This modification was necessary due to the lack of a finite goal region. Our system never actually reaches the goal, which is an infinitely small point.
Reference: <author> Sutton, R. S. </author> <year> (1990). </year> <title> Integrated architectures for learning, planning, and reacting based on approximating dynamic programming. </title> <booktitle> In Seventh International Machine Learning Workshop, </booktitle> <pages> pages 216-224. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. http://envy.cs.umass.edu/People/sutton/publications.html. </address>
Reference-contexts: It is important to distinguish Q learning from direct reinforcement learning. It may be the case that Q learning is an efficient planning algorithm, using a model and mental simulation to plan good policies in model based reinforcement learning <ref> (Sutton, 1990, 1991a,b) </ref>.
Reference: <author> Sutton, R. S. </author> <year> (1991a). </year> <title> Dyna, an integrated architecture for learning, planning and reacting. </title> <booktitle> http://envy.cs.umass.edu/People/sutton/publications.html, Working Notes of the 1991 AAAI Spring Symposium on Integrated Intelligent Architectures pp. 151-155 and SIGART Bulletin 2, </booktitle> <pages> pp. 160-163. </pages>
Reference: <author> Sutton, R. S. </author> <year> (1991b). </year> <title> Planning by incremental dynamic programming. </title> <booktitle> In Eighth International Machine Learning Workshop, </booktitle> <pages> pages 353-357. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. http://envy.cs.umass.edu/People/sutton/publications.html. </address>
Reference: <author> Sutton, R. S., Barto, A. G., and Williams, R. J. </author> <year> (1992). </year> <title> Reinforcement learning is direct adaptive optimal control. </title> <journal> IEEE Control Systems Magazine, 12:19|22. </journal>
Reference-contexts: This paper explores the training data requirements of two kinds of reinforcement learning algorithms, direct (model-free) and indirect (model-based), when continuous actions are available. Direct reinforcement learning algorithms learn a policy or value function without explicitly representing a model of the controlled system <ref> (Sutton et al., 1992) </ref>. Model-based approaches learn an explicit model of the system simultaneously with a value function and policy (Sutton, 1990, 1991a,b; Barto et al., 1995; Kael-bling et al., 1996).
Reference: <author> Tesauro, G. </author> <year> (1995). </year> <title> Temporal difference learning and TD-gammon. </title> <journal> Communications of the ACM, </journal> <pages> pages 58|67. </pages>
Reference: <author> Watkins, C. J. C. H. and Dayan, P. </author> <year> (1992). </year> <note> Q-learning. Machine Learning, 8:279|292. </note>
Reference-contexts: On the other hand, the learning algorithm may compute for long periods of time before finding good policies, so the computing efficiency can be quite low. In a direct (model-free) reinforcement learning paradigm such as Q learning <ref> (Watkins and Dayan, 1992) </ref> applied directly to an actual robot, the robot must be moved for each time step of learning algorithm execution, so the computing efficiency in terms of number of robot movements considered is reflected directly in the data efficiency.
Reference: <author> Williams, R. J. </author> <year> (1990). </year> <title> Simple statistical gradient-following algorithms for connectionist reinforcement learning. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 229-256. </pages>
Reference: <author> Zhang, W. and Dietterich, T. </author> <year> (1995). </year> <title> A reinforcement learning approach to job-shop scheduling. </title> <booktitle> In Proceedings of the Fourteenth IJCAI. </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> San Mateo, CA. </address> <month> 8 </month>
References-found: 34

