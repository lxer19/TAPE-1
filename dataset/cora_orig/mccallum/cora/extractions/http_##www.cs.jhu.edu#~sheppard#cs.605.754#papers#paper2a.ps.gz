URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/papers/paper2a.ps.gz
Refering-URL: http://www.cs.jhu.edu/~sheppard/cs.605.754/sched.html
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Data Mining and Knowledge Discovery,  On Comparing Classifiers: A Critique of Current Research and Methods  
Author: STEVEN L. SALZBERG 
Keyword: classification, comparative studies, statistical methods  
Address: Baltimore, MD 21218, USA  
Affiliation: Department of Computer Science, Johns Hopkins University,  
Note: c 1999 Kluwer Academic Publishers, Boston. Manufactured in The Netherlands.  
Pubnum: 1,  
Email: salzberg@cs.jhu.edu  Editor:  
Date: 1-12 (1999)  
Abstract: An important component of many data mining projects is finding a good classification algorithm, a process that requires very careful thought about experimental design. If not done very carefully, comparative studies of classification and other types of algorithms can easily result in statistically invalid conclusions. This is especially true when one is using data mining techniques to analyze very large databases, which inevitably contain some statistically unlikely data. This paper describes several phenomena that can, if ignored, invalidate an experimental comparison. These phenomena and the conclusions that follow apply not only to classification, but to computational experiments in almost any aspect of data mining. The paper also discusses why comparative analysis is more important in evaluating some types of algorithms than for others, and provides some suggestions about how to avoid the pitfalls suffered by many experimental studies. 
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> D. Aha. </author> <title> Generalizing from case studies: A case study. </title> <booktitle> In Proc. Ninth Internatl. Workshop on Machine Learning, </booktitle> <pages> pages 1-10, </pages> <address> San Mateo, CA, 1992. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: (p-values) would have to be, e.g., 0.005 in order to obtain levels comparable to a single experiment using a level of 0.05. (This assumes, unrealistically, that the experiments are independent.) But few if any experimenters keep careful count of how many adjustments they consider. (Kibler and Langley [14] and Aha <ref> [1] </ref> suggest, as an alternative, that the parameter settings themselves be studied as independent variables, and that their effects be measured on artificial data. A greater problem occurs when one uses an algorithm that has been used before: that algorithm may already have been tuned on public databases.
Reference: 2. <author> William Cochran and Gertrude Cox. </author> <title> Experimental Designs. </title> <publisher> Wiley, </publisher> <address> 2nd edition, </address> <year> 1957. </year>
Reference-contexts: As pointed out by Feelders and Verkooijen [6], finding the proper statistical procedure 8 STEVEN L. SALZBERG to compare two or more classification algorithms can be quite difficult, and requires more than an introductory level knowledge of statistics. A good general reference for experimental design is Cochran and Cox <ref> [2] </ref>, and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [9]. Jensen [12, 13] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Jensen and Cohen [11] discuss some specific ways to remove optimistic statistical bias from such experiments.
Reference: 3. <author> F. </author> <title> Denton. Data mining as an industry. </title> <journal> Review of Economics and Statistics, </journal> <volume> 67 </volume> <pages> 124-127, </pages> <year> 1985. </year>
Reference-contexts: Denton <ref> [3] </ref> made similar observations about how the reviewing and publication process can skew results. Although the data mining community is much broader than the classification community, it is likely that benchmark databases will emerge, and that different researchers will test their mining techniques on them.
Reference: 4. <author> T. Dietterich. </author> <title> Statistical tests for comparing supervised learning algorithms. </title> <type> Technical report, </type> <institution> Oregon State University, Corvallis, </institution> <address> OR, </address> <year> 1996. </year>
Reference-contexts: This problem is widespread in comparative machine learning studies. (One of the authors of the study cited above has written recently, in an apparent about-face, that the paired t-test has "a high probability of Type I error ... and should never be used" <ref> [4] </ref>.) It is worth noting here that even statisticians have difficulty agreeing on the correct framework for hypothesis testing in complex experimental designs. For example, the whole framework of using alpha levels and p-values has been questioned when more than two hypotheses are under consideration [18]. 3.1.
Reference: 5. <author> U. M. Fayyad and K. B. Irani. </author> <title> Multi-interval discretization of continuous valued attributes for classification learning. </title> <booktitle> In Proc. 13th Internatl. Joint Conf. on Artificial Intelligence, </booktitle> <pages> pages 1022-1027, </pages> <address> Chambery, France, 1993. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Equally important for many problems is the representation of the data, which may vary from one study to the next even when the same basic dataset is used. For example, numeric values are sometimes converted to a discrete set of intervals, especially when using decision tree algorithms <ref> [5] </ref>. Whenever tuning takes place, every adjustment should really be considered a separate experiment.
Reference: 6. <author> A. Feelders and W. Verkooijen. </author> <title> Which method learns most from the data? In Prelim. </title> <booktitle> Papers of the Fifth Intern. Workshop on Artificial Intelligence and Statistics, </booktitle> <pages> pages 219-225, </pages> <address> Fort Lauderdale, Florida, </address> <year> 1995. </year>
Reference-contexts: Statisticians have been aware of this problem for a very long time; it is known as the multiplicity effect. At least two recent papers have focused their attention nicely on how classification researchers might address this effect <ref> [8, 6] </ref>. In particular, let ff be the probability that if no differences exist among our algorithms, we will make at least one mistake; i.e., we will find at least one significant difference. Thus ff is the percent of the time in which we (the experimenters) make an error. <p> If the N is the number of agreements and N &gt;> n, then it can be argued that our belief that the algorithms are doing the same thing should increase regardless of the pattern of disagreement. As pointed out by Feelders and Verkooijen <ref> [6] </ref>, finding the proper statistical procedure 8 STEVEN L. SALZBERG to compare two or more classification algorithms can be quite difficult, and requires more than an introductory level knowledge of statistics.
Reference: 7. <author> Arthur Flexer. </author> <title> Statistical evaluation of neural network experiments: Minimal requirements and current practice. </title> <type> Technical Report OEFAI-TR-95-16, </type> <institution> Austrian Research Institute for Artificial Intelligence, Vienna, Austria, </institution> <year> 1995. </year> <note> To appear in R. </note> <editor> Trappl (ed.), </editor> <booktitle> Proc. Thirteenth European Meeting on Cybernetics and Systems Research, </booktitle> <year> 1996. </year>
Reference-contexts: His survey found that a strikingly high percentage of new algorithms (29%) were not evaluated on any real problem at all, and that very few (only 8%) were compared to more than one alternative on real data. In the survey by Flexer <ref> [7] </ref> of experimental neural network papers, only 3 out of 43 studies in leading journals used a separate data set for parameter tuning, which leaves open the possibility that many of the reported results were overly optimistic. <p> This point will seem obvious to many experimental researchers, but the fact is that papers are still appearing in which this methodology is not followed. In the survey by Flexer <ref> [7] </ref>, only 3 out of 43 experimental papers in leading neural network journals used a separate data set for parameter tuning; the remaining 40 papers either did not explain how they adjusted parameters or else did their adjustments after using the test set.
Reference: 8. <author> O. Gascuel and G. Caraux. </author> <title> Statistical significance in inductive learning. </title> <booktitle> In Proc. of the European Conf. on Artificial Intelligence (ECAI), </booktitle> <pages> pages 435-439, </pages> <address> New York, 1992. </address> <publisher> Wiley. </publisher>
Reference-contexts: Statisticians have been aware of this problem for a very long time; it is known as the multiplicity effect. At least two recent papers have focused their attention nicely on how classification researchers might address this effect <ref> [8, 6] </ref>. In particular, let ff be the probability that if no differences exist among our algorithms, we will make at least one mistake; i.e., we will find at least one significant difference. Thus ff is the percent of the time in which we (the experimenters) make an error.
Reference: 9. <author> David Hildebrand. </author> <title> Statistical Thinking for Behavioral Scientists. </title> <publisher> Duxbury Press, </publisher> <address> Boston, MA, </address> <year> 1986. </year>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand <ref> [9] </ref>. Jensen [12, 13] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Jensen and Cohen [11] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows.
Reference: 10. <author> R. Holte. </author> <title> Very simple classification rules perform well on most commonly used datasets. </title> <journal> Machine Learning, </journal> <volume> 11(1) </volume> <pages> 63-90, </pages> <year> 1993. </year>
Reference-contexts: The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, [20] has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski [17]), to cite just two examples. Holte <ref> [10] </ref> collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them. Any new experiments on these and other UCI datasets run the risk of finding "significant" results that are no more than statistical accidents, as explained in Section 3.2. <p> It is not valid to make general statements about other datasets. The only way this might be valid would be if the UCI repository were known to represent a larger population of classification problems. In fact, though, as argued persuasively by Holte <ref> [10] </ref> and others, the UCI repository is a very limited sample of problems, many of which are quite easy for a classifier. (Many of them may represent the same concept class, for example many might be almost linearly separable, as suggested by the strong performance of the perceptron algorithm on one
Reference: 11. <author> D. Jensen and P. Cohen. </author> <title> Overfitting in inductive learning algorithms: Why it occurs and how to correct it. Under submission, </title> <year> 1996. </year>
Reference-contexts: Here I focus on design of experiments, which has been the subject of little concern in the machine learning community until recently (with some exceptions, such as [14] and <ref> [11] </ref>); this is rather unfortunate because the result is a growing number of poorly-designed experiments. <p> A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [9]. Jensen [12, 13] discusses a framework for experimental comparison of classifiers and addresses significance testing, and Jensen and Cohen <ref> [11] </ref> discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows. For each trial, the data set is copied and class labels are replaced with random class labels.
Reference: 12. <author> David Jensen. </author> <title> Knowledge discovery through induction with randomization testing. </title> <editor> In G. Piatetsky-Shapiro, editor, </editor> <booktitle> Proc. 1991 Knowledge Discovery in Databases Workshop, </booktitle> <pages> pages 148-159, </pages> <address> Menlo Park, CA, 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [9]. Jensen <ref> [12, 13] </ref> discusses a framework for experimental comparison of classifiers and addresses significance testing, and Jensen and Cohen [11] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows.
Reference: 13. <author> David Jensen. </author> <title> Labeling space: A tool for thinking about significance testing in knowledge discovery. Office of Technology Assessment, U.S. </title> <booktitle> Congress, </booktitle> <year> 1995. </year>
Reference-contexts: A good general reference for experimental design is Cochran and Cox [2], and descriptions of ANOVA and experimental design can be found in introductory texts such as Hildebrand [9]. Jensen <ref> [12, 13] </ref> discusses a framework for experimental comparison of classifiers and addresses significance testing, and Jensen and Cohen [11] discuss some specific ways to remove optimistic statistical bias from such experiments. One important innovation they discuss is randomization testing, in which one derives a reference distribution as follows.
Reference: 14. <author> D. Kibler and P. Langley. </author> <title> Machine learning as an experimental science. </title> <booktitle> In Proceedings of 1988 European Working Session on Learning, </booktitle> <pages> pages 81-92, </pages> <year> 1988. </year>
Reference-contexts: Here I focus on design of experiments, which has been the subject of little concern in the machine learning community until recently (with some exceptions, such as <ref> [14] </ref> and [11]); this is rather unfortunate because the result is a growing number of poorly-designed experiments. <p> then significance levels (p-values) would have to be, e.g., 0.005 in order to obtain levels comparable to a single experiment using a level of 0.05. (This assumes, unrealistically, that the experiments are independent.) But few if any experimenters keep careful count of how many adjustments they consider. (Kibler and Langley <ref> [14] </ref> and Aha [1] suggest, as an alternative, that the parameter settings themselves be studied as independent variables, and that their effects be measured on artificial data.
Reference: 15. <author> P.M. Murphy. </author> <title> UCI repository of machine learning databases a machine-readable data repository. </title> <institution> Maintained at the Department of Information and Computer Science, University of California, Irvine. </institution> <note> Anonymous FTP from ics.uci.edu in the directory pub/machine-learning-databases, </note> <year> 1995. </year>
Reference-contexts: This is a healthy development and it represents an important step in the maturation of the field. One indication of this maturation is the creation and maintenance of the UC Irvine repository of machine learning databases <ref> [15] </ref>, which now contains over 100 datasets that have appeared in published work. This repository makes it very easy for machine learning researchers to compare new algorithms to previous work.
Reference: 16. <author> Lutz Prechelt. </author> <title> A study of experimental evaluations of neural network algorithms: </title> <booktitle> Current research practice. In Neural Information Processing Systems 1995, </booktitle> <year> 1995. </year>
Reference-contexts: Classification research, which is a component of data mining as well as a subfield of machine learning, has always had a need for very specific, focused studies that compare algorithms carefully. The evidence to date is that good evaluations are not done nearly enough | for example, Prechelt <ref> [16] </ref> recently surveyed nearly 200 experimental papers on neural network learning algorithms and found most of ON COMPARING CLASSIFIERS 3 them to have serious experimental deficiencies.
Reference: 17. <author> N. Qian and T. Sejnowski. </author> <title> Predicting the secondary structure of globular proteins using neural network models. </title> <journal> Journal of Molecular Biology, </journal> <volume> 202 </volume> <pages> 65-884, </pages> <year> 1988. </year>
Reference-contexts: The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, [20] has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski <ref> [17] </ref>), to cite just two examples. Holte [10] collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them.
Reference: 18. <author> Adrian Raftery. </author> <title> Bayesian model selection in social research (with discussion by Andrew Gelman, </title> <editor> Donald B. Rubin, and Robert M. Hauser). In Peter Marsden, editor, </editor> <booktitle> Sociological Methodology 1995, </booktitle> <pages> pages 111-196. </pages> <address> Blackwells, Oxford, UK, </address> <year> 1995. </year>
Reference-contexts: For example, the whole framework of using alpha levels and p-values has been questioned when more than two hypotheses are under consideration <ref> [18] </ref>. 3.1. Alternative statistical tests One obvious problem with the experimental design cited above is that it only considers overall accuracy on a test set.
Reference: 19. <author> C. Schaffer. </author> <title> A conservation law for generalization performance. </title> <booktitle> In Proc. 11th Internatl. Joint Conf. on Machine Learning, </booktitle> <pages> pages 259-265, </pages> <address> Los Altos, CA, 1994. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: The same observation can be made for data mining: no single technique is likely to work best on all databases. Recent theoretical work has shown that, with certain assumptions, no classifier is always better than another one <ref> [23, 19] </ref>. However, experimental science is concerned with data that occurs in the real world, and it is not clear that these theoretical limitations are relevant. Some research on classification algorithms therefore focuses on characterizing the types of problems for which different classifiers work best.
Reference: 20. <author> T. Sejnowski and C. Rosenberg. </author> <title> Parallel networks that learn to pronounce english text. </title> <journal> Complex Systems, </journal> <volume> 1 </volume> <pages> 145-168, </pages> <year> 1987. </year>
Reference-contexts: For example, Fisher's iris data has been around for 60 years and has been used in hundreds (maybe thousands) of studies. The NetTalk dataset of English pronunciation data (introduced by Sejnowski and Rosenberg, <ref> [20] </ref> has been used in numerous experiments, as has the protein secondary structure data (introduced by Qian and Sejnowski [17]), to cite just two examples. Holte [10] collected results on 16 different datasets, and found as many as 75 different published accuracy figures for some of them.
Reference: 21. <author> Jude W. Shavlik, Raymond J. Mooney, and Geoffrey G. Towell. </author> <title> Symbolic and neural learning algorithms: An experimental comparison. </title> <journal> Machine Learning, </journal> <volume> 6 </volume> <pages> 111-143, </pages> <year> 1991. </year>
Reference-contexts: UCI repository is a very limited sample of problems, many of which are quite easy for a classifier. (Many of them may represent the same concept class, for example many might be almost linearly separable, as suggested by the strong performance of the perceptron algorithm on one well-known comparative study <ref> [21] </ref>.) Thus the evidence is strong that results on the UCI datasets do not apply to all classification problems, and the repository is not a proper "sample" of classification problems. This is not by any means to say that the UCI repository should not exist.
Reference: 22. <author> D. Wettschereck and T. Dietterich. </author> <title> An experimental comparison of the nearest-neighbor and nearest-hyperrectangle algorithms. </title> <journal> Machine Learning, </journal> <volume> 19(1) </volume> <pages> 5-28, </pages> <year> 1995. </year>
Reference-contexts: Nonetheless, examples of such studies (e.g., comparing k-NN to a much newer algorithm) are still appearing in the machine learning community <ref> [22] </ref>. On the other hand, it would be interesting if a new method were shown to beat k-NN consistently. It would also be interesting to quantify more precisely the conditions under which a particular algorithm could be expected to outperform or underperform k-NN. 2.3. <p> The use of the wrong p-value makes it even more likely that some experiments will find significance where none exists. Nonetheless, many researchers proceed with using a simple t-test to compare multiple algorithms on multiple datasets from the UCI repository; see, e.g., Wettschereck and Dietterich <ref> [22] </ref>. Although easy to conduct, the t-test is simply the wrong test for such an experimental design. The t-test assumes that the test sets for each "treatment" (each algorithm) are independent.
Reference: 23. <author> D. Wolpert. </author> <title> On the connection between in-sample testing and generalization error. </title> <journal> Complex Systems, </journal> <volume> 6 </volume> <pages> 47-94, </pages> <year> 1992. </year>
Reference-contexts: The same observation can be made for data mining: no single technique is likely to work best on all databases. Recent theoretical work has shown that, with certain assumptions, no classifier is always better than another one <ref> [23, 19] </ref>. However, experimental science is concerned with data that occurs in the real world, and it is not clear that these theoretical limitations are relevant. Some research on classification algorithms therefore focuses on characterizing the types of problems for which different classifiers work best.
References-found: 23

