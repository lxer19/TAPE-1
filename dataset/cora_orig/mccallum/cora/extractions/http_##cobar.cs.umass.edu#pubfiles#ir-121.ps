URL: http://cobar.cs.umass.edu/pubfiles/ir-121.ps
Refering-URL: http://cobar.cs.umass.edu/pubfiles/
Root-URL: 
Title: Automatic Essay Grading Using Text Categorization Techniques  
Author: Leah S. Larkey 
Web: www.cs.umass.edu/~larkey  
Address: Amherst, MA 01003-4610, USA  
Affiliation: Center for Intelligent Information Retrieval Department of Computer Science University of Massachusetts, Amherst  
Abstract: Several standard text-categorization techniques were applied to the problem of automated essay grading. Bayesian independence classifiers and k-nearest-neighbor classifiers were trained to assign scores to manually-graded essays. These scores were combined with several other summary text measures using linear regression. The classifiers and regression equations were then applied to a new set of essays. The classifiers worked very well. The agreement between the automated grader and the final manual grade was as good as the agreement between human graders. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.P. Callan, W.B. Croft, and J. Broglio. </author> <title> TREC and TIPSTER experiments with INQUERY. </title> <booktitle> Information Processing and Management, </booktitle> <pages> pages 327-343, </pages> <year> 1995. </year>
Reference-contexts: In our implementation, the similarity between a test essay and the training set was measured by the Inquery retrieval system, a probabilistic retrieval system using tf idf weighting <ref> [1] </ref>. The entire test document was submitted as a query against a database of training documents. The resulting ranking score, or belief score, was used as the similarity metric. The parameter k, the number of top-ranked documents over which to average, was tuned on the training set.
Reference: [2] <author> William S. Cooper. </author> <title> Some inconsistencies and mis-nomers in probabilistic information retrieval. </title> <booktitle> In Proceedings of the Fourteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 57-61, </pages> <year> 1991. </year>
Reference-contexts: Bayes theorem is used to estimate the probability of category membership for each category and each document. Probability estimates are based on the co-occurrence of categories and the selected features in the training corpus, and some independence assumptions <ref> [2] </ref>. The two phases of training, feature selection and training of coefficients, were carried out in manner similar to that of [6], and are described more fully below.
Reference: [3] <author> Norbert Fuhr. </author> <title> Models for retrieval with probabilistic indexing. </title> <booktitle> Information Processing and Management, </booktitle> <volume> 25(1) </volume> <pages> 55-72, </pages> <year> 1989. </year>
Reference-contexts: Bayesian independence classifiers, first proposed by Maron [10], are a type of general linear classifier (see the excellent overview in [9]). They estimate the probability that a document is a positive exemplar of a category, given the presence of certain words in the document. Fuhr <ref> [3] </ref> and Lewis [8] have explored improvements to Maron's model. Our model is similar to Lewis's, and has the following characteristics: First, a set of features (terms) is selected separately for each classifier. Bayes theorem is used to estimate the probability of category membership for each category and each document.
Reference: [4] <author> Robert Krovetz. </author> <title> Viewing morphology as an inference process. </title> <booktitle> In Proceedings of the Sixteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 191-203, </pages> <year> 1993. </year>
Reference-contexts: The two phases of training, feature selection and training of coefficients, were carried out in manner similar to that of [6], and are described more fully below. Feature Selection First, occurrences of 418 stop-words were removed from the essays, and the remaining terms were stemmed using the kstem stemmer <ref> [4] </ref>. Any stemmed terms found in at least three essays in the positive training set were feature candidates. The selection of features from this set was carried out independently for each binary classifier as follows.
Reference: [5] <author> T. Landauer, D. Laham, B. Rehder, and M. Schreiner. </author> <title> How well can passage meaning be derived without using word order? A comparison of latent semantic analysis and humans. </title> <booktitle> In Proceedings of the Nineteenth Annual Conference of the Cognitive Science Society, </booktitle> <year> 1997. </year>
Reference-contexts: Another approach in more recent work has been to use a k-nearest-neighbor algorithm to access the k essays most similar to the new essay to be graded. The new essay receives a final score which is a weighted average of grades from those similar essays <ref> [5] </ref>. Now that students routinely produce essays by typing them directly into computers, computer grading is being revisited as a practical possibility. While there is a well justified resistance to letting the computer be the sole judge of the quality of student's work, it can play a useful auxiliary role. <p> Our results differ from previous work, which always found some kind of essay length variable to be extremely important. In [12], a large proportion of the variance was always accounted for by the fourth root of the essay length, and in <ref> [5] </ref>, a vector length variable was very important. In contrast, our results only found length variables to be prominent when Bayesian classifiers were not included in the regression. <p> The numbers are very close. 5 Discussion Automated essay grading works surprisingly well. Correlations are generally in the high .70's and .80's, depending upon essay type and presumably upon the quality of the human ratings. These levels are comparable to those attained by Landauer et al. <ref> [5] </ref> and Page [12]. These correlations seem high, and are comparable to the correlations between human judges. <p> This work showed the k-nearest-neighbor approach to be distinctly inferior to both the other approaches. Recently, Landauer, et al. <ref> [5] </ref> have applied Latent Semantic Analysis in a k-nearest-neighbor approach to the problem of essay grading. They got very good results, which suggests that the use of more sophisticated features or a different similarity metric may work better.
Reference: [6] <author> Leah S. Larkey and W. Bruce Croft. </author> <title> Combining classifiers in text categorization. </title> <booktitle> In Proceedings of the 19 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 289-298, </pages> <year> 1996. </year>
Reference-contexts: Probability estimates are based on the co-occurrence of categories and the selected features in the training corpus, and some independence assumptions [2]. The two phases of training, feature selection and training of coefficients, were carried out in manner similar to that of <ref> [6] </ref>, and are described more fully below. Feature Selection First, occurrences of 418 stop-words were removed from the essays, and the remaining terms were stemmed using the kstem stemmer [4]. Any stemmed terms found in at least three essays in the positive training set were feature candidates. <p> Another interesting outcome of the parameter tuning on these data was the high value of k found for the k-nearest-neighbor classifier. In previous work done by us and others using k-nearest-neighbor classification for text, values of k on the order of 10 to 30 were found to be optimal <ref> [6, 15, 11, 13] </ref>. We were surprised to find much higher values of k to be optimal for essay grading. A reasonable explanation may be the following.
Reference: [7] <author> David Lewis. </author> <title> An evaluation of phrasal and clustered representations on a text categorization task. </title> <booktitle> In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 37-50, </pages> <year> 1992. </year>
Reference-contexts: This is based on Lewis's binary model <ref> [7] </ref>, which assigns 0's or 1's for feature weights depending upon whether terms are present or absent in a document. <p> Category cutoffs were chosen to put the correct number of training essays into each grade. This technique is known as proportional assignment <ref> [7] </ref>. These cutoff scores were then used to determine the assignment of grades from scores in the test set. <p> We were surprised to find that the tuning of our Bayesian classifiers preferred so many features. The usual guidelines are to have a ratio of 5 to 10 training samples per features, though others recommend having as many as 50 to 100 <ref> [7] </ref>. We used as many as 680 features in some of our classifiers, which seemed large, so we did some additional post hoc analyses to see how the test results varied with this parameter.
Reference: [8] <author> David D. Lewis. </author> <title> Representation and Learning in Information Retrieval. </title> <type> PhD thesis, </type> <institution> University of Mas-sachusetts, </institution> <year> 1992. </year>
Reference-contexts: Bayesian independence classifiers, first proposed by Maron [10], are a type of general linear classifier (see the excellent overview in [9]). They estimate the probability that a document is a positive exemplar of a category, given the presence of certain words in the document. Fuhr [3] and Lewis <ref> [8] </ref> have explored improvements to Maron's model. Our model is similar to Lewis's, and has the following characteristics: First, a set of features (terms) is selected separately for each classifier. Bayes theorem is used to estimate the probability of category membership for each category and each document.
Reference: [9] <author> David D. Lewis, Robert E. Shapire, James P. Callan, and Ron Papka. </author> <title> Training algorithms for linear text classifiers. </title> <booktitle> In Proceedings of the 19 th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 298-306, </pages> <year> 1996. </year>
Reference-contexts: Bayesian independence classifiers, first proposed by Maron [10], are a type of general linear classifier (see the excellent overview in <ref> [9] </ref>). They estimate the probability that a document is a positive exemplar of a category, given the presence of certain words in the document. Fuhr [3] and Lewis [8] have explored improvements to Maron's model.
Reference: [10] <author> M.E. Maron. </author> <title> Automatic indexing: An experimental inquiry. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 8 </volume> <pages> 404-417, </pages> <year> 1961. </year>
Reference-contexts: Bayesian independence classifiers, first proposed by Maron <ref> [10] </ref>, are a type of general linear classifier (see the excellent overview in [9]). They estimate the probability that a document is a positive exemplar of a category, given the presence of certain words in the document. Fuhr [3] and Lewis [8] have explored improvements to Maron's model.
Reference: [11] <author> Brij Masand, Gordon Linoff, and David Waltz. </author> <title> Classifying news stories using memory based reasoning. </title> <booktitle> In Proceedings of the Fifteenth Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, </booktitle> <pages> pages 59-65, </pages> <year> 1992. </year>
Reference-contexts: Another interesting outcome of the parameter tuning on these data was the high value of k found for the k-nearest-neighbor classifier. In previous work done by us and others using k-nearest-neighbor classification for text, values of k on the order of 10 to 30 were found to be optimal <ref> [6, 15, 11, 13] </ref>. We were surprised to find much higher values of k to be optimal for essay grading. A reasonable explanation may be the following.
Reference: [12] <author> Ellis B. </author> <title> Page. Computer grading of student prose, using modern concepts and software. </title> <journal> Journal of Experimental Education, </journal> <volume> 62(2) </volume> <pages> 127-142, </pages> <year> 1994. </year>
Reference-contexts: 1 Introduction Researchers have attempted to automate the grading of student essays since the 1960's <ref> [12] </ref>. The approach has been to define a large number of objectively measurable features in the essays, such as essay length, average word length, etc. and use multiple linear regression to try to predict the scores that human graders would give these essays. <p> following eleven features were used to characterize each document: * The number of characters in the document (Chars) * The number of words in the document (Words) * The number of different words in the document (Diffwds) * The fourth root of the number of words in the document (Page <ref> [12] </ref> finds this to be a useful predictive variable.) (Rootwds) * The number of sentences in the document (Sents) * Average word length (Wordlen=Chars/Words) * Average sentence length (Sentlen=Words/Sents) * Number of words longer than 5 characters (BW5) * Number of words longer than 6 characters (BW6) * Number of words <p> This impression is confirmed by an examination of the correlation matrix containing all the variables that went into the regression equation. Our results differ from previous work, which always found some kind of essay length variable to be extremely important. In <ref> [12] </ref>, a large proportion of the variance was always accounted for by the fourth root of the essay length, and in [5], a vector length variable was very important. In contrast, our results only found length variables to be prominent when Bayesian classifiers were not included in the regression. <p> The numbers are very close. 5 Discussion Automated essay grading works surprisingly well. Correlations are generally in the high .70's and .80's, depending upon essay type and presumably upon the quality of the human ratings. These levels are comparable to those attained by Landauer et al. [5] and Page <ref> [12] </ref>. These correlations seem high, and are comparable to the correlations between human judges. <p> This is about the same as the agreement between the two human judges on G1 and G2. Other studies do not report actual agreement between the automatic graders and humans, nor do they mention computing a discrete grade at all. Previous work, particularly by Page <ref> [12] </ref>, has had great success with text-complexity variables like those listed in section 3.3. We found these variables to be adequate only for one of the five data sets, G1. G1 was the only opinion question in the group.
Reference: [13] <author> C. Stanfill and David Waltz. </author> <title> Toward memory-based reasoning. </title> <journal> Communications of the ACM, </journal> <volume> 29(12) </volume> <pages> 1213-1228, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Another interesting outcome of the parameter tuning on these data was the high value of k found for the k-nearest-neighbor classifier. In previous work done by us and others using k-nearest-neighbor classification for text, values of k on the order of 10 to 30 were found to be optimal <ref> [6, 15, 11, 13] </ref>. We were surprised to find much higher values of k to be optimal for essay grading. A reasonable explanation may be the following.
Reference: [14] <editor> C.J. van Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, London, </address> <note> second edition, </note> <year> 1979. </year>
Reference-contexts: Any stemmed terms found in at least three essays in the positive training set were feature candidates. The selection of features from this set was carried out independently for each binary classifier as follows. Expected mutual information (EMIM) <ref> [14] </ref> was computed for each feature, and the features were rank ordered by EMIM score. From this set, the final number of features chosen for the classifier was tuned on the training set of data.
Reference: [15] <author> Yiming Yang and Christopher G. Chute. </author> <title> An application of Expert Network to clinical classification and MEDLINE indexing. </title> <booktitle> In Proceedings of the Eighteenth Annual Symposium on Computer Applications in Medical Care, </booktitle> <pages> pages 157-161, </pages> <year> 1994. </year> <month> 6 </month>
Reference-contexts: Another interesting outcome of the parameter tuning on these data was the high value of k found for the k-nearest-neighbor classifier. In previous work done by us and others using k-nearest-neighbor classification for text, values of k on the order of 10 to 30 were found to be optimal <ref> [6, 15, 11, 13] </ref>. We were surprised to find much higher values of k to be optimal for essay grading. A reasonable explanation may be the following.
References-found: 15

