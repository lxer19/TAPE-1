URL: ftp://ftp.cs.man.ac.uk/pub/ai/jls/nips_93.ps.Z
Refering-URL: http://www.cs.man.ac.uk/ai/jls/jls.html
Root-URL: http://www.cs.man.ac.uk
Title: Non-linear Statistical Analysis and Self-Organizing Hebbian Networks  
Author: Jonathan L. Shapiro and Adam Prugel-Bennett 
Address: Manchester, UK M13 9PL  
Affiliation: Department of Computer Science The University, Manchester  
Abstract: Neurons learning under an unsupervised Hebbian learning rule can perform a nonlinear generalization of principal component analysis. This relationship between nonlinear PCA and nonlinear neurons is reviewed. The stable fixed points of the neuron learning dynamics correspond to the maxima of the statistic optimized under nonlinear PCA. However, in order to predict what the neuron learns, knowledge of the basins of attractions of the neuron dynamics is required. Here the correspondence between nonlinear PCA and neural networks breaks down. This is shown for a simple model. Methods of statistical mechanics can be used to find the optima of the objective function of non-linear PCA. This determines what the neurons can learn. In order to find how the solutions are partitioned amoung the neurons, however, one must solve the dynamics. 
Abstract-found: 1
Intro-found: 1
Reference: <author> J. Hertz, A. Krogh, and R.G. Palmer. </author> <year> (1991). </year> <title> Introduction to the Theory of Neural Computation. </title> <publisher> Addison-Wesley. </publisher>
Reference-contexts: It is often said that the desired result should be p (~x) = r (~x), although for Kohonen 1-d feature maps has been shown to be p (~x) = r (~x) 2=3 <ref> (see for example, Hertz, Krogh, and Palmer 1991) </ref>. We have found that he partitioning cannot be calculated by finding the optima of the objective function. For example, in the case of weakly correlated patterns, the global maxima is the most likely pattern, whereas all of the patterns are local maxima.
Reference: <author> J. Karhunen and J. Joutsensalo. </author> <title> (1992) Nonlinear Hebbian algorithms for sinusoidal frequency estimation, </title> <booktitle> in Artificial Neural Networks, </booktitle> <volume> 2, </volume> <editor> I. Akeksander and J. Taylor, editors, North-Holland. </editor> <title> Erkki Oja. (1982) A simplified neuron model as a principal component analyzer. </title> <journal> em J. Math. Bio., </journal> <volume> 15 </volume> <month> 267-273. </month> <title> Erkki Oja. (1989) Neural networks, principal components, and subspaces. </title> <journal> Int. J. of Neural Systems, </journal> <volume> 1(1) </volume> <pages> 61-68. </pages>
Reference: <author> E. Oja, H. Ogawa, and J. Wangviwattan. </author> <title> (1992) Principal Component Analysis by homogeneous neural networks: Part II: </title> <journal> analysis and extension of the learning algorithms IEICE Trans. on Information and Systems, E75-D, </journal> <volume> 3, </volume> <pages> pp 376-382. </pages>
Reference: <author> E. Oja. </author> <title> (1993) Nonlinear PCA: </title> <booktitle> algorithms and applications, in Proceedings of World Congress on Neural Networks, </booktitle> <address> Portland, Or. </address> <year> 1993. </year>
Reference-contexts: performance of the networks? Do these networks learn to extract features of the input data which are different from those learned by linear neurons? Currently in the literature, the phrase non-linear PCA is used to describe what is learned by any non-linear generalization of Oja neurons or other PCA networks <ref> (see for example, Oja, 1993 and Taylor, 1993) </ref>. In this paper, we discuss the relationship between a particular form of non-linear Hebbian neurons (Prugel-Bennett and Shapiro, 1992) and a particular generalization of non-linear PCA (Softky and Kammen 1991).
Reference: <author> A. Prugel-Bennett and Jonathan L. Shapiro. </author> <title> (1993) Statistical Mechanics of Unsupervised Hebbian Learning. </title> <journal> J. Phys. A: </journal> <volume> 26, </volume> <pages> 2343. </pages>
Reference: <author> A. Prugel-Bennett and Jonathan L. Shapiro. </author> <title> (1994) The Partitioning Problem for Unsupervised Learning for Non-linear Neurons. </title> <journal> J. Phys. </journal> <note> A to appear. </note>
Reference: <author> T. D. Sanger. </author> <title> (1989) Optimal Unsupervised Learning in a Single-Layer Linear Feedforward Neural Network. </title> <booktitle> Neural Networks 2, </booktitle> <pages> 459-473. </pages>
Reference: <author> Jonathan L. Shapiro and A. </author> <month> Prugel-Bennett </month> <year> (1992), </year> <title> Unsupervised Hebbian Learning and the Shape of the Neuron Activation Function, </title> <booktitle> in Artificial Neural Networks, </booktitle> <volume> 2, </volume> <editor> I. Akeksander and J. Taylor, editors, </editor> <publisher> North-Holland. </publisher>
Reference: <author> W. Softky and D. </author> <month> Kammen </month> <year> (1991). </year> <title> Correlations in High Dimensional or Asymmetric Data Sets: </title> <booktitle> Hebbian Neuronal Processing. Neural Networks 4, </booktitle> <pages> pp 337-347. </pages>
Reference-contexts: In this paper, we discuss the relationship between a particular form of non-linear Hebbian neurons (Prugel-Bennett and Shapiro, 1992) and a particular generalization of non-linear PCA <ref> (Softky and Kammen 1991) </ref>. It is clear that non-linear neurons can perform very differently from linear ones. This has been shown through analysis (Prugel-Bennett and Shapiro, 1993) and in application (Karhuenen and Joutsensalo, 1992). It can also be very useful way of understanding what the neurons learn.
Reference: <author> J. Taylor, </author> <title> (1993) Forms of Memory, </title> <booktitle> in Proceedings of World Congress on Neural Networks, </booktitle> <address> Portland, Or. </address> <year> 1993. </year>
Reference-contexts: performance of the networks? Do these networks learn to extract features of the input data which are different from those learned by linear neurons? Currently in the literature, the phrase non-linear PCA is used to describe what is learned by any non-linear generalization of Oja neurons or other PCA networks <ref> (see for example, Oja, 1993 and Taylor, 1993) </ref>. In this paper, we discuss the relationship between a particular form of non-linear Hebbian neurons (Prugel-Bennett and Shapiro, 1992) and a particular generalization of non-linear PCA (Softky and Kammen 1991).
References-found: 10

