URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95571-S.ps.gz
Refering-URL: http://www.crpc.rice.edu/CRPC/softlib/TRs_online.html
Root-URL: 
Email: fshubu,markhill,larusg@cs.wisc.edu  fshamik,saltzg@cs.umd.edu  amr@cs.princeton.edu  
Title: Efficient Support for Irregular Applications on Distributed-Memory Machines  
Author: Shubhendu S. Mukherjee Shamik D. Sharma Mark D. Hill James R. Larus Anne Rogers and Joel Saltz 
Address: TR-488-95, Maryland CS-TR-3453 UMIACS-TR-95-46, Wisconsin CS TR 1271.  1210 West Dayton Street Madison, WI 53706, USA  4166 A.V. Williams Building College Park, MD 20742, USA  35 Olden Street Princeton, NJ 08544, USA  
Affiliation: Princeton CS  Computer Sciences Department University of Wisconsin-Madison  Department of Computer Science University of Maryland  Department of Computer Science Princeton University  
Date: July 1995.  
Note: Appears in ACM SIGPLAN Symposium on Principles Practice of Parallel Programming (PPoPP),  
Abstract: This paper explores three issues|partitioning, mutual exclusion, and data transfer|crucial to the efficient execution of irregular problems on distributed-memory machines. Unlike previous work, we studied the same programs running in three alternative systems on the same hardware base (a Thinking Machines CM-5): the CHAOS irregular application library, Transparent Shared Memory (TSM), and eXtensible Shared Memory (XSM). CHAOS and XSM performed equivalently for all three applications. Both systems were somewhat (13%) to significantly faster (991%) than TSM. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J.E. Barnes and P. Hut. </author> <title> A Hierarchical O(N log N) Force Calculation Algorithm. </title> <journal> Nature, </journal> <volume> 324(4) </volume> <pages> 446-449, </pages> <month> December </month> <year> 1986. </year>
Reference-contexts: Mirchandaney et al. [15] suggest protocol enhancements to Tread-Marks [6]|a distributed shared-memory system| to improve mutual exclusion and communication for irregular scientific problems. Falsafi et al. [10] describe how custom Tempest protocols improved the performance of two irregular applications (Barnes <ref> [1] </ref> and EM3D).
Reference: [2] <author> M. J. Berger and S. H. Bokhari. </author> <title> A Partitioning Strategy for PDEs across Multiprocessors. </title> <booktitle> In Proceedings of the 1985 International Conference on Parallel Processing, </booktitle> <month> August </month> <year> 1985. </year>
Reference-contexts: CHAOS supports data partitioners, such as recursive coordinate bisection (RCB) <ref> [2] </ref>, recursive spectral bisection [17], and others. It also provides loop iteration partition-ers for rules like owner-computes and almost-owner-computes. Support for Global Address Space. CHAOS implements a global address space for irregularly distributed arrays. <p> Unfortunately, these names usually do not reflect the mesh's structure. As a result, block or cyclic partitions can lead to excessive communication. To rectify this, all three implementations (CHAOS, TSM, and XSM) partition the nodes using recursive coordinate bisection (RCB) <ref> [2] </ref>, which groups related nodes. Once the nodes have been grouped, a simple partitioning scheme suffices for the edges.
Reference: [3] <author> B. R. Brooks, R. E. Bruccoleri, B. D. Olafson, D . J. States, S. Swamintathan, and M. Karplus. Charmm: </author> <title> A program for macromolecular energy, minimization, and dynamics calculation. </title> <journal> Journal of Computational Chemistry, </journal> <volume> 4(187), </volume> <year> 1983. </year>
Reference: [4] <author> Soumen Chakrabarti and Katherine Yelick. </author> <title> Implementing an Irregular Application on a Distributed Memory Multiprocessor. </title> <booktitle> In Fourth ACM SIGPLAN Symposium on Principles & Practice of Parallel Programming (PPOPP), </booktitle> <pages> pages 169-178, </pages> <month> May </month> <year> 1993. </year>
Reference-contexts: Koelbel and Mehrotra built a system, Kali [12], that is similar to PARTI [20], CHAOS' predecessor. Culler et al. [7] discuss many of the same issues as this paper in describing their improvements to EM3D running under Split-C. Chakrabarti and Yelick <ref> [4] </ref> describe parallelizing the Grobner basis problem, which is an irregular application with a finer granularity of sharing than those in this paper. Unlike this paper, previous work did not compare alternative implementations. Two recent papers address some issues on shared-memory machines.
Reference: [5] <author> Chialin Chang, Alan Sussman, and Joel Saltz. </author> <title> Support for Distributed Dynamic Data Structures in C++. </title> <institution> Technical Report CS-TR-3416 and UMIACS-TR-95-19, Computer Science Department, University of Maryland, College Park, </institution> <month> January </month> <year> 1995. </year>
Reference-contexts: Its main advantage is that it requires only clearly-defined modifications to source code. The CHAOS project is exploring ways to extend the library to a wider range of irregular applications <ref> [5] </ref>. * TSM performs well for applications whose natural partitions result in acceptable communication overhead (e.g., have good spatial and temporal shared data locality). TSM also supports any application in a straight-forward manner.
Reference: [6] <author> Alan L. Cox, Sandhya Dwarkadas, Pete Keleher, Honghui Lu, Ramakrishnan Rajamony, and Willy Zwaenepoel. </author> <title> Software Versus Hardware Shared-Memory Implementation: A Case Study. </title> <booktitle> In Proceedings of the 21st Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 106-117, </pages> <month> April </month> <year> 1994. </year>
Reference: [7] <author> D. E. Culler, A. Dusseau, S. C. Goldstein, A. Krishnamurthy, S. Lumetta, T. von Eicken, and K. Yelick. </author> <title> Parallel Programming in Split-C. </title> <booktitle> In Proceedings of Supercomputing '93, </booktitle> <pages> pages 262-273, </pages> <month> November </month> <year> 1993. </year>
Reference-contexts: Section 7 presents our conclusions. 2 Related Work Most work on irregular applications has focused on message-passing distributed-memory machines. Koelbel and Mehrotra built a system, Kali [12], that is similar to PARTI [20], CHAOS' predecessor. Culler et al. <ref> [7] </ref> discuss many of the same issues as this paper in describing their improvements to EM3D running under Split-C. Chakrabarti and Yelick [4] describe parallelizing the Grobner basis problem, which is an irregular application with a finer granularity of sharing than those in this paper.
Reference: [8] <author> Raja Das, Joel Saltz, and Reinhard von Hanxleden. </author> <title> Slicing Analysis and Indirect Accesses to Distributed Arrays. </title> <booktitle> In Sixth Annual Workshop on Languages and Compilers for Parallel Computing, </booktitle> <institution> chapter e. </institution> <note> To appear, </note> <month> August </month> <year> 1993. </year>
Reference-contexts: Nevertheless, developing a new protocol can require considerable effort to under stand a program's communication bottlenecks. Although this paper considered hand-written applications, a more important use of these systems may be as a compiler run-time system. CHAOS is already being used in this role <ref> [8] </ref>. 11 Acknowledgements We are indebted to the members of the Wiscon-sin Wind Tunnel 3 and CHAOS 4 projects for help with Blizzard, CHAOS, and applications.
Reference: [9] <author> Raja Das, Mustafa Uysal, Joel Saltz, and Yuan-Shin Hwang. </author> <title> Communication Optimizations for Irregular Scientific Computations on Distributed Memory Architectures. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 22(3) </volume> <pages> 462-479, </pages> <month> Septem-ber </month> <year> 1994. </year>
Reference-contexts: A coherence protocol manages the caches and ensures that all processors see a consistent view of memory. However, when a machine's protocol does not match a program's sharing pattern, the protocol can cause excessive communication and overhead. The CHAOS system is a well-proven library that supports irregular applications <ref> [9] </ref> and mitigates the problems of message-passing machines. CHAOS offers parallel data partitioners, a global address space for distributed arrays , and operations to move data between processors. <p> It greatly eases the task of programming irregular applications by hiding communication and buffer management and by providing a portable framework for programming these applications. Previous research showed that CHAOS 1 achieved good speedups on message-passing machines for irregular applications <ref> [9, 16, 24] </ref>. In this paper, we use an implementation of CHAOS on a Thinking Machines CM-5. Recent research in computer architecture has led to another alternative: hybrid shared-memory and message-passing machines that offer programmers the opportunity to select coherence protocols and fall back to message-passing communication [13, 14, 19].
Reference: [10] <author> Babak Falsafi, Alvin Lebeck, Steven Reinhardt, Ioannis Schoinas, Mark D. Hill, James Larus, Anne Rogers, and David Wood. </author> <title> Application-Specific Protocols for User-Level Shared Memory. </title> <booktitle> In Proceedings of Supercomputing '94, </booktitle> <pages> pages 380-389, </pages> <month> November </month> <year> 1994. </year> <note> 3 URL http://www.cs.wisc.edu/~wwt 4 URL http://www.cs.umd.edu/projects/hpsl.html </note>
Reference-contexts: Mirchandaney et al. [15] suggest protocol enhancements to Tread-Marks [6]|a distributed shared-memory system| to improve mutual exclusion and communication for irregular scientific problems. Falsafi et al. <ref> [10] </ref> describe how custom Tempest protocols improved the performance of two irregular applications (Barnes [1] and EM3D). <p> Inappropriate accesses (e.g., a store into a ReadOnly block) generate faults that are vectored to a user-level handler. The Tempest Interface Specification [18] and several other papers discuss Tempest in more detail <ref> [10, 11, 19] </ref>. TSM uses a COMA-like transparent shared-memory cache-coherence protocol called stache [19]. The stache protocol uses a fraction of the local memory as a large, fully-associative cache to hold data evicted from the hardware cache. <p> We can improve this implementation further by writing an update protocol that captures the sharing of blocks in array x during the first iteration and directly sends updates before the inner loop in subsequent iterations. Falsafi et al. <ref> [10] </ref> discuss several flavors of custom update protocols. 5 Results This section describes how we ran three irregular applications|unstructured, moldyn, and DSMC| using three alternative systems: CHAOS, transparent shared memory (TSM) on Tempest, and extensible shared memory (XSM) on Tempest.
Reference: [11] <author> Mark D. Hill, James R. Larus, and David A. Wood. </author> <note> Tempest: </note>
Reference-contexts: Recent research in computer architecture has led to another alternative: hybrid shared-memory and message-passing machines that offer programmers the opportunity to select coherence protocols and fall back to message-passing communication [13, 14, 19]. The Wisconsin Wind Tunnel project's approach is a portable, user-level interface called Tempest <ref> [11, 19, 18] </ref>, which provides message-passing communication and mechanisms to construct shared-memory protocols. In particular, Tempest provides programs with the novel ability to copy and move data without changing its address (renaming it). In this paper, we use an implementation of Tempest called Blizzard [23] that runs on a CM-5. <p> Tempest is designed so that it can be supported on many platforms, providing portability across these systems. The Blizzard system implements the Tempest substrate on a Thinking Machines CM-5 and is being ported to the Wisconsin COW (Cluster of Workstations) <ref> [11] </ref>. for (i = 0; i &lt; number timesteps; i++) f for (j = start edge; j &lt;= end edge; j++) f n1 = edge [j].left node; n2 = edge [j].right node; w = f (n1, n2, j); lock (y lock [n1]); y [n1] + = g1 (x [n1], x [n2], <p> Inappropriate accesses (e.g., a store into a ReadOnly block) generate faults that are vectored to a user-level handler. The Tempest Interface Specification [18] and several other papers discuss Tempest in more detail <ref> [10, 11, 19] </ref>. TSM uses a COMA-like transparent shared-memory cache-coherence protocol called stache [19]. The stache protocol uses a fraction of the local memory as a large, fully-associative cache to hold data evicted from the hardware cache.
References-found: 11

