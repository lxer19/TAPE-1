URL: http://www.cs.berkeley.edu/~russell/papers/aij-cnt.ps
Refering-URL: http://www.cs.berkeley.edu/~russell/publications.html
Root-URL: 
Title: Rationality and Intelligence  
Author: Stuart Russell 
Address: Berkeley, CA 94720, USA  
Affiliation: Computer Science Division University of California  
Abstract: The long-term goal of our field is the creation and understanding of intelligence. Productive research in AI, both practical and theoretical, benefits from a notion of intelligence that is precise enough to allow the cumulative development of robust systems and general results. The concept of rational agency has long been considered a leading candidate to fulfill this role. This paper outlines a gradual evolution in the formal conception of rationality that brings it closer to our informal conception of intelligence and simultaneously reduces the gap between theory and practice. Some directions for future research are indicated.
Abstract-found: 1
Intro-found: 1
Reference: [ Agre and Chapman, 1987 ] <author> Philip E. Agre and David Chapman. Pengi: </author> <title> an implementation of a theory of activity. </title> <booktitle> In Proceedings of the Tenth International Joint Conference on Artificial Intelligence (IJCAI-87), </booktitle> <pages> pages 268-272, </pages> <address> Milan, Italy, August 1987. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: for example) helps in three ways: first, it allows us to view such cognitive faculties as planning and reasoning as occurring in the service of finding the right thing to do; second, it encompasses rather than excludes the position that systems can do the right thing without such cognitive faculties <ref> [ Agre and Chapman, 1987; Brooks, 1989 ] </ref> ; third, it allows more freedom to consider various specifications, boundaries, and interconnections of subsystems.
Reference: [ Bacchus and Grove, 1995 ] <author> Fahiem Bacchus and Adam Grove. </author> <title> Graphical models for preference and utility. </title> <booktitle> In Proceedings of the Eleventh Conference on Uncertainty in Artificial Intelligence (UAI-95), </booktitle> <pages> pages 3-10, </pages> <address> Montreal, 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways [ Keeney and Raiffa, 1976 ] , yet such methods have made few inroads into AI (but see <ref> [ Wellman, 1985; Bacchus and Grove, 1995 ] </ref> ). We also have little idea how to specify utility over time, and although the question has been raised often, we do not have a satisfactory understanding of the relationship 5 between goals and utility.
Reference: [ Breese and Fehling, 1990 ] <author> J. S. Breese and M. R. Fehling. </author> <title> Control of problem-solving: Principles and architecture. </title> <editor> In R. D. Shachter, T. Levitt, L. Kanal, and J. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 4. </booktitle> <address> Elsevier/North-Holland, Amsterdam, London, New York, </address> <year> 1990. </year>
Reference: [ Brooks, 1989 ] <author> R. A. Brooks. </author> <title> Engineering approach to building complete, </title> <booktitle> intelligent beings. Proceedings of the SPIEThe International Society for Optical Engineering, </booktitle> <volume> 1002 </volume> <pages> 618-625, </pages> <year> 1989. </year>
Reference-contexts: for example) helps in three ways: first, it allows us to view such cognitive faculties as planning and reasoning as occurring in the service of finding the right thing to do; second, it encompasses rather than excludes the position that systems can do the right thing without such cognitive faculties <ref> [ Agre and Chapman, 1987; Brooks, 1989 ] </ref> ; third, it allows more freedom to consider various specifications, boundaries, and interconnections of subsystems.
Reference: [ Brooks, 1991 ] <author> R. A. Brooks. </author> <title> Intelligence without representation. </title> <journal> Artificial Intelligence, </journal> <volume> 47(1-3):139-159, </volume> <year> 1991. </year>
Reference-contexts: That Rod Brooks's 1991 Computers and Thought lecture was titled Intelligence without Reason (see also <ref> [ Brooks, 1991 ] </ref> ) emphasizes the fact that reasoning is (perhaps) a derived property of agents that might, or might not, be a good implementation scheme to achieve rational behaviour. Justifying the cognitive structures that many AI researchers take for granted is not an easy problem.
Reference: [ Carnap, 1950 ] <author> Rudolf Carnap. </author> <title> Logical Foundations of Probability. </title> <publisher> University of Chicago Press, </publisher> <address> Chicago, Illinois, </address> <year> 1950. </year>
Reference-contexts: In the logical view of rationality, learning has received almost no attentionindeed, Newell's analysis precludes learning at the knowledge level. In the decision-theoretic view, Bayesian updating provides a model for rational learning, but this pushes the question back to the prior <ref> [ Carnap, 1950 ] </ref> . The question of rational priors, particularly for expressive representation languages, remains unsettled. Another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions.
Reference: [ Cassandra et al., 1994 ] <author> A. R. Cassandra, L. P. Kaelbling, and M. L. Littman. </author> <title> Acting optimally in partially observable stochastic domains. </title> <booktitle> In Proceedings of the Twelfth National Conference on Artificial Intelligence (AAAI-94), </booktitle> <pages> pages 1023-1028, </pages> <address> Seattle, Washington, </address> <month> August </month> <year> 1994. </year> <note> AAAI Press. </note>
Reference: [ Cherniak, 1986 ] <author> C. Cherniak. </author> <title> Minimal Rationality. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference: [ Davis, 1980 ] <author> Randall Davis. </author> <title> Meta-rules: reasoning about control. </title> <journal> Artificial Intelligence, </journal> <volume> 15(3) </volume> <pages> 179-222, </pages> <month> December </month> <year> 1980. </year>
Reference-contexts: TEIRESIAS <ref> [ Davis, 1980 ] </ref> established the idea that explicit, domain-specific metaknowledge was an important aspect of expert system creation. Thus, metaknowledge is a sort of extra domain knowledge, over and above the object-level domain knowledge, that one has to add to an AI system to get it to work well.
Reference: [ Dean and Boddy, 1988 ] <author> Thomas Dean and Mark Boddy. </author> <title> An analysis of time-dependent planning. </title> <booktitle> In Proceedings of the Seventh National Conference on Artificial Intelligence (AAAI-88), </booktitle> <pages> pages 49-54, </pages> <address> St. Paul, Minnesota, 21-26 August 1988. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: This method was implemented for two-player games, two-player games with chance nodes, and single-agent search. In each case, the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional, highly-engineered algorithms. Another general class of metareasoning problems arises with anytime <ref> [ Dean and Boddy, 1988 ] </ref> or flexible [ Horvitz, 1987 ] algorithms, which are algorithms designed to return results whose quality varies with the amount of time allocated to computation.
Reference: [ Dean and Kanazawa, 1989 ] <author> Thomas Dean and Keiji Kanazawa. </author> <title> A model for reasoning about persistence and causation. </title> <journal> Computational Intelligence, </journal> <volume> 5(3) </volume> <pages> 142-150, </pages> <year> 1989. </year> <month> 23 </month>
Reference-contexts: Since most real-world applications are partially observable, nondeterministic, dynamic, and continuous, the lack of emphasis is somewhat surprising. There are, however, several new bricks under construction. For example, dynamic probabilistic networks (DPNs) <ref> [ Dean and Kanazawa, 1989 ] </ref> provide a mechanism to maintain beliefs about the current state of a dynamic, partially observable, nondeterministic environment, and to project forward the effects of actions.
Reference: [ Dean et al., 1995 ] <editor> Thomas L. Dean, John Aloimonos, and James F. Allen. </editor> <booktitle> Artificial Intelligence: Theory and Practice. </booktitle> <address> Benjamin/Cummings, Redwood City, California, </address> <year> 1995. </year>
Reference-contexts: The agent-based view of AI has moved quickly from workshops on situatedness and embed-dedness to mainstream textbooks <ref> [ Russell and Norvig, 1995; Dean et al., 1995 ] </ref> and buzzwords 3 in Newsweek. Rational agents, loosely speaking, are agents whose actions make sense from the point of view of the information possessed by the agent and its goals (or the task for which it was designed).
Reference: [ Dennett, 1986 ] <author> Daniel C. Dennett. </author> <title> The moral first aid manual. Tanner lectures on human values, </title> <institution> University of Michigan, </institution> <year> 1986. </year>
Reference-contexts: The requirement that policies be feasible for limited agents was discussed extensively by Cherniak [ 1986 ] and Harman [ 1983 ] . A philosophical proposal generally consistent with the notion of bounded optimality can be found in the Moral First Aid Manual <ref> [ Dennett, 1986 ] </ref> . Dennett explicitly discusses the idea of reaching an optimum within the space of feasible decision procedures, using as an example the Ph.D. admissions procedure of a philosophy department.
Reference: [ Doyle and Patil, 1991 ] <author> J. Doyle and R. S. Patil. </author> <title> Two theses of knowledge representation: language restrictions, taxonomic classification, and the utility of representation services. </title> <journal> Artificial Intelligence, </journal> <volume> 48(3) </volume> <pages> 261-297, </pages> <month> April </month> <year> 1991. </year>
Reference: [ Ephrati and Rosenschein, 1991 ] <author> E. Ephrati and J. S. Rosenschein. </author> <title> The clarke tax as a consensus mechanism among automated agents. </title> <booktitle> In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <volume> volume 1, </volume> <pages> pages 173-178, </pages> <address> Anaheim, California, July 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: Thus, one possible way to control complexity is to constrain the negotiation problem so that optimal decisions can be made easily. For example, the Clarke Tax mechanism can be used to ensure that the best policy for each agent is simply to state its preferences truthfully <ref> [ Ephrati and Rosenschein, 1991 ] </ref> . Of course, this approach does not necessarily result in optimal behaviour by the ensemble of agents; nor does it solve the problem of complexity in interacting with the rest of the environment.
Reference: [ Forbes et al., 1995 ] <author> Jeff Forbes, Tim Huang, Keiji Kanazawa, and Stuart Russell. </author> <title> The BATmobile: Towards a Bayesian automated taxi. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models [ Lauritzen, 1995; Russell et al., 1995 ] . The Bayesian Automated Taxi (a.k.a. BATmobile) project <ref> [ Forbes et al., 1995 ] </ref> is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Ginsberg and Geddis, 1991 ] <author> M. L. Ginsberg and D. F. Geddis. </author> <booktitle> Is there any need for domain-dependent control information? In Proceedings of the Ninth National Conference on Artificial Intelligence (AAAI-91), </booktitle> <volume> volume 1, </volume> <pages> pages 452-457, </pages> <address> Anaheim, California, July 1991. </address> <publisher> AAAI Press. </publisher>
Reference-contexts: On the other hand, in the work on rational metar-easoning described above, it is clear that the metatheory describing the effects of computations is domain-independent <ref> [ Russell and Wefald, 1991a; Ginsberg and Geddis, 1991 ] </ref> . In principle, no additional domain knowledge is needed to assess the benefits of a computation. In practice, metar-easoning from first principles can be very expensive.
Reference: [ Good, 1971 ] <author> I. J. </author> <title> Good. Twenty-seven principles of rationality. </title> <editor> In V. P. Godambe and D. A. Sprott, editors, </editor> <booktitle> Foundations of Statistical Inference, </booktitle> <pages> pages 108-141. </pages> <publisher> Holt, Rinehart, Winston, </publisher> <address> Toronto, </address> <year> 1971. </year>
Reference: [ Harman, 1983 ] <author> Gilbert H. Harman. </author> <title> Change in View: </title> <booktitle> Principles of Reasoning. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1983. </year>
Reference: [ Horvitz and Breese, 1990 ] <author> E.J. Horvitz and J.S. Breese. </author> <title> Ideal partition of resources for metar-easoning. </title> <type> Technical Report KSL-90-26, </type> <institution> Knowledge Systems Laboratory, Stanford University, Stanford, California, </institution> <month> February </month> <year> 1990. </year>
Reference: [ Horvitz, 1987 ] <author> E. J. Horvitz. </author> <title> Problem-solving design: Reasoning about computational value, trade-offs, and resources. </title> <booktitle> In Proceedings of the Second Annual NASA Research Forum, </booktitle> <pages> pages 26-43, </pages> <address> Moffett Field, California, </address> <year> 1987. </year> <institution> NASA Ames Research Center. </institution> <month> 24 </month>
Reference-contexts: In each case, the same general metareasoning scheme resulted in efficiency improvements of roughly an order of magnitude over traditional, highly-engineered algorithms. Another general class of metareasoning problems arises with anytime [ Dean and Boddy, 1988 ] or flexible <ref> [ Horvitz, 1987 ] </ref> algorithms, which are algorithms designed to return results whose quality varies with the amount of time allocated to computation.
Reference: [ Horvitz, 1989 ] <author> E. J. Horvitz. </author> <title> Reasoning about beliefs and actions under computational resource constraints. </title> <editor> In Laveen N. Kanal, Tod S. Levitt, and John F. Lemmer, editors, </editor> <booktitle> Uncertainty in Artificial Intelligence 3, </booktitle> <pages> pages 301-324. </pages> <address> Elsevier/North-Holland, Amsterdam, London, New York, </address> <year> 1989. </year>
Reference: [ Howard, 1966 ] <author> Ronald A. Howard. </author> <title> Information value theory. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-2:22-26, </volume> <year> 1966. </year>
Reference-contexts: The basic idea is that object-level computations are actions with costs (the passage of time) and benefits (improvements in decision quality). A rational metalevel selects computations according to their expected utility. Rational metareasoning has as a precursor the theory of information value <ref> [ Howard, 1966 ] </ref> the notion that one can calculate the decision-theoretic value of acquiring an additional piece of information by simulating the decision process that would be followed given each possible outcome of the information request, thereby estimating the expected improvement in decision quality averaged over those outcomes.
Reference: [ Kalman, 1960 ] <author> R. E. </author> <title> Kalman. A new approach to linear filtering and prediction problems. </title> <journal> Journal of Basic Engineering, </journal> <pages> pages 35-46, </pages> <month> March </month> <year> 1960. </year>
Reference-contexts: Also, the rapid improvement in the speed and accuracy of computer vision systems has made interfacing with continuous physical environments more practical. In particular, the application of Kalman filtering <ref> [ Kalman, 1960 ] </ref> , a widely used technique in control theory, allows robust and efficient tracking of moving objects; DPNs extend Kalman filtering to allow more general representations of world state.
Reference: [ Kearns et al., 1992 ] <author> M. Kearns, R. Schapire, , and L. Sellie. </author> <title> Toward efficient agnostic learning. </title> <booktitle> In Proceedings of the Fifth Annual ACM Workshop on Computational Learning Theory (COLT-92), </booktitle> <address> Pittsburgh, Pennsylvania, July 1992. </address> <publisher> ACM Press. </publisher>
Reference-contexts: Results from computational learning theory, particularly in the agnostic learning model <ref> [ Kearns et al., 1992 ] </ref> , can provide learning methods with the required properties.
Reference: [ Keeney and Raiffa, 1976 ] <author> Ralph L. Keeney and Howard Raiffa. </author> <title> Decisions with Multiple Objectives: Preferences and Value Tradeoffs. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1976. </year>
Reference-contexts: Another aspect of perfect rationality that is lacking is the development of a suitable body of techniques for the specification of utility functions. In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways <ref> [ Keeney and Raiffa, 1976 ] </ref> , yet such methods have made few inroads into AI (but see [ Wellman, 1985; Bacchus and Grove, 1995 ] ).
Reference: [ Kumar and Varaiya, 1986 ] <author> P.R. Kumar and Pravin Varaiya. </author> <title> Stochastic systems: Estimation, identification, and adaptive control. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1986. </year>
Reference-contexts: In the decision-theoretic tradition, the design of calculatively rational agents has largely gone on outside AIfor example, in stochastic optimal control theory <ref> [ Kumar and Varaiya, 1986 ] </ref> . Representations have usually been very impoverished (state-based rather than sentential) and solvable problems have been either very small or very specialized.
Reference: [ Laird et al., 1986 ] <author> J. E. Laird, P. S. Rosenbloom, and A. Newell. </author> <title> Chunking in Soar: the anatomy of a general learning mechanism. </title> <journal> Machine Learning, </journal> <volume> 1 </volume> <pages> 11-46, </pages> <year> 1986. </year>
Reference-contexts: Several agent architectures including SOAR <ref> [ Laird et al., 1986 ] </ref> use compilation to speed up all forms of problem solving.
Reference: [ Lauritzen, 1995 ] <author> S. L. Lauritzen. </author> <title> The EM algorithm for graphical association models with missing data. </title> <journal> Computational Statistics and Data Analysis, </journal> <volume> 19 </volume> <pages> 191-201, </pages> <year> 1995. </year>
Reference-contexts: Recently, Parr and Russell [ 1995 ] , among others, have had some success in applying reinforcement learning to partially observable environments. Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> . The Bayesian Automated Taxi (a.k.a. BATmobile) project [ Forbes et al., 1995 ] is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Levesque and Brachman, 1987 ] <author> H. J. Levesque and R. J. Brachman. </author> <title> Expressiveness and tractability in knowledge representation and reasoning. </title> <journal> Computational Intelligence, </journal> <volume> 3(2) </volume> <pages> 78-93, </pages> <month> May </month> <year> 1987. </year>
Reference-contexts: This position was expounded in two fascinating Computers and Thought lectures given by Hector Levesque in 1985 <ref> [ Levesque, 1986; Levesque and Brachman, 1987 ] </ref> and by Henry Kautz in 1989. The accompanying research results on tractable sublanguages are perhaps best seen as indications of where complexity may be an issue rather than as a solution to the problem of complexity.
Reference: [ Levesque, 1986 ] <author> H. J. Levesque. </author> <title> Making believers out of computers. </title> <journal> Artificial Intelligence, </journal> <volume> 30(1) </volume> <pages> 81-108, </pages> <month> October </month> <year> 1986. </year>
Reference-contexts: This position was expounded in two fascinating Computers and Thought lectures given by Hector Levesque in 1985 <ref> [ Levesque, 1986; Levesque and Brachman, 1987 ] </ref> and by Henry Kautz in 1989. The accompanying research results on tractable sublanguages are perhaps best seen as indications of where complexity may be an issue rather than as a solution to the problem of complexity.
Reference: [ Matheson, 1968 ] <author> J. E. Matheson. </author> <title> The economic value of analysis and computation. </title> <journal> IEEE Transactions on Systems Science and Cybernetics, </journal> <volume> SSC-4(3):325-332, </volume> <year> 1968. </year>
Reference: [ Megiddo and Wigderson, 1986 ] <author> N. Megiddo and A. Wigderson. </author> <title> On play by means of computing machines. </title> <editor> In Joseph Y. Halpern, editor, </editor> <booktitle> Theoretical Aspects of Reasoning about Knowledge: Proceedings of the 1986 Conference (TARK-86), </booktitle> <pages> pages 259-274, </pages> <address> Monterey, California, March 19-22 1986. </address> <publisher> IBM and AAAI, Morgan Kaufmann. </publisher> <pages> 25 </pages>
Reference-contexts: This leads to different results because it limits the ability of each agent to do unlimited simulation of the other, who is also doing unlimited simulation of the first, and so on. Even 11 the requirement of computability makes a significant difference <ref> [ Megiddo and Wigderson, 1986 ] </ref> . Bounds on the complexity of players have also become a topic of intense interest.
Reference: [ Minton, 1996 ] <author> Steven Minton. </author> <title> Is there any need for domain-dependent control information? a reply. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI-96), </booktitle> <volume> volume 1, </volume> <pages> pages 855-862, </pages> <address> Portland, Oregon, </address> <month> August </month> <year> 1996. </year> <note> AAAI Press. </note>
Reference-contexts: In practice, metar-easoning from first principles can be very expensive. To avoid this, the results of metalevel analysis for particular domains can be compiled into domain-specific metaknowledge, or such knowledge can be learned directly from experience [ Russell and Wefald, 1991a, Chapter 6 ] , <ref> [ Minton, 1996 ] </ref> . This view of emerging computational expertise leads to a fundamental insight into intelligence namely, that there is an interesting sense in which algorithms are not a necessary part of AI systems.
Reference: [ Newell, 1982 ] <author> Allen Newell. </author> <title> The knowledge level. </title> <journal> Artificial Intelligence, </journal> <volume> 18(1) </volume> <pages> 82-127, </pages> <year> 1982. </year>
Reference-contexts: The point is that perfectly rational behaviour is a well-defined function of E and U, which I will call the task environment. The problem of computing this function is addressed below. The theoretical role of perfect rationality within AI is well-described by Newell's paper on the Knowledge Level <ref> [ Newell, 1982 ] </ref> . Knowledge-level analysis of AI systems relies on an assumption of perfect rationality. It can be used to establish an upper bound on the performance of any possible system, by establishing what a perfectly rational agent would do given the same knowledge.
Reference: [ Papadimitriou and Yannakakis, 1994 ] <author> C. H. Papadimitriou and M. Yannakakis. </author> <title> On complexity as bounded rationality. </title> <booktitle> In Symposium on Theory of Computation (STOC-94), </booktitle> <year> 1994. </year>
Reference: [ Parr and Russell, 1995 ] <author> Ronald Parr and Stuart Russell. </author> <title> Approximating optimal policies for partially observable stochastic domains. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <address> Montreal, Canada, August 1995. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Russell and Norvig, 1995 ] <author> Stuart J. Russell and Peter Norvig. </author> <title> Artificial Intelligence: A Modern Approach. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, New Jersey, </address> <year> 1995. </year>
Reference-contexts: The agent-based view of AI has moved quickly from workshops on situatedness and embed-dedness to mainstream textbooks <ref> [ Russell and Norvig, 1995; Dean et al., 1995 ] </ref> and buzzwords 3 in Newsweek. Rational agents, loosely speaking, are agents whose actions make sense from the point of view of the information possessed by the agent and its goals (or the task for which it was designed). <p> It is possible to define some basic properties of task environments that, together with the complexity of the problem, lead to identifiable requirements on the corresponding rational agent designs <ref> [ Russell and Norvig, 1995, Ch. 2 ] </ref> . The principal properties are whether the environment is fully observable or partially observable, whether it is deterministic or stochastic, whether it is static (i.e., does not change except when the agent acts) or dynamic, and whether it is discrete or continuous.
Reference: [ Russell and Subramanian, 1995 ] <author> Stuart J. Russell and Devika Subramanian. </author> <title> Provably bounded-optimal agents. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 3, </volume> <month> May </month> <year> 1995. </year>
Reference-contexts: Specifying that actions or computations be rational is of no use if no real agents can fulfill the specification. The designer controls the program. In <ref> [ Russell and Subramanian, 1995 ] </ref> , the notion of feasibility for a given machine is introduced to describe the set of all agent functions that can be implemented by some agent program running on that machine. <p> The O () notation was developed to deal with this and provides a much more robust way to describe complexity that is independent of machine speeds and implementation details. This robustness is also essential in allowing complexity results to develop cumulatively. In <ref> [ Russell and Subramanian, 1995 ] </ref> , the corresponding notion is asymptotic bounded optimality (ABO). As with classical complexity, we can define both average-case and worst-case ABO, where case here means the environment. <p> The components fulfill certain behavioural specifications and interact in well-defined ways. To produce a composite bounded-optimal design, the optimization problem involves allocating execution time to components [ Zilberstein and Russell, 1996 ] or arranging the order of execution of the components <ref> [ Russell and Subramanian, 1995 ] </ref> to maximize overall performance. As illustrated earlier in the discussion of universal ABO algorithms, the techniques for optimizing temporal behaviour are largely orthogonal to the content of the system components, which can therefore be optimized separately. <p> In <ref> [ Russell and Subramanian, 1995 ] </ref> , it is shown that approximately bounded optimal designs can be guaranteed with high probability if each component is learned in such a way that its output quality is close to optimal among all components of a given execution time. <p> Acknowledgements An earlier version of this paper appeared in the Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, published by IJCAII. That paper drew on previous work with Eric Wefald [ Russell and Wefald, 1991a ] and Devika Subramanian <ref> [ Russell and Subramanian, 1995 ] </ref> . The latter work contains a more rigorous analysis of many of the concepts presented here.
Reference: [ Russell and Wefald, 1989 ] <author> Stuart J. Russell and Eric H. Wefald. </author> <title> On optimal game-tree search using rational meta-reasoning. </title> <booktitle> In Proceedings of the Eleventh International Joint Conference on Artificial Intelligence (IJCAI-89), </booktitle> <pages> pages 334-340, </pages> <address> Detroit, Michigan, August 1989. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Russell and Wefald, 1991a ] <author> Stuart J. Russell and Eric H. Wefald. </author> <title> Do the Right Thing: Studies in Limited Rationality. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1991. </year>
Reference-contexts: The metalevel is a second decision-making process whose application domain consists of the object-level computations themselves and the computational objects and states that they affect. Metareasoning has a long history in AI, going back at least to the early 1970s (see <ref> [ Russell and Wefald, 1991a ] </ref> for historical details). One can also view selective search methods and pruning strategies as embodying metalevel expertise concerning the desirability of pursuing particular object-level search operations. The theory of rational metareasoning formalizes the intuition that the metalevel can do the right thinking. <p> On the other hand, in the work on rational metar-easoning described above, it is clear that the metatheory describing the effects of computations is domain-independent <ref> [ Russell and Wefald, 1991a; Ginsberg and Geddis, 1991 ] </ref> . In principle, no additional domain knowledge is needed to assess the benefits of a computation. In practice, metar-easoning from first principles can be very expensive. <p> In practice, metar-easoning from first principles can be very expensive. To avoid this, the results of metalevel analysis for particular domains can be compiled into domain-specific metaknowledge, or such knowledge can be learned directly from experience <ref> [ Russell and Wefald, 1991a, Chapter 6 ] </ref> , [ Minton, 1996 ] . This view of emerging computational expertise leads to a fundamental insight into intelligence namely, that there is an interesting sense in which algorithms are not a necessary part of AI systems. <p> Recent results [ Tsitsiklis and Van Roy, 1996 ] provide convergence guarantees for reinforcement learning with a fairly broad class of function approximators. One can use such learning methods for metalevel information, e.g., the value of computation. In <ref> [ Russell and Wefald, 1991a, Chapter 6 ] </ref> , this is shown to be an effective technique. Formal results on convergence to optimal control of search would be of great interest. <p> Acknowledgements An earlier version of this paper appeared in the Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence, published by IJCAII. That paper drew on previous work with Eric Wefald <ref> [ Russell and Wefald, 1991a ] </ref> and Devika Subramanian [ Russell and Subramanian, 1995 ] . The latter work contains a more rigorous analysis of many of the concepts presented here.
Reference: [ Russell and Wefald, 1991b ] <author> Stuart J. Russell and Eric H. </author> <title> Wefald. </title> <booktitle> Principles of metareasoning. Artificial Intelligence, </booktitle> <address> 49(1-3):361-395, </address> <month> May </month> <year> 1991. </year>
Reference: [ Russell and Zilberstein, 1991 ] <author> S. Russell and S. Zilberstein. </author> <title> Composing real-time systems. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <address> Sydney, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: In this way, we can construct a complex system that can handle arbitrary and unexpected real-time demands exactly as if it knew the exact time available in advance, with just a small ( 4) constant factor penalty in speed <ref> [ Russell and Zilberstein, 1991 ] </ref> . Second, one has to allocate the available computation optimally among the components to maximize the total output quality. <p> As an illustration of how ABO is a useful abstraction, one can show that under certain restrictions one can construct universal ABO programs that are ABO for any time variation in the utility function, using the doubling construction from <ref> [ Russell and Zilberstein, 1991 ] </ref> . Further directions for bounded optimality research are discussed below. 7 What Is To Be Done? This section describes some of the research activities that will, I hope, help to turn bounded optimality into a creative tool for AI system design.
Reference: [ Russell et al., 1995 ] <author> Stuart Russell, John Binder, Daphne Koller, and Keiji Kanazawa. </author> <title> Local learning in probabilistic networks with hidden variables. </title> <booktitle> In Proceedings of the Fourteenth International Joint Conference on Artificial Intelligence (IJCAI-95), </booktitle> <pages> pages 1146-52, </pages> <address> Montreal, Canada, </address> <month> August </month> <year> 1995. </year> <note> Morgan Kaufmann. 26 </note>
Reference-contexts: Recently, Parr and Russell [ 1995 ] , among others, have had some success in applying reinforcement learning to partially observable environments. Finally, learning methods for static and dynamic probabilistic networks with hidden variables (i.e., for partially observable environments) may make it possible to acquire the necessary environment models <ref> [ Lauritzen, 1995; Russell et al., 1995 ] </ref> . The Bayesian Automated Taxi (a.k.a. BATmobile) project [ Forbes et al., 1995 ] is an attempt to combine all these new bricks to solve an interesting application problem, namely driving a car on a freeway.
Reference: [ Simon, 1955 ] <author> Herbert A. Simon. </author> <title> A behavioral model of rational choice. </title> <journal> Quarterly Journal of Economics, </journal> <volume> 69 </volume> <pages> 99-118, </pages> <year> 1955. </year>
Reference-contexts: The simplest type of metareasoning trades off the expected increase in decision quality for a single algorithm, as measured by a performance profile, against the cost of time <ref> [ Simon, 1955 ] </ref> . A greedy termination condition is optimal if the second derivative of the performance profile is negative. More complex problems arise if one wishes to build complex real-time systems from anytime components.
Reference: [ Simon, 1958 ] <author> Herbert A. Simon. </author> <title> Rational choice and the structure of the environment. In Models of Bounded Rationality, volume 2. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1958. </year>
Reference: [ Sondik, 1971 ] <author> E. J. Sondik. </author> <title> The Optimal Control of Partially Observable Markov Decision Processes. </title> <type> PhD thesis, </type> <institution> Stanford University, Stanford, California, </institution> <year> 1971. </year>
Reference: [ Tadepalli, 1991 ] <author> Prasad Tadepalli. </author> <title> A formalization of explanation-based macro-operator learning. </title> <booktitle> In Proceedings of the Twelfth International Joint Conference on Artificial Intelligence (IJCAI-91), </booktitle> <pages> pages 616-622, </pages> <address> Sydney, August 1991. </address> <publisher> Morgan Kaufmann. </publisher>
Reference: [ Tsitsiklis and Van Roy, 1996 ] <author> John N. Tsitsiklis and Benjamin Van Roy. </author> <title> An analysis of temporal-difference learning with function approximation. </title> <type> Technical Report LIDS-P-2322, </type> <institution> Laboratory for Information and Decision Systems, Massachusetts Institute of Technology, </institution> <year> 1996. </year>
Reference-contexts: The key additional step is to analyze the way in which slight imperfection in each component carries through to slight imperfection in the whole agent. * Reinforcement learning can be used to learn value information such as utility functions. Recent results <ref> [ Tsitsiklis and Van Roy, 1996 ] </ref> provide convergence guarantees for reinforcement learning with a fairly broad class of function approximators. One can use such learning methods for metalevel information, e.g., the value of computation.
Reference: [ Wellman, 1985 ] <author> Michael P. Wellman. </author> <title> Reasoning about preference models. </title> <type> Technical Report MIT/LCS/TR-340, </type> <institution> Laboratory for Computer Science, MIT, Cambridge, Massachusetts, </institution> <year> 1985. </year>
Reference-contexts: In economics, many results have been derived on the decomposition of overall utility into attributes that can be combined in various ways [ Keeney and Raiffa, 1976 ] , yet such methods have made few inroads into AI (but see <ref> [ Wellman, 1985; Bacchus and Grove, 1995 ] </ref> ). We also have little idea how to specify utility over time, and although the question has been raised often, we do not have a satisfactory understanding of the relationship 5 between goals and utility.
Reference: [ Wellman, 1994 ] <author> Michael P. Wellman. </author> <title> A market-oriented programming environment and its application to distributed multicommodity flow problems. </title> <journal> Journal of Artificial Intelligence Research, </journal> <volume> 1(1) </volume> <pages> 1-23, </pages> <year> 1994. </year>
Reference: [ Zilberstein and Russell, 1996 ] <author> Shlomo Zilberstein and Stuart Russell. </author> <title> Optimal composition of real-time systems. </title> <journal> Artificial Intelligence, </journal> <volume> 83, </volume> <year> 1996. </year> <month> 27 </month>
Reference-contexts: Second, one has to allocate the available computation optimally among the components to maximize the total output quality. Although this is NP-hard for the general case, it can be solved in time linear in program size when the call graph of the components is tree-structured <ref> [ Zilberstein and Russell, 1996 ] </ref> . <p> The components fulfill certain behavioural specifications and interact in well-defined ways. To produce a composite bounded-optimal design, the optimization problem involves allocating execution time to components <ref> [ Zilberstein and Russell, 1996 ] </ref> or arranging the order of execution of the components [ Russell and Subramanian, 1995 ] to maximize overall performance. <p> Thus, techniques such as the doubling construction and the time allocation algorithm in <ref> [ Zilberstein and Russell, 1996 ] </ref> can be seen as domain-independent tools for agent design. They enable bounded optimality results that do not depend on the specific temporal aspects of the environment class.
References-found: 52

