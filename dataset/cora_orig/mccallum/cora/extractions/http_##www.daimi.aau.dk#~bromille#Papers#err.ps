URL: http://www.daimi.aau.dk/~bromille/Papers/err.ps
Refering-URL: http://www.daimi.aau.dk/~bromille/Papers/index.html
Root-URL: http://www.daimi.aau.dk
Email: Email: bromille@brics.dk  
Title: Error Correcting Codes, Perfect Hashing Circuits, and Deterministic Dynamic Dictionaries  
Author: Peter Bro Miltersen 
Date: July 8, 1997  
Address: Aarhus.  
Affiliation: BRICS University of  
Abstract: We consider dictionaries of size n over the finite universe U = f0; 1g w and introduce a new technique for their implementation: error correcting codes. The use of such codes makes it possible to replace the use of strong forms of hashing, such as universal hashing, with much weaker forms, such as clustering. We use our approach to construct, for any * &gt; 0, a deterministic solution to the dynamic dictionary problem using linear space, with worst case time O(n * ) for insertions and deletions, and worst case time O(1) for lookups. This is the first deterministic solution to the dynamic dictionary problem with linear space, constant query time, and non-trivial update time. In particular, we get a solution to the static dictionary problem with O(n) space, worst case query time O(1), and deterministic initialization time O(n 1+* ). The best previous deterministic initialization time for such dictionaries, due to Andersson, is O(n 2+* ). The model of computation for these bounds is a unit cost RAM with word size w (i.e. matching the universe), and a standard instruction set. The constants in the big-O's are independent upon w. The solutions are weakly non-uniform in w, i.e. the code of the algorithm contains word sized constants, depending on w, which must be computed at compile-time, rather than at run-time, for the stated run-time bounds to hold. An ingredient of our proofs, which may be interesting in its own right, is the following observation: A good error correcting code for a bit vector fitting into a word can be computed in O(1) time on a RAM with unit cost multiplication. As another application of our technique in a different model of computation, we give a new construction of perfect hashing circuits, improving a construction by Goldreich and Wigderson. In particular, we show that for any set S f0; 1g w of size n, there is a Boolean circuit C of size O(w log w) with w inputs and 2 log n outputs so that the function defined by C is 1-1 on S. The best previous bound on the size of such a circuit was O(w log w log log w). 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Andersson. </author> <booktitle> Faster deterministic sorting and searching in linear space In 37th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 135-141, </pages> <address> Burlington, Vermont, </address> <year> 1996. </year>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>. <p> Fredman, Komlos, and Szemeredi [14] proved that their dictionary can be initialized deterministically in time O (n 3 w). Raman [20] shows that by derandomizing multiplicative hashing [9] using conditional probabilities, this can be improved to O (n 2 w). Finally, Andersson <ref> [1] </ref> shows that Raman's solution can be modified to give a time bound O (n 2+* ), i.e., that the dependence on w can be removed. Interestingly, Andersson uses fusion trees [15], and this introduces a flaw, weak non-uniformity into the solution. <p> Although Andersson does not state the following lemma explicitly in <ref> [1] </ref>, it is the essence of the proof of his Observation 1. Lemma 10 (Andersson) Suppose a linear space, constant query time static dictionary for a set of keys S can be constructed in time f (n)w O (1) for some function f (n) n.
Reference: [2] <author> A. Andersson, P.B. Miltersen, S. Riis, and M. </author> <title> Thorup. Static dictionaries on AC 0 RAMs: Query time fi( log n= log log n) is necessary and sufficient. </title> <booktitle> In 36th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 538-546, </pages> <address> Burlington, Vermont, </address> <year> 1996. </year>
Reference-contexts: In this paper we introduce an alternative to universal hashing for implementing linear space dictionaries with constant query time: Error correcting codes combined with clustering, a weak form of hashing introduced by Andersson et al <ref> [2] </ref>, used to partition an input set into Hamming balls. In [2], a full solution to the dictionary problem was obtained by applying a special purpose hash function, a cluster buster, after the application of a clustering function. <p> In this paper we introduce an alternative to universal hashing for implementing linear space dictionaries with constant query time: Error correcting codes combined with clustering, a weak form of hashing introduced by Andersson et al <ref> [2] </ref>, used to partition an input set into Hamming balls. In [2], a full solution to the dictionary problem was obtained by applying a special purpose hash function, a cluster buster, after the application of a clustering function. <p> In Section 3 we reprove a lemma about clustering functions from <ref> [2] </ref> in a stronger form. In Section 4, we show how to combine error correcting codes and clustering with the static dictionaries of Raman and Andersson to get the improved initialization time for static dictionaries. In Section 5 we show how to dynamize our solution. <p> For a nice survey of this correspondence and its applications, see Stinton [22]. 3 Clustering The following Lemma is proved in a weaker form in <ref> [2] </ref>. Here, what is important to us is the good dependence on n in the time bound. <p> Since r = O (log n), we have s = O (1). Now view each key x = x [1]x <ref> [2] </ref> : : :x [w 0 ] in S 00 as the following string of length s: x [1 : : : k 1 ] x [k 1 + 1 : : : k 2 ] : : : x [k s2 + 1 : : : k s1 ] x <p> o m1 as XOR-gates of odd fan-in l 0 d (l + 1)= log ( 1 12ffi )e, each adding size l 0 to the circuit, each taking a subset of the q i 's as inputs (here, we are essentially using one of the more powerful clustering functions from <ref> [2] </ref>). We will fix the inputs of the o i 's iteratively. Suppose o 1 ; : : : ; o i have been fixed and let C i be the circuit mapping the input to o 1 ; o 2 ; : : : o i . <p> Let P i = ffx; yg 2 E 0 (S)jC i (x) = C i (y)g. Now suppose we pick the l 0 inputs of o i+1 randomly from the q j 's. As observed in <ref> [2] </ref>, for each fx; yg 2 P i , the probability that fx; yg is in P i+1 is at most 1 2 (1 + (1 2ffi) l 0 2 (1 + 2 (l+1) ).
Reference: [3] <author> A.M. Ben-Amram and Z. Galil. </author> <booktitle> When can we sort in o(n log n) time? In 34th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 538-546, </pages> <address> Palo Alto, California, </address> <year> 1993. </year>
Reference-contexts: Interestingly, Andersson uses fusion trees [15], and this introduces a flaw, weak non-uniformity into the solution. In the terminology of Ben-Amram and Galil <ref> [3] </ref>, a trans-dichotomous algorithm is called uniform if the code runs within the stated bounds even if the word size of the machine is not known until "run-time", i.e., if the run-time code of the algorithm takes w as an input.
Reference: [4] <author> J. Bierbrauer, I. Johansson, G. Kabatianskii, and B. Smeets. </author> <title> On families of hash functions via geometric codes and concatenation. </title> <booktitle> In: Advances in Cryptology - CRYPTO '93, Lecture Notes in Computer Science vol. </booktitle> <volume> 773, </volume> <pages> pp. 331-342, </pages> <publisher> Springer, </publisher> <year> 1993. </year>
Reference-contexts: Pairwise independence being a generalization of universality, it is interesting to note that Bierbrauer et al <ref> [4] </ref> show a completely different connection between error correcting codes and universal families; namely that a good error correcting code can be used to derive a (nearly) universal family of hash functions on an exponentially smaller domain.
Reference: [5] <author> A. Brodnik and J.I. Munro. </author> <title> Membership in constant time and minimum space. </title> <booktitle> In: Proceedings of the 1st European Symposium on Algorithms, Lecture Notes in Computer Science, </booktitle> <volume> Vol. 855, </volume> <pages> page 72, </pages> <year> 1994. </year>
Reference: [6] <author> J.L. Carter and M.N. Wegman. </author> <title> Universal classes of hash functions. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 18(2) </volume> <pages> 143-154, </pages> <month> April </month> <year> 1979. </year>
Reference-contexts: Their solution is based on universal hashing <ref> [6] </ref>. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing [13, 7, 8, 9, 10, 20, 1].
Reference: [7] <editor> M. Dietzfelbinger and F. Meyer auf der Heide, </editor> <title> Dynamic hashing in real time, </title> <editor> in: J. Buchmann, H. Ganzinger, W. J. Paul (Eds.): </editor> <booktitle> Informatik Festschrift zum 60. </booktitle> <editor> Geburtstag von Gunter Hotz, </editor> <title> Teubner-Texte zur Informatik, Band 1, </title> <editor> B. G. </editor> <publisher> Teubner, </publisher> <year> 1992, </year> <pages> pp. </pages> <month> 95-119. </month> <title> (A preliminary version appeared under the title "A New Universal Class of Hash Functions and Dynamic Hashing in Real Time" in ICALP'90.) </title>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>.
Reference: [8] <author> M. Dietzfelbinger, J. Gil, Y. Matias and N. Pippenger. </author> <title> Polynomial hash functions are reliable. </title> <booktitle> In: Proc. 19th Int'l. Colloq. on Automata, Languages and Programming, Lecture Notes in Computer Science Vol. </booktitle> <volume> 623, </volume> <pages> pages 235-246, </pages> <publisher> Springer-Verlag, </publisher> <month> July </month> <year> 1992. </year> <month> 9 </month>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>.
Reference: [9] <author> M. Dietzfelbinger, T. Hagerup, J. Katajainen, and M. Penttonen. </author> <title> A reliable randomized algorithm for the closest-pair problem. </title> <type> Technical Report 513, </type> <institution> Fachbereich Informatik, Universitat Dortmund, </institution> <address> Dortmund, Germany, </address> <year> 1993. </year>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>. <p> Fredman, Komlos, and Szemeredi [14] proved that their dictionary can be initialized deterministically in time O (n 3 w). Raman [20] shows that by derandomizing multiplicative hashing <ref> [9] </ref> using conditional probabilities, this can be improved to O (n 2 w). Finally, Andersson [1] shows that Raman's solution can be modified to give a time bound O (n 2+* ), i.e., that the dependence on w can be removed. <p> All log's are base 2. 2 Multiplicative error correcting codes Here, we show how to do error correcting codes on words in constant time with multiplication. The similarity to multiplicative hashing by Dietzfelbinger et al <ref> [9] </ref> is obvious. 3 Let e : f0; 1g w ! f0; 1g kw .
Reference: [10] <author> M. Dietzfelbinger, A. Karlin, K. Mehlhorn, F. Meyer Auf Der Heide, H. Rohnert, R .E. Tarjan, </author> <title> Dynamic perfect hashing: upper and lower bounds, </title> <journal> SIAM J. Comput. </journal> <month> 23 </month> <year> (1994) </year> <month> 738-761. </month>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>.
Reference: [11] <author> M. Dietzfelbinger. </author> <title> Universal Hashing and k-wise Independent Random Variables via Integer Arithmetic without Primes. </title> <booktitle> In Proc. 13th Annual Symposium on Theoretical Aspects of Computer Science, Lecture Notes in Computer Science vol. </booktitle> <volume> 1046, </volume> <pages> pp. 569-580, </pages> <publisher> Springer, </publisher> <year> 1996. </year>
Reference-contexts: The statement of the lemma follows by a derivation similar to the one in Lemma 5. 2 Thus, an alternative proof for showing that multiplication yields good error correcting codes is to combine the above proposition with Dietzfelbinger's result <ref> [11] </ref> that the family of functions of the form e a is a pairwise independent family. If one is interested in a self-contained proof, one should note that Dietzfelbinger's proof of pairwise independence is much more complicated than the proof of Lemma 5.
Reference: [12] <author> S.I. Gelfand, </author> <title> R.L. Dobrushin, and M.S. Pinsker. On the complexity of coding. </title> <booktitle> In Second International Symposium on Information Theory, </booktitle> <pages> pages 177-184, </pages> <address> Akademiai Kiado, Budapest, </address> <year> 1973. </year>
Reference-contexts: This can be optimized somewhat, but since the circuits of [17] are based on circuits for w-wise independent hashfunctions, it is not obvious how to get an o (w 2 ) bound. Using error correcting codes with linear sized encoding circuits <ref> [12, 21] </ref> combined with clustering, we obtain the following improved bounds: Theorem 4 For every S f0; 1g w of size 2 k , the following circuits exist: * For any * &gt; 0, a circuit of size O (w) mapping S 1-1 to (2 + *)k bits. * A circuit <p> Proof Gelfand, Dobrushin, and Pinsker <ref> [12] </ref> have shown that good error correcting codes with linear sized encoding circuits exists, that is, for any w, there is a circuit E of size O (w) mapping f0; 1g w to f0; 1g w 0 with w 0 = O (w) so that for all x; y 2 f0;
Reference: [13] <author> A. Fiat, M. Naor, J.P. Schmidt and A. Siegel, </author> <title> Non-Oblivious Hashing. </title> <booktitle> In: Proceedings of the Twentieth Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 367-376, </pages> <year> 1988. </year>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>.
Reference: [14] <author> M.L. Fredman, J. Komlos, and E. Szemeredi. </author> <title> Storing a sparse table with O(1) worst case access time. </title> <journal> Journal of the ACM, </journal> <volume> 31(3) </volume> <pages> 538-544, </pages> <month> July </month> <year> 1984. </year>
Reference-contexts: All instructions are unit cost. fl Supported by the ESPRIT Long Term Research Programme of the EU under project number 20244 (ALCOM-IT). y Basic Research in Computer Science, Centre of the Danish National Research Foundation 1 The seminal result of Fredman, Komlos, and Szemeredi <ref> [14] </ref> shows that in this model, it is possible to construct static dictionaries for sets of size n using O (n) registers, so that queries can be answered in time O (1). Their solution is based on universal hashing [6]. <p> Previously, no linear space deterministic solution to the dictionary problem with constant query time and non-trivial update time has been given, but there has been a sequence of results on deterministically initializing static dictionaries. Fredman, Komlos, and Szemeredi <ref> [14] </ref> proved that their dictionary can be initialized deterministically in time O (n 3 w). Raman [20] shows that by derandomizing multiplicative hashing [9] using conditional probabilities, this can be improved to O (n 2 w).
Reference: [15] <author> M.L. Fredman and D.E. Willard. </author> <title> Surpassing the information theoretic bound with fusion trees. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 47 </volume> <pages> 424-436, </pages> <year> 1993. </year>
Reference-contexts: Finally, Andersson [1] shows that Raman's solution can be modified to give a time bound O (n 2+* ), i.e., that the dependence on w can be removed. Interestingly, Andersson uses fusion trees <ref> [15] </ref>, and this introduces a flaw, weak non-uniformity into the solution.
Reference: [16] <author> M.L. Fredman and D.E. Willard. </author> <title> Trans-dichotomous algorithms for minimum spanning trees and shortest paths. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 48(3) </volume> <pages> 533-551, </pages> <month> June </month> <year> 1994. </year>
Reference-contexts: We consider solutions to the dictionary problem on a RAM with registers containing w bits, i.e. we assume that the word size matches the size of the universe (this is often known as the trans-dichotomous model <ref> [16] </ref>). The RAM operates on its registers with a standard instruction set of direct and indirect addressing, conditional jump, addition, subtraction, bitwise Boolean operations, shifts, and multiplication.
Reference: [17] <author> O. Goldreich and A. Wigderson, </author> <title> On the Circuit Complexity of Perfect Hashing, </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <address> TR96-04, </address> <year> 1996. </year>
Reference-contexts: As another application of our technique in a different model of computation, we give a new construction of perfect hashing circuits, improving a construction by Goldreich and Wigderson <ref> [17] </ref>. <p> For m &lt; 2k, the situation becomes worse: If the proofs of <ref> [17] </ref> are left unmodified, the dependence on w becomes cubic. This can be optimized somewhat, but since the circuits of [17] are based on circuits for w-wise independent hashfunctions, it is not obvious how to get an o (w 2 ) bound. <p> For m &lt; 2k, the situation becomes worse: If the proofs of <ref> [17] </ref> are left unmodified, the dependence on w becomes cubic. This can be optimized somewhat, but since the circuits of [17] are based on circuits for w-wise independent hashfunctions, it is not obvious how to get an o (w 2 ) bound.
Reference: [18] <author> F.J. MacWilliams and N.J.A Sloane. </author> <title> The Theory of Error-Correcting Codes. </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1977. </year>
Reference-contexts: So, plugging in d = dffikwe, the probability of small Hamming distance for some x; y is at most P w;k;ffi = ( dffikwe1 X j=0 kw For ffi &lt; 1 2 the inequality P bffinc j=0 j 2 nH (ffi) holds (see e.g. <ref> [18, page 310] </ref>). <p> The non-standard unit cost instructions we need are error : f0; 1g w ! f0; 1g O (w) which computes an explicit good error correcting code, such as a Justesen code (see, for instance, MacWilliams and Sloan <ref> [18] </ref>), and collect, which takes as inputs a word containing a mask m and a word x and returns the concatenation of the bits of x which are marked by m. For instance, collect (0100100001001001; 1101011110111010) = 10010.
Reference: [19] <author> K. Mehlhorn and A. Tsakalidis. </author> <title> Data Structures. </title> <booktitle> In: Handbook of Theoretical Computer Science, Vol. A: Algorithms and complexity, </booktitle> <pages> pp. 303-341, </pages> <publisher> MIT Press, </publisher> <year> 1994. </year>
Reference-contexts: Apart from some tuning of parameters, the dynamic solution is a standard dynamization of the static solution (see Mehlhorn and Tsakalidis <ref> [19, Section 10] </ref> for a survey on dynamization). The concrete details of the dynamization are copied more or less verbatim from Thorup's elegant description of his merge heaps [25].
Reference: [20] <author> R. Raman. </author> <title> Priority queues: Small, monotone, </title> <booktitle> and trans-dichotomus. In Proceedings 4th European Symposium on Algorithms, volume 1136 of Lecture Notes in Computer Science, </booktitle> <pages> pages 121-137. </pages> <publisher> Springer-Verlag, </publisher> <year> 1996. </year>
Reference-contexts: Their solution is based on universal hashing [6]. Since then, several variations of their scheme, all with constant query time, but with different additional desirable properties have appeared, all based on universal hashing <ref> [13, 7, 8, 9, 10, 20, 1] </ref>. <p> Fredman, Komlos, and Szemeredi [14] proved that their dictionary can be initialized deterministically in time O (n 3 w). Raman <ref> [20] </ref> shows that by derandomizing multiplicative hashing [9] using conditional probabilities, this can be improved to O (n 2 w). Finally, Andersson [1] shows that Raman's solution can be modified to give a time bound O (n 2+* ), i.e., that the dependence on w can be removed. <p> As lemmas, we need two of the previous results on initializing static dictionaries deterministically. The first is from <ref> [20] </ref>: Theorem 9 (Raman) A static dictionary using linear space and with worst case constant query time for a set of keys S f0; 1g w of size n can be constructed in worst case deterministic time O (n 2 w).
Reference: [21] <author> D.A. Spielman. </author> <title> Linear-time encodable and decodable error-correcting codes. </title> <booktitle> In Proceedings 27th annual ACM symposium on the theory of computing, </booktitle> <pages> pages 388-397, </pages> <address> Las Vegas, Nevada, </address> <year> 1994. </year>
Reference-contexts: This can be optimized somewhat, but since the circuits of [17] are based on circuits for w-wise independent hashfunctions, it is not obvious how to get an o (w 2 ) bound. Using error correcting codes with linear sized encoding circuits <ref> [12, 21] </ref> combined with clustering, we obtain the following improved bounds: Theorem 4 For every S f0; 1g w of size 2 k , the following circuits exist: * For any * &gt; 0, a circuit of size O (w) mapping S 1-1 to (2 + *)k bits. * A circuit <p> A more recent reference for such codes is Spielman <ref> [21] </ref>; his codes can also be decoded in linear time, but we only need the encoding circuits. Step 1. As basis for our perfect hashing circuit we take a copy of E, applied to the input vector.
Reference: [22] <author> D.R. Stinton. </author> <title> On the connections between universal hashing, combinatorial designs and error-correcting codes. </title> <booktitle> Electronic Colloquium on Computational Complexity, </booktitle> <address> TR95-052, </address> <year> 1995. </year>
Reference-contexts: For a nice survey of this correspondence and its applications, see Stinton <ref> [22] </ref>. 3 Clustering The following Lemma is proved in a weaker form in [2]. Here, what is important to us is the good dependence on n in the time bound.
Reference: [23] <author> S.C. Sahinalp and U. Vishkin. </author> <title> Efficient approximate and dynamic matching of patterns using a labelling paradigm. </title> <booktitle> In 36th IEEE Symposium on Foundations of Computer Science, </booktitle> <pages> pages 320-328, </pages> <address> Burlington, Vermont, </address> <year> 1996. </year>
Reference-contexts: Using more modern data structures for maintaining strings, in particular Sahinalp and Vishkin's dynamic pattern matching algorithm <ref> [23] </ref> yields the improved initialization time.
Reference: [24] <author> R.E. Tarjan and A.C. Yao. </author> <title> Storing a sparse table. </title> <journal> Communications of the ACM, </journal> <volume> 22(11) </volume> <pages> 606-611. </pages>
Reference-contexts: In order to argue the (mainly philosophical) point that error correcting codes combined with clustering yields a true alternative to universal hashing, we present, in Section 6, an alternative implementation, replacing Raman's structure with the double displacement structure of Tarjan and Yao <ref> [24] </ref>. This solution uses a non-standard instruction set. Finally, in Section 7 we prove our results on perfect hashing circuits. <p> The keys in S 00 have length w 00 = O (log n). Now, we store the keys of S 00 using Tarjan and Yao's double displacement scheme <ref> [24] </ref>. This is a scheme, with no trace of universal hashing (it was invented in 1978), for the static dictionary problem for a set S f0; 1g w of size n with space O (n) and query time O (w= log n).
Reference: [25] <author> M. </author> <title> Thorup. On RAM priority queues. </title> <booktitle> In 7th ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <pages> pages 59-67, </pages> <address> Atlanta, Georgia, </address> <year> 1996. </year> <month> 10 </month>
Reference-contexts: Apart from some tuning of parameters, the dynamic solution is a standard dynamization of the static solution (see Mehlhorn and Tsakalidis [19, Section 10] for a survey on dynamization). The concrete details of the dynamization are copied more or less verbatim from Thorup's elegant description of his merge heaps <ref> [25] </ref>. We first construct a solution with a fixed capacity N , space O (N ) and with update time O (N * ).
References-found: 25

