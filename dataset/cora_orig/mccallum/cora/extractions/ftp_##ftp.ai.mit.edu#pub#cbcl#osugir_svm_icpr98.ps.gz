URL: ftp://ftp.ai.mit.edu/pub/cbcl/osugir_svm_icpr98.ps.gz
Refering-URL: http://www.ai.mit.edu/people/eosuna/publications.html
Root-URL: 
Title: Reducing the run-time complexity of Support Vector Machines  
Author: Edgar Osuna and Federico Girosi 
Keyword: Support Vector Machines, Machine Learning, Optimization.  
Address: Cambridge, MA 02139, USA  
Affiliation: Center for Biological and Computational Learning Massachusetts Institute of Technology  
Note: (To appear in ICPR'98, Brisbane, Australia,  
Email: e-mail: feosuna,girosig@ai.mit.edu  
Date: August 16-20 1998)  
Abstract: The Support Vector Machine (SVM) is a new and promising technique for classification and regression, developed by V. Vapnik and his group at AT&T Bell Labs [2, 9]. The technique can be seen as a new training algorithm for Polynomial, Radial Basis Function and Multi-Layer Perceptron networks. SVMs are currently considered slower at run-time than other techniques with similar generalization performance. In this paper we focus on SVM for classification and investigate the problem of reducing its run-time complexity. We present two relevant results: a) the use of SVM itself as a regression tool to approximate the decision surface with a user-specified accuracy; and b) a reformulation of the training problem that yields the exact same decision surface using a smaller number of basis functions. We believe that this reformulation offers great potential for future improvements of the technique. For most of the selected problems, both approaches give reductions of run-time in the 50-95% range, with no system degradation.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> C. Burges. </author> <title> Simplified support vector decision rules. </title> <booktitle> Procedings of the 13th International Conference on Machine Learning, </booktitle> <pages> pages 71-77, </pages> <year> 1996. </year>
Reference-contexts: This approach, which for radial kernels K resembles the technique of "moving centers" [4, 7], has been pursued by C. Burges <ref> [1] </ref>, but the procedure is hard to implement and lacks a principled way for controlling the approximation accuracy. 2. <p> This situation tends to occur when the data is highly (if not totally) separable, and worsens as the dimensionality of the feature space grows. Up to date, the best we can do in cases like this is to use the reduced set method of Burges <ref> [1] </ref>. 9 2. Although a delayed column generation algorithm can be devised for decomposing and solving our primal reformulation, memory limitations make it prohibitive for large data sets (beyond 10.000). Alternatives to this problem are still an open area of research.
Reference: [2] <author> C. Cortes and V. Vapnik. </author> <title> Support vector networks. </title> <journal> Machine Learning, </journal> <volume> 20 </volume> <pages> 1-25, </pages> <year> 1995. </year>
Reference-contexts: Details on this technique can be found in <ref> [9, 2, 3] </ref>. A simple intuition about the technique can be obtained in the case of a linear classifier, with linearly separable classes. <p> Notice that, according to the definition of K, eq. (1) has the form f (x) = (w z (x) + b), and therefore represents a linear separating surface in the feature space. The parameters i are found by solving the following QP problem <ref> [9, 2, 3] </ref>: Maximize W (fl) = fl T 1 1 2 fl T Dfl subject to fl T y = 0 (2) where (fl) i = i , (1) i = 1, D ij = y i y j K (x i ; x j ), and C is a
Reference: [3] <author> F. Girosi. </author> <title> An equivalence between sparse approximation and Support Vector Machines. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> (in press). </note>
Reference-contexts: Details on this technique can be found in <ref> [9, 2, 3] </ref>. A simple intuition about the technique can be obtained in the case of a linear classifier, with linearly separable classes. <p> Notice that, according to the definition of K, eq. (1) has the form f (x) = (w z (x) + b), and therefore represents a linear separating surface in the feature space. The parameters i are found by solving the following QP problem <ref> [9, 2, 3] </ref>: Maximize W (fl) = fl T 1 1 2 fl T Dfl subject to fl T y = 0 (2) where (fl) i = i , (1) i = 1, D ij = y i y j K (x i ; x j ), and C is a <p> The solution is typically sparse, that is, only some coefficients ff i will be different from zero, and their number is critically controlled by the parameter *, which also controls the accuracy of the approximation. The relationship between SVRM and sparse approximation has been studied in <ref> [3] </ref>.
Reference: [4] <author> J. Moody and C. Darken. </author> <title> Fast learning in networks of locally-tuned processing units. </title> <journal> Neural Computation, </journal> <volume> 1(2) </volume> <pages> 281-294, </pages> <year> 1989. </year>
Reference-contexts: This approach, which for radial kernels K resembles the technique of "moving centers" <ref> [4, 7] </ref>, has been pursued by C. Burges [1], but the procedure is hard to implement and lacks a principled way for controlling the approximation accuracy. 2.
Reference: [5] <author> M. Oren, C. Papageorgiou, P. Sinha, E. Osuna, and T. Poggio. </author> <title> Pedestrian detection using wavelet templates. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, </booktitle> <pages> pages 193-199, </pages> <address> Puerto Rico, </address> <month> June 16-20 </month> <year> 1997. </year>
Reference-contexts: Examples of this issue arise in face and people detection systems <ref> [6, 5] </ref>, where SVMs are used to distinguish an object from the background, or in checks and ZIP code readers, where the system is required to spend fractions of a second per check or envelope. Here are some different ways to obtain a speedup: 1.
Reference: [6] <author> E. Osuna, R. Freund, and F. Girosi. </author> <title> Training support vector machines: an application to face detection. </title> <booktitle> In Proc. Computer Vision and Pattern Recognition, </booktitle> <address> Puerto Rico, </address> <month> June 16-20 </month> <year> 1997. </year>
Reference-contexts: Examples of this issue arise in face and people detection systems <ref> [6, 5] </ref>, where SVMs are used to distinguish an object from the background, or in checks and ZIP code readers, where the system is required to spend fractions of a second per check or envelope. Here are some different ways to obtain a speedup: 1.
Reference: [7] <author> T. Poggio and F. Girosi. </author> <title> Regularization algorithms for learning that are equivalent to multilayer networks. </title> <journal> Science, </journal> <volume> 247 </volume> <pages> 978-982, </pages> <year> 1990. </year>
Reference-contexts: This approach, which for radial kernels K resembles the technique of "moving centers" <ref> [4, 7] </ref>, has been pursued by C. Burges [1], but the procedure is hard to implement and lacks a principled way for controlling the approximation accuracy. 2.
Reference: [8] <author> M. Pontil and A. Verri. </author> <title> Properties of support vector machines. </title> <booktitle> Neural Computation, </booktitle> <year> 1998. </year> <note> (in press). </note>
Reference-contexts: its degree of separability, and pays a linear penalty C in the cost function. 7 One can show that the minimum of this problem gives the same separating surface of the classical approach, but uses a (possibly) different expression for w (work along these lines has also been done in <ref> [8] </ref>). It is not guaranteed that the new representation of the hyperplane is more sparse than the classical solution.
Reference: [9] <author> V. Vapnik. </author> <title> The Nature of Statistical Learning Theory. </title> <publisher> Springer-Verlag, </publisher> <year> 1995. </year> <month> 10 </month>
Reference-contexts: Details on this technique can be found in <ref> [9, 2, 3] </ref>. A simple intuition about the technique can be obtained in the case of a linear classifier, with linearly separable classes. <p> In this case a SVM will separate the data with an hyperplane (described by w x + b = 0) that leaves the maximum margin between the two classes (see fig. 1). It can be shown <ref> [9] </ref> that maximizing the margin is equivalent to minimizing an upper bound on the generalization error of the classifier, providing a very strong theoretical motivation for this technique. <p> Notice that, according to the definition of K, eq. (1) has the form f (x) = (w z (x) + b), and therefore represents a linear separating surface in the feature space. The parameters i are found by solving the following QP problem <ref> [9, 2, 3] </ref>: Maximize W (fl) = fl T 1 1 2 fl T Dfl subject to fl T y = 0 (2) where (fl) i = i , (1) i = 1, D ij = y i y j K (x i ; x j ), and C is a
References-found: 9

