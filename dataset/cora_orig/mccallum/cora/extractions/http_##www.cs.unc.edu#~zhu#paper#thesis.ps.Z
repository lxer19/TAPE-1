URL: http://www.cs.unc.edu/~zhu/paper/thesis.ps.Z
Refering-URL: http://www.cs.unc.edu/~zhu/paper.html
Root-URL: http://www.cs.unc.edu
Title: Efficient First-Order Semantic Deduction Techniques  
Author: by Yunshan Zhu Advisor: Prof. David A. Plaisted Reader: Prof. Owen L. Astrachan Reader: Prof. Alan W. Biermann 
Degree: A dissertation submitted to the faculty of the  in partial fulfillment of the requirements for the degree of Doctor of Philosophy in the  Approved by:  Committee Member: Prof. John Halton Committee Member: Prof. Dinesh Manocha  
Date: 1998  
Address: Chapel Hill  
Affiliation: University of North Carolina at Chapel Hill  Department of Computer Science.  
Abstract-found: 0
Intro-found: 1
Reference: [BCM96] <author> A. Boudet, E. Contejean, and C Marche. </author> <title> AC-complete unification and its application to theorem proving. </title> <editor> In H. Ganzinger, editor, </editor> <booktitle> Proceedings of the 7th International Conference on Rewriting Techniques and Applications(RTA-96), </booktitle> <pages> pages 18-32, </pages> <address> New Brunswick, NJ, USA, </address> <month> July </month> <year> 1996. </year>
Reference-contexts: A flattened term represents all terms equivalent up to the AC axioms. Since flattening breaks the original term structure, special unification techniques are needed. AC unification algorithms <ref> [Sti81, Dom91, BCM96] </ref> are developed to compute all possible most general unifiers of two terms up to associativity and commutativity axioms. Special AC termination orderings are also needed to show termination of AC rewriting systems.
Reference: [BDP89] <author> Leo Bachmair, N. Dershowitz, and D. Plaisted. </author> <title> Completion without failure. </title> <editor> In Hassan A it-Kaci and Maurice Nivat, editors, </editor> <title> Resolution of Equations in Algebraic Structures 2: </title> <booktitle> Rewriting Techniques, </booktitle> <pages> pages 1-30, </pages> <address> New York, 1989. </address> <publisher> Academic Press. </publisher>
Reference-contexts: Chapter 3 AC Equality 3.1 Introduction Knuth-Bendix completion [KB70] and its extensions <ref> [BDP89] </ref> have been widely used for equational reasoning. One of the main bottlenecks of completion-based inference strategies is the large number of critical pairs generated. This is particularly evident when dealing with equational problems involving associative and commutative functions, i.e. AC equational problems. <p> The algorithm was initially proposed in [KB70], and is often called Knuth-Bendix completion, or simply, completion. Sometimes, it is not possible to orient an equation, i.e. f (x; y) = f (y; x), in either direction while maintaining termination. Thus Knuth- Bendix completion is extended to unfailing completion <ref> [BDP89] </ref>. In unfailing completion, if a critical pair (s; t) can not be oriented then it is added as an equation s = t. Equation s = t may participate in further critical pair generation. It is regarded as containing rules s ! t and t ! s.
Reference: [BGLS92] <author> L. Bachmair, H. Ganzinger, C. Lynch, and W. Snyder. </author> <title> Basic paramodu-lation and basic strict superposition. </title> <booktitle> In Proceedings of the 11th International Conference on Automated Deduction, </booktitle> <pages> pages 462-476, </pages> <year> 1992. </year>
Reference-contexts: We were also able to use the equational prover as a component in a general first order theorem prover. There are some previous works on reducing unnecessary equational inferences, both for AC equational problems and equational problems in general <ref> [ZK90, BGLS92] </ref>. Most of them are fundamentally different from our approach. They reduce the number of critical pairs generated by blocking out certain positions for overlapping two rewrite rules. Some of these works might be combined with our approach.
Reference: [BH96] <author> M. Bonacina and J. Hsiang. </author> <title> On the modelling of search in theorem proving towards a theory of strategy analysis. </title> <note> submitted, </note> <year> 1996. </year>
Reference-contexts: Goubault [Gou94] showed that the problem of determining the minimal number of copies of first-order clauses needed for a proof is p 2 complete. The paper [Let93] studies how accurately the length of a derivation reflects the actual complexity of a proof. Hsiang and Bonacina <ref> [BH96] </ref> present a formalism that facilitates the study of infinite search spaces. [PZ97a] presents complexity analysis of common theorem proving strategies with respect to a number of complexity measures. The analysis has a number of interesting implications.
Reference: [Bir35] <author> G. Birkhoff. </author> <title> On the structure of abstract algebras. </title> <journal> Proc. Cambridge Philos. Soc., </journal> <volume> 31 </volume> <pages> 433-454, </pages> <year> 1935. </year>
Reference-contexts: By Birkhoff's theorem <ref> [Bir35] </ref>, R = j= r = s iff r $ fl R s. This essentially shows the soundness and completeness of equational reasoning based on term rewriting.
Reference: [BP85] <author> L. Bachmair and D. Plaisted. </author> <title> Termination orderings for associative-commutative rewriting systems. </title> <journal> J. Symbolic Computation, </journal> <volume> 1 </volume> <pages> 329-349, </pages> <year> 1985. </year>
Reference-contexts: Special AC termination orderings are also needed to show termination of AC rewriting systems. Many commonly used orderings, such as recursive path ordering and lexicographic path ordering, are no longer well founded when flattened terms are used. Several AC termination ordering have been devised <ref> [DHJP83, BP85, KSZ90] </ref>. We study the problem of AC equational reasoning with a different approach. We propose a procedure called consistent unfailing completion in which only consistent rules and equations are used for critical pair generation and rewriting.
Reference: [Byl91] <author> T. Bylander. </author> <title> Complexity result for planning. </title> <booktitle> In Proc. IJCAI-91, </booktitle> <pages> pages 294-279, </pages> <address> Sidney, Australia, </address> <year> 1991. </year>
Reference-contexts: For the purpose of complexity analysis, we are only interested in planning problems with polynomial-length solutions, or equivalently, bounded planning problems [KMS96]. Without the polynomial-length constraint, the STRIPS planning problems is PSPACE complete <ref> [Byl91] </ref> and the ADL planning problem is undecidable [Ped94]. Theorem 5.2.1 The problem of determining whether a binary resolution proof of length n exists for a set of Horn clauses is NP-complete.[PZ97a] Planning problems represented in STRIPS can be represented in first-order logic using situation calculus.
Reference: [CL73] <author> C. Chang and R. Lee. </author> <title> Symbolic Logic and Mechanical Theorem Proving. </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: We also define orderings used in our implementation of the ordered semantic hyper linking strategy. We will assume a general knowledge of first order logic and refutational theorem proving. We refer readers to <ref> [CL73] </ref> for a general introduction to these areas. A term is a well-formed expression containing function symbols, constant symbols, and variables, as, f (X; g (a)). An atom is a predicate symbol followed by a list of 8 terms, as, p (X; g (a; b)). <p> It is an A resolution if the literals (or subsets) B 1 and B 2 of resolution are minimal in their respective clauses in a pre-specified ordering on literals. For a discussion of these strategies, see <ref> [CL73, Fit90, Lov78, WOLB84] </ref>. Definition 4.2.5 A resolution proof from a set S of clauses is a sequence C 1 ; C 2 ; ; C n of clauses, where each C i is either in S or is a resolvent of previous clauses in the sequence. <p> We note that although the depth measure is inference based, it can be viewed in a non-inference based manner, as suggested by [PZ96]. It is equivalent to the depth of the minimal closed binary semantic tree over S (for the definition of this, see for 58 example <ref> [CL73] </ref>). In general, the proof complexity measures that are not inference based are instance based, that is, they are based on Herbrand's theorem. Herbrand's theorem says that if a set S of clauses is unsatisfiable, then there is an unsatisfiable set T of ground instances of the clauses in S.
Reference: [CP94] <author> Heng Chu and D. Plaisted. </author> <title> Semantically guided first-order theorem proving using hyper-linking. </title> <booktitle> In Proceedings of the Twelfth International Conference on Automated Deduction, </booktitle> <pages> pages 192-206, </pages> <year> 1994. </year> <booktitle> Lecture Notes in Artificial Intelligence 814. </booktitle>
Reference-contexts: The history of instance-based theorem proving strategies traces back to [Gil60]. Most early attempts were based on enumerating Herbrand instances, and they have not been very successful partially due to the lack of unification and partially due to the lack of control in instance generation. <ref> [GHL63, WB87, Sla67, CP94] </ref> used semantics to guide the search of theorem provers. The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking. <p> The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S <ref> [CP94] </ref> adds semantic support to clause linking. RRTP [PP97] generates instances using replacement rules and uses the propositional decision procedure in [ZS94] to detect propositional unsatisfiability. Ordered semantic hyper linking improves upon CLIN and CLIN-S. CLIN-S restricts input semantics to be either finite or decidable. <p> We use I (s) to represent the element denoted by term s. We use I 0 to name the input semantics. A current semantics is represented as I 0 [EL], where EL stands for eligible literals. Some of these notations were also used in <ref> [CP94] </ref>.
Reference: [CR79] <author> S. A. Cook and R. Reckhow. </author> <title> The relative efficiency of propositional proof systems. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 44(1) </volume> <pages> 36-50, </pages> <month> March </month> <year> 1979. </year> <month> 139 </month>
Reference-contexts: In the past, there has been some work on the complexity of theorem proving, but mostly with respect to the length of proofs, and not to the difficulty of finding a proof. For some examples of studies of proof lengths, see <ref> [CR79, Hak85, Urq87, Ede92] </ref>. Haken [Hak85] showed that for a set of propositional problems, known as the pigeonhole problems, resolution needs to generate an exponential number of clauses in order to find a proof. This means that the minimum proof length is exponential for resolution, in this case.
Reference: [DHJP83] <author> Nachum Dershowitz, J. Hsiang, N. Josephson, and David A. Plaisted. </author> <title> Associative-commutative rewriting. </title> <booktitle> In Proceedings of the Eighth International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 940-944, </pages> <month> August </month> <year> 1983. </year>
Reference-contexts: Special AC termination orderings are also needed to show termination of AC rewriting systems. Many commonly used orderings, such as recursive path ordering and lexicographic path ordering, are no longer well founded when flattened terms are used. Several AC termination ordering have been devised <ref> [DHJP83, BP85, KSZ90] </ref>. We study the problem of AC equational reasoning with a different approach. We propose a procedure called consistent unfailing completion in which only consistent rules and equations are used for critical pair generation and rewriting.
Reference: [DJ90] <author> N. Dershowitz and J.-P. Jouannaud. </author> <title> Rewrite systems. </title> <editor> In J. van Leeuwen, editor, </editor> <booktitle> Handbook of Theoretical Computer Science. </booktitle> <publisher> North-Holland, </publisher> <address> Ams-terdam, </address> <year> 1990. </year>
Reference-contexts: We then present the consistent unfailing completion algorithm and show its completeness. We then show how the algorithm can be refined and implemented. We give some test results in the end. 3.2 Background In this section, we define some relevant concepts. For surveys on equational reasoning, see <ref> [Pla93, DJ90, Klo92] </ref>. We follow the conventions of [Pla93]. We use f; g; h; : : : as function symbols, a; b; c; : : : as constant symbols, and x; y; z; : : : as variables.
Reference: [Dom91] <author> E. Domenjoud. </author> <title> Ac-unification through order-sorted ac1-unification. </title> <booktitle> In Proceedings of the 4th International Conference on rewriting techniques and applications. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1991. </year> <booktitle> Lecture Notes in Computer Science volume 488. </booktitle>
Reference-contexts: A flattened term represents all terms equivalent up to the AC axioms. Since flattening breaks the original term structure, special unification techniques are needed. AC unification algorithms <ref> [Sti81, Dom91, BCM96] </ref> are developed to compute all possible most general unifiers of two terms up to associativity and commutativity axioms. Special AC termination orderings are also needed to show termination of AC rewriting systems.
Reference: [DP60] <author> M. Davis and H. Putnam. </author> <title> A computing procedure for quantification theory. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 7 </volume> <pages> 201-215, </pages> <year> 1960. </year>
Reference-contexts: Hyper-resolution greatly reduces the number of possible resolvents, but it still generates redundant clauses, and subsumption tests are needed to remove the redundant clauses. [Pla94b] has studied the duplication in various propositional strategies. The ground procedure of ordered semantic hyper linking is in essence similar to the Davis-Putnam method <ref> [DP60] </ref>. Both are model finding strategies, and prove unsatisfiability by exhausting all possible models. Both use case analysis to handle non-Horn clauses, although in somewhat different formats.
Reference: [Ede92] <author> E. Eder. </author> <title> Relative Complexities of First-Order Calculi. </title> <publisher> Vieweg, </publisher> <address> Braun-schweig, </address> <year> 1992. </year>
Reference-contexts: In the past, there has been some work on the complexity of theorem proving, but mostly with respect to the length of proofs, and not to the difficulty of finding a proof. For some examples of studies of proof lengths, see <ref> [CR79, Hak85, Urq87, Ede92] </ref>. Haken [Hak85] showed that for a set of propositional problems, known as the pigeonhole problems, resolution needs to generate an exponential number of clauses in order to find a proof. This means that the minimum proof length is exponential for resolution, in this case.
Reference: [Fit90] <author> Melvin Fitting. </author> <title> First-Order Logic and Automated Theorem Proving. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1990. </year>
Reference-contexts: It is an A resolution if the literals (or subsets) B 1 and B 2 of resolution are minimal in their respective clauses in a pre-specified ordering on literals. For a discussion of these strategies, see <ref> [CL73, Fit90, Lov78, WOLB84] </ref>. Definition 4.2.5 A resolution proof from a set S of clauses is a sequence C 1 ; C 2 ; ; C n of clauses, where each C i is either in S or is a resolvent of previous clauses in the sequence.
Reference: [FN71] <author> R.E. Fikes and N.J. Nilsson. </author> <title> STRIPS: a new approach to the application of theorem proving to problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 2 </volume> <pages> 189-208, </pages> <year> 1971. </year>
Reference-contexts: They may be important for handling large planning problems. We conclude the chapter in section 9. Parts of the chapter have been published [PZ97c][PZ97e]. 84 5.1 First-order planning Most planning algorithms and planners developed in planning literature use STRIPS-based planning representations. STRIPS was proposed by Fikes and Nilsson <ref> [FN71] </ref>. It avoids the frame axioms of situation calculus representation, commonly regarded as a major source of inefficiency. However, the expressiveness of the original STRIPS is very limited; it does not allow conditional effect, universal quantification, etc. A spectrum of planning representations has been proposed.
Reference: [Geo87] <editor> M.P. Georgeff. </editor> <booktitle> Planning, </booktitle> <pages> pages 359-400. </pages> <publisher> Annual Review Inc, </publisher> <address> Palo Alto, </address> <year> 1987. </year>
Reference-contexts: There are several different understandings of what the frame problem refers to <ref> [Geo87] </ref>. We are concerned with efficient axiomatization of the frame axioms, which are used to describe properties that are not affected by an action [MH69]. A number of approaches have been proposed to handle this problem [Rei80, McD82].
Reference: [GHL63] <author> H. Gelernter, J.R. Hansen, and D.W. Loveland. </author> <title> Empirical explorations of the geometry theorem proving machine. </title> <editor> In E. Feigenbaum and J. Feld-man, editors, </editor> <booktitle> Computers and Thought, </booktitle> <pages> pages 153-167. </pages> <publisher> McGraw-Hill, </publisher> <address> New York, </address> <year> 1963. </year>
Reference-contexts: The history of instance-based theorem proving strategies traces back to [Gil60]. Most early attempts were based on enumerating Herbrand instances, and they have not been very successful partially due to the lack of unification and partially due to the lack of control in instance generation. <ref> [GHL63, WB87, Sla67, CP94] </ref> used semantics to guide the search of theorem provers. The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking.
Reference: [Gil60] <author> P. C. Gilmore. </author> <title> A proof method for quantification theory. </title> <journal> IBM Journal of Research and Development, </journal> <volume> 4 </volume> <pages> 28-35, </pages> <year> 1960. </year>
Reference-contexts: OSHL might not be as efficient as the method of [KMS96] on specific problems, but is perhaps more general. The history of instance-based theorem proving strategies traces back to <ref> [Gil60] </ref>. Most early attempts were based on enumerating Herbrand instances, and they have not been very successful partially due to the lack of unification and partially due to the lack of control in instance generation. [GHL63, WB87, Sla67, CP94] used semantics to guide the search of theorem provers.
Reference: [GN87] <author> Michael R. Genesereth and Nils J. Nilsson. </author> <booktitle> Logical Foundations of Artificial Intelligence. </booktitle> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> Los Altos, CA, </address> <year> 1987. </year> <month> 140 </month>
Reference-contexts: We use lower case letters to denote constants, predicates and functions. We use A to denote actions, S to denote states. We will briefly discuss some relevant background of situation calculus in this section. Readers 98 should consult textbooks for detailed discussions <ref> [GN87] </ref>. A classical situation calculus domain theory consists of the following groups of axioms.
Reference: [Gou94] <author> Jean Goubault. </author> <title> The complexity of resource-bounded first-order classical logic. </title> <editor> In P. Enjalbert, E.W. Mayr, and K.W. Wagner, editors, </editor> <booktitle> 11th Symposium on Theoretical Aspects of Computer Science, </booktitle> <pages> pages 59-70, </pages> <address> Caen, France, February 1994. </address> <publisher> Springer Verlag LNCS 775. </publisher>
Reference-contexts: However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. The price for this is that we need to consider specific search strategies, which was not necessary in [Pla94b, Pla94c]. Plaisted [Pla84] and Goubault <ref> [Gou94] </ref> studied complete problems in first-order logic. In particular, [Pla84] studied the problem of determining the minimal depth of binary resolution proofs from a set of clauses. <p> A series of completeness results was obtained, ranging from nondeterministic exponential time 54 for general first-order clauses to exponential time for some restricted subsets. Many other completeness results related to first-order logic were also presented in [Pla84]. Goubault <ref> [Gou94] </ref> showed that the problem of determining the minimal number of copies of first-order clauses needed for a proof is p 2 complete. The paper [Let93] studies how accurately the length of a derivation reflects the actual complexity of a proof. <p> Some previous studies of the complexities of various resource-bounded theorem proving problems <ref> [Pla84, Gou94] </ref> are relevant here. We mention some of these results, and state their relevance at the end of this section. <p> that is, clauses of size exponential in n are possibly needed for n step resolution proofs. 2 Theorem 4.3.4 The problem of determining whether a set S of first-order clauses has a Herbrand set T with jT j = d is p 2 complete, where d is represented in unary <ref> [Gou94] </ref>. Theorem 4.3.5 The satisfiability problem for Schonfinkel-Bernays form formulas is NEXPTIME complete [Pla84].
Reference: [Gre81] <author> C. C. Green. </author> <title> Application of theorem proving to problem solving. </title> <booktitle> Readings in Artificial Intelligence, </booktitle> <year> 1981. </year>
Reference-contexts: Similar results hold for some other restricted class of situation calculus planning problems. Thus for planning problems with polynomial size solutions, ADL based planners are no worse than STRIPS based planners, both of which usually have single exponential complexity. Green <ref> [Gre81] </ref> pioneered the work in applying general theorem proving strategies in the domain of problem solving. He used binary resolution as a planning strategy. Only some very simple planning problems could be solved. In fact, simple blocks world problems are still difficult for most state-of-the-art theorem provers [SS97].
Reference: [Hak85] <author> A. Haken. </author> <title> The intractability of resolution. </title> <journal> Theoretical Computer Science, </journal> <volume> 39 </volume> <pages> 297-308, </pages> <year> 1985. </year>
Reference-contexts: In the past, there has been some work on the complexity of theorem proving, but mostly with respect to the length of proofs, and not to the difficulty of finding a proof. For some examples of studies of proof lengths, see <ref> [CR79, Hak85, Urq87, Ede92] </ref>. Haken [Hak85] showed that for a set of propositional problems, known as the pigeonhole problems, resolution needs to generate an exponential number of clauses in order to find a proof. This means that the minimum proof length is exponential for resolution, in this case. <p> In the past, there has been some work on the complexity of theorem proving, but mostly with respect to the length of proofs, and not to the difficulty of finding a proof. For some examples of studies of proof lengths, see [CR79, Hak85, Urq87, Ede92]. Haken <ref> [Hak85] </ref> showed that for a set of propositional problems, known as the pigeonhole problems, resolution needs to generate an exponential number of clauses in order to find a proof. This means that the minimum proof length is exponential for resolution, in this case. <p> The A-resolution proof has a length of at most exp (M dup fl s lin (S)). Since A-resolution is a special case of binary resolution, the depth measure and the length measure are polynomially bounded and exponentially bounded with respect to the duplication measure, respectively. Haken <ref> [Hak85] </ref> shows that an exponential number of resolvents are needed for every resolution proof of the pigeon hole problem. The pigeon hole problem is a propositional problem, and its duplication measure equals the input clause size. Thus the bound of length measure with respect to duplication measure is tight.
Reference: [KB70] <author> D.E. Knuth and P.B. Bendix. </author> <title> Simple word problems in universal algebras. </title> <booktitle> In Computational Problems in Abstract Algebra, </booktitle> <pages> pages 263-297. </pages> <publisher> Pergamon, Oxford, </publisher> <address> U.K., </address> <year> 1970. </year>
Reference-contexts: Chapter 3 AC Equality 3.1 Introduction Knuth-Bendix completion <ref> [KB70] </ref> and its extensions [BDP89] have been widely used for equational reasoning. One of the main bottlenecks of completion-based inference strategies is the large number of critical pairs generated. This is particularly evident when dealing with equational problems involving associative and commutative functions, i.e. AC equational problems. <p> Theorem 3.2.2 can be used to generate such 41 new rules. The idea is to add rule s ! t or s t to R while preserving termination for all critical pairs (s; t) in R. The algorithm was initially proposed in <ref> [KB70] </ref>, and is often called Knuth-Bendix completion, or simply, completion. Sometimes, it is not possible to orient an equation, i.e. f (x; y) = f (y; x), in either direction while maintaining termination. Thus Knuth- Bendix completion is extended to unfailing completion [BDP89].
Reference: [KBL93] <author> H. Kleine Buening and T. Lettman. </author> <title> Search space and average proof length of resolution. </title> <type> Unpublished, </type> <year> 1993. </year>
Reference-contexts: The difficulty of finding a proof, and the size of the search space, are more relevant for the efficiency of theorem provers than the size of a minimal proof. There has been very little work on search space size. The paper <ref> [KBL93] </ref> shows that many refinements of resolution do not increase a certain measure of search space size by more than a factor of four, but does not compare refinements with one another.
Reference: [Klo92] <author> Jan Willem Klop. </author> <title> Term rewriting systems. </title> <editor> In S. Abramsky, D. M. Gab-bay, and T. S. E. Maibaum, editors, </editor> <booktitle> Handbook of Logic in Computer Science, </booktitle> <volume> volume 2, chapter 1, </volume> <pages> pages 1 - 117. </pages> <publisher> Oxford University Press, Oxford, </publisher> <year> 1992. </year>
Reference-contexts: We then present the consistent unfailing completion algorithm and show its completeness. We then show how the algorithm can be refined and implemented. We give some test results in the end. 3.2 Background In this section, we define some relevant concepts. For surveys on equational reasoning, see <ref> [Pla93, DJ90, Klo92] </ref>. We follow the conventions of [Pla93]. We use f; g; h; : : : as function symbols, a; b; c; : : : as constant symbols, and x; y; z; : : : as variables.
Reference: [KMS96] <author> H. Kautz, D. McAllester, and B. Selman. </author> <title> Encoding plans in propositional logic. </title> <booktitle> In Proceedings of Knowledge Representation and Reasoning (KR-96), </booktitle> <address> Boston,MA, </address> <year> 1996. </year>
Reference-contexts: The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local search. There are also some recent works in applying efficient propositional procedure in various domains. In <ref> [KMS96] </ref>, planning problems are converted into propositional problems, and efficient propositional decision procedures are used to tackle those problems. Interestingly, that approach shares the same principle as OSHL and instance-based strategies in general. <p> In OSHL, the original problems are expressed in first order logic, the conversion to propositional problems is automatic, and the domain knowledge is represented in the form of input semantics. OSHL might not be as efficient as the method of <ref> [KMS96] </ref> on specific problems, but is perhaps more general. The history of instance-based theorem proving strategies traces back to [Gil60]. <p> He used binary resolution as a planning strategy. Only some very simple planning problems could be solved. In fact, simple blocks world problems are still difficult for most state-of-the-art theorem provers [SS97]. The work by Kautz and Selman <ref> [KS92, KS96, KMS96] </ref> represents another approach of applying general theorem proving techniques in planning. They convert STRIPS planning problems into propositional satisfiability problems. One advantage of using 85 propositional logic is that much of the first-order inference overhead can be avoided. <p> For the purpose of complexity analysis, we are only interested in planning problems with polynomial-length solutions, or equivalently, bounded planning problems <ref> [KMS96] </ref>. Without the polynomial-length constraint, the STRIPS planning problems is PSPACE complete [Byl91] and the ADL planning problem is undecidable [Ped94].
Reference: [KS92] <author> H. Kautz and B. Selman. </author> <title> Planning as satisfiablity. </title> <booktitle> In Proc. ECAI-92, </booktitle> <pages> pages 359-363, </pages> <address> Vienna, Austria, </address> <year> 1992. </year>
Reference-contexts: He used binary resolution as a planning strategy. Only some very simple planning problems could be solved. In fact, simple blocks world problems are still difficult for most state-of-the-art theorem provers [SS97]. The work by Kautz and Selman <ref> [KS92, KS96, KMS96] </ref> represents another approach of applying general theorem proving techniques in planning. They convert STRIPS planning problems into propositional satisfiability problems. One advantage of using 85 propositional logic is that much of the first-order inference overhead can be avoided.
Reference: [KS96] <author> H. Kautz and B. Selman. </author> <title> Pushing teh envelope: Planning, propositional logic, and stochastic search. </title> <booktitle> In Proceeding of the Thirteenth National conference of Artificial Intelligence (AAAI-96), </booktitle> <address> Portland, OR, </address> <year> 1996. </year>
Reference-contexts: He used binary resolution as a planning strategy. Only some very simple planning problems could be solved. In fact, simple blocks world problems are still difficult for most state-of-the-art theorem provers [SS97]. The work by Kautz and Selman <ref> [KS92, KS96, KMS96] </ref> represents another approach of applying general theorem proving techniques in planning. They convert STRIPS planning problems into propositional satisfiability problems. One advantage of using 85 propositional logic is that much of the first-order inference overhead can be avoided.
Reference: [KSZ90] <author> D. Kapur, G. Sivakumar, and H. Zhang. </author> <title> A new method for proving termination of ac-rewrite systems. </title> <booktitle> In Proc. of Tenth Conference on Foundations of Software Technology and Theoretical Computer Science, </booktitle> <pages> pages 133-148, </pages> <address> December 1990. </address> <publisher> Springer Verlag LNCS 472. </publisher>
Reference-contexts: Special AC termination orderings are also needed to show termination of AC rewriting systems. Many commonly used orderings, such as recursive path ordering and lexicographic path ordering, are no longer well founded when flattened terms are used. Several AC termination ordering have been devised <ref> [DHJP83, BP85, KSZ90] </ref>. We study the problem of AC equational reasoning with a different approach. We propose a procedure called consistent unfailing completion in which only consistent rules and equations are used for critical pair generation and rewriting.
Reference: [LB77] <author> D. Lankford and A.M. Ballantyne. </author> <title> Decision problems for simple equational theories with commutative-associative axioms: Complete sets of commutative-associative reductions. </title> <type> Technical Report Memo ATP-39, </type> <institution> Department of Mathematics and Computer Science, University of Texas, Austin, TX, </institution> <year> 1977. </year> <month> 141 </month>
Reference-contexts: Since com 38 mutativity axioms cannot be oriented in any termination ordering, many of these new rewrite rules generated from the AC axioms cannot be simplified. Special completion procedures are developed to handle AC equational problems <ref> [LB77, PS81] </ref>. Most of the approaches in the literature use flattened terms. For example, +(a; +(b; c)) is represented as +(a; b; c), and terms +(a; c; b) and +(a; b; c) are considered identical. A flattened term represents all terms equivalent up to the AC axioms.
Reference: [Let93] <author> R. Letz. </author> <title> On the polynomial transparency of resolution. </title> <booktitle> In Proceedings of the 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 123-129, </pages> <year> 1993. </year>
Reference-contexts: Many other completeness results related to first-order logic were also presented in [Pla84]. Goubault [Gou94] showed that the problem of determining the minimal number of copies of first-order clauses needed for a proof is p 2 complete. The paper <ref> [Let93] </ref> studies how accurately the length of a derivation reflects the actual complexity of a proof. Hsiang and Bonacina [BH96] present a formalism that facilitates the study of infinite search spaces. [PZ97a] presents complexity analysis of common theorem proving strategies with respect to a number of complexity measures. <p> Note that we need to include the maximum clause size as an input, otherwise we cannot obtain the NP membership of the problem. This is due to the fact that resolution for first-order logic is not polynomially transparent <ref> [Let93] </ref>, that is, clauses of size exponential in n are possibly needed for n step resolution proofs. 2 Theorem 4.3.4 The problem of determining whether a set S of first-order clauses has a Herbrand set T with jT j = d is p 2 complete, where d is represented in unary
Reference: [Lif87] <author> V. Lifschitz. </author> <title> On the semantics of STRIPS. </title> <booktitle> In Reasoning about Actions and Plans: Proceedings of the 1986 Workshop, </booktitle> <address> Timberline,Oregon, </address> <year> 1987. </year>
Reference-contexts: Most of them can be understood as extending the original STRIPS while still allowing efficient search strategies. For example, UCPOP [PW92] uses an extension of STRIPS that permits conditional effects and some limited form of universal quantification. The original STRIPS lacks a well-defined semantics <ref> [Lif87] </ref>. ADL [Ped94] is a remedy of STRIPS in that respect. ADL is closely related to situation calculus [MH69]. In fact, ADL can be regarded as a restriction of situation calculus representation in which frame axioms need not to be explicitly stated.
Reference: [LO85] <author> E. Lusk and R. Overbeek. </author> <title> Non-horn problems. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 1 </volume> <pages> 103-114, </pages> <year> 1985. </year>
Reference-contexts: Natural semantics greatly reduces the number of instances generated by OSHL. e4, e5 and e6 show the effect of equality support in the prover. They, along with e1-e3, are a set of equality benchmark problems proposed by <ref> [LO85] </ref>. None of them can be solved without the equality support. In the current implementation, OSHL is not as efficient on pure equality problems as OTTER. We ran OSHL on some problems from the TPTP library. Natural semantics plays an important role in guiding the search of OSHL. <p> Consistency checking based on these conditions greatly reduces the number of critical pairs generated. In the examples that we tested, it shows a factor of 2 to 4 reduction. Our implementation of the algorithm is able to solve all six of equality benchmark problems proposed by <ref> [LO85] </ref>. Consistent unfailing completion does not require the use of flattened terms. AC 39 unification is therefore not needed. Detecting inconsistency takes quadratic time and generating consistent permutations takes single exponential time. Thus the worse case complexity of an inference rule in consistent unfailing completion is single exponential. <p> We implemented the algorithm with several hundred lines of Prolog code. We tested a number of pure equality problems. The results are listed in Table 3.1. E1-E6 is a set of benchmark problems for equality proposed by <ref> [LO85] </ref>. E3 and E6 involve AC functions. wos21 (equality version), RNG015-6, RNG023-6 and RNG024-6 are problems from ring theory, all four of them involve AC functions. For non-AC problems, the procedure is same as unfailing completion.
Reference: [Lov78] <author> D. Loveland. </author> <title> Automated Theorem Proving: A Logical Basis. </title> <publisher> North-Holland, </publisher> <address> New York, </address> <year> 1978. </year>
Reference-contexts: It is an A resolution if the literals (or subsets) B 1 and B 2 of resolution are minimal in their respective clauses in a pre-specified ordering on literals. For a discussion of these strategies, see <ref> [CL73, Fit90, Lov78, WOLB84] </ref>. Definition 4.2.5 A resolution proof from a set S of clauses is a sequence C 1 ; C 2 ; ; C n of clauses, where each C i is either in S or is a resolvent of previous clauses in the sequence.
Reference: [LP92] <author> S.-J. Lee and D. Plaisted. </author> <title> Eliminating duplication with the hyper-linking strategy. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 9(1) </volume> <pages> 25-42, </pages> <year> 1992. </year>
Reference-contexts: The semantics allowed in these provers were somewhat limited. CLIN <ref> [LP92] </ref> uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking. RRTP [PP97] generates instances using replacement rules and uses the propositional decision procedure in [ZS94] to detect propositional unsatisfiability. Ordered semantic hyper linking improves upon CLIN and CLIN-S.
Reference: [McC90] <author> W. McCune. </author> <title> Otter 2.0 (theorem prover). In M.E. </title> <editor> Stickel, editor, </editor> <booktitle> Proceedings of the 10th International Conference on Automated Deduction, </booktitle> <pages> pages 663-4, </pages> <month> July </month> <year> 1990. </year>
Reference-contexts: OSHL allows any semantics that can be expressed as a ground decision procedure. OSHL imposes an ordering on all ground literals. The ordering allows OSHL to have a more systematic model generation strategy. OSHL also permits the use of term rewriting to handle equalities. We compare OSHL with OTTER <ref> [McC90] </ref>, which is a state of the art resolution-based theorem prover. OTTER is very efficient in handling Horn problems and UR resolvable problems. It is not as efficient for non-Horn problems. This is particu 7 larly evident on examples like the pigeon-hole problems.
Reference: [McC94] <author> William W. McCune. </author> <title> OTTER 3.0 Reference Manual and Guide. </title> <institution> Argonne National Laboratory, Argonne, Illinois, </institution> <month> January </month> <year> 1994. </year>
Reference-contexts: Table 2.2 shows some of the problems that OSHL solved. We compare OSHL with a state of the art theorem prover OTTER <ref> [McC94] </ref> developed at Argonne National Labs. <p> Our implementation serves the purpose of demonstrating the effect of consistency checking. A state of the art equality prover can solve most of the listed problems in tens of seconds <ref> [McC94] </ref>. Efficient data structures can be used to improve our current implementation.
Reference: [McC96] <author> W. McCune. </author> <title> Solution of the robbins problem. </title> <type> draft, </type> <year> 1996. </year>
Reference-contexts: AC equational reasoning is often difficult. One of the most recognized recent successes of automated reasoning, perhaps of AI in general, is the solution of the Robbins Problem. The Robbins Problem is in fact formulated as an equational problem with AC functions <ref> [McC96] </ref>. Most term rewriting systems do inferences by generating rewrite rules using Knuth-Bendix completion or unfailing completion. Associativity axioms and commutativity axioms often cause an explosion of new rewrite rules in the completion process.
Reference: [McD82] <author> Drew McDermott. </author> <title> A temporal logic for reasoning about processes and plans. </title> <journal> Cognitive Sciences, </journal> <volume> 6 </volume> <pages> 101-155, </pages> <year> 1982. </year>
Reference-contexts: There are several different understandings of what the frame problem refers to [Geo87]. We are concerned with efficient axiomatization of the frame axioms, which are used to describe properties that are not affected by an action [MH69]. A number of approaches have been proposed to handle this problem <ref> [Rei80, McD82] </ref>. We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes [Ped89, Sch90, Rei91].
Reference: [MH69] <author> J. McCarthy and P. J. Hayes. </author> <booktitle> Some philosophical problems from the standpoint of artificial intelligence, </booktitle> <pages> pages 463-502. </pages> <publisher> Ellis Horwood, </publisher> <address> Chichester, England, </address> <year> 1969. </year>
Reference-contexts: For example, UCPOP [PW92] uses an extension of STRIPS that permits conditional effects and some limited form of universal quantification. The original STRIPS lacks a well-defined semantics [Lif87]. ADL [Ped94] is a remedy of STRIPS in that respect. ADL is closely related to situation calculus <ref> [MH69] </ref>. In fact, ADL can be regarded as a restriction of situation calculus representation in which frame axioms need not to be explicitly stated. State-transition model [Ped94] can be used as the underlying semantics of several common planning representations. <p> There are several different understandings of what the frame problem refers to [Geo87]. We are concerned with efficient axiomatization of the frame axioms, which are used to describe properties that are not affected by an action <ref> [MH69] </ref>. A number of approaches have been proposed to handle this problem [Rei80, McD82]. We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure.
Reference: [Par97] <author> M. Paramasivam. </author> <title> Instance-Based First-Order Methods Using Propositional Calculus Provers. </title> <type> PhD thesis, </type> <institution> University of North Carolina at Chapel Hill, </institution> <year> 1997. </year>
Reference-contexts: Instances of the input clauses are generated using these replacement rules. The generation of these instances in essence corresponds to the expansion of definitions. Replacement rules have been extensively studied in <ref> [Par97] </ref>. We propose a variant of the definitional replacement rule in [Par97]. We also emphasize the automatic generation of definitional replacement rules . Definition 2.8.1 A replacement rule is an expression of the form R ! r L 1 ; L 2 ; : : : ; L n . <p> Instances of the input clauses are generated using these replacement rules. The generation of these instances in essence corresponds to the expansion of definitions. Replacement rules have been extensively studied in <ref> [Par97] </ref>. We propose a variant of the definitional replacement rule in [Par97]. We also emphasize the automatic generation of definitional replacement rules . Definition 2.8.1 A replacement rule is an expression of the form R ! r L 1 ; L 2 ; : : : ; L n . R is called a replacement literal.
Reference: [Ped89] <author> E.P.D. Pednault. </author> <title> ADL: Exploring the middle ground between STRIPS and the situation calculus. </title> <editor> In R. Reiter R.J. Brachman, H. Levesque, editor, </editor> <booktitle> Proceedings of the First International Conference on Principles of Konwledge Representation and Reasoning, </booktitle> <pages> pages 324-332, </pages> <year> 1989. </year>
Reference-contexts: We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes <ref> [Ped89, Sch90, Rei91] </ref>. In [Ped89], the axioms for a domain theory are classified as effect axioms and frame axioms. Positive (Negative) effect axioms assert positive (negative) properties when an action is applied to a state. <p> We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes [Ped89, Sch90, Rei91]. In <ref> [Ped89] </ref>, the axioms for a domain theory are classified as effect axioms and frame axioms. Positive (Negative) effect axioms assert positive (negative) properties when an action is applied to a state. Based on a completeness assumption of fluent pre 97 condition, frame axioms can be derived from effect axioms.
Reference: [Ped94] <author> Edwin P.D. Pednault. </author> <title> ADL and the state-transition model of action. </title> <journal> Journal of Logic and Computation, </journal> <volume> 4 </volume> <pages> 467-512, </pages> <year> 1994. </year>
Reference-contexts: Most of them can be understood as extending the original STRIPS while still allowing efficient search strategies. For example, UCPOP [PW92] uses an extension of STRIPS that permits conditional effects and some limited form of universal quantification. The original STRIPS lacks a well-defined semantics [Lif87]. ADL <ref> [Ped94] </ref> is a remedy of STRIPS in that respect. ADL is closely related to situation calculus [MH69]. In fact, ADL can be regarded as a restriction of situation calculus representation in which frame axioms need not to be explicitly stated. State-transition model [Ped94] can be used as the underlying semantics of <p> ADL <ref> [Ped94] </ref> is a remedy of STRIPS in that respect. ADL is closely related to situation calculus [MH69]. In fact, ADL can be regarded as a restriction of situation calculus representation in which frame axioms need not to be explicitly stated. State-transition model [Ped94] can be used as the underlying semantics of several common planning representations. Finite state automata, STRIPS, ADL, and situation calculus can incrementally represent a larger subset of planning problems in state transition model. The increased expressiveness of planning representations often does not substantially increase its computational complexity. <p> For the purpose of complexity analysis, we are only interested in planning problems with polynomial-length solutions, or equivalently, bounded planning problems [KMS96]. Without the polynomial-length constraint, the STRIPS planning problems is PSPACE complete [Byl91] and the ADL planning problem is undecidable <ref> [Ped94] </ref>. Theorem 5.2.1 The problem of determining whether a binary resolution proof of length n exists for a set of Horn clauses is NP-complete.[PZ97a] Planning problems represented in STRIPS can be represented in first-order logic using situation calculus.
Reference: [Pla84] <author> D. Plaisted. </author> <title> Complete problems in the first-order predicate calculus. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 29 </volume> <pages> 8-35, </pages> <year> 1984. </year> <month> 142 </month>
Reference-contexts: However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. The price for this is that we need to consider specific search strategies, which was not necessary in [Pla94b, Pla94c]. Plaisted <ref> [Pla84] </ref> and Goubault [Gou94] studied complete problems in first-order logic. In particular, [Pla84] studied the problem of determining the minimal depth of binary resolution proofs from a set of clauses. <p> The price for this is that we need to consider specific search strategies, which was not necessary in [Pla94b, Pla94c]. Plaisted <ref> [Pla84] </ref> and Goubault [Gou94] studied complete problems in first-order logic. In particular, [Pla84] studied the problem of determining the minimal depth of binary resolution proofs from a set of clauses. A series of completeness results was obtained, ranging from nondeterministic exponential time 54 for general first-order clauses to exponential time for some restricted subsets. <p> A series of completeness results was obtained, ranging from nondeterministic exponential time 54 for general first-order clauses to exponential time for some restricted subsets. Many other completeness results related to first-order logic were also presented in <ref> [Pla84] </ref>. Goubault [Gou94] showed that the problem of determining the minimal number of copies of first-order clauses needed for a proof is p 2 complete. The paper [Let93] studies how accurately the length of a derivation reflects the actual complexity of a proof. <p> Some previous studies of the complexities of various resource-bounded theorem proving problems <ref> [Pla84, Gou94] </ref> are relevant here. We mention some of these results, and state their relevance at the end of this section. <p> We mention some of these results, and state their relevance at the end of this section. Theorem 4.3.1 The problem of determining whether a depth d binary resolution proof exists from a set S of first-order clauses is NEXPTIME complete, where d is represented in unary <ref> [Pla84] </ref>. Definition 4.3.1 A Horn clause is a clause containing at most one positive literal. Thus f:P; :Q; Rg is a Horn clause. <p> A Horn set is a set of Horn clauses. 66 Theorem 4.3.2 The problem of determining whether a binary resolution proof with length n or less exists from a set S of propositional Horn clauses is NP complete, where n is represented in unary <ref> [Pla84] </ref>. From theorem 4.3.2 we derive a corresponding NP-completeness result for first-order logic. <p> Theorem 4.3.5 The satisfiability problem for Schonfinkel-Bernays form formulas is NEXPTIME complete <ref> [Pla84] </ref>. Theorem 4.3.6 The problem of determining whether a set S of first-order clauses has a Herbrand set T such that every clause D in T has linear size s lin (D) n is co-NEXPTIME hard, where n is represented in unary. 67 Proof.
Reference: [Pla93] <author> D. Plaisted. </author> <title> Equational reasoning and term rewriting systems. </title> <editor> In D. Gab-bay, C. Hogger, J. A. Robinson, and J. Siekmann, editors, </editor> <booktitle> Handbook of Logic in Artificial Intelligence and Logic Programming, </booktitle> <volume> volume 1, </volume> <pages> pages 273-364. </pages> <publisher> Oxford University Press, </publisher> <year> 1993. </year>
Reference-contexts: We then present the consistent unfailing completion algorithm and show its completeness. We then show how the algorithm can be refined and implemented. We give some test results in the end. 3.2 Background In this section, we define some relevant concepts. For surveys on equational reasoning, see <ref> [Pla93, DJ90, Klo92] </ref>. We follow the conventions of [Pla93]. We use f; g; h; : : : as function symbols, a; b; c; : : : as constant symbols, and x; y; z; : : : as variables. <p> We then show how the algorithm can be refined and implemented. We give some test results in the end. 3.2 Background In this section, we define some relevant concepts. For surveys on equational reasoning, see [Pla93, DJ90, Klo92]. We follow the conventions of <ref> [Pla93] </ref>. We use f; g; h; : : : as function symbols, a; b; c; : : : as constant symbols, and x; y; z; : : : as variables. An equation is an expression of the form s = t where s and t are terms. <p> However, the use of equation in rewriting is restricted : s can be replaced by t only if s &gt; t, where &gt; is the termination ordering. We give names to a set of inference rules. We follow the convention used in <ref> [Pla93] </ref>. The rule U CP (!; !) is the operation of generating critical pairs between two rewrite rules. If a critical pair can be oriented, a new rewrite rule is added to the rewrite system; otherwise, an equation is added. U CP stands for Unfailing Critical Pair generation.
Reference: [Pla94a] <author> D. Plaisted. </author> <title> Ordered semantic hyper-linking. </title> <type> Technical Report MPI-I-94-235, </type> <institution> Max-Planck Institut fuer Informatik, Saarbruecken, Germany, </institution> <year> 1994. </year>
Reference-contexts: The only methods that are expected single exponential with respect to M sub or M lin , assuming that a propositional satisfiability test runs in expected polynomial time, are subterm-size bounded clause linking with subterm factoring, subterm-size clause enumeration, or some other enumeration method such as <ref> [Pla94a] </ref>. It will be advantageous to choose goal-sensitive versions of these strategies. The enumerative approach becomes even more feasible when there are equations, since then the enumerative methods only need to generate ground clauses that are in normal form with respect to a term-rewriting system. <p> Since many ground clauses are not in normal form, this increases the sizes of the terms that can be generated. All in all, we feel that these several strategies, especially that of <ref> [Pla94a] </ref>, have the most long-term promise for a generally useful theorem prover. 4.7 Discussions We see that much can be said about the complexities of various strategies on various kinds of input clauses. This can help to give insight into their performance in practice.
Reference: [Pla94b] <author> D. Plaisted. </author> <title> The search efficiency of theorem proving strategies. </title> <booktitle> In Proceedings of the Twelfth International Conference on Automated Deduction, </booktitle> <pages> pages 57-71, </pages> <year> 1994. </year> <booktitle> Lecture Notes in Artificial Intelligence 814. </booktitle>
Reference-contexts: For the problem in Figure 2.3, binary resolution will generate many more resolvents than OSHL. Hyper-resolution greatly reduces the number of possible resolvents, but it still generates redundant clauses, and subsumption tests are needed to remove the redundant clauses. <ref> [Pla94b] </ref> has studied the duplication in various propositional strategies. The ground procedure of ordered semantic hyper linking is in essence similar to the Davis-Putnam method [DP60]. Both are model finding strategies, and prove unsatisfiability by exhausting all possible models. <p> Their paper considers monotone refinements of resolution; these do not allow deletion operations such as deletion of subsumed clauses. However, the results are otherwise very general. An exception is the work of Plaisted <ref> [Pla94b, Pla94c] </ref>, which considers the size of the search space generated. However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. <p> <ref> [Pla94b, Pla94c] </ref>, which considers the size of the search space generated. However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. The price for this is that we need to consider specific search strategies, which was not necessary in [Pla94b, Pla94c]. Plaisted [Pla84] and Goubault [Gou94] studied complete problems in first-order logic. In particular, [Pla84] studied the problem of determining the minimal depth of binary resolution proofs from a set of clauses. <p> All-positive (all-negative) resolution is a restriction of resolution where the resolvent contains no positive (negative) literals. All-positive resolution is a goal sensitive strategy, however it is inefficient in practice. All-positive resolution is exponential for propositional Horn clauses; while hyper-resolution, or all-negative resolution, is linear <ref> [Pla94b] </ref>. Hyper-resolution is essentially a forward chaining technique. It is proof sensitive, but not goal sensitive. For planning problems represented in situation calculus, forward chaining generates all states reachable from the initial state and all properties of these states. <p> The properties of those states are usually represented by disjunctions, and the axioms describing these properties are non-Horn first-order clauses. Resolution based strategies do not handle non-Horn problems well. One inefficiency comes from the so-called "duplication by combination" <ref> [Pla94b] </ref>. Namely, resolution combines literals from different clauses. In a proof for a non-Horn problem, one literal might be propagated to many clauses. That often causes the search to be buried in a large number of big clauses.
Reference: [Pla94c] <author> D. Plaisted. </author> <title> The search efficiency of theorem proving strategies: an analytical comparison. </title> <type> Technical Report MPI-I-94-233, </type> <institution> Max-Planck Institut fuer Informatik, Saarbruecken, Germany, </institution> <year> 1994. </year>
Reference-contexts: Their paper considers monotone refinements of resolution; these do not allow deletion operations such as deletion of subsumed clauses. However, the results are otherwise very general. An exception is the work of Plaisted <ref> [Pla94b, Pla94c] </ref>, which considers the size of the search space generated. However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. <p> <ref> [Pla94b, Pla94c] </ref>, which considers the size of the search space generated. However, this work is largely propositional in nature, and here we wish to extend this work to an inherently first-order context. The price for this is that we need to consider specific search strategies, which was not necessary in [Pla94b, Pla94c]. Plaisted [Pla84] and Goubault [Gou94] studied complete problems in first-order logic. In particular, [Pla84] studied the problem of determining the minimal depth of binary resolution proofs from a set of clauses.
Reference: [PP97] <author> M. Paramasivam and D. </author> <title> A Plaisted. A Replacement Rule Theorem Prover. </title> <journal> Journal of Automated Reasoning, </journal> <volume> Forthcoming, </volume> <year> 1997. </year>
Reference-contexts: The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking. RRTP <ref> [PP97] </ref> generates instances using replacement rules and uses the propositional decision procedure in [ZS94] to detect propositional unsatisfiability. Ordered semantic hyper linking improves upon CLIN and CLIN-S. CLIN-S restricts input semantics to be either finite or decidable. OSHL allows any semantics that can be expressed as a ground decision procedure.
Reference: [PS81] <author> G.E. Peterson and M.E. Stickel. </author> <title> Complete sets of reductions for some equational theories. </title> <journal> J. Assoc. Comput. Mach., </journal> <volume> 28(2) </volume> <pages> 233-264, </pages> <year> 1981. </year>
Reference-contexts: Since com 38 mutativity axioms cannot be oriented in any termination ordering, many of these new rewrite rules generated from the AC axioms cannot be simplified. Special completion procedures are developed to handle AC equational problems <ref> [LB77, PS81] </ref>. Most of the approaches in the literature use flattened terms. For example, +(a; +(b; c)) is represented as +(a; b; c), and terms +(a; c; b) and +(a; b; c) are considered identical. A flattened term represents all terms equivalent up to the AC axioms.
Reference: [PSK95] <author> D. Plaisted and Andrea Sattler-Klein. </author> <title> Proof lengths for equational completion. </title> <type> Technical Report SEKI Report SR-95-06, </type> <institution> University of Kaiser-slautern, Kaiserslautern, Germany, </institution> <year> 1995. </year>
Reference-contexts: Each such unification does not increase the subterm size of S 0 , as we showed in <ref> [PSK95] </ref> and above in theorem 4.2.2. Therefore s sub (S 0 fl) s sub (S 0 ) jT js sub (S). Also, let fl 0 be flffi where ffi replaces all remaining variables by a fixed constant symbol. Then by lemma 4.2.8, S 0 fl 0 is unsatisfiable. <p> We encourage others to fill in the gaps we have left. In addition, this analysis can be extended to strategies with special rules for the equality predicate; for a start in this direction, see <ref> [PSK95] </ref>. Still, we believe that we have made a significant contribution. Especially as machines become faster and faster, asymptotic performance will play a larger and larger role, and constant factors of speed obtained by good programming technique or choice of programming language will become relatively less important.
Reference: [PW92] <author> J. S. Penberthy and D. Weld. UCPOP: </author> <title> A sound, complete, partial-order planner for ADL. </title> <booktitle> In The Third International Conference on Knowledge Representation and Reasoning (KR-92), </booktitle> <pages> pages 103-114, </pages> <address> Cambridge, Mas-sachusetts, </address> <year> 1992. </year>
Reference-contexts: However, the expressiveness of the original STRIPS is very limited; it does not allow conditional effect, universal quantification, etc. A spectrum of planning representations has been proposed. Most of them can be understood as extending the original STRIPS while still allowing efficient search strategies. For example, UCPOP <ref> [PW92] </ref> uses an extension of STRIPS that permits conditional effects and some limited form of universal quantification. The original STRIPS lacks a well-defined semantics [Lif87]. ADL [Ped94] is a remedy of STRIPS in that respect. ADL is closely related to situation calculus [MH69].
Reference: [PZ96] <author> M. Paramasivam and Y. Zhu. </author> <type> Personal communications, </type> <year> 1996. </year>
Reference-contexts: The proof depth complexity measure M pd (S) of S is the minimum depth of any resolution proof of the empty clause from S. We note that although the depth measure is inference based, it can be viewed in a non-inference based manner, as suggested by <ref> [PZ96] </ref>. It is equivalent to the depth of the minimal closed binary semantic tree over S (for the definition of this, see for 58 example [CL73]). In general, the proof complexity measures that are not inference based are instance based, that is, they are based on Herbrand's theorem.
Reference: [PZ97a] <author> D. Plaisted and Y. Zhu. </author> <title> The Efficiency Theorem Proving Strategies : A Comparative and Asymptotic Analysis. </title> <publisher> Vieweg, </publisher> <year> 1997. </year>
Reference-contexts: We also study the relationships among different complexity measures. We show that ordered semantic hyper linking is asymptotically efficient with respect to instance-based complexity measures. This confirms the experimental result that ordered semantic hyper linking is efficient on near-propositional problems. <ref> [PZ97a] </ref> presents a thorough analysis of various theorem proving strategies. It reveals some theoretical inefficiencies in several traditional theorem proving strategies and suggests possible improvements. We investigate the application of first-order deduction in AI planning. <p> Chapter 4 describes the complexity analysis of ordered semantic hyper linking and other theorem proving strategies. Chapter 5 contains the work in AI planning. Chapter 4 is a part of joint work by Prof. Plaisted and the author on first-order complexity analysis <ref> [PZ97a] </ref>. Preliminary versions of materials in Chapter 2,3 and 5 have been published as [PZ97d, PZ97b, PZ97c, PZ97e]. Chapter 2 Ordered Semantic Hyper Linking 2.1 Introduction In this chapter, we present a novel first order theorem proving strategy ordered semantic hyper linking. <p> The paper [Let93] studies how accurately the length of a derivation reflects the actual complexity of a proof. Hsiang and Bonacina [BH96] present a formalism that facilitates the study of infinite search spaces. <ref> [PZ97a] </ref> presents complexity analysis of common theorem proving strategies with respect to a number of complexity measures. The analysis has a number of interesting implications. For example, reveals a number of surprising differences in efficiency among common theorem proving strategies, ranging from single to quintuple exponential. <p> Thus we expect the worst case complexity to be single exponential. However, one has to be careful in selecting theorem proving strategies and organizing the search. For instance, binary resolution is single exponential with respect to the length measure and double exponential with respect to the depth measure <ref> [PZ97a] </ref>. For STRIPS problems (represented in first-order logic), both length measure and depth measure of binary resolution are linearly bounded by the length of the plan. Thus length-bound binary resolution is single exponential on the STRIPS planning problems, while depth-bound binary resolution is double exponential.
Reference: [PZ97b] <author> D. Plaisted and Y. Zhu. </author> <title> Equational reasoning using ac constraints. </title> <booktitle> In Proceedings of the Fifteenth International Joint Conference on Artificial Intelligence (IJCAI-97), </booktitle> <pages> pages 108-113, </pages> <month> August </month> <year> 1997. </year>
Reference-contexts: Chapter 5 contains the work in AI planning. Chapter 4 is a part of joint work by Prof. Plaisted and the author on first-order complexity analysis [PZ97a]. Preliminary versions of materials in Chapter 2,3 and 5 have been published as <ref> [PZ97d, PZ97b, PZ97c, PZ97e] </ref>. Chapter 2 Ordered Semantic Hyper Linking 2.1 Introduction In this chapter, we present a novel first order theorem proving strategy ordered semantic hyper linking. Ordered semantic hyper linking (OSHL) is an instance-based refutational theorem proving strategy. It is sound and complete. OSHL is proposi-tionally efficient.
Reference: [PZ97c] <author> D. Plaisted and Y. Zhu. FOLPLAN: </author> <title> A semantically guided first-order planner. </title> <booktitle> In Proceedings of the 10th International FLAIRS Conference, </booktitle> <month> May </month> <year> 1997. </year> <month> 143 </month>
Reference-contexts: Chapter 5 contains the work in AI planning. Chapter 4 is a part of joint work by Prof. Plaisted and the author on first-order complexity analysis [PZ97a]. Preliminary versions of materials in Chapter 2,3 and 5 have been published as <ref> [PZ97d, PZ97b, PZ97c, PZ97e] </ref>. Chapter 2 Ordered Semantic Hyper Linking 2.1 Introduction In this chapter, we present a novel first order theorem proving strategy ordered semantic hyper linking. Ordered semantic hyper linking (OSHL) is an instance-based refutational theorem proving strategy. It is sound and complete. OSHL is proposi-tionally efficient.
Reference: [PZ97d] <author> D. Plaisted and Y. Zhu. </author> <title> Ordered semantic hyper linking. </title> <booktitle> In Proceedings of the Fourteenth National Conference on Artificial Intelligence(AAAI-97), </booktitle> <pages> pages 472-477, </pages> <month> July </month> <year> 1997. </year>
Reference-contexts: Chapter 5 contains the work in AI planning. Chapter 4 is a part of joint work by Prof. Plaisted and the author on first-order complexity analysis [PZ97a]. Preliminary versions of materials in Chapter 2,3 and 5 have been published as <ref> [PZ97d, PZ97b, PZ97c, PZ97e] </ref>. Chapter 2 Ordered Semantic Hyper Linking 2.1 Introduction In this chapter, we present a novel first order theorem proving strategy ordered semantic hyper linking. Ordered semantic hyper linking (OSHL) is an instance-based refutational theorem proving strategy. It is sound and complete. OSHL is proposi-tionally efficient.
Reference: [PZ97e] <author> D. Plaisted and Y. Zhu. </author> <title> Situation calculus with aspect. </title> <booktitle> In IASTED International Conference on Artificial Intelligence and Soft Computing'97, </booktitle> <month> July </month> <year> 1997. </year>
Reference-contexts: Chapter 5 contains the work in AI planning. Chapter 4 is a part of joint work by Prof. Plaisted and the author on first-order complexity analysis [PZ97a]. Preliminary versions of materials in Chapter 2,3 and 5 have been published as <ref> [PZ97d, PZ97b, PZ97c, PZ97e] </ref>. Chapter 2 Ordered Semantic Hyper Linking 2.1 Introduction In this chapter, we present a novel first order theorem proving strategy ordered semantic hyper linking. Ordered semantic hyper linking (OSHL) is an instance-based refutational theorem proving strategy. It is sound and complete. OSHL is proposi-tionally efficient.
Reference: [Rei80] <author> Raymond Reiter. </author> <title> A logic for default reasoning. </title> <journal> Artificial Intelligence, </journal> <volume> 13 </volume> <pages> 81-132, </pages> <year> 1980. </year>
Reference-contexts: There are several different understandings of what the frame problem refers to [Geo87]. We are concerned with efficient axiomatization of the frame axioms, which are used to describe properties that are not affected by an action [MH69]. A number of approaches have been proposed to handle this problem <ref> [Rei80, McD82] </ref>. We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes [Ped89, Sch90, Rei91].
Reference: [Rei91] <author> Raymond Reiter. </author> <title> The frame problem in the situation calculus: A simple solution(sometimes) and a completeness result for goal regression, </title> <address> pages 359-380. </address> <publisher> Academic Press, </publisher> <address> San Diego, CA, </address> <year> 1991. </year>
Reference-contexts: We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes <ref> [Ped89, Sch90, Rei91] </ref>. In [Ped89], the axioms for a domain theory are classified as effect axioms and frame axioms. Positive (Negative) effect axioms assert positive (negative) properties when an action is applied to a state. <p> Only a constant number of axioms are needed for each fluent, and the actions are universally quantified. In <ref> [Rei91] </ref>, Pednault's and Schubert's work are refined and combined into a unified framework. The basic idea is to collect all positive (negative) effect axioms related to a fluent to form a general positive (negative) effect axiom. <p> For the above examples, we say action move affects fluent on; action toss affects fluent head and fluent tail. 2 In <ref> [Rei91] </ref> an extra condition P oss (A; S) is added to the right hand of sentences in (II) and (III). That has the effect of restricting the applicability of certain frame axioms.
Reference: [Rob65] <author> J. Robinson. </author> <title> A machine-oriented logic based on the resolution principle. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 12 </volume> <pages> 23-41, </pages> <year> 1965. </year>
Reference-contexts: An A-resolution proof is a resolution proof in which every resolution is an A-resolution. Definition 4.2.6 A (refutational) theorem proving method is complete if for every unsatisfiable clause set S, there is a proof that S is unsatisfiable using the method. It is known that resolution is complete <ref> [Rob65] </ref>. Many of the refinements of resolution are also complete, including P 1 -deduction, negative resolution, and A-resolution. Unit resolution, however, is not complete. For those who may not be familiar with the refutational style of theorem proving, we give an example.
Reference: [Sch90] <author> L.K. Schubert. </author> <title> Monotonic Solution of the Frame Problem in the Situation Calculus: an Efficient Method for Worlds with Fully Specified Actions, </title> <address> pages 23-67. </address> <publisher> Academic Press, </publisher> <address> Boston, MA, </address> <year> 1990. </year>
Reference-contexts: We are interested in representations within first order logic for which sound and complete mechanized inference engines exist. Many non-monotonic approaches suffer from the absence of even a partial decision procedure. Some recent work related to the frame problem includes <ref> [Ped89, Sch90, Rei91] </ref>. In [Ped89], the axioms for a domain theory are classified as effect axioms and frame axioms. Positive (Negative) effect axioms assert positive (negative) properties when an action is applied to a state. <p> However, vacuous effect axioms are needed to complete a domain theory. The total number of axioms is still O (F fl A), where F is the number of fluents and A is the number of actions. Schubert proposed to use explanation closure axioms to replace the frame axioms <ref> [Sch90] </ref>. Assuming all actions that affect a fluent can be completely characterized, which is referred to as the explanation closure assumption, only O (F ) number of axioms are needed to describe the effect of frame axioms.
Reference: [SK93] <author> B. Selman and H. Kautz. </author> <title> An empirical study of greedy local search for satisfiability testing. </title> <booktitle> In Proceedings of the Eleventh National Conference on Artificial Intelligence(AAAI-93), </booktitle> <address> Washington, DC, </address> <month> July </month> <year> 1993. </year>
Reference-contexts: An interpretation is represented as a decision procedure of ground literals. A natural semantics often greatly reduces the search space of OSHL. 6 There has been some interest recently in studying propositional decision procedures. Although propositional satisfiability test is an NP-complete problem, <ref> [SK93] </ref> has showed that many propositional problems can be solved quickly. Several efficient propositional provers have been implemented [ZS94] [SK93], and they perform very well on a wide range of problems. The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local <p> Although propositional satisfiability test is an NP-complete problem, <ref> [SK93] </ref> has showed that many propositional problems can be solved quickly. Several efficient propositional provers have been implemented [ZS94] [SK93], and they perform very well on a wide range of problems. The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local search. There are also some recent works in applying efficient propositional procedure in various domains. <p> test is an NP-complete problem, <ref> [SK93] </ref> has showed that many propositional problems can be solved quickly. Several efficient propositional provers have been implemented [ZS94] [SK93], and they perform very well on a wide range of problems. The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local search. There are also some recent works in applying efficient propositional procedure in various domains. In [KMS96], planning problems are converted into propositional problems, and efficient propositional decision procedures are used to tackle those problems.
Reference: [Sla67] <author> James R. Slagle. </author> <title> Automatic theorem proving with renamable and semantic resolution. </title> <journal> Journal of the ACM, </journal> <volume> 14(4) </volume> <pages> 687-697, </pages> <month> October </month> <year> 1967. </year>
Reference-contexts: The history of instance-based theorem proving strategies traces back to [Gil60]. Most early attempts were based on enumerating Herbrand instances, and they have not been very successful partially due to the lack of unification and partially due to the lack of control in instance generation. <ref> [GHL63, WB87, Sla67, CP94] </ref> used semantics to guide the search of theorem provers. The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking.
Reference: [SS97] <author> G. Sutcliffe and C.B. Suttner. </author> <title> The Results of the CADE-13 ATP System Competition. </title> <journal> Journal of Automated Reasoning, </journal> <volume> Forthcoming, </volume> <year> 1997. </year>
Reference-contexts: Green [Gre81] pioneered the work in applying general theorem proving strategies in the domain of problem solving. He used binary resolution as a planning strategy. Only some very simple planning problems could be solved. In fact, simple blocks world problems are still difficult for most state-of-the-art theorem provers <ref> [SS97] </ref>. The work by Kautz and Selman [KS92, KS96, KMS96] represents another approach of applying general theorem proving techniques in planning. They convert STRIPS planning problems into propositional satisfiability problems. One advantage of using 85 propositional logic is that much of the first-order inference overhead can be avoided.
Reference: [SSY93] <author> Geoff Sutcliffe, Christian Suttner, and Theodor Yemenis. </author> <title> The TPTP problem library. </title> <type> Technical Report 93/11, </type> <institution> Department of Computer Science, James Cook University, Australia, </institution> <year> 1993. </year>
Reference-contexts: Now mathematical logic serves as a foundation of mathematics. Although Godel's incompleteness theorem shows that there exists no complete and consistent axiomatization of arith-metics, many mathematical theories can be represented in first-order logic. In the TPTP library <ref> [SSY93] </ref>, one will find thousands of mathematical theorems in the group theory, ring theory, set theory, etc. Current state of the art theorem provers can solve many of those theorems automatically. Recently, some open problems in mathematics have been solved by automated theorem provers. <p> Although some hard problems can be solved using a trivial semantics, the prover performs best when a natural semantics is used. We generated natural semantics for many problems by hand. The process is easier than it sounds. For example, many of the GRP problems from TPTP library <ref> [SSY93] </ref> have the same set of axioms.
Reference: [Sti81] <author> M.E. Stickel. </author> <title> A unification algorithm for associative-commutative functions. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 28 </volume> <pages> 423-434, </pages> <year> 1981. </year>
Reference-contexts: A flattened term represents all terms equivalent up to the AC axioms. Since flattening breaks the original term structure, special unification techniques are needed. AC unification algorithms <ref> [Sti81, Dom91, BCM96] </ref> are developed to compute all possible most general unifiers of two terms up to associativity and commutativity axioms. Special AC termination orderings are also needed to show termination of AC rewriting systems.
Reference: [Sti84] <author> M Stickel. </author> <title> A case study of theorem proving by the knuth-bendix method:discovering that x 3 = x implies ring commutativity. </title> <booktitle> In Proceedings of the 7th International Conference on Automated Deduction, </booktitle> <pages> pages 248-258, </pages> <year> 1984. </year>
Reference-contexts: Veroff obtained a proof of the problem using AURA [Ver81]. However, the input contained many additional clauses that facilitated the proof. Stickel was the first to prove the problem with a natural set of input equations <ref> [Sti84] </ref>. The proof took over 14 hours. Zhang and Kapur could find a proof in a few minutes using RRL [ZK90]. Both [Sti84] and [ZK90] used approaches based on AC unifications. <p> However, the input contained many additional clauses that facilitated the proof. Stickel was the first to prove the problem with a natural set of input equations <ref> [Sti84] </ref>. The proof took over 14 hours. Zhang and Kapur could find a proof in a few minutes using RRL [ZK90]. Both [Sti84] and [ZK90] used approaches based on AC unifications.
Reference: [Urq87] <author> A. Urquhart. </author> <title> Hard examples for resolution. </title> <journal> J. ACM, </journal> <volume> 34(1) </volume> <pages> 209-219, </pages> <year> 1987. </year> <month> 144 </month>
Reference-contexts: In the past, there has been some work on the complexity of theorem proving, but mostly with respect to the length of proofs, and not to the difficulty of finding a proof. For some examples of studies of proof lengths, see <ref> [CR79, Hak85, Urq87, Ede92] </ref>. Haken [Hak85] showed that for a set of propositional problems, known as the pigeonhole problems, resolution needs to generate an exponential number of clauses in order to find a proof. This means that the minimum proof length is exponential for resolution, in this case.
Reference: [Ver81] <author> R.L. Veroff. </author> <title> Canonicalization and demodulation. </title> <type> Technical Report ANL-81-6, </type> <institution> Argonne National Laboratory, Argonne, IL, </institution> <year> 1981. </year>
Reference-contexts: Veroff obtained a proof of the problem using AURA <ref> [Ver81] </ref>. However, the input contained many additional clauses that facilitated the proof. Stickel was the first to prove the problem with a natural set of input equations [Sti84]. The proof took over 14 hours. Zhang and Kapur could find a proof in a few minutes using RRL [ZK90].
Reference: [WB87] <author> T.C. Wang and W.W. Bledsoe. </author> <title> Hierarchical deduction. </title> <journal> Journal of Automated Reasoning, </journal> <volume> 3 </volume> <pages> 35-77, </pages> <year> 1987. </year>
Reference-contexts: The history of instance-based theorem proving strategies traces back to [Gil60]. Most early attempts were based on enumerating Herbrand instances, and they have not been very successful partially due to the lack of unification and partially due to the lack of control in instance generation. <ref> [GHL63, WB87, Sla67, CP94] </ref> used semantics to guide the search of theorem provers. The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking.
Reference: [WOLB84] <author> L. Wos, R. Overbeek, E. Lusk, and J. Boyle. </author> <title> Automated Reasoning: Introduction and Applications. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1984. </year>
Reference-contexts: It is an A resolution if the literals (or subsets) B 1 and B 2 of resolution are minimal in their respective clauses in a pre-specified ordering on literals. For a discussion of these strategies, see <ref> [CL73, Fit90, Lov78, WOLB84] </ref>. Definition 4.2.5 A resolution proof from a set S of clauses is a sequence C 1 ; C 2 ; ; C n of clauses, where each C i is either in S or is a resolvent of previous clauses in the sequence.
Reference: [Wos88] <author> L. Wos. </author> <title> Automated Reasoning: 33 Basic Research Problems. </title> <publisher> Prentice Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1988. </year>
Reference: [ZK90] <author> H. Zhang and D. Kapur. </author> <title> Unnecessary inferences in associative-commutative completion procedures. </title> <journal> Mathematical Systems Theory, </journal> <volume> 23 </volume> <pages> 175-206, </pages> <year> 1990. </year>
Reference-contexts: However, the input contained many additional clauses that facilitated the proof. Stickel was the first to prove the problem with a natural set of input equations [Sti84]. The proof took over 14 hours. Zhang and Kapur could find a proof in a few minutes using RRL <ref> [ZK90] </ref>. Both [Sti84] and [ZK90] used approaches based on AC unifications. <p> Stickel was the first to prove the problem with a natural set of input equations [Sti84]. The proof took over 14 hours. Zhang and Kapur could find a proof in a few minutes using RRL <ref> [ZK90] </ref>. Both [Sti84] and [ZK90] used approaches based on AC unifications. <p> We were also able to use the equational prover as a component in a general first order theorem prover. There are some previous works on reducing unnecessary equational inferences, both for AC equational problems and equational problems in general <ref> [ZK90, BGLS92] </ref>. Most of them are fundamentally different from our approach. They reduce the number of critical pairs generated by blocking out certain positions for overlapping two rewrite rules. Some of these works might be combined with our approach.
Reference: [ZS94] <author> Hantao Zhang and Mark E. Stickel. </author> <title> Implementing the Davis-Putnam algorithm by tries. </title> <type> Technical report, </type> <institution> Department of Computer Science, University of Iowa, </institution> <year> 1994. </year>
Reference-contexts: A natural semantics often greatly reduces the search space of OSHL. 6 There has been some interest recently in studying propositional decision procedures. Although propositional satisfiability test is an NP-complete problem, [SK93] has showed that many propositional problems can be solved quickly. Several efficient propositional provers have been implemented <ref> [ZS94] </ref> [SK93], and they perform very well on a wide range of problems. The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local search. There are also some recent works in applying efficient propositional procedure in various domains. <p> Although propositional satisfiability test is an NP-complete problem, [SK93] has showed that many propositional problems can be solved quickly. Several efficient propositional provers have been implemented <ref> [ZS94] </ref> [SK93], and they perform very well on a wide range of problems. The prover in [ZS94] is based on the Davis-Putnam method, and the prover in [SK93] is based on randomized local search. There are also some recent works in applying efficient propositional procedure in various domains. <p> The semantics allowed in these provers were somewhat limited. CLIN [LP92] uses a strategy based on clause linking. CLIN-S [CP94] adds semantic support to clause linking. RRTP [PP97] generates instances using replacement rules and uses the propositional decision procedure in <ref> [ZS94] </ref> to detect propositional unsatisfiability. Ordered semantic hyper linking improves upon CLIN and CLIN-S. CLIN-S restricts input semantics to be either finite or decidable. OSHL allows any semantics that can be expressed as a ground decision procedure. OSHL imposes an ordering on all ground literals.
References-found: 77

