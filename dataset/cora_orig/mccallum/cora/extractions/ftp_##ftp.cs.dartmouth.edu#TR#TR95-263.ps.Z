URL: ftp://ftp.cs.dartmouth.edu/TR/TR95-263.ps.Z
Refering-URL: http://www.cs.dartmouth.edu/reports/abstracts/TR95-263/
Root-URL: http://www.cs.dartmouth.edu
Title: Workloads  
Author: Nils Nieuwejaar and David Kotz Apratim Purakayastha and Carla Schlatter Ellis Michael Best 
Keyword: parallel I/O, file systems, workload characterization, file access pat terns, multiprocessor file systems.  
Address: College,  University,  
Affiliation: Dartmouth  Duke  Thinking Machines Corporation.  
Note: Parallel Scientific  M.I.T., email: mikeb@media.mit.edu; also affiliated with Thinking Machines Corporation. This work was supported in part by the National Science Foundation under grant number CCR-9113170, the National Center for Supercomputing Applications, NASA Ames Research Center under agreement num ber NCC 2-849, and  
Pubnum: Dartmouth Technical Report: PCS-TR95-263  
Email: email: fnils,dfkg@cs.dartmouth.edu.  email: fap,carlag@cs.duke.edu.  
Date: March 29, 1996  
Abstract: Dartmouth College Computer Science Technical Report PCS-TR95-263. Available at URL ftp://ftp.cs.dartmouth.edu/TR/TR95-263.ps.Z. Appears, with small revisions, in IEEE Transactions on Parallel and Distributed Systems. Copyright 1996 IEEE. File-Access Characteristics of Abstract Phenomenal improvements in the computational performance of multiprocessors have not been matched by comparable gains in I/O system performance. This imbalance has resulted in I/O becoming a significant bottleneck for many scientific applications. One key to overcoming this bottleneck is improving the performance of parallel file systems. The design of a high-performance parallel file system requires a comprehensive understanding of the expected workload. Unfortunately, until recently, no general workload studies of parallel file systems have been conducted. The goal of the CHARISMA project was to remedy this problem by characterizing the behavior of several production workloads, on different machines, at the level of individual reads and writes. The first set of results from the CHARISMA project describe the workloads observed on an Intel iPSC/860 and a Thinking Machines CM-5. This paper is intended to compare and contrast these two workloads for an understanding of their essential similarities and differences, isolating common trends and platform-dependent variances. Using this comparison, we are able to gain more insight into the general principles that should guide parallel file-system design. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> David Kotz and Nils Nieuwejaar, </author> <title> "File-system workload on a scientific multiprocessor", </title> <booktitle> IEEE Parallel and Distributed Technology, </booktitle> <pages> pp. 51-60, </pages> <month> Spring </month> <year> 1995. </year>
Reference-contexts: We have so far completed characterization studies on an Intel iPSC/860 at NASA's Ames Research Center <ref> [1] </ref> and on a Thinking Machines CM-5 at the National Center for Supercomputing Applications [2]. <p> As part of the CHARISMA project, we have traced parallel I/O requests by a live, production mix of user programs on an Intel iPSC <ref> [1] </ref> and on a CM-5 [2]. No other study has included more than one machine or programming platform. 2.2 Existing Parallel File Systems A single, coherent model of parallel file-access has not yet emerged. <p> Even when there was not a lot of byte-sharing, there was usually a large amount of block-sharing. Overall, the amount of block sharing implies strong interprocess spatial locality, and suggests that caching at the I/O nodes may improve system performance <ref> [1] </ref>. 5 Conclusions and Recommendations Across the two machines and two programming models covered in this paper, we found important similarities and differences. Compared to uniprocessor workloads, all three parallel workloads used much larger files, and were dominated by writes.
Reference: [2] <author> Apratim Purakayastha, Carla Schlatter Ellis, David Kotz, Nils Nieuwejaar, and Michael Best, </author> <title> "Characterizing parallel file-access patterns on a large-scale multiprocessor", </title> <booktitle> in Proceedings of the Ninth International Parallel Processing Symposium, </booktitle> <month> Apr. </month> <year> 1995, </year> <pages> pp. 165-172. </pages>
Reference-contexts: We have so far completed characterization studies on an Intel iPSC/860 at NASA's Ames Research Center [1] and on a Thinking Machines CM-5 at the National Center for Supercomputing Applications <ref> [2] </ref>. <p> As part of the CHARISMA project, we have traced parallel I/O requests by a live, production mix of user programs on an Intel iPSC [1] and on a CM-5 <ref> [2] </ref>. No other study has included more than one machine or programming platform. 2.2 Existing Parallel File Systems A single, coherent model of parallel file-access has not yet emerged. Parallel-I/O models are often closely tied to a particular machine architecture as well as to a programming model.
Reference: [3] <author> Rick Floyd, </author> <title> "Short-term file reference patterns in a UNIX environment", </title> <type> Tech. Rep. 177, </type> <institution> Dept. of Computer Science, Univ. of Rochester, </institution> <month> Mar. </month> <year> 1986. </year>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [3, 4] </ref> and Ousterhout et al. [5] measured isolated Unix workstations, and Baker et al. measured a distributed Unix system (Sprite) [6]. Ramakrishnan et al. [7] studied access patterns in a commercial computing environment on a VAX/VMS platform. <p> Another common trend across all three platforms was that there were very few files that were both read and written (3.5% in CFS, 5.4% in CMMD, 5.8% in CMF). This behavior is also common in Unix file systems <ref> [3] </ref> and may be accentuated here by the difficulty in coordinating concurrent reads and writes to the same file.
Reference: [4] <author> Richard Allen Floyd and Carla Schlatter Ellis, </author> <title> "Directory reference patterns in hierarchical file systems", </title> <journal> IEEE Transactions on Knowledge and Data Engineering, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 238-247, </pages> <month> June </month> <year> 1989. </year>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis <ref> [3, 4] </ref> and Ousterhout et al. [5] measured isolated Unix workstations, and Baker et al. measured a distributed Unix system (Sprite) [6]. Ramakrishnan et al. [7] studied access patterns in a commercial computing environment on a VAX/VMS platform.
Reference: [5] <author> John Ousterhout, Herve Da Costa, David Harrison, John Kunze, Mike Kupfer, and James Thompson, </author> <title> "A trace driven analysis of the UNIX 4.2 BSD file system", </title> <booktitle> in Proceedings of the Tenth ACM Symposium on Operating Systems Principles, </booktitle> <month> Dec. </month> <year> 1985, </year> <pages> pp. 15-24. </pages>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [3, 4] and Ousterhout et al. <ref> [5] </ref> measured isolated Unix workstations, and Baker et al. measured a distributed Unix system (Sprite) [6]. Ramakrishnan et al. [7] studied access patterns in a commercial computing environment on a VAX/VMS platform. These studies all cover general-purpose (engineering and office) workloads with 2 uniprocessor applications. <p> We conclude that future parallel file systems must focus on providing low latency for small requests as well as high bandwidth for large requests. 4.4 Sequentiality One common characteristic of previous file system workload studies, particularly of scientific workloads, is that files are typically accessed sequentially <ref> [5, 6, 10] </ref>. We define a sequential request to be one that begins at a higher file offset than the point where the previous request from that compute node ended. This is a looser definition of sequential than is used in the studies referred to above.
Reference: [6] <author> Mary G. Baker, John H. Hartman, Michael D. Kupfer, Ken W. Shirriff, and John K. Ousterhout, </author> <title> "Measurements of a distributed file system", </title> <booktitle> in Proceedings of the Thirteenth ACM Symposium on Operating Systems Principles, </booktitle> <year> 1991, </year> <pages> pp. 198-212. </pages>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [3, 4] and Ousterhout et al. [5] measured isolated Unix workstations, and Baker et al. measured a distributed Unix system (Sprite) <ref> [6] </ref>. Ramakrishnan et al. [7] studied access patterns in a commercial computing environment on a VAX/VMS platform. These studies all cover general-purpose (engineering and office) workloads with 2 uniprocessor applications. <p> Fig. 3 shows a wide range in the size of files from system to system. 3 Most of the files accessed with CFS were between 10 KB and 1 MB. Although these files were larger than those in a general-purpose file system <ref> [6] </ref>, they were smaller than we would expect to see in a scientific supercomputing environment [10]. Files on the CM-5 were significantly larger than on the iPSC, and the sizes were much more evenly distributed. <p> We conclude that future parallel file systems must focus on providing low latency for small requests as well as high bandwidth for large requests. 4.4 Sequentiality One common characteristic of previous file system workload studies, particularly of scientific workloads, is that files are typically accessed sequentially <ref> [5, 6, 10] </ref>. We define a sequential request to be one that begins at a higher file offset than the point where the previous request from that compute node ended. This is a looser definition of sequential than is used in the studies referred to above. <p> In uniprocessor and distributed-system workloads, concurrent sharing is known to be uncommon, and writing to concurrently shared files is almost unheard of <ref> [6] </ref>. In a parallel file system, of course, concurrent file sharing among processes within a job is presumably the norm, while concurrent file sharing between jobs is likely to be rare.
Reference: [7] <author> K. K. Ramakrishnan, P. Biswas, and Ramakrishna Karedla, </author> <title> "Analysis of file I/O traces in commercial computing environments", </title> <booktitle> in Proceedings of ACM SIGMETRICS and PERFORMANCE '92, </booktitle> <year> 1992, </year> <pages> pp. 78-90. </pages>
Reference-contexts: General-purpose workstations. Uniprocessor file access patterns have been measured many times. Floyd and Ellis [3, 4] and Ousterhout et al. [5] measured isolated Unix workstations, and Baker et al. measured a distributed Unix system (Sprite) [6]. Ramakrishnan et al. <ref> [7] </ref> studied access patterns in a commercial computing environment on a VAX/VMS platform. These studies all cover general-purpose (engineering and office) workloads with 2 uniprocessor applications.
Reference: [8] <author> Juan Miguel del Rosario and Alok Choudhary, </author> <title> "High performance I/O for parallel computers: Problems and prospects", </title> <journal> IEEE Computer, </journal> <volume> vol. 27, no. 3, </volume> <pages> pp. 59-68, </pages> <month> Mar. </month> <year> 1994. </year>
Reference-contexts: Scientific vector applications. Some studies specifically examined scientific workloads on vector machines. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications <ref> [8] </ref>. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [9]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [10], focusing primarily on access rates.
Reference: [9] <author> Michael L. Powell, </author> <title> "The DEMOS File System", </title> <booktitle> in Proceedings of the Sixth ACM Symposium on Operating Systems Principles, </booktitle> <month> Nov. </month> <year> 1977, </year> <pages> pp. 33-42. </pages>
Reference-contexts: Scientific vector applications. Some studies specifically examined scientific workloads on vector machines. Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [8]. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system <ref> [9] </ref>. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [10], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [11], giving a good picture of long-term, whole-file access patterns.
Reference: [10] <author> Ethan L. Miller and Randy H. Katz, </author> <title> "Input/output behavior of supercomputer applications", </title> <booktitle> in Proceedings of Supercomputing '91, </booktitle> <month> Nov. </month> <year> 1991, </year> <pages> pp. 567-576. </pages>
Reference-contexts: Del Rosario and Choudhary provide an informal characterization of grand-challenge applications [8]. Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [9]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns <ref> [10] </ref>, focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [11], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [12, 13]. <p> Although these files were larger than those in a general-purpose file system [6], they were smaller than we would expect to see in a scientific supercomputing environment <ref> [10] </ref>. Files on the CM-5 were significantly larger than on the iPSC, and the sizes were much more evenly distributed. <p> We conclude that future parallel file systems must focus on providing low latency for small requests as well as high bandwidth for large requests. 4.4 Sequentiality One common characteristic of previous file system workload studies, particularly of scientific workloads, is that files are typically accessed sequentially <ref> [5, 6, 10] </ref>. We define a sequential request to be one that begins at a higher file offset than the point where the previous request from that compute node ended. This is a looser definition of sequential than is used in the studies referred to above.
Reference: [11] <author> Ethan L. Miller and Randy H. Katz, </author> <title> "An analysis of file migration in a UNIX supercomputing environment", </title> <booktitle> in Proceedings of the 1993 Winter USENIX Conference, </booktitle> <month> Jan. </month> <year> 1993, </year> <pages> pp. 421-434. </pages>
Reference-contexts: Powell measured a set of static characteristics (file sizes) of a Cray-1 file system [9]. Miller and Katz traced specific I/O-intensive Cray applications to determine the per-file access patterns [10], focusing primarily on access rates. Miller and Katz also measured secondary-tertiary file migration patterns on a Cray <ref> [11] </ref>, giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate [12, 13]. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads.
Reference: [12] <author> Barbara K. Pasquale and George C. Polyzos, </author> <title> "A static analysis of I/O characteristics of scientific applications in a production workload", </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 388-397. </pages>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [11], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [12, 13] </ref>. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads. Unlike workstation file-system workloads, files tend to be large (many megabytes or gigabytes) and they tend to be accessed with large requests.
Reference: [13] <author> Barbara K. Pasquale and George C. Polyzos, </author> <title> "A case study of a scientific application I/O behavior", </title> <booktitle> in Proceedings of the International Workshop on Modeling, Analysis, </booktitle> <institution> and Simulation of Computer and Telecommunication Systems, </institution> <year> 1994, </year> <pages> pp. 101-106. </pages>
Reference-contexts: Miller and Katz also measured secondary-tertiary file migration patterns on a Cray [11], giving a good picture of long-term, whole-file access patterns. Pasquale and Polyzos studied I/O-intensive Cray applications, focusing on patterns in the I/O rate <ref> [12, 13] </ref>. All of these studies are limited to single-process applications on vector supercomputers. These studies identify several characteristics that are common among supercomputer file-system workloads. Unlike workstation file-system workloads, files tend to be large (many megabytes or gigabytes) and they tend to be accessed with large requests.
Reference: [14] <author> Thomas W. Crockett, </author> <title> "File concepts for parallel I/O", </title> <booktitle> in Proceedings of Supercomputing '89, </booktitle> <year> 1989, </year> <pages> pp. 574-579. </pages>
Reference-contexts: Like workstation workloads, files are typically accessed both completely and sequentially. Scientific parallel applications. Experimental studies of I/O from parallel scientific programs running on multiprocessors are rare. Crockett <ref> [14] </ref> and Kotz [15] hypothesize about the character of a parallel scientific file-system workload. Reddy and Banerjee chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [16]. <p> Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18].
Reference: [15] <author> David Kotz and Carla Schlatter Ellis, </author> <title> "Prefetching in file systems for MIMD multiprocessors", </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> vol. 1, no. 2, </volume> <pages> pp. 218-230, </pages> <month> Apr. </month> <year> 1990. </year>
Reference-contexts: Like workstation workloads, files are typically accessed both completely and sequentially. Scientific parallel applications. Experimental studies of I/O from parallel scientific programs running on multiprocessors are rare. Crockett [14] and Kotz <ref> [15] </ref> hypothesize about the character of a parallel scientific file-system workload. Reddy and Banerjee chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns [16].
Reference: [16] <author> A. L. Narasimha Reddy and Prithviraj Banerjee, </author> <title> "A study of I/O behavior of Perfect benchmarks on a multiprocessor", </title> <booktitle> in Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <year> 1990, </year> <pages> pp. 312-321. </pages>
Reference-contexts: Crockett [14] and Kotz [15] hypothesize about the character of a parallel scientific file-system workload. Reddy and Banerjee chose five sequential scientific applications from the PERFECT benchmarks and parallelized them for an eight-processor Alliant, finding only sequential file-access patterns <ref> [16] </ref>. This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. [17] studied individual parallel scientific applications, measuring temporal 3 patterns in I/O rates.
Reference: [17] <author> R. Cypher, A. Ho, S. Konstantinidou, and P. Messina, </author> <title> "Architectural requirements of parallel scientific applications with explicit communication", </title> <booktitle> in Proceedings of the 20th Annual International Symposium on Computer Architecture, </booktitle> <year> 1993, </year> <pages> pp. 2-13. </pages>
Reference-contexts: This study is interesting, but far from what we need: the sample size is small; the programs are parallelized sequential programs, not parallel programs per se; and the I/O itself was not parallelized. Cypher et al. <ref> [17] </ref> studied individual parallel scientific applications, measuring temporal 3 patterns in I/O rates. Galbreath et al. [18] present a useful high-level characterization based on anecdotal evidence.
Reference: [18] <author> N. Galbreath, W. Gropp, and D. Levine, </author> <title> "Applications-driven parallel I/O", </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 462-471. </pages>
Reference-contexts: Cypher et al. [17] studied individual parallel scientific applications, measuring temporal 3 patterns in I/O rates. Galbreath et al. <ref> [18] </ref> present a useful high-level characterization based on anecdotal evidence. Bagrodia et al. [19] have proposed using Pablo to analyze and characterize specific applications, and Crandall et al. performed such an analysis on three scientific applications [20]. <p> Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers <ref> [30, 31, 25, 18] </ref>. Finally, in addition to shared file pointers, MPI-IO allows applications to describe a mapping from a linear file to the compute nodes running the application in terms of higher-level data structures [32].
Reference: [19] <author> Rajive Bagrodia, Andrew Chien, Yarson Hsu, and Daniel Reed, "Input/output: </author> <title> Instrumentation, characterization, modeling and management policy", </title> <type> Tech. Rep. </type> <institution> CCSF-41, Scalable I/O Initiative, Caltech Concurrent Supercomputing Facilities, Caltech, </institution> <year> 1994. </year>
Reference-contexts: Cypher et al. [17] studied individual parallel scientific applications, measuring temporal 3 patterns in I/O rates. Galbreath et al. [18] present a useful high-level characterization based on anecdotal evidence. Bagrodia et al. <ref> [19] </ref> have proposed using Pablo to analyze and characterize specific applications, and Crandall et al. performed such an analysis on three scientific applications [20].
Reference: [20] <author> Phyllis E. Crandall, Ruth A. Aydt, Andrew A. Chien, and Daniel A. Reed, </author> <title> "Input/output characteristics of scalable parallel applications", </title> <booktitle> in Proceedings of Supercomputing '95, </booktitle> <month> Dec. </month> <year> 1995, </year> <note> To appear. </note>
Reference-contexts: Galbreath et al. [18] present a useful high-level characterization based on anecdotal evidence. Bagrodia et al. [19] have proposed using Pablo to analyze and characterize specific applications, and Crandall et al. performed such an analysis on three scientific applications <ref> [20] </ref>. As part of the CHARISMA project, we have traced parallel I/O requests by a live, production mix of user programs on an Intel iPSC [1] and on a CM-5 [2].
Reference: [21] <author> Paul Pierce, </author> <title> "A concurrent file system for a highly parallel mass storage system", </title> <booktitle> in Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <year> 1989, </year> <pages> pp. 155-160. </pages>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18]. <p> Mode 2 is like mode 1, but enforces a round-robin ordering 5 of accesses across all nodes, and mode 3 is like mode 2 but restricts the access sizes to be identical. More details about CFS, and its performance, can be found in <ref> [21, 35, 36] </ref>. 2.3.2 Thinking Machines CM-5 and the Scalable File System The CM-5 is a distributed-memory machine with many (tens to thousands) SPARC-based Processing Nodes, and a small number of Control Processors (CPs). Processing nodes are logically grouped into partitions, each of which is managed by a CP.
Reference: [22] <author> Paul J. Roy, </author> <title> "Unix file access and caching in a multicomputer environment", </title> <booktitle> in Proceedings of the Usenix Mach III Symposium, </booktitle> <year> 1993, </year> <pages> pp. 21-37. </pages>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18].
Reference: [23] <author> Michael L. Best, Adam Greenberg, Craig Stanfill, and Lewis W. Tucker, </author> <title> "CMMD I/O: A parallel Unix I/O", </title> <booktitle> in Proceedings of the Seventh International Parallel Processing Symposium, </booktitle> <year> 1993, </year> <pages> pp. 489-495. 32 </pages>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18]. <p> CMMD I/O is also layered on top of SFS and, like CFS, provides a variety of I/O modes <ref> [39, 23] </ref>. CMMD's local-independent mode, like mode 0 in CFS, gives each process its own view of the file, and allows each process to make arbitrary requests to the file. In global-independent mode each process has a private file pointer, but all other state is shared.
Reference: [24] <author> David Kotz, </author> <title> "Multiprocessor file system interfaces", </title> <booktitle> in Proceedings of the Second International Conference on Parallel and Distributed Information Systems, </booktitle> <year> 1993, </year> <pages> pp. 194-201. </pages>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18]. <p> The current interface forces the programmer to break down large parallel I/O activities into small, non-consecutive 29 requests. We believe that a control-parallel model should support strided I/O requests from the programmer's interface to the compute node, and from the compute node to the I/O node <ref> [24, 44] </ref>. A strided request can effectively increase the request size, which lowers overhead and introduces opportunities for low-level optimization [45].
Reference: [25] <author> S. R. Chapple and S. M. Trewin, </author> <title> PUL-GF Prototype User Guide, </title> <month> Feb. </month> <year> 1993, </year> <institution> EPCC-KTP-PUL-GF-UG 0.1. [26] "KSR1 technology background", Kendall Square Research, </institution> <month> Jan. </month> <year> 1992. </year>
Reference-contexts: Most extend a traditional file abstraction (a growable, addressable, linear sequence of bytes) with some parallel file-access methods. The most common provide I/O modes that specify whether and how parallel processes share a common file pointer <ref> [14, 21, 22, 23, 24, 25] </ref>. Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18]. <p> Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers <ref> [30, 31, 25, 18] </ref>. Finally, in addition to shared file pointers, MPI-IO allows applications to describe a mapping from a linear file to the compute nodes running the application in terms of higher-level data structures [32].
Reference: [27] <author> Orran Krieger and Michael Stumm, </author> <title> "HFS: a flexible file system for large-scale multiprocessors", </title> <booktitle> in Proceedings of the 1993 DAGS/PC Symposium, </booktitle> <address> Hanover, NH, </address> <month> June </month> <year> 1993, </year> <institution> Dartmouth Institute for Advanced Graduate Studies, </institution> <note> pp. 6-14. </note>
Reference-contexts: The most common provide I/O modes that specify whether and how parallel processes share a common file pointer [14, 21, 22, 23, 24, 25]. Some systems are based on a memory-mapped interface <ref> [26, 27] </ref>, and two provide a way for the user to specify per-process logical views of the file [28, 29]. Some provide SIMD-style transfers [30, 31, 25, 18].
Reference: [28] <author> Peter F. Corbett, Dror G. Feitelson, Jean-Pierre Prost, and Sandra Johnson Baylor, </author> <title> "Parallel access to files in the Vesta file system", </title> <booktitle> in Proceedings of Supercomputing '93, </booktitle> <year> 1993, </year> <pages> pp. 472-481. </pages>
Reference-contexts: Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file <ref> [28, 29] </ref>. Some provide SIMD-style transfers [30, 31, 25, 18]. Finally, in addition to shared file pointers, MPI-IO allows applications to describe a mapping from a linear file to the compute nodes running the application in terms of higher-level data structures [32].
Reference: [29] <author> Erik DeBenedictis and Juan Miguel del Rosario, </author> <title> "nCUBE parallel I/O software", </title> <booktitle> in Eleventh Annual IEEE International Phoenix Conference on Computers and Communications (IPCCC), </booktitle> <month> Apr. </month> <year> 1992, </year> <pages> pp. </pages> <month> 0117-0124. </month> <title> [30] "Connection Machine model CM-2 technical summary", </title> <type> Tech. Rep. </type> <institution> HA87-4, Thinking Machines, </institution> <month> Apr. </month> <year> 1987. </year> <title> [31] "Parallel file I/O routines", </title> <institution> MasPar Computer Corporation, </institution> <year> 1992. </year>
Reference-contexts: Some systems are based on a memory-mapped interface [26, 27], and two provide a way for the user to specify per-process logical views of the file <ref> [28, 29] </ref>. Some provide SIMD-style transfers [30, 31, 25, 18]. Finally, in addition to shared file pointers, MPI-IO allows applications to describe a mapping from a linear file to the compute nodes running the application in terms of higher-level data structures [32].
Reference: [32] <author> Peter Corbett, Dror Feitelson, Yarson Hsu, Jean-Pierre Prost, Marc Snir, Sam Fineberg, Bill Nitzberg, Bernard Traversat, and Parkson Wong, </author> <title> "MPI-IO: a parallel file I/O interface for MPI", </title> <type> Tech. Rep. </type> <institution> NAS-95-002, NASA Ames Research Center, </institution> <month> Jan. </month> <year> 1995, </year> <note> Version 0.3. </note>
Reference-contexts: Some provide SIMD-style transfers [30, 31, 25, 18]. Finally, in addition to shared file pointers, MPI-IO allows applications to describe a mapping from a linear file to the compute nodes running the application in terms of higher-level data structures <ref> [32] </ref>. Clearly, the industrial and research communities have not yet settled on a single new model for file access. Thus, some aspects of a parallel file-system workload are dependent on the particular model provided to the user.
Reference: [33] <institution> Intel Corporation, </institution> <note> iPSC/2 and iPSC/860 User's Guide, </note> <month> Apr. </month> <year> 1991. </year>
Reference-contexts: The I/O nodes are based on the Intel i386 processor and each controls a single SCSI disk drive. There may also be one or more service nodes that handle such things as Ethernet connections or interactive shells <ref> [33] </ref>. At the time of our study, the iPSC/860 at NAS had 128 compute nodes and 10 I/O nodes. Each compute node had 8 MB of memory, and each I/O node had 4 MB of memory and a single 760 MB disk drive [34].
Reference: [34] <institution> NASA Ames Research Center, Moffet Field, CA, NAS User Guide, </institution> <address> 6.1 edition, </address> <month> Mar. </month> <year> 1993. </year>
Reference-contexts: At the time of our study, the iPSC/860 at NAS had 128 compute nodes and 10 I/O nodes. Each compute node had 8 MB of memory, and each I/O node had 4 MB of memory and a single 760 MB disk drive <ref> [34] </ref>. There was also a single service node that handled a 10-Mbit Ethernet connection to the host computer. The total I/O capacity was 7.6 GB and the total bandwidth was less than 10 MB/s. Intel's Concurrent File System (CFS) stripes each file across all disks in 4 KB blocks.
Reference: [35] <author> James C. French, Terrence W. Pratt, and Mriganka Das, </author> <title> "Performance measurement of the Concurrent File System of the Intel iPSC/2 hypercube", </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> vol. 17, no. </volume> <pages> 1-2, pp. 115-121, </pages> <month> January and February </month> <year> 1993. </year>
Reference-contexts: Mode 2 is like mode 1, but enforces a round-robin ordering 5 of accesses across all nodes, and mode 3 is like mode 2 but restricts the access sizes to be identical. More details about CFS, and its performance, can be found in <ref> [21, 35, 36] </ref>. 2.3.2 Thinking Machines CM-5 and the Scalable File System The CM-5 is a distributed-memory machine with many (tens to thousands) SPARC-based Processing Nodes, and a small number of Control Processors (CPs). Processing nodes are logically grouped into partitions, each of which is managed by a CP.
Reference: [36] <author> Bill Nitzberg, </author> <title> "Performance of the iPSC/860 Concurrent File System", </title> <type> Tech. Rep. </type> <institution> RND-92-020, NAS Systems Division, NASA Ames, </institution> <month> Dec. </month> <year> 1992. </year>
Reference-contexts: Mode 2 is like mode 1, but enforces a round-robin ordering 5 of accesses across all nodes, and mode 3 is like mode 2 but restricts the access sizes to be identical. More details about CFS, and its performance, can be found in <ref> [21, 35, 36] </ref>. 2.3.2 Thinking Machines CM-5 and the Scalable File System The CM-5 is a distributed-memory machine with many (tens to thousands) SPARC-based Processing Nodes, and a small number of Control Processors (CPs). Processing nodes are logically grouped into partitions, each of which is managed by a CP. <p> Similarly, 90% of all writes under CFS were for fewer than 100 bytes, but those writes transferred only 3% of all data written. The number of small requests is surprising due to their poor performance in CFS <ref> [36] </ref>. CMMD's interface is 3 As there were many small files as well as several distinct peaks across the whole range of sizes, there was no constant granularity that captured the detail we felt was important in a histogram. <p> As with the iPSC, small requests on the CM-5 are known to perform poorly. Although accesses on the CM-5 were larger than those observed on the iPSC, they were 18 still significantly smaller than the tens or hundreds of kilobytes used in typical performance analyses of these systems <ref> [36, 43] </ref>. Studies have shown that large I/O requests are common in scientific applications running on supercomputers, but we have now seen that small requests are common in scientific applications running on parallel computers.
Reference: [37] <institution> Thinking Machines Corporation, </institution> <type> CM5 Technical Summary, </type> <month> November </month> <year> 1993. </year> <month> 33 </month>
Reference-contexts: Each job executes on a single partition. Generally, each processing node in a partition executes the same program, although they may execute different instructions (SPMD-style). Within individual partitions, jobs are timeshared. The processing nodes communicate via two scalable inter-processor communication networks <ref> [37] </ref>. Although it is possible for users' jobs running in different partitions to communicate with one another, it is rarely done in practice. The CM-5 supports a variety of I/O devices [37, 38]. <p> Within individual partitions, jobs are timeshared. The processing nodes communicate via two scalable inter-processor communication networks [37]. Although it is possible for users' jobs running in different partitions to communicate with one another, it is rarely done in practice. The CM-5 supports a variety of I/O devices <ref> [37, 38] </ref>. This study focuses on the Scalable Disk Array (SDA), as it was the primary high-volume, high-bandwidth storage device on the CM-5 at NCSA. The SDA is an expandable RAID-3 disk system that typically provides I/O bandwidths of 33-264 MB/sec.
Reference: [38] <institution> Thinking Machines Corporation, </institution> <note> CM-5 I/O System Programming Guide Version 7.2, </note> <month> September </month> <year> 1993. </year>
Reference-contexts: Within individual partitions, jobs are timeshared. The processing nodes communicate via two scalable inter-processor communication networks [37]. Although it is possible for users' jobs running in different partitions to communicate with one another, it is rarely done in practice. The CM-5 supports a variety of I/O devices <ref> [37, 38] </ref>. This study focuses on the Scalable Disk Array (SDA), as it was the primary high-volume, high-bandwidth storage device on the CM-5 at NCSA. The SDA is an expandable RAID-3 disk system that typically provides I/O bandwidths of 33-264 MB/sec.
Reference: [39] <institution> Thinking Machines Corporation, </institution> <note> CMMD Reference Manual Version 3.0, </note> <month> May </month> <year> 1993. </year>
Reference-contexts: CMMD I/O is also layered on top of SFS and, like CFS, provides a variety of I/O modes <ref> [39, 23] </ref>. CMMD's local-independent mode, like mode 0 in CFS, gives each process its own view of the file, and allows each process to make arbitrary requests to the file. In global-independent mode each process has a private file pointer, but all other state is shared.
Reference: [40] <institution> NCSA Consulting Staff and NCSA CM-5 Systems Staff, </institution> <type> "Personal communication", </type> <month> June </month> <year> 1994. </year>
Reference-contexts: While it is possible to write data in this mode, it is most likely to be used to read header information or a shared configuration file. At NCSA, CMF users outnumber CMMD users by a factor of about 7 to 3 <ref> [40] </ref>. 7 3 Methods Given the diversity of multiprocessor file systems, it is not possible to construct an architecture-independent workload study. Thus, it is important to study a variety of platforms. By comparing and contrasting results from production workloads on multiple platforms, we may derive several benefits.
Reference: [41] <author> Russell Carter, Bob Ciotti, Sam Fineberg, and Bill Nitzberg, </author> <title> "NHT-1 I/O benchmarks", </title> <type> Tech. Rep. RND-92-016, </type> <institution> NAS Systems Division, NASA Ames, </institution> <month> Nov. </month> <year> 1992. </year>
Reference-contexts: Simple benchmarking of the instrumented library revealed that the overhead added by our 9 instrumentation was virtually undetectable in most cases. The worst case we found was a 7% increase in execution time on one run of the NAS NHT-1 Application-I/O Benchmark <ref> [41] </ref>. After the instrumented library was put into production use, anecdotal evidence suggests that there was no noticeable performance loss. Although we collected about 700 MB of data, our trace files accounted for less than 1% of the total CFS traffic.
Reference: [42] <author> James C. </author> <title> French, "A global time reference for hypercube multiprocessors", </title> <booktitle> in Fourth Conference on Hypercube Concurrent Computers and Applications, </booktitle> <year> 1989, </year> <pages> pp. 217-220. </pages>
Reference-contexts: Ordering the records was complicated by the lack of synchronized clocks on the iPSC/860. Each node maintains its own clock; the clocks are synchronized at system startup but each drifts significantly after that <ref> [42] </ref>. We partially compensated for the asynchrony by timestamping each block of records when it left the compute node and again when it was received at the data collector.
Reference: [43] <author> Thomas T. Kwan and Daniel A. Reed, </author> <title> "Performance of the CM-5 scalable file system", </title> <booktitle> in Proceedings of the 8th ACM International Conference on Supercomputing, </booktitle> <month> July </month> <year> 1994, </year> <pages> pp. 156-165. </pages>
Reference-contexts: As with the iPSC, small requests on the CM-5 are known to perform poorly. Although accesses on the CM-5 were larger than those observed on the iPSC, they were 18 still significantly smaller than the tens or hundreds of kilobytes used in typical performance analyses of these systems <ref> [36, 43] </ref>. Studies have shown that large I/O requests are common in scientific applications running on supercomputers, but we have now seen that small requests are common in scientific applications running on parallel computers.
Reference: [44] <author> Nils Nieuwejaar and David Kotz, </author> <title> "Low-level interfaces for high-level parallel I/O", </title> <booktitle> in IPPS '95 Workshop on I/O in Parallel and Distributed Systems, </booktitle> <month> Apr. </month> <year> 1995, </year> <pages> pp. 47-62. </pages>
Reference-contexts: regularity of interval sizes, many applications clearly used regular, structured access patterns, possibly because much of the data was in matrix form. 4.6 Strided Access To better understand the structure and causes of the regular but non-consecutive access patterns, we examined the trace files for evidence of strided access patterns <ref> [44] </ref>. 4.6.1 Simple-Strided We refer to a series of I/O requests as a simple-strided access pattern if each request is for the same number of bytes, and if the file pointer is incremented by the same amount between each request. <p> The current interface forces the programmer to break down large parallel I/O activities into small, non-consecutive 29 requests. We believe that a control-parallel model should support strided I/O requests from the programmer's interface to the compute node, and from the compute node to the I/O node <ref> [24, 44] </ref>. A strided request can effectively increase the request size, which lowers overhead and introduces opportunities for low-level optimization [45].
Reference: [45] <author> David Kotz, </author> <title> "Disk-directed I/O for MIMD multiprocessors", </title> <booktitle> in Proceedings of the 1994 Symposium on Operating Systems Design and Implementation, </booktitle> <month> Nov. </month> <year> 1994, </year> <pages> pp. 61-74. 34 </pages>
Reference-contexts: We believe that a control-parallel model should support strided I/O requests from the programmer's interface to the compute node, and from the compute node to the I/O node [24, 44]. A strided request can effectively increase the request size, which lowers overhead and introduces opportunities for low-level optimization <ref> [45] </ref>.
References-found: 42

