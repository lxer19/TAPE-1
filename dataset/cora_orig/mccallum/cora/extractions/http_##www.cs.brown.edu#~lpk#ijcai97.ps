URL: http://www.cs.brown.edu/~lpk/ijcai97.ps
Refering-URL: 
Root-URL: 
Email: fhs,lpkg @cs.brown.edu  
Title: Learning Topological Maps with Weak Local Odometric Information  
Author: Hagit Shatkay Leslie Pack Kaelbling 
Date: 1910  
Address: Box  Providence, RI 02912-1910  
Affiliation: Computer Science Department,  Brown University  
Abstract: Topological maps provide a useful abstraction for robotic navigation and planning. Although stochastic maps can theoretically be learned using the Baum-Welch algorithm, without strong prior constraint on the structure of the model it is slow to converge, requires a great deal of data, and is often stuck in local minima. In this paper, we consider a special case of hidden Markov models for robot-navigation environments, in which states are associated with points in a metric configuration space. We assume that the robot has some odometric ability to measure relative transformations between its configurations. Such odometry is typically not precise enough to suffice for building a global map, but it does give valuable local information about relations between adjacent states. We present an extension of the Baum-Welch algorithm that takes advantage of this local odo-metric information, yielding faster convergence to better solutions with less data.
Abstract-found: 1
Intro-found: 1
Reference: [ Basye et al., 1995 ] <author> K. Basye, T. Dean and L. P. Kaelbling. </author> <title> Learning dynamics: System identification for perceptually challenged agents. </title> <journal> Artificial Intelligence, </journal> <volume> 72(1), </volume> <year> 1995. </year>
Reference: [ Cassandra et al., 1996 ] <author> A. R. Cassandra, L. P. Kaelbling and J. A. Kurien. </author> <title> Acting under uncertainty: Discrete Bayesian models for mobile-robot navigation. </title> <booktitle> In Proceedings of IEEE/RSJ International Conference on Intelligent Robots and Systems, </booktitle> <year> 1996. </year>
Reference: [ Cheeseman et al., 1990 ] <author> P. Cheeseman et al. </author> <title> Autoclass: A Bayesian classification system. </title> <editor> In J. W. Shavlik and T. G. Dietterich, editors, </editor> <booktitle> Readings in Machine Learning, </booktitle> <pages> pages 296-306. </pages> <publisher> Morgan-Kaufmann, </publisher> <year> 1990. </year>
Reference-contexts: We then compute fl and ~ values, with the fl values being 0 or 1, since we use a deterministic clustering algorithm (it might be beneficial to use a stochastic clustering algorithm, such as Autoclass <ref> [ Cheeseman et al., 1990 ] </ref> ). The A, B, and R matrices are all estimated from fl and ~ as described in the previous section.
Reference: [ Dempster et al., 1977 ] <author> A. P. Dempster, N. M. Laird and D. B. Rubin. </author> <title> Maximum likelihood from incomplete data via the EM algorithm. </title> <journal> Journal of the Royal Statistical Society, </journal> <volume> 39(1) </volume> <pages> 1-38, </pages> <year> 1977. </year>
Reference-contexts: The Baum-Welch algorithm is an expectation-maximization (EM) algorithm <ref> [ Dempster et al., 1977 ] </ref> ; it alternates between * the E-step of computing the state-occupation probabilities fl at each time in the sequence given E and the current model , and * the M-step of finding a new model that maximizes Pr (Ej; fl).
Reference: [ Engelson and McDermott, 1992 ] <author> S. P. Engelson and D. V. McDermott. </author> <title> Error correction in mobile robot map learning. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <pages> pages 2555-2560, </pages> <address> Nice, France, </address> <month> May </month> <year> 1992. </year>
Reference: [ Juang et al., 1986 ] <author> B. H. Juang, S. E. Levinson and M. M. Sondhi. </author> <title> Maximum likelihood estimation for multivariate mixture observations of Markov chains. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 32(2), </volume> <month> March </month> <year> 1986. </year>
Reference: [ Juang, 1985 ] <author> B. H. Juang. </author> <title> Maximum likelihood estimation for mixture multivariate stochastic observations of Markov chains. </title> <journal> AT&T Technical Journal, </journal> <volume> 64(6), </volume> <month> July-August </month> <year> 1985. </year>
Reference-contexts: An EM algorithm is guaranteed to provide monotonically increasing convergence of Pr (Ej). Baum-Welch has been proven to be an EM algorithm; it has also been provably extended to real-valued observations <ref> [ Liporace, 1982; Juang, 1985 ] </ref> . Our algorithm introduces an additional matrix, and enforces the first two geometric consistency constraints on the M-step, but like the standard Baum-Welch it is still guaranteed to converge to a local maximum of the likelihood function [ Shatkay and Kael-bling, 1997 ] .
Reference: [ Koenig and Simmons, 1996a ] <author> S. Koenig and R. G. Sim-mons. </author> <title> Passive distance learning for robot navigation. </title> <booktitle> In Proceedings of the Thirteenth International Conference on Machine Learning, </booktitle> <pages> pages 266-274, </pages> <year> 1996. </year>
Reference: [ Koenig and Simmons, 1996b ] <author> S. Koenig and R. G. Sim-mons. </author> <title> Unsupervised learning of probabilistic models for robot navigation. </title> <booktitle> In Proceedings of the IEEE International Conference on Robotics and Automation, </booktitle> <year> 1996. </year>
Reference: [ Kuipers and Byun, 1991 ] <author> B. Kuipers and Y.-T. Byun. </author> <title> A robot exploration and mapping strategy based on a semantic hierarchy of spatial representations. </title> <journal> Journal of Robotics and Autonomous Systems, </journal> <volume> 8 </volume> <pages> 47-63, </pages> <year> 1991. </year>
Reference: [ Kullback and Leibler, 1951 ] <author> S. Kullback and R. A. Leibler. </author> <title> On information and sufficiency. </title> <journal> Annals of Mathematical Statistics, </journal> <volume> 22(1) </volume> <pages> 79-86, </pages> <year> 1951. </year>
Reference: [ Liporace, 1982 ] <author> L. A. Liporace. </author> <title> Maximum likelihood estimation for multivariate observations of Markov sources. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 28(5), </volume> <year> 1982. </year>
Reference-contexts: An EM algorithm is guaranteed to provide monotonically increasing convergence of Pr (Ej). Baum-Welch has been proven to be an EM algorithm; it has also been provably extended to real-valued observations <ref> [ Liporace, 1982; Juang, 1985 ] </ref> . Our algorithm introduces an additional matrix, and enforces the first two geometric consistency constraints on the M-step, but like the standard Baum-Welch it is still guaranteed to converge to a local maximum of the likelihood function [ Shatkay and Kael-bling, 1997 ] .
Reference: [ Nourbakhsh et al., 1995 ] <author> I. Nourbakhsh, R. Powers and S. Birchfield. Dervish: </author> <title> An office-navigating robot. </title> <journal> AI Magazine, </journal> <volume> 16(1) </volume> <pages> 53-60, </pages> <year> 1995. </year>
Reference: [ Pierce and Kuipers, 1997 ] <author> D. Pierce and B. Kuipers. </author> <title> Map learning with uninterpreted sensors and effectors. </title> <journal> Artificial Intelligence, </journal> <note> 1997. (To appear). </note>
Reference: [ Rabiner, 1989 ] <author> L. R. Rabiner. </author> <title> A tutorial on hidden Markov models and selected applications in speech recognition. </title> <booktitle> Proceedings of the IEEE, </booktitle> <volume> 77(2) </volume> <pages> 257-285, </pages> <month> February </month> <year> 1989. </year>
Reference-contexts: An ultimate goal is for an agent to be able to learn such models automatically, both for robustness and in 1 Actions are modeled by pomdps but not by hmms. order to cope with new and changing environments. The Baum-Welch algorithm <ref> [ Rabiner, 1989 ] </ref> is frequently used to learn hmms. Since pomdps are a simple extension of hmms, they can, theoretically, be learned with a simple extension to the Baum-Welch algorithm. <p> These are essentially the same formulae appearing in <ref> [ Rabiner, 1989 ] </ref> , but taking into account the density of the relational observation. 4.2 Updating Model Parameters In this phase of the algorithm, the goal is to find a new model, , that maximizes P (Ej; fl).
Reference: [ Shatkay and Kaelbling, 1997 ] <author> H. Shatkay and L. P. Kael-bling. </author> <title> Learning hidden Markov models with geometric information. </title> <type> Technical Report CS-97-04, </type> <institution> Department of Computer Science, Brown University, </institution> <month> April </month> <year> 1997. </year>
Reference: [ Simmons and Koenig, 1995 ] <author> R. G. Simmons and S. Koenig. </author> <title> Probabilistic navigation in partially observable environments. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence, </booktitle> <year> 1995. </year>
Reference: [ Thrun and Bucken, 1996a ] <author> S. Thrun and A. Bucken. </author> <title> Integrating grid-based and topological maps for mobile robot navigation. </title> <booktitle> In Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <pages> pages 944-950, </pages> <year> 1996. </year>
Reference: [ Thrun and Bucken, 1996b ] <author> S. Thrun and A. Bucken. </author> <title> Learning maps for indoor mobile robot navigation. </title> <type> Technical Report CMU-CS-96-121, </type> <institution> School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA, </address> <month> April </month> <year> 1996. </year>
References-found: 19

