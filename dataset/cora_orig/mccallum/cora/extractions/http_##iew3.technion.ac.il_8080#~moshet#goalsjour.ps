URL: http://iew3.technion.ac.il:8080/~moshet/goalsjour.ps
Refering-URL: http://iew3.technion.ac.il:8080/~moshet/
Root-URL: http://www.aic.nrl.navy.mil/~aha/people.html
Title: Goal Evaluation: Problems and Solutions  
Author: Moshe Tennenholtz 
Address: Haifa 32000 Israel  
Affiliation: Industrial Engineering and Management Technion Israel Institute of Technology  
Abstract: We discuss the problem of estimating the exact agent's preferences on outcomes. This problem has been almost neglected in the recent years, although its solution is a crucial step in the design of many AI systems. We show that a straightforward use of the classical methods for determining the exact preferences on outcomes is not satisfactory in a multi-agent setting. We define a basic setting where the problem of goal evaluation can be addressed, discuss possible solutions, and show how the existence of different perspectives affects goal evaluation in a non-trivial manner.
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> James Allen and James Hendler, editors. </editor> <booktitle> Readings in Planning. </booktitle> <publisher> Morgan Kaufmann Publishers, </publisher> <year> 1990. </year>
Reference-contexts: Devise an accurate evaluation of her goals. 2. Devise appropriate plans to satisfy her goals. These two tasks are both important, although much more effort has been devoted in the recent years to the latter task. The field of planning <ref> [1] </ref>, as well as much of the work in decision theory and decision support systems (e.g., [20],[24]) are excellent examples in this regard. Work in the area of Distributed Artificial Intelligence (DAI) [4] is concerned with a group of goal-seeking agents that try to devise plans for achieving their goals. <p> tuple (L i ; l i 0 ; U i ; A i ; T i ), L i is a set of possible local states of agent i, l i 0 2 L i is an initial state of agent i, U i : L i fi O ! <ref> [0; 1] </ref> is the utility function of agent i, 4 , A i is a set of possible actions for agent i, and T i is a state transition function T i : L i fiA 1 fiA 2 ! L i , that determines how the local state of agent
Reference: [2] <author> K.J. </author> <title> Arrow. Social Choice and Individual Values (2nd Ed.). </title> <publisher> Yale University Press, </publisher> <year> 1963. </year>
Reference: [3] <author> A.G. Barto, S.J. Bradtke, and S.P. Singh. </author> <title> Real-time learning and control using asynchronous dynamic programming. </title> <type> Technical Report 91-57, </type> <institution> Computer and Information Science, University of Massachusetts at Amherst, </institution> <year> 1991. </year>
Reference-contexts: It is well known that there are tight connections between these areas and fundamental aspects of AI, such as reasoning about uncertainty [25], non-monotonic reasoning [6], learning <ref> [3] </ref>, and multi-agent systems [9]. In their work, Von-Neumann and Morgenstern assume an agent has a set of possible outcomes to consider, and a set of preferences on these outcomes. The agent's preferences are assumed to obey some natural requirements or postulates.
Reference: [4] <editor> A. H. Bond and L. Gasser. </editor> <booktitle> Readings in Distributed Artificial Intelligence. </booktitle> <publisher> Ablex Publishing Corporation, </publisher> <year> 1988. </year>
Reference-contexts: The field of planning [1], as well as much of the work in decision theory and decision support systems (e.g., [20],[24]) are excellent examples in this regard. Work in the area of Distributed Artificial Intelligence (DAI) <ref> [4] </ref> is concerned with a group of goal-seeking agents that try to devise plans for achieving their goals. Hence, DAI seems to be tightly related to the field of Game Theory [18].
Reference: [5] <author> R. Davis and R. G. Smith. </author> <title> Negotiation as a metaphor for distributed problem solving. </title> <journal> Artificial Intelligence, </journal> <volume> 20(1) </volume> <pages> 63-109, </pages> <year> 1983. </year> <month> 16 </month>
Reference-contexts: This defines a general procedure and framework for goal evaluation. Notice that GES systems can be associated with negotiation among agents in the stage of goal evaluation, rather than in the stage of problem solving <ref> [5] </ref>. The question is whether GES systems enable to ascribe goals to the agents. We will make use of the following definition.
Reference: [6] <author> J. Doyle and M.P. Wellman. </author> <title> Impediments to Universal Preference-Based Default Theories. </title> <booktitle> In Proceedings of the 1st conference on principles of knowledge representation and reasoning, </booktitle> <year> 1989. </year>
Reference-contexts: The work of Von-Neumann and Morgenstern is the basis of utility theory, which by itself is the basis of decision theory and mathematical economics. It is well known that there are tight connections between these areas and fundamental aspects of AI, such as reasoning about uncertainty [25], non-monotonic reasoning <ref> [6] </ref>, learning [3], and multi-agent systems [9]. In their work, Von-Neumann and Morgenstern assume an agent has a set of possible outcomes to consider, and a set of preferences on these outcomes. The agent's preferences are assumed to obey some natural requirements or postulates.
Reference: [7] <author> C. Dwork and Y. Moses. </author> <title> Knowledge and Common Knowledge in a Byzantine Environment: Crash Failures. </title> <journal> Information and Computation, </journal> <volume> 88(2) </volume> <pages> 156-186, </pages> <year> 1990. </year>
Reference-contexts: We will be able to show that, in some natural cases, it is possible to reach stable goals that are common-knowledge. We will discuss these cases in Section 4. Nevertheless, 1 It is well-known that common knowledge is needed for successful joint actions (e.g., <ref> [7] </ref>), and that establishing goals is an essential first step in agent activities [17]. 3 as we will show in Section 5, there are cases where although we converge to stable goals that are common-knowledge among the agents, these goals do not satisfy trivial requirements. <p> Formally, a protocol P i , for agent i, is a function P i : L i ! A i . In general, the protocol P i may be arbitrary. Nevertheless, since we are interested in goal evaluation, we will assume that each agent obeys the "full history protocol" <ref> [7] </ref> which sends "all relevant information" an agent has (and in particular its preferences) to the other agent. Naturally, this is the best that can be done for accurate goal evaluation. 5 We call a pair of local states, with a unique state for each agent, a system configuration.
Reference: [8] <author> J. Fox. </author> <title> Decision Theory and Autonamous Systems. </title> <editor> In M.G. Singh and L. Trave-Massuyes, editors, </editor> <title> Decision Support Systems and Qualitative Reasoning. </title> <publisher> North-Holland, </publisher> <year> 1991. </year>
Reference-contexts: We showed illuminating results on how the existence 15 of several perspectives might affect the evaluation of goals in multi-agent settings. Last but not least, work on decision theory and autonomous systems <ref> [8] </ref> considered some weaknesses of current decision support technologies. Our work is, in a sense, complementary to that work. We discuss several basic and non-trivial problems in the essential step of goal evaluation, when decisions have to be taken by several decision-makers.
Reference: [9] <author> P.J. Gmytrasiewicz, E.H. Durfee, and D.K. Wehe. </author> <title> A decison-theoretic approach to coordinating multi-agent interactions. </title> <booktitle> In Proc. 12th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 62-68, </pages> <year> 1991. </year>
Reference-contexts: It is well known that there are tight connections between these areas and fundamental aspects of AI, such as reasoning about uncertainty [25], non-monotonic reasoning [6], learning [3], and multi-agent systems <ref> [9] </ref>. In their work, Von-Neumann and Morgenstern assume an agent has a set of possible outcomes to consider, and a set of preferences on these outcomes. The agent's preferences are assumed to obey some natural requirements or postulates.
Reference: [10] <author> J. Halpern and Y. Moses. </author> <title> Knowledge and common knowledge in a distributed environment. </title> <type> Technical Report RJ 4421, </type> <institution> IBM, </institution> <year> 1984. </year>
Reference-contexts: We assume that (as in most work on the design of multi-agent systems) the agents are (basically) cooperative. Hence, in order to coordinate their activities in a successful manner the agents need to have common-knowledge <ref> [10] </ref> of their respective goals. 1 However, since agents need to announce their goals in order to achieve common-knowledge about them, we reach a situation where the goals the agents started with are no longer valid. We will return to this point in Section 2.
Reference: [11] <author> J. Y. Halpern. </author> <title> Reasoning about knowledge: An overview. </title> <booktitle> In Theoretical Aspects of Reasoning About Knowledge: Proceedings of the 1988 conference, </booktitle> <pages> pages 1-17, </pages> <year> 1988. </year>
Reference-contexts: However, as this section and the following ones will demonstrate, fairly natural solutions do exist, for many natural domains. In order to address the problem in a satisfactory manner, we have to be more precise about the notion of an agent. Following work in Distributed Systems <ref> [11] </ref> we take an agent to have a state and to execute actions. The actions an agent performs may change her state as well as the other agents' states. At each state each agent has certain preferences, and can be ascribed a utility function at that state.
Reference: [12] <author> S. Kraus and J. Wilkenfeld. </author> <title> The Function of Time in Cooperative Negotiations. </title> <booktitle> In Proc. of AAAI-91, </booktitle> <pages> pages 179-184, </pages> <year> 1991. </year>
Reference-contexts: In the next stage, we need to decide on appropriate plans, and computational mechanisms such as negotiations <ref> [12] </ref>, deals [19], or social laws ([15],[23]), may be helpful in this regard. At first glance, this solution may seem quite satisfactory, and indeed research on multi-agent systems have implicitly assumed that the goal evaluation problem is solved, and concentrated on computational mechanisms for group decision making and problem solving.
Reference: [13] <author> R. D. Luce and H. Raiffa. </author> <title> Games and Decisions- Introduction and Critical Survey. </title> <publisher> John Wiley and Sons, </publisher> <year> 1957. </year>
Reference-contexts: 1 Introduction The notion of a goal-seeking agent is fundamental to AI, as well as to other areas, such as decision theory and mathematical economics <ref> [13] </ref>. The goal of an agent is, in general, a preference relation on the possible outcomes of her actions. Given an accurate evaluation of her goals, the agent will try to achieve her goals in an efficient manner.
Reference: [14] <author> J. McCarthy. </author> <title> Notes on Formalizing Context. </title> <booktitle> In Proc. 13th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 555-560, </pages> <year> 1993. </year>
Reference-contexts: Problems related to the existence of varieties of contexts (or multiple perspectives) have been recognized to be of considerable importance to the AI community ([22], <ref> [14] </ref>). We showed illuminating results on how the existence 15 of several perspectives might affect the evaluation of goals in multi-agent settings. Last but not least, work on decision theory and autonomous systems [8] considered some weaknesses of current decision support technologies.
Reference: [15] <author> Y. Moses and M. Tennenholtz. </author> <title> On Formal Aspects of Artificial Social Systems. </title> <type> Technical Report CS91-01, </type> <institution> Weizmann Institute, </institution> <year> 1991. </year>
Reference: [16] <author> J. Von Neumann and O. Morgenstern. </author> <title> Theory of Games and Economic Behavior. </title> <publisher> Princeton University Press, Princeton, </publisher> <year> 1944. </year>
Reference-contexts: For example, an agent may prefer going to a basketball game than going to a football game, and the utility function is a formal entity that captures these preferences. Moreover, the fundamental work of Von-Neumann and Morgenstern <ref> [16] </ref> shows that if an agent's preferences obey some natural postulates, then the agent can be ascribed a unique utility function which assigns to each potential outcome a number between 0 and 1. Given the above discussion, one might assume that the problem of goal evaluation is solved. <p> We can show: Theorem 5.4: Standardized ffi-GES systems obey MIA. Proof: The proof stems from the fact we have fixed best and worst outcomes, and hence by adding new alternatives we do not change the initial utilities of previously existing alternatives. As Von-Neumann and Morgen-stern <ref> [16] </ref> show, the evaluation of the utility of each outcome o is based on calculating a probability p for which the agent will be indifferent between achieving o, to achieving its best outcome with probability p and its worst outcome with probability 1 p.
Reference: [17] <author> D.A. Norman. </author> <title> Cognitive Engineering. In D.A. </title> <editor> Norman and S.W. Draper, editors, </editor> <title> User Centered System Design. </title> <publisher> Lawrence Erlbaum As-soc., </publisher> <year> 1986. </year> <month> 17 </month>
Reference-contexts: We will discuss these cases in Section 4. Nevertheless, 1 It is well-known that common knowledge is needed for successful joint actions (e.g., [7]), and that establishing goals is an essential first step in agent activities <ref> [17] </ref>. 3 as we will show in Section 5, there are cases where although we converge to stable goals that are common-knowledge among the agents, these goals do not satisfy trivial requirements. <p> The identity of the goal to be obtained is crucial, and in many cases it has to be carefully computed. This is an essential stage in decision making <ref> [17] </ref>. This decision will be usually based upon a large amount of data and computation.
Reference: [18] <author> G. Owen. </author> <title> Game Theory (2nd Ed.). </title> <publisher> Academic Press, </publisher> <year> 1982. </year>
Reference-contexts: Work in the area of Distributed Artificial Intelligence (DAI) [4] is concerned with a group of goal-seeking agents that try to devise plans for achieving their goals. Hence, DAI seems to be tightly related to the field of Game Theory <ref> [18] </ref>. As a result, it is only natural that work in DAI adopts game-theoretic representations of the agents' goals, and concentrates on computational mechanisms for group decision making and multi-agent activity. The general form of a goal in a multi-agent setting is given by a utility function. <p> Another example is continuity: if o 1 is preferable to o 2 2 The interested reader may consult <ref> [18] </ref> for details. 4 which is preferable to o 3 , then there exists a probability p such that the agent is indifferent between reaching o 2 for sure, and reaching o 1 or o 3 with probabilities p and 1 p respectively. <p> The Von-Neumann and Morgenstern theory, and the theories that followed it, show how to evaluate the goals of a single agent. Whenever we have an interaction between several agents (i.e. a game <ref> [18] </ref>), then each joint action of the agents is associated with an outcome. The utility of an outcome from the point of view of an agent is assumed to be prescribed by the Von-Neumann Morgenstern theory (and in the case of uncertainty, by the Savage theory [21]).
Reference: [19] <author> J. S. Rosenschein and M. R. Genesereth. </author> <title> Deals Among Rational Agents. </title> <booktitle> In Proc. 9th International Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 91-99, </pages> <year> 1985. </year>
Reference-contexts: In the next stage, we need to decide on appropriate plans, and computational mechanisms such as negotiations [12], deals <ref> [19] </ref>, or social laws ([15],[23]), may be helpful in this regard. At first glance, this solution may seem quite satisfactory, and indeed research on multi-agent systems have implicitly assumed that the goal evaluation problem is solved, and concentrated on computational mechanisms for group decision making and problem solving.
Reference: [20] <author> A. P. Sage. </author> <title> Decision Support Systems Engineering. </title> <publisher> John Wiley and Sons, </publisher> <year> 1991. </year>
Reference: [21] <author> L.J. Savage. </author> <title> The Foundations of Statistics. </title> <publisher> Dover Publications, </publisher> <address> New York, </address> <year> 1972. </year>
Reference-contexts: The utility of an outcome from the point of view of an agent is assumed to be prescribed by the Von-Neumann Morgenstern theory (and in the case of uncertainty, by the Savage theory <ref> [21] </ref>). Here comes the main problem with the use of the above-mentioned fundamental work, if we wish to apply it to actual decision making or planning in multi-agent domains. <p> In addition to the classical Von-Neumann Morgenstern theory mentioned in the body of this paper, there are several basic works that are relevant to our work. In his work on the foundations of statistics, Savage <ref> [21] </ref> shows how one can ascribe subjective probability and utilities to an agent functioning in uncertain territory.
Reference: [22] <author> Y. Shoham. </author> <title> Varieties of Context. </title> <booktitle> In Artificial Intelligence and Mathematical Theories of Computation. </booktitle> <publisher> Academic Press, </publisher> <address> San Diego and Lon-don, </address> <year> 1991. </year>
Reference: [23] <author> Y. Shoham and M. Tennenholtz. </author> <title> On the Synthesis of Useful Social Laws for Artificial Agent Societies. </title> <booktitle> In Proc. of AAAI-92, </booktitle> <pages> pages 276-281, </pages> <year> 1992. </year>
Reference: [24] <author> M.G. Singh and L. Trave-Massuyes. </author> <title> Decision Support Systems and Qualitative Reasoning. </title> <publisher> North-Holland, </publisher> <year> 1991. </year>
Reference: [25] <author> Michael P. Wellman. </author> <title> Formulation of Tradeoffs in Planning Under Uncertainty. </title> <publisher> Pitman, </publisher> <address> London, </address> <year> 1990. </year> <month> 18 </month>
Reference-contexts: The work of Von-Neumann and Morgenstern is the basis of utility theory, which by itself is the basis of decision theory and mathematical economics. It is well known that there are tight connections between these areas and fundamental aspects of AI, such as reasoning about uncertainty <ref> [25] </ref>, non-monotonic reasoning [6], learning [3], and multi-agent systems [9]. In their work, Von-Neumann and Morgenstern assume an agent has a set of possible outcomes to consider, and a set of preferences on these outcomes. The agent's preferences are assumed to obey some natural requirements or postulates.
References-found: 25

