URL: http://www.cs.rice.edu/~adve/CS615/now_case.ps
Refering-URL: http://www.cs.rice.edu/~adve/comp615.html
Root-URL: 
Title: A Case for NOW (Networks of Workstations)  and the NOW team  
Author: Thomas E. Anderson, David E. Culler, David A. Patterson, 
Keyword: sage Passing, File Systems, Network Virtual Memory, Global Resource Manage ment, Availability  
Date: November 16, 1994  
Abstract: In this paper, we argue that because of recent technology advances, networks of workstations (NOWs) are poised to become the primary computing infrastructure for science and engineering, from low end interactive computing to demanding sequential and parallel applications. We identify three opportunities for NOWs that will benefit end-users: dramatically improving virtual memory and file system performance by using the aggregate DRAM of a NOW as a giant cache for disk; achieving cheap, highly available, and scalable file storage by using redundant arrays of workstation disks, using the LAN as the I/O backplane; and finally, multiple CPUs for parallel computing. We describe the technical challenges in exploiting these opportunities namely, efficient communication hardware and software, global coordination of multiple workstation operating systems, and enterprise-scale network file systems. We are currently building a 100-node NOW prototype to demonstrate that practical solutions exist to these technical challenges. Keywords: Networks of Workstations, Communications, Parallel Computing, Mes 
Abstract-found: 1
Intro-found: 1
Reference: [Arp*94] <author> R. Arapaci, A. Dusseau, A. Vahdat, T. Anderson, and D. Patterson, </author> <title> The Interaction of Parallel and Sequential Workloads on a Network of Workstations, </title> <note> submitted for publication. </note>
Reference-contexts: The key question is whether a NOW can run large programs with the performance as a dedicated large computer and run small programs with the interactivity of a dedicated workstation. To investigate this combination we simulated the impact sequential workstation jobs and MPPs jobs may have on one another <ref> [Arp*94] </ref>. We collected traces from a local cluster of 53 DECstation 5000/133s with 64 MB of memory used by electrical engineering graduate students. Two user-level daemons logged information every two seconds on CPU, memory, disk, keyboard and mouse activity. <p> At the same time, supercomputer users also look suspiciously at NOW, fearing that interactive users will have priority and demanding applications will only be allowed to run at night. Like anyone else, super computer users work during the daytime, and therefore need good response time even during the daytime <ref> [Arp*94] </ref>. GLUnix needs to address both of these concerns, along with being tolerant of individual node failures. We discuss each of these issues in turn.
Reference: [Broo92] <author> E. D. Brooks III, </author> <title> Massive Parallelism Overcomes Shared-Memory Limitations, </title> <booktitle> Computers in Physics, Mar 1992, </booktitle> <volume> no. 2, </volume> <pages> pp. 139-45. </pages>
Reference: [Cul*93] <author> D. Culler, R. Karp, D. Patterson, A. Sahay, K. Schauser, E. Santos, R. Sub-ramonian, T. von Eicken, </author> <title> LogP: Towards a Realistic Model of Parallel Computation, </title> <booktitle> Proc. 4th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming, </booktitle>
Reference-contexts: In discussing communication performance it is impor tant to distinguish the time spent in the actual network hardware, which we call latency from that spent in the processor in preparing to send or receive a message, which we call overhead <ref> [Cul*93] </ref>. Network latency can potentially be overlapped with computation, while overhead is processor cycles that cannot be used for computation. MPP communication performance derives from several factors. The routing components are fast, single-chip switches employing cut-through routing with short wire delays and wide links.
Reference: [Dah*94] <author> M. Dahlin, R. Wang, T. Anderson, and D. Patterson, </author> <title> Cooperative Caching: Using Remote Client Memory to Improve File System Performance, </title> <booktitle> Proc. of the First Conference on Operating Systems Design and Implementation, </booktitle> <month> Nov., </month> <year> 1994. </year>
Reference-contexts: Assuming each client workstation has 16MB of file cache and the server cache is 128MB, cooperative caching reduced disk reads by a factor of 2, improving file read performance by 80% <ref> [Dah*94] </ref>. 3.2 Redundant Arrays of Workstation Disks RAID (Redundant Arrays of Inexpensive Disks) systems deliver higher bandwidth, capacity, and availability than can be achieved by a single large disk by hooking Cache Miss Rate Read Response Time Client-server 16% 2.8 ms Cooperative caching 8% 1.6 ms TABLE 3.
Reference: [Jac*94] <author> M. Jacobson, R. Turco, R. Lu and O. Toon. GATOR: </author> <title> A gas, aerosol, transport, and radiation modeling system for studying urban and regional air pollution, </title> <note> submitted for publication. Conclusion </note>
References-found: 5

