URL: http://www.mli.gmu.edu/~bloedorn/papers/mli98-8.ps
Refering-URL: http://www.mli.gmu.edu/kpubs.html
Root-URL: 
Email: bloedorn@mitre.org  michalski@gmu.edu  
Title: DATA-DRIVEN CONSTRUCTIVE INDUCTION: A Methodology and Its Applications  
Author: Eric Bloedorn Ryszard S. Michalski and 
Keyword: machine learning, inductive learning, constructive induction, attribute generation, attribute selection, attribute abstraction, discretization.  
Address: 1820 Dolley Madison Blvd., W640 4400 University Drive McLean, VA 22102 Fairfax, VA 22030 USA USA  Fairfax, VA 22030 Warsaw, ul. Ordona 21 USA Poland  
Affiliation: The MITRE Corporation and George Mason University  George Mason University Institute of Computer Science 4400 University Drive Polish Academy of Sciences  
Abstract: The presented methodology concerns constructive induction, viewed generally as a process combining two intertwined searches: first for the best representation space, and second for the best hypothesis in that space. The first search employs a range of operators for improving the initial representation space, such as operators for generating new attributes, selecting best attributes among the given ones, and for abstracting attributes. In the methodology presented, these operators are chosen on the basis of the analysis of training data, hence the term data-driven. The second search employs an AQ-type rule learning to the examples projected at each iteration to the newly modified representation space. The aim of the search is to determine a generalized description of examples that optimizes a task-oriented multicriterion evaluation function. The two searches are intertwined, as they are executed in a loop in which one feeds into another. Experimental applications of the methodology to text categorization and natural scene interpretation demonstrate a significant practical utility of the proposed methodology. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Michalski, </author> <title> R.S., "Pattern Recognition as Knowledge-Guided Computer Induction," </title> <type> Technical Report No. 927, </type> <institution> Department of Computer Science, University of Illinois, Urbana-Champaign, IL, </institution> <year> 1978. </year>
Reference-contexts: To cope with inadequacy of attributes provided in the original data, the idea of constructive induction has been proposed <ref> [1] </ref>. The original formulation of the idea was concerned primarily with generating additional, more task relevant attributes from the originally given, in order to improve the learning process.
Reference: [2] <author> Thrun. S.B., Bala, J., Bloedorn, E., Bratko, I., Cestnik, B., Cheng, J., De Jong, K.A., Dzeroski, S., Fahlman, S.E., Hamann, R., Kaufman, K., Keller, S., Kononenko, I., Kreuziger, J., Michalski, R.S., Mitchell, T., Pachowicz, P., Vafaie, H., Van de Velde, W., Wenzel, W., Wnek, J., and Zhang, J., </author> <title> The MONKs Problems: A Performance Comparison of Different Learning Algorithms, </title> <note> (revised version), </note> <institution> Carnegie Mellon University, </institution> <address> Pittsburgh, PA, CMU-CS-91-197, </address> <year> 1991. </year>
Reference-contexts: In some inductive learning problems, the boundaries of concepts in the example representation space are very complex and thus difficult to learn. Such a problem is illustrated in Figure 1a. This is the socalled second Monks problem, which was used in the international competition of machine learning program <ref> [2] </ref>. 1 The paper makes a distinction between an attribute (a one-argument function that maps objects to attribute values; e.g., color or length of an object) and a feature (that expresses a specific value or property of an object (e.g., red or long).
Reference: [3] <author> Wnek, J., and Michalski, </author> <title> R.S., "Hypothesis-driven Constructive Induction in AQ17-HCI: A Method and Experiments," </title> <journal> Machine Learning, </journal> <volume> 14, </volume> <editor> p. </editor> <volume> 139-168, Vol. </volume> <editor> p. </editor> <year> 1994. </year>
Reference-contexts: Therefore, a general view of constructive induction has been proposed, in which it is a process of learning concepts desciptions that employs two intertwined searches: one for the best representation space, and the second for the best hypothesis in the space <ref> [3] </ref> When searching for the best representation space, the system may make no commitment as to the description language used for creating a hypothesis, or may be dependent on the description language. <p> In data-driven constructive induction (DCI), the search is based on the analysis of the input examples (data); in hypothesis-driven constructive induction (HCI) the search is based on the analysis of intermediate hypotheses); and in knowledge-driven constructive induction (KCI) the search exploits domain knowledge provided by the expert <ref> [3] </ref>. There is also a multistrategy constructive induction (MCI), that employs two or more previous methods [8].
Reference: [4] <author> Dougherty, J., Kohavi, R., and Sahami, M., </author> <title> "Supervised and Unsupervised Discretization of Continuous Features", </title> <booktitle> Proceedings of the Twelfth International Conference on Machine Learning, p. </booktitle> <pages> 194-202. </pages> <address> San Francisco, </address> <year> 1995. </year>
Reference-contexts: It also may cause overfitting the data. An attribute overprecision frequently occurs when attributes are continuous. To avoid a potential problem, such attributes can be discretized, that is, their domain is split to ranges of values <ref> [4] </ref>. Formally, such a discretization is a form of an attribute abstraction operation 2 . Discretization is frequently done without taking into consideration other operators for changing the representation space.
Reference: [5] <author> Bloedorn, E., and Michalski, </author> <title> R.S., "The AQ17-DCI System and it Application to World Economics," </title> <booktitle> Proceedings of the Ninth International Symposium on Methodologies for Intelligent Systems, </booktitle> <address> p. Zakopane, Poland, </address> <year> 1996. </year>
Reference-contexts: An example of a multi-attribute interaction is the even/odd parity classification problem (classifying binary strings on the basis of parity of the number they represent). When there is complex multi-attribute interaction, attribute construction methods can be used that combine attributes in a problem-relevant manner <ref> [5] </ref>, [6]. When the set of training examples contains attributes that are irrelevant for the given task, this can lead to a spurious hypothesis. Conventional selective induction learning methods are usually not affected by a small number of irrelevant attributes.
Reference: [6] <author> Hu, Y., and Kibler, D., </author> <title> "Generation of Attributes for Learning Algorithms", </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence (AAAI96), </booktitle> <address> p . 806-811, Portland, OR, </address> <year> 1996. </year>
Reference-contexts: An example of a multi-attribute interaction is the even/odd parity classification problem (classifying binary strings on the basis of parity of the number they represent). When there is complex multi-attribute interaction, attribute construction methods can be used that combine attributes in a problem-relevant manner [5], <ref> [6] </ref>. When the set of training examples contains attributes that are irrelevant for the given task, this can lead to a spurious hypothesis. Conventional selective induction learning methods are usually not affected by a small number of irrelevant attributes.
Reference: [7] <author> John, G., Kohavi, R., and Pfleger, K., </author> <title> "Irrelevant Features and the Subset Selection Problem", </title> <booktitle> Proceedings of the Eleventh International Conference on Machine Learning (ML94), </booktitle> <publisher> Morgan Kaufmann, </publisher> <address> p. 121-129. </address> <year> 1994. </year>
Reference-contexts: Conventional selective induction learning methods are usually not affected by a small number of irrelevant attributes. Detecting and removing irrelevant attributes can be done either by a filter approach or a wrapper approach <ref> [7] </ref>. In the filter approach, attribute selection is performed as a preprocessing step to induction. Because it is separated from the induction algorithm, filters are fast, they can be used with any induction algorithms once filtering is done, and can be used on large datasets.
Reference: [8] <author> Bloedorn, E., Wnek J., and Michalski, </author> <title> R.S., Multistrategy Constructive Induction, </title> <booktitle> Proceedings of the Second International Workshop on Machine Learning (MSL93), </booktitle> <address> Harpers Ferry, WV, </address> <publisher> Morgan Kaufmann, </publisher> , <month> May 27-29, </month> <year> 1993. </year>
Reference-contexts: There is also a multistrategy constructive induction (MCI), that employs two or more previous methods <ref> [8] </ref>. The above general view of constructive induction and the understanding of different information sources for its implementation has been in the last several years used to guide the development of such constructive induction programs as AQ17-DCI, AQ17-HCI and AQ17-MCI. <p> Currently implemented in the binary group are the relational operator (determining whether the first input is less than, greater than, or equal to the second one) and a number of mathematical operators including addition, subtraction, absolute difference, multiplication, and integer division (Table 1). 3 The AQ17-MCI <ref> [8] </ref> method extends this model by including additional representation space modification operators and by using meta-rules which relate problem characteristics to appropriate representation space modification operators. In the attribute construction process each possible combination of attributes and the operators selected by the user from Table 1 is generated and evaluated. <p> Another topic for future work is the problem of learning metarules that would automatically guide the application of the representation space modification operators for any given problem. Initial work in this directions has been described in <ref> [8] </ref>. Trial experiments on the application of the AQ17-DCI methodology to two problemstext categorization and natural scene interpretationindicate a great promise of the presented ideas for real-world inductive learning problems. Acknowledgements This research was conducted in the Machine Learning and Inference Laboratory at George Mason University.
Reference: [9] <author> Wnek, J., Kaufman, K., Bloedorn, E., and Michalski, </author> <title> R.S., "Selective Inductive Learning Method AQ15c: The Method and User's Guide", Reports of the Machine Learning and Inference Laboratory, </title> <institution> MLI95-4, George Mason University, Fairfax, VA, </institution> <year> 1995. </year>
Reference-contexts: The final rules are evaluated on the testing examples to determine a more precise estimate of their performance accuracy. The search for hypotheses within a given representation space is performed in AQ17-DCI by the AQ algorithm as implemented in AQ15c <ref> [9] </ref>. AQ15c performs a separate and conquer strategy to determine a set of decision rules which jointly cover all the positive examples and none of the negative examples (in the default case).
Reference: [10] <author> Kerber, R., "ChiMerge: </author> <title> Discretization of Numeric Attributes," </title> <booktitle> Proceedings of the Tenth National Conference on Artificial Intelligence, p. </booktitle> <pages> 123-128, </pages> <address> San Jose, CA, </address> <year> 1992. </year>
Reference-contexts: The end result is a set of decision rules for each class in the data. An example of a rule produced by AQ is shown below. Class1 &lt;= [color = blue] &[height &gt; 5] & [shape = square or triangle] or <ref> [height &gt; 10] </ref> & [shape = square] This rule states "An object is in class1 if it is blue, its height is greater than 5, and shape is square or triangle, or if its height is greater than 5 and its shape is square". <p> The current AQ17-DCI employs the information gain ratio. It selects for future processing the attributes with gain ratio greater than or equal to some predefined threshold. Attribute abstraction in AQ17-DCI uses the ChiMerge algorithm to create ranges of attribute values as described in <ref> [10] </ref>. This is a bottom-up algorithm in which initially all values are stored in separate intervals, and then merged into ranges until a termination condition is met.
Reference: [11] <author> Bloedorn, E., Mani, I., and MacMillan T.R., </author> <title> Machine Learning of User Profiles: Representational Issues, </title> <booktitle> Proceedings of the Thirteenth National Conference on Artificial Intelligence, </booktitle> <address> Portland, OR, </address> <month> August, </month> <year> 1996. </year>
Reference-contexts: Table 2. Attributes used for text description. One of the most difficult aspects of this problem is in finding a good representation for the text. The use of constructive induction method builds on work reported in <ref> [11] </ref>, which found that a hybrid representation for text, consisting of extracted subjects, person, organization and location (POL) attributes and keywords, coupled with a generalization hierarchy performs well for modeling newswire text (Table 2). In this previous work, combinations and subsets of attributes were generated and evaluated by hand. <p> In this domain, articles are represented by the set of attributes including subject categories, POL entities and keywords as shown in Table 2. In the previous work, reported in <ref> [11] </ref>, the experiments were performed on representations consisting of a) only keywords (KEYWORDS), b) only POLs (POL), c) only subjects (SUBJECTS) and d) all attributes (COMPLETE). The COMPLETE and SUBJECTS attribute sets were found to be the highest performing in the experiments previously performed.
Reference: [12] <author> Michalski, </author> <title> R.S., "Inferential Theory of Learning: Developing Foundations for Multistrategy Learning," Machine Learning: A Multistrategy Approach, R.S. </title> <editor> Michalski and G. Tecuci (Eds.), </editor> <address> San Mateo, CA: </address> <publisher> Morgan Kaufmann, </publisher> <year> 1993. </year>
Reference-contexts: The difficulty of describing an interaction depends on the description language used by the learning algorithm. For most symbolic inductive learning algorithms interactions involving logical conjunction or disjunction are easy to describe. However, interactions such as those most simply 2 The Inferential Theory of Learning <ref> [12] </ref> identifies abstraction as any transmutation that reduces the amount of detail used to describe a given reference set. represented as the equality or product of attributes may create significant difficulties for such methods.
References-found: 12

