URL: http://theory.cs.ucdavis.edu/theory_html/papers/graph.ps.gz
Refering-URL: http://theory.cs.ucdavis.edu/
Root-URL: http://www.cs.ucdavis.edu
Email: blackj@cs.ucdavis.edu  martel@cs.ucdavis.edu  
Title: Designing Fast Graph Data Structures: An Experimental Approach  
Author: John R. Black, Jr. Charles U. Martel 
Note: This work was supported by NSF grant CCR 94-03651.  
Date: October 28, 1998  
Address: Davis  Davis  
Affiliation: Department of Computer Science University of California,  Department of Computer Science University of California,  
Abstract: We show that simple alternatives to standard graph representations can substantially improve performance. We obtained these results by conducting an experimental study of the performance of basic data structures for storing lists of values. We then used these results to design and evaluate fast implementations of Breadth-First-Search (BFS) and Depth-First-Search (DFS). Using our improved data structures can improve performance of BFS and DFS by a factor of 20 or more over a standard representation. Our main focus is on understanding and quantifying architectural effects on algorithm performance. The most important aspect is improving data locality since the penalty for cache misses is becoming a significant overhead for modern machines. For the basic data structures we show that array-based lists are much faster than basic linked list implementations for sequential access (sometimes by a factor of 10 or more). We also suggest a simple enhancement to the linked list which improves performance. For lists of boolean values, we show that a bit-vector type approach is faster than either an integer or character array. In all cases we get a fairly precise characterization of the performance as a function of list and cache size. The experiments we use also provide a fairly simple set of tests to explore these basic characteristics on a new computer system, and they can provide a foundation for more complex experiments. We show that the basic data structure performance results translate fairly well to DFS and BFS imple mentation. For dense graphs an adjacency matrix using a bit-vector is the universal winner (often resulting in speedups of a factor of 20 or more), while for sparse graphs an array-based adjacency list is best.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. Aggrawal, K. Chandra and M. Snir. </author> <title> Hierarchical memory with block transfer. </title> <booktitle> In 28th IEEE Symposium on the Foundations of Computer Science, </booktitle> <pages> 204-216, </pages> <year> 1987. </year>
Reference-contexts: There has also been prior work on algorithm design and analysis for memory hierarchies <ref> [1, 2, 3, 4] </ref> but most of this prior work assumes greater control over movement between the memory levels than exists in most current systems. Moret and Shapiro discuss cache effects on graph algorithms in their minimum spanning tree paper [21].
Reference: [2] <author> A. Aggrawal, K. Chandra and M. Snir. </author> <title> A model for hierarchical memory. </title> <booktitle> In 19th ACM Symposium on the Theory of Computing, </booktitle> <pages> 305-314, </pages> <year> 1987. </year>
Reference-contexts: There has also been prior work on algorithm design and analysis for memory hierarchies <ref> [1, 2, 3, 4] </ref> but most of this prior work assumes greater control over movement between the memory levels than exists in most current systems. Moret and Shapiro discuss cache effects on graph algorithms in their minimum spanning tree paper [21].
Reference: [3] <author> A. Aggrawal, and J. Vitter. </author> <title> The input/output complexity of sorting and related problems. </title> <journal> Communications of the ACM, </journal> <volume> 31(9) </volume> <pages> 1116-1127, </pages> <year> 1988. </year>
Reference-contexts: There has also been prior work on algorithm design and analysis for memory hierarchies <ref> [1, 2, 3, 4] </ref> but most of this prior work assumes greater control over movement between the memory levels than exists in most current systems. Moret and Shapiro discuss cache effects on graph algorithms in their minimum spanning tree paper [21].
Reference: [4] <author> J. Alpern, L. Carter, E. Feig, and T. Selker. </author> <title> The uniform memory hierarchy model of computation. </title> <journal> Algorithmica, </journal> <volume> 12(2) </volume> <pages> 72-109, </pages> <year> 1994. </year>
Reference-contexts: There has also been prior work on algorithm design and analysis for memory hierarchies <ref> [1, 2, 3, 4] </ref> but most of this prior work assumes greater control over movement between the memory levels than exists in most current systems. Moret and Shapiro discuss cache effects on graph algorithms in their minimum spanning tree paper [21].
Reference: [5] <author> R. Ahuja, M. Kodialam, A. Mishra and J. Orlin. </author> <title> Computational testing of maximum flow algorithms. Sloan working Paper, </title> <publisher> MIT, </publisher> <year> 1992. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees [21], network flow and matching <ref> [5, 8, 15, 23] </ref>, and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [6] <author> Eli Biham. </author> <title> A fast new DES implementation in software. </title> <institution> Technion, Computer Science Dept. </institution> <note> Technical Report CS0891-1997. </note>
Reference-contexts: Compilers are becoming increasingly effective at automatically extracting this parallelism from programs, but careful coding can result in substantial improvements <ref> [22, 6, 25] </ref>.
Reference: [7] <author> S. Carr, K. Mckinley, and C. Tseng. </author> <title> Compiler optimizations for improving data locality. </title> <booktitle> In Sixth International Conference on Architectural Support for Programming Languages and Operating Systems, </booktitle> <pages> 252-262, </pages> <year> 1994. </year>
Reference-contexts: Note also that discovering which types of effects are not important is also quite helpful for experimental design. 1.1 Related Work Because of its importance, compiler writers have spent considerable effort on generating code with good locality <ref> [7] </ref>, however substantial additional improvements can be gained by proper algorithm design.
Reference: [8] <author> B. Cherkassky, A. Goldberg, P. Martin, J. Setubal, and J. Stolfi. </author> <title> Augment or push? A computational study of bipartite matching and unit capacity flow algorithms. </title> <type> Technical Report 97-127, </type> <institution> NEC Research Institute, Inc., </institution> <month> August </month> <year> 1997. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees [21], network flow and matching <ref> [5, 8, 15, 23] </ref>, and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> This shows the general lack of focus on these issues by experimenters. However, our results were at least partly responsible for a recoding of some matching algorithms in one of these studies to unify the graph representation used <ref> [8] </ref>. Note also that discovering which types of effects are not important is also quite helpful for experimental design. 1.1 Related Work Because of its importance, compiler writers have spent considerable effort on generating code with good locality [7], however substantial additional improvements can be gained by proper algorithm design.
Reference: [9] <author> B. Cherkassky, A. Goldberg, and T. Radzik. </author> <title> Shortest paths algorithms: theory and experimental evaluation. </title> <journal> Mathematical Programming, </journal> <volume> Vol. 73, </volume> <pages> 129-174, </pages> <month> June </month> <year> 1996. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths <ref> [13, 9] </ref>, minimum spanning trees [21], network flow and matching [5, 8, 15, 23], and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> Certainly FORTRAN programmers had little temptation to use expensive linked lists for adjacency lists, and there is literature which discusses the array-based adjacency lists we use for our DFS/BFS algorithms <ref> [13, 9] </ref>. Bit arrays and word level parallelism are often suggested for transitive closure algorithms and often mentioned for adjacency matrix representations. However, despite this awareness of the possible benefits, there has been little detailed study of these effects.
Reference: [10] <author> C. Chekuri, A. Goldberg, D. Karger, M. Levine, and C. Stein, </author> <title> Experimental study of minimum cut algorithms. </title> <type> Technical Report 96-132, </type> <institution> NEC Research Institute, Inc., </institution> <month> October </month> <year> 1996. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees [21], network flow and matching [5, 8, 15, 23], and min-cut algorithms <ref> [10] </ref>. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [11] <author> T. Cormen, C. Leiserson, R. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> McGraw-Hill, </publisher> <year> 1990. </year>
Reference-contexts: Our main focus is to look at what makes for an efficient graph representation. Thus, we first study the performance of the basic data structures in isolation and then use the indicated structures in the most fundamental graph algorithms: Breadth-First-Search (BFS) and Depth-First-Search (DFS) <ref> [11] </ref>. We also consider ways to predict performance and give indications of the effectiveness of these predictions by careful measurements of both actual run time and other statistics such as cache misses and number of instructions executed.
Reference: [12] <institution> Digital Semiconductor 21164 Alpha Microprocessor Hardware Reference Manual. Digital Equipment Corporation, Maynard, </institution> <address> MA, </address> <year> 1997. </year>
Reference-contexts: DEC0 and DEC1 have a 64K byte cache and use 16-byte cache blocks. Both Alphas have an 8K byte on-chip direct mapped L1 data-cache and a 96K 3-way set-associative L2 cache (21064 is off-chip, 21164 is on-chip) and both use 32-byte cache blocks <ref> [12] </ref>. The Pentium has an L1 cache of 16K for instruction and 16K for data, both 4-way associative with 32-byte line size. The L2 cache is 512K bytes and has a direct 175 MHz bus.
Reference: [13] <author> G. Gallo and S. Pallottino. </author> <title> Shortest paths algorithms. </title> <journal> Annals of Operations Research, </journal> <volume> vol. 13, </volume> <pages> pp. 3-79, </pages> <year> 1988. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths <ref> [13, 9] </ref>, minimum spanning trees [21], network flow and matching [5, 8, 15, 23], and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> Certainly FORTRAN programmers had little temptation to use expensive linked lists for adjacency lists, and there is literature which discusses the array-based adjacency lists we use for our DFS/BFS algorithms <ref> [13, 9] </ref>. Bit arrays and word level parallelism are often suggested for transitive closure algorithms and often mentioned for adjacency matrix representations. However, despite this awareness of the possible benefits, there has been little detailed study of these effects. <p> We focused on six different data structures to represent the graphs tested. Three of these were adjacency lists, and three were adjacency matrices. The three adjacency list implementations were (1) a straightforward linked list, called "LL" above, (2) an array-based adjacency list <ref> [13] </ref>, and (3) a "blocked" linked list where each node of the list contains multiple data items. The three adjacency matrix implementations were (1) a two-dimensional array of integers, (2) a two-dimensional array of characters, and (3) a two-dimensional array of bits. Results were collected on all platforms.
Reference: [14] <author> E. Horowitz and S. Sahni. </author> <title> Fundamentals of Computer Algorithms. </title> <publisher> Computer Science Press, </publisher> <year> 1978. </year>
Reference-contexts: It is also clear that the considerations we discuss in this paper are often ignored in the algorithms community. We found only one set of algorithms or data structures books (those by Horowitz and Sahni <ref> [14] </ref>) which even mentions using an array to represent an adjacency list. Also, texts often discuss the time-space trade-off between bitpacked and non-packed data structures, but rarely the caching benefits of the former. Such discussions are incomplete since they equate instruction count with speed.
Reference: [15] <author> D. Johnson and C. McGeoch Ed. </author> <title> Network Flows and Matching. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <year> 1993. </year> <month> 15 </month>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees [21], network flow and matching <ref> [5, 8, 15, 23] </ref>, and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> Data representation issues are also sometimes ignored. One paper in the first DIMACS challenge compared the performance of two matching algorithms (Micali-Vazirani and Gabow's) but used linked lists for the adjacency list of one and an array for the adjacency list of the other <ref> [15] </ref>. Understanding the quantitative tradeoffs is important since in many complex graph algorithms there may be obvious benefits to using a linked structure (such as when graph contraction is being done).
Reference: [16] <author> D. Knuth. </author> <title> The Stanford GraphBase. </title> <publisher> ACM Press, </publisher> <year> 1994. </year>
Reference-contexts: We show that the performance predicted by the analysis above is exhibited by these algorithms. The graphs we chose for our experiments were taken from two sources: Knuth's Stanford Graph-Base <ref> [16] </ref> and internally generated random graphs. The GraphBase allowed us to quickly and easily generate a wide variety of graphs which are both standard and widely available.
Reference: [17] <author> A. LaMarca and R. Ladner. </author> <title> The influence of caches on the performance of heaps. Journal of Experimental Algorithms, </title> <type> Vol.1, </type> <year> 1996. </year>
Reference-contexts: Thus they attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [20, 17, 18] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [18] <author> A. LaMarca and R. Ladner. </author> <title> The influence of caches on the performance of sorting. </title> <booktitle> In the Eighth Annual ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1997. </year>
Reference-contexts: Thus they attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [20, 17, 18] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [19] <author> A. LaMarca and R. Ladner. </author> <title> Cache Performance Analysis of Algorithms. </title> <type> Preprint, </type> <year> 1997. </year>
Reference-contexts: Lebeck and Wood focused on recoding the SPEC benchmarks and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality. They also developed a new methodology for analyzing cache effects <ref> [19] </ref>. While we didn't directly use their analysis tools since they were for direct-mapped caches and somewhat different access patterns, our analysis does use some of their ideas. These papers show that substantial improvements in performance can be gained by improving data locality.
Reference: [20] <author> A. Lebeck and D. Wood. </author> <title> Cache profiling and the spec benchmarks: a case study. </title> <journal> Computer, </journal> <volume> 27(10) </volume> <pages> 15-26, </pages> <year> 1994. </year>
Reference-contexts: Thus they attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure. More recently several researchers have focused on designing algorithms to improve cache performance by improving the locality of the algorithms <ref> [20, 17, 18] </ref>. Lebeck and Wood focused on recoding the SPEC benchmarks and also developed a cache-profiler to help in the design of faster algorithms. LaMarca and Ladner came up with improved heap and later sorting algorithms by improving locality.
Reference: [21] <author> B. Moret and H. Shapiro. </author> <title> An Empirical Assessment of Algorithms for Constructing a Minimum spanning tree. </title> <booktitle> DIMACS Series in Discrete Mathematics and Theoretical Computer Science, </booktitle> <volume> vol. 15, </volume> <pages> pp. 99-117, </pages> <year> 1994. </year>
Reference-contexts: The desire to understand how different algorithms perform in practice has led to a recent increase in the experimental study of algorithms. There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees <ref> [21] </ref>, network flow and matching [5, 8, 15, 23], and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices. <p> Moret and Shapiro discuss cache effects on graph algorithms in their minimum spanning tree paper <ref> [21] </ref>. They comment that data caching and performance is affected by the method used to store a graph. Thus they attempt to normalize for machine effects by using running times relative to the time needed to scan the adjacency structure.
Reference: [22] <author> R. Orni and U. Vishkin. </author> <title> Two computer systems paradoxes: </title> <journal> serialize-to-parallelize and queuing concurrent-writes. </journal> <note> Preprint 1995. </note>
Reference-contexts: Compilers are becoming increasingly effective at automatically extracting this parallelism from programs, but careful coding can result in substantial improvements <ref> [22, 6, 25] </ref>.
Reference: [23] <author> J. Setubal. </author> <title> Sequential and parallel experimental results with bipartite matching algorithms. </title> <type> Technical Report EC-96-09, </type> <institution> Institute of Computing, University of Campinas, Brasil, </institution> <year> 1996. </year>
Reference-contexts: There have been a number of experimental studies of graph algorithms which focus on important problems such as shortest paths [13, 9], minimum spanning trees [21], network flow and matching <ref> [5, 8, 15, 23] </ref>, and min-cut algorithms [10]. These experiments provide valuable insight into the performance of different algorithms and can suggest new algorithmic choices.
Reference: [24] <author> A. Srivastava and A. Eustace. </author> <title> ATOM: A system for building customized program analysis tools. </title> <booktitle> In Proceedings fo the 1994 ACM Symposium on Programming Language Design and Implementation, </booktitle> <pages> pages 196-205, </pages> <year> 1994. </year>
Reference-contexts: Timings of Basic Data Structures for 4 Processors Our experiments using Atom <ref> [24] </ref> to study the cache misses on Alpha1 shows very much what 6 we predicted. When n exceeds the cache size there are almost exactly n=8 cache misses for ARRAY reflecting the 8 integers per cache block brought in by each miss. <p> We also used pixie to collect timing, instruction count, and profiling statistics. Instruction count and cache miss statistics on the Alphas were collected using Atom which is a 14 package developed by DEC for instrumenting code on Alphas <ref> [24] </ref>. It should be noted that the cache misses are computed using the actual instruction stream of the program but are based on a simulation of the memory system. All of our reported results are the average of at least 10 timing experiments.
Reference: [25] <author> U. Vishkin. </author> <title> Can parallel algorithms enhance serial implementation? Communications of the ACM, </title> <booktitle> 39(9) (1996), </booktitle> <pages> 88-91. 16 </pages>
Reference-contexts: Compilers are becoming increasingly effective at automatically extracting this parallelism from programs, but careful coding can result in substantial improvements <ref> [22, 6, 25] </ref>.
References-found: 25

