URL: http://www.cs.panam.edu/~meng/unix-home/Research/Papers/nanjing.ps
Refering-URL: http://www.cs.panam.edu/~meng/unix-home/Research/Papers/
Root-URL: http://www.cs.panam.edu
Email: Email: fchen, meng, foxg@cs.panam.edu  
Title: Efficient Algorithms for Learning Decision Trees Over Large Domains  
Author: Zhixiang Chen Xiannong Meng Richard Fox 
Address: 1201 West University Drive, Edinburg, TX 78539-2999, USA  
Affiliation: Department of Computer Science, University of Texas Pan American  
Abstract: One obstacle of applying the monotone theory in [4, 5] on designing efficient learning algorithm is that exponentially many minterms may be produced. In this paper we consider another approach - an extension of the monotone system (X n ; ; ) to a -efficient monotone system (X n ; ; ; ), where (X; ) is a partially ordered set. We use the partial order to "cluster" many minterms into a "large minterm" and give sufficient conditions for designing efficient learning algorithms. We then apply this approach on designing new algorithms for learning decision trees over large domains. For example, we show that -monotone decision trees over any alphabet are efficiently learnable using equivalence and membership queries. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Aiello and M. Mihail, </author> <title> "Learning the fourier spectrum of probabilistic lists and trees", </title> <booktitle> Proc. of the ACM-SIAM Symposium on Discrete Algorithms, </booktitle> <year> 1991. </year>
Reference: [2] <author> D. Angluin, </author> <title> "Queries and concept learning", </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <year> 1988, </year> <pages> pages 319-342. </pages>
Reference-contexts: 1 Introduction Bshouty's monotone theory [4, 5], which was motivated by Angluin's algorithm for learning monotone DNF formulas <ref> [2] </ref>, is summarized by three algorithms: the fl-Algorithm, the F DNF-Algorithm and the fl+FDNF-Algorithm. Given a monotone basis for a target concept class C, the first algorithm learns any target function F = " OE2 M OE F 2 C from positive counterexamples. <p> We list several open problems in section 7. 2 2 The Learning Model, Definitions and Notations Our learning model is the on-line learning from equivalence and membership queries introduced in Angluin <ref> [2] </ref> and Littlestone [17]. The goal of a learning algorithm for a class C of boolean-valued functions over the domain is to learn any unknown target function f 2 C that has been fixed by a teacher.
Reference: [3] <author> A. Blum, </author> <title> "Separating PAC and mistake-bounded learning models over the boolean domain", </title> <booktitle> Proc. of the 31th Annual ACM Symposium on Foundations of Computer Science, </booktitle> <pages> pages 211-218, </pages> <year> 1990. </year>
Reference: [4] <author> N. Bshouty, </author> <title> "Exact learning boolean functions via the monotone theory", </title> <journal> Information and Computation, </journal> <volume> Vol. 123, </volume> <pages> pages 146-153, </pages> <year> 1995. </year>
Reference-contexts: 1 Introduction Bshouty's monotone theory <ref> [4, 5] </ref>, which was motivated by Angluin's algorithm for learning monotone DNF formulas [2], is summarized by three algorithms: the fl-Algorithm, the F DNF-Algorithm and the fl+FDNF-Algorithm. <p> The third algorithm is a combination of the first two. The third algorithm assumes that it knows some information of the monotone basis and learns a target using the first two algorithms. The monotone theory <ref> [4, 5] </ref> is a remarkable success in designing efficient algorithm for learning subclasses of boolean functions, boolean decision trees, and decision trees over any alphabet. It was also used to learn decision trees with large output domains [7]. <p> Similarly, F 2 tF is called a F -DNF, and size FDNF (F ) is the minimal t such that F = F 1 _ _ F t , where F i 2 F . More definitions and notations can be found in <ref> [4] </ref>. 3 -Efficient Monotone Systems, fl + + DNF-Algorithms 3.1 The -Efficient Monotone System We say (X n ; ; ; ) is a -efficient monotone system if (X n ; ; ) is a monotone system and (X; ) is a partially ordered set. <p> decision tree T over X n is the same as a DSL-decision tree except that we use subsets from W (X) to label edges of T . 7 5 Applications of the -Efficient Monotone Theory 5.1 Hamming Partial Order, 1 , and a We first introduce the Hamming partial order <ref> [4] </ref>. <p> We only consider a 2 Z N such that a = 0 or ajN . Remark 5.9. For any a such that gcd (a; N ) = 1, module decision trees with the value label a belong to the decision tree over Z N studied in <ref> [4] </ref>. Consider the system (Z n N ; H ; L (Z n N ); a ).
Reference: [5] <author> N. Bshouty, </author> <title> "Exact learning via the monotone theory", </title> <type> unpublished manuscript, </type> <year> 1995. </year>
Reference-contexts: 1 Introduction Bshouty's monotone theory <ref> [4, 5] </ref>, which was motivated by Angluin's algorithm for learning monotone DNF formulas [2], is summarized by three algorithms: the fl-Algorithm, the F DNF-Algorithm and the fl+FDNF-Algorithm. <p> The third algorithm is a combination of the first two. The third algorithm assumes that it knows some information of the monotone basis and learns a target using the first two algorithms. The monotone theory <ref> [4, 5] </ref> is a remarkable success in designing efficient algorithm for learning subclasses of boolean functions, boolean decision trees, and decision trees over any alphabet. It was also used to learn decision trees with large output domains [7]. <p> Examples of the DSL-decision trees include D s -decision trees <ref> [5] </ref>, module decision trees [6, 8, 9] in which each internal node is labeled by a module function, and -monotone decision trees which will be defined in section 4. <p> There is a proof of (2) regarding D s -decision trees in <ref> [5] </ref>. Similar proofs can be obtained for module decision trees and -monotone decision trees. In contrast to the exponential L (X n )-DNF size, a DSL-decision tree has a very simple representation with elements from G (or from F ) . <p> This observation motivated the authors to study the problem of "clustering" many local minterms with a "high-level construct" so that one can learn the target concept by learning those "high-level constructs". In essence, Bshouty's group monotone theory <ref> [5] </ref> is a way to overcome the exponential -DNF size problem of a target concept in certain aspects by mapping many minterms into one "large minterm" over a group so that one can learn a target function via learning those "large minterms". <p> It is the union of two disjoint chains (f0; 2; 4; 6g; 2 ) and (f1; 3; 5; 7g; 2 ). It is clear that (Z 8 ; 2 ) has no minimum or maximum elements. Bshouty <ref> [5] </ref> proved that if a partially ordered set is perfect then it is a lattice or adding a maximum element to it will make it a lattice. Hence, (Z 8 ; 2 ) is not perfect. In general, Lemma 6.1.
Reference: [6] <author> N. Bshouty, Z. Chen, S. Decatur, S. Homer, </author> <title> "On the learnability of Z N -DNF formulas", </title> <booktitle> Proc. of the 8th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 198-205, </pages> <year> 1995. </year>
Reference-contexts: Examples of the DSL-decision trees include D s -decision trees [5], module decision trees <ref> [6, 8, 9] </ref> in which each internal node is labeled by a module function, and -monotone decision trees which will be defined in section 4.
Reference: [7] <author> N. Bshouty, C. Tamon, D. Wilson, </author> <title> "On learning decision trees with large output domains", </title> <booktitle> Proc. of the 8th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 190-197, </pages> <year> 1995. </year>
Reference-contexts: The monotone theory [4, 5] is a remarkable success in designing efficient algorithm for learning subclasses of boolean functions, boolean decision trees, and decision trees over any alphabet. It was also used to learn decision trees with large output domains <ref> [7] </ref>. To apply the monotone theory to learn a concept class C, one needs to build a monotone system (X; ; ), to calculate the monotone dimension, and to estimate the -DNF size of a target concept with respect to the partial order .
Reference: [8] <author> Z. Chen, R. Fox, X. Meng, </author> <title> "On learning decision trees with module nodes", </title> <booktitle> Proc. of the 16th Annual Conference of the International Association of Management, Supplement to Computer Science Volume 16, </booktitle> <year> 1998. </year>
Reference-contexts: Examples of the DSL-decision trees include D s -decision trees [5], module decision trees <ref> [6, 8, 9] </ref> in which each internal node is labeled by a module function, and -monotone decision trees which will be defined in section 4.
Reference: [9] <author> Z. Chen, X. Meng, </author> <title> "A faster algorithm for learning decision trees with module nodes", </title> <booktitle> Proc. of the Ninth Midwest Artificial Intelligence and Cognitive Science Conference, </booktitle> <pages> pages 83-90, </pages> <year> 1998. </year>
Reference-contexts: Examples of the DSL-decision trees include D s -decision trees [5], module decision trees <ref> [6, 8, 9] </ref> in which each internal node is labeled by a module function, and -monotone decision trees which will be defined in section 4.
Reference: [10] <author> A. Ehrenfeucht and D. Haussler, </author> <title> "Learning decision trees from random examples", </title> <journal> Information and Computation, </journal> <volume> 3, </volume> <pages> pages 231-246, </pages> <year> 1989. </year>
Reference: [11] <author> T. Hancock, </author> <title> "Learning 2 DNF formulas and k decision trees", </title> <booktitle> Proc. of the 4th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 199-209, </pages> <year> 1991 </year>
Reference: [12] <author> T. Hancock, </author> <title> "Learning k decision trees on the uniform distribution", </title> <booktitle> Proc. of the 6rd Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 352-360, </pages> <year> 1993 </year>
Reference: [13] <author> T. Hancock and Y. Mansour, </author> <title> "Learning monotone k DNF formulas on product distribution", </title> <booktitle> Proc. of the 4th Annual ACM Conference on Computational Learning Theory, </booktitle> <pages> pages 179-183, </pages> <year> 1991 </year>
Reference: [14] <author> N. Jacobson, </author> <title> Basic Algebra I, </title> <editor> W. H. </editor> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1985. </year>
Reference: [15] <author> M. Kamber, L. Winstone, W. Gong, S. Cheng, and J. Han, </author> <title> "Generalization and decision tree induction: Efficient classification in data mining", </title> <booktitle> Proc. of 1997 Int'l Workshop on Research Issues on Data Engineering (RIDE'97), </booktitle> <address> England, </address> <year> 1997. </year>
Reference-contexts: Practical uses of decision trees have ranged from mechanical diagnosis [22] to design [18] to decision making in business, for instance to assess consumer credit [21]. In those practical applications, DSL-decision trees or the like are used. For example, in <ref> [15] </ref>, a trees in figure 1 was generated by an improved version of Quilan's ID3 [20] for data mining of a CITYDATA database. This paper is organized as follows. In section 2, we introduce the learning model and definitions as well as notations needed for defining monotone systems.
Reference: [16] <author> E. Kushilevitz and Y. Mansour, </author> <title> "Learning decision trees using the fourier spectrum", </title> <booktitle> Proc. of the 23th Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 455-464, </pages> <year> 1991. </year> <month> 11 </month>
Reference: [17] <author> N. Littlestone, </author> <title> "Learning quickly when irrelevant attributes abound: A new linear-threshold algorithm", </title> <journal> Machine Learning, </journal> <volume> 2, </volume> <pages> pages 285-318, </pages> <year> 1988. </year>
Reference-contexts: We list several open problems in section 7. 2 2 The Learning Model, Definitions and Notations Our learning model is the on-line learning from equivalence and membership queries introduced in Angluin [2] and Littlestone <ref> [17] </ref>. The goal of a learning algorithm for a class C of boolean-valued functions over the domain is to learn any unknown target function f 2 C that has been fixed by a teacher.
Reference: [18] <editor> D. Michie, </editor> <booktitle> "Current developments in expert systems", in Proc. 2nd Australian Conference on Applications of Expert Systems, </booktitle> <pages> pages 163-182, </pages> <address> Sydney, Australia. </address>
Reference-contexts: Our approach is conceptually simpler than the group monotone theory but can achieve efficient learning algorithms which cannot be obtained via the group monotone theory. Practical uses of decision trees have ranged from mechanical diagnosis [22] to design <ref> [18] </ref> to decision making in business, for instance to assess consumer credit [21]. In those practical applications, DSL-decision trees or the like are used. For example, in [15], a trees in figure 1 was generated by an improved version of Quilan's ID3 [20] for data mining of a CITYDATA database.
Reference: [19] <author> J. Mingers, </author> <title> "An empirical comparison of selection measures for decision-trees induction", </title> <journal> Machine Learning, </journal> <volume> 3, </volume> <pages> pages 319-342, </pages> <year> 1989. </year>
Reference: [20] <author> J. Quinlan, </author> <title> "Induction of decision trees", </title> <journal> Machine Learning, </journal> <volume> 1, </volume> <pages> pages 81-106, </pages> <year> 1986. </year>
Reference-contexts: In those practical applications, DSL-decision trees or the like are used. For example, in [15], a trees in figure 1 was generated by an improved version of Quilan's ID3 <ref> [20] </ref> for data mining of a CITYDATA database. This paper is organized as follows. In section 2, we introduce the learning model and definitions as well as notations needed for defining monotone systems. In section 3, we present the -efficient monotone theory.
Reference: [21] <author> E. Rich and K. Knight, </author> <booktitle> Artificial Intelligence, 2nd edition, </booktitle> <publisher> McGraw Hill, </publisher> <year> 1991. </year>
Reference-contexts: Practical uses of decision trees have ranged from mechanical diagnosis [22] to design [18] to decision making in business, for instance to assess consumer credit <ref> [21] </ref>. In those practical applications, DSL-decision trees or the like are used. For example, in [15], a trees in figure 1 was generated by an improved version of Quilan's ID3 [20] for data mining of a CITYDATA database. This paper is organized as follows.
Reference: [22] <author> V. Sembugamoorthy and B. Chandrasekaran, </author> <title> "Functional representation of devices and compilation of diagnostic problem-solving systems", in Experience, Memory and Learning, </title> <editor> J. Kolodner and C. Reisbeck editors, </editor> <publisher> Lawrence Erlbaum Associates, </publisher> <year> 1986. </year>
Reference-contexts: Our approach is conceptually simpler than the group monotone theory but can achieve efficient learning algorithms which cannot be obtained via the group monotone theory. Practical uses of decision trees have ranged from mechanical diagnosis <ref> [22] </ref> to design [18] to decision making in business, for instance to assess consumer credit [21]. In those practical applications, DSL-decision trees or the like are used.
Reference: [23] <author> L. Valiant, </author> <title> "A theory of the learnable", </title> <journal> Communications of the ACM, </journal> <volume> 27, </volume> <pages> pages 1134-1142, </pages> <year> 1984. </year> <month> 12 </month>
References-found: 23

