URL: http://www.cs.purdue.edu/homes/saw/cs590c/lectures/week6/itpack2cdoc.ps
Refering-URL: http://www.cs.purdue.edu/homes/saw/cs590c/lectures/
Root-URL: http://www.cs.purdue.edu
Title: ITPACK 2C: A FORTRAN Package for Solving Large Sparse Linear Systems by Adaptive Accelerated Iterative Methods  
Author: David R. Kincaid, John R. Respess, and David M. Young Roger G. Grimes 
Date: October 11, 1995  
Affiliation: University of Texas at Austin  Boeing Computer Services Company  
Abstract: ITPACK 2C is a collection of seven FORTRAN subroutines for solving large sparse linear systems by adaptive accelerated iterative algorithms. Basic iterative procedures, such as the Jacobi method, the Successive Overrelaxation method, the Symmetric Successive Overrelaxation method, and the RS method for the reduced system are combined, where possible, with acceleration procedures such as Chebyshev (Semi-Iteration) and Conjugate Gradient for rapid convergence. Automatic selection of the acceleration parameters and the use of accurate stopping criteria are major features of this software package. While the ITPACK routines can be called with any linear system containing positive diagonal elements, they are the most successful in solving systems with symmetric positive definite or mildly nonsymmetric coefficient matrices. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> S. Eisenstat, A. George, R. Grimes, D. Kincaid, and A. Sherman. </author> <title> "Some Comparisons of Software Packages for Large Sparse Linear Systems," in Advances in Computer Methods for Partial Differential Equations III, </title> <editor> (R. Vichnevetsky and R. Stepleman, eds.), </editor> <booktitle> Publ. IMACS, </booktitle> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, New Jersey, 08903, </address> <year> 1979, </year> <pages> pp. 98-106. </pages>
Reference-contexts: example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2. <p> methods when using a red-black indexing. 6 Numerical Results The iterative algorithms in ITPACK have been tested over a wide class of matrix problems arising from elliptic partial differential equations with Dirichlet, Neumann, and mixed boundary conditions on arbitrary two-dimensional regions (including cracks and holes) and on rectangular three-dimensional regions <ref> [1] </ref>. Both finite-difference and finite-element procedures have been employed to obtain the linear systems. The two sample problems presented here, while simple to pose, are representative of the behavior of the ITPACK routines for more complex problems. <p> Some additional improvements and corrections were made in the 2C version. The algorithms in ITPACK are not guaranteed to converge for all linear systems but have been shown to work successfully for a large number of symmetric and nonsymmetric systems which arise from solving elliptic partial differential equations <ref> [1, 13] </ref>. The numerical algorithms in ITPACK 2C correspond to those described in the appendix of technical report [5] and outlined in the book [7]. In particular, the SOR code is based on 18 an algorithm suggested to us by L. Hageman. Various other algorithms exist for iterative methods.
Reference: [2] <author> G. Golub and R. Varga. </author> <title> "Chebyshev Semi-Iterative Methods, Successive Overrelaxation Iterative Methods, and Second-Order Richardson Iterative Methods," Parts I & II, </title> <journal> Numerische Mathematik, </journal> <volume> Vol. 3, </volume> <year> 1961, </year> <pages> pp. 147-168. </pages>
Reference-contexts: On the black unknowns, the Cyclic Chebyshev Semi-Iterative (CCSI) method of Golub and Varga <ref> [2] </ref> gives the same result as the RSSI method. The CCSI and RSSI methods converge at the same rate, and each of them converges twice as fast as the JSI method.
Reference: [3] <author> R. Grimes, D. Kincaid, W. Macgregor, and D. Young. </author> <title> "ITPACK Report: Adaptive Iterative Algorithms Using Symmetric Sparse Storage," CNA-139, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, August </month> <year> 1978. </year>
Reference-contexts: The ITPACK routines used iterative algorithms which were refined from the prototype programs. However, these routines were designed to solve large sparse linear systems of algebraic equations instead of partial differential equations. The use of three interchangeable symmetric sparse storage modes in ITPACK 1.0 <ref> [3] </ref> allowed for great flexibility and made it possible to solve a wider class of problems than the prototype programs and to study different storage modes for iterative methods.
Reference: [4] <author> R. Grimes, D. Kincaid, and D. Young. </author> <title> "ITPACK 2.0 User's Guide," CNA-150, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, August </month> <year> 1979. </year>
Reference-contexts: In other words, iterative methods may not converge when applied to systems with coefficient matrices which are completely general with no special properties. This article discusses the usage of ITPACK and gives a few test results. The description of the iterative methods is given in <ref> [4] </ref>. The underlying theory on which the iterative algorithms are based is described in [6]. A survey of the iterative methods in ITPACK is presented in [11]. Throughout this paper, we adopt notation such as SOR () when referring to a subroutine and A (*) for a single-dimensioned array. <p> example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2. <p> The iterative algorithms used in ITPACK are quite complicated and some knowledge of iterative methods is necessary to completely understand them. The interested reader should consult the technical report <ref> [4] </ref> and the book [6] for details. <p> Default: 1 [0: fixed iterative parameters used for SME, CME, OMEGA, SPECR, and BETAB (nonadaptive); 1: fully adaptive procedures used for all parameters; 2: (SSOR methods only) SPECR determined adaptively and CME, BETAB, and OMEGA fixed; 3: (SSOR methods only) BETAB fixed and all other parameters determined adaptively] (See <ref> [4, 6] </ref> for details and RPARM (I); I = 2; 3; 5; 6; 7 for CME, SME, OMEGA, SPECR, BETAB, respectively. These parameters are set by subroutine DFAULT () or by the user.) 6 IPARM (7) ICASE is the adaptive procedure case switch for the JSI and SSOR methods. <p> The stopping criterion is a test of whether ZETA is greater than the ratio of the two norm of the pseudo-residual vector and the two norm of the current iteration vector times a constant involving an eigenvalue estimate. (See <ref> [4, 6] </ref> for details.) Default: 5 fi 10 6 RPARM (2) CME is the estimate of the largest eigenvalue of the Jacobi matrix. It changes to a new estimate if the adaptive procedure is used. CME M (B). <p> When the spectral radius of LU is less than or equal to 1 4 , the "SSOR condition" is satisfied for some problems provided one uses the natural ordering. (See <ref> [4, 5, 18] </ref> for additional details.) Default: 0:25. RPARM (8) TOL is the tolerance factor near machine relative precision, SRELPR. <p> The use of three interchangeable symmetric sparse storage modes in ITPACK 1.0 [3] allowed for great flexibility and made it possible to solve a wider class of problems than the prototype programs and to study different storage modes for iterative methods. The next version, ITPACK 2.0 <ref> [4] </ref>, was significantly faster than its predecessor since it was restricted to allow only one sparse symmetric storage format. Most of the iterative algorithms utilized in the 2.0 version of this package assume that the coefficient matrix of the linear system is symmetric positive definite.
Reference: [5] <author> R. Grimes, D. Kincaid, and D. Young. </author> <title> "ITPACK 2A: A FORTRAN Implementation of Adaptive Accelerated Iterative Methods for Solving Large Sparse Linear Systems," CNA-164, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, October </month> <year> 1980. </year>
Reference-contexts: example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2. <p> When the spectral radius of LU is less than or equal to 1 4 , the "SSOR condition" is satisfied for some problems provided one uses the natural ordering. (See <ref> [4, 5, 18] </ref> for additional details.) Default: 0:25. RPARM (8) TOL is the tolerance factor near machine relative precision, SRELPR. <p> As with many packages, the need to handle a slightly larger class of problems, namely, nearly symmetric systems, soon became evident. This required adapting the routines to allow a switch for either a symmetric or nonsymmetric storage mode in ITPACK 2A <ref> [5] </ref>. Moreover, a modification of the Conjugate Gradient algorithms was developed to handle nearly symmetric systems [12]. <p> The numerical algorithms in ITPACK 2C correspond to those described in the appendix of technical report <ref> [5] </ref> and outlined in the book [7]. In particular, the SOR code is based on 18 an algorithm suggested to us by L. Hageman. Various other algorithms exist for iterative methods. For example, S.
Reference: [6] <author> L. Hageman and D. Young. </author> <title> Applied Iterative Methods, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981. </year>
Reference-contexts: All methods are available with adaptive parameter estimation and automatic stopping tests. When using the RS method it is required that the linear system be reordered into a "red-black" 1 system <ref> [6, 12] </ref>. A switch to compute, if possible, the red-black indexing, permute the linear system, and permute associated vectors is provided. The successful convergence of iterative methods may be dependent on conditions that are difficult to determine in advance. <p> This article discusses the usage of ITPACK and gives a few test results. The description of the iterative methods is given in [4]. The underlying theory on which the iterative algorithms are based is described in <ref> [6] </ref>. A survey of the iterative methods in ITPACK is presented in [11]. Throughout this paper, we adopt notation such as SOR () when referring to a subroutine and A (*) for a single-dimensioned array. <p> example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2. <p> The iterative algorithms used in ITPACK are quite complicated and some knowledge of iterative methods is necessary to completely understand them. The interested reader should consult the technical report [4] and the book <ref> [6] </ref> for details. <p> Default: 1 [0: fixed iterative parameters used for SME, CME, OMEGA, SPECR, and BETAB (nonadaptive); 1: fully adaptive procedures used for all parameters; 2: (SSOR methods only) SPECR determined adaptively and CME, BETAB, and OMEGA fixed; 3: (SSOR methods only) BETAB fixed and all other parameters determined adaptively] (See <ref> [4, 6] </ref> for details and RPARM (I); I = 2; 3; 5; 6; 7 for CME, SME, OMEGA, SPECR, BETAB, respectively. These parameters are set by subroutine DFAULT () or by the user.) 6 IPARM (7) ICASE is the adaptive procedure case switch for the JSI and SSOR methods. <p> has Property A, since m (B) = M (B). 3 Also, if A is an L-matrix, then for the Jacobi matrix, we have jm (B)j M (B) and SME is always CME (Case II). 4 Selecting the correct case may increase the rate of convergence of the iterative method. (See <ref> [6] </ref> for additional discussion on Case I and II. Also, see RPARM (I); I = 2; 3 for CME, SME, respectively.) IPARM (8) NWKSP is the amount of workspace used. It is used for output only. <p> The stopping criterion is a test of whether ZETA is greater than the ratio of the two norm of the pseudo-residual vector and the two norm of the current iteration vector times a constant involving an eigenvalue estimate. (See <ref> [4, 6] </ref> for details.) Default: 5 fi 10 6 RPARM (2) CME is the estimate of the largest eigenvalue of the Jacobi matrix. It changes to a new estimate if the adaptive procedure is used. CME M (B). <p> Default: 0:0 RPARM (11) DIGIT1 is the approximate number of digits using the estimated relative error with the final approximate solution. It is computed as the negative of the logarithm base ten of the final value of the stopping test. (See details below or <ref> [6] </ref>.) Default: 0:0 RPARM (12) DIGIT2 is the approximate number of digits using the estimated relative residual with the final approximate solution. <p> This estimate is related to the condition number of the original linear system and, therefore, it will not be accurate if the system is ill-conditioned. (See details below or <ref> [6] </ref>.) Default: 0:0 DIGIT1 is determined from the actual stopping test computed on the final iteration, whereas DIGIT2 is based on the computed residual vector using the final approximate solution after the algorithm has converged. <p> corresponding to diagonal entry A i;i are to be eliminated, then the right-hand side is adjusted to b i b i =A i;i and b j b j b i A i;j for j 6= i. 9 stopping test has not worked successfully or the original system is ill-conditioned. (See <ref> [6] </ref> for additional details.) For storage of certain intermediate results, the solution modules require a real vector WKSP (*) and a corresponding variable NW indicating the available space. The length of the workspace array varies with each solution module and the maximum amount needed is given in the following table. <p> The CCSI and RSSI methods converge at the same rate, and each of them converges twice as fast as the JSI method. This is a theoretical result <ref> [6] </ref> and does not count the time involved in establishing the red-black indexing and the red-black partitioned system. Similarly, the Cyclic Conjugate Gradient (CCG) method with respect to the black unknowns, considered by Reid [16] (see also Hageman and Young [6]), gives the same results as the RSCG method. <p> This is a theoretical result <ref> [6] </ref> and does not count the time involved in establishing the red-black indexing and the red-black partitioned system. Similarly, the Cyclic Conjugate Gradient (CCG) method with respect to the black unknowns, considered by Reid [16] (see also Hageman and Young [6]), gives the same results as the RSCG method. Also, the CCG and the RSCG methods converge at the same rate, and each of them converges, theoretically, exactly twice as fast as the JCG method.
Reference: [7] <author> L. Hayes and D. Young. </author> <title> "The Accelerated SSOR Method for Solving Large Linear Systems: Preliminary Report," CNA-123, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, May </month> <year> 1977. </year>
Reference-contexts: The numerical algorithms in ITPACK 2C correspond to those described in the appendix of technical report [5] and outlined in the book <ref> [7] </ref>. In particular, the SOR code is based on 18 an algorithm suggested to us by L. Hageman. Various other algorithms exist for iterative methods. For example, S.
Reference: [8] <author> D. Kincaid and R. Grimes. </author> <title> "Numerical Studies of Several Adaptive Iterative Algorithms," CNA-126, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, August </month> <year> 1977. </year>
Reference-contexts: be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2. <p> Initially, prototype programs were written based on preliminary iterative algorithms involving adaptive selection of parameters and automatic stopping procedures. These programs were tested on a large set of elliptic partial differential equations over domains compatible with the subroutine REGION () <ref> [8] </ref> which superimposed a square grid over the domain. These routines were designed for solving self-adjoint elliptic partial differential equations. Next a preliminary version of ITPACK was coded in standard FORTRAN. The ITPACK routines used iterative algorithms which were refined from the prototype programs.
Reference: [9] <author> D. Kincaid, R. Grimes, W. Macgregor, and D. Young. </author> <title> "ITPACK|Adaptive Iterative Algorithms Using Symmetric Sparse Storage," </title> <booktitle> in Symposium on Reservoir Simulation, Society of Petroleum Engineers of AIME, 6200 North Central Expressway, </booktitle> <address> Dallas, Texas, 75206, </address> <month> February </month> <year> 1979, </year> <pages> pp. 151-160. </pages>
Reference-contexts: example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 9; 12] and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 8; 9] </ref> 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system to be solved. The data structure for the matrix of the system is either the symmetric or nonsymmetric sparse storage format described in Section 2.
Reference: [10] <author> D. Kincaid, R. Grimes, and D. Young. </author> <title> "The Use of Iterative Methods for Solving Large Sparse PDE-Related Linear Systems," Mathematics and Computers in Simulation XXI, </title> <publisher> North-Holland Publishing Company, </publisher> <address> New York, </address> <year> 1979, </year> <pages> pp. 368-375. </pages>
Reference: [11] <author> D. Kincaid and D. Young. </author> <title> "Survey of Iterative Methods," </title> <journal> in Encyclopedia of Computer Sciences and Technology, </journal> <note> Vol. 13 (J. </note> <editor> Belzer, A. Holzman, and A. Kent, eds.), </editor> <publisher> Marcel Dekker, Inc., </publisher> <address> New York, </address> <year> 1979, </year> <pages> pp. 354-391. 20 </pages>
Reference-contexts: This article discusses the usage of ITPACK and gives a few test results. The description of the iterative methods is given in [4]. The underlying theory on which the iterative algorithms are based is described in [6]. A survey of the iterative methods in ITPACK is presented in <ref> [11] </ref>. Throughout this paper, we adopt notation such as SOR () when referring to a subroutine and A (*) for a single-dimensioned array.
Reference: [12] <author> D. Kincaid and D. Young. </author> <title> "Adapting Iterative Algorithms Developed for Symmetric Systems to Nonsymmetric Systems," in Elliptic Problem Solvers, </title> <editor> (M. Schultz, ed.), </editor> <publisher> Academic Press, </publisher> <address> New York, 1981, p. </address> <pages> 353-359. </pages>
Reference-contexts: All methods are available with adaptive parameter estimation and automatic stopping tests. When using the RS method it is required that the linear system be reordered into a "red-black" 1 system <ref> [6, 12] </ref>. A switch to compute, if possible, the red-black indexing, permute the linear system, and permute associated vectors is provided. The successful convergence of iterative methods may be dependent on conditions that are difficult to determine in advance. <p> example, the coefficient matrix 2 6 6 11: 0: 0: 14: 15: 0: 0: 33: 0: 0: 15: 0: 0: 45: 55: 7 7 5 would be represented in nonsymmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 14:; 44:; 45:; 15:; 45:; 55:] IA (fl) = <ref> [1; 4; 5; 6; 9; 12] </ref> and in symmetric sparse storage as A (fl) = [11:; 14:; 15:; 22:; 33:; 44:; 45:; 55:] IA (fl) = [1; 4; 5; 6; 8; 9] 3 Usage The user is expected to provide the coefficient matrix and the right-hand side of the linear system <p> This required adapting the routines to allow a switch for either a symmetric or nonsymmetric storage mode in ITPACK 2A [5]. Moreover, a modification of the Conjugate Gradient algorithms was developed to handle nearly symmetric systems <ref> [12] </ref>. ITPACK has been improved in the 2B version [14] by (a) writing more efficient versions of several key subroutines, (b) incorporating Basic Linear Algebra Subprograms, BLAS [15], and (c) improving the user interface with better printing and documentation. Some additional improvements and corrections were made in the 2C version.
Reference: [13] <author> D. Kincaid. </author> <title> "Acceleration Parameters for a Symmetric Successive Overrelaxation Conjugate Gradient Method for Nonsymmetric Systems," in Advances in Computer Methods for Partial Differential Equations IV, </title> <editor> (R. Vichnevetsky and R. Stepleman, eds.), </editor> <booktitle> Publ. IMACS, </booktitle> <institution> Department of Computer Science, Rutgers University, </institution> <address> New Brunswick, New Jersey, 08903, </address> <year> 1981, </year> <pages> pp. 294-299. </pages>
Reference-contexts: Some additional improvements and corrections were made in the 2C version. The algorithms in ITPACK are not guaranteed to converge for all linear systems but have been shown to work successfully for a large number of symmetric and nonsymmetric systems which arise from solving elliptic partial differential equations <ref> [1, 13] </ref>. The numerical algorithms in ITPACK 2C correspond to those described in the appendix of technical report [5] and outlined in the book [7]. In particular, the SOR code is based on 18 an algorithm suggested to us by L. Hageman. Various other algorithms exist for iterative methods.
Reference: [14] <author> D. Kincaid, R. Grimes, J. Respess, and D. Young. </author> <title> "ITPACK 2B: A FORTRAN Implementation of Adaptive Accelerated Iterative Methods for Solving Large Sparse Linear Systems," CNA-173, Center for Numerical Analysis, </title> <institution> University of Texas, Austin, Texas, </institution> <month> 78712, September </month> <year> 1981. </year>
Reference-contexts: This required adapting the routines to allow a switch for either a symmetric or nonsymmetric storage mode in ITPACK 2A [5]. Moreover, a modification of the Conjugate Gradient algorithms was developed to handle nearly symmetric systems [12]. ITPACK has been improved in the 2B version <ref> [14] </ref> by (a) writing more efficient versions of several key subroutines, (b) incorporating Basic Linear Algebra Subprograms, BLAS [15], and (c) improving the user interface with better printing and documentation. Some additional improvements and corrections were made in the 2C version.
Reference: [15] <author> C. Lawson, R. Hanson, D. Kincaid, and F. Krogh. </author> <title> "Basic Linear Algebra Subprograms for FORTRAN Usage," </title> <journal> ACM Transactions on Mathematical Software, </journal> <volume> Vol. 5., No. 3, </volume> <month> September </month> <year> 1979, </year> <pages> pp. 308-323. </pages>
Reference-contexts: Moreover, a modification of the Conjugate Gradient algorithms was developed to handle nearly symmetric systems [12]. ITPACK has been improved in the 2B version [14] by (a) writing more efficient versions of several key subroutines, (b) incorporating Basic Linear Algebra Subprograms, BLAS <ref> [15] </ref>, and (c) improving the user interface with better printing and documentation. Some additional improvements and corrections were made in the 2C version.
Reference: [16] <author> J. Reid. </author> <title> "The Use of Conjugate Gradients for Systems of Linear Equations Possessing Property A," </title> <journal> SIAM Journal of Numerical Analysis, </journal> <volume> Vol. 9, </volume> <year> 1972, </year> <pages> pp. 325-332. </pages>
Reference-contexts: This is a theoretical result [6] and does not count the time involved in establishing the red-black indexing and the red-black partitioned system. Similarly, the Cyclic Conjugate Gradient (CCG) method with respect to the black unknowns, considered by Reid <ref> [16] </ref> (see also Hageman and Young [6]), gives the same results as the RSCG method. Also, the CCG and the RSCG methods converge at the same rate, and each of them converges, theoretically, exactly twice as fast as the JCG method.
Reference: [17] <author> J. Rice and R. Boisvert. </author> <title> Solving Elliptic Problems Using ELLPACK. </title> <address> New York: </address> <publisher> Springer-Verlag, </publisher> <year> 1985. </year>
Reference-contexts: Hageman. Various other algorithms exist for iterative methods. For example, S. Eisenstat has an implementation of the Symmetric Successive Overrelaxation preconditioned Conjugate Gradient procedure. 7 Modules based on the seven iterative routines in ITPACK have been incorporated into the elliptic partial differential equation solving package ELLPACK <ref> [17] </ref> together with all the necessary translation routines needed. The user-oriented modules described in Section 4 are not in ELLPACK. Moreover, if the ELLPACK system is not being used to generate the linear system for ITPACK, it is recommended that ITPACK be used as a stand-alone package apart from ELLPACK.
Reference: [18] <author> D. Young. </author> <title> Iterative Solution of Large Linear Systems, </title> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1971. </year>
Reference-contexts: When the spectral radius of LU is less than or equal to 1 4 , the "SSOR condition" is satisfied for some problems provided one uses the natural ordering. (See <ref> [4, 5, 18] </ref> for additional details.) Default: 0:25. RPARM (8) TOL is the tolerance factor near machine relative precision, SRELPR. <p> Otherwise, the linear system is used in the order it is given which we call the "natural ordering." The Successive Overrelaxation (SOR) method has been shown to be more effective with the red-black ordering than with the natural ordering for some problems <ref> [18] </ref>. In the SOR algorithm, the first iteration uses ! = 1 and the stopping criterion is set to a large value so that at least one Gauss-Seidel iteration is performed before an approximate value for the optimum relaxation parameter is computed.
Reference: [19] <author> D. Young and D. Kincaid. </author> <title> "The ITPACK Package for Large Sparse Linear Systems," in Elliptic Problem Solvers, </title> <editor> (M. Schultz, ed.), </editor> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1981, </year> <pages> pp. 163-185. </pages>
References-found: 19

