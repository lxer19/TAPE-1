URL: http://www.iscs.nus.sg/~plong/papers/unihalf.ps
Refering-URL: 
Root-URL: 
Title: On the sample complexity of PAC learning halfspaces against the uniform distribution lower bound on
Author: Philip M. Long 
Note: d  This work was done while the author was at Duke University supported by Air Force Office of Scientific Research grant F49620-92-J-0515.  
Date: November 2, 1995  
Address: 3040 Cornwallis Road P.O. Box 12194  Triangle Park, North Carolina 27709 USA  
Affiliation: Research Triangle Institute  Research  
Abstract: We prove an 
Abstract-found: 1
Intro-found: 1
Reference: [ASE92] <author> N. Alon, J.H. Spencer, and P. Erdos. </author> <title> The Probabilistic Method. </title> <publisher> Wiley, </publisher> <year> 1992. </year>
Reference-contexts: In the case of halfspaces, however, we needed the stronger bound of (5). The lower bound for halfspaces is then proved by establishing a tight enough lower bound on M D (HALF d ; 2*) and applying (5). For this, we use a probabilistic method trick from <ref> [ASE92] </ref>. Usually, to use the probabilistic method to show that a large set with a given property exists, one randomly picks a suitably large set, and shows that the probability that the randomly chosen set has the desired property is nonzero. <p> This methodology appears to be too crude for this application. Instead, we use the "removing blemishes" trick from <ref> [ASE92] </ref>, where one randomly picks a large set, shows that the expected number of elements which interfere with the large set having the desired property is not too big, and therefore that there exists a large set without too many such "blemishes", then removes those offending elements to get the required <p> For this, we use the "removing blemishes" probabilistic method trick from <ref> [ASE92] </ref>. 4 Lemma 6 For all 0 &lt; * &lt; 1=2, d 2 N, M UBALL d (HALF d ; *) p 2 1 d1 Proof: For each unit-length ~w 2 R d , define f ~w to be the homogeneous halfspace whose normal vector is ~w, i.e. the function f
Reference: [Bau90] <author> E.B. Baum. </author> <title> The perceptron algorithm is fast for nonmalicious distributions. </title> <journal> Neural Computation, </journal> <volume> 2 </volume> <pages> 248-260, </pages> <year> 1990. </year>
Reference-contexts: We will make use of the following easily verified technical lemmas. Lemma 4 For all x 2 [0; =4], tan x 2x: Lemma 5 (see <ref> [Bau90] </ref>) For all d 2 N, if V d is the volume of the unit ball in R d , then for all d 2, p Now we are ready to lower bound M UBALL d (HALF d ; *).
Reference: [BEHW89] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> JACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: For each class of finite VC-dimension, this matched earlier upper bounds on the sample-complexity of distribution-free PAC learning <ref> [BEHW89, HLW90] </ref> to within log factors. Since the VC-dimension of homogeneous halfspaces in R d is d, (1) implies a lower bound of d + * 1 on the sample complexity of distribution-free PAC learning for this class. <p> This definition is stronger than we need. Obviously, the set of halfspaces has continuous hard pairs with respect to the uniform distribution on the unit ball. The following theorem is a fairly straightforward extension of the corresponding result in <ref> [BEHW89] </ref>. Theorem 8 Choose X, a probability distribution D over X, and F f0; 1g X . If F has continuous hard pairs with respect to D then m (F; D; *; ffi) = 1 log ffi : Proof: Choose 0 &lt; * &lt; 1=2.
Reference: [BI91] <author> G. Benedek and A. Itai. </author> <title> Learnability with respect to fixed distributions. </title> <journal> Theoretical Computer Science, </journal> <volume> 86(2) </volume> <pages> 377-389, </pages> <year> 1991. </year>
Reference-contexts: In the original formulation of the model, distribution-free PAC learning, the distribution D is arbitrary and unknown. Benedek and Itai <ref> [BI91] </ref> studied learning with respect to particular, known D. In each case, after receiving (x 1 ; f (x 1 )); :::; (x m ; f (x m )), the learner outputs a hypothesis h for f . <p> One can then define M D (F; *) to be the largest subset of elements of F , each pair of which is at least * far apart as measured by D . Benedek and Itai <ref> [BI91] </ref> proved a general lower bound of log 2 ((1 ffi)M D (F; 2*)) (3) on the sample complexity of learning F with respect to D. <p> has proved that in general M D (F; *) 10 VCdim (F ) ; (4) the best one could ever get by directly applying (3) is a lower bound on the sample complexity of VCdim (F ) log 1 + log (1 ffi) : However, we adapt the proof of <ref> [BI91] </ref> to prove a lower bound on the sample complexity for the class HALF d of homogeneous halfspaces in d dimensions, when ffi = 1=2, for all distributions D, of d 1 (M D (HALF d ; 2*)=4) 1=(d1) : (5) One can easily see how to use the same technique <p> Since the uniform distribution does not have this property, this trick cannot be used for our problem. 2 These definitions are a commonly studied version of Valiant's PAC model [Val84]; study of the distribution specific version was initiated by Benedek and Itai <ref> [BI91] </ref>. Choose a set X, a probability distribution D over X, and a set F of functions from X to f0; 1g. Define D : f0; 1g X fi f0; 1g X ! [0; 1] by D (f; g) = Pr u2D (f (u) 6= g (u)). <p> Its proof is based on <ref> [BI91, Lemma 4.8] </ref>. Lemma 3 Choose d 2 N; d 2, a distribution D over R d , and * &gt; 0.
Reference: [Cov65] <author> Thomas M. </author> <title> Cover. Geometrical and statistical properties of systems of linear inequalities with applications in pattern recognition. </title> <journal> IEEE Trans. on Electronic Computers, </journal> <volume> EC-14:326-334, </volume> <year> 1965. </year>
Reference-contexts: Theorem 1 m (HALF d ; UBALL d ; *; ffi) = * 1 log ffi : 3.1 The d=* term We will make use of the following bound on the "growth function" of halfspaces, due to Cover <ref> [Cov65] </ref> (see [HKP91, page 113]). Lemma 2 ([Cov65]) Choose d 2 N.
Reference: [EHKV89] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference-contexts: in the distribution-free case, for any D), with probability (with respect to the random choice of x 1 ; :::; x m ) at least 1 ffi, outputs a hypothesis of accuracy at least as good as *. (See Section 2 for a precise definition.) Ehrenfeucht, Haussler, Kearns and Valiant <ref> [EHKV89] </ref> proved a general lower bound of VCdim (F ) * 1 log ffi (1) on the sample complexity of distribution-free PAC learning, where VCdim (F ) is the VC-dimension [VC71] of F , a measure of its complexity (see Section 2 for a precise definition). <p> Since the VC-dimension of homogeneous halfspaces in R d is d, (1) implies a lower bound of d + * 1 on the sample complexity of distribution-free PAC learning for this class. The distributions used in the lower bound proof of <ref> [EHKV89] </ref> are concentrated on VCdim (F ) elements of the domain, and almost all the weight is concentrated on one point. It has been argued that since such distributions are inaccurate models of real-world learning situations, the lower bound of [EHKV89], while interesting theoretically, does not provide even qualitative guidance concerning <p> The distributions used in the lower bound proof of <ref> [EHKV89] </ref> are concentrated on VCdim (F ) elements of the domain, and almost all the weight is concentrated on one point. It has been argued that since such distributions are inaccurate models of real-world learning situations, the lower bound of [EHKV89], while interesting theoretically, does not provide even qualitative guidance concerning the difficulty of real-world learning. <p> where the goal of the algorithm is to get a hypothesis of a given accuracy with a given probability. 1 Moreover, their results depend on the unproven replica hypothesis. 2 Definitions Denote the reals by R and the positive integers by N. 1 The distribution constructed in the proof of <ref> [EHKV89] </ref> had the property that any reasonable algorithm always had an error of at most a constant times *, which enabled them to argue that producing a hypothesis that was good on average was effectively equivalent to producing a hypothesis that is good with a certain probability (see [EHKV89] for further <p> proof of <ref> [EHKV89] </ref> had the property that any reasonable algorithm always had an error of at most a constant times *, which enabled them to argue that producing a hypothesis that was good on average was effectively equivalent to producing a hypothesis that is good with a certain probability (see [EHKV89] for further details). Since the uniform distribution does not have this property, this trick cannot be used for our problem. 2 These definitions are a commonly studied version of Valiant's PAC model [Val84]; study of the distribution specific version was initiated by Benedek and Itai [BI91].
Reference: [Hau91] <author> D. Haussler. </author> <title> Sphere packing numbers for subsets of the boolean n-cube with bounded Vapnik-Chervonenkis dimension. </title> <type> Technical Report UCSC-CRL-91-41, </type> <institution> University of California at Santa Cruz, </institution> <year> 1991. </year>
Reference-contexts: Benedek and Itai [BI91] proved a general lower bound of log 2 ((1 ffi)M D (F; 2*)) (3) on the sample complexity of learning F with respect to D. However, as Haussler <ref> [Hau91] </ref> has proved that in general M D (F; *) 10 VCdim (F ) ; (4) the best one could ever get by directly applying (3) is a lower bound on the sample complexity of VCdim (F ) log 1 + log (1 ffi) : However, we adapt the proof of
Reference: [HKP91] <author> J. A. Hertz, A. Krogh, and R. Palmer. </author> <title> Introduction to the theory of neural computation. </title> <publisher> Addison-Wesley, </publisher> <year> 1991. </year>
Reference-contexts: Theorem 1 m (HALF d ; UBALL d ; *; ffi) = * 1 log ffi : 3.1 The d=* term We will make use of the following bound on the "growth function" of halfspaces, due to Cover [Cov65] (see <ref> [HKP91, page 113] </ref>). Lemma 2 ([Cov65]) Choose d 2 N.
Reference: [HLW90] <author> D. Haussler, N. Littlestone, and M.K. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> December </month> <year> 1990. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: For each class of finite VC-dimension, this matched earlier upper bounds on the sample-complexity of distribution-free PAC learning <ref> [BEHW89, HLW90] </ref> to within log factors. Since the VC-dimension of homogeneous halfspaces in R d is d, (1) implies a lower bound of d + * 1 on the sample complexity of distribution-free PAC learning for this class.
Reference: [OH91] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Computational Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <pages> pages 75-87. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: The most closely related previous work that we are aware of is that of Opper and Haussler <ref> [OH91] </ref>.
Reference: [Val84] <author> L.G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: 1 Introduction In the PAC ("probably approximately correct") learning model <ref> [Val84] </ref>, the learner wishes to approximately learn an unknown function f from some domain X to f0; 1g, chosen from a known class F of such functions. <p> Since the uniform distribution does not have this property, this trick cannot be used for our problem. 2 These definitions are a commonly studied version of Valiant's PAC model <ref> [Val84] </ref>; study of the distribution specific version was initiated by Benedek and Itai [BI91]. Choose a set X, a probability distribution D over X, and a set F of functions from X to f0; 1g.
Reference: [VC71] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year>
Reference-contexts: of accuracy at least as good as *. (See Section 2 for a precise definition.) Ehrenfeucht, Haussler, Kearns and Valiant [EHKV89] proved a general lower bound of VCdim (F ) * 1 log ffi (1) on the sample complexity of distribution-free PAC learning, where VCdim (F ) is the VC-dimension <ref> [VC71] </ref> of F , a measure of its complexity (see Section 2 for a precise definition). For each class of finite VC-dimension, this matched earlier upper bounds on the sample-complexity of distribution-free PAC learning [BEHW89, HLW90] to within log factors. <p> strategies for X, for a probability distribution D on X, * &gt; 0, and ffi &gt; 0, define m (F; D; *; ffi) = minfr 2 N : 9A 2 A; 8f 2 F; Pr ~x2D r ( D (A (sample (~x; f )); f ) *) ffig: The VC-dimension <ref> [VC71] </ref> of F is defined to be maxfd : 9x 1 ; :::; x d 2 X; f (f (x 1 ); :::; f (x d )) : f 2 F g = f0; 1g d g: For d 2 N , define * HALF d to be the set of
References-found: 12

