URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/Preprints/pub41.ps.gz
Refering-URL: http://www.cs.yale.edu/HTML/YALE/CS/HyPlans/douglas-craig/ccd-preprints.html
Root-URL: http://www.cs.yale.edu
Title: Variants of matrix-matrix multiplication for Fortran-90  
Author: Craig C. Douglas and Gordon Slishman 
Keyword: Key words. BLAS, matrix multiplication, Winograd's variant of Strassen's algorithm, multilevel algorithms AMS(MOS) subject classification. Numerical Analysis: Numerical Linear Algebra  
Abstract: The Fortran-90 standard requires an intrinsic function matmul which multiplies two matrices together to produce a third as the result. However, the standard does not specify which algorithm to use. We consider an extension to the matmul syntax which allows a Winograd variant of Strassen's algorithm to be added. We discuss an implementation that is in a commercial Fortran-90 offering. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> ANSI. </author> <title> American National Standard Programming Language Fortran 90. </title> <address> New York, X3.198-1992 edition, </address> <year> 1992. </year>
Reference-contexts: 1 Introduction Fortran-90 <ref> [1] </ref> has a rich set of intrinsic functions built into it that operate on large objects such as vectors and matrices. We would like to draw attention to the fact that the algorithms that implement these functions are not usually specified in the standard.
Reference: [2] <author> C. C. Douglas and J. Douglas. </author> <title> A unified convergence theory for abstract multigrid or multilevel algorithms, serial and parallel. </title> <journal> SIAM J. Numer. Anal., </journal> <volume> 30 </volume> <pages> 136-158, </pages> <year> 1993. </year>
Reference-contexts: O. Box 218, Yorktown Heights, NY 10598. where M , K, and N are natural numbers. Strassen's method recursively works with sets of 2fi2 submatrices to form the product using 7 matrix multiplications instead of the obvious 8. This is not very different from standard multilevel methods <ref> [2] </ref> used routinely to solve partial differential equations.
Reference: [3] <author> C. C. Douglas, M. Heroux, G. Slishman, and R. M. Smith. Gemmw: </author> <title> A portable Level 3 BLAS Winograd variant of Strassen's matrix-matrix multiply algorithm. </title> <journal> J. Comput. Phys., </journal> <volume> 110 </volume> <pages> 1-10, </pages> <year> 1994. </year>
Reference-contexts: S 2 T 3 = M 5 + M 6 T 1 = M 1 + M 2 T 2 = T 1 + M 4 M 7 = A 22 S 8 C 22 = T 2 + M 5 C 11 = M 2 + M 3 (see <ref> [3] </ref>, [6], and [7]). The principal advantage of (1) is that it is an O (n 2:81 ) algorithm (for square matrices of order n), whereas the classical algorithm is O (n 3 ). The prin-cipal disadvantage is that it is not stable for certain row or column scalings [4]. <p> The dimensions of matrices are recursively split according to the rules in <ref> [3] </ref> until one of the dimensions is less than mindim. Then the classical algorithm is used to do the multiplication. <p> The elements lay in the range (0; 1). Typical speedups over the classical algorithm are the following: n Speedup 200 1.0270 400 1.1862 600 1.2099 800 1.3241 1000 1.3437 1200 1.3883 1400 1.4271 A highly portable implementation of (1) is available on netlib (gemmw in the linalg directory; see <ref> [3] </ref>). This is not the IBM implementation (which is written in Fortran-90). Gemmw is written in a combination of C, Fortran, and calls to commonly available libraries (e.g., BLAS). Common values for mindim in gemmw are 32, 64, 96, 128, 192, and 256.
Reference: [4] <author> N. J. Higham. </author> <title> Exploiting fast matrix multiplication within the Level 3 BLAS. </title> <journal> ACM Trans. Math. Soft., </journal> <volume> 16 </volume> <pages> 352-368, </pages> <year> 1990. </year>
Reference-contexts: The principal advantage of (1) is that it is an O (n 2:81 ) algorithm (for square matrices of order n), whereas the classical algorithm is O (n 3 ). The prin-cipal disadvantage is that it is not stable for certain row or column scalings <ref> [4] </ref>. A minor nuisance is that it also requires extra storage for saving some of the intermediate matrix calculations (approximately 2n 2 =3). The operation counts do not give a complete feel for the value of using (1) instead of the classical algorithm.
Reference: [5] <author> IBM. </author> <title> AIX XL Fortran Compiler/6000: Language Reference. </title> <address> North York, Ontario, </address> <note> 3, release 2 edition, </note> <year> 1994. </year>
Reference-contexts: We would like to draw attention to the fact that the algorithms that implement these functions are not usually specified in the standard. As a result of this, IBM TM recently added a Winograd variant of Strassen's algorithm in its Fortran-90 compiler, XLF 3.1.1 (the April, 1994 update <ref> [5] </ref>). This compiler is currently available on the Internet by anonymous ftp from software.watson.ibm.com in the directory pub/aix3/xlf. While the default is to use the classical algorithm, a simple change to a program results in much higher performance, though with a risk.
Reference: [6] <author> V. Strassen. </author> <title> Gaussian elimination is not optimal. </title> <journal> Numer. Math., </journal> <volume> 13 </volume> <pages> 354-356, </pages> <year> 1969. </year>
Reference-contexts: 2 T 3 = M 5 + M 6 T 1 = M 1 + M 2 T 2 = T 1 + M 4 M 7 = A 22 S 8 C 22 = T 2 + M 5 C 11 = M 2 + M 3 (see [3], <ref> [6] </ref>, and [7]). The principal advantage of (1) is that it is an O (n 2:81 ) algorithm (for square matrices of order n), whereas the classical algorithm is O (n 3 ). The prin-cipal disadvantage is that it is not stable for certain row or column scalings [4].
Reference: [7] <author> S. Winograd. </author> <title> Some remarks on fast multiplication of polynomials. </title> <editor> In J. F. Traub, editor, </editor> <booktitle> Complexity of Sequential and Parallel Numerical Algorithms, </booktitle> <pages> pages 181-196. </pages> <publisher> Academic Press, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: 3 = M 5 + M 6 T 1 = M 1 + M 2 T 2 = T 1 + M 4 M 7 = A 22 S 8 C 22 = T 2 + M 5 C 11 = M 2 + M 3 (see [3], [6], and <ref> [7] </ref>). The principal advantage of (1) is that it is an O (n 2:81 ) algorithm (for square matrices of order n), whereas the classical algorithm is O (n 3 ). The prin-cipal disadvantage is that it is not stable for certain row or column scalings [4].
References-found: 7

