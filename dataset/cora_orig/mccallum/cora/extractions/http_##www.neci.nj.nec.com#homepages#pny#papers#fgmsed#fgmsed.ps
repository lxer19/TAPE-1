URL: http://www.neci.nj.nec.com/homepages/pny/papers/fgmsed/fgmsed.ps
Refering-URL: http://www.neci.nj.nec.com/homepages/pny/papers/fgmsed/main.html
Root-URL: 
Email: research.nj.nec.com.  
Title: Finite Growth Models and the Learning of Edit Distance Costs  
Author: Eric Sven Ristad Peter N. Yianilos 
Keyword: String Edit Distance, String Similarity, Expectation Maximization (EM), Hidden Markov Mod els (HMM), Baum-Welch Reestimation, Maximum Likelihood, Stochastic Models, Metric Learning.  
Address: Email: ristad@ cs.princeton.edu.  Email: pny@  
Affiliation: Department of Computer Science, Princeton University.  NEC Research Institute; and Department of Computer Science, Princeton University.  
Date: July, 1996  
Abstract: We introduce finite growth models (FGMs) and apply them to the problem of learning optimal costs for string edit distance. We present the first algorithm that learns the optimal insertion, deletion and substitution costs from a training corpus of similar strings. Since edit distance has found widespread application, our discovery of a simple way to learn costs may lead to improved performance in many problem areas. Our approach also yields a more powerful class of learnable string edit distances. Finite growth models are acyclic stochastic automata which allow observations to be processed in many orderings and groupings not just one-by-one in sequential order. These capabilities, not present in earlier approaches, are essential to our stochastic formulation of string edit distance, and separate our approach from the related field of hidden Markov modeling. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> L. E. Baum, </author> <title> An inequality and associated maximization technique in statistical estimatation of probabilistic functions of Markov processes, Inequalities, </title> <booktitle> 3 (1972), </booktitle> <pages> pp. 1-8. </pages>
Reference-contexts: The complete dynamic program is then given by the following simple algorithm: 3 Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) t e p e ( g e (x)) This is the forward step of the Baum-Welch forward-backward algorithm <ref> [2, 3, 1] </ref>. 1 After it has run, ff n equals Pr (x). However the other ff values have meaning as well. In general ff i is the probability of arriving at v i and observing that part of x corresponding to the paths between v i and the source. <p> Like an FGM, a hidden Markov model (HMM) is a doubly stochastic finite state automaton. Since their introduction in the 1960's <ref> [2, 3, 1] </ref>, HMMs have been widely adopted in speech recognition, handwriting recognition, and computational biology.
Reference: [2] <author> L. E. Baum and J. E. Eagon, </author> <title> An inequality with application to statistical estimation for probabalistic functions of a Markov process and to models for ecology, </title> <journal> Bull. AMS, </journal> <volume> 73 (1967), </volume> <pages> pp. 360-363. </pages>
Reference-contexts: The complete dynamic program is then given by the following simple algorithm: 3 Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) t e p e ( g e (x)) This is the forward step of the Baum-Welch forward-backward algorithm <ref> [2, 3, 1] </ref>. 1 After it has run, ff n equals Pr (x). However the other ff values have meaning as well. In general ff i is the probability of arriving at v i and observing that part of x corresponding to the paths between v i and the source. <p> Like an FGM, a hidden Markov model (HMM) is a doubly stochastic finite state automaton. Since their introduction in the 1960's <ref> [2, 3, 1] </ref>, HMMs have been widely adopted in speech recognition, handwriting recognition, and computational biology.
Reference: [3] <author> L. E. Baum, T. Petrie, G. Soules, and N. Weiss, </author> <title> A maximization technique occurring in the statistical analysis of probabilistic functions of Markov chains, </title> <journal> Ann. Math Stat., </journal> <volume> 41 (1970), </volume> <pages> pp. 164-171. </pages>
Reference-contexts: The complete dynamic program is then given by the following simple algorithm: 3 Algorithm 1 procedure forward ff 1 = 1 ff i = e2I (v i ) ff s (e) t e p e ( g e (x)) This is the forward step of the Baum-Welch forward-backward algorithm <ref> [2, 3, 1] </ref>. 1 After it has run, ff n equals Pr (x). However the other ff values have meaning as well. In general ff i is the probability of arriving at v i and observing that part of x corresponding to the paths between v i and the source. <p> Like an FGM, a hidden Markov model (HMM) is a doubly stochastic finite state automaton. Since their introduction in the 1960's <ref> [2, 3, 1] </ref>, HMMs have been widely adopted in speech recognition, handwriting recognition, and computational biology.
Reference: [4] <author> T. M. Cover and J. A. Thomas, </author> <title> Elements of Information Theory, </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: This can also be seen by recognizing it as [D (k 0 ) + H ()], where D (k) is the Kullbak-Leibler distance, and H () denotes entropy (see <ref> [4] </ref>). Thus maximizing the first term, written Q (; 0 ) in the literature, will surely increase log Pr (xj 0 ) and hence Pr (xj 0 ).
Reference: [5] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum-likelihood from incomplete data via the EM algorithm, </title> <journal> J. Royal Statistical Society Ser. B (methodological), </journal> <volume> 39 (1977), </volume> <pages> pp. 1-38. 20 </pages>
Reference-contexts: Then Pr (ejx) = 1 Several years following the development of HMMs, essentially the same mathematical ideas expressed in the Baum-Welch algorithm were rediscovered as the expectation maximization (EM) algorithm for mixture densities <ref> [5] </ref>. This work focused on parameter estimation for mixture densities, and seems to have had considerable influence on a somewhat independent community. A nice survey is provided by [10].
Reference: [6] <author> P. A. V. Hall and G. R. Dowling, </author> <title> Approximate string matching, </title> <journal> Computing Surveys, </journal> <volume> 12 (1980), </volume> <pages> pp. 381-402. </pages>
Reference-contexts: In this paper, we introduce finite growth models and apply them to the problem of learning a similarity measure on strings. Our development leads to a natural reformulation of string edit distance in stochastic terms, first described by Hall and Dowling in <ref> [6] </ref>. The central advantage of stochastic reformulation is that we may adapt the classic Baum-Welch reestimation algorithm to efficiently learn the optimal insertion, deletion and substitution costs from a training corpus of similar strings. This set may be thought of as positive examples of the are similar concept. <p> The table need not be symmetric. Recall that a simple dynamic program finds the edit distance in O (jsj jtj) time. See [12] for a thorough treatment of string edit distance, or <ref> [6] </ref> for more compact discussion. 12 3.1 A Stochastic Formulation Rather than imagining an editing process that transforms s into t, we equivalently imagine that the pair (s; t) is grown in a series of steps of three kinds: joint steps, left steps, and right steps. <p> This outlook was first introduced in section 3.2.2 of <ref> [6] </ref>. Our contribution is the further development of this viewpoint to include the learning of costs, and the construction of more sophisticated distance functions as described in section 5. The initial state of the process is (;; ;), where ; denotes the null string. <p> From the cost table we may construct a choice model and observation models, completing the FGM. The algorithm is then the ff computation of algorithm 1, and was first reported as a simple recurrence in <ref> [6] </ref>. We assume without loss of generality that jsj jtj. With reference to figure 5, we choose the topological vertex ordering which starts at the source, scans columns from top to bottom, and after each proceeds to the top of the adjacent column. <p> The logarithm E of the joint probability of s and t is returned rather than the probability itself, so that the caller need not deal with extended range floating point values. As observed by <ref> [6] </ref>, the algorithm's structure resembles that of the standard dynamic program for edit distance. <p> With FGMs one may model new problems not easily dealt with in the HMM framework. In this paper we demonstrated that the well-known notion of string edit distance can be naturally cast into stochastic terms (following <ref> [6] </ref>) and expressed in the FGM framework. The string edit distance application has no natural expression in the HMM framework because observations must be generated in multiple orderings and sometimes in pairs.
Reference: [7] <author> X. D. Huang, Y. Ariki, and M. A. Jack, </author> <title> Hidden Markov Models for Speech Recognition, </title> <publisher> Edinburgh University Press, </publisher> <year> 1990. </year>
Reference-contexts: In general, any density for which a maximum-likelihood estimate is readily available, may be used as an observation model. Through the FGM graph structure, normal mixtures may be expressed, and through parameter tying the semi-continuous models of [8] are readily implemented. Both are common in speech recognition systems. See <ref> [7] </ref> for a treatment of HMMs including a compact discussion of the discrete and continuous maximum-likelihood optimization problems discussed above. Theorem 1 Let F be an FGM with parameter set , and x denote an observation. <p> They are typically applied in time-series settings when it is reasonable to assume that the observed data are generated or approximated well by the output of an automaton which changes state and generates output values stochastically. See <ref> [9, 7] </ref> for HMM overviews. In this section, we compare these two model classes and highlight three essential differences between them. 1. The first essential difference is that FGMs are acyclic, and therefore a given FGM can only generate objects containing a fixed number of components.
Reference: [8] <author> X. D. Huang and M. A. Jack, </author> <title> Unified modeling of vector quantization and hidden markov models using semi-continuous hidden markov models, </title> <booktitle> in Proc. ICASSP, </booktitle> <year> 1989, </year> <pages> pp. 639-642. </pages>
Reference-contexts: In general, any density for which a maximum-likelihood estimate is readily available, may be used as an observation model. Through the FGM graph structure, normal mixtures may be expressed, and through parameter tying the semi-continuous models of <ref> [8] </ref> are readily implemented. Both are common in speech recognition systems. See [7] for a treatment of HMMs including a compact discussion of the discrete and continuous maximum-likelihood optimization problems discussed above. Theorem 1 Let F be an FGM with parameter set , and x denote an observation.
Reference: [9] <author> A. B. Poritz, </author> <title> Hidden Markov models: a guided tour, </title> <booktitle> in Proc. </booktitle> <address> ICASSP-88, </address> <year> 1988, </year> <pages> pp. 7-13. </pages>
Reference-contexts: They are typically applied in time-series settings when it is reasonable to assume that the observed data are generated or approximated well by the output of an automaton which changes state and generates output values stochastically. See <ref> [9, 7] </ref> for HMM overviews. In this section, we compare these two model classes and highlight three essential differences between them. 1. The first essential difference is that FGMs are acyclic, and therefore a given FGM can only generate objects containing a fixed number of components.
Reference: [10] <author> R. A. Redner and H. F. Walker, </author> <title> Mixture densities, maximum likelihood, and the EM algorithm, </title> <journal> SIAM Review, </journal> <volume> 26 (1984), </volume> <pages> pp. 195-239. </pages>
Reference-contexts: This work focused on parameter estimation for mixture densities, and seems to have had considerable influence on a somewhat independent community. A nice survey is provided by <ref> [10] </ref>. Our later mathematical development will draw from the EM perspective, so in what follows we will use the phrase Baum-Welch/EM to refer to our adaptation of these ideas to the FGM setting. 2 First compute ff values without using the observation models. <p> Because FGMs are discrete state models, our lemma deals with finite mixtures, and is stated so as to fit into our development. But the lemma and its brief proof carry over easily to the continuous mixture case. See <ref> [10] </ref> pages 214-217 for general and compact development. We state and prove it here so that our treatment of FGMs is self-contained. <p> In this case the lemma can be strengthened to say that process of repeated reestimation will converge to a local maximum of the likelihood function. These comments apply to theorem 1 as well. See See <ref> [10] </ref> for a survey of the convergence properties of the EM algorithm. If one of the component densities is itself a mixture, then one may in principle expand the original mixture to absorb the submixture. But equivalently a weighted Baum-Welch/EM step may be taken for the submixture in isolation.
Reference: [11] <author> E. S. Ristad and P. N. Yianilos, </author> <title> Probability value library, </title> <type> tech. rep., </type> <institution> Princeton University, Department of Computer Science, </institution> <year> 1994. </year>
Reference-contexts: Even the final value Pr (x) can become too small to represent since the probability of generating any particular observation, in general declines exponentially with dimension. Logarithmic representation is one solution, but we instead prefer a floating point representation with extended exponent range as reported in <ref> [11] </ref>. Because the fl variables are normalized by Pr (x), they may be adequately represented using standard floating point. For any x, the fl e values must satisfy the following constraint which can in practice be used to check the algorithms above. <p> This is easy to see since all paths through this fringe are of probability zero given (s + ; t + ). The result is algorithm 4. It requires an (jsj + 1) fi 2 array ff of extended exponent range floating point values (see section 2.1 and <ref> [11] </ref>). We write ff i;j to denote an element, where 1 i jsj + 1 and j is 1 or 2. This array corresponds to the vertices within two adjancent columns of the FGM as depicted in figure 5.
Reference: [12] <author> D. Sankoff and J. B. Kruskal, </author> <title> Macromolecules: The Theory and Practice of Sequence Comparison, </title> <publisher> Addison-Wesley, </publisher> <year> 1983, 1983. </year> <month> 21 </month>
Reference-contexts: The first row gives insertion costs, the first column gives deletion costs, and the remaining entries specify substitution costs. The table need not be symmetric. Recall that a simple dynamic program finds the edit distance in O (jsj jtj) time. See <ref> [12] </ref> for a thorough treatment of string edit distance, or [6] for more compact discussion. 12 3.1 A Stochastic Formulation Rather than imagining an editing process that transforms s into t, we equivalently imagine that the pair (s; t) is grown in a series of steps of three kinds: joint steps,
References-found: 12

