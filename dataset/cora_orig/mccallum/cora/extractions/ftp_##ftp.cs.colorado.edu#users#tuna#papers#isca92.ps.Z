URL: ftp://ftp.cs.colorado.edu/users/tuna/papers/isca92.ps.Z
Refering-URL: http://www.cs.colorado.edu/~tuna/papers/index.html
Root-URL: http://www.cs.colorado.edu
Title: The Impact of Communication Locality on Large-Scale Multiprocessor Performance  
Author: Kirk L. Johnson 
Address: Cambridge, Massachusetts 02139  
Affiliation: Laboratory for Computer Science Massachusetts Institute of Technology  
Abstract: As multiprocessor sizes scale and computer architects turn to interconnection networks with non-uniform communication latencies, the lure of exploiting communication locality to increase performance becomes inevitable. Models that accurately quantify locality effects provide invaluable insight into the importance of exploiting locality as machine sizes and features change. This paper presents a framework for modeling the impact of communication locality on system performance. The framework provides a means for combining simple models of application, processor, and network behavior to obtain a combined model that accurately reflects feedback effects between processors and networks. We introduce a model that characterizes application behavior with three parameters that capture computation grain, sensitivity to communication latency, and amount of locality present at execution time. The combined model is validated with measurements taken from a detailed simulator for a complete multiprocessor system. Using the combined model, we show that exploiting communication locality provides gains which are at most linear in the factor by which average communication distance is reduced when the number of outstanding communication transactions per processor is bounded. The combined model is also used to obtain rough upper bounds on the performance improvement from exploiting locality to minimize communication distance. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Anant Agarwal. </author> <title> Limits on Interconnection Network Performance. </title> <booktitle> IEEE Transations on Parallel and Distributed Systems, </booktitle> <pages> pages 398-412, </pages> <month> October </month> <year> 1991. </year>
Reference-contexts: We believe that modifications to allow for circuit-switched networks would be straightforward. The network model used in this paper is that for packet-switched k-ary n-dimensional torus networks with separate unidirectional channels in both mesh directions presented by Agarwal in <ref> [1] </ref>. The model assumes that messages are wormhole routed according to an e-cube routing scheme [6 ]. <p> Under these circumstances, essentially random communication patterns result. For torus-connected k-ary n-dimensional interconnection networks, assuming random communication patterns and nodes that never send messages to themselves, the average distance traversed by a message, d, is given by 5 d = 4 (k n 1) See <ref> [1] </ref> or [11] for details. For this application and network, because the communication patterns of the application are the same as the network topology, an ideal mapping in which every communication requires but a single network hop is trivially obtained. Such a mapping represents the expected best case. <p> Because they assume that communication latency is proportional to communication distance, the results presented in this paper lend credence to their model. Finally, Agarwal <ref> [1] </ref> presents the network model described in Section 2. He uses the model to demonstrate that exploitation of physical locality enables networks to provide lower message latencies for a given message injection rate. <p> We present a framework for combining this model with models for communication mechanisms and interconnection networks. Behavior predicted by the combined model agrees closely with that 10 observed in detailed simulations of a complete multiprocessor sys-tem. Previous studies of interconnection network behavior (e.g. <ref> [1 ] </ref>) fail to account for the feedback between networks and applications. We show that when the number of communication transactions each multiprocessor node can have outstanding is bounded, this feedback prevents processors from loading k-ary n-dimensional mesh interconnection networks to a point where communication latencies become unbounded.
Reference: [2] <author> Anant Agarwal, David Chaiken, Kirk Johnson, David Kranz, John Kubiatowicz, Kiyoshi Kurihara, Beng-Hong Lim, Gino Maa, and Dan Nussbaum. </author> <title> The MIT Alewife Machine: A Large-Scale Distributed-Memory Multiprocessor. </title> <type> Technical Report MIT/LCS/TM-454, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> June </month> <year> 1991. </year>
Reference-contexts: The section concludes with a presentation of simulation results validating the models developed in Section 2. 3.1 The Architecture The architecture simulated in these experiments is that of the MIT Alewife machine <ref> [2] </ref>. The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a low-dimensional mesh.
Reference: [3] <author> Anant Agarwal, Beng-Hong Lim, David Kranz, and John Kubia-towicz. </author> <month> APRIL: </month> <title> A Processor Architecture for Multiprocessing. </title> <booktitle> In Proceedings of the 17th Annual International Symposium on Computer Architecture, </booktitle> <pages> pages 104-114, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The basic Alewife architecture consists of processor/memory nodes communicating over a packet-switched interconnection network organized as a low-dimensional mesh. Each processor/memory node consists of a Sparcle processor <ref> [3] </ref>, a floating-point coprocessor, several megabytes of DRAM, a 64-kilobyte unified instruction/data cache (direct mapped, 16-byte lines), and a controller that serves as both memory and network interface. In order to support block multithreading, Sparcle provides four hardware contexts. Switches between hardware contexts can be effected in 11 cycles.
Reference: [4] <author> David Chaiken, John Kubiatowicz, and Anant Agarwal. </author> <title> LimitLESS Directories: A Scalable Cache Coherence Scheme. </title> <booktitle> In Proceedings of the Fourth International Conference on Architectural Support for Programming Languages and Operating Systems (ASPLOS IV), </booktitle> <pages> pages 224-234, </pages> <month> April </month> <year> 1991. </year>
Reference-contexts: In order to support block multithreading, Sparcle provides four hardware contexts. Switches between hardware contexts can be effected in 11 cycles. In addition to providing the processor direct access to message sending and receiving facilities, the memory/network interface implements the LimitLESS protocol <ref> [4] </ref> to provide coherent caches and shared memory. Components on a processor/memory node are clocked at 33 to 40 MHz. The mesh-organized interconnection network provides two eight-bit unidirectional channels between each neighboring pair of nodes; one for each direction.
Reference: [5] <author> Suresh Chittor and Richard Enbody. </author> <title> Performance Evaluation of Mesh-Connected Wormhole-Routed Networks for Interprocessor Communication in Multicomputers. </title> <booktitle> In Proceedings of Supercomputing '90, </booktitle> <pages> pages 647-656, </pages> <month> November </month> <year> 1990. </year>
Reference-contexts: "2x faster" represents the architecture described in Section 3. reduce communication latencies when random mappings are used without changing the situation when ideal mappings are used, the impact of exploiting physical locality on end performance is lower when higher dimensional networks (n &gt; 2) are used. 5 Related Work In <ref> [5] </ref>, Chittor and Enbody present data obtained from running experiments similar to those described in Section 3 on the Ame-tek 2010, a distributed-memory, mesh-connected multiprocessor somewhat similar to that used in this paper.
Reference: [6] <author> William J. Dally. </author> <title> Performance Analysis of k-ary n-cube Interconnection Networks. </title> <journal> IEEE Transations on Computers, </journal> <pages> pages 775-785, </pages> <month> June </month> <year> 1990. </year>
Reference-contexts: The network model used in this paper is that for packet-switched k-ary n-dimensional torus networks with separate unidirectional channels in both mesh directions presented by Agarwal in [1]. The model assumes that messages are wormhole routed according to an e-cube routing scheme <ref> [6 ] </ref>. <p> A moderate amount of buffering is provided on each switch. Messages are wormhole routed according to an e-cube routing scheme <ref> [6] </ref>. All simulations were run assuming a 64-node machine organized as a two-dimensional, radix-eight torus.
Reference: [7] <author> Kirk Johnson and Anant Agarwal. </author> <title> The Impact of Communication Locality on Large-Scale Multiprocessor Performance. </title> <type> Technical Report MIT/LCS/TM-463, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> February </month> <year> 1992. </year>
Reference-contexts: Accordingly, this paper does not treat such an extension. The modeled values shown in Sections 3 and 4, however, reflect the inclusion of this factor as discussed in <ref> [7 ] </ref>. 2.5 Combined Model We obtain the combined model by using the node and network models to provide feedback to one another so that individual nodes "back off" as message latencies increase, injecting messages into the network at rates appropriate to the message latencies they actually observe. <p> By combining Equations (10), (11), and (14), one can show that as average communication distances increase, the average time it takes a message to travel a single network hop approaches a limiting value given by T h = 2n See <ref> [7] </ref> for details. Note that this limiting value depends only on average message size B, latency sensitivity s, and network dimension n.
Reference: [8] <author> Clyde P. Kruskal and Marc Snir. </author> <title> The Performance of Multistage Interconnection Networks for Multiprocessors. </title> <journal> IEEE Transactions on Computers, </journal> <pages> pages 1091-1098, </pages> <month> December </month> <year> 1983. </year>
Reference-contexts: In this paper, we use a model for packet-switched, buffered mesh networks, but the framework can easily accommodate models for other types of packet-switched networks such as that for indirect networks given in <ref> [8 ] </ref>. We believe that modifications to allow for circuit-switched networks would be straightforward. The network model used in this paper is that for packet-switched k-ary n-dimensional torus networks with separate unidirectional channels in both mesh directions presented by Agarwal in [1].
Reference: [9] <author> Dan Nussbaum and Anant Agarwal. </author> <title> Scalability of Parallel Machines. </title> <journal> Communications of the ACM, </journal> <pages> pages 56-61, </pages> <month> March </month> <year> 1991. </year>
Reference-contexts: Architectural locality represents the ability of an architecture to exploit 1 Conventional wisdom dictates that this increase is logarithmic in the number of processors in the system; proponents of "three-space realizability" arguments claim a somewhat higher rate <ref> [9] </ref>. Appears in Proceedings of the 19th Annual International Sym posium on Computer Architecture, May 1992. application locality. Two components contribute to application locality. The first, temporal locality, represents the effect of decreasing the communication frequency between application threads.
Reference: [10] <author> G. N. S. Prasanna. </author> <title> Structure Driven Multiprocessor Compilation of Numeric Problems. </title> <type> Technical Report MIT/LCS/TR-502, </type> <institution> MIT Laboratory for Computer Science, </institution> <month> April </month> <year> 1991. </year>
Reference: [11] <author> Isaac D. Scherson and Peter F. Corbett. </author> <title> Communications Overhead and the Expected Speedup of Multidimensional Mesh-Connected Parallel Processors. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <pages> pages 86-96, </pages> <month> January </month> <year> 1991. </year>
Reference-contexts: Under these circumstances, essentially random communication patterns result. For torus-connected k-ary n-dimensional interconnection networks, assuming random communication patterns and nodes that never send messages to themselves, the average distance traversed by a message, d, is given by 5 d = 4 (k n 1) See [1] or <ref> [11] </ref> for details. For this application and network, because the communication patterns of the application are the same as the network topology, an ideal mapping in which every communication requires but a single network hop is trivially obtained. Such a mapping represents the expected best case. <p> From their measurements, they extrapolate that the impact of network contention will be far more substantial as machine sizes scale. Both conclusions are well borne out by the model presented in this paper. On a different tack, Scherson and Corbett <ref> [11] </ref> introduce a framework for bounding the maximum expected speedup of different types of applications running on mesh-connected multiprocessors. Because they assume that communication latency is proportional to communication distance, the results presented in this paper lend credence to their model.
Reference: [12] <author> Michael E. Wolf and Monica S. Lam. </author> <title> A Data Locality Optimizing Algorithm. </title> <booktitle> In Proceedings of the ACM SIGPLAN '91 Conference on Programming Language Design and Implementation, </booktitle> <pages> pages 30-44, </pages> <month> June </month> <year> 1991. </year> <month> 11 </month>
Reference-contexts: This paper focuses on the impact of the latter two approaches on multiprocessor performance. Numerous researchers have demonstrated the importance of the first approach (exploiting temporal locality); compilation techniques for increasing temporal locality continue to be an active area of research <ref> [10 , 12] </ref>. 1.3 Contributions of this Paper This paper describes a framework for analyzing the impact of communication costs on end application performance for a broad class of multiprocessors.
References-found: 12

