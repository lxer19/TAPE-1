URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/rbd/www/papers/beattrack.ps
Refering-URL: http://www.cs.cmu.edu/afs/cs.cmu.edu/user/rbd/www/bib-beattrack.html
Root-URL: 
Title: Tracking Musical Beats in Real Time  
Author: Paul E. Allen and Roger B. Dannenberg 
Address: Pittsburgh, PA 15213 USA  
Affiliation: School of Computer Science Carnegie Mellon University  
Abstract: Identifying the temporal location of downbeats is a fundamental musical skill. After briefly discussing the perceptual information available in beat tracking, we survey previous attempts to automate this process in both real time and non-real time. Our attempt to add flexibility to Mont-Reynaud's model [4] by parameterizing the confidence and history mechanisms failed to yield a satisfactory beat tracker. Observing that this and previous models are constrained to hold a single current notion of beat timing and placement, we find that they will fail to predict beats and not recover beyond the point at which a mistake is first made. We propose a new model that uses beam search [1] to simultaneously consider multiple interpretations of the performance. At any time, predictions of beat timing and placement can be made according to the most credible of many interpretations under consideration. Credibility is determined by a heuristic evaluation function. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <editor> Barr, Avron and Feigenbaum, Edward A. (editors). </editor> <booktitle> The Handbook of Artificial Intelligence. </booktitle> <publisher> Addison-Wesley, </publisher> <address> Menlo Park,CA, </address> <year> 1981. </year>
Reference-contexts: If it makes a mistake at some point it most likely will never be able to recover and will fail to predict beats beyond the point at which the mistake was made. We have implemented a method that uses realtime beam search <ref> [1] </ref> to allow the beat tracker to consider several possible states at once. The method uses a history mechanism similar to the previous method but no longer relies solely on a confidence measure to guide it in making decisions. <p> Instead, a credibility measure is used so that at any given time there is a set of active states that represent the most credible interpretations for the portion of the performance encountered so far. Each state has a credibility value determined by a heuristic evaluation function <ref> [1] </ref> that measures the credibility of the particular interpretation that the state represents. The state that describes the most credible interpretation is used as the basis for making beat phase and period predictions until a new note onset is processed and a new most credible state is found.
Reference: [2] <author> Chafe, Chris; Mont-Reynaud, Bernard; and Rush, Loren. </author> <title> Toward an Intelligent Editor of Digital Audio: Recognition of Musical Constructs. </title> <journal> Computer Music Journal 6(1) </journal> <pages> 30-41, </pages> <year> 1982. </year>
Reference-contexts: Previous work in the same area [7, 11] avoided the issue altogether by assuming input consisted of ideal note durations. Chafe, Mont-Reynaud, and Rush <ref> [2] </ref> use methods similar to Longuet-Higgins' 1976 and 1978 work in a non-realtime transcription oriented task. Using multiple passes over the input, their method finds anchor points to which to attach beats. Metric analysis then finishes the placement of beats while determining meter.
Reference: [3] <author> Dannenberg, Roger B. </author> <title> An On-Line Algorithm for Real-Time Accompaniment. </title> <booktitle> In Proceedings of the 1984 International Computer Music Conference, </booktitle> <pages> pages 193-198. </pages> <year> 1984. </year>
Reference-contexts: Thus, except for review of some previous beat tracking work, we consider only realtime beat tracking for the remainder of this paper. Perceptual Information Available in Beat Tracking Beat tracking is differentiated from the score following of Dannenberg <ref> [3] </ref> and Vercoe [12] primarily by the absence of a score as part of the input to the process. The minimum input needed by a beat tracker is temporal 1 information consisting of the note onset times of a single instrument voice from which inter-onset intervals may be derived.
Reference: [4] <author> Dannenberg, Roger B. and Mont-Reynaud, Bernard. </author> <title> Following an Improvisation in Real Time. </title> <booktitle> In Proceedings of the 1987 International Computer Music Conference, </booktitle> <pages> pages 241-248. </pages> <year> 1987. </year>
Reference-contexts: Their approach is not readily amenable to realtime applications, but they have a good discussion of the timing and tempo variation in performances that needs to be overcome when attacking the beat tracking problem. As part of a realtime jazz improvisation following task, Dannenberg and Mont-Reynaud <ref> [4] </ref> describe an 2 implementation of a method for realtime beat tracking that incorporates the concept of confidence and a history mechanism that uses a weighted average of previous perceived tempos to compute the current perceived tempo. <p> Note that these transcriptions contain no higher level musical organization such as measure bars. They are simply sequences of metrical note values. An issue that must be addressed before this method is usable is that of how to establish the initial tempo. Dannenberg and Mont-Reynaud <ref> [4] </ref> describe a method of estimating the initial tempo, but currently, since we are most interested in the main beat tracking algorithm, we avoid the problem by requiring that our method be given the metrical value of the first note of the performance.
Reference: [5] <author> Dannenberg, Roger B. </author> <note> The CMU MIDI Toolkit Carnegie Mellon University, </note> <institution> Pittsburgh, </institution> <address> PA, </address> <year> 1989. </year> <note> Version 3.0. </note>
Reference-contexts: Of course this same filtering also may be used to remove from the input stream some types of ornamentations such as grace notes. Results We have implemented this new beat tracking method in C running on a Commodore Amiga using the Carnegie Mellon MIDI Toolkit <ref> [5] </ref> for support. It accepts realtime input in the form of MIDI note on/off messages from either a MIDI instrument external to the Amiga or from playback of MIDI sequences stored on the computer.
Reference: [6] <author> Desain, Peter and Honing, Henkjan. </author> <title> The Quantization of Musical Time: A Connectionist Approach. </title> <journal> Computer Music Journal 13(3) </journal> <pages> 56-66, </pages> <year> 1989. </year>
Reference-contexts: Using multiple passes over the input, their method finds anchor points to which to attach beats. Metric analysis then finishes the placement of beats while determining meter. This is an example where higher level musical analysis has been applied to beat tracking. Desain and Honing <ref> [6] </ref> report on an iterative method that uses relaxation techniques for quantizing actual note intervals into musical note values. <p> Though this example does not contain any tempo changes it does contain a few triplets and is an example of the precision (or lack thereof) of actual recorded input. Another specific example of a short performance of this class that our method handles well is given in <ref> [6] </ref> as Figure 6. In this example, the flexibility inherent in the model to deal with imprecisely played note values is exhibited.
Reference: [7] <author> Longuet-Higgins, H. C. and Steedman, M. J. </author> <title> On Interpreting Bach. </title> <editor> In Meltzer, B. and Michie, D. (editors), </editor> <booktitle> Machine Intelligence 6, </booktitle> <pages> pages 221-241. </pages> <publisher> Edinburgh University Press, Edinburgh, </publisher> <year> 1971. </year>
Reference-contexts: Related work by Longuet-Higgins and Lee [10] on perceiving rhythm addressed the problem of using actual performance input by basing their model of metrical inferences on relative note durations. Previous work in the same area <ref> [7, 11] </ref> avoided the issue altogether by assuming input consisted of ideal note durations. Chafe, Mont-Reynaud, and Rush [2] use methods similar to Longuet-Higgins' 1976 and 1978 work in a non-realtime transcription oriented task. Using multiple passes over the input, their method finds anchor points to which to attach beats.
Reference: [8] <author> Longuet-Higgins, H. C. </author> <title> Perception of Melodies. </title> <booktitle> Nature 263 </booktitle> <pages> 646-653, </pages> <year> 1976. </year>
Reference-contexts: Previous Research Much of the previous work on beat tracking has come about as a byproduct of research directed at other music understanding problems such as transcription and the perception of rhythm, meter, and melody. Longuet-Higgins <ref> [8, 9] </ref> describes what is probably the first attempt to follow the beat of an actual performance in the face of changing tempo. His simple, but fairly powerful, model is based on looking for note onsets occurring close to expected beats. <p> Of course, not all music fits these rules, so we designed the system so that rules can be selectively disabled. Although not intentionally derived as such, these rules form a context sensitive grammar for allowable meter similar in spirit to the context free grammars for meter used by Longuet-Higgins <ref> [8, 9, 10] </ref>. The second pruning technique we use involves the credibility of the state and the order in which expansions are made. <p> There is enough time to do an order of magnitude more processing on each input, but these numbers prove to be usable most of the time. A specific example of a performance from this class where this model performs well may be found in <ref> [8] </ref> as Figure 9a. Though this example does not contain any tempo changes it does contain a few triplets and is an example of the precision (or lack thereof) of actual recorded input. <p> What often results is unmusical beat tracking performance as shown previously in Figure 5. Even with these problems, the model is often still usable when applied to performances from the second category. This is shown by the ability to perform well for the example given in <ref> [8] </ref> as values for the model used for this example are the same as previously mentioned except that the triplet penalty is 0.0 and the short note penalty is 0.1.
Reference: [9] <author> Longuet-Higgins, H. C. </author> <title> The Perception of Music. </title> <booktitle> Interdisciplinary Science Reviews 3(2) </booktitle> <pages> 148-156, </pages> <year> 1978. </year>
Reference-contexts: Previous Research Much of the previous work on beat tracking has come about as a byproduct of research directed at other music understanding problems such as transcription and the perception of rhythm, meter, and melody. Longuet-Higgins <ref> [8, 9] </ref> describes what is probably the first attempt to follow the beat of an actual performance in the face of changing tempo. His simple, but fairly powerful, model is based on looking for note onsets occurring close to expected beats. <p> Of course, not all music fits these rules, so we designed the system so that rules can be selectively disabled. Although not intentionally derived as such, these rules form a context sensitive grammar for allowable meter similar in spirit to the context free grammars for meter used by Longuet-Higgins <ref> [8, 9, 10] </ref>. The second pruning technique we use involves the credibility of the state and the order in which expansions are made.
Reference: [10] <author> Longuet-Higgins, H. C. and Lee, C. S. </author> <title> The Perception of Musical Rhythms. </title> <booktitle> Perception 11 </booktitle> <pages> 115-128, </pages> <year> 1982. </year>
Reference-contexts: Longuet-Higgins found that a value of one to two tenths of a second for the tolerance, e, worked k+1 well. This work is part of a non-realtime computational psychological model of the perception of Western classical melody proposed by Longuet-Higgins. Related work by Longuet-Higgins and Lee <ref> [10] </ref> on perceiving rhythm addressed the problem of using actual performance input by basing their model of metrical inferences on relative note durations. Previous work in the same area [7, 11] avoided the issue altogether by assuming input consisted of ideal note durations. <p> Of course, not all music fits these rules, so we designed the system so that rules can be selectively disabled. Although not intentionally derived as such, these rules form a context sensitive grammar for allowable meter similar in spirit to the context free grammars for meter used by Longuet-Higgins <ref> [8, 9, 10] </ref>. The second pruning technique we use involves the credibility of the state and the order in which expansions are made.
Reference: [11] <author> Steedman, Mark J. </author> <title> The Perception of Musical Rhythm and Metre. </title> <booktitle> Perception 6 </booktitle> <pages> 555-569, </pages> <year> 1977. </year>
Reference-contexts: Related work by Longuet-Higgins and Lee [10] on perceiving rhythm addressed the problem of using actual performance input by basing their model of metrical inferences on relative note durations. Previous work in the same area <ref> [7, 11] </ref> avoided the issue altogether by assuming input consisted of ideal note durations. Chafe, Mont-Reynaud, and Rush [2] use methods similar to Longuet-Higgins' 1976 and 1978 work in a non-realtime transcription oriented task. Using multiple passes over the input, their method finds anchor points to which to attach beats.
Reference: [12] <author> Vercoe, Barry and Puckette, Miller. </author> <title> Synthetic Rehearsal: Training the Synthetic Performer. </title> <booktitle> In Proceedings of the 1985 International Computer Music Conference, </booktitle> <pages> pages 275-278. </pages> <year> 1985. </year> <pages> xi </pages>
Reference-contexts: Thus, except for review of some previous beat tracking work, we consider only realtime beat tracking for the remainder of this paper. Perceptual Information Available in Beat Tracking Beat tracking is differentiated from the score following of Dannenberg [3] and Vercoe <ref> [12] </ref> primarily by the absence of a score as part of the input to the process. The minimum input needed by a beat tracker is temporal 1 information consisting of the note onset times of a single instrument voice from which inter-onset intervals may be derived.
References-found: 12

