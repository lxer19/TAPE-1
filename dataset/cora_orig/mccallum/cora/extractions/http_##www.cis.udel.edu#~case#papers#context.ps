URL: http://www.cis.udel.edu/~case/papers/context.ps
Refering-URL: http://www.cis.udel.edu/~case/colt.html
Root-URL: http://www.cis.udel.edu
Email: Email: case@cis.udel.edu  Email: sanjay@iscs.nus.edu.sg.  Email: m ott@ira.uka.de.  Email: arun@cse.unsw.edu.au.  Email: fstephan@math.uni-heidelberg.de.  
Title: Learning From Context Without Coding-Tricks  
Author: John Case Sanjay Jain Matthias Ott Arun Sharma Frank Stephan 
Note: Supported by the Deutsche Forschungsgemeinschaft (DFG) Graduiertenkolleg "Be-herrschbarkeit komplexer Systeme" (GRK 209/2-96).  Supported by Australian Research Council Grant A49600456.  Supported by the Deutsche Forschungsgemeinschaft (DFG) Grant Am 60/9-1.  
Address: Singapore  Heidelberg  19716, USA,  119260, Republic of Singapore,  76128 Karlsruhe, Germany,  Sydney 2052, Australia,  Heidelberg, Im Neuenheimer Feld 294, 69120 Heidelberg, Germany,  
Affiliation: University of Delaware  National University of  Universitat Karlsruhe  University of New South Wales  Universitat  Department of CIS, University of Delaware, Newark, DE  Department of Information Systems and Computer Science, National University of Singapore, Singapore  Institut fur Logik, Komplexitat und Deduktionssysteme, Universitat Karlsruhe,  School of Computer Science and Engineering, University of New South Wales,  -Mathematisches Institut, Universitat  
Abstract: Empirical studies of multitask learning provide some evidence that the performance of a learning system on its intended targets improves by presenting to the learning system additional related tasks, also called contexts, as additional input. Angluin, Gasarch, and Smith, as well as Kinber, Smith, Velauthapillai, and Wiehagen have provided mathematical justification for this phenomenon in the inductive inference framework. However, their proofs rely heavily on self-referential coding tricks, that is, they directly code the solution of the learning problem into the context. In this work we prove, in the inductive inference setting, that multitask learning is extremely powerful even without using obvious coding tricks. Coding tricks are avoided in the powerful sense of Fulk's notion of robust learning. Also, studied is the difficulty of the functional dependence between the intended target tasks and useful associated contexts. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> D. Angluin, W. Gasarch, and C. Smith. </author> <title> Training sequences. </title> <journal> Theoretical Computer Science, </journal> <volume> 66(3) </volume> <pages> 255-272, </pages> <year> 1989. </year>
Reference-contexts: Other examples where multitask learning has successfully been applied to real world problems appear in [37, 33, 30, 14]. Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in <ref> [1] </ref>. 1 More technical theoretical work appears in [23, 26]. The theoretical papers, [1, 23], provide theorems witnessing situations in which learnability absolutely (not just empirically) passes from impossible to possible in the presence of suitable auxiliary contexts to be learned. <p> Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26]. The theoretical papers, <ref> [1, 23] </ref>, provide theorems witnessing situations in which learnability absolutely (not just empirically) passes from impossible to possible in the presence of suitable auxiliary contexts to be learned. <p> We see, then, situations in which it is useful to learn additional task (s) simultaneously (i.e., in parallel) and situations in which it is useful to have learned (or to be given how to do) some task (s) previously (i.e., in some sequence). <ref> [1] </ref> also theoretically exhibits cases for sequential dependence and as well as the above mentioned cases for parallel dependence. [38] theoretically exhibits cases of arbitrary dependence: diagraph dependence. 1 putable f j f (0) is the numerical name for a program for f g.
Reference: [2] <author> D. Angluin, W. I. Gasarch, and C. H. Smith. </author> <title> Training sequences. </title> <journal> Theoretical Computer Science, </journal> <volume> 66(3) </volume> <pages> 25-272, </pages> <year> 1989. </year>
Reference-contexts: This model is similar to the parallel learning model of Angluin, Gasarch, and Smith <ref> [2] </ref> where they require the learner to output also a program for the context g. Our Theorem 4.5 in Section 4 is a robust version of their [2, Theorem 6]. <p> This model is similar to the parallel learning model of Angluin, Gasarch, and Smith [2] where they require the learner to output also a program for the context g. Our Theorem 4.5 in Section 4 is a robust version of their <ref> [2, Theorem 6] </ref>. The above model of ConEx-identification may be viewed as a bit too strong since it requires a learner to be successful on each target with every context. In practice, it may be possible to determine a single suitable context for each target. <p> The concept ConEx is related to the notion of parallel learning studied by Angluin, Gasarch and Smith <ref> [2] </ref>. The main difference being that in the learning type from [2], the learning machine was required to infer programs for both input functions, not just the target. In Theorem 4.5 we will also investigate the robust version of parallel learning as defined in [2], and present a "coding free" proof <p> The concept ConEx is related to the notion of parallel learning studied by Angluin, Gasarch and Smith <ref> [2] </ref>. The main difference being that in the learning type from [2], the learning machine was required to infer programs for both input functions, not just the target. In Theorem 4.5 we will also investigate the robust version of parallel learning as defined in [2], and present a "coding free" proof for one of the results from [2]. <p> studied by Angluin, Gasarch and Smith <ref> [2] </ref>. The main difference being that in the learning type from [2], the learning machine was required to infer programs for both input functions, not just the target. In Theorem 4.5 we will also investigate the robust version of parallel learning as defined in [2], and present a "coding free" proof for one of the results from [2]. <p> the learning type from <ref> [2] </ref>, the learning machine was required to infer programs for both input functions, not just the target. In Theorem 4.5 we will also investigate the robust version of parallel learning as defined in [2], and present a "coding free" proof for one of the results from [2]. <p> ParEx coincides exactly with the 2-ary parallel learning type as defined in <ref> [2] </ref>. Analogously to the other robust variants, we let RobParEx contain all classes P 2 S 2 such that ffi (f ); fi (g) j (f; g) 2 P g 2 ParEx for all general computable operators fi. <p> Analogously to the other robust variants, we let RobParEx contain all classes P 2 S 2 such that ffi (f ); fi (g) j (f; g) 2 P g 2 ParEx for all general computable operators fi. Thus, the following theorem provides a surprising robust version of <ref> [2, Theorem 6] </ref>. Theorem 4.5 There exists a class P R fi R such that P j 1 = P j 2 = R, but P 2 RobParEx.
Reference: [3] <author> G. Baliga and J. </author> <title> Case. Learning with higher order additional information. </title> <editor> In K. Jantke and S. Arikawa, editors, </editor> <booktitle> Algorithmic Learning Theory, volume 872 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 64-75. </pages> <publisher> Springer-Verlag, </publisher> <address> Berlin, Reinhardsbrunn Castle, Germany, </address> <month> October </month> <year> 1994. </year>
Reference-contexts: One sees similar attempts in animal training by shaping desired behavior through a succession of approximations [21, 15] (e.g., to teach a dog to jump through a hoop of fire, first teach it to jump, then to jump through a non-fiery hoop, and then add fire). [9] and <ref> [3] </ref>, in a related vein, show how to improve learnability by the providing of information in addition to data about the desired task: algorithms already nearly right and higher-order but correct programs, respectively.
Reference: [4] <author> K. Bartlmae, S. Gutjahr, and G. Nakhaeizadeh. </author> <title> Incorporating prior knowledge about financial markets through neural multitask learning. </title> <booktitle> In Proceedings of the Fifth International Conference on Neural Networks in the Capital Markets, </booktitle> <year> 1997. </year>
Reference-contexts: For example, an experimental system to predict the value of German Daimler stock performed better when it was modified to track simultaneously the German stock-index DAX <ref> [4] </ref>. The value of the Daimler stock here is the primary or target concept and the value of the DAX|a related concept|provides useful auxiliary context.
Reference: [5] <author> J. Barzdin. </author> <title> Two theorems on the limiting synthesis of functions. In Theory of Algorithms and Programs, Latvian State University, </title> <journal> Riga, </journal> <volume> 210 </volume> <pages> 82-88, </pages> <year> 1974. </year>
Reference-contexts: To simplify our exposition, though, in the present version of the paper, we will state all our results about computable operators in terms of general computable operators. 2 sequence of programs and beyond some point in this sequence all the programs compute f <ref> [5, 10] </ref>. In Section 3 we consider essentially the model of Kinber, Smith, Velauthapillai, and Wieha-gen [24].
Reference: [6] <author> L. Blum and M. Blum. </author> <title> Toward a mathematical theory of inductive inference. </title> <journal> Information and Control, </journal> <volume> 28 </volume> <pages> 125-155, </pages> <year> 1975. </year>
Reference-contexts: A machine M Ex-identifies computable f just in case M , fed the values of f , outputs a sequence of programs eventually converging to a program for f <ref> [6, 10] </ref>. A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. <p> A machine M Bc-identifies computable f just in case M , fed the values of f , outputs a 2 And it's a very large class of computable functions. <ref> [6] </ref> essentially show that such classes contain a finite variant of each computable function! 3 In the present paper, it would take us too far afield to go into more detail about Barzdi~ns' conjecture itself. 4 These are called recursive operators in [35].
Reference: [7] <author> R. A. Caruana. </author> <title> Multitask connectionist learning. </title> <booktitle> In Proceedings of the 1993 Connectionist Models Summer School, </booktitle> <pages> pages 372-379, </pages> <year> 1993. </year>
Reference-contexts: 1 Introduction There is empirical evidence that sometimes performance of learning systems improves when they are modified to learn auxiliary, "related" tasks (called contexts) in addition to the primary tasks of interest <ref> [7, 8, 28] </ref>. For example, an experimental system to predict the value of German Daimler stock performed better when it was modified to track simultaneously the German stock-index DAX [4].
Reference: [8] <author> R. A. Caruana. </author> <title> Algorithms and applications for multitask learning. </title> <booktitle> In Proceedings 13th International Conference on Machine Learning, </booktitle> <pages> pages 87-95. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: 1 Introduction There is empirical evidence that sometimes performance of learning systems improves when they are modified to learn auxiliary, "related" tasks (called contexts) in addition to the primary tasks of interest <ref> [7, 8, 28] </ref>. For example, an experimental system to predict the value of German Daimler stock performed better when it was modified to track simultaneously the German stock-index DAX [4]. <p> The additional task of recognizing road stripes was able to improve empirically the performance of a system learning to steer a car to follow the road <ref> [8] </ref>. Other examples where multitask learning has successfully been applied to real world problems appear in [37, 33, 30, 14].
Reference: [9] <author> J. Case, S. Kaufmann, E. Kinber, and M. Kummer. </author> <title> Learning recursive functions from approximations. </title> <booktitle> In Proceedings of the Second European Conference on Computational Learning Theory (EuroCOLT'95), volume 904 of Lecture Notes in Artificial Intelligence, </booktitle> <pages> pages 140-153, </pages> <month> March </month> <year> 1995. </year> <note> Journal version to appear in Journal of Computer and System Sciences (Special Issue for EuroCOLT'95). </note>
Reference-contexts: One sees similar attempts in animal training by shaping desired behavior through a succession of approximations [21, 15] (e.g., to teach a dog to jump through a hoop of fire, first teach it to jump, then to jump through a non-fiery hoop, and then add fire). <ref> [9] </ref> and [3], in a related vein, show how to improve learnability by the providing of information in addition to data about the desired task: algorithms already nearly right and higher-order but correct programs, respectively.
Reference: [10] <author> J. Case and C. Smith. </author> <title> Comparison of identification criteria for machine inductive inference. </title> <journal> Theoretical Computer Science, </journal> <volume> 25 </volume> <pages> 193-220, </pages> <year> 1983. </year>
Reference-contexts: A machine M Ex-identifies computable f just in case M , fed the values of f , outputs a sequence of programs eventually converging to a program for f <ref> [6, 10] </ref>. A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. <p> To simplify our exposition, though, in the present version of the paper, we will state all our results about computable operators in terms of general computable operators. 2 sequence of programs and beyond some point in this sequence all the programs compute f <ref> [5, 10] </ref>. In Section 3 we consider essentially the model of Kinber, Smith, Velauthapillai, and Wieha-gen [24].
Reference: [11] <author> H. de Garis. </author> <title> Genetic programming: Building nanobrains with genetically programmed neural network modules. </title> <booktitle> In IJCNN: International Joint Conference on Neural Networks, </booktitle> <volume> volume 3, </volume> <pages> pages 511-516. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, New Jersey, </address> <month> June 17-21, </month> <year> 1990. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [12] <author> H. de Garis. </author> <title> Genetic programming: Modular neural evolution for darwin machines. </title> <editor> In M. Caudill, editor, </editor> <booktitle> IJCNN-90-WASH DC; International Joint Conference on Neural Networks, </booktitle> <volume> volume 1, </volume> <pages> pages 194-197. </pages> <publisher> Lawrence Erlbaum Associates, Publishers, </publisher> <address> Hillsdale, New Jersey, </address> <month> January </month> <year> 1990. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [13] <author> H. de Garis. </author> <title> Genetic programming: Building artificial nervous systems with genetically programmed neural network modules. </title> <editor> In B. Soucek and T. I. Group, editors, </editor> <booktitle> Neural and Intelligenct Systems Integeration: Fifth and Sixth Generation Integerated Reasoning Information Systems, chapter 8, </booktitle> <pages> pages 207-234. </pages> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1991. </year> <month> 11 </month>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [14] <author> T. G. Dietterich, H. Hild, and G. Bakiri. </author> <title> A comparison of ID3 and backpropogation for English text-to-speech mapping. </title> <journal> Machine Learning, </journal> <volume> 18(1) </volume> <pages> 51-80, </pages> <year> 1995. </year>
Reference-contexts: The additional task of recognizing road stripes was able to improve empirically the performance of a system learning to steer a car to follow the road [8]. Other examples where multitask learning has successfully been applied to real world problems appear in <ref> [37, 33, 30, 14] </ref>. Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26].
Reference: [15] <author> M. Domjan and B. Burkhard. </author> <title> The Principles of Learning and Behavior. </title> <publisher> Brooks/Cole Publishing Company, </publisher> <address> Pacific Grove, California, 2nd edition, </address> <year> 1986. </year>
Reference-contexts: Mastery of previous tasks provides useful context for the next. One sees similar attempts in animal training by shaping desired behavior through a succession of approximations <ref> [21, 15] </ref> (e.g., to teach a dog to jump through a hoop of fire, first teach it to jump, then to jump through a non-fiery hoop, and then add fire). [9] and [3], in a related vein, show how to improve learnability by the providing of information in addition to data
Reference: [16] <author> S. Fahlman. </author> <title> The recurrent cascade-correlation architecture. </title> <editor> In R. Lippmann, J. Moody, and D. Touretzky, editors, </editor> <booktitle> Advances in Neural Information Processing Systems 3, </booktitle> <pages> pages 190-196. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1991. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [17] <author> L. Fortnow, W. Gasarch, S. Jain, E. Kinber, M. Kummer, S. Kurtz, M. Pleszkoch, T. Sla-man, R. Solovay, and F. Stephan. </author> <title> Extremes in the degrees of inferability. </title> <journal> Annals of Pure and Applied Logic, </journal> <volume> 66 </volume> <pages> 21-276, </pages> <year> 1994. </year>
Reference-contexts: This is achieved by suitably modifying (1) in the just above proof. For example, for all non-high sets A there exist (a; a + 1)RobEx-inferable classes which are not in Ex [A] (see <ref> [17, 27] </ref>). 8 However, along the lines of [24] it follows that (b; b)Ex = Ex, in particular, (b; b)RobEx = RobEx for all b 1. Thus, it is not possible to improve Theorem 3.3 to (b; b)RobEx-learning. <p> An abundance of such A exist by <ref> [17, 27] </ref>. Theorem 5.2 Let S 2 Ex [A] Ex such that S contains all almost constant functions. Then S is SelEx-learnable as witnessed by some general A-computable operator. We now turn our attention to context mappings which are implementable by program mappings.
Reference: [18] <author> R. V. Freivalds and R. Wiehagen. </author> <title> Inductive inference with additional information. </title> <journal> Elek-tronische Informationsverarbeitung und Kybernetik, </journal> <volume> 15 </volume> <pages> 179-185, </pages> <year> 1979. </year>
Reference-contexts: (a; b)Ex is closed under union with finite sets, and (B) R, the class of all the computable functions, can be identified in the Ex sense, if one is given an upper bound on the minimal program for the input function in addition to the graph of the input function <ref> [18] </ref> (see also [22]). <p> (A) ConEx is closed under union with finite sets, and (B) R, the class of all the computable functions, can be identified in the Ex sense, if one is given an upper bound on the minimal program for the input function in addition to the graph of the input function <ref> [18] </ref> (see also [22]).
Reference: [19] <author> M. Fulk. </author> <title> Robust separations in inductive inference. </title> <booktitle> In Proceedings of the 31st Annual Symposium on Foundations of Computer Science, </booktitle> <pages> pages 405-410, </pages> <address> St. Louis, Missouri, </address> <year> 1990. </year>
Reference-contexts: The use of such coding tricks severely detracts from these otherwise interesting results. A crucial contribution of the present paper is to employ the notion of robust identification essentially from <ref> [19] </ref> to show (in Sections 3 and Section 4 below) that these sorts of existence theorems hold without the use of obvious coding tricks! In Section 5 below we present results about the problem of finding useful auxiliary contexts to enable the robust learning of classes of functions which might not <p> Hence, the simple scrambling transformation L will wreak havoc with proofs of existence results about learnability which employ SD. Barzdi~ns conjectured that learnability restricted to the avoidance of coding tricks as above would have a particularly simple form. Fulk <ref> [19] </ref> formally refuted this conjecture, but, for the present paper, we only borrow variants of some of Fulk's precise definitions. 3 Before we introduce robust identification, it is necessary to precisely define the transformations we will consider for scrambling coding tricks. <p> The scrambling transformation L above is clearly a general computable operator as is S . 5 Based on <ref> [19] </ref>, we define S R to be robustly Ex-identifiable just in case, for all general computable operators , (S) is Ex-identifiable. In the present paper we employ variants of this to overcome coding tricks in models of multi-task learning, and we will show many of our existence theorems hold robustly.
Reference: [20] <author> J. Helm. </author> <title> On effectively computable operators. </title> <journal> Zeitschrift fur Mathematische Logik und Grundlagen der Mathematik, </journal> <volume> 17 </volume> <pages> 231-244, </pages> <year> 1971. </year>
Reference-contexts: then they are equivalent to the operators computed by Turing machines where the total function argument is treated as an oracle [35]. 5 Some of our results hold for a strictly more general class of computable operators, i.e., computable operators which are merely required to map computable functions into same <ref> [20, 29] </ref>, but it is still open whether all our results do. Of course, L and S also map computable functions into same.
Reference: [21] <author> E. Hilgard and G. Bower. </author> <title> Theories of Learning. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, NJ, 4th edition, </address> <year> 1975. </year>
Reference-contexts: Mastery of previous tasks provides useful context for the next. One sees similar attempts in animal training by shaping desired behavior through a succession of approximations <ref> [21, 15] </ref> (e.g., to teach a dog to jump through a hoop of fire, first teach it to jump, then to jump through a non-fiery hoop, and then add fire). [9] and [3], in a related vein, show how to improve learnability by the providing of information in addition to data
Reference: [22] <author> S. Jain and A. Sharma. </author> <title> Learning with the knowledge of an upper bound on program size. </title> <journal> Information and Computation, </journal> <volume> 102(1) </volume> <pages> 118-166, </pages> <month> Jan. </month> <year> 1993. </year>
Reference-contexts: closed under union with finite sets, and (B) R, the class of all the computable functions, can be identified in the Ex sense, if one is given an upper bound on the minimal program for the input function in addition to the graph of the input function [18] (see also <ref> [22] </ref>). <p> closed under union with finite sets, and (B) R, the class of all the computable functions, can be identified in the Ex sense, if one is given an upper bound on the minimal program for the input function in addition to the graph of the input function [18] (see also <ref> [22] </ref>).
Reference: [23] <author> E. Kinber, C. Smith, M. Velauthapillai, and R. Wiehagen. </author> <booktitle> On learning learning multiple concepts in parallel. In Proceedings of the Workshop on Computational Learning Theory, </booktitle> <pages> pages 175-181. </pages> <publisher> ACM, </publisher> <address> NY, </address> <year> 1993. </year>
Reference-contexts: Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in <ref> [23, 26] </ref>. The theoretical papers, [1, 23], provide theorems witnessing situations in which learnability absolutely (not just empirically) passes from impossible to possible in the presence of suitable auxiliary contexts to be learned. <p> Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26]. The theoretical papers, <ref> [1, 23] </ref>, provide theorems witnessing situations in which learnability absolutely (not just empirically) passes from impossible to possible in the presence of suitable auxiliary contexts to be learned.
Reference: [24] <author> E. Kinber, C. H. Smith, M. Velauthapillai, and R. Wiehagen. </author> <booktitle> On learning multiple concepts in parallel. In Proceedings of Sixth Annual Conference on Computational Learning Theory, </booktitle> <pages> pages 175-181, </pages> <address> Santa Cruz, CA, USA, 1993. </address> <publisher> ACM Press. </publisher>
Reference-contexts: In Section 3 we consider essentially the model of Kinber, Smith, Velauthapillai, and Wieha-gen <ref> [24] </ref>. At least implicit in their model is the definition: a learner M is said to (a; b)Ex-identify a set of b pairwise distinct functions just in case M , fed graphs of the b functions simultaneously, Ex-identifies at least a of them. <p> Unfortunately, their proof of this result uses a coding trick that detracts from its strength. Our Theorem 4.4 below in Section 3 nicely provides the robust version of this result from <ref> [24] </ref>! The above model of parallel learning may be viewed as learning from arbitrary context. No distinction is made between which function is the target concept and which function provides the context. Let R R fi R be given. <p> The resulting learning notion, which we formally introduce in the next definition, has essentially already been introduced and studied by Kinber, Smith, Velauthapillai and Wiehagen <ref> [24, 25] </ref> (see also the work of Kummer and Stephan [26]). <p> For this version, it was shown in <ref> [24] </ref> that, for all a, there is a class S of functions which is not in Bc but is (a; a + 1)Ex-learnable without any mind changes. <p> More precisely, each non-empty finite subset F of S contains one function which holds programs for all other functions of F in its values. This coding can easily be destroyed by applying a computable operator on the functions in S. Therefore, the proof given in <ref> [24] </ref> cannot be applied to handle the following robust version of multitask learning with arbitrary context. Definition 3.2 S R is in (a; b)RobEx if fi (S) 2 (a; b)Ex for all general computable operators fi. <p> This is achieved by suitably modifying (1) in the just above proof. For example, for all non-high sets A there exist (a; a + 1)RobEx-inferable classes which are not in Ex [A] (see [17, 27]). 8 However, along the lines of <ref> [24] </ref> it follows that (b; b)Ex = Ex, in particular, (b; b)RobEx = RobEx for all b 1. Thus, it is not possible to improve Theorem 3.3 to (b; b)RobEx-learning.
Reference: [25] <author> E. Kinber and R. Wiehagen. </author> <title> Parallel learning a recursion-theoretic approach. </title> <type> Infomrakik-Preprint 10, </type> <institution> Fachbereich Informatik, Humbold-Universitat, </institution> <year> 1991. </year>
Reference-contexts: The resulting learning notion, which we formally introduce in the next definition, has essentially already been introduced and studied by Kinber, Smith, Velauthapillai and Wiehagen <ref> [24, 25] </ref> (see also the work of Kummer and Stephan [26]).
Reference: [26] <author> M. Kummer and F. Stephan. </author> <title> Inclusion problems in parallel learning and games. </title> <journal> Journal of Computer and System Sciences (Special Issue COLT'94), </journal> <volume> 52(3) </volume> <pages> 403-420, </pages> <year> 1996. </year>
Reference-contexts: Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in <ref> [23, 26] </ref>. The theoretical papers, [1, 23], provide theorems witnessing situations in which learnability absolutely (not just empirically) passes from impossible to possible in the presence of suitable auxiliary contexts to be learned. <p> The resulting learning notion, which we formally introduce in the next definition, has essentially already been introduced and studied by Kinber, Smith, Velauthapillai and Wiehagen [24, 25] (see also the work of Kummer and Stephan <ref> [26] </ref>).
Reference: [27] <author> M. Kummer and F. Stephan. </author> <title> On the structure of degrees of inferability. </title> <journal> Journal of Computer and System Sciences, </journal> <volume> 52(2) </volume> <pages> 214-238, </pages> <month> Apr. </month> <year> 1996. </year>
Reference-contexts: This is achieved by suitably modifying (1) in the just above proof. For example, for all non-high sets A there exist (a; a + 1)RobEx-inferable classes which are not in Ex [A] (see <ref> [17, 27] </ref>). 8 However, along the lines of [24] it follows that (b; b)Ex = Ex, in particular, (b; b)RobEx = RobEx for all b 1. Thus, it is not possible to improve Theorem 3.3 to (b; b)RobEx-learning. <p> An abundance of such A exist by <ref> [17, 27] </ref>. Theorem 5.2 Let S 2 Ex [A] Ex such that S contains all almost constant functions. Then S is SelEx-learnable as witnessed by some general A-computable operator. We now turn our attention to context mappings which are implementable by program mappings.
Reference: [28] <author> S. Matwin and M. </author> <title> Kubat. The role of context in concept learning. </title> <editor> In M. Kubat and G. Widmer, editors, </editor> <booktitle> Proceedings of the ICML-96 Pre-Conference Workshop on Learning in Context-Sensitive Domains, </booktitle> <address> Bari, Italy, </address> <pages> pages 1-5, </pages> <year> 1996. </year> <month> 12 </month>
Reference-contexts: 1 Introduction There is empirical evidence that sometimes performance of learning systems improves when they are modified to learn auxiliary, "related" tasks (called contexts) in addition to the primary tasks of interest <ref> [7, 8, 28] </ref>. For example, an experimental system to predict the value of German Daimler stock performed better when it was modified to track simultaneously the German stock-index DAX [4]. <p> Other examples where multitask learning has successfully been applied to real world problems appear in [37, 33, 30, 14]. Importantly, this empirical phenomena of such context sensitivity in machine learning <ref> [28] </ref> is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26].
Reference: [29] <author> A. Meyer and P. Fischer. </author> <title> Computational speed-up by effective operators. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 37 </volume> <pages> 48-68, </pages> <year> 1972. </year>
Reference-contexts: then they are equivalent to the operators computed by Turing machines where the total function argument is treated as an oracle [35]. 5 Some of our results hold for a strictly more general class of computable operators, i.e., computable operators which are merely required to map computable functions into same <ref> [20, 29] </ref>, but it is still open whether all our results do. Of course, L and S also map computable functions into same.
Reference: [30] <author> T. Mitchell, R. Caruana, D. Freitag, J. McDermott, and D. Zabowski. </author> <title> Experience with a learning, personal assistant. </title> <journal> Communications of the ACM, </journal> <volume> 37 </volume> <pages> 80-91, </pages> <year> 1994. </year>
Reference-contexts: The additional task of recognizing road stripes was able to improve empirically the performance of a system learning to steer a car to follow the road [8]. Other examples where multitask learning has successfully been applied to real world problems appear in <ref> [37, 33, 30, 14] </ref>. Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26].
Reference: [31] <author> P. Odifreddi. </author> <title> Classical Recursion Theory. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1989. </year>
Reference-contexts: However, if one wants to reduce this upper bound, the problem arises that these program mappings are generally not invariant with respect to different indices of the same function <ref> [31] </ref>. 11 And transforming an arbitrary program mapping into an invariant (or extensional) one generally requires an oracle of degree K 0 .
Reference: [32] <author> D. Osherson, M. Stob, and S. Weinstein. </author> <title> Systems that Learn. </title> <publisher> MIT Press, </publisher> <address> Cambridge, Massachusetts, </address> <year> 1986. </year>
Reference-contexts: Fin denotes the collection of all finitely learnable classes S, that is, the classes which are Ex-learnable without any mind changes. It is well known that Fin Ex Bc and R 0;1 62 Bc (see, e.g., <ref> [32] </ref>). 3 Learning From Arbitrary Contexts A very restricted form of multitask learning arises when we require that the learning machine be successful with any context from the concept class under consideration.
Reference: [33] <author> L. Pratt, J. Mostow, and C. Kamm. </author> <title> Direct transfer of learned information among neural networks. </title> <booktitle> In Proceedings of the 9th National Conference on Artificial Intelligence (AAAI-91), </booktitle> <year> 1991. </year>
Reference-contexts: The additional task of recognizing road stripes was able to improve empirically the performance of a system learning to steer a car to follow the road [8]. Other examples where multitask learning has successfully been applied to real world problems appear in <ref> [37, 33, 30, 14] </ref>. Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26].
Reference: [34] <author> H. Rogers. </author> <title> Godel numberings of partial recursive functions. </title> <journal> Journal of Symbolic Logic, </journal> <volume> 23 </volume> <pages> 331-341, </pages> <year> 1958. </year>
Reference-contexts: We are using an acceptable programming system ' 0 ; ' 1 ; : : : for the class of all partial computable functions <ref> [34, 36] </ref>. MinInd (f ) = minfe j ' e = f g is the minimal index of a partial computable function f with respect to this programming system. R denotes the set of all (total) computable functions; R 0;1 denotes the class of all f0; 1g-valued functions from R.
Reference: [35] <author> H. Rogers. </author> <title> Theory of Recursive Functions and Effective Computability. </title> <publisher> McGraw Hill, </publisher> <address> New York, 1967. </address> <publisher> Reprinted, MIT Press, </publisher> <year> 1987. </year>
Reference-contexts: it's a very large class of computable functions. [6] essentially show that such classes contain a finite variant of each computable function! 3 In the present paper, it would take us too far afield to go into more detail about Barzdi~ns' conjecture itself. 4 These are called recursive operators in <ref> [35] </ref>. If, as in most of the present paper, we are interested in feeding such operators total function inputs only, then they are equivalent to the operators computed by Turing machines where the total function argument is treated as an oracle [35]. 5 Some of our results hold for a strictly <p> conjecture itself. 4 These are called recursive operators in <ref> [35] </ref>. If, as in most of the present paper, we are interested in feeding such operators total function inputs only, then they are equivalent to the operators computed by Turing machines where the total function argument is treated as an oracle [35]. 5 Some of our results hold for a strictly more general class of computable operators, i.e., computable operators which are merely required to map computable functions into same [20, 29], but it is still open whether all our results do. <p> M makes a mind change at 6 A formal definition appears in Section 5 below. Further discussion of the equivalence to the standard topological notion of continuity may be found in <ref> [35] </ref>. 4 stage n + 1 on input f if ? 6= M (f [n]) 6= M (f [n + 1]). <p> Let C 0 be a general computable operator such that, (a) for all total f , C 0 (f ) = C (f ), and, (b) for all t , fx j C 0 (t )(x)#g is finite, and a canonical index <ref> [35] </ref> for it can be effectively determined from t . Note that such a C 0 can easily be constructed by "slowing down" C appropriately. Now the function s (t ) = maxf j (8x &lt; jj)[C 0 (t )(x) # = (x)]g is computable. <p> Section 1 above, continuous operators are essentially characterized as those for which an algorithm for passing from the graph of the input partial function to that of the output partial function may or may not have access to some non-computable oracle. 11 They are not extensional in the terminology of <ref> [35] </ref>. 10
Reference: [36] <author> J. Royer. </author> <title> A Connotational Theory of Program Structure. </title> <booktitle> Lecture Notes in Computer Science 273. </booktitle> <publisher> Springer-Verlag, </publisher> <year> 1987. </year>
Reference-contexts: We are using an acceptable programming system ' 0 ; ' 1 ; : : : for the class of all partial computable functions <ref> [34, 36] </ref>. MinInd (f ) = minfe j ' e = f g is the minimal index of a partial computable function f with respect to this programming system. R denotes the set of all (total) computable functions; R 0;1 denotes the class of all f0; 1g-valued functions from R.
Reference: [37] <author> T. J. Sejnowski and C. Rosenberg. NETtalk: </author> <title> A parallel network that learns to read aloud. </title> <type> Technical Report JHU-EECS-86-01, </type> <institution> Johns Hopkins University, </institution> <year> 1986. </year>
Reference-contexts: The additional task of recognizing road stripes was able to improve empirically the performance of a system learning to steer a car to follow the road [8]. Other examples where multitask learning has successfully been applied to real world problems appear in <ref> [37, 33, 30, 14] </ref>. Importantly, this empirical phenomena of such context sensitivity in machine learning [28] is also supported, for example, by mathematical existence theorems for this phenomena (and variants) in [1]. 1 More technical theoretical work appears in [23, 26].
Reference: [38] <author> C. Smith and H. Tu. </author> <title> Training digraphs. </title> <journal> Information Processing Letters, </journal> <volume> 53 </volume> <pages> 184-192, </pages> <year> 1995. </year>
Reference-contexts: task (s) simultaneously (i.e., in parallel) and situations in which it is useful to have learned (or to be given how to do) some task (s) previously (i.e., in some sequence). [1] also theoretically exhibits cases for sequential dependence and as well as the above mentioned cases for parallel dependence. <ref> [38] </ref> theoretically exhibits cases of arbitrary dependence: diagraph dependence. 1 putable f j f (0) is the numerical name for a program for f g.
Reference: [39] <author> S. Thrun. </author> <title> Is learning the n-th thing any easier than learning the first? In Advances in Neural Information Processing Systems, 8. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1996. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [40] <author> S. Thrun and J. Sullivan. </author> <title> Discovering structure in multiple learning tasks: The TC algorithm. </title> <booktitle> In Proceeedings of the Thirteenth International Conference on Machine Learning (ICML-96), </booktitle> <pages> pages 489-497. </pages> <publisher> Morgan Kaufmann, </publisher> <address> San Francisco, CA, </address> <year> 1996. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [41] <author> F. Tsung and G. Cottrell. </author> <title> A sequential adder using recurrent networks. </title> <booktitle> In IJCNN-89-WASHINGTON D.C.: International Joint Conference on Neural Networks, </booktitle> <volume> volume 2, </volume> <pages> pages 133-139. </pages> <publisher> IEEE Service Center, </publisher> <address> Piscataway, New Jersey, </address> <month> June 18-22, </month> <year> 1989. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [42] <author> A. Waibel. </author> <title> Connectionist glue: Modular design of neural speech systems. </title> <editor> In D. Touretzky, G. Hinton, and T. Sejnowski, editors, </editor> <booktitle> Proceeedings of the 1988 Connectionist Models Summer School, </booktitle> <pages> pages 417-425. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1989. </year>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
Reference: [43] <author> A. Waibel. </author> <title> Consonant recognition by modular construction of large phonemic time-delay neural networks. </title> <editor> In D. S. Touretzky, editor, </editor> <booktitle> Advances in Neural Information Processing Systems I, </booktitle> <pages> pages 215-223. </pages> <publisher> Morgan Kaufmann Publishers, Inc., </publisher> <address> San Mateo, California, </address> <year> 1989. </year> <month> 13 </month>
Reference-contexts: A class of functions S is Ex-identifiable just in case there is a machine that Ex-identifies each member of S. Here is a particularly simple example of a self-referential coding trick. Let SD = f com 1 In other empirical work (for example, <ref> [42, 43, 41, 11, 12, 13, 16, 40, 39] </ref>), one pre-trains on a succession of prior tasks to achieve success on a current task. Mastery of previous tasks provides useful context for the next.
References-found: 43

