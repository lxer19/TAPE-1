URL: http://www.iscs.nus.sg/~plong/papers/chars.ps
Refering-URL: 
Root-URL: 
Email: shai@cs.technion.ac.il  cesabian@dsi.unimi.it  haussler@cse.ucsc.edu  plong@cs.duke.edu  
Title: Characterizations of Learnability for Classes of f0; ng-valued Functions  
Author: Shai Ben-David Nicolo Cesa-Bianchi David Haussler Philip M. Long 
Date: February 5, 1995  
Address: Haifa 32000, Israel  Via Comelico 39, 20135 Milano, Italy  Santa Cruz, CA 95064, USA  P.O. Box 90129 Durham, NC 27708, USA  
Affiliation: Technion  Universita di Milano  University of California at Santa Cruz  Duke University  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> N. Alon. </author> <title> On the density of sets of vectors. </title> <journal> Discrete Mathematics, </journal> <volume> 24 </volume> <pages> 177-184, </pages> <year> 1983. </year>
Reference-contexts: This definition of shattering was investigated in <ref> [1, 9, 15, 2] </ref>. Unfortunately, using this extension, if n &gt; 1, the set f0; 1g m , which has 2 m elements, has dimension 0. <p> A possible direction for future work is the investigation of the relationships of the results proven here to the real-valued case. In fact certain notions of dimension, such as the Pollard's P -dimension, are naturally extended to classes of real-valued functions taking values in a bounded real interval (say <ref> [0; 1] </ref> for simplicity). A real-valued learning problem can be reduced to the multi-valued case through a discretization of the range [0; 1] into a set f0; : : : ; ng (see [8]). <p> In fact certain notions of dimension, such as the Pollard's P -dimension, are naturally extended to classes of real-valued functions taking values in a bounded real interval (say <ref> [0; 1] </ref> for simplicity). A real-valued learning problem can be reduced to the multi-valued case through a discretization of the range [0; 1] into a set f0; : : : ; ng (see [8]). The number of discrete elements into which the continuous range 18 is broken is proportional to 1 * , where * is the required bound on the error of the hypothesis. <p> On the other hand, some properties true in the discrete case are lost in the continuous one. For instance, while in the discrete case, the finiteness of the P -dimension is equivalent to the finiteness of the uniform P dimension, the class of monotone increasing functions on <ref> [0; 1] </ref> has infinite P -dimension but uniform P -dimension equal to 1. Recent results [3] show that the equivalence between learning and robust learning does not carry on to the real-valued case. Namely, there are classes of [0; 1]-valued functions which are learnable but not in a robust way. <p> of the uniform P dimension, the class of monotone increasing functions on <ref> [0; 1] </ref> has infinite P -dimension but uniform P -dimension equal to 1. Recent results [3] show that the equivalence between learning and robust learning does not carry on to the real-valued case. Namely, there are classes of [0; 1]-valued functions which are learnable but not in a robust way. Acknowledgements Part of this research was done while Nicolo Cesa-Bianchi was visiting UC Santa Cruz partially supported by the "Progetto finalizzato sistemi informatici e calcolo parallelo" of CNR under grant 91.00884.69.115.09672.
Reference: [2] <author> R.P. Anstee. </author> <title> A forbidden configuration theorem of Alon. </title> <journal> Journal of Combinatorial Theory, Series A, </journal> <volume> 47 </volume> <pages> 16-27, </pages> <year> 1988. </year>
Reference-contexts: This definition of shattering was investigated in <ref> [1, 9, 15, 2] </ref>. Unfortunately, using this extension, if n &gt; 1, the set f0; 1g m , which has 2 m elements, has dimension 0.
Reference: [3] <author> P.L. Bartlett, </author> <title> P.M. Long, and R.C. Williamson. Fat-shattering and the learnability of real-valued functions. </title> <type> Manuscript, </type> <year> 1993. </year>
Reference-contexts: For instance, while in the discrete case, the finiteness of the P -dimension is equivalent to the finiteness of the uniform P dimension, the class of monotone increasing functions on [0; 1] has infinite P -dimension but uniform P -dimension equal to 1. Recent results <ref> [3] </ref> show that the equivalence between learning and robust learning does not carry on to the real-valued case. Namely, there are classes of [0; 1]-valued functions which are learnable but not in a robust way.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M.K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the ACM, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: An example along these lines is the characterization of those classes of binary functions that are learnable in Valiant's PAC model in terms of the Vapnik-Chervonenkis dimension 1 proved by Blumer, Ehrenfeucht, Haussler, and Warmuth <ref> [4] </ref>. A natural way to extend the PAC model is to consider the learning of general multi-valued (instead of just binary) functions. <p> A further extension to Valiant's learning framework can be obtained within the more general pattern recognition model studied by Vapnik [17]. This framework, often called the "robust" or "agnostic" PAC model, was discussed in an appendix of <ref> [4] </ref> and studied in a more general setting in [7]. In the robust PAC model the learner's task is to generate (with high probability) a nearly optimal deterministic approximation of a stochastic relationship using hypotheses chosen from a given class of functions. <p> insisted that the shattered indices satisfy 1 i 1 &lt; i 2 &lt; &lt; i k m, which is perhaps the easiest way to think of this and the following definitions of shattering. 4 such a notion of dimension from being a characterization of learnability in the model studied here <ref> [4] </ref>. To define a generalization that yields bounds on jSj for sets S of a given dimension that are polynomial in m and therefore sufficiently strong for learning in our model, we look for a "translation" of multi-valued vectors into binary vectors. <p> After Vapnik [18], we will adopt a naive attitude toward measurability, assuming that every set encountered in our proofs is measurable. If one prefers, one may assume that the domain of any probability space we describe is countable, although considerably weaker assumptions, similar to those used in <ref> [4, 7] </ref>, suffice. If X is a set, P is a probability distribution over X, and f maps X to R, let E x2P [f (x)] denote the expectation of f with respect to P . <p> Then the set of all functions from X to fa 1 ; a 2 g trivially has -dimension and uniform -dimension 0. However, this class is isomorphic to the set of all f0; 1g-valued functions defined on X, which was shown in <ref> [4] </ref> to not be PAC-learnable if X is infinite. 2 Say that a family of mappings from f0; : : : ; ng to f0; 1; flg provides a characterization of learnability if and only if for any family F of f0; : : : ; ng-valued functions the learnability of <p> Proof. Choose a distinguisher . By Theorem 16, F is learnable if and only if the uniform -dimension of F is finite. By Lemma 19, this is equivalent to the finiteness of the VC-dimension of F for all 2 . By the results of <ref> [4] </ref>, for each 2 , F is learnable exactly when it has finite VC-dimension. <p> The term PAC learning sample complexity is often used to denote the slowest growing function m for which such a learning strategy exists. By generalizing results from <ref> [4] </ref>, we now prove upper bounds on the PAC learning sample complexity for the multi-valued case. <p> Now observe that the problem of learning the target function f 2 F to within accuracy * &gt; 0 reduces to the problem of identifying any f 2 F for which Pr (l 01;f = 1) *. By the results of <ref> [4] </ref>, a sample size of order as specified in formula (2) is sufficient to ensure that, with probability at least 1 ffi, any hypothesis h for which l 01;h (x; a) = 0 for all pairs (x; a) in the sample achieves this goal. 2 We can generalize Theorem 26 and
Reference: [5] <author> R.M. Dudley. </author> <title> Universal Donsker classes and metric entropy. </title> <journal> The Annals of Probability, </journal> <volume> 15(4) </volume> <pages> 1306-1326, </pages> <year> 1987. </year>
Reference-contexts: More precisely, we present a general scheme for extending the VC-dimension to classes of f0; : : : ; ng-valued functions for any positive integer n. Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension <ref> [5, 10] </ref>, Pollard's pseudo-dimension [12, 13, 7], and a generalization proposed by Vapnik (see, e.g. [18]). <p> Pollard's pseudo-dimension [13, 7] is the P -dimension, where P = f P;k : 0 &lt; k ng and P;k is defined by P;k (a) = 1 if a k 0 otherwise. * Vapnik's dimension [18] is the uniform P -dimension, where P is defined above. * The Graph dimension <ref> [5, 10] </ref> is the G -dimension, where G = f G;k : k 2 f0; : : : ; ngg and G;k is defined by G;k (a) = 1 if a = k 0 otherwise. * The Natarajan dimension [10] is the N -dimension, where N = f N;k;l : k;
Reference: [6] <author> A. Ehrenfeucht, D. Haussler, M. Kearns, and L.G. Valiant. </author> <title> A general lower bound on the number of examples needed for learning. </title> <journal> Information and Computation, </journal> <volume> 82(3) </volume> <pages> 247-251, </pages> <year> 1989. </year>
Reference: [7] <author> D. Haussler. </author> <title> Decision theoretic generalizations of the PAC model for neural net and other learning applications. </title> <journal> Information and Computation, </journal> <volume> 100 </volume> <pages> 78-150, </pages> <year> 1992. </year>
Reference-contexts: Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension [5, 10], Pollard's pseudo-dimension <ref> [12, 13, 7] </ref>, and a generalization proposed by Vapnik (see, e.g. [18]). <p> A further extension to Valiant's learning framework can be obtained within the more general pattern recognition model studied by Vapnik [17]. This framework, often called the "robust" or "agnostic" PAC model, was discussed in an appendix of [4] and studied in a more general setting in <ref> [7] </ref>. In the robust PAC model the learner's task is to generate (with high probability) a nearly optimal deterministic approximation of a stochastic relationship using hypotheses chosen from a given class of functions. <p> In Section 2.2 we will investigate some properties of this family that will prove useful for showing results about learnability. 2.1 Previously known examples Several previously defined notions of dimension correspond to particular choices of the set of mappings. Some of them are listed below. * Pollard's pseudo-dimension <ref> [13, 7] </ref> is the P -dimension, where P = f P;k : 0 &lt; k ng and P;k is defined by P;k (a) = 1 if a k 0 otherwise. * Vapnik's dimension [18] is the uniform P -dimension, where P is defined above. * The Graph dimension [5, 10] is <p> After Vapnik [18], we will adopt a naive attitude toward measurability, assuming that every set encountered in our proofs is measurable. If one prefers, one may assume that the domain of any probability space we describe is countable, although considerably weaker assumptions, similar to those used in <ref> [4, 7] </ref>, suffice. If X is a set, P is a probability distribution over X, and f maps X to R, let E x2P [f (x)] denote the expectation of f with respect to P . <p> Corollary 14 implies that (5. ) 1.). Lemma 15 and Theorem 13 imply that (1. , 4.). The implication (4. ) 3.) is an immediate consequence of the results in [19] and the implication (3. ) 5.) is a special case of <ref> [7, Lemma 1] </ref>. This completes the proof. 2 We remark that a result essentially equivalent to implication (4. ) 5.) was also shown in [11, Theorem 5.4, p. 114]. <p> This definition of learnability is a restriction of that studied in <ref> [17, 7] </ref> and we refer the interested reader to these sources for additional motivation. Using the results of previous sections we can quickly prove the following theorem. Theorem 21 F is learnable if and only if F is robustly learnable. Proof. <p> For the other direction just observe that learnability is clearly implied by robust learnability. 2 Note that the equivalence between learnability and robust learnability could have been more directly demonstrated by combining Natarajan's [10] and Haussler's <ref> [7] </ref> results. 4.2 General loss functions A more general error model than that of Section 3 can be considered. A natural choice could be a model in which certain errors are more serious than others.
Reference: [8] <author> D. Haussler and P.M. </author> <title> Long. A generalization of Sauer's lemma. </title> <type> Technical Report UCSC-CRL-90-15, </type> <institution> UC Santa Cruz, </institution> <year> 1990. </year> <month> 19 </month>
Reference-contexts: A real-valued learning problem can be reduced to the multi-valued case through a discretization of the range [0; 1] into a set f0; : : : ; ng (see <ref> [8] </ref>). The number of discrete elements into which the continuous range 18 is broken is proportional to 1 * , where * is the required bound on the error of the hypothesis.
Reference: [9] <author> M.G. Karpovsky and V.D. Milman. </author> <title> Coordinate density of sets of vectors. </title> <journal> Discrete Mathematics, </journal> <volume> 24 </volume> <pages> 177-184, </pages> <year> 1978. </year>
Reference-contexts: This definition of shattering was investigated in <ref> [1, 9, 15, 2] </ref>. Unfortunately, using this extension, if n &gt; 1, the set f0; 1g m , which has 2 m elements, has dimension 0.
Reference: [10] <author> B.K. Natarajan. </author> <title> On learning sets and functions. </title> <journal> Machine Learning, </journal> <volume> 4 </volume> <pages> 67-97, </pages> <year> 1989. </year>
Reference-contexts: A natural way to extend the PAC model is to consider the learning of general multi-valued (instead of just binary) functions. Natarajan <ref> [10] </ref> introduced a generalization of the Vapnik-Chervonenkis dimension, and showed that his notion of dimension characterized the learnability of classes of f0; :::; ng-valued functions for all fixed n &gt; 1. <p> More precisely, we present a general scheme for extending the VC-dimension to classes of f0; : : : ; ng-valued functions for any positive integer n. Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension <ref> [10] </ref>, the Graph dimension [5, 10], Pollard's pseudo-dimension [12, 13, 7], and a generalization proposed by Vapnik (see, e.g. [18]). In extending Valiant's PAC model, we assume (see also [10]) that a "target" function is chosen from a given class of multi-valued functions and the learner is to select from the <p> More precisely, we present a general scheme for extending the VC-dimension to classes of f0; : : : ; ng-valued functions for any positive integer n. Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension <ref> [5, 10] </ref>, Pollard's pseudo-dimension [12, 13, 7], and a generalization proposed by Vapnik (see, e.g. [18]). <p> Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension <ref> [10] </ref>, the Graph dimension [5, 10], Pollard's pseudo-dimension [12, 13, 7], and a generalization proposed by Vapnik (see, e.g. [18]). In extending Valiant's PAC model, we assume (see also [10]) that a "target" function is chosen from a given class of multi-valued functions and the learner is to select from the same class a function that yields a good approximation of the target. <p> Pollard's pseudo-dimension [13, 7] is the P -dimension, where P = f P;k : 0 &lt; k ng and P;k is defined by P;k (a) = 1 if a k 0 otherwise. * Vapnik's dimension [18] is the uniform P -dimension, where P is defined above. * The Graph dimension <ref> [5, 10] </ref> is the G -dimension, where G = f G;k : k 2 f0; : : : ; ngg and G;k is defined by G;k (a) = 1 if a = k 0 otherwise. * The Natarajan dimension [10] is the N -dimension, where N = f N;k;l : k; <p> P -dimension, where P is defined above. * The Graph dimension [5, 10] is the G -dimension, where G = f G;k : k 2 f0; : : : ; ngg and G;k is defined by G;k (a) = 1 if a = k 0 otherwise. * The Natarajan dimension <ref> [10] </ref> is the N -dimension, where N = f N;k;l : k; l 2 f0; : : : ; ng; k 6= lg and N;k;l is defined by N;k;l (a) = &gt; &lt; 1 if a = k * otherwise. 5 Let B be the set of all mappings from f0; <p> 2 ! d N " d B + ln 2 =) d B ln 2 d N d B y ln (ey) + ln 2 () d B ln 2 d B y + d N ln 2y d N ln 2y 3 A looser bound was proved by Natarajan <ref> [10] </ref>. <p> This definition of learnability is essentially that studied in <ref> [10] </ref>, which in turn was based on Valiant's PAC model [16]. <p> Proof. The implication (3. ) 5.) in Theorem 16 holds also in the robust learning model. For the other direction just observe that learnability is clearly implied by robust learnability. 2 Note that the equivalence between learnability and robust learnability could have been more directly demonstrated by combining Natarajan's <ref> [10] </ref> and Haussler's [7] results. 4.2 General loss functions A more general error model than that of Section 3 can be considered. A natural choice could be a model in which certain errors are more serious than others.
Reference: [11] <author> B.K. Natarajan. </author> <title> Machine Learning: A Theoretical Approach. </title> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: The implication (4. ) 3.) is an immediate consequence of the results in [19] and the implication (3. ) 5.) is a special case of [7, Lemma 1]. This completes the proof. 2 We remark that a result essentially equivalent to implication (4. ) 5.) was also shown in <ref> [11, Theorem 5.4, p. 114] </ref>. The concept of distinguisher is a kind of metacharacterization, as it characterizes those which in turn characterize learnability both through the finiteness of the -dimension and through the finiteness of the uniform -dimension. <p> The theorem is proven. 2 The proof of the "only if" part in Theorem 22 is analogous to the proof of <ref> [11, Theorem 5.6, p. 121] </ref>, where a similar statement is proven for classes of real-valued functions using the notion of a metric instead of the more general notion of loss. In Section 3 we introduced the class l 01;F of 0-1 loss functions induced by F .
Reference: [12] <author> D. Pollard. </author> <title> Convergence of Stochastic Processes. </title> <publisher> Springer Verlag, </publisher> <year> 1984. </year>
Reference-contexts: Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension [5, 10], Pollard's pseudo-dimension <ref> [12, 13, 7] </ref>, and a generalization proposed by Vapnik (see, e.g. [18]).
Reference: [13] <author> D. Pollard. </author> <title> Empirical Processes : Theory and Applications, </title> <booktitle> volume 2 of NSF-CBMS Regional Conference Series in Probability and Statistics. </booktitle> <institution> Institute of Math. Stat. and Am. Stat. Assoc., </institution> <year> 1990. </year>
Reference-contexts: Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension [5, 10], Pollard's pseudo-dimension <ref> [12, 13, 7] </ref>, and a generalization proposed by Vapnik (see, e.g. [18]). <p> In Section 2.2 we will investigate some properties of this family that will prove useful for showing results about learnability. 2.1 Previously known examples Several previously defined notions of dimension correspond to particular choices of the set of mappings. Some of them are listed below. * Pollard's pseudo-dimension <ref> [13, 7] </ref> is the P -dimension, where P = f P;k : 0 &lt; k ng and P;k is defined by P;k (a) = 1 if a k 0 otherwise. * Vapnik's dimension [18] is the uniform P -dimension, where P is defined above. * The Graph dimension [5, 10] is
Reference: [14] <author> J. Shawe-Taylor, M. Anthony, </author> <title> and R.L. Biggs. Bounding sample size with the Vapnik Chervonenkis dimension. </title> <type> Technical Report CSD-TR-618, </type> <institution> University of London, Royal Halloway and Bedford New College, </institution> <year> 1989. </year>
Reference-contexts: From Theorem 9, we may conclude that jT j ( d B e (n+1) 2 2d N ) d N . Thus, 2d N 2 d B : Using the approximation ln x xy ln (ey) (see <ref> [14] </ref>), which holds for any pair x; y of real positive numbers, we derive the following chain of implications for all y &lt; ln 2, 2 d B d B e (n + 1) 2 ! d N " d B + ln 2 =) d B ln 2 d N
Reference: [15] <author> J.M. Steele. </author> <title> Existence of submatrices with all possible columns. </title> <journal> Journal of Combina torial Theory, Series A, </journal> <volume> 24 </volume> <pages> 84-88, </pages> <year> 1978. </year>
Reference-contexts: This definition of shattering was investigated in <ref> [1, 9, 15, 2] </ref>. Unfortunately, using this extension, if n &gt; 1, the set f0; 1g m , which has 2 m elements, has dimension 0.
Reference: [16] <author> L. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-1142, </pages> <year> 1984. </year>
Reference-contexts: This definition of learnability is essentially that studied in [10], which in turn was based on Valiant's PAC model <ref> [16] </ref>.
Reference: [17] <author> V.N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer Verlag, </publisher> <year> 1982. </year>
Reference-contexts: A further extension to Valiant's learning framework can be obtained within the more general pattern recognition model studied by Vapnik <ref> [17] </ref>. This framework, often called the "robust" or "agnostic" PAC model, was discussed in an appendix of [4] and studied in a more general setting in [7]. <p> This definition of learnability is a restriction of that studied in <ref> [17, 7] </ref> and we refer the interested reader to these sources for additional motivation. Using the results of previous sections we can quickly prove the following theorem. Theorem 21 F is learnable if and only if F is robustly learnable. Proof.
Reference: [18] <author> V.N. Vapnik. </author> <title> Inductive principles of the search for empirical dependences. </title> <booktitle> In Proceedings of the 2nd Annual Workshop on Computational Learning Theory, </booktitle> <year> 1989. </year>
Reference-contexts: Our scheme defines a wide family of notions of dimension including as special cases the Natarajan dimension [10], the Graph dimension [5, 10], Pollard's pseudo-dimension [12, 13, 7], and a generalization proposed by Vapnik (see, e.g. <ref> [18] </ref>). In extending Valiant's PAC model, we assume (see also [10]) that a "target" function is chosen from a given class of multi-valued functions and the learner is to select from the same class a function that yields a good approximation of the target. <p> Some of them are listed below. * Pollard's pseudo-dimension [13, 7] is the P -dimension, where P = f P;k : 0 &lt; k ng and P;k is defined by P;k (a) = 1 if a k 0 otherwise. * Vapnik's dimension <ref> [18] </ref> is the uniform P -dimension, where P is defined above. * The Graph dimension [5, 10] is the G -dimension, where G = f G;k : k 2 f0; : : : ; ngg and G;k is defined by G;k (a) = 1 if a = k 0 otherwise. * <p> We describe a number of characterizations of learnability for classes of f0; : : : ; ng-valued functions proving, in particular, that for any distinguisher , a class is learnable if and only if its -dimension is finite. After Vapnik <ref> [18] </ref>, we will adopt a naive attitude toward measurability, assuming that every set encountered in our proofs is measurable. If one prefers, one may assume that the domain of any probability space we describe is countable, although considerably weaker assumptions, similar to those used in [4, 7], suffice.
Reference: [19] <author> V.N. Vapnik and A.Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-280, </pages> <year> 1971. </year> <month> 20 </month>
Reference-contexts: Thus, one may use whichever of these notions of dimension is most convenient for analyzing a given class of functions, and have a good estimate for all 1 Defined by Vapnik and Chervonenkis <ref> [19] </ref>. 3 of them. A further extension to Valiant's learning framework can be obtained within the more general pattern recognition model studied by Vapnik [17]. <p> F is learnable. Proof: Theorem 13 implies that (1. , 2.). Corollary 14 implies that (5. ) 1.). Lemma 15 and Theorem 13 imply that (1. , 4.). The implication (4. ) 3.) is an immediate consequence of the results in <ref> [19] </ref> and the implication (3. ) 5.) is a special case of [7, Lemma 1]. This completes the proof. 2 We remark that a result essentially equivalent to implication (4. ) 5.) was also shown in [11, Theorem 5.4, p. 114].
References-found: 19

