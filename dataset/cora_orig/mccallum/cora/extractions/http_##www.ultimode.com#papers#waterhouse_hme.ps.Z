URL: http://www.ultimode.com/papers/waterhouse_hme.ps.Z
Refering-URL: http://www.ultimode.com/stevew/papers.html
Root-URL: 
Title: CLASSIFICATION USING HIERARCHICAL MIXTURES OF EXPERTS  
Author: S.R.Waterhouse A.J.Robinson 
Date: 177-186  
Note: In Proc. 1994 IEEE Workshop on Neural Networks for Signal Processing IV pp.  
Address: Trumpington Street, Cambridge CB2 1PZ, England  
Affiliation: Cambridge University Engineering Department  
Abstract: There has recently been widespread interest in the use of multiple models for classification and regression in the statistics and neural networks communities. The Hierarchical Mixture of Experts (HME) [1] has been successful in a number of regression problems, yielding significantly faster training through the use of the Expectation Maximisation algorithm. In this paper we extend the HME to classification and results are reported for three common classification benchmark tests: Exclusive-Or, N-input Parity and Two Spirals. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> M. I. Jordan and R. A. Jacobs, </author> <title> Hierarchical Mixtures of Experts and the EM algorithm, </title> <journal> Neural Computation, </journal> <volume> vol. 6, </volume> <pages> pp. 181-214, </pages> <year> 1994. </year>
Reference-contexts: Different approaches use different techniques to divide the problem into sub-problems and to calculate the best solution to the problem from the outputs of the experts. The architecture described in this paper, the Hierarchical Mixtures of Experts (HME) <ref> [1] </ref>, employs probabilistic methods in both the way it divides the input space and the way it combines the outputs from the experts. The paper is organised as follows. <p> In the original HME each expert was linear and performed a regression task <ref> [1] </ref>. In this paper, each expert is non-linear and performs multi-way classification. Our general classification problem may be considered as follows. At time t during training, we observe an input vector x (t) which belongs to class n.
Reference: [2] <author> A. P. Dempster, N. M. Laird, and D. B. Rubin, </author> <title> Maximum likelihood from incomplete data via the EM algorithm, </title> <journal> Journal of the Royal Statistical Society, Series B, </journal> <volume> vol. 39, </volume> <pages> pp. 1-38, </pages> <month> June </month> <year> 1977. </year>
Reference-contexts: The paper is organised as follows. The HME architecture is described, along with the use of the Expectation Maximisation (EM) algorithm <ref> [2] </ref> which is used to estimate its parameters. The extension of the HME to classification is discussed, including the required modifications to the training algorithm. The results obtained on two classification simulations are presented: N-input Parity and the `Two Spirals' problem.
Reference: [3] <author> L. Breiman, J. Friedman, R. Olshen, and C. J. Stone, </author> <title> Classification and Regression Trees. </title> <editor> Wadswoth and Brooks/Cole, </editor> <year> 1984. </year>
Reference-contexts: The simplest approach is to divide the problem into sub-problems having no common elements a `hard split' of the data. The optimum output of the experts assigned to each sub-problem may then be chosen on a `winner-takes-all' (WTA) basis. Classification and Regression Trees (CART) <ref> [3] </ref> are based on this principle. Alternatively, the outputs of the experts may be combined in a weighted sum with weights derived from the performance of the experts in their partition of the input space; this is the principle behind Stacked Gener-alisation [4]. <p> In addition we have described how the use of redundancy in the HME may reduce the chance of local maxima. In these networks, the redundant experts are typically inactive after a few epochs, which suggests that they could be `pruned' using similar techniques to those developed for CART <ref> [3] </ref> . Alternatively we may start with a small network of, say 2 experts and grow the tree using CART principles.
Reference: [4] <author> D. H. Wolpert, </author> <title> Stacked generalization, </title> <type> Tech. Rep. </type> <institution> LA-UR-90-3460, The Santa Fe Institute, 1660 Old Pecos Trail, Suite A, </institution> <address> Santa Fe, NM, 87501, </address> <year> 1993. </year>
Reference-contexts: Classification and Regression Trees (CART) [3] are based on this principle. Alternatively, the outputs of the experts may be combined in a weighted sum with weights derived from the performance of the experts in their partition of the input space; this is the principle behind Stacked Gener-alisation <ref> [4] </ref>. The most advanced method is to divide the problem into sub-problems which can have common elements a `soft split' of the input space into a series of overlapping clusters. The outputs can be chosen either using WTA or stochastically.
Reference: [5] <author> J. S. Bridle, </author> <title> Probabilistic interpretation of feedforward classification network outputs, with relationships to statistical pattern recognition, in Neuro-computing: Algorithms, </title> <editor> Architectures and Applicatations (F. Fougelman-Soulie and J. Herault, </editor> <booktitle> eds.), </booktitle> <pages> pp. 227-236, </pages> <publisher> Springer-Verlag, </publisher> <year> 1989. </year>
Reference-contexts: Unlike CART, the shape of the HME network is pre-determined heuristically before training. Expert network EN (1, 1) is a single layer network with `softmax` activation function <ref> [5] </ref> whose j th output is m 11j = P (yjx, z 1 , z 11 , Q 11 ) = exp (q T , k=1 11k x) , where Q 11 is a parameter matrix, consisting of N independent vectors fq 11k g.
Reference: [6] <author> P. McCullagh and J. A. Nelder, </author> <title> Generalized Linear Models. </title> <publisher> London: Chapman and Hall, </publisher> <year> 1989. </year>
Reference-contexts: Solving the M-Step Since each EN and GN is a simple network with a single layer of weights, we may solve the maximum likelihood problems relatively easily. We update the parameter vectors for each output of the networks independently, given the Generalised Linear <ref> [6] </ref> assumption that the outputs are independent. <p> This method is equivalent to the Iteratively Reweighted Least Squares algorithm (IRLS) of Generalised Linear Models <ref> [6] </ref>. Implementation Issues Variation in M-Step Iterations. Although the basic EM algorithm dictates that the M step should be iterated until convergence, the Generalised EM algorithm (GEM) relaxes this constraint, requiring only an increase in the likelihood in the M-step.
Reference: [7] <author> S. E. Fahlman, </author> <title> Faster-learning variations on back-propagation: An empirical study, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: SIMULATIONS In this paper we follow the work of <ref> [7] </ref> in using strict methods when reporting learning speeds and network performance. In particular we use a 40-20-40 threshold criterion which dictates that an output is only correct if it is greater than 0. 6. We define an epoch as one pass through the training set. <p> HME (1:2) with a total of 9 parameters. By way of comparison, conventional feed forward networks, using a 2-2-1 structure can solve this problem at the 0. 6 threshold in an average of 19 epochs of quickprop <ref> [7] </ref>. Using the delta-bar-delta rule, an average training time of 250.4 epochs has been reported [9]. Using the HME, we reach the 0. 6 threshold in an average of 2. 76 epochs, and the 0. 99 threshold in an average of 11. 5 epochs. <p> Future directions for this work will focus on removing the need for matrix inversion by using some form of approximation to the inverse Hessian in (1). The use of fast gradient descent methods such as quickprop <ref> [7] </ref> would move the HME closer to true connectionist methods and reduce the computational load of the M-step. In addition we have described how the use of redundancy in the HME may reduce the chance of local maxima.
Reference: [8] <author> M. Minsky and S. Papert, </author> <title> Perceptrons: An Introduction to Computational Geometry. </title> <address> Cambridge, MA: </address> <publisher> MIT Press, </publisher> <year> 1969. </year>
Reference-contexts: The special case of 2-input parity is the Exclusive Or function (XOR) which was shown to be impossible for simple single layer networks to approximate <ref> [8] </ref>. In this paper we show that the HME can solve this problem efficiently using only three single layer networks, in the form of one GN and two ENs. We also describe solutions for 3 to 8 input parity, with learning times faster than conventional feed-forward networks.
Reference: [9] <author> R. A. Jacobs, </author> <title> Increased rates of convergence through learning rate adaptation, </title> <booktitle> Neural Networks, </booktitle> <volume> vol. 1, </volume> <pages> pp. 295-307, </pages> <year> 1988. </year>
Reference-contexts: By way of comparison, conventional feed forward networks, using a 2-2-1 structure can solve this problem at the 0. 6 threshold in an average of 19 epochs of quickprop [7]. Using the delta-bar-delta rule, an average training time of 250.4 epochs has been reported <ref> [9] </ref>. Using the HME, we reach the 0. 6 threshold in an average of 2. 76 epochs, and the 0. 99 threshold in an average of 11. 5 epochs. Of all these trials of the HME on the XOR problem, none failed to converge or had to be restarted.
Reference: [10] <author> G. Tesauro and R. Janssens, </author> <title> Scaling relationships in back-propagation learning: Dependence on predicate order, </title> <type> Tech. Rep. </type> <institution> CCSR-88-1, Center for Complex Systems Research, University of Illinois at Urbana Champagne, </institution> <year> 1988. </year>
Reference-contexts: The results on the N-input Parity problem are shown in Table 2. By way of comparison, the best performance reported on 8-input Parity by a 8-16-1 back-propagation network is 2000 epochs of standard back-propagation <ref> [10] </ref> and 172 epochs of quickprop. The table shows the average results over 50 trials. The number of tests which failed to converge to the correct solution is shown as % NC. Using the HME, N-input parity requires at least N experts.
Reference: [11] <author> S. E. Fahlman and C. Lebiere, </author> <title> The Cascade-Correlation learning architecture, </title> <type> Tech. Rep. </type> <institution> CMU-CS-90-100, School of Computer Science, Carnegie Mellon University, </institution> <address> Pittsburgh, PA 15213, </address> <year> 1990. </year>
Reference-contexts: The points in the test set are offset vertically from the points in the learning set by 0. 1. The best solutions to the spirals problem have been obtained using Cascade Correlation <ref> [11] </ref>. This is capable of approximating the problem in 1700 epochs using around 140 parameters.
Reference: [12] <author> K. J. Lang and M. J. Witbrock, </author> <title> Learning to tell two spirals apart, </title> <booktitle> in Proceedings of the 1988 Connectionist Models Summer School, </booktitle> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year> <title> a binary branching HME with 10 levels. </title>
Reference-contexts: The best solutions to the spirals problem have been obtained using Cascade Correlation [11]. This is capable of approximating the problem in 1700 epochs using around 140 parameters. Back-propagation networks have been used to solve the problem <ref> [12] </ref> using a 2-5-5-1 network with shortcut connections between layers in 20, 000 epochs using conventional gradient descent with momentum and 8, 000 epochs using quickprop, using a similar number of parameters. Using a conventional 2-5-5-1 network without shortcuts took 60, 000 epochs of quickprop.
References-found: 12

