URL: ftp://ftp.cs.rochester.edu/pub/u/michael/IPL96.ps.gz
Refering-URL: http://www.cs.rochester.edu/u/michael/pseudocode/IPL96-pc.html
Root-URL: 
Email: fgchunt,michael,srini,scottg@cs.rochester.edu  
Title: An Efficient Algorithm for Concurrent Priority Queue Heaps  
Author: Galen C. Hunt, Maged M. Michael, Srinivasan Parthasarathy, Michael L. Scott 
Keyword: concurrency, data structures, priority queue, heap, bit-reversal, synchronization.  
Address: Rochester, Rochester, NY 14627-0226, USA  
Affiliation: Department of Computer Science, University of  
Abstract: We present a new algorithm for concurrent access to array-based priority queue heaps. Deletions proceed top-down as they do in a previous algorithm due to Rao and Kumar [7], but insertions proceed bottom-up, and consecutive insertions use a bit-reversal technique to scatter accesses across the fringe of the tree, to reduce contention. Because insertions do not have to traverse the entire height of the tree (as they do in previous work), as many as O(M) operations can proceed in parallel, rather than O(log M ) on a heap of size M . Experimental results on a Silicon Graphics Challenge multiprocessor demonstrate good overall performance for the new algorithm on small heaps, and significant performance improvements over known alternatives on large heaps with mixed insertion/deletion workloads. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> J. Biswas and J. C. Browne. </author> <title> Simultaneous Update of Priority Structures. </title> <booktitle> In Proceedings of the 1987 International Conference on Parallel Processing, </booktitle> <pages> pages 124-131, </pages> <address> St. Charles, IL, </address> <month> Aug. </month> <year> 1987. </year>
Reference-contexts: Since updates to the heap typically modify only a small fraction of the nodes, more concurrency should be achievable by allowing processes to access the heap concurrently as long as they do not interact with each other. Biswas and Browne <ref> [1] </ref> proposed a scheme that allows many insertions and deletions to proceed concurrently. Their scheme relies on the presence of maintenance processes that dequeue sub-operations from a FIFO work queue. Sub-operations are placed on the work queue by the processes performing insert and delete operations. <p> The new algorithm avoids deadlock among concurrent accesses without forcing insertions to proceed top-down [7], or introducing a work queue and extra processes <ref> [1] </ref>. Bottom-up insertions reduce contention for the topmost nodes of the heap, and avoid the need for a full-height traversal in many cases. The new algorithm also uses bit-reversal to increase concurrency among consecutive insertions, allowing them to follow mostly-disjoint paths.
Reference: [2] <author> T. H. Cormen, C. E. Leiserson, and R. L. Rivest. </author> <title> Introduction to Algorithms. </title> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1990. </year>
Reference-contexts: 1 Introduction The heap data structure is widely used as a priority queue <ref> [2] </ref>. The basic operations on a priority queue are insert and delete. Insert inserts a new item in the queue and delete removes and returns the highest priority item from the queue. <p> It also uses a "bit-reversal" technique to scatter accesses across the fringe of the tree to reduce contention. 2 The New Algorithm The new algorithm augments the standard heap data structure <ref> [2] </ref> with a 1 Array based heaps can be considered as a binary tree that is filled at all levels except possibly the last level. <p> The insert operation is complete. If the tag of the current node is not equal to the operation's pid, then the inserted item must have been moved upwards by a delete operation. The insert operation moves upward in pursuit of the inserted item. In some definitions of heaps <ref> [2] </ref>, all nodes in the last level of the heap to the left of the last item have to be non-empty. Since this is not required by priority queue semantics, in the new algorithm we chose to relax this restriction to reduce lock contention, and thereby permit more concurrency. <p> Since this is not required by priority queue semantics, in the new algorithm we chose to relax this restriction to reduce lock contention, and thereby permit more concurrency. Under our relaxed model, consecutive insertions traverse different sub-trees by using a "bit-reversal" technique similar to that of an FFT computation <ref> [2] </ref>. For example, in the third level of a heap (nodes 8-15, where node 1 is the root), eight consecutive insertions would start from the nodes 8, 12, 10, 14, 9, 13, 11, and 15, respectively.
Reference: [3] <author> D. W. Jones. </author> <title> Concurrent Operations on Priority Queues. </title> <journal> Comm. of the ACM, </journal> <volume> 32(1) </volume> <pages> 132-137, </pages> <month> Jan. </month> <year> 1989. </year>
Reference-contexts: Rao and Kumar [7] present another scheme that avoids deadlock by using top-down insertions, where an inserted item has to traverse a path through the whole height of the heap (insertions in a traditional sequential heap proceed bottom-up). Jones <ref> [3] </ref> presents a concurrent priority queue algorithm using skew heaps. 1 He notes that top-down insertions in array-based heaps are inefficient, while bottom-up insertions would cause deadlock if they collide with top-down deletions without using extra server processes.
Reference: [4] <author> J. M. Mellor-Crummey and M. L. Scott. </author> <title> Algorithms for Scalable Synchronization on Shared-Memory Multiprocessors. </title> <journal> ACM Transactions on Computer Systems, </journal> <volume> 9(1) </volume> <pages> 21-65, </pages> <month> Feb. </month> <year> 1991. </year>
Reference-contexts: For mutual exclusion we used test-and-test-and-set locks with backoff using the MIPS R4000 load-linked and store-conditional instructions. On small-scale multiprocessors like the Challenge, these locks have low overhead compared to other more scalable locks. <ref> [4] </ref>. To evaluate the performance of the algorithms under different levels of contention, we varied the number of processes in our experiments. Each process runs on a dedicated processor in a tight loop that repeatedly updates a shared heap.
Reference: [5] <author> J. Mohan. </author> <title> Experience with Two Parallel Programs Solving the Travelling Salesman Problem. </title> <booktitle> In Proceedings of the 1983 International Conference on Parallel Processing, </booktitle> <pages> pages 191-193, </pages> <year> 1983. </year>
Reference: [6] <author> M. J. Quinn and N. Deo. </author> <title> Parallel Graph Algorithms. </title> <journal> ACM Computing Surveys, </journal> <volume> 16(3) </volume> <pages> 319-348, </pages> <month> Sept. </month> <year> 1984. </year>
Reference: [7] <author> V. N. Rao and V. Kumar. </author> <title> Concurrent Access of Priority Queues. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 37(12) </volume> <pages> 1657-1665, </pages> <month> Dec. </month> <year> 1988. </year> <month> 11 </month>
Reference-contexts: The work queue is used to avoid deadlock due to insertions and deletions proceeding in opposite directions in the tree. The need for a work queue and maintenance processes causes this scheme to incur substantial overhead. Rao and Kumar <ref> [7] </ref> present another scheme that avoids deadlock by using top-down insertions, where an inserted item has to traverse a path through the whole height of the heap (insertions in a traditional sequential heap proceed bottom-up). <p> The new algorithm avoids deadlock among concurrent accesses without forcing insertions to proceed top-down <ref> [7] </ref>, or introducing a work queue and extra processes [1]. Bottom-up insertions reduce contention for the topmost nodes of the heap, and avoid the need for a full-height traversal in many cases. The new algorithm also uses bit-reversal to increase concurrency among consecutive insertions, allowing them to follow mostly-disjoint paths. <p> The new algorithm also uses bit-reversal to increase concurrency among consecutive insertions, allowing them to follow mostly-disjoint paths. Empirical results, comparing the new algorithm, the single-lock algorithm, and Rao and Kumar's top-down insertion algorithm <ref> [7] </ref> on an SGI Challenge, show that the new algorithm provides reasonable performance on small heaps, and significantly superior performance on large heaps under high levels of contention. 10 Acknowledgments We thank Greg Andrews and the anonymous referees for their useful comments that improved both the quality and conciseness of this
References-found: 7

