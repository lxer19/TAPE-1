URL: http://www.cs.umd.edu/~flip/sigmod.ps.gz
Refering-URL: http://www.cs.umd.edu/~flip/resume.html
Root-URL: 
Email: flip@cs.umd.edu  jag@research.att.com  christos@cs.umd.edu  
Title: Efficiently Supporting Ad Hoc Queries in Large Datasets of Time Sequences  
Author: Flip Korn H. V. Jagadish Christos Faloutsos 
Address: College Park, MD 20742  Florham Park, NJ 07932  College Park, MD 20742  
Affiliation: Dept. of Computer Science University of Maryland  AT&T Laboratories  Dept. of Computer Science and Inst. for Systems Research University of Maryland  
Abstract: Ad hoc querying is difficult on very large datasets, since it is usually not possible to have the entire dataset on disk. While compression can be used to decrease the size of the dataset, compressed data is notoriously difficult to index or access. In this paper we consider a very large dataset comprising multiple distinct time sequences. Each point in the sequence is a numerical value. We show how to compress such a dataset into a format that supports ad hoc querying, provided that a small error can be tolerated when the data is uncompressed. Experiments on large, real world datasets (AT&T customer calling patterns) show that the proposed method achieves an average of less than 5% error in any data value after compressing to a mere 2.5% of the original space (i.e., a 40:1 compression ratio), with these numbers not very sensitive to dataset size. Experiments on aggregate queries achieved a 0.5% reconstruction error with a space requirement under 2%. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> Rakesh Agrawal, Christos Faloutsos, and Arun Swami. </author> <title> Efficient similarity search in sequence databases. </title> <booktitle> In Fourth Int. Conf. on Foundations of Data Organization and Algorithms (FODO), </booktitle> <pages> pages 69-84, </pages> <address> Evanston, Illinois, </address> <month> October </month> <year> 1993. </year> <note> also available through anonymous ftp, from olympos.cs.umd.edu: ftp/pub/TechReports/fodo.ps. </note>
Reference-contexts: In many practical signals, it is the case that most of the "energy" (or "information") is concentrated in the first few Fourier coefficients [21]. One can then throw away the remaining coefficients. This effect has also been observed in the data mining context <ref> [1] </ref>. The DFT and other associated methods (e.g., DCT, DWT) are all linear transformations, which effectively consider an M -long time sequence as a point in M -d space, and rotate the axes.
Reference: [2] <author> Richard A. Becker, John M. Chambers, and Al-lan R. Wilks. </author> <title> The New S Language. Wadsworth & Brooks/Cole Advanced Books & Software, </title> <address> Pacific Grove, CA, </address> <year> 1988. </year>
Reference-contexts: However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. In our experiments we used an off-the-shelf clustering method from the `S' statistical package <ref> [2] </ref>. The method is quadratic on the number of records N , and it builds a cluster-hierarchy, which we truncate at the appropriate levels, to obtain the desirable number of clusters.
Reference: [3] <author> R.O. Duda and P.E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1973. </year>
Reference-contexts: This application of clustering is known in the signal processing literature as vector quantization [16]. Clustering has attracted tremendous interest, from diverse fields and for diverse applications: in information retrieval for grouping together documents represented as vectors [20]; in pattern matching, for grouping together samples of the training set <ref> [3] </ref>; in the social and natural sciences for statistical analysis [9]. Excellent surveys on clustering include [18, 13, 26]. Although useful in numerous applications, in our setting clustering might not scale-up. <p> from matrix algebra. a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [11]), text retrieval under the name of Latent Semantic Indexing [4], pattern recognition and dimensionality reduction as the Karhunen-Loeve (KL) transform <ref> [3] </ref>, and face recognition [25]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [24] or [15] for more details.
Reference: [4] <author> Susan T. Dumais. </author> <title> Latent semantic indexing (lsi) and trec-2. </title> <editor> In D. K. Harman, editor, </editor> <booktitle> The Second Text Retrieval Conference (TREC-2), </booktitle> <pages> pages 105-115, </pages> <address> Gaithersburg, MD, </address> <month> March </month> <year> 1994. </year> <note> NIST. Special publication 500-215. </note>
Reference-contexts: after compression, % of original Table 2: Symbols, definitions and notation from matrix algebra. a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [11]), text retrieval under the name of Latent Semantic Indexing <ref> [4] </ref>, pattern recognition and dimensionality reduction as the Karhunen-Loeve (KL) transform [3], and face recognition [25]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [24] or [15] for more details.
Reference: [5] <author> Susan Eggers and Arie Shoshani. </author> <title> Efficient access of compressed data. </title> <booktitle> In Proceedings of the 6th VLDB Conference, </booktitle> <volume> volume 6, </volume> <pages> pages 205-211, </pages> <year> 1980. </year>
Reference-contexts: No previous work, to our knowledge, has addressed the problem we study in this paper, even though work on data compression abounds. Some interesting work has been done on compression with fast searching in a large database of bit vectors <ref> [12, 5] </ref>. Our work is different because our focus is on a dataset of real-valued numbers rather than bit vectors. Well-designed index structures are necessary to support ad hoc queries. There has been much work on index structures, including some excellent recent work specifically aimed at decision support [8, 10].
Reference: [6] <author> Martin Ester, Hans-Peter Kriegel, and Xiaowei Xu. </author> <title> Knowledge discovery in large spatial databases: Focusing techniques for efficient class identification. </title> <booktitle> Proc. of 4th International Symposium on Large Spatial Databases, </booktitle> <year> 1995. </year>
Reference-contexts: Faster, approximate algorithms include the popular "k-means" algorithm [17], which requires a constant, but large number of passes over the dataset, thus becoming impractical for the huge datasets we have in mind. Recent fast clustering algorithms for huge databases include CLARANS [14], BIRCH [28], and CLUDIS <ref> [6] </ref>. However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. <p> Recent fast clustering algorithms for huge databases include CLARANS [14], BIRCH [28], and CLUDIS <ref> [6] </ref>. However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. In our experiments we used an off-the-shelf clustering method from the `S' statistical package [2].
Reference: [7] <author> Christos Faloutsos. </author> <title> Searching Multimedia Databases by Content. </title> <publisher> Kluwer Academic Inc., </publisher> <year> 1996. </year> <note> ISBN 0 7923-9777-0. </note>
Reference-contexts: In our experiments, we use DCT as representative of the spectral methods because it is very close to optimal when the data is correlated <ref> [7, p. 109] </ref>, as is the case in our datasets. 3 Introduction to SVD The proposed method is based on the so-called Singular Value Decomposition (SVD) of the data matrix. <p> Moreover, the columns of the V matrix are the eigenvectors of the C matrix. C = V fi fl fi V (6) Proof: See <ref> [7] </ref>. 2 The intuitive meaning of the M fiM matrix C = X t fiX is that it gives the column-to-column similarities.
Reference: [8] <author> J. Gray, A. Bosworth, A. Layman, and H. Pirahesh. </author> <title> Data cube: a relational aggregation operator generalizing group-by, cross-tab, and sub-totals. </title> <type> Technical Report No. </type> <institution> MSR-TR-95-22, Microsoft, </institution> <year> 1995. </year>
Reference-contexts: Our work is different because our focus is on a dataset of real-valued numbers rather than bit vectors. Well-designed index structures are necessary to support ad hoc queries. There has been much work on index structures, including some excellent recent work specifically aimed at decision support <ref> [8, 10] </ref>. However, the design of indices is not the focus of this paper.
Reference: [9] <author> John A. Hartigan. </author> <title> Clustering Algorithms. </title> <publisher> John Wiley & Sons, </publisher> <year> 1975. </year>
Reference-contexts: Clustering has attracted tremendous interest, from diverse fields and for diverse applications: in information retrieval for grouping together documents represented as vectors [20]; in pattern matching, for grouping together samples of the training set [3]; in the social and natural sciences for statistical analysis <ref> [9] </ref>. Excellent surveys on clustering include [18, 13, 26]. Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters [26], are typically O (N 2 ) or O (N log N ).
Reference: [10] <author> Ted Johnson and Dennis Shasha. </author> <title> Hierarchical split cube forests for decision support. </title> <type> Technical report, Draft, </type> <month> September </month> <year> 1996. </year>
Reference-contexts: Our work is different because our focus is on a dataset of real-valued numbers rather than bit vectors. Well-designed index structures are necessary to support ad hoc queries. There has been much work on index structures, including some excellent recent work specifically aimed at decision support <ref> [8, 10] </ref>. However, the design of indices is not the focus of this paper.
Reference: [11] <author> I.T. Jolliffe. </author> <title> Principal Component Analysis. </title> <publisher> Springer Verlag, </publisher> <year> 1986. </year>
Reference-contexts: SP E normalized root mean squared error s% disk space after compression, % of original Table 2: Symbols, definitions and notation from matrix algebra. a popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis <ref> [11] </ref>), text retrieval under the name of Latent Semantic Indexing [4], pattern recognition and dimensionality reduction as the Karhunen-Loeve (KL) transform [3], and face recognition [25]. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. <p> Also, recall that the rank of a matrix is the highest number of linearly independent rows (or columns). Eq. 3 equivalently states that a matrix X can be brought in the following form, the so-called spectral decomposition <ref> [11, p. 11] </ref>: X = 1 u 1 fi v 1 + 2 u 2 fi v 2 + : : : + r u r fi v r (4) where u i , and v i are column vectors of the U and V matrices respectively, and i the diagonal
Reference: [12] <author> J. Li, D. Rotem, and H. Wong. </author> <title> A new compression method with fast searching on large databases. </title> <booktitle> In Proceedings of the 13th VLDB Conference, </booktitle> <volume> volume 13, </volume> <pages> pages 311-318, </pages> <address> Brighton, England, </address> <year> 1987. </year>
Reference-contexts: No previous work, to our knowledge, has addressed the problem we study in this paper, even though work on data compression abounds. Some interesting work has been done on compression with fast searching in a large database of bit vectors <ref> [12, 5] </ref>. Our work is different because our focus is on a dataset of real-valued numbers rather than bit vectors. Well-designed index structures are necessary to support ad hoc queries. There has been much work on index structures, including some excellent recent work specifically aimed at decision support [8, 10].
Reference: [13] <author> F. Murtagh. </author> <title> A survey of recent advances in hierarchical clustering algorithms. </title> <journal> The Computer Journal, </journal> <volume> 26(4) </volume> <pages> 354-359, </pages> <year> 1983. </year>
Reference-contexts: Excellent surveys on clustering include <ref> [18, 13, 26] </ref>. Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters [26], are typically O (N 2 ) or O (N log N ).
Reference: [14] <author> Raymond T. Ng and Jiawei Han. </author> <title> Efficient and effective clustering methods for spatial data mining. </title> <booktitle> Proc. of VLDB Conf., </booktitle> <pages> pages 144-155, </pages> <month> September </month> <year> 1994. </year>
Reference-contexts: Faster, approximate algorithms include the popular "k-means" algorithm [17], which requires a constant, but large number of passes over the dataset, thus becoming impractical for the huge datasets we have in mind. Recent fast clustering algorithms for huge databases include CLARANS <ref> [14] </ref>, BIRCH [28], and CLUDIS [6]. However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. <p> We tried using a small sample to do the clustering and then assigning the remaining records to the existing clusters, but this gave very poor results. As mentioned, clustering for large datasets is the topic of recent research (BIRCH [28], CLARANS <ref> [14] </ref>, etc.); however, none of these algorithms scales up for high-dimensional points. Thus, we focus our attention on SVDD for the rest of the experiments. 1,000, 2,000, 5,000, 10,000, 20,000, 50,000 and the full set of N =100,000 customers from the `phone100K' dataset, for the SVDD method.
Reference: [15] <author> William H. Press, Saul A. Teukolsky, William T. Vet-terling, and Brian P. Flannery. </author> <title> Numerical Recipes in C. </title> <publisher> Cambridge University Press, </publisher> <year> 1992. </year> <note> 2nd Edition. </note>
Reference-contexts: SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [24] or <ref> [15] </ref> for more details. <p> Proof: See <ref> [15, p. 59] </ref>. 2 Recall that a matrix U is called column-orthonormal if its columns u i are mutually orthogonal unit vectors. Equivalently: U t fi U = I, where I is the identity ma-trix.
Reference: [16] <author> Lawrence Rabiner and Biing-Hwang Juang. </author> <title> Fundamentals of Speech Recognition. </title> <publisher> Prentice Hall, </publisher> <year> 1993. </year>
Reference-contexts: Reconstruction in this case is particularly simple: To find the value of cell x i;j , find the cluster-representative for the i-th customer, and return its j-th entry. This application of clustering is known in the signal processing literature as vector quantization <ref> [16] </ref>. Clustering has attracted tremendous interest, from diverse fields and for diverse applications: in information retrieval for grouping together documents represented as vectors [20]; in pattern matching, for grouping together samples of the training set [3]; in the social and natural sciences for statistical analysis [9]. <p> Fourier analysis is perhaps the best known of the standard techniques, although there is a plethora of other techniques, such as wavelets [19], linear predictive coding <ref> [16] </ref>, and so forth. Consider Fourier analysis, where a given time signal is "transformed" to obtain a set of Fourier coefficients. In many practical signals, it is the case that most of the "energy" (or "information") is concentrated in the first few Fourier coefficients [21].
Reference: [17] <author> Lawrence Richard Rabiner and Bernard Gold. </author> <title> Theory and Application of Digital Signal Processing. </title> <publisher> Prentice-Hall, </publisher> <address> Englewood Cliffs, N.J., </address> <year> 1975. </year>
Reference-contexts: Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters [26], are typically O (N 2 ) or O (N log N ). Faster, approximate algorithms include the popular "k-means" algorithm <ref> [17] </ref>, which requires a constant, but large number of passes over the dataset, thus becoming impractical for the huge datasets we have in mind. Recent fast clustering algorithms for huge databases include CLARANS [14], BIRCH [28], and CLUDIS [6]. However, these have only been tried for M =2 dimensions.
Reference: [18] <author> Edie Rasmussen. </author> <title> Clustering algorithms. </title> <editor> In William B. Frakes and Ricardo Baeza-Yates, editors, </editor> <booktitle> Information Retrieval: Data Structures and Algorithms, </booktitle> <pages> pages 419-442. </pages> <publisher> Prentice Hall, </publisher> <year> 1992. </year>
Reference-contexts: Excellent surveys on clustering include <ref> [18, 13, 26] </ref>. Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters [26], are typically O (N 2 ) or O (N log N ).
Reference: [19] <author> Mary Beth Ruskai, Gregory Beylkin, Ronald Coif-man, Ingrid Daubechies, Stephane Mallat, Yves Meyer, and Louise Raphael. </author> <title> Wavelets and Their Applications. </title> <publisher> Jones and Bartlett Publishers, </publisher> <address> Boston, MA, </address> <year> 1992. </year>
Reference-contexts: Fourier analysis is perhaps the best known of the standard techniques, although there is a plethora of other techniques, such as wavelets <ref> [19] </ref>, linear predictive coding [16], and so forth. Consider Fourier analysis, where a given time signal is "transformed" to obtain a set of Fourier coefficients. In many practical signals, it is the case that most of the "energy" (or "information") is concentrated in the first few Fourier coefficients [21].
Reference: [20] <author> G. Salton, E.A. Fox, and H. Wu. </author> <title> Extended boolean information retrieval. </title> <journal> CACM, </journal> <volume> 26(11) </volume> <pages> 1022-1036, </pages> <month> November </month> <year> 1983. </year>
Reference-contexts: This application of clustering is known in the signal processing literature as vector quantization [16]. Clustering has attracted tremendous interest, from diverse fields and for diverse applications: in information retrieval for grouping together documents represented as vectors <ref> [20] </ref>; in pattern matching, for grouping together samples of the training set [3]; in the social and natural sciences for statistical analysis [9]. Excellent surveys on clustering include [18, 13, 26]. Although useful in numerous applications, in our setting clustering might not scale-up.
Reference: [21] <author> Manfred Schroeder. </author> <title> Fractals, Chaos, Power Laws: Minutes From an Infinite Paradise. W.H. </title> <publisher> Freeman and Company, </publisher> <address> New York, </address> <year> 1991. </year>
Reference-contexts: Consider Fourier analysis, where a given time signal is "transformed" to obtain a set of Fourier coefficients. In many practical signals, it is the case that most of the "energy" (or "information") is concentrated in the first few Fourier coefficients <ref> [21] </ref>. One can then throw away the remaining coefficients. This effect has also been observed in the data mining context [1]. <p> In the `phone2000' case, it consistently had the highest reconstruction error. For stocks prices, which are modeled well as random walks <ref> [21] </ref>, it is believed to be the best among the spectral methods, exactly because successive stock prices are highly correlated.
Reference: [22] <author> D.G. Severance and G.M. Lohman. </author> <title> Differential files: Their application to the maintenance of large databases. </title> <journal> ACM TODS, </journal> <volume> 1(3) </volume> <pages> 256-267, </pages> <month> September </month> <year> 1976. </year>
Reference-contexts: This should be done in a hash table, where the key is the combination of (row*M +column), that is, the order of the cell in the row-major scanning. Optionally, we could use a main-memory Bloom filter <ref> [22] </ref>, which would predict the majority of non-outliers, and thus save several probes into the hash table.
Reference: [23] <author> James A. Storer. </author> <title> Data Compression: Methods and Theory. </title> <publisher> Computer Science Press, Inc., </publisher> <year> 1988. </year>
Reference-contexts: We examine the first three in the next three subsections, and present SVD in detail in the next section. 2.1 String Compression Algorithms for lossless string compression are widely available (e.g., gzip, based on the well-known Lempel-Ziv algorithm [29], Huffman coding, arithmetic coding, etc.; see <ref> [23] </ref>). While these techniques can achieve fairly good compression, the difficulty with them has to do with reconstruction of the compressed data.
Reference: [24] <author> Gilbert Strang. </author> <title> Linear Algebra and its Applications. </title> <publisher> Academic Press, </publisher> <year> 1980. </year> <note> 2nd edition. </note>
Reference-contexts: SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See <ref> [24] </ref> or [15] for more details.
Reference: [25] <author> M. Turk and A. Pentland. </author> <title> Eigenfaces for recognition. </title> <journal> Journal of Cognitive Neuroscience, </journal> <volume> 3(1) </volume> <pages> 71-86, </pages> <year> 1991. </year>
Reference-contexts: popular and powerful operation, and it has been used in numerous applications, such as statistical analysis (as the driving engine behind the Principal Component Analysis [11]), text retrieval under the name of Latent Semantic Indexing [4], pattern recognition and dimensionality reduction as the Karhunen-Loeve (KL) transform [3], and face recognition <ref> [25] </ref>. SVD is particularly useful in settings that involve least-squares optimization such as in linear regression, dimensionality reduction, and matrix approximation. See [24] or [15] for more details.
Reference: [26] <editor> C.J. Van-Rijsbergen. </editor> <booktitle> Information Retrieval. </booktitle> <address> Butter-worths, London, England, </address> <year> 1979. </year> <note> 2nd edition. </note>
Reference-contexts: Excellent surveys on clustering include <ref> [18, 13, 26] </ref>. Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters [26], are typically O (N 2 ) or O (N log N ). <p> Excellent surveys on clustering include [18, 13, 26]. Although useful in numerous applications, in our setting clustering might not scale-up. The so-called "sound" clustering algorithms, which presumably give the highest quality clusters <ref> [26] </ref>, are typically O (N 2 ) or O (N log N ). Faster, approximate algorithms include the popular "k-means" algorithm [17], which requires a constant, but large number of passes over the dataset, thus becoming impractical for the huge datasets we have in mind.
Reference: [27] <author> Andreas S. Weigend and Neil A. Gerschenfeld. </author> <title> Time Series Prediction: Forecasting the Future and Understanding the Past. </title> <publisher> Addison Wesley, </publisher> <year> 1994. </year>
Reference-contexts: Error measure: There are many different measures that one could use for reconstruction error, based on different application needs. The root-mean-squared-error (absolute or relative) is the typical error measure for forecasting applications in time series <ref> [27] </ref>. We use this metric, once again, normalized with respect to the standard deviation of the data values being recorded.
Reference: [28] <author> T. Zhang, R. Ramakrishnan, and M. Livny. </author> <title> Birch: An efficient data clustering method for very large databases. </title> <booktitle> In SIGMOD '96, </booktitle> <pages> pages 103-114, </pages> <address> Montreal, Canada, </address> <month> June </month> <year> 1996. </year>
Reference-contexts: Faster, approximate algorithms include the popular "k-means" algorithm [17], which requires a constant, but large number of passes over the dataset, thus becoming impractical for the huge datasets we have in mind. Recent fast clustering algorithms for huge databases include CLARANS [14], BIRCH <ref> [28] </ref>, and CLUDIS [6]. However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. <p> Recent fast clustering algorithms for huge databases include CLARANS [14], BIRCH <ref> [28] </ref>, and CLUDIS [6]. However, these have only been tried for M =2 dimensions. They will probably suffer in high dimensionalities (e.g., M 100), if they are based on R*-trees [6] or any other related spatial access method [28]. In our experiments we used an off-the-shelf clustering method from the `S' statistical package [2]. The method is quadratic on the number of records N , and it builds a cluster-hierarchy, which we truncate at the appropriate levels, to obtain the desirable number of clusters. <p> We tried using a small sample to do the clustering and then assigning the remaining records to the existing clusters, but this gave very poor results. As mentioned, clustering for large datasets is the topic of recent research (BIRCH <ref> [28] </ref>, CLARANS [14], etc.); however, none of these algorithms scales up for high-dimensional points. Thus, we focus our attention on SVDD for the rest of the experiments. 1,000, 2,000, 5,000, 10,000, 20,000, 50,000 and the full set of N =100,000 customers from the `phone100K' dataset, for the SVDD method.
Reference: [29] <author> J. Ziv and A. Lempel. </author> <title> A universal algorithm for sequential data compression. </title> <journal> IEEE Trans. Information Theory, </journal> <volume> IT-23(3):337-343, </volume> <month> May </month> <year> 1977. </year>
Reference-contexts: We examine the first three in the next three subsections, and present SVD in detail in the next section. 2.1 String Compression Algorithms for lossless string compression are widely available (e.g., gzip, based on the well-known Lempel-Ziv algorithm <ref> [29] </ref>, Huffman coding, arithmetic coding, etc.; see [23]). While these techniques can achieve fairly good compression, the difficulty with them has to do with reconstruction of the compressed data.
References-found: 29

