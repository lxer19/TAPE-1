URL: http://www.cs.ucf.edu:80/~vision/PAPERS/cvpr941.ps
Refering-URL: http://www.cs.ucf.edu:80/~vision/tech_papers.html
Root-URL: 
Title: A Survey of Motion Analysis from Moving Light Displays  
Author: Claudette Cedras and Mubarak Shah 
Address: Orlando, FL 32816  
Affiliation: Computer Science Dept. University of Central Florida  
Date: June 20-24, 1994, pages 214-221.  
Note: IEEE CVPR-94, Seattle, Washington,  
Abstract: Motion-based recognition deals with the recognition of objects or motions directly from the motion information extracted from a sequence of images. There are two main steps in this approach. The first consists of finding an appropriate representation for the objects or motions, from the motion cues of the sequence, and then organize them into useful representations. The second step consists of the matching of some unknown input with a model. This paper provides a review of recent developments in motion-based recognition. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> K. Akita. </author> <title> Image Sequence Analysis of Real World Human Motion. </title> <journal> Pattern Recognition, </journal> <volume> 17(1) </volume> <pages> 73-83, </pages> <year> 1984. </year>
Reference-contexts: The tracking consists of determining the location and shape of body parts from frame to frame, while labeling involves identifying them. In Akita's work <ref> [1] </ref>, the recognition of the parts is done in the following order: legs, head, arms and trunk.
Reference: [2] <author> M. C. Allmen and C. R. Dyer. </author> <title> Cyclic Motion Detection Using Spatiotemporal Surfaces and Curves. </title> <booktitle> In ICPR-90, </booktitle> <pages> pages 365-370. </pages>
Reference-contexts: Furthermore, different cyclic motions could occur concurrently with the same or with different frequencies and phase relative to each other. Based on studies of the human visual system, Allmen and Dyer <ref> [2] </ref> argue that cyclic motion detection (1) does not depend on prior recognition of the moving object; (2) does not depend on the absolute position of the object; (3) needs sequences containing at least two complete cycles; (4) is sensitive to different scales, i.e. cycles at different levels of a moving
Reference: [3] <author> M. C. Allmen and C. R. Dyer. </author> <title> Computing Spatiotem-poral Relations for Dynamic Perceptual Organization. </title> <journal> CVGIP:IU, </journal> <volume> 58(3) </volume> <pages> 338-351, </pages> <year> 1993. </year>
Reference-contexts: The determination of those types of motion invariants that are reliable and stable provides a new avenue for this type of research. Similarly, the clustering of spatiotemporal flow curves can provide a representation for coherent motions like a translation or rotation <ref> [3] </ref>. A hierarchical clustering of these curves can lead to the detection of different objects, their particular motion, their occlusion/disocclusion, and even relative and common motion could be inferred. Dynamic perceptual organization can be a very useful research direction that could lead to interesting approaches and results.
Reference: [4] <author> C. D. Barclay, J. E. Cutting, and L. T. Kozlowski. </author> <title> Temporal and Spatial Factors in Gait Perception that Influence Gender Recognition. </title> <journal> Perception and Psychophysics, </journal> <volume> 23(2) </volume> <pages> 145-152, </pages> <year> 1978. </year>
Reference-contexts: A set of static spots remained meaningless to observers, while their relative movement created a vivid impression of a person walking, running, dancing, etc. The gender of a person, even the gait of a friend can be recognized based solely on the motion of those spots <ref> [4] </ref>. There are two theories about the interpretation of MLD type stimuli. In the first, people use motion information in the MLD to recover the 3D structure (structure from motion) and subsequently use the structure for recognition.
Reference: [5] <author> Z. Chen and H.-J. Lee. </author> <title> Knowledge-Guided Visual Perception of 3-D Human Gait from a Single Image Sequence. </title> <journal> SMC, </journal> <volume> 22(2) </volume> <pages> 336-342, </pages> <year> 1992. </year>
Reference-contexts: The validity of the updated model is verified by comparing its projection on the image plane with the edges extracted from the sequence. Tracking with Stick-Figure Models. The work done by Chen and Lee <ref> [5] </ref> is divided in two parts. The first consists of finding all allowed 3D configurations of the body for each frame.
Reference: [6] <author> J. E. Cutting and D. R. Proffitt. </author> <title> The Minimum Principle and the Perception of Absolute, Common, and Relative Motions. </title> <journal> Cognitive Psychology, </journal> <volume> 14 </volume> <pages> 211-246, </pages> <year> 1982. </year>
Reference-contexts: However, absolute values might sometimes be inadequate. Cutting and Proffitt <ref> [6] </ref> showed that relative motion is an important aspect in human visual perception. This kind of information should therefore be very helpful in computer vision systems. Multiple trajectories can be used to compute relative motion.
Reference: [7] <author> T. J. Darrell and A. P. Pentland. </author> <title> Recognition of Space-Time Gestures Using a Distributed Representation. </title> <type> Technical Report TR-197, </type> <institution> M.I.T. Media Laboratory Vision and Modeling Group, </institution> <year> 1992. </year>
Reference-contexts: Each image in a sequence can then be expressed as a linear combination of the eigen images. Darrell and Pentland used a set of model views for hand gestures <ref> [7] </ref>. Their method automatically stores the appropriate number of views necessary to represent the object using correlation. If optical flow can be reliably extracted, flow correlation might be more appropriate as compared to correlation using plain graylevels. <p> Two studies are described below. Darrell and Pentland <ref> [7] </ref> present an automatic view-based approach to build the set of models from which gesture models will be created. Model views of an object are built using normalized correlation. The object is tracked, and when the correlation score drops below a threshold, a new model view is added.
Reference: [8] <author> J. W. Davis and M. Shah. </author> <title> Gesture Recognition. </title> <publisher> ECCV-94. </publisher>
Reference-contexts: Dynamic time warping is performed to adjust all gestures to the same length. An unknown gesture is similarly correlated and its score plotted, for all view models. The matching is then done by comparing the correlation scores between the unknown and the model gestures. Davis and Shah <ref> [8] </ref> report a simple method for hand gesture recognition by tracking the trajectory followed by each finger and using their motion as a basis for recognition. The direction of motion and displacement of each finger are stored, along with the name of the gesture, in a simple data structure.
Reference: [9] <author> S. A. Engel and J. M. Rubin. </author> <title> Detecting Visual Motion Boundaries. </title> <booktitle> In Proc. Workshop on Motion, </booktitle> <pages> 107-111, </pages> <year> 1986. </year>
Reference-contexts: This results in a set of TPS contours, each contour corresponding to a change in motion. The representation has been shown to distinguish basic motions like translation, rotation, projectile and cycloid. Based on psychophysical considerations, Engel and Rubin <ref> [9] </ref> described the significant changes in motion as motion boundaries, i.e. motion events that partition a global motion into its psychological parts, of which they found five types: smooth starts, smooth stops, pauses, impulse starts and impulse stops.
Reference: [10] <author> K. E. Finn and A. A. Montgomery. </author> <title> Automatic Optically-Based Recognition of Speech. </title> <journal> PRL, </journal> <volume> 8 </volume> <pages> 159-164, </pages> <year> 1988. </year>
Reference-contexts: For instance, the phonemes "b", "p" and "m" sound different but look the same when spoken <ref> [10] </ref>. In Petajan et al. [25], the lipreading task is performed with the help of a codebook. The codebook is a set of mouth images containing all possible aspects of the mouth's appearance. In particular, the code-book contains images of the mouth opening area. <p> A spoken word consists of a sequence of codebook images. Words are compared by computing a distance value between an unknown word and the models. Finn and Montgomery use a combination of distances between different points around the mouth <ref> [10] </ref>. Twelve dots were placed around the mouth of speaker and tracked during the experiments; a total of fourteen distances were measured, and used as a feature vector. Recognition consisted of computing a total rms value between two utterances. <p> Methods for lipreading should attempt to achieve the following goals. They should be able to perform in unconstrained or little constrained environments, they should achieve speaker independence, address continuous speech and ultimately perform in real-time. None of the methods described above meet all these goals. Finn and Montgomery <ref> [10] </ref> place markers around the mouth of a speaker; Martin and Shah's work [21] is computationally expensive; Mase and Pentland [22] address continuous speech but the difficult part, according to the authors, is to detect the beginning of the first word.
Reference: [11] <author> N. H. Goddard. </author> <title> The Perception of Articulated Motion: Recognizing Moving Light Displays. </title> <type> PhD thesis, </type> <institution> University of Rochester, </institution> <year> 1992. </year>
Reference-contexts: They used polar velocity representation (s; ), and the features extracted for the detection of the perceptual boundaries are first and second derivatives of the speed s and the second derivative of the direction . Goddard <ref> [11] </ref> used, as motion events, changes in rotational velocity of body segments along with changes in their direction. The angular velocity range was partitioned into six, and the four quadrants were used for the orientation. <p> The different motions of the parts need to be determined with respect to each other. The scope of God-dard's thesis <ref> [11] </ref> is the recognition of 400ms moving light displays generated from actors using a connectionist approach. Given the set of trajectories of points in a sequence, line segments are extracted.
Reference: [12] <author> K. Gould and M. A. Shah. </author> <title> The Trajectory Primal Sketch: a Multi-Scale Scheme for Representing Motion Characteristics. </title> <booktitle> In CVPR, </booktitle> <year> 1989. </year>
Reference-contexts: Motion events are usually detected by the presence of discon-tinuities that can be found by looking at derivatives of the velocity, for example. Gould and Shah's Trajectory Primal Sketch (TPS) is a representation for the significant changes in motion <ref> [12] </ref>. Changes are identified at various scales by computing the scale-space of the velocity curves v x and v y extracted from the trajectory of a point. This results in a set of TPS contours, each contour corresponding to a change in motion.
Reference: [13] <author> D. D. Hoffman and B. E. Flinchbaugh. </author> <title> The Interpretation of Biological Motion. </title> <journal> Biological Cybernetics, </journal> <volume> 42 </volume> <pages> 195-204, </pages> <year> 1982. </year>
Reference-contexts: According to the second theory, motion information is directly used to identify a motion, without any structure recovery. There has been significant interest over the last decade, in the computer vision community, in the structure from motion theory (e.g. <ref> [13, 29, 30] </ref>). In that approach, 3D coordinates of points on the moving objects and their 3D motion is recovered from a sequence of frames. This problem is formulated in terms of systems of nonlinear or linear equations given 2D location of moving points among a few frames.
Reference: [14] <author> D. C. Hogg. </author> <title> Interpreting Images of a Known Moving Object. </title> <type> PhD thesis, </type> <institution> University of Sussex, </institution> <year> 1984. </year>
Reference-contexts: In both, this parameter is determined and plotted as a function of the frame number, along with the body position with respect to the world coordinates. In Hogg's work <ref> [14] </ref>, a frame by frame analysis provides an estimate of the person's 3D position and its posture. Important parameters are the posture parameter P ST R, speed and direction of motion.
Reference: [15] <author> G. Johansson. </author> <title> Visual Perception of Biological Motion and a Model for its Analysis. </title> <journal> Perception and Psychophysics, </journal> <volume> 14(2) </volume> <pages> 210-211, </pages> <year> 1973. </year>
Reference-contexts: Our sensitivity and ease of perception and interpretation of motion suggests that our visual system is well adapted to temporal information. Motion perception has been studied extensively using Johansson's moving light displays (MLDs) <ref> [15] </ref>. MLDs consist of bright spots attached to an actor dressed in black, and moving in front of a dark background. The collection of spots carry only 2D and no structural information, since they are not connected.
Reference: [16] <author> M. Kirby, F. Weisser, and G. Dangelmayr. </author> <title> A Model Problem in the Representation of Digital Image Sequences. </title> <journal> PR, </journal> <volume> 26(1) </volume> <pages> 63-73, </pages> <year> 1993. </year>
Reference-contexts: Eigen images extracted from a set of graylevel images of an object provide enough information to directly represent new images of that object. Kirby et al. <ref> [16] </ref> used that method with mouth images. <p> Each model optical flow frame is compared to each optical flow frame of the unknown sequence using correlation. Kirby et al. <ref> [16] </ref> chose to express mouth images as a linear combination of the fixed set of the eigenvectors of the ensemble averaged covariance matrix C (section 2.3). <p> Petajan et al. [25] have an interesting method, since when the codebook is complete, a distance table is computed which makes image comparison fast. Kirby et al. <ref> [16] </ref> provide a nice basis for lipreading, however more extensive work is required. 3.3 Gesture Interpretation Humans have the capability or can develop the ability to interpret gestures, and gestural languages have been developed to allow hearing impaired people to communicate more easily. Two studies are described below.
Reference: [17] <author> D. Koller, N. Heinze, and H.-H. Nagel. </author> <title> Algorithmic Characterization of Vehicle Trajectories from Image Sequences by Motion Verbs. </title> <booktitle> In CVPR, </booktitle> <pages> pages 90-95, </pages> <year> 1991. </year> <note> Extended version. </note>
Reference-contexts: Both works above do perform in real-time; as for the rest, much work remains. 3.4 Motion Verb Recognition Motion verb recognition deals with the association of natural language verbs with the motion performed by a moving object. Koller et al. <ref> [17] </ref> devised a method which automatically characterizes the trajectory of moving vehicles in an intersection. The motion verbs were divided into four different categories.
Reference: [18] <author> H.-J. Lee and Z. Chen. </author> <title> Determination of 3D Human Body Postures from a Single View. </title> <journal> CVGIP, </journal> <volume> 30 </volume> <pages> 148-168, </pages> <year> 1985. </year>
Reference-contexts: Tracking with Stick-Figure Models. The work done by Chen and Lee [5] is divided in two parts. The first consists of finding all allowed 3D configurations of the body for each frame. This process is described in <ref> [18] </ref>, where the possible location of all joints is determined in space, and knowledge of physical and motion constraints is used to eliminate invalid configurations. The second part is to find the sequence of configurations that would best represent the walking motion.
Reference: [19] <author> M. K. Leung and Y.-H. Yang. </author> <title> A Region Based Approach for Human Body Motion Analysis. </title> <journal> PR, </journal> <volume> 20(3) </volume> <pages> 321-329, </pages> <year> 1987. </year>
Reference-contexts: If window codes cannot find a correspondence, then a key frame sequence is used to find the current posture. Leung and Yang <ref> [19] </ref> also tackle the problem of body labeling in a sequence. The labeling is made up of two steps. The region description process abstracts the segmented image (see [20]) to extract the antiparallel lines (apars) that will be used for labeling.
Reference: [20] <author> M. K. Leung and Y.-H. Yang. </author> <title> Human Body Motion Segmentation in a Complex Scene. </title> <journal> PR, </journal> <volume> 20(3) </volume> <pages> 55-64, </pages> <year> 1987. </year>
Reference-contexts: Leung and Yang [19] also tackle the problem of body labeling in a sequence. The labeling is made up of two steps. The region description process abstracts the segmented image (see <ref> [20] </ref>) to extract the antiparallel lines (apars) that will be used for labeling. The most appropriate apars are then chosen for body part identification according to heuristics related to width ratios and likelihood.
Reference: [21] <author> G. A. Martin and M. Shah. </author> <title> Lipreading Using Optical Flow. </title> <booktitle> In Proc. Nat. Conf. on Undergraduate Research, </booktitle> <year> 1992. </year>
Reference-contexts: Their method automatically stores the appropriate number of views necessary to represent the object using correlation. If optical flow can be reliably extracted, flow correlation might be more appropriate as compared to correlation using plain graylevels. Martin and Shah <ref> [21] </ref> used a sequence of dense optical flow around the mouth of a speaker, which are then correlated for matching. Using binarized images, Yamato et al. [33] extracted a mesh feature from each frame of a sequence. <p> O (t) and E (t) are computed at each frame, and time warping is performed, to normalize the time to speak each word. A sampling of the functions for each model was used for matching with a similar sampling of an unknown. Martin and Shah <ref> [21] </ref> use a sequence of dense optical flow fields around the mouth of a speaker, which are spatially warped, temporally warped, then correlated, for matching with a sequence of optical flow frames. <p> None of the methods described above meet all these goals. Finn and Montgomery [10] place markers around the mouth of a speaker; Martin and Shah's work <ref> [21] </ref> is computationally expensive; Mase and Pentland [22] address continuous speech but the difficult part, according to the authors, is to detect the beginning of the first word.
Reference: [22] <author> K. Mase and A. Pentland. </author> <title> Lip Reading: Automatic Visual Recognition of Spoken Words. </title> <type> Technical Report 117, </type> <institution> M.I.T. Media Lab Vision Science, </institution> <year> 1989. </year>
Reference-contexts: Twelve dots were placed around the mouth of speaker and tracked during the experiments; a total of fourteen distances were measured, and used as a feature vector. Recognition consisted of computing a total rms value between two utterances. Mase and Pentland <ref> [22] </ref> observed that the most important features that affect mouth shape relate to the elongation and the opening of the mouth, affecting upper and lower lips. <p> None of the methods described above meet all these goals. Finn and Montgomery [10] place markers around the mouth of a speaker; Martin and Shah's work [21] is computationally expensive; Mase and Pentland <ref> [22] </ref> address continuous speech but the difficult part, according to the authors, is to detect the beginning of the first word. Petajan et al. [25] have an interesting method, since when the codebook is complete, a distance table is computed which makes image comparison fast.
Reference: [23] <author> M. P. Murray. </author> <title> Gait as a Total Pattern of Movement. </title> <journal> American Journal of Physical Medicine, </journal> <volume> 46(1) </volume> <pages> 290-333, </pages> <year> 1967. </year>
Reference-contexts: Joint angles have been extensively studied in physical medicine <ref> [23] </ref>. They are more formally expressed as flexion/extension, abduction/adduction and rotation angles. Studies have also shown that the forward motion is almost constant within a walking cycle, while the vertical displacement of the head is relatively small considering the global motion.
Reference: [24] <author> R. C. Nelson and R. Polana. </author> <title> Qualitative Recognition of Motion Using Temporal Texture. </title> <journal> CVGIP:IU, </journal> <volume> 56(1) </volume> <pages> 78-89, </pages> <year> 1992. </year>
Reference-contexts: Instead, the ability to have a more general idea about the content of a frame might be sufficient. Features generated from the use of information over a relatively large region or over whole images are referred to here as region-based features. For instance, Nelson and Polana <ref> [24] </ref> gathered a set of four features based on the computation of the normal flow, i.e. the flow component parallel to the gradient, over regions of interest. <p> Natural language semantic components were developed to describe motion concepts using English motion verbs. 3.5 Temporal Textures Classification In their paper, Nelson and Polana describe how the movement of the ripples on water, the wind in the leaves of trees, can be classified <ref> [24] </ref>. Those motions, referred to as temporal textures, show complex and non-rigid motions. The term temporal texture is used to emphasize that the motion patterns are of indeterminate spatial and temporal extent.
Reference: [25] <author> E. D. Petajan, B. Bischoff, D. Bodoff, and N. M. Brooke. </author> <title> An Improved Automatic Lipreading System to Enhance Speech Recognition. </title> <booktitle> In SIGCHI '88: Human Factors in Computing Systems, </booktitle> <pages> pages 19-25, </pages> <year> 1988. </year>
Reference-contexts: An image is divided into a grid, and the proportion of black to white pixels in each grid element is computed; the ordered set of ratios for an image is called the mesh feature. Petajan et al. <ref> [25] </ref>, also with bina-rized images, created a codebook of mouth opening images, i.e. a set of images of the different shapes of the mouth. 2.4 Matching Once the representation has been defined and the features from both models and unknown sequences properly organized, a comparison must be made so that classification <p> For instance, the phonemes "b", "p" and "m" sound different but look the same when spoken [10]. In Petajan et al. <ref> [25] </ref>, the lipreading task is performed with the help of a codebook. The codebook is a set of mouth images containing all possible aspects of the mouth's appearance. In particular, the code-book contains images of the mouth opening area. A spoken word consists of a sequence of codebook images. <p> Finn and Montgomery [10] place markers around the mouth of a speaker; Martin and Shah's work [21] is computationally expensive; Mase and Pentland [22] address continuous speech but the difficult part, according to the authors, is to detect the beginning of the first word. Petajan et al. <ref> [25] </ref> have an interesting method, since when the codebook is complete, a distance table is computed which makes image comparison fast.
Reference: [26] <author> R. Polana and R. C. Nelson. </author> <title> Recognizing Activities. </title> <note> Submitted to CVPR 1994. </note>
Reference-contexts: Once all those values are computed, they are put into vector form for classification. In another paper <ref> [26] </ref>, the same authors use a similar vector representation in order to recognize periodic motions like walking and running. It is also based on the computation of the normal flow, from which statistics are gathered over a selected portion of the sequence of images. <p> The periodic motion is extracted from the graylevel signals using a Fourier transform and an overall periodicity measure is computed. In an upcoming paper <ref> [26] </ref>, the same authors describe a method for recognizing different periodic motions. Tsai et al. [31] use spatiotemporal curvature as a representation. Some preprocessing is performed, in particular an autocorrelation to emphasized self--similarity within the curvature function. <p> Tsai et al. and Polana and Nelson used the Fourier transform to directly detect periodicity, which seems a reasonable and sensible thing to do. Furthermore this method is more robust to uncorrelated noise. Recognition using cyclic motion has been reported in <ref> [31, 26] </ref>. 3.2 Lipreading Lipreading is a very difficult task, especially since certain phonemes can appear visually identical (phonemes are minimal meaningful units of sound from which two words can be distinguished). For instance, the phonemes "b", "p" and "m" sound different but look the same when spoken [10].
Reference: [27] <author> R. Polana and R. C. Nelson. </author> <title> Detecting Activities. </title> <booktitle> In CVPR, </booktitle> <pages> pages 2-7, </pages> <year> 1993. </year>
Reference-contexts: A modified version of a uniform cost algorithm was used for cycle detection. An advantage of curvature scale-space is that it is possible to detect cycles at different scales. Polana and Nelson <ref> [27] </ref> first computed a reference curve, which is the approximate trajectory of a moving object's centroid, and aligned the frames with respect to it.
Reference: [28] <author> K. Rohr. </author> <title> Towards Model-Based Recognition of Human Movements in Image Sequences. </title> <journal> CVGIP: IU, </journal> <volume> 59(1) </volume> <pages> 94-115, </pages> <year> 1994. </year>
Reference-contexts: The plausibility is a weighted function of the plausibility of each part of the model, and is computed with the help of the projection of the model contours and the actual image edge points. The process is then repeated for each frame. Rohr's method comprises two phases <ref> [28] </ref>. The first phase, called the initialization phase, provides an estimate for the posture parameter pose and 3D position of the body using a linear regression method. The second phase, using the estimate of the first phase, uses a Kalman filter approach to incrementally estimate the model parameters.
Reference: [29] <author> H. Shariat and K. Price. </author> <title> How to Use More than Two Frames to Estimate Motion. </title> <booktitle> In Proc. Workshop on Motion, </booktitle> <pages> pages 119-124, </pages> <year> 1986. </year>
Reference-contexts: According to the second theory, motion information is directly used to identify a motion, without any structure recovery. There has been significant interest over the last decade, in the computer vision community, in the structure from motion theory (e.g. <ref> [13, 29, 30] </ref>). In that approach, 3D coordinates of points on the moving objects and their 3D motion is recovered from a sequence of frames. This problem is formulated in terms of systems of nonlinear or linear equations given 2D location of moving points among a few frames.
Reference: [30] <author> M. Subbarao. </author> <title> Interpretation of Image Motion Fields: A Spatiotemporal Approach. </title> <booktitle> In Proc. Workshop on Motion, </booktitle> <pages> pages 157-165, </pages> <year> 1986. </year>
Reference-contexts: According to the second theory, motion information is directly used to identify a motion, without any structure recovery. There has been significant interest over the last decade, in the computer vision community, in the structure from motion theory (e.g. <ref> [13, 29, 30] </ref>). In that approach, 3D coordinates of points on the moving objects and their 3D motion is recovered from a sequence of frames. This problem is formulated in terms of systems of nonlinear or linear equations given 2D location of moving points among a few frames.
Reference: [31] <author> P.-S. Tsai, M. Shah, K. Keiter, and T. Kasparis. </author> <title> Cyclic Motion Detection. </title> <type> Technical Report CS-TR-93-08, C.S. </type> <institution> Dept., Univ. of Central Florida, </institution> <year> 1993. </year>
Reference-contexts: The periodic motion is extracted from the graylevel signals using a Fourier transform and an overall periodicity measure is computed. In an upcoming paper [26], the same authors describe a method for recognizing different periodic motions. Tsai et al. <ref> [31] </ref> use spatiotemporal curvature as a representation. Some preprocessing is performed, in particular an autocorrelation to emphasized self--similarity within the curvature function. A Fourier transform is finally applied to that signal to detect the presence of cycles and their period. <p> Tsai et al. and Polana and Nelson used the Fourier transform to directly detect periodicity, which seems a reasonable and sensible thing to do. Furthermore this method is more robust to uncorrelated noise. Recognition using cyclic motion has been reported in <ref> [31, 26] </ref>. 3.2 Lipreading Lipreading is a very difficult task, especially since certain phonemes can appear visually identical (phonemes are minimal meaningful units of sound from which two words can be distinguished). For instance, the phonemes "b", "p" and "m" sound different but look the same when spoken [10].
Reference: [32] <author> J. K. Tsotsos, J. Mylopoulos, H. D. Covvey, and S. W. Zucker. </author> <title> A Framework for Visual Motion Understanding. </title> <journal> PAMI, </journal> <volume> 2(6) </volume> <pages> 563-573, </pages> <year> 1980. </year>
Reference-contexts: In addition, predicates are used, whose truth value is determined at each time instant. Using this information, an interpretation is sought, along with the time period for which it is true. The goal of Tsotsos' work <ref> [32] </ref> was to build an artificial intelligence system, called ALVEN, capable of using motion information to recognize normal and abnormal behavior of a heart's left ventricular motion.
Reference: [33] <author> J. Yamato, J. Ohya, and K. Ishii. </author> <title> Recognizing Human Action in Time-Sequential Images Using Hidden Mardov Model. </title> <booktitle> In CVPR, </booktitle> <pages> pages 379-385, </pages> <year> 1992. </year>
Reference-contexts: If optical flow can be reliably extracted, flow correlation might be more appropriate as compared to correlation using plain graylevels. Martin and Shah [21] used a sequence of dense optical flow around the mouth of a speaker, which are then correlated for matching. Using binarized images, Yamato et al. <ref> [33] </ref> extracted a mesh feature from each frame of a sequence. An image is divided into a grid, and the proportion of black to white pixels in each grid element is computed; the ordered set of ratios for an image is called the mesh feature. <p> Scenarios represent temporal series of events, with information on sequence and duration. Goddard demonstrated that the changes in angular velocities are sufficient for recognition. Discrimination between different tennis strokes was investigated by Yamato et al. <ref> [33] </ref> using Hidden Markov Models (HMM). They can be seen simply as symbol generating machines. An image sequence is processed in three steps. In the first, a mesh feature is extracted, then associated to a symbol by a clustering technique.
References-found: 33

