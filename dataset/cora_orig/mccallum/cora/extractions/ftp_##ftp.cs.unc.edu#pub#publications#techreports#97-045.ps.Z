URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/97-045.ps.Z
Refering-URL: ftp://ftp.cs.unc.edu/pub/publications/techreports/FILE.html
Root-URL: http://www.cs.unc.edu
Title: A Survey of Polygonal Simplification Algorithms  
Author: David Luebke 
Address: Chapel Hill  
Affiliation: Department of Computer Science University of North Carolina at  
Pubnum: UNC Technical Report TR97-045  
Abstract: 1. ABSTRACT 
Abstract-found: 1
Intro-found: 1
Reference: [Aliaga 97] <author> Aliaga, Daniel. </author> <title> SGI Performance Tips (Talk). </title> <note> For more information see: http://www.cs.unc.edu/~aliaga/IR-perf.html. </note>
Reference-contexts: Experiments on an SGI Onyx with InfiniteReality graphics, for example, indicate that Gouraud-shaded depth-buffered unlit triangles render two to three times faster in a display list than in a tightly optimized immediate mode display loop <ref> [Aliaga 97] </ref>. For these reasons view-dependent techniques seem unlikely to completely supplant view-independent techniques in the near future. 5.4 Geometry Compression A field closely related to polygonal simplification is geometry compression.
Reference: [Clark 76] <author> Clark, James H. </author> <title> Hierarchical Geometric Models for Visible Surface Algorithms, </title> <journal> Communications of the ACM, </journal> <volume> Vol. 19, No 10, </volume> <pages> pp 547-554. </pages>
Reference-contexts: As early as 1976 James Clark described the benefits of representing objects within a scene at several resolutions, and flight simulators have long used hand-crafted multi-resolution models of airplanes to guarantee a constant frame rate <ref> [Clark 76, Cosman 81] </ref>. Recent years have seen a flurry of research into generating such multi-resolution repr e-sentations of objects automatically by simplifying the polygonal geometry of the object.
Reference: [Cohen 96] <author> Cohen, Jon, A. Varshney, D. Manocha, G. Turk, H. Weber, P. Agarwal, F. Brooks, W. Wright. </author> <title> Simplification Envelopes, </title> <journal> Computer Graphics, </journal> <volume> Vol. 30 (SIGGRAPH 96). </volume>
Reference-contexts: which are perhaps the most likely models to have complex topologies. 4.7 Simplification Envelopes Simplification envelopes, presented by Jonathan Cohen, Ami-tabh Varshney, Dinesh Manocha, Greg Turk, Hans Weber, Pankaj Agrawal, Frederick Brooks, and William Wright, provide a method of guaranteeing fidelity bounds while enforcing global as well as local topology <ref> [Cohen 96] </ref>. Simplification envelopes per se are more of a framework than an individual algorithm, and the authors of this paper present two examples of algorithms within this framework.
Reference: [Cosman 81] <author> Cosman, M., and R. Schumacker. </author> <title> System Strategies to Optimize CIG Image Content. </title> <booktitle> Proceedings Image II Conference (Scotsdale, </booktitle> <address> Arizona), </address> <year> 1981. </year>
Reference-contexts: As early as 1976 James Clark described the benefits of representing objects within a scene at several resolutions, and flight simulators have long used hand-crafted multi-resolution models of airplanes to guarantee a constant frame rate <ref> [Clark 76, Cosman 81] </ref>. Recent years have seen a flurry of research into generating such multi-resolution repr e-sentations of objects automatically by simplifying the polygonal geometry of the object.
Reference: [Deering 95] <author> Deering, Michael. </author> <title> Geometry Compression, </title> <journal> Computer Graphics, </journal> <volume> Vol. 29 (SIGGRAPH 95). </volume>
Reference-contexts: Deering introduced the first geometry compression algorithm for general 3-D polygonal models. His approach applies quantization and lossy compression to attributes such as the position, normal, and color of vertices, achieving compression rates of 6-10:1 <ref> [Deering 95] </ref>. Taubin and Rossignac extend this idea by compressing the topological connectivity of polygons in the mesh. Decomposing the triangulated model into a tree of linear triangle strips allows significant compression of connectivity information, averaging roughly two bits per triangle [Taubin 96].
Reference: [Eck 95] <author> Eck, Matthias, T. DeRose, T. Duchamp, H. Hoppe, M. Lounsbery, W. Stuetzle. </author> <title> Multiresolution Analysis of Arbitrary Meshes, </title> <journal> Computer Graphics, </journal> <volume> Vol. 29 (SIGGRAPH 95). </volume>
Reference-contexts: 97], independently developing a system similar in many ways to the Luebke-Erikson algorithm described above. 4.10 Multiresolution Analysis of Arbitrary Meshes This adaptive subdivision algorithm by Matthias Eck, Tony DeRose, Tom Duchamp, Hughes Hoppe, Michael Lounsbery, and Werner Stuetzle uses a compact wavelet representation to guide the recursive subdivision process <ref> [Eck 95] </ref>. By adding or subtracting wavelet coefficients the algorithm can smoothly interpolate between levels of detail. The algorithm provides fidelity-based simplification by using enough wavelet coefficients to guarantee that the simplified surface lies within a user-specified distance of the original model.
Reference: [Garland 94] <author> Garland, Michael, and P. Heckbert. </author> <title> Multiresolution Modeling for Fast Rendering. </title> <booktitle> Proceedings of Graphics Interface 94 (1994). </booktitle>
Reference-contexts: As a starting point for such a metric, they suggest the sum of the squared distances in RGB color space between corresponding pixels. <ref> [Garland 94] </ref>. 5.3 View-dependence View-dependent algorithms are quite new to the field of general polygonal simplification, and possess some definite advantages over view-independent approaches. View-independent methods are less general, making some implicit assumptions regarding object size. To begin with, physically large objects must be subdivided.
Reference: [He 95] <author> Taosong He, L. Hong, A. Kaufman, A. Varshney, and S. Wang. </author> <title> Voxel-Based Object Simplification. </title> <booktitle> Proceedings Visualization 95, </booktitle> <publisher> IEEE Computer Society Press (Atlanta, </publisher> <address> GA), </address> <year> 1995, </year> <pages> pp. 296-303. </pages>
Reference: [Hoppe 93] <author> Hoppe, Hughes. </author> <title> Mesh Optimization, </title> <journal> Computer Graphics, </journal> <volume> Vol. 27 (SIGGRAPH 93). </volume>
Reference-contexts: Writing a robust system based on simplification envelopes is a substantial undertaking. 4.8 Mesh Optimization This paper by Hughes Hoppe, Tony DeRose, Tom Duchamp, John McDonald, and Werner Stuetzle, describes a complex sampling approach, which evolved out of the authors work on surface reconstruction of laser-scanned datasets <ref> [Hoppe 93] </ref>. Surface reconstruction is the problem of creating a three-dimensional mesh from a collection of sample points. Mesh optimization, as the name suggests, treats simplification as an optimization problem.
Reference: [Hoppe 96] <author> Hoppe, Hughes. </author> <title> Progressive Meshes, </title> <journal> Computer Graphics, </journal> <volume> Vol. 30 (SIGGRAPH 96). </volume>
Reference-contexts: Second, a recursive subdivision of the base model may not be able to capture the exact geometry of the original model, especially around sharp corners and creases in the mesh <ref> [Hoppe 96] </ref>. Decimation techniques iteratively remove vertices or faces from the mesh, retriangulating the resulting hole after each step. This process continues until it reaches a user-specified degree of simplification. <p> Among the contributions of this paper was the introduction of a method to smoothly interpolate between different levels of detail, a process for which Hughes Hoppe has since used the term geomorph <ref> [Hoppe 96] </ref>. 4.3 Multi-Resolution 3D Approximations for Rendering Complex Scenes This vertex-merging algorithm by Jarek Rossignac and Paul Borrel is one of the few schemes that neither requires nor preserves valid topology. The algorithm can therefore deal robustly with degenerate models with which other approaches have little or no success. <p> The fidelity of the resulting simplifications is quite high for smooth organic forms, but the algorithm has difficulty capturing sharp features in the original model unless the features happen to fall along a division in the base mesh <ref> [Hoppe 96] </ref>. 4.11 Surface Simplification Using Quadric Error Metrics This recent view-independent vertex-merging algorithm by Mi-chael Garland and Paul Heckbert strikes perhaps the best ba lance yet between speed, fidelity, and robustness. <p> The simplicity and robust nature of vertex merging no doubt play a large part in this trend. Earlier work by Hoppe and Ronfard has probably played a part as well by demonstrating that high-quality simplification is possible with an algorithm based entirely on edge collapses <ref> [Hoppe 96, Ronfard 96] </ref>. Re presentations such as progressive meshes and the HDS vertex tree provide a very general framework for experimenting with different simplification strategies, including the relatively new view-dependent criteria. <p> These algorithms transmit a coarse version of the data first, followed by a stream of refinements, which the receiving process uses to reconstruct the original. The progressive mesh representation is by design well suited for progressive transmission of polygonal models <ref> [Hoppe 96] </ref>. If the mainstream debut of 3-D graphics occurs on the scale of the WWW, polygonal simplification algorithms may well be measured by their ability to support compression and progre s-sive transmission. 6.
Reference: [Hoppe 97] <author> Hoppe, Hughes. </author> <title> View-Dependent Refinement of Progressive Meshes, </title> <journal> Computer Graphics, </journal> <volume> Vol. 31 (SIGGRAPH 97). </volume>
Reference-contexts: Hoppe also describes how to model some of these attributes in the energy function, allowing normals, color, and material identifiers to guide the simplification process. Finally, Hoppe has recently extended progressive meshes to perform view-dependent simplification at run time <ref> [Hoppe 97] </ref>, independently developing a system similar in many ways to the Luebke-Erikson algorithm described above. 4.10 Multiresolution Analysis of Arbitrary Meshes This adaptive subdivision algorithm by Matthias Eck, Tony DeRose, Tom Duchamp, Hughes Hoppe, Michael Lounsbery, and Werner Stuetzle uses a compact wavelet representation to guide the recursive subdivision process <p> ISSUES AND TRENDS 5.1 Mechanism The field of polygonal simplification appears to be converging on vertex merging as the underlying mechanism for polygon reduction. All four surface simplification papers in the SIG-GRAPH 97 conference, for example, present algorithms based on merging vertices <ref> [Hoppe 97, Luebke 97, Garland 97, Popovic 97] </ref>. The simplicity and robust nature of vertex merging no doubt play a large part in this trend. <p> Again, view-dependent techniques can be designed to automatically merge objects without requiring the user to explicitly establish a hierarchy of objects to be merged [Luebke 97]. Finally, view-dependent methods offer the possibility of more sophisticated simplification criteria. Some examples include preservation of silhouettes <ref> [Hoppe 97, Luebke 97, Xia 96] </ref>, preservation of specular highlights [Xia 96], and aggressive simplification of backfacing regions [Hoppe 97]. View-independent algorithms can address none of these criteria. However, view-dependence also suffers some significant drawbacks. View-dependent methods inherently involve more run-time computation than view-independent approaches. <p> Finally, view-dependent methods offer the possibility of more sophisticated simplification criteria. Some examples include preservation of silhouettes [Hoppe 97, Luebke 97, Xia 96], preservation of specular highlights [Xia 96], and aggressive simplification of backfacing regions <ref> [Hoppe 97] </ref>. View-independent algorithms can address none of these criteria. However, view-dependence also suffers some significant drawbacks. View-dependent methods inherently involve more run-time computation than view-independent approaches. When the CPU rather than the graphics subsystem is the limiting factor in rendering performance, view-dependent approaches become less attractive.
Reference: [Lorenson 87] <author> Lorenson, William E., and H. Cline. </author> <title> Marching Cubes: </title>
Reference-contexts: Schroeder, Jonathan A. Zarge, and William E. Lorenson [Schroeder 92] coined the term decimation for iterative removal of vertices. Schroeders decimation scheme is designed to operate on the output of the Marching Cubes algorithm for extracting isosurfaces from volumetric data <ref> [Lorenson 87] </ref>, and is still the most commonly used algorithm for this purpose. Marching Cubes output is often heavily overtesselated, with coplanar regions divided into many more polygons than necessary, and Schroeders algorithm excels at removing this redundant geometry.
References-found: 12

