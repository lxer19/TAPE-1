URL: http://www.cs.wisc.edu/~solodov/solsva96vi.ps.Z
Refering-URL: http://www.cs.wisc.edu/~solodov/solodov.html
Root-URL: http://www.cs.wisc.edu
Title: A NEW PROJECTION METHOD FOR VARIATIONAL INEQUALITY PROBLEMS  
Author: M. V. SOLODOV AND B. F. SVAITER 
Keyword: Key words. variational inequalities, projection methods, pseudomonotone maps  
Date: 199x 000  
Note: SIAM J. CONTROL AND OPTIMIZATION c 199x Society for Industrial and Applied Mathematics Vol. xx, No. pp. xx-xx, xxx  AMS subject classifications. 49M45, 90C25, 90C33  
Abstract: We propose a new projection algorithm for solving the variational inequality problem, where the underlying function is continuous and satisfies a certain generalized monotonicity assumption (for example, it can be pseudomonotone). The method is simple and admits a nice geometric interpretation. It consists of two steps. First, we construct an appropriate hyperplane which strictly separates the current iterate from the solutions of the problem. This procedure requires a single projection onto the feasible set and employs an Armijo-type linesearch along a feasible direction. Then the next iterate is obtained as the projection of the current iterate onto the intersection of the feasible set with the halfspace containing the solution set. Thus, in contrast with most other projection-type methods, only two projection operations per iteration are needed. The method is shown to be globally convergent to a solution of the variational inequality problem under minimal assumptions. Preliminary computational experience is also reported. 1. Introduction. We consider the classical variational inequality problem [1, 3, 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> A. A. Auslender, </author> <title> Optimisation Methodes Numeriques, </title> <publisher> Masson, </publisher> <address> Paris, </address> <year> 1976. </year>
Reference-contexts: 1. Introduction. We consider the classical variational inequality problem <ref> [1, 3, 7] </ref> VI (F; C), which is to find a point x fl such that x fl 2 C; hF (x fl ); x x fl i 0 for all x 2 C;(1.1) where C is a closed convex subset of &lt; n , h; i denotes the usual inner <p> Since x i 2 C but x i 62 H i , there exist fi 2 <ref> [0; 1] </ref> such that ~x = fix i + (1 fi)y 2 C " @H i , where @H i := fx 2 &lt; n j hF (z i ); x z i i = 0g.
Reference: [2] <author> J. F. Bonnans, </author> <title> Local analysis of Newton-type methods for variational inequalities and nonlinear programming, </title> <journal> Applied Mathematics and Optimization, </journal> <volume> 29 (1994), </volume> <pages> pp. 161-186. </pages>
Reference: [3] <author> R. W. Cottle, F. Giannessi, and J.-L. Lions, </author> <title> Variational Inequalities and Complementarity Problems : Theory and Applications, </title> <publisher> Wiley, </publisher> <address> New York, </address> <year> 1980. </year>
Reference-contexts: 1. Introduction. We consider the classical variational inequality problem <ref> [1, 3, 7] </ref> VI (F; C), which is to find a point x fl such that x fl 2 C; hF (x fl ); x x fl i 0 for all x 2 C;(1.1) where C is a closed convex subset of &lt; n , h; i denotes the usual inner
Reference: [4] <author> S. C. Dafermos, </author> <title> An iterative scheme for variational inequalities, </title> <journal> Mathematical Programming, </journal> <volume> 26 (1983), </volume> <pages> pp. 40-47. </pages>
Reference: [5] <author> M. Fukushima, </author> <title> Equivalent differentiable optimization problems and descent methods for asymmetric variational inequality problems, </title> <journal> Mathematical Programming, </journal> <volume> 53 (1992), </volume> <pages> pp. 99-110. </pages>
Reference: [6] <author> E. M. Gafni and D. P. Bertsekas, </author> <title> Two-metric projection methods for constrained optimization, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 22 (1984), </volume> <pages> pp. 936-964. </pages>
Reference-contexts: The proof then follows the pattern of the proof of Theorem 2.3, with (2.7) replaced by i kr (x i ; i )k 4 : We next use the fact (see <ref> [6, Lemma 1] </ref>) that kr (x i ; i )k minf1; i gkr (x i )k: It follows that kx i+1 x fl k 2 kx i x fl k 2 kx i+1 x i k 2 ( i =M ) 2 2 Taking into account that i = fl k
Reference: [7] <author> R. Glowinski, J.-L. Lions, and R. </author> <title> Tr emoli eres, Numerical Analysis of Variational Inequalities, </title> <publisher> North-Holland, </publisher> <address> Amsterdam, </address> <year> 1981. </year>
Reference-contexts: 1. Introduction. We consider the classical variational inequality problem <ref> [1, 3, 7] </ref> VI (F; C), which is to find a point x fl such that x fl 2 C; hF (x fl ); x x fl i 0 for all x 2 C;(1.1) where C is a closed convex subset of &lt; n , h; i denotes the usual inner
Reference: [8] <author> P. T. Harker and J.-S. Pang, </author> <title> A damped-Newton method for the linear complementarity problem, in Computational Solution of Nonlinear Systems of Equations. Lectures in Applied Mathematics 26, </title> <editor> G. Allgower and K. Georg, eds., </editor> <year> 1990, </year> <pages> pp. 265-284. </pages>
Reference-contexts: For the sixth problem, n = 20 and F () is affine, i.e., F (x) = M x + q; with the matrix M randomly generated as suggested in <ref> [8] </ref> : M = AA &gt; + B + D; PROJECTION METHOD FOR VARIATIONAL INEQUALITIES 9 where every entry of the n fi n matrix A and of the n fi n skew-symmetric matrix B is uniformly generated from (5; 5) and every diagonal entry of the n fi n diagonal
Reference: [9] <author> A. N. Iusem and B. F. Svaiter, </author> <title> A variant of Korpelevich's method for variational inequalities with a new search strategy, </title> <journal> Optimization, </journal> <volume> 42 (1997), </volume> <pages> pp. 309-321. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in <ref> [10, 12, 17, 33, 9] </ref>. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). <p> The same holds for the modified projection-type method in [32]. Clearly, this can be very computationally expensive. A novel idea to get around this inefficiency was proposed in <ref> [9] </ref> for the extragradient method. Here we will use this idea to devise a new projection algorithm which has even better properties, both theoretically and in our computational experience. The algorithm proposed here allows a nice geometric interpretation which is given in Figure 1.1 for its simplest version. <p> At each iteration, our algorithm uses one projection onto the set C (to construct the separating hyperplane H i ), and one projection onto the intersection C " H i which gives the next iterate. Before proceeding, we emphasize the differences between the method of <ref> [9] </ref> and our Algorithms 2.1 and 2.2. First of all, the second projection step in our method is onto the intersection C " H i . In [9], first x i is projected onto the separating hyperplane @H i (this point is denoted by x i on Figure 1.1) and then <p> Before proceeding, we emphasize the differences between the method of <ref> [9] </ref> and our Algorithms 2.1 and 2.2. First of all, the second projection step in our method is onto the intersection C " H i . In [9], first x i is projected onto the separating hyperplane @H i (this point is denoted by x i on Figure 1.1) and then onto C (on Figure 1.1, the resulting point is denoted by P C [x i ]). <p> It can be verified that our iterate x i+1 is closer to the solution set S than the iterate computed by the method of <ref> [9] </ref>. We also avoid the extra work of computing the point x i (even though it can be carried out explicitly, this is still some extra work). Furthermore, the search direction in our method is not the same as in [9] for the following reasons. <p> set S than the iterate computed by the method of <ref> [9] </ref>. We also avoid the extra work of computing the point x i (even though it can be carried out explicitly, this is still some extra work). Furthermore, the search direction in our method is not the same as in [9] for the following reasons. The search directions we use here are P C [x i i F (x i )] x i , where the stepsizes i are chosen so that they are of the same order as the stepsizes i generating separating hyperplanes H i (see Algorithm 2.2). <p> The important point is that we allow both of them to go to zero if needed (but at the same rate). Coordination of the two stepsizes proves to be very significant in our computational experience (see Section 3). We point out that <ref> [9] </ref> does not permit the stepsizes go to zero in the first projection step even if the stepsizes generating the hyperplanes go to zero, and the proof there does not handle this case. <p> The above mentioned modifications seem to make a drastic difference in the numerical performance when our algorithm is compared to that of <ref> [9] </ref>. Our preliminary computational experience with the new algorithm is quite encouraging and is reported in Section 3. However, we emphasize that comprehensive numerical study is not the primary focus of this paper. <p> However, we emphasize that comprehensive numerical study is not the primary focus of this paper. Finally, our convergence results are stated under the assumption (1.2) which is considerably weaker than monotonicity of F () used in <ref> [9] </ref>. 2. The algorithm and its convergence. We first note that solutions of VI (F; C) coincide with zeroes of the following projected residual function r (x) := x P C [x F (x)]; that is x 2 S if and only if r (x) = 0. <p> For example, both of them can go to zero if needed. This is in contrast with the method of <ref> [9] </ref> where the stepsize in the first projection step can never go to zero, even when i does. We found this coordination mechanism important in our computational results reported in Section 3. Note also that both stepsizes can increase from one iteration to the next. Algorithm 2.2. <p> For benchmark, we compared the performance of this implementation with analogous implementations of two versions of the extragradient method (as described in [17] and [33]), and with the modified projection algorithm as given in [32, Algorithm 3.2]. We also implemented the algorithm of <ref> [9] </ref> and tested it on the same problems as the other four methods. We do not report here the full results for the method of [9], mainly because they were rather poor. In particular, like the extragradient method, the method of [9] failed on the first two problems, and was by <p> We also implemented the algorithm of <ref> [9] </ref> and tested it on the same problems as the other four methods. We do not report here the full results for the method of [9], mainly because they were rather poor. In particular, like the extragradient method, the method of [9] failed on the first two problems, and was by far the worst among all the methods on the remaining test problems. <p> We also implemented the algorithm of <ref> [9] </ref> and tested it on the same problems as the other four methods. We do not report here the full results for the method of [9], mainly because they were rather poor. In particular, like the extragradient method, the method of [9] failed on the first two problems, and was by far the worst among all the methods on the remaining test problems. Thus we found it to be not useful for benchmark comparison (unfortunately, no computational experience was reported in [9]). <p> In particular, like the extragradient method, the method of <ref> [9] </ref> failed on the first two problems, and was by far the worst among all the methods on the remaining test problems. Thus we found it to be not useful for benchmark comparison (unfortunately, no computational experience was reported in [9]). By contrast, our algorithm seems to perform better than the alternatives in most cases. The choice of linearly constrained variational inequalities for our experiments is not incidental.
Reference: [10] <author> A. N. Iusem, </author> <title> An iterative algorithm for the variational inequality problem, </title> <journal> Computational and Applied Mathematics, </journal> <volume> 13 (1994), </volume> <pages> pp. 103-114. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in <ref> [10, 12, 17, 33, 9] </ref>. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). <p> We note that in the case when F () is not Lipschitz continuous or the Lipschitz constant is not known, the extragradient method, as described in <ref> [12, 17, 10, 33] </ref>, requires a linesearch procedure to compute the stepsize with a new projection needed for each trial point. The same holds for the modified projection-type method in [32]. Clearly, this can be very computationally expensive.
Reference: [11] <author> S. Karamardian, </author> <title> Complementarity problems over cones with monotone and pseudomonotone maps, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 18 (1976), </volume> <pages> pp. 445-455. </pages>
Reference-contexts: x fl i 0 for all x 2 C:(1.2) It is clear that (1.2) is satisfied if F () is monotone, i.e., hF (x) F (y); x yi 0 for all x; y 2 &lt; n : More generally, (1.2) also holds if F () is pseudomonotone (as defined in <ref> [11] </ref>), i.e., for all x; y 2 &lt; n Moreover, it is not difficult to construct examples where (1.2) is satisfied but F () is not monotone or pseudomonotone everywhere.
Reference: [12] <author> E. N. Khobotov, </author> <title> A modification of the extragradient method for the solution of variational inequalities and some optimization problems, </title> <journal> USSR Computational Mathematics and Mathematical Physics, </journal> <volume> 27 (1987), </volume> <pages> pp. 1462-1473. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in <ref> [10, 12, 17, 33, 9] </ref>. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). <p> We note that in the case when F () is not Lipschitz continuous or the Lipschitz constant is not known, the extragradient method, as described in <ref> [12, 17, 10, 33] </ref>, requires a linesearch procedure to compute the stepsize with a new projection needed for each trial point. The same holds for the modified projection-type method in [32]. Clearly, this can be very computationally expensive.
Reference: [13] <author> G. M. Korpelevich, </author> <title> The extragradient method for finding saddle points and other problems, </title> <journal> Matecon, </journal> <volume> 12 (1976), </volume> <pages> pp. 747-756. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in <ref> [13] </ref> and later refined and extended in [10, 12, 17, 33, 9]. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein).
Reference: [14] <author> T. L. Magnanti and G. Perakis, </author> <title> On the convergence of classical variational inequality algorithms, </title> <type> working paper, </type> <institution> Operations Research Center, MIT, Cambridge, Massachusetts, </institution> <month> May </month> <year> 1993. </year>
Reference: [15] <author> O. L Mangasarian and M. V. Solodov, </author> <title> A linearly convergent derivative-free descent method for strongly monotone complementarity problems. Computational Optimization and Applications, to appear. [16] , Nonlinear complementarity as unconstrained and constrained minimization, </title> <journal> Mathematical Programming, </journal> <volume> 62 (1993), </volume> <pages> pp. 277-297. </pages>
Reference: [17] <author> P. Marcotte, </author> <title> Application of Khobotov's algorithm to variational inequalities and network equilibrium problems, </title> <journal> Information Systems and Operational Research, </journal> <volume> 29 (1991), </volume> <pages> pp. 258-270. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in <ref> [10, 12, 17, 33, 9] </ref>. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). <p> We note that in the case when F () is not Lipschitz continuous or the Lipschitz constant is not known, the extragradient method, as described in <ref> [12, 17, 10, 33] </ref>, requires a linesearch procedure to compute the stepsize with a new projection needed for each trial point. The same holds for the modified projection-type method in [32]. Clearly, this can be very computationally expensive. <p> For benchmark, we compared the performance of this implementation with analogous implementations of two versions of the extragradient method (as described in <ref> [17] </ref> and [33]), and with the modified projection algorithm as given in [32, Algorithm 3.2]. We also implemented the algorithm of [9] and tested it on the same problems as the other four methods. <p> Though our experience is limited in scope, it suggests that the new projection method is a valuable alternative to the extragradient <ref> [17, 33] </ref> and modified projection [32] methods. We describe the test details below. All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. <p> In the implementation of our Algorithm 2.2, we choose = :3, 1 = 1, fl = :5 and = 4. Implementations of the modified projection method and the extragradient method of <ref> [17] </ref> are the same as reported in [32]. In particular, the parameters for both algorithms were tuned to optimize the performance. In the implementation of the other version of the extragradient method [33, Algorithm C], we set parameters as follows : = :1, ff = :3 and fl = 1:7. <p> In general, the more positive components x 0 has, the least iterations the modified projection method needs to solve the problem. The extragradient method of Marcotte <ref> [17] </ref> managed to solve the problem for all starting points, with the average of 62 iterations (which is again considerably higher than 43 iterations needed when starting at the unit vector). <p> Extragradient <ref> [17] </ref> 4 Extragradient [33] 5 Name n iter.(nf=np) 3 CPU iter.(nf=np) 3 CPU Mathiesen 3 - | - KojimaSh 4 16 (36=36) 0.5 78 (157=79) 2.5 Nash5 5 43 (89=89) 1.8 92 (184=93) 2.8 Nash10 10 84 (172=172) 3.4 103 (191=172) 5.5 HPHard 20 532 (1067=1067) 163 1003 (2607=1607) 562 qHPHard <p> On the Mathiesen problem, we ran each method twice with x 0 = (:1; :8; :1) and x 0 = (:4; :3; :3) respectively; on the other problems, we used x 0 = (1; :::; 1). 4 The extragradient method as described in <ref> [17] </ref>, with fi = :7 and initial ff = 1. Dash | indicates that the method did not converge. 5 The extragradient method as described in [33, Algorithm C], with = :1, ff = :3, fl = 1:7. Dash | indicates that the method did not converge.
Reference: [18] <author> P. Marcotte and J.-P. Dussault, </author> <title> A note on a globally convergent Newton method for solving monotone variational inequalities, </title> <journal> Operations Research Letters, </journal> <volume> 6 (1987), </volume> <pages> pp. </pages> <month> 35-42. </month> <title> [19] , A sequential linear programming algorithm for solving monotone variational inequalities, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 27 (1989), </volume> <pages> pp. 1260-1278. </pages>
Reference: [20] <author> P. Marcotte and J.-H. Wu, </author> <title> On the convergence of projection methods: application to the decomposition of affine variational inequalities, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 85 (1995), </volume> <pages> pp. 347-362. </pages>
Reference: [21] <author> L. Mathiesen, </author> <title> An algorithm based on a sequence of linear complementarity problems applied to a Walrasian equilibrium model: An example, </title> <journal> Mathematical Programming, </journal> <volume> 37 (1987), </volume> <pages> pp. 1-18. </pages>
Reference-contexts: We describe the test details below. All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. The first test problem, used first by Mathiesen <ref> [21] </ref>, and later in [24, 36], has F (x 1 ; x 2 ; x 3 ) = [ : 9 (5x 2 + 3x 3 )=x 1 :1 (5x 2 + 3x 3 )=x 2 5 3] ; + j x 1 + x 2 + x 3 = 1;
Reference: [22] <author> J.-S. Pang, </author> <title> Asymmetric variational inequality problems over product sets: applications and iterative methods, </title> <journal> Mathematical Programming, </journal> <volume> 31 (1985), </volume> <pages> pp. 206-219. </pages>
Reference: [23] <author> J.-S. Pang and D. Chan, </author> <title> Iterative methods for variational and complementarity problems, </title> <journal> Mathematical Programming, </journal> <volume> 24 (1982), </volume> <pages> pp. 284-313. </pages>
Reference: [24] <author> J.-S. Pang and S. A. Gabriel, NE/SQP: </author> <title> A robust algorithm for the nonlinear complemen 12 M. </title> <editor> V. SOLODOV AND B. F. </editor> <booktitle> SVAITER tarity problem, Mathematical Programming, 60 (1993), </booktitle> <pages> pp. 295-337. </pages>
Reference-contexts: All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. The first test problem, used first by Mathiesen [21], and later in <ref> [24, 36] </ref>, has F (x 1 ; x 2 ; x 3 ) = [ : 9 (5x 2 + 3x 3 )=x 1 :1 (5x 2 + 3x 3 )=x 2 5 3] ; + j x 1 + x 2 + x 3 = 1; x 1 x 2 <p> For the third to fifth problems, F () is the function from, respectively, the Kojima-Shindo NCP (with n = 4) and the Nash-Cournot NCP (with n = 5 and n = 10) <ref> [24, pp. 321-322] </ref>.
Reference: [25] <author> R. Pini and C. Singh, </author> <title> A survey of recent (1985-1995) advances in generalized convexity with applications to duality theory and optimality conditions, </title> <journal> Optimization, </journal> <year> (1997). </year>
Reference-contexts: Typically, condition (1.2) holds under some kind of generalized monotonicity assumptions on F (), some of which are not difficult to check (see <ref> [26, 25] </ref>). In the case when F () is strongly monotone and/or the feasible set C has some special structure (e.g.
Reference: [26] <author> S. Schaible, S. Karamardian, and J.-P. Crouzeix, </author> <title> Characterizations of generalized monotone maps, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 76 (1993), </volume> <pages> pp. 399-413. </pages>
Reference-contexts: Typically, condition (1.2) holds under some kind of generalized monotonicity assumptions on F (), some of which are not difficult to check (see <ref> [26, 25] </ref>). In the case when F () is strongly monotone and/or the feasible set C has some special structure (e.g.
Reference: [27] <author> M. V. Solodov, </author> <title> Implicit Lagrangian. An invited article for the Encyclopedia of Optmimization, </title> <editor> C. Floudas and P. Pardalos (editors), </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1999. </year> <title> [28] , Some optimization reformulations of the extended linear complementarity problem. Computational Optimization and Applications, to appear. [29] , Stationary points of bound constrained reformulations of complementarity problems, </title> <journal> Journal of Optimization Theory and Applications, </journal> <volume> 94 (1997), </volume> <pages> pp. 449-467. </pages>
Reference: [30] <author> M. V. Solodov and B. F. Svaiter, </author> <title> A globally convergent inexact Newton method for systems of monotone equations, in Reformulation - Nonsmooth, Piecewise Smooth, Semismooth and Smoothing Methods, </title> <editor> M. Fukushima and L. Qi, eds., </editor> <publisher> Kluwer Academic Publishers, </publisher> <year> 1998, </year> <pages> pp. </pages> <month> 355-369. </month> <title> [31] , A truly globally convergent Newton-type method for the monotone nonlinear complementarity problem, </title> <note> 1998. SIAM Journal on Optimization, submitted. </note>
Reference-contexts: Some of the projection ideas presented here also proved to be useful in devising truly globally convergent (i.e. the whole sequence of iterates is globally convergent to a solution without any regularity assumptions) and locally superlinearly convergent inexact Newton methods for solving systems of monotone equations <ref> [30] </ref> and monotone nonlinear complementarity problems [31]. 1 Algorithm 2.2 with = :3, 1 = 1, fl = :5 and = 4. 2 Modified projection method as described in [32, Algorithm 3.2] and parameters set as reported in that reference (P = I, ff 1 = 1, = 1:5, = :1
Reference: [32] <author> M. V. Solodov and P. Tseng, </author> <title> Modified projection-type methods for monotone variational inequalities, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 34 (1996), </volume> <pages> pp. 1814-1830. </pages>
Reference-contexts: In the general case when F () and C do not possess any special structure, relatively few methods are applicable. In that case, projection-type algorithms are of particular relevance (we refer the reader to <ref> [32] </ref> for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in [10, 12, 17, 33, 9]. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also <p> reader to <ref> [32] </ref> for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in [10, 12, 17, 33, 9]. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). In this paper, we are mainly concerned with the general case when the projection operator P C [x] := arg min ky xk is computationally expensive (i.e., one has to solve an optimization problem to find a projection). <p> The same holds for the modified projection-type method in <ref> [32] </ref>. Clearly, this can be very computationally expensive. A novel idea to get around this inefficiency was proposed in [9] for the extragradient method. Here we will use this idea to devise a new projection algorithm which has even better properties, both theoretically and in our computational experience. <p> The idea is to use for the first projection step at the current iteration the stepsize which is not too different from the stepsize computed at the previous iteration (a similar technique was also used in <ref> [32, Algorithm 3.2] </ref>). Note that Algorithm 2.2 has PROJECTION METHOD FOR VARIATIONAL INEQUALITIES 7 a certain coordination between the stepsizes i in the first projection step and i in the step computing the separating hyperplane. For example, both of them can go to zero if needed. <p> For benchmark, we compared the performance of this implementation with analogous implementations of two versions of the extragradient method (as described in [17] and [33]), and with the modified projection algorithm as given in <ref> [32, Algorithm 3.2] </ref>. We also implemented the algorithm of [9] and tested it on the same problems as the other four methods. We do not report here the full results for the method of [9], mainly because they were rather poor. <p> Though our experience is limited in scope, it suggests that the new projection method is a valuable alternative to the extragradient [17, 33] and modified projection <ref> [32] </ref> methods. We describe the test details below. All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. <p> it suggests that the new projection method is a valuable alternative to the extragradient [17, 33] and modified projection <ref> [32] </ref> methods. We describe the test details below. All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. <p> In the implementation of our Algorithm 2.2, we choose = :3, 1 = 1, fl = :5 and = 4. Implementations of the modified projection method and the extragradient method of [17] are the same as reported in <ref> [32] </ref>. In particular, the parameters for both algorithms were tuned to optimize the performance. In the implementation of the other version of the extragradient method [33, Algorithm C], we set parameters as follows : = :1, ff = :3 and fl = 1:7. <p> We have chosen the problem Nash5 because its function is defined on the positive orthant only, so that the boundary effect can also be studied. The modified projection method of <ref> [32] </ref> had trouble when starting at points with many zero components (such as, for example, the point (5; 0; 0; 0; 0)). This is not very surprising, since the modified projection is actually an infeasible method, i.e., the iterates generated need not belong to the feasible set. <p> On average, 32 iterations were required for convergence, which is less than for the modified projection or the extragradient methods. However, the extragradient method appeared to be somewhat more robust for this problem. 10 M. V. SOLODOV AND B. F. SVAITER Algorithm 2.2 1 Modfied Projection <ref> [32] </ref> 2 Name n iter.(nf=np) 3 CPU iter.(nf=np) 3 CPU Mathiesen 3 14 (53=28) 0.5 30 (68=38) 0.9 14 (55=28) 0.5 25 (56=31) 0.7 KojimaSh 4 7 (16=14) 0.3 38 (84=46) 0.7 Nash5 5 24 (100=48) 1.5 74 (155=81) 1.9 Nash10 10 34 (140=68) 3 93 (192=99) 2.7 HPHard 20 379 <p> convergent to a solution without any regularity assumptions) and locally superlinearly convergent inexact Newton methods for solving systems of monotone equations [30] and monotone nonlinear complementarity problems [31]. 1 Algorithm 2.2 with = :3, 1 = 1, fl = :5 and = 4. 2 Modified projection method as described in <ref> [32, Algorithm 3.2] </ref> and parameters set as reported in that reference (P = I, ff 1 = 1, = 1:5, = :1 and fi = :3). 3 For all methods, the termination criterion is kr (x)k 10 4 . (nf denotes the total number of times F () is evaluated and
Reference: [33] <author> D. Sun, </author> <title> A new step-size skill for solving a class of nonlinear projection equations, </title> <journal> Journal of Computational Mathematics, </journal> <volume> 13 (1995), </volume> <pages> pp. 357-368. </pages>
Reference-contexts: In that case, projection-type algorithms are of particular relevance (we refer the reader to [32] for a more detailed discussion). The oldest algorithm of this class is the extragradient method proposed in [13] and later refined and extended in <ref> [10, 12, 17, 33, 9] </ref>. Some new projection-type algorithms which appear to be more efficient than the extragradient method, were recently introduced in [32] (see also references therein). <p> We note that in the case when F () is not Lipschitz continuous or the Lipschitz constant is not known, the extragradient method, as described in <ref> [12, 17, 10, 33] </ref>, requires a linesearch procedure to compute the stepsize with a new projection needed for each trial point. The same holds for the modified projection-type method in [32]. Clearly, this can be very computationally expensive. <p> For benchmark, we compared the performance of this implementation with analogous implementations of two versions of the extragradient method (as described in [17] and <ref> [33] </ref>), and with the modified projection algorithm as given in [32, Algorithm 3.2]. We also implemented the algorithm of [9] and tested it on the same problems as the other four methods. <p> Though our experience is limited in scope, it suggests that the new projection method is a valuable alternative to the extragradient <ref> [17, 33] </ref> and modified projection [32] methods. We describe the test details below. All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. <p> Implementations of the modified projection method and the extragradient method of [17] are the same as reported in [32]. In particular, the parameters for both algorithms were tuned to optimize the performance. In the implementation of the other version of the extragradient method <ref> [33, Algorithm C] </ref>, we set parameters as follows : = :1, ff = :3 and fl = 1:7. <p> Extragradient [17] 4 Extragradient <ref> [33] </ref> 5 Name n iter.(nf=np) 3 CPU iter.(nf=np) 3 CPU Mathiesen 3 - | - KojimaSh 4 16 (36=36) 0.5 78 (157=79) 2.5 Nash5 5 43 (89=89) 1.8 92 (184=93) 2.8 Nash10 10 84 (172=172) 3.4 103 (191=172) 5.5 HPHard 20 532 (1067=1067) 163 1003 (2607=1607) 562 qHPHard 20 461 (926=925) <p> Dash | indicates that the method did not converge. 5 The extragradient method as described in <ref> [33, Algorithm C] </ref>, with = :1, ff = :3, fl = 1:7. Dash | indicates that the method did not converge. PROJECTION METHOD FOR VARIATIONAL INEQUALITIES 11 Acknowledgments. We are grateful to the two anonymous referees and the Editor for constructive suggestions which led to improvements in the paper.
Reference: [34] <author> P. Tseng, </author> <title> Applications of a splitting algorithm to decomposition in convex programming and variational inequalities, </title> <journal> SIAM Journal on Control and Optimization, </journal> <volume> 29 (1991), </volume> <pages> pp. 119-138. </pages>
Reference: [35] <author> E. H. Zarantonello, </author> <title> Projections on convex sets in Hilbert space and spectral theory, in Contributions to Nonlinear Functional Analysis, </title> <editor> E. Zarantonello, ed., </editor> <publisher> Academic Press, </publisher> <address> New York, New York, </address> <year> 1971, </year> <pages> pp. 237-424. </pages>
Reference-contexts: V. SOLODOV AND B. F. SVAITER where H i = fx 2 &lt; n j hF (z i ); x z i i 0g: The following well-known properties of the projection operator will be used in the sequel. Lemma 2.1. <ref> [35] </ref> Let B be any nonempty closed convex set in &lt; n . For any x; y 2 &lt; n and any z 2 B the following properties hold. 1. hx P B [x]; z P B [x]i 0. We start with a preliminary result.
Reference: [36] <author> L. Zhao and S. Dafermos, </author> <title> General economic equilibrium and variational inequalities, </title> <journal> Operations Research Letters, </journal> <volume> 10 (1991), </volume> <pages> pp. 369-376. </pages>
Reference-contexts: All Matlab codes were run on the Sun UltraSPARCstation 1 under Matlab version 5.0.0.4064. Our test problems are the same as used in [32] to test the modified projection method in the nonlinear case. The first test problem, used first by Mathiesen [21], and later in <ref> [24, 36] </ref>, has F (x 1 ; x 2 ; x 3 ) = [ : 9 (5x 2 + 3x 3 )=x 1 :1 (5x 2 + 3x 3 )=x 2 5 3] ; + j x 1 + x 2 + x 3 = 1; x 1 x 2 <p> In the implementation of the other version of the extragradient method [33, Algorithm C], we set parameters as follows : = :1, ff = :3 and fl = 1:7. On the Mathiesen problem, we used the same x 0 as in <ref> [36] </ref>; on the other problems, we used x 0 = (1; :::; 1). (The F () from the Mathiesen problem and from the Nash-Cournot NCP are defined on the positive orthant only.) The test results are summarized in Tables 3.1 and 3.2.
References-found: 31

