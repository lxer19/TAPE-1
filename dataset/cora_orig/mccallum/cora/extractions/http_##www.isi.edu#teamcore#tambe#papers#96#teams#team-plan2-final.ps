URL: http://www.isi.edu/teamcore/tambe/papers/96/teams/team-plan2-final.ps
Refering-URL: http://www.isi.edu/teamcore/tambe/agent.html
Root-URL: http://www.isi.edu
Email: tambe@isi.edu  
Title: Executing Team Plans in Dynamic, Multi-agent Domains  
Author: Milind Tambe 
Web: http://www.isi.edu/soar/tambe  
Address: 4676 Admiralty Way, Marina del Rey, CA 90292  
Affiliation: Information Sciences Institute University of Southern California  
Abstract: This paper focuseson flexible teamwork in dynamic and real-world multi-agent domains. Such teamwork is not simply a union of agents' simultaneous execution of individual plans, even if such execution pre-coordinated. Indeed, uncertainties in complex, dynamic domains often obstruct pre-planned coordination, with a resultant breakdown in teamwork. The central hypothesis in this paper is that for durable teamwork, agents should be provided explicit team plans, which directly express a team's joint activities. When agents execute such team plans, they abide by certain commonsense conventions of teamwork. Essentially, such conventions provide a deeper model of teamwork, facilitating flexible reasoning about coordination activities. Such a framework also frees the planner or the knowledge engineer from specifying very detailed low-level coordination plans. This framework has been implemented in the context of a real-world synthetic environment for helicopter-combat simulation. 1 
Abstract-found: 1
Intro-found: 1
Reference: <author> Bates, J.; Loyall, A. B.; and Reilly, W. S. </author> <year> 1992. </year> <title> Integrating reactivity, goals and emotions in a broad agent. </title> <type> Technical Report CMU-CS-92-142, </type> <institution> School of Computer Science, Carnegie Mellon University. </institution>
Reference-contexts: 1 Introduction Many AI researchers are today striving to build agents for complex, dynamic multi-agent domains. Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill (Pimentel & Teixeira 1994) or combat (Tambe et al. 1995)), virtual interactive fiction <ref> (Bates, Loyall, & Reilly 1992) </ref> and RoboCup robotic and virtual soccer (Kitano et al. 1995). This paper focuses on plan execution in such dynamic, multi-agent domains. While this topic is well-investigated in the literature, most research has focused on individuals rather than agent teams.
Reference: <author> Cohen, P. R., and Levesque, H. J. </author> <year> 1991. </year> <note> Teamwork. Nous 35. </note>
Reference-contexts: Thus, an individual agent directly executes reactive team plans, which may hierarchically expand out into reactive plans for its role in the team. These team plans are based on the joint intentions framework <ref> (Cohen & Levesque 1991) </ref>, which forms the basis of certain commonsense conventions for teamwork although these conventions have been modified to accommodate the constraints that appear typical in some real-world dynamic domains. 2 When agents execute such team plans, they abide by these teamwork conventions. <p> In such cases, the team member must have this private belief become mutual belief <ref> (Cohen & Levesque 1991) </ref>; and the joint venture is now dissolved. Establishing such mutual beliefs often requires an agent to create a communicative goal.
Reference: <author> Davis, E. </author> <year> 1990. </year> <title> Representations of commonsense knowledge. </title> <address> Palo Alto, CA: </address> <publisher> Morgan Kaufmann. </publisher>
Reference-contexts: As mentioned above, such an operator hierarchy can be seen as a hierarchical reactive plan. 2 Commonsense is to be understood here as the common knowledge about the (social) world that is possessed by every schoolchild and the methods for making obvious inferences from this knowledge <ref> (Davis 1990) </ref>. 2 Domain and Initial Experiences The domain of our work is a real-world battlefield simulator, commercially developed for the military for training (Tambe et al. 1995). We are building intelligent pilot agents for synthetic aircraft in this environment.
Reference: <author> Firby, J. </author> <year> 1987. </year> <title> An investigation into reactive planning in complex domains. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: In the case of individuals, the difficulty in traditional plan-execution i.e., execution of a rigid precomputed list of actions is now well-recognized. Instead, individual agents in dynamic environments are often based on hierarchical reactive plans <ref> (Firby 1987) </ref>. For instance, agents built in PRS (Ingrand et al. 1992), RAP (Firby 1987), BB1 (Hayes-Roth, Brownston, & Gen 1995), Soar (Newell 1990) and other architectures in 1 This research was supported as part of contract N66001-95-C-6013 from ARPA/ISO. Domain expertise was provided by Dave Sullivan of BMH Inc. <p> In the case of individuals, the difficulty in traditional plan-execution i.e., execution of a rigid precomputed list of actions is now well-recognized. Instead, individual agents in dynamic environments are often based on hierarchical reactive plans <ref> (Firby 1987) </ref>. For instance, agents built in PRS (Ingrand et al. 1992), RAP (Firby 1987), BB1 (Hayes-Roth, Brownston, & Gen 1995), Soar (Newell 1990) and other architectures in 1 This research was supported as part of contract N66001-95-C-6013 from ARPA/ISO. Domain expertise was provided by Dave Sullivan of BMH Inc.
Reference: <author> Grosz, B. J., and Sidner, C. L. </author> <year> 1990. </year> <title> Plans for discourse. In Intentions in Communication. </title> <address> Cambridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 417-445. </pages>
Reference-contexts: Such teamwork cannot necessarily be decomposed as coordinated individual activities. Consider the example of two children collaboratively building a single tower of blocks they are not coordinating to build two separate towers of blocks with gaps in just the right places <ref> (Grosz & Sidner 1990) </ref>. In soccer, two teammates together can execute a wall pass to dodge an opponent; however, no individual can execute a wall pass in isolation.
Reference: <author> Hayes-Roth, B.; Brownston, L.; and Gen, R. V. </author> <year> 1995. </year> <title> Multiagent collaobration in directed improvisation. </title> <booktitle> In Proceedings of the International Conference on Multi-Agent Systems (ICMAS-95). </booktitle>
Reference: <author> Huber, M., and Durfee, E. </author> <year> 1995. </year> <title> On acting together: Without communication. </title> <booktitle> In Proceedings of the AAAI Spring Symposium on Reasoning about Mental states. </booktitle>
Reference-contexts: We are aware of only two other efforts to operationalize the joint intentions framework Jennings's work on an industrial multi-agent setting (Jennings 1995), Huber and Durfee's effort in a testbed domain <ref> (Huber & Durfee 1995) </ref>. Both suggest modifications to the framework in the course of its operationalization. However, there are several key differences.
Reference: <author> Ingrand, F. F.; Georgeff, M. P.; ; and Rao, A. S. </author> <year> 1992. </year> <title> An architecture for real-time reasoning and system control. </title> <journal> IEEE EXPERT 7(6). </journal>
Reference-contexts: In the case of individuals, the difficulty in traditional plan-execution i.e., execution of a rigid precomputed list of actions is now well-recognized. Instead, individual agents in dynamic environments are often based on hierarchical reactive plans (Firby 1987). For instance, agents built in PRS <ref> (Ingrand et al. 1992) </ref>, RAP (Firby 1987), BB1 (Hayes-Roth, Brownston, & Gen 1995), Soar (Newell 1990) and other architectures in 1 This research was supported as part of contract N66001-95-C-6013 from ARPA/ISO. Domain expertise was provided by Dave Sullivan of BMH Inc.
Reference: <author> Jennings, N. </author> <year> 1995. </year> <title> Controlling cooperative problem solving in industrial multi-agent systems using joint intentions. </title> <booktitle> Artificial Intelligence 75. </booktitle>
Reference-contexts: Individual agents are often provided individual plans to achieve individual goals, with detailed precomputed plans for coordination and communication; team activities are either not represented explicitly, or rely on shallow rules <ref> (Jennings 1995) </ref>. However, an analogy appears to hold: just as detailed precomputed plans were deemed inadequate for an individual in dynamic domains, rigid precomputed coordination actions appear inadequate to enable flexible teamwork in dynamic environments. <p> Next, Cheetah42, the next in command, will replace Cheetah41 as the commander. 5 Related Work Few other research efforts have implemented theories of joint or collaborative action. We are aware of only two other efforts to operationalize the joint intentions framework Jennings's work on an industrial multi-agent setting <ref> (Jennings 1995) </ref>, Huber and Durfee's effort in a testbed domain (Huber & Durfee 1995). Both suggest modifications to the framework in the course of its operationalization. However, there are several key differences. <p> However, lower-level team operators may be affected, and individuals reason about both the team operator unachievability and repair. Repair is important, else the en tire team effort will go to waste. Finally, in <ref> (Jennings 1995) </ref> issues of communication risk and cost are not considered. Our recent work on team tracking (Tambe 1996) which involves inferring other team's joint goals and intentions based on observations of their actions is the predecessor to the work reported here. <p> Agents need not be provided with detailed, low-level coordination plans. Other potential advantages of such a framework include: (i) Improved self-explanations, e.g., an agent can better explain the reasons underlying its communication; and (ii) Guidance for knowledge acquisition, e.g., the framework helps to avoid omissions <ref> (Jennings 1995) </ref>. For a further understanding of the issues involved in more complex teams, we are currently implementing this framework for agents in the RoboCup virtual soccer tournament (Kitano et al. 1995).
Reference: <author> Kinny, D.; Ljungberg, M.; Rao, A.; Sonenberg, E.; Tid-hard, G.; and Werner, E. </author> <year> 1992. </year> <title> Planned team activity. </title> <editor> In Castelfranchi, C., and Werner, E., eds., </editor> <booktitle> Artificial Social Systems, Lecture notes in AI 830. </booktitle> <publisher> Springer Verlag, </publisher> <address> New York. </address>
Reference: <author> Kitano, H.; Asada, M.; Kuniyoshi, Y.; Noda, I.; and Os-awa, E. </author> <year> 1995. </year> <title> Robocup: The robot world cup initiative. </title> <booktitle> In Proceedings of IJCAI-95 Workshop on Entertainment and AI/Alife. </booktitle>
Reference-contexts: Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill (Pimentel & Teixeira 1994) or combat (Tambe et al. 1995)), virtual interactive fiction (Bates, Loyall, & Reilly 1992) and RoboCup robotic and virtual soccer <ref> (Kitano et al. 1995) </ref>. This paper focuses on plan execution in such dynamic, multi-agent domains. While this topic is well-investigated in the literature, most research has focused on individuals rather than agent teams. <p> For a further understanding of the issues involved in more complex teams, we are currently implementing this framework for agents in the RoboCup virtual soccer tournament <ref> (Kitano et al. 1995) </ref>.
Reference: <author> Levesque, H. J.; Cohen, P. R.; and Nunes, J. </author> <year> 1990. </year> <title> On acting together. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence. </booktitle> <address> Menlo Park, Calif.: </address> <publisher> AAAI press. </publisher>
Reference: <author> Newell, A. </author> <year> 1990. </year> <title> Unified Theories of Cognition. </title> <address> Cam-bridge, Mass.: </address> <publisher> Harvard Univ. Press. </publisher>
Reference-contexts: Instead, individual agents in dynamic environments are often based on hierarchical reactive plans (Firby 1987). For instance, agents built in PRS (Ingrand et al. 1992), RAP (Firby 1987), BB1 (Hayes-Roth, Brownston, & Gen 1995), Soar <ref> (Newell 1990) </ref> and other architectures in 1 This research was supported as part of contract N66001-95-C-6013 from ARPA/ISO. Domain expertise was provided by Dave Sullivan of BMH Inc. Thanks to Wei-Min Shen for valuable comments on an earlier draft of this paper. dynamic domains may be characterized in this fashion.
Reference: <author> Pimentel, K., and Teixeira, K. </author> <year> 1994. </year> <title> Virtual reality: Through the new looking glass. Blue Ridge Summit, </title> <address> PA: Windcrest/McGraw-Hill. </address>
Reference-contexts: 1 Introduction Many AI researchers are today striving to build agents for complex, dynamic multi-agent domains. Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill <ref> (Pimentel & Teixeira 1994) </ref> or combat (Tambe et al. 1995)), virtual interactive fiction (Bates, Loyall, & Reilly 1992) and RoboCup robotic and virtual soccer (Kitano et al. 1995). This paper focuses on plan execution in such dynamic, multi-agent domains.
Reference: <author> Rosenbloom, P. S.; Laird, J. E.; Newell, A.; ; and McCarl, R. </author> <year> 1991. </year> <title> A preliminary analysis of the soar architecture as a basis for general intelligence. </title> <journal> Artificial Intelligence 47(1-3):289-325. </journal>
Reference: <author> Searle, J. R. </author> <year> 1990. </year> <title> Collective intention and action. </title> <address> Cam-bridge, MA: </address> <publisher> MIT Press. </publisher> <pages> 401-415. </pages>
Reference: <author> Tambe, M., and Rosenbloom, P. S. </author> <year> 1995. </year> <title> RESC: An approach for real-time, dynamic agent tracking. </title> <booktitle> In Proceedings of the International Joint Conference on Artificial Intelligence (IJCAI). </booktitle>
Reference-contexts: 1 Introduction Many AI researchers are today striving to build agents for complex, dynamic multi-agent domains. Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill (Pimentel & Teixeira 1994) or combat <ref> (Tambe et al. 1995) </ref>), virtual interactive fiction (Bates, Loyall, & Reilly 1992) and RoboCup robotic and virtual soccer (Kitano et al. 1995). This paper focuses on plan execution in such dynamic, multi-agent domains. <p> understood here as the common knowledge about the (social) world that is possessed by every schoolchild and the methods for making obvious inferences from this knowledge (Davis 1990). 2 Domain and Initial Experiences The domain of our work is a real-world battlefield simulator, commercially developed for the military for training <ref> (Tambe et al. 1995) </ref>. We are building intelligent pilot agents for synthetic aircraft in this environment. These pilot agents have participated in large scale combat exercises, some involving expert human pilots. <p> These pilot agents have participated in large scale combat exercises, some involving expert human pilots. This paper will focus on pilot agents for a company of (up to eight) attack helicopters, which execute their mission in a synthetic 3D terrain, complete with hills, valleys, roads and ridges <ref> (Tambe, Schwamb, & Rosenbloom 1995) </ref>. As shown in Figure 1, in a typical attack mission, the company may fly 15-20 kilometers or more in various formations, to halt at a holding point. <p> Our first implementation of the pilot agents for helicopters focused on enabling individuals to cope with the complexities of this dynamic domain <ref> (Tambe, Schwamb, & Rosen-bloom 1995) </ref>. Figure 2 illustrates a portion of the operator hierarchy for an individual (at any one time, only one path in this hierarchy from the root to a leaf node is active). <p> Thus, this was a reasonable and serious effort to enable a company of pilot agents to coordinate their mission execution it is not much different from previous such efforts, at least in the synthetic battlefield domain <ref> (Tambe et al. 1995) </ref>. <p> In some specific circumstances, agents also track their team or particular teammates rather than waiting for communication from those agents. Agent tracking is based on the RESC technique <ref> (Tambe & Rosenbloom 1995) </ref>, which uses the existing operator hierarchy for tracking others' higher level goals (it executes those operators, and matches operators' predictions with actual observations). However, since many agents act in parallel, it is difficult to track all agents simultaneously.
Reference: <author> Tambe, M.; Johnson, W. L.; Jones, R.; Koss, F.; Laird, J. E.; Rosenbloom, P. S.; and Schwamb, K. </author> <year> 1995. </year> <title> Intelligent agents for interactive simulation environments. </title> <journal> AI Magazine 16(1). </journal>
Reference-contexts: 1 Introduction Many AI researchers are today striving to build agents for complex, dynamic multi-agent domains. Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill (Pimentel & Teixeira 1994) or combat <ref> (Tambe et al. 1995) </ref>), virtual interactive fiction (Bates, Loyall, & Reilly 1992) and RoboCup robotic and virtual soccer (Kitano et al. 1995). This paper focuses on plan execution in such dynamic, multi-agent domains. <p> understood here as the common knowledge about the (social) world that is possessed by every schoolchild and the methods for making obvious inferences from this knowledge (Davis 1990). 2 Domain and Initial Experiences The domain of our work is a real-world battlefield simulator, commercially developed for the military for training <ref> (Tambe et al. 1995) </ref>. We are building intelligent pilot agents for synthetic aircraft in this environment. These pilot agents have participated in large scale combat exercises, some involving expert human pilots. <p> These pilot agents have participated in large scale combat exercises, some involving expert human pilots. This paper will focus on pilot agents for a company of (up to eight) attack helicopters, which execute their mission in a synthetic 3D terrain, complete with hills, valleys, roads and ridges <ref> (Tambe, Schwamb, & Rosenbloom 1995) </ref>. As shown in Figure 1, in a typical attack mission, the company may fly 15-20 kilometers or more in various formations, to halt at a holding point. <p> Our first implementation of the pilot agents for helicopters focused on enabling individuals to cope with the complexities of this dynamic domain <ref> (Tambe, Schwamb, & Rosen-bloom 1995) </ref>. Figure 2 illustrates a portion of the operator hierarchy for an individual (at any one time, only one path in this hierarchy from the root to a leaf node is active). <p> Thus, this was a reasonable and serious effort to enable a company of pilot agents to coordinate their mission execution it is not much different from previous such efforts, at least in the synthetic battlefield domain <ref> (Tambe et al. 1995) </ref>. <p> In some specific circumstances, agents also track their team or particular teammates rather than waiting for communication from those agents. Agent tracking is based on the RESC technique <ref> (Tambe & Rosenbloom 1995) </ref>, which uses the existing operator hierarchy for tracking others' higher level goals (it executes those operators, and matches operators' predictions with actual observations). However, since many agents act in parallel, it is difficult to track all agents simultaneously.
Reference: <author> Tambe, M.; Schwamb, K.; and Rosenbloom, P. S. </author> <year> 1995. </year> <title> Building intelligent pilots for simulated rotary wing aircraft. </title> <booktitle> In Proceedings of the Fifth Conference on Computer Generated Forces and Behavioral Representation. </booktitle>
Reference-contexts: 1 Introduction Many AI researchers are today striving to build agents for complex, dynamic multi-agent domains. Such domains include virtual theatre (Hayes-Roth, Brownston, & Gen 1995), realistic virtual training environments (e.g., for emergency drill (Pimentel & Teixeira 1994) or combat <ref> (Tambe et al. 1995) </ref>), virtual interactive fiction (Bates, Loyall, & Reilly 1992) and RoboCup robotic and virtual soccer (Kitano et al. 1995). This paper focuses on plan execution in such dynamic, multi-agent domains. <p> understood here as the common knowledge about the (social) world that is possessed by every schoolchild and the methods for making obvious inferences from this knowledge (Davis 1990). 2 Domain and Initial Experiences The domain of our work is a real-world battlefield simulator, commercially developed for the military for training <ref> (Tambe et al. 1995) </ref>. We are building intelligent pilot agents for synthetic aircraft in this environment. These pilot agents have participated in large scale combat exercises, some involving expert human pilots. <p> These pilot agents have participated in large scale combat exercises, some involving expert human pilots. This paper will focus on pilot agents for a company of (up to eight) attack helicopters, which execute their mission in a synthetic 3D terrain, complete with hills, valleys, roads and ridges <ref> (Tambe, Schwamb, & Rosenbloom 1995) </ref>. As shown in Figure 1, in a typical attack mission, the company may fly 15-20 kilometers or more in various formations, to halt at a holding point. <p> Our first implementation of the pilot agents for helicopters focused on enabling individuals to cope with the complexities of this dynamic domain <ref> (Tambe, Schwamb, & Rosen-bloom 1995) </ref>. Figure 2 illustrates a portion of the operator hierarchy for an individual (at any one time, only one path in this hierarchy from the root to a leaf node is active). <p> Thus, this was a reasonable and serious effort to enable a company of pilot agents to coordinate their mission execution it is not much different from previous such efforts, at least in the synthetic battlefield domain <ref> (Tambe et al. 1995) </ref>. <p> In some specific circumstances, agents also track their team or particular teammates rather than waiting for communication from those agents. Agent tracking is based on the RESC technique <ref> (Tambe & Rosenbloom 1995) </ref>, which uses the existing operator hierarchy for tracking others' higher level goals (it executes those operators, and matches operators' predictions with actual observations). However, since many agents act in parallel, it is difficult to track all agents simultaneously.
Reference: <author> Tambe, M. </author> <year> 1996. </year> <title> Tracking dynamic team activity. </title> <booktitle> In Proceedings of the National Conference on Artificial Intelligence (AAAI). </booktitle>
Reference-contexts: It is important to note, though, that team state and team operators are only an individual's model of the on-going team activities, team members do not physically share memory. In fact, in <ref> (Tambe 1996) </ref>, we have used these team operators for tracking other team's activities, i.e., inferring the joint goals and intentions of those teams, and tracking their progress. <p> Repair is important, else the en tire team effort will go to waste. Finally, in (Jennings 1995) issues of communication risk and cost are not considered. Our recent work on team tracking <ref> (Tambe 1996) </ref> which involves inferring other team's joint goals and intentions based on observations of their actions is the predecessor to the work reported here.
References-found: 20

