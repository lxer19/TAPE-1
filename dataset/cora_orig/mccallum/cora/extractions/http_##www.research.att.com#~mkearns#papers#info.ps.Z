URL: http://www.research.att.com/~mkearns/papers/info.ps.Z
Refering-URL: http://www.research.att.com/~mkearns/
Root-URL: 
Title: Bounds on the Sample Complexity of Bayesian Learning Using Information Theory and the VC Dimension  
Author: David Haussler Michael Kearns Robert Schapire 
Affiliation: U.C. Santa Cruz  AT&T Bell Labs  AT&T Bell Labs  
Abstract: In this paper we study a Bayesian or average-case model of concept learning with a twofold goal: to provide more precise characterizations of learning curve (sample complexity) behavior that depend on properties of both the prior distribution over concepts and the sequence of instances seen by the learner, and to smoothly unite in a common framework the popular statistical physics and VC dimension theories of learning curves. To achieve this, we undertake a systematic investigation and comparison of two fundamental quantities in learning and information theory: the probability of an incorrect prediction for an optimal learning algorithm, and the Shannon information gain. This study leads to a new understanding of the sample complexity of learning in several existing models.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> P. Assouad. </author> <title> Densite et dimension. </title> <journal> Annales de l'Institut Fourier, </journal> <volume> 33(3) </volume> <pages> 233-282, </pages> <year> 1983. </year>
Reference-contexts: (y) [ m+1 (y)G ( m+1 (y)) + (1 m+1 (y))G (1 m+1 (y))] The form of the expression inside the expectation of Equation (4) is pG (p)+(1p)G (1p) (using the substitution p = m+1 (f )), and is suggestive of a binary "entropy", in which we interpret p 2 <ref> [0; 1] </ref> as a probability, and G (p) to be the "information" conveyed by the occurrence of an event whose probability is p. <p> )] = E f2P 2 For the probability of mistake of the Gibbs algorithm, we have E f2P [Gibbs m+1 (f )] = E f2P [1 m+1 (f )] = E f2P [2 m+1 (f )(1 m+1 (f ))] (7) Now it is easily verified that for any p 2 <ref> [0; 1] </ref>, min (p; 1 p) 2p (1 p) 2 Let us now define an inverse to H by letting H 1 (q), for q 2 [0; 1], be the unique p 2 [0; 1=2] such that H (p) = q. <p> m+1 (f )] = E f2P [2 m+1 (f )(1 m+1 (f ))] (7) Now it is easily verified that for any p 2 <ref> [0; 1] </ref>, min (p; 1 p) 2p (1 p) 2 Let us now define an inverse to H by letting H 1 (q), for q 2 [0; 1], be the unique p 2 [0; 1=2] such that H (p) = q. <p> The following important combinatorial result relating dim m (F ; x) and j F m (x)j has been proven independently by Sauer [27], Vapnik and Chervonenkis [34], and others (see As souad <ref> [1] </ref>): for all x, log j F dim m (F;x) X i (1 + o (1)) dim m (F ; x) log dim m (F ; x) where o (1) is a quantity that goes to zero as ff = m=dim m (F ; x) goes to infinity.
Reference: [2] <author> J. M. Barzdin and R. V. Freivald. </author> <title> On the prediction of general recursive functions. </title> <journal> Soviet Mathematics-Doklady, </journal> <volume> 13 </volume> <pages> 1224-1228, </pages> <year> 1972. </year>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models.
Reference: [3] <author> E. Baum and D. Haussler. </author> <title> What size net gives valid generalization? Neural Computation, </title> <booktitle> 1(1) </booktitle> <pages> 151-160, </pages> <year> 1989. </year>
Reference-contexts: A typical decomposition might let F i be all neural networks of a given type with at most i weights, in which case d i = O (i log i) <ref> [3] </ref>.
Reference: [4] <author> A. Blumer, A. Ehrenfeucht, D. Haussler, and M. K. Warmuth. </author> <title> Learnability and the Vapnik-Chervonenkis dimension. </title> <journal> Journal of the Association for Computing Machinery, </journal> <volume> 36(4) </volume> <pages> 929-965, </pages> <year> 1989. </year>
Reference-contexts: One of these, arising from the study of Valiant's distribution-free or probably approximately correct model [32] and having roots in the pattern recognition and minimax decision theory literature, characterizes the distribution-free, worst-case sample complexity of concept learning in terms of a combinatorial parameter known as the Vapnik-Chervonenkis (VC) dimension <ref> [34, 4] </ref>. In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory [10, 31, 15, 29, 24]. <p> These and many other examples are given in the papers of Dudley [13] and Blumer et al. <ref> [4] </ref> and elsewhere.
Reference: [5] <author> W. Buntine. </author> <title> A Theory of Learning Classification Rules. </title> <type> PhD thesis, </type> <institution> University of Technology, </institution> <address> Sydney, </address> <year> 1990. </year> <month> 26 </month>
Reference-contexts: This study leads to a new understanding of the sample complexity of learning in several existing models. One of our main motivations for this research arises from the frequent claims of machine learning practitioners that sample complexity bounds derived via the VC dimension are overly pessimistic in practice <ref> [5, 26] </ref>. This pessimism can be traced to three assumptions that are implicit in results that are based on the VC dimension. The first pessimistic assumption is that only the worst-case performance over possible target concepts counts. This is the minimax pessimism.
Reference: [6] <author> W. Buntine and A. Weigend. </author> <title> Bayesian back propagation. </title> <type> Unpublished manuscript, </type> <year> 1991. </year>
Reference-contexts: The first pessimistic assumption is that only the worst-case performance over possible target concepts counts. This is the minimax pessimism. We may think of an adversary choosing the hardest possible concept 1 More general Bayesian approaches to learning in neural networks are described in the recent papers <ref> [21, 6] </ref>. for the learner, rather than the Bayesian approach which incorporates prior beliefs regarding which concepts might be "more likely".
Reference: [7] <author> B. Clarke and A. Barron. </author> <title> Entropy, risk and the Bayesian central limit theorem. </title> <type> manuscript. </type>
Reference-contexts: In a later companion paper, we hope to develop our methods further and apply them to more varied and realistic models; some of this ongoing work is outlined in Section 12. Many beautiful results on the performance of Bayesian methods are also given in the statistics literature, see e.g. <ref> [8, 7] </ref> and references therein. 2 Summary of results Following a brief introduction of some notation in Section 3, our results begin in Section 4. Here we define the Shannon information gain of an example, and introduce the two learning algorithms we shall study.
Reference: [8] <author> B. Clarke and A. Barron. </author> <title> Information-theoretic asymptotics of Bayes methods. </title> <journal> IEEE Transactions on Information Theory, </journal> <volume> 36(3) </volume> <pages> 453-471, </pages> <year> 1990. </year>
Reference-contexts: In a later companion paper, we hope to develop our methods further and apply them to more varied and realistic models; some of this ongoing work is outlined in Section 12. Many beautiful results on the performance of Bayesian methods are also given in the statistics literature, see e.g. <ref> [8, 7] </ref> and references therein. 2 Summary of results Following a brief introduction of some notation in Section 3, our results begin in Section 4. Here we define the Shannon information gain of an example, and introduce the two learning algorithms we shall study.
Reference: [9] <author> T. Cover and J. Thomas. </author> <title> Elements of Information Theory. </title> <publisher> Wiley, </publisher> <year> 1991. </year>
Reference-contexts: As in the paper of Tishby, Levin and Solla [31], we upper bound the probability of mistake by the information gain. We also provide an information-theoretic lower bound on the probability of mistake, which can be viewed as a special case of Fano's inequality <ref> [9, 14] </ref>. Together these bounds provide a general characterization of learning curve behavior that is accurate to within a logarithmic factor.
Reference: [10] <author> J. Denker, D. Schwartz, B. Wittner, S. Solla, R. Howard, L. Jackel, and J. </author> <title> Hopfield. Automatic learning, rule extraction and generalization. </title> <journal> Complex Syst., </journal> <volume> 1 </volume> <pages> 877-922, </pages> <year> 1987. </year>
Reference-contexts: In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory <ref> [10, 31, 15, 29, 24] </ref>. While each of these theories has its own distinct strengths and drawbacks, there is little understanding of what relationships hold between them. In this paper, we study an average-case or Bayesian model of learning with two primary goals. <p> Thus, the Gibbs algorithm simply chooses a hypothesis randomly (according to P) from F among those that are consistent with the labels seen so far. The Gibbs algorithm is the "zero-temperature" limit of the learning algorithm studied in several recent papers <ref> [10, 31, 15, 29] </ref>. It is important to note that both the Bayes and Gibbs algorithms are quite different from the well-known maximum a posteriori algorithm, which chooses the hypothesis ^ f that maximizes the posterior probability P m [ ^ f ].
Reference: [11] <author> A. DeSantis, G. Markowski, and M. N. Wegman. </author> <title> Learning probabilistic prediction functions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 312-328. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1988. </year>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models.
Reference: [12] <author> R. O. Duda and P. E. Hart. </author> <title> Pattern Classification and Scene Analysis. </title> <publisher> Wiley, </publisher> <year> 1973. </year>
Reference-contexts: The first learning algorithm we consider is called the Bayes optimal classification algorithm <ref> [12] </ref>, or the Bayes algorithm for short. It is a special case of the weighted majority algorithm [20].
Reference: [13] <author> R. M. Dudley. </author> <title> A course on empirical processes. </title> <booktitle> Lecture Notes in Mathematics, </booktitle> <volume> 1097 </volume> <pages> 2-142, </pages> <year> 1984. </year>
Reference-contexts: These and many other examples are given in the papers of Dudley <ref> [13] </ref> and Blumer et al. [4] and elsewhere.
Reference: [14] <author> R. Fano. </author> <title> Class notes for course 6.574. </title> <type> Technical report, </type> <institution> Massachusetts Institute of Technology, </institution> <year> 1952. </year>
Reference-contexts: As in the paper of Tishby, Levin and Solla [31], we upper bound the probability of mistake by the information gain. We also provide an information-theoretic lower bound on the probability of mistake, which can be viewed as a special case of Fano's inequality <ref> [9, 14] </ref>. Together these bounds provide a general characterization of learning curve behavior that is accurate to within a logarithmic factor.
Reference: [15] <author> G. Gyorgyi and N. Tishby. </author> <title> Statistical theory of learning a rule. </title> <editor> In K. Thuemann and R. Koeberle, editors, </editor> <title> Neural Networks and Spin Glasses. </title> <publisher> World Scientific, </publisher> <year> 1990. </year>
Reference-contexts: In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory <ref> [10, 31, 15, 29, 24] </ref>. While each of these theories has its own distinct strengths and drawbacks, there is little understanding of what relationships hold between them. In this paper, we study an average-case or Bayesian model of learning with two primary goals. <p> Thus, the Gibbs algorithm simply chooses a hypothesis randomly (according to P) from F among those that are consistent with the labels seen so far. The Gibbs algorithm is the "zero-temperature" limit of the learning algorithm studied in several recent papers <ref> [10, 31, 15, 29] </ref>. It is important to note that both the Bayes and Gibbs algorithms are quite different from the well-known maximum a posteriori algorithm, which chooses the hypothesis ^ f that maximizes the posterior probability P m [ ^ f ]. <p> mechanics, it is shown that for m &gt;> d &gt;> 1, E f2P;x2D fl [Bayes m (x; f )] 0:44d (compared with the 0:5d=m conjectured general upper bound and the d=m proven general upper bound given for any class of VC dimension d above) and, as was previously shown in <ref> [15] </ref>, E f2P;x2D fl [Gibbs m (x; f)] 0:62d (compared with the 2d=m general upper bound proven above).
Reference: [16] <author> D. Haussler. </author> <title> Sphere packing numbers for subsets of the Boolean n-cube with bounded Vapnik-Chervonenkis dimension. </title> <type> Technical Report UCSC-CRL-91-41, </type> <institution> University of Calif. Computer Research Laboratory, </institution> <address> Santa Cruz, CA, </address> <year> 1991. </year>
Reference-contexts: Alternately, we can also derive the result directly from the lemmas used in establishing their Theorem 2.3. This latter approach is outlined in the discussion section of <ref> [16] </ref>. From Equation (21) we can also obtain similar upper bounds for the Gibbs algorithm.
Reference: [17] <author> D. Haussler, N. Littlestone, and M. Warmuth. </author> <title> Predicting f0; 1g-functions on randomly drawn points. </title> <type> Technical Report UCSC-CRL-90-54, </type> <institution> University of California Santa Cruz, Computer Research Laboratory, </institution> <month> Dec. </month> <year> 1990. </year> <note> To appear in Information and Computation. </note>
Reference-contexts: 2m m ] dim (F ) log dim (F ) and m X Bayes i (x; f )] E f2P;x2D fl [ m X Gibbs i (x; f )] (1 + o (1))E x2D fl [ 2 m ] dim (F ) log dim (F ) Haussler, Littlestone and Warmuth <ref> [17] </ref> (Section 3, latter part) show that specific distributions D and priors P can be constructed for each of the classes F listed above (i.e., (homogeneous) linear threshold functions, indicator functions for axis-parallel rectangles and unions of intervals) for which E f2P;x2D fl [ i=1 Bayes i (x; f )] (1 <p> However, using the results of Haussler, Littlestone and Warmuth <ref> [17] </ref>, we can show that 2 for all P, E f2P;x2D fl [Bayes m (x; f )] E x2D fl [ dim m (F ; x) ] m Ignoring the middle bound for the moment, the proof of this fact is straightforward, given the results of Haussler, Littlestone and Warmuth [17] <p> <ref> [17] </ref>, we can show that 2 for all P, E f2P;x2D fl [Bayes m (x; f )] E x2D fl [ dim m (F ; x) ] m Ignoring the middle bound for the moment, the proof of this fact is straightforward, given the results of Haussler, Littlestone and Warmuth [17] (which are not straightforward to prove, as far as we know). <p> by the optimality of Bayes algorithm. (From a statistical viewpoint, here we are just using the fact that the Bayes risk is always less than the maximum risk of any statistical procedure.) To prove the middle bound of Equation (21), we can generalize the proof of Haussler, Lit-tlestone and Warmuth's <ref> [17] </ref> Theorem 2.3 to obtain this sharper, instance space distribution dependent form of the bound for the 1-inclusion graph algorithm for all target concepts, and then apply the argument described in the previous paragraph to obtain the desired result. <p> The same specific distributions and priors constructed by Haussler, Littlestone and War-muth <ref> [17] </ref> that we mentioned above also show that for each of the classes F of (homogeneous) linear threshold functions, indicator functions for axis-parallel rectangles and unions of intervals, there is an instance space distribution D and a prior P such that E f2P;x2D fl [Bayes m (x; f)] (1 o (1)) <p> The distributions used in the lower bounds from the latter part of Section 3 of Haussler, Littlestone and Warmuth <ref> [17] </ref> mentioned above are unfortunately not very natural. <p> A similar result holds for the Bayes algorithm with an additional factor of two. A bound similar to that given in Equation (23) is given by Haussler, Littlestone and Warmuth <ref> [17] </ref>, but with a slightly higher constant.
Reference: [18] <author> N. Littlestone. </author> <title> Mistake Bounds and Logarithmic Linear-threshold Learning Algorithms. </title> <type> PhD thesis, </type> <institution> University of California Santa Cruz, </institution> <year> 1989. </year>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models.
Reference: [19] <author> N. Littlestone, P. M. Long, and M. K. Warmuth. </author> <title> On-line learning of linear functions. </title> <booktitle> In Proceedings of the Twenty Third Annual ACM Symposium on Theory of Computing, </booktitle> <pages> pages 465-475, </pages> <year> 1991. </year>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models.
Reference: [20] <author> N. Littlestone and M. Warmuth. </author> <title> The weighted majority algorithm. </title> <type> Technical Report UCSC-CRL-89-16, </type> <institution> Computer Research Laboratory, University of Santa Cruz, </institution> <month> July </month> <year> 1989. </year>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models. <p> The first learning algorithm we consider is called the Bayes optimal classification algorithm [12], or the Bayes algorithm for short. It is a special case of the weighted majority algorithm <ref> [20] </ref>. For any m and b 2 f0; 1g, define F b m (x; f ) = F b m (f ) = f ^ f 2 F m (x; f ) : ^ f (x m+1 ) = bg.
Reference: [21] <author> D. MacKay. </author> <title> Bayesian Methods for Adaptive Models. </title> <type> PhD thesis, </type> <institution> California Institute of Technology, </institution> <year> 1992. </year>
Reference-contexts: The first pessimistic assumption is that only the worst-case performance over possible target concepts counts. This is the minimax pessimism. We may think of an adversary choosing the hardest possible concept 1 More general Bayesian approaches to learning in neural networks are described in the recent papers <ref> [21, 6] </ref>. for the learner, rather than the Bayesian approach which incorporates prior beliefs regarding which concepts might be "more likely".
Reference: [22] <author> P. Massart. </author> <title> Rates of convergence in the central limit theorem for empirical processes. Annales de l'Institut H. </title> <journal> Poincare Probab. Statist., </journal> <volume> 22 </volume> <pages> 381-423, </pages> <year> 1986. </year> <month> 27 </month>
Reference-contexts: Note that the trick employed in the proof above of varying the additional number of instances k to get better averages has also been used in [28] and <ref> [22] </ref> to get other bounds on related measures based on the VC dimension.
Reference: [23] <author> B. K. Natarajan. </author> <title> Learning over classes of distributions. </title> <booktitle> In Proceedings of the 1988 Workshop on Computational Learning Theory, </booktitle> <pages> pages 408-409, </pages> <address> San Mateo, CA, </address> <year> 1988. </year> <note> published by Morgan Kaufmann. </note>
Reference-contexts: Thus, the VC dimension is also based on a worst-case assumption over instance space distributions. In addition to the VC dimension, Vapnik and Chervonenkis have a distribution-specific formulation that overcomes this limitation [35], but apart from Natarajan's work <ref> [23] </ref>, it has not been used much in computational learning theory. We extend this idea further in Section 9. The third and perhaps most subtle pessimistic assumption can be seen by noting that the VC dimension provides upper bounds on the learning curves of any consistent learning algorithm.
Reference: [24] <author> M. Opper and D. Haussler. </author> <title> Calculation of the learning curve of Bayes optimal classification algorithm for learning a perceptron with noise. </title> <booktitle> In Computational Learning Theory: Proceedings of the Fourth Annual Workshop, </booktitle> <pages> pages 75-87. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1991. </year>
Reference-contexts: In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory <ref> [10, 31, 15, 29, 24] </ref>. While each of these theories has its own distinct strengths and drawbacks, there is little understanding of what relationships hold between them. In this paper, we study an average-case or Bayesian model of learning with two primary goals. <p> hypothesis of an algorithm at time m to be the (possibly probabilistic) mapping f : X ! f0; 1g obtained by letting f (x) be the prediction of the algorithm when x m+1 = x.) This drawback is absent in our second learning algorithm, which is called the Gibbs algorithm <ref> [24] </ref>, and behaves as follows: Given the labels f (x 1 ); : : : ; f (x m ), a hypothesis concept ^ f is chosen randomly according to the posterior distribution P m . <p> It also follows that the expected total number of mistakes of the Bayes and the Gibbs algorithms differ by a factor of at most about 1:44 in each of these cases; this was not previously known. Opper and Haussler <ref> [24] </ref> give a similar comparison between the instantaneous mistake bounds for the Bayes and Gibbs algorithms for homogeneous linear threshold functions using different priors and instance space distributions. <p> The distributions used in the lower bounds from the latter part of Section 3 of Haussler, Littlestone and Warmuth [17] mentioned above are unfortunately not very natural. However, in a recent paper <ref> [24] </ref> the natural case in which F is the set of homogeneous linear threshold functions on &lt; d and both the distribution D and the prior P on possible target concepts (represented also by vectors in &lt; d ) are uniform on the unit sphere in &lt; d is examined. (For
Reference: [25] <author> A. Renyi. </author> <title> Probability Theory. </title> <publisher> North Holland, </publisher> <address> Amsterdam, </address> <year> 1970. </year>
Reference-contexts: Renyi <ref> [25] </ref>) H P ( F 1 (x 1 )) + H P ( F Thus E f2P;x2f (x 1 ;x 2 );(x 2 ;x 1 )g [I 2 (x; f )] 2 1 (x 1 )) + 2 1 (x 2 )) Since x 1 and x 2 were arbitrary, we
Reference: [26] <author> W. Sarrett and M. Pazzani. </author> <title> Average case analysis of empirical and explanation-based learning algorithms. </title> <type> Technical Report 89-35, </type> <institution> UC Irvine, </institution> <year> 1989. </year> <note> To appear in Machine Learning. </note>
Reference-contexts: This study leads to a new understanding of the sample complexity of learning in several existing models. One of our main motivations for this research arises from the frequent claims of machine learning practitioners that sample complexity bounds derived via the VC dimension are overly pessimistic in practice <ref> [5, 26] </ref>. This pessimism can be traced to three assumptions that are implicit in results that are based on the VC dimension. The first pessimistic assumption is that only the worst-case performance over possible target concepts counts. This is the minimax pessimism.
Reference: [27] <author> N. Sauer. </author> <title> On the density of families of sets. </title> <journal> Journal of Combinatorial Theory (Series A), </journal> <volume> 13 </volume> <pages> 145-147, </pages> <year> 1972. </year>
Reference-contexts: These and many other examples are given in the papers of Dudley [13] and Blumer et al. [4] and elsewhere. The following important combinatorial result relating dim m (F ; x) and j F m (x)j has been proven independently by Sauer <ref> [27] </ref>, Vapnik and Chervonenkis [34], and others (see As souad [1]): for all x, log j F dim m (F;x) X i (1 + o (1)) dim m (F ; x) log dim m (F ; x) where o (1) is a quantity that goes to zero as ff = m=dim
Reference: [28] <author> J. Shawe-Taylor, M. Anthony, and N. Biggs. </author> <title> Bounding sample size with the Vapnik-Chervonenkis dimension. </title> <type> Technical Report CSD-TR-618, </type> <institution> University of London, Surrey, </institution> <address> England, </address> <year> 1989. </year>
Reference-contexts: Note that the trick employed in the proof above of varying the additional number of instances k to get better averages has also been used in <ref> [28] </ref> and [22] to get other bounds on related measures based on the VC dimension.
Reference: [29] <author> H. Sompolinsky, N. Tishby, and H. Seung. </author> <title> Learning from examples in large neural networks. </title> <journal> Phys.Rev.Lett., </journal> <volume> 65 </volume> <pages> 1683-1686, </pages> <year> 1990. </year>
Reference-contexts: In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory <ref> [10, 31, 15, 29, 24] </ref>. While each of these theories has its own distinct strengths and drawbacks, there is little understanding of what relationships hold between them. In this paper, we study an average-case or Bayesian model of learning with two primary goals. <p> Thus, the Gibbs algorithm simply chooses a hypothesis randomly (according to P) from F among those that are consistent with the labels seen so far. The Gibbs algorithm is the "zero-temperature" limit of the learning algorithm studied in several recent papers <ref> [10, 31, 15, 29] </ref>. It is important to note that both the Bayes and Gibbs algorithms are quite different from the well-known maximum a posteriori algorithm, which chooses the hypothesis ^ f that maximizes the posterior probability P m [ ^ f ].
Reference: [30] <author> M. Talagrand. </author> <title> Donsker classes of sets. Probability Theory and Related Fields, </title> <booktitle> 78 </booktitle> <pages> 169-191, </pages> <year> 1988. </year>
Reference-contexts: We conjecture that in fact the lower bound is correct, and thus the upper bounds in Equations (21) and 2 Vapnik had obtained the special case of this result for homogeneous linear threshold functions [33]. Also, see <ref> [30] </ref> for further interesting properties of E x2D fl [dim m (F; x)]. 17 (22) can each be improved by a factor of 1=2.
Reference: [31] <author> N. Tishby, E. Levin, and S. Solla. </author> <title> Consistent inference of probabilities in layered networks: predictions and generalizations. </title> <booktitle> In IJCNN International Joint Conference on Neural Networks, </booktitle> <volume> volume II, </volume> <pages> pages 403-409. </pages> <publisher> IEEE, </publisher> <year> 1989. </year>
Reference-contexts: In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory <ref> [10, 31, 15, 29, 24] </ref>. While each of these theories has its own distinct strengths and drawbacks, there is little understanding of what relationships hold between them. In this paper, we study an average-case or Bayesian model of learning with two primary goals. <p> In Section 5 we prove that the probabilities of mistake for our two learning algorithms can be bounded above and below by simple functions of the expected information gain. As in the paper of Tishby, Levin and Solla <ref> [31] </ref>, we upper bound the probability of mistake by the information gain. We also provide an information-theoretic lower bound on the probability of mistake, which can be viewed as a special case of Fano's inequality [9, 14]. <p> Thus, the Gibbs algorithm simply chooses a hypothesis randomly (according to P) from F among those that are consistent with the labels seen so far. The Gibbs algorithm is the "zero-temperature" limit of the learning algorithm studied in several recent papers <ref> [10, 31, 15, 29] </ref>. It is important to note that both the Bayes and Gibbs algorithms are quite different from the well-known maximum a posteriori algorithm, which chooses the hypothesis ^ f that maximizes the posterior probability P m [ ^ f ].
Reference: [32] <author> L. G. Valiant. </author> <title> A theory of the learnable. </title> <journal> Communications of the ACM, </journal> <volume> 27(11) </volume> <pages> 1134-42, </pages> <year> 1984. </year>
Reference-contexts: One of these, arising from the study of Valiant's distribution-free or probably approximately correct model <ref> [32] </ref> and having roots in the pattern recognition and minimax decision theory literature, characterizes the distribution-free, worst-case sample complexity of concept learning in terms of a combinatorial parameter known as the Vapnik-Chervonenkis (VC) dimension [34, 4].
Reference: [33] <author> V. N. Vapnik. </author> <title> Theorie der Zeichenerkennung. </title> <publisher> Akademie-Verlag, </publisher> <year> 1979. </year>
Reference-contexts: We conjecture that in fact the lower bound is correct, and thus the upper bounds in Equations (21) and 2 Vapnik had obtained the special case of this result for homogeneous linear threshold functions <ref> [33] </ref>. Also, see [30] for further interesting properties of E x2D fl [dim m (F; x)]. 17 (22) can each be improved by a factor of 1=2.
Reference: [34] <author> V. N. Vapnik. </author> <title> Estimation of Dependences Based on Empirical Data. </title> <publisher> Springer-Verlag, </publisher> <address> New York, </address> <year> 1982. </year>
Reference-contexts: One of these, arising from the study of Valiant's distribution-free or probably approximately correct model [32] and having roots in the pattern recognition and minimax decision theory literature, characterizes the distribution-free, worst-case sample complexity of concept learning in terms of a combinatorial parameter known as the Vapnik-Chervonenkis (VC) dimension <ref> [34, 4] </ref>. In contrast, the average-case sample complexity of learning in neural networks has recently been investigated from a standpoint that is essentially Bayesian 1 , and is strongly influenced by ideas and tools from statistical physics, as well as by information theory [10, 31, 15, 29, 24]. <p> These and many other examples are given in the papers of Dudley [13] and Blumer et al. [4] and elsewhere. The following important combinatorial result relating dim m (F ; x) and j F m (x)j has been proven independently by Sauer [27], Vapnik and Chervonenkis <ref> [34] </ref>, and others (see As souad [1]): for all x, log j F dim m (F;x) X i (1 + o (1)) dim m (F ; x) log dim m (F ; x) where o (1) is a quantity that goes to zero as ff = m=dim m (F ; x) <p> Vapnik has extended the theory to include the case when F has infinite VC dimension, but can be decomposed into a sequence F 1 F 2 : : : of subclasses with nonzero, finite VC dimensions d 1 ; d 2 ; : : :, respectively <ref> [34] </ref>. A typical decomposition might let F i be all neural networks of a given type with at most i weights, in which case d i = O (i log i) [3].
Reference: [35] <author> V. N. Vapnik and A. Y. Chervonenkis. </author> <title> On the uniform convergence of relative frequencies of events to their probabilities. </title> <journal> Theory of Probability and its Applications, </journal> <volume> 16(2) </volume> <pages> 264-80, </pages> <year> 1971. </year>
Reference-contexts: Thus, the VC dimension is also based on a worst-case assumption over instance space distributions. In addition to the VC dimension, Vapnik and Chervonenkis have a distribution-specific formulation that overcomes this limitation <ref> [35] </ref>, but apart from Natarajan's work [23], it has not been used much in computational learning theory. We extend this idea further in Section 9. <p> f2P;x2D fl [ i=1 Bayes i (x; f )] E f2P;x2D fl [ m X Gibbs i (x; f)] 1 E x2D fl [log j F The expectation E x2D fl [log j F is the VC entropy defined by Vapnik and Chervonenkis in their seminal paper on uniform convergence <ref> [35] </ref>, and plays a central role in their characterization of the uniform convergence of empirical frequencies to probabilities in a class of events. Here we see how simple information-theoretic arguments can be used to relate the VC entropy to the learning curves of the Bayes and Gibbs algorithms.
Reference: [36] <author> V. Vovk. </author> <title> Aggregating strategies. </title> <booktitle> In Proceedings of the Third Annual Workshop on Computational Learning Theory, </booktitle> <pages> pages 371-383. </pages> <publisher> Morgan Kaufmann, </publisher> <year> 1990. </year> <month> 28 </month>
Reference-contexts: In doing so, we borrow from and contribute to the work on weighted majority and aggregating learning strategies <ref> [18, 20, 36, 11, 2, 19] </ref>, as well as to the VC dimension and statistical physics work. This study leads to a new understanding of the sample complexity of learning in several existing models.
References-found: 36

