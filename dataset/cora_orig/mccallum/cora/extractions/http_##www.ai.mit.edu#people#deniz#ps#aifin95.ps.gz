URL: http://www.ai.mit.edu/people/deniz/ps/aifin95.ps.gz
Refering-URL: http://www.ai.mit.edu/people/deniz/papers.html
Root-URL: 
Email: mdlm@ai.mit.edu, deniz@ai.mit.edu  
Title: A critique of the standard neural network application to financial time series analysis  
Author: Michael de la Maza and Deniz Yuret 
Address: Room NE43-815 545 Technology Square Cambridge, MA 02139  
Affiliation: Numinous Noetics Group Artificial Intelligence Laboratory Massachusetts Institute of Technology  
Abstract: Neural networks are one of the most widely used artificial intelligence methods for financial time series analysis. In this paper we describe the standard application of neural networks and suggest that it has two shortcomings. First, backpropagation search takes place in sum of squared errors space instead of risk-adjusted return space. Second, the standard neural network has difficulty ignoring noise and focusing in on discoverable regularities. Both problems are illustrated with simple examples. We suggest ways of overcoming these problems.
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> E. M. Azoff. </author> <title> Nueral Network Time Series Forecasting of Financial Markets. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1994. </year>
Reference-contexts: This "standard" was created by averaging several dozen applications of neural networks that appear in the applied computational finance literature (see, e.g., <ref> [1] </ref>). More complete and general discussions of neural networks can be found elsewhere (e.g., [9, 10]). The standard neural network consists of three sequential layers of nodes in the following order: input layer, hidden layer, and output layer.
Reference: [2] <author> A. Barron. </author> <title> Universal approximation bounds for superpositions of a sigmoidal function. </title> <type> Technical Report 58, </type> <institution> University of Illinois at Urbana-Champaign, Champaign, IL, </institution> <year> 1991. </year>
Reference-contexts: Neural networks have been shown to have some very desirable theoretical properties (see, e.g., <ref> [2] </ref>) which have made them the tool of choice for many inference tasks. * The non-parametric nature of neural networks. Researchers who want to "let the data speak for itself" prefer neural networks to standard statistical methods.
Reference: [3] <author> D. Goldberg. </author> <title> Genetic Algorithms in Search, Optimization and Machine Learning. </title> <publisher> Addison-Wesley, </publisher> <address> Reading, MA, </address> <year> 1989. </year>
Reference-contexts: In this section we speculate on ways of overcoming these limitations. One way to address both problems is to change the search algorithm from backpropagation to simulated annealing [7, 5], genetic algorithms <ref> [4, 3] </ref>, standard gradient descent, or any one of the many optimization methods which allow search over arbitrary utility functions.
Reference: [4] <author> J. H. Holland. </author> <title> Adaptation in Natural and Artificial Systems. </title> <publisher> University of Michigan Press, </publisher> <address> Ann Arbor, </address> <year> 1975. </year>
Reference-contexts: In this section we speculate on ways of overcoming these limitations. One way to address both problems is to change the search algorithm from backpropagation to simulated annealing [7, 5], genetic algorithms <ref> [4, 3] </ref>, standard gradient descent, or any one of the many optimization methods which allow search over arbitrary utility functions.
Reference: [5] <author> L. Ingber. </author> <title> Simulated annealing: Practice versus theory. </title> <booktitle> Mathematical and Computer Modeling, </booktitle> <volume> 18(11) </volume> <pages> 29-57, </pages> <year> 1993. </year>
Reference-contexts: In this section we speculate on ways of overcoming these limitations. One way to address both problems is to change the search algorithm from backpropagation to simulated annealing <ref> [7, 5] </ref>, genetic algorithms [4, 3], standard gradient descent, or any one of the many optimization methods which allow search over arbitrary utility functions.
Reference: [6] <author> M. I. Jordan and R. A. Jacobs. </author> <title> Hierarchical mixtures of experts and the EM algorithm. </title> <type> Technical Report 1440, </type> <institution> Artificial Intelligence Laboratory, Massachusetts Institute of Technology, </institution> <address> Cambridge, MA, </address> <year> 1993. </year>
Reference-contexts: Success in finding such a transformation would probably be helpful in finding solutions to other problems not associated with financial time series analysis. A possible solution to the second problem has been explored by Jordan and Jacobs <ref> [6] </ref>.
Reference: [7] <author> S. Kirkpatrick, C. D. Gelatt, Jr., and M. P. Vecchi. </author> <title> Optimization by simulated annealing. </title> <journal> Science, </journal> <volume> 220 </volume> <pages> 671-680, </pages> <year> 1983. </year>
Reference-contexts: In this section we speculate on ways of overcoming these limitations. One way to address both problems is to change the search algorithm from backpropagation to simulated annealing <ref> [7, 5] </ref>, genetic algorithms [4, 3], standard gradient descent, or any one of the many optimization methods which allow search over arbitrary utility functions.
Reference: [8] <author> W. S. McCulloch and W. H. Pitts. </author> <title> A logical calculus of the ideas immanent in nervous activity. </title> <journal> Bulletin of Mathematical Biophysics, </journal> <volume> 5 </volume> <pages> 115-133, </pages> <year> 1943. </year>
Reference-contexts: 1 Introduction Ever since McCulloch and Pitts <ref> [8] </ref> published their landmark paper describing a neural calculus, researchers have explored how biologically inspired networks might be employed to solve a wide class of problems.
Reference: [9] <author> S. K. Rogers and M. Kabrisky. </author> <title> An Introduction to Biological and Artificial Neural Networks for Pattern Recognition. </title> <publisher> SPIE Optical Engineering Press, Bellingham, </publisher> <address> WA, </address> <year> 1991. </year>
Reference-contexts: This "standard" was created by averaging several dozen applications of neural networks that appear in the applied computational finance literature (see, e.g., [1]). More complete and general discussions of neural networks can be found elsewhere (e.g., <ref> [9, 10] </ref>). The standard neural network consists of three sequential layers of nodes in the following order: input layer, hidden layer, and output layer.
Reference: [10] <editor> D. E. Rumelhart and J. L. McClelland, editors. </editor> <booktitle> Parallel Distributed Processing: Explorations in the Microstructure of Cognition. </booktitle> <publisher> MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1986. </year>
Reference-contexts: This "standard" was created by averaging several dozen applications of neural networks that appear in the applied computational finance literature (see, e.g., [1]). More complete and general discussions of neural networks can be found elsewhere (e.g., <ref> [9, 10] </ref>). The standard neural network consists of three sequential layers of nodes in the following order: input layer, hidden layer, and output layer.
Reference: [11] <author> G. Soros. </author> <title> The Alchemy of Finance: Reading the Mind of the Market. </title> <publisher> John Wiley & Sons, Inc., </publisher> <address> New York, </address> <year> 1994. </year> <month> 5 </month>
Reference-contexts: We illustrate this shortcoming with a simple artificial time series. 3.1 Sum of squared errors vs. risk-adjusted return My financial success stands in stark contrast with my ability to forecast events. - George Soros <ref> [11, page 301] </ref> We present two trading strategies which have the property that one is superior when sum of squared errors is the utility measure but is inferior when risk-adjusted return, as defined by the Sharpe ratio, is the utility measure.
References-found: 11

