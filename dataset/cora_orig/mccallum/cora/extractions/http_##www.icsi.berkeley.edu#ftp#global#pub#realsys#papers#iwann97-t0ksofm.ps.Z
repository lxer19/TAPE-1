URL: http://www.icsi.berkeley.edu/ftp/global/pub/realsys/papers/iwann97-t0ksofm.ps.Z
Refering-URL: http://www.icsi.berkeley.edu/real/spert/t0spert_pubs.html
Root-URL: http://www.icsi.berkeley.edu
Title: A Fast Kohonen Net Implementation for  
Author: Spert-II Krste Asanovic 
Address: Berkeley, CA 94720-1776  
Affiliation: Computer Science Division Department of Electrical Engineering and Computer Sciences University of California at Berkeley  
Abstract: We present an implementation of Kohonen Self-Organizing Feature Maps for the Spert-II vector microprocessor system. The implementation supports arbitrary neural map topologies and arbitrary neighborhood functions. For small networks, as used in real-world tasks, a single Spert-II board is measured to run Kohonen net classification at up to 208 million connections per second (MCPS). On a speech coding benchmark task, Spert-II performs on-line Kohonen net training at over 100 million connection updates per second (MCUPS). This represents almost a factor of 10 improvement compared to previously reported implementations. The asymptotic peak speed of the system is 213 MCPS and 213 MCUPS.
Abstract-found: 1
Intro-found: 1
Reference: 1. <author> Wawrzynek, J., Asanovic, K., Kingsbury, B. E. D., Beck, J., Johnson, D., Morgan, N.: Spert-II: </author> <title> A vector microprocessor system, </title> <journal> IEEE Computer, </journal> <volume> 29(3) </volume> <pages> 79-86, </pages> <month> March </month> <year> 1996. </year>
Reference-contexts: 1 Introduction Spert-II is a workstation accelerator constructed around the T0 vector microprocessor <ref> [1] </ref>. Although most production use of Spert-II systems has been to accelerate error backpropagation training of multi-layer perceptrons used within continuous speech recognition systems, we designed Spert-II as a flexible, general-purpose accelerator. In this paper we report on a second neural net application, the Kohonen Self-Organizing Feature Map (KSOFM). <p> T0 has no floating-point coprocessor, but the T0 kernel traps and emulates any MIPS-II floating-point instructions to simplify software porting. To date, 25 Spert-II systems have been installed at 8 sites in the USA and Europe. For further information on T0 and Spert-II see <ref> [1, 5, 8] </ref>. 4 Mapping Kohonen Nets to T0 We have implemented two vectorized library routines for KSOFMs. These are both coded in assembler, but provide a standard C function interface. The routines have been designed to support arbitrary neural map topologies and arbitrary neighborhood functions.
Reference: 2. <author> Kohonen, T.: </author> <title> Self-organizing formation of topologically correct feature maps, </title> <journal> Biological Cybernetics, </journal> <volume> 43(1) </volume> <pages> 59-69, </pages> <year> 1982. </year>
Reference-contexts: We compare the benchmark results with previously reported numbers before concluding. 2 Kohonen Net Algorithms Kohonen self-organizing feature maps (KSOFM) are a class of artificial neural networks that can form a non-linear mapping from a higher dimensional input space to a lower dimensional neuron map with no external supervision <ref> [2] </ref>. They have been used in a wide range of applications including image classification and data compression [3]. Once trained, the network can be used to classify novel patterns by finding the neuron with the weight vector closest to the input vector.
Reference: 3. <author> Myklebust, G., Solheim, J. G.: </author> <title> Parallel self-organizing maps for actual applications, </title> <booktitle> Proceedings of the IEEE International Conference on Neural Networks, </booktitle> <address> Perth, </address> <year> 1995. </year>
Reference-contexts: They have been used in a wide range of applications including image classification and data compression <ref> [3] </ref>. Once trained, the network can be used to classify novel patterns by finding the neuron with the weight vector closest to the input vector. <p> The neighborhood function fl (t; d (i; c (t))) restricts the weight updates to a neighborhood around the winning neuron. Typically, the size of the affected neighborhood is also reduced during the training procedure. Kohonen nets used in practice are quite small <ref> [3] </ref>, and during training the neighborhood radius shrinks rapidly so that it is only a few neurons wide for most of the time. These attributes make Kohonen nets difficult to parallelize efficiently. Finding the winning neuron requires global communication for every pattern. <p> On T0, the vector extract instruction executes in VMP to allow vector arithmetic instructions to be overlapped and the complete sum and index reduction operation takes 45 cycles. Table 1 gives the measured performance of the forward pass routine on T0 for various sized networks taken from <ref> [3] </ref>. 4.2 Weight Update The second routine implements (2) with the following C interface: int update (size_t n_inputs, size_t n_neurons, size_t stride, const short* X, const short* F, int rshift, short* W) The routine modifies weights for n neurons neurons located in contiguous columns of the weight matrix starting at the <p> The stride argument gives the total number of neurons in the array, and hence the Table 1. Spert-II KSOFM forward pass performance on real-world networks taken from <ref> [3] </ref>. Application Neuron Topology Input dimension Spert-II (MCPS) Speech coding 10fi10 12 100.1 16fi16 12 132.9 Radar clutter classification 10fi10 11 93.4 20fi20 11 130.9 Gas concentration 12fi12 32 159.3 Binocular Receptive Fields 16fi16 256 208.9 element stride between rows in the matrix. <p> Training time is given over all 30,000 patterns. These are small networks and small training databases, with total run times of around one second, yet T0 still achieves high performance. Table 3 compares Spert-II performance with other reported implementations of this task <ref> [3, 9] </ref>. On larger networks and with larger training sets, the performance will asymptotically approach the peak forward pass performance, yielding up to 213 MCUPS. Table 2. Performance of Spert-II on the EPFL benchmark for KSOFM training for all 30,000 training patterns. <p> The other training parameters are ff 0 = 0:1, K ff = 0:0025, and K R = 0:02. Neuron Topology R 0 Spert-II Time (s) Spert-II (MCUPS) 10fi10 5 0.795 45.2 20fi20 10 1.431 100.6 Table 3. Reported performance numbers for KSOFM <ref> [3, 9] </ref>. a Mantra I runs a smaller problem (6fi10) due to memory limitations. b RENNS numbers are estimated based on [3], with the 100 neuron curve used for the benchmark figure, and the 10,000 neuron curve for the peak. <p> Neuron Topology R 0 Spert-II Time (s) Spert-II (MCUPS) 10fi10 5 0.795 45.2 20fi20 10 1.431 100.6 Table 3. Reported performance numbers for KSOFM [3, 9]. a Mantra I runs a smaller problem (6fi10) due to memory limitations. b RENNS numbers are estimated based on <ref> [3] </ref>, with the 100 neuron curve used for the benchmark figure, and the 10,000 neuron curve for the peak.
Reference: 4. <author> Cornu, T., Ienne, P.: </author> <title> Performance of digital neuro-computers, </title> <booktitle> Proceedings Fourth International Conference on Microelectronics for Neural Networks and Fuzzy Systems, </booktitle> <month> September </month> <year> 1994, </year> <pages> 87-93. </pages>
Reference-contexts: The training algorithm can be modified to run in "bunch" mode, with weights updated only after presenting a group, or bunch, of patterns to allow a higher presentation rate on parallel implementations, but this can cause slower algorithmic convergence and longer overall training times <ref> [4] </ref>. 3 T0 and Spert-II Overview For the Spert-II project, we desired an architecture that was efficient at running not only a variety of neural network algorithms, but also other computation intensive components within typical real-world applications. We also required that the architecture be straightforward to program. <p> This rate represents only 9.2% of peak arithmetic performance, but fortunately training neighborhoods shrink rapidly in practice, and so only a small fraction of the neurons need to be updated. 4.3 Speech Coding Benchmark To measure training performance, we used a benchmark supplied by EPFL, Switzerland <ref> [4] </ref>, which uses a KSOFM to implement speech coding by vector quantization. This benchmark has 12-dimensional input vectors mapped to a 2-dimensional neuron grid of varying size. A set of 30,000 training patterns were supplied.
Reference: 5. <author> Asanovic, K., Johnson, D.: </author> <title> Torrent architecture manual, </title> <type> Technical Report, </type> <institution> Computer Science Division, University of California at Berkeley, CSD-97-930, </institution> <year> 1997. </year>
Reference-contexts: We also required that the architecture be straightforward to program. These goals led us to design a new vector instruction set architecture (ISA), "Torrent" <ref> [5] </ref>, based on the industry standard MIPS-II RISC ISA [6]. The Torrent ISA is very similar to that of a traditional vector supercomputer [7], including vector registers, vector length control, strided and scatter/gather vector memory instructions, and conditional operations. <p> T0 has no floating-point coprocessor, but the T0 kernel traps and emulates any MIPS-II floating-point instructions to simplify software porting. To date, 25 Spert-II systems have been installed at 8 sites in the USA and Europe. For further information on T0 and Spert-II see <ref> [1, 5, 8] </ref>. 4 Mapping Kohonen Nets to T0 We have implemented two vectorized library routines for KSOFMs. These are both coded in assembler, but provide a standard C function interface. The routines have been designed to support arbitrary neural map topologies and arbitrary neighborhood functions.
Reference: 6. <author> Kane, G.: </author> <title> MIPS RISC Architecture (R2000/R3000), </title> <publisher> Prentice Hall, </publisher> <year> 1989. </year>
Reference-contexts: We also required that the architecture be straightforward to program. These goals led us to design a new vector instruction set architecture (ISA), "Torrent" [5], based on the industry standard MIPS-II RISC ISA <ref> [6] </ref>. The Torrent ISA is very similar to that of a traditional vector supercomputer [7], including vector registers, vector length control, strided and scatter/gather vector memory instructions, and conditional operations.
Reference: 7. <author> Russel, R. M.: </author> <title> The CRAY-1 computer system, </title> <journal> Communications of the ACM, </journal> <volume> 21(1) </volume> <pages> 63-72, </pages> <month> January </month> <year> 1978. </year>
Reference-contexts: We also required that the architecture be straightforward to program. These goals led us to design a new vector instruction set architecture (ISA), "Torrent" [5], based on the industry standard MIPS-II RISC ISA [6]. The Torrent ISA is very similar to that of a traditional vector supercomputer <ref> [7] </ref>, including vector registers, vector length control, strided and scatter/gather vector memory instructions, and conditional operations. T0 (Torrent-0) is the first implementation of the Torrent ISA, and is a full-custom single-chip vector microprocessor developed in a collaboration between UCB and ICSI [8].
Reference: 8. <author> Asanovic, K., Beck, J.: </author> <title> T0 engineering data, </title> <type> Technical Report, </type> <institution> Computer Science Division, University of California at Berkeley, CSD-97-931, </institution> <year> 1997. </year>
Reference-contexts: T0 (Torrent-0) is the first implementation of the Torrent ISA, and is a full-custom single-chip vector microprocessor developed in a collaboration between UCB and ICSI <ref> [8] </ref>. T0 was fabricated using 1.0 m scalable CMOS design rules and two layers of metal. First silicon was received in April 1995, and is fully functional with no known bugs. The die measures 16:75mm fi 16:75mm, and contains 730,701 transistors. <p> T0 has no floating-point coprocessor, but the T0 kernel traps and emulates any MIPS-II floating-point instructions to simplify software porting. To date, 25 Spert-II systems have been installed at 8 sites in the USA and Europe. For further information on T0 and Spert-II see <ref> [1, 5, 8] </ref>. 4 Mapping Kohonen Nets to T0 We have implemented two vectorized library routines for KSOFMs. These are both coded in assembler, but provide a standard C function interface. The routines have been designed to support arbitrary neural map topologies and arbitrary neighborhood functions.
Reference: 9. <author> Ienne, P., Cornu, T., Kuhn, G.: </author> <title> Special-purpose digital hardware for neural networks: An architectural survey, </title> <journal> Journal of VLSI Signal Processing, </journal> <volume> 13(1) </volume> <pages> 5-25, </pages> <year> 1996. </year> <title> This article was processed using the L a T E X macro package with LLNCS style </title>
Reference-contexts: Training time is given over all 30,000 patterns. These are small networks and small training databases, with total run times of around one second, yet T0 still achieves high performance. Table 3 compares Spert-II performance with other reported implementations of this task <ref> [3, 9] </ref>. On larger networks and with larger training sets, the performance will asymptotically approach the peak forward pass performance, yielding up to 213 MCUPS. Table 2. Performance of Spert-II on the EPFL benchmark for KSOFM training for all 30,000 training patterns. <p> The other training parameters are ff 0 = 0:1, K ff = 0:0025, and K R = 0:02. Neuron Topology R 0 Spert-II Time (s) Spert-II (MCUPS) 10fi10 5 0.795 45.2 20fi20 10 1.431 100.6 Table 3. Reported performance numbers for KSOFM <ref> [3, 9] </ref>. a Mantra I runs a smaller problem (6fi10) due to memory limitations. b RENNS numbers are estimated based on [3], with the 100 neuron curve used for the benchmark figure, and the 10,000 neuron curve for the peak.
References-found: 9

