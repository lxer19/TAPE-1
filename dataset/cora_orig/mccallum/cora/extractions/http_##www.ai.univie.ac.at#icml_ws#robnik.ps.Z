URL: http://www.ai.univie.ac.at/icml_ws/robnik.ps.Z
Refering-URL: http://www.ai.univie.ac.at/icml_ws/program.html
Root-URL: 
Email: e-mail: fMarko.Robnik, Igor.Kononenkog@fri.uni-lj.si  
Phone: tel.: +386-61-1768386, fax: +386-61-1768386,  
Title: Context-sensitive attribute estimation in regression  
Author: Marko Robnik- Sikonja and Igor Kononenko 
Address: Trzaska 25, 1001 Ljubljana, Slovenia,  
Affiliation: University of Ljubljana, Faculty of Computer and Information Science,  
Abstract: One of key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. Heuristic measures mostly assume independence of attributes so their use is non-optimal in domains with strong dependencies between attributes. For the same reason they are also mostly unable to recognize context dependent features. Relief and its extension Re-liefF are statistical methods capable of correctly estimating the quality of attributes in classification problems with strong dependencies between attributes. By exploiting local information provided by different contexts they provide a global view and recognize contextual attributes. After the analysis of ReliefF we have extended it to continuous class problems. Regressional ReliefF (RReliefF) and ReliefF provide a unified view on estimating attribute quality. The experiments show that RReliefF correctly estimates the quality of attributes, recognizes the contextual attributes and can be used for non myopic learning of the regression trees.
Abstract-found: 1
Intro-found: 1
Reference: <author> Bergadano, F., Matwin, S., Michalski, R., and Zhang, J. </author> <year> (1992). </year> <title> Learning two-tiered descriptions of flexible concepts: The POSEIDON system. </title> <journal> Machine Learning, </journal> <volume> 8 </volume> <pages> 5-43. </pages>
Reference: <author> Breiman, L., Friedman, L., Olshen, R., and Stone, C. </author> <year> (1984). </year> <title> Classification and regression trees. </title> <publisher> Wadsworth Inc., </publisher> <address> Belmont, California. </address>
Reference-contexts: The majority of current propositional inductive learning systems predict discrete class. They can also solve the continuous class problems by discretizing the class in advance. This approach is often inappropriate. Regression learning systems, e.g., CART <ref> (Breiman et al., 1984) </ref>, Retis (Karalic, 1992), M5 (Quinlan, 1992), predict continuous class directly. One of the key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. <p> Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes (e.g., information gain (Hunt et al., 1966), gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), and j-measure (Smyth and Good-man, 1990) for discrete class and the mean squared and the absolute error (Breiman et al., 1984) for continuous class) and are therefore non-optimal in domains with strong dependencies between attributes. <p> Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes (e.g., information gain (Hunt et al., 1966), gini index <ref> (Breiman et al., 1984) </ref>, distance measure (Mantaras, 1989), and j-measure (Smyth and Good-man, 1990) for discrete class and the mean squared and the absolute error (Breiman et al., 1984) for continuous class) and are therefore non-optimal in domains with strong dependencies between attributes. For the same reason they are also unable to use the contextual information, i.e., to detect and evaluate contextual attributes. <p> We compare the estimates of RReliefF with the mean squared error (MSE) as a measure of the attribute's quality <ref> (Breiman et al., 1984) </ref>. This measure is standard in regression tree systems. <p> If a discrete at tribute is selected it is binarized optimally according to the purity of the split <ref> (Breiman et al., 1984) </ref> and if a continuous attribute is chosen, the best split point is found with the same criterion. For simplicity reason our system has no pruning capabilities.
Reference: <author> Brodley, C. E. </author> <year> (1995). </year> <title> Automatic selection of split criterion during tree growing based on node location. </title> <booktitle> In Proceedings of the XII International Conference on Machine Learning. </booktitle> <publisher> Morgan Kauf-mann. </publisher>
Reference: <author> Domingos, P. </author> <year> (1996). </year> <title> Context-sensitive feature selection for lazy learners. </title> <journal> Artificial Intelligence Review. </journal> <note> (to appear). </note>
Reference-contexts: The threshold values can be set by the user for each attribute individually, which is especially appropriate when we are dealing with measured attributes. Thresholds they can be learned in advance considering the context (Ricci and Avesani, 1995) or automatically set to sensible defaults <ref> (Domingos, 1996) </ref>. The sigmoidal function would be even more general, but its parameters do not have such straightforward interpretation.
Reference: <author> Elomaa, T. and Ukkonen, E. </author> <year> (1994). </year> <title> A geometric approach to feature selection. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Proceedings of European Conference on Machine Learning, </booktitle> <pages> pages 351-354. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: Relief (Kira and Rendell, 1992a; 1992b) and its extended version ReliefF (Kononenko, 1994) are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit (Hong, 1994) and a geometrical approach <ref> (Elomaa and Ukkonen, 1994) </ref>. After the analysis of ReliefF we have extended it to continuous class problems.
Reference: <author> Hong, S. J. </author> <year> (1994). </year> <title> Use of contextual information for feature ranking and discretization. </title> <type> Technical Report RC19664, </type> <institution> IBM. </institution> <note> to appear in IEEE Trans. on Knowledge and Data Engineering. </note>
Reference-contexts: Relief (Kira and Rendell, 1992a; 1992b) and its extended version ReliefF (Kononenko, 1994) are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit <ref> (Hong, 1994) </ref> and a geometrical approach (Elomaa and Ukkonen, 1994). After the analysis of ReliefF we have extended it to continuous class problems. <p> If A i is the continuous attribute, dif f (A i ; 2; 5) = j25j 7 0:43. So, with this form of dif f function continuous attributes are underestimated. We can overcome this problem with the ramp function as proposed by <ref> (Hong, 1994) </ref>.
Reference: <author> Hunt, E., Martin, J., and Stone, P. </author> <year> (1966). </year> <title> Experiments in Induction. </title> <publisher> Academic Press, </publisher> <address> New York. </address>
Reference-contexts: One of the key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes. Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes (e.g., information gain <ref> (Hunt et al., 1966) </ref>, gini index (Breiman et al., 1984), distance measure (Mantaras, 1989), and j-measure (Smyth and Good-man, 1990) for discrete class and the mean squared and the absolute error (Breiman et al., 1984) for continuous class) and are therefore non-optimal in domains with strong dependencies between attributes.
Reference: <author> Karalic, A. </author> <year> (1992). </year> <title> Employing linear regression in regression tree leaves. </title> <editor> In Neumann, B., editor, </editor> <booktitle> Proceedings of ECAI'92, </booktitle> <pages> pages 440-441. </pages> <publisher> John Wiley & Sons. </publisher>
Reference-contexts: The majority of current propositional inductive learning systems predict discrete class. They can also solve the continuous class problems by discretizing the class in advance. This approach is often inappropriate. Regression learning systems, e.g., CART (Breiman et al., 1984), Retis <ref> (Karalic, 1992) </ref>, M5 (Quinlan, 1992), predict continuous class directly. One of the key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes.
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992a). </year> <title> The feature selection problem: traditional methods and new algorithm. </title> <booktitle> In Proceedings of AAAI'92. </booktitle>
Reference: <author> Kira, K. and Rendell, L. A. </author> <year> (1992b). </year> <title> A practical approach to feature selection. </title> <editor> In D.Sleeman and P.Edwards, editors, </editor> <booktitle> Proceedings of International Conference on Machine Learning, </booktitle> <pages> pages 249-256. </pages> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Kononenko, I. </author> <year> (1994). </year> <title> Estimating attributes: analysis and extensions of Relief. </title> <editor> In De Raedt, L. and Bergadano, F., editors, </editor> <booktitle> Machine Learning: ECML-94, </booktitle> <pages> pages 171-182. </pages> <publisher> Springer Verlag. </publisher>
Reference-contexts: For the same reason they are also unable to use the contextual information, i.e., to detect and evaluate contextual attributes. Relief (Kira and Rendell, 1992a; 1992b) and its extended version ReliefF <ref> (Kononenko, 1994) </ref> are aware of the contextual information and can correctly estimate the quality of attributes in classification problems with strong dependencies between attributes. Similar approaches are the contextual merit (Hong, 1994) and a geometrical approach (Elomaa and Ukkonen, 1994). <p> Further study shall develop techniques similar to (Brodley, 1995; Lubinsky, 1995) to detect the appropriate point in the tree building to make such a switch. Analogously to extensions in ReliefF <ref> (Kononenko, 1994) </ref> we have extended RReliefF with handling of noisy and incomplete data and preliminary results show promising robustness. Both, RReliefF in regression and ReliefF in classification are estimators of Equation 7, which gives a unified view on the estimation of the quality of attributes for classification and regression.
Reference: <author> Kononenko, I., Simec, E., and Robnik- Sikonja, M. </author> <year> (1996). </year> <title> Overcoming the myopia of inductive learning algorithms with ReliefF. </title> <journal> Applied Intelligence. </journal> <note> (in press). </note>
Reference-contexts: Its intrinsically contextual nature allows it to recognize contextual attributes. Using it in learning regression trees seems promising. The RReliefF's estimates, similarly as the ReliefF's estimates in classification <ref> (Kononenko et al., 1996) </ref>, become unreliable with small number of examples. It seems that in such situation both variants of ReliefF tend to overfit the data. When the number of instances is too small for ReliefF one should switch to estimating the quality of attributes with ordinary impurity measures.
Reference: <author> Kubat, M. </author> <year> (1991). </year> <title> Conceptual inductive learning the case of unreliable teachers. </title> <journal> Artificial Intelligence, </journal> <volume> 52 </volume> <pages> 169-182. </pages>
Reference: <author> Lubinsky, D. J. </author> <year> (1995). </year> <title> Increasing the performance and consistency of classification trees by using the accuracy criterion at the leaves. </title> <booktitle> In Proceedings of the XII International Conference on Machine Learning. </booktitle> <publisher> Morgan Kaufmann. </publisher>
Reference: <author> Mantaras, R. </author> <year> (1989). </year> <title> ID3 revisited: A distance based criterion for attribute selection. </title> <booktitle> In Proceedings of Int. Symp. Methodologies for Intelligent Systems, </booktitle> <address> Charlotte, North Carolina, USA. </address>
Reference-contexts: Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes (e.g., information gain (Hunt et al., 1966), gini index (Breiman et al., 1984), distance measure <ref> (Mantaras, 1989) </ref>, and j-measure (Smyth and Good-man, 1990) for discrete class and the mean squared and the absolute error (Breiman et al., 1984) for continuous class) and are therefore non-optimal in domains with strong dependencies between attributes.
Reference: <author> Quinlan, J. </author> <year> (1992). </year> <title> Learning with continuous classes. </title> <editor> In Adams, A. and Sterling, L., editors, </editor> <booktitle> Proceedings of the 5th Australian Joint Conference on Artificial Intelligence, </booktitle> <pages> pages 343-348. </pages> <publisher> World Scientific. </publisher>
Reference-contexts: The majority of current propositional inductive learning systems predict discrete class. They can also solve the continuous class problems by discretizing the class in advance. This approach is often inappropriate. Regression learning systems, e.g., CART (Breiman et al., 1984), Retis (Karalic, 1992), M5 <ref> (Quinlan, 1992) </ref>, predict continuous class directly. One of the key issues in both discrete and continuous class prediction and in machine learning in general seems to be the problem of estimating the quality of attributes.
Reference: <author> Ricci, F. and Avesani, P. </author> <year> (1995). </year> <title> Learning a local similarity metric for case-based reasoning. </title> <booktitle> In Proceedings of the international conference on case-based reasoning (ICCBR-95), </booktitle> <address> Sesimbra, Portugal. </address>
Reference-contexts: The threshold values can be set by the user for each attribute individually, which is especially appropriate when we are dealing with measured attributes. Thresholds they can be learned in advance considering the context <ref> (Ricci and Avesani, 1995) </ref> or automatically set to sensible defaults (Domingos, 1996). The sigmoidal function would be even more general, but its parameters do not have such straightforward interpretation.
Reference: <author> Smyth, P. and Goodman, R. </author> <year> (1990). </year> <title> Rule induction using information theory. </title> <editor> In Piatetsky-Shapiro, G. and Frawley, W., editors, </editor> <title> Knowledge Discovery in Databases. </title> <publisher> MIT Press. </publisher>
Reference-contexts: Heuristic measures for estimating the attribute's quality mostly assume the independence of attributes (e.g., information gain (Hunt et al., 1966), gini index (Breiman et al., 1984), distance measure (Mantaras, 1989), and j-measure <ref> (Smyth and Good-man, 1990) </ref> for discrete class and the mean squared and the absolute error (Breiman et al., 1984) for continuous class) and are therefore non-optimal in domains with strong dependencies between attributes.
Reference: <author> Turney, P. D. </author> <year> (1993). </year> <title> Exploiting context when learning to classify. </title> <editor> In Brazdil, P. B., editor, </editor> <booktitle> Machine Learning: ECML-93, </booktitle> <pages> pages 402-407. </pages> <publisher> Springer Verlag. </publisher>
Reference: <author> Widmer, G. </author> <year> (1996). </year> <title> Recognition and exploatation of contextual clues via incremental meta-learning. </title> <booktitle> In Proceedings of the Thirtheen International Machine Learning Conference. </booktitle> <publisher> (in press). </publisher>
Reference-contexts: First we have ex amined the ability of RReliefF to recognize and rank important attributes (contextual and predictive as defined in <ref> (Widmer, 1996) </ref>), and then we have tested it in regression tree building. We compare the estimates of RReliefF with the mean squared error (MSE) as a measure of the attribute's quality (Breiman et al., 1984). This measure is standard in regression tree systems.
Reference: <author> Widmer, G. and Kubat, M. </author> <year> (1993). </year> <title> Effective learning in dynamic environments by explicit context tracking. </title> <editor> In Brazdil, P. B., editor, </editor> <booktitle> Machine Learning: ECML-93, </booktitle> <pages> pages 227-243. </pages> <publisher> Springer Verlag. </publisher>
References-found: 21

