URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR94455-S.ps
Refering-URL: http://www.cs.rice.edu:80/~roth/papers.html
Root-URL: 
Title: Context Optimization for SIMD Execution  
Author: Ken Kennedy Gerald Roth 
Address: Houston, TX 77251  
Affiliation: Department of Computer Science Rice University,  
Abstract: One issue that SIMD compilers must address is generating code to change the machine context; i.e., disabling processors not involved in the current computation. We present two compiler optimizations that reduce the cost of context changes. The first optimization, context partitioning, reorders the Fortran 90 code so that as subgrid loops are generated, as many statements as possible that require the same context are placed in the same loop nest. The second optimization, context splitting, splits the iteration space of the subgrid loops into sets that have invariant contexts. This allows us to hoist the code that sets the machine context out of the subgrid loops. 
Abstract-found: 1
Intro-found: 1
Reference: [1] <author> W. Abu-Sufah. </author> <title> Improving the Performance of Virtual Memory Computers. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <year> 1979. </year>
Reference-contexts: It is useful for reducing loop overhead and improving data locality. Unfortunately, loop fusion is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 19] </ref>. The existence of such a dependence means that fusion is not safe. In our case however, no such fusion-preventing dependences can exist between adjacent subgrid loops.
Reference: [2] <author> E. Albert, K. Knobe, J. Lukas, and G. Steele, Jr. </author> <title> Compiling Fortran 8x array features for the Connection Machine computer system. </title> <booktitle> In Proceedings of the ACM SIG-PLAN Symposium on Parallel Programming: Experience with Applications, Languages, and Systems (PPEALS), </booktitle> <address> New Haven, CT, </address> <month> July </month> <year> 1988. </year>
Reference-contexts: It describes the steps necessary in translating a Fortran 90D program for execution on a SIMD architecture. For a comparison, Albert et al. give an overview of compiling for the Connection Machine in Paris mode <ref> [2] </ref>, and Sabot describes compiling for the Connection Machine in Slicewise mode [16]. 2.2.1 Array distribution To exploit parallelism, the Fortran 90D SIMD compiler distributes the data arrays across the PE array so that each PE has some of the data to process. <p> The new split version reduced the execution time of the hand-optimized MPL version by 12%, compared to the 13% reduction of the original split version. 5 Related work Work at Compass by Albert et al. describes the generation and optimization of context setting code <ref> [2] </ref>. They avoid redundant context computations when adjacent statements operate under the same context. They also perform classical optimizations on the context expressions, such as common subexpression elimination. They mention the possibility of reordering computations to minimize context changes, but they do not discuss such transformations.
Reference: [3] <author> F. Allen and J. Cocke. </author> <title> A catalogue of optimizing transformations. </title> <editor> In J. Rustin, editor, </editor> <booktitle> Design and Optimization of Compilers. </booktitle> <publisher> Prentice-Hall, </publisher> <year> 1972. </year>
Reference-contexts: In this case, the PE array can be thought of as a multidimensional vector register. Just as in vector register allocation, loop fusion <ref> [3] </ref> can be a powerful optimization. It is useful for reducing loop overhead and improving data locality. Unfortunately, loop fusion is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed [1, 19].
Reference: [4] <author> J. R. Allen. </author> <title> Dependence Analysis for Subscripted Variables and Its Application to Program Transformations. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> April </month> <year> 1983. </year>
Reference-contexts: Since each PE is in fact a serial processor, the array expressions must be scalarized, i.e., translated into serial code <ref> [4, 21] </ref>. The serial code operates on the data local to a PE. If an array is distributed such that the subgrid allocated to each PE has several elements, then the serial code is placed in a loop (or loop nest as required) that iterates over the subgrid. <p> Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) [14] which must be acyclic. Since we are working with a basic block of statements, our dependence graph will contain only loop-independent dependences <ref> [4] </ref> and thus meets that criteria. Besides the ddg, the algorithm takes two other arguments: the set of congruence classes contained in the ddg, 4 and a priority ordering of the congruence classes. We create congruence classes for scalar statements, communication statements and each set of congruent array statements.
Reference: [5] <author> J. R. Allen and K. Kennedy. </author> <title> Vector register allocation. </title> <journal> IEEE Transactions on Computers, </journal> <volume> 41(10) </volume> <pages> 1290-1317, </pages> <month> Oc-tober </month> <year> 1992. </year>
Reference-contexts: is performed on array X in Figure 3, the following sub grid loop will be generated: DO I = 1, Extent 1 ! Extent 1 = 16 X 0 (I) = X 0 (I) + 1.0 ENDDO Subgrid looping is very closely related to sectioning used for allocating vector registers <ref> [5] </ref>. In this case, the PE array can be thought of as a multidimensional vector register. Just as in vector register allocation, loop fusion [3] can be a powerful optimization. It is useful for reducing loop overhead and improving data locality. Unfortunately, loop fusion is not always safe.
Reference: [6] <author> U. Banerjee. </author> <title> Speedup of ordinary programs. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, University of Illinois at Urbana-Champaign, </institution> <month> October </month> <year> 1979. </year> <note> Report No. 79-989. </note>
Reference-contexts: To take advantage of this invariance, context splitting will modify the subgrid loop by performing loop splitting, also called index set splitting <ref> [6, 21] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. Unlike context partitioning, context splitting is a SIMD-only optimization.
Reference: [7] <author> D. Callahan and K. Kennedy. </author> <title> Compiling programs for distributed-memory multiprocessors. </title> <journal> Journal of Supercomputing, </journal> <volume> 2 </volume> <pages> 151-169, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Our philosophy is to use the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [7, 22] </ref>. 2.2.3 Communication generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations to move data so that all operands of an ex pression reside on the PE which will perform the com 2 putation.
Reference: [8] <author> K. Droegemeier, M. Xue, P. Reid, J. Bradley, and R. Lind-say. </author> <title> Development of the CAPS advanced regional prediction system (ARPS): An adaptive, massively parallel, multi-scale prediction model. </title> <booktitle> In Proceedings of the 9th Conference on Numerical Weather Prediction, </booktitle> <publisher> American Meteorological Society, </publisher> <month> October </month> <year> 1991. </year>
Reference-contexts: The first section of code was taken from a Fortran 90 version of the ARPS weather prediction code <ref> [8] </ref>. It initializes 16 two-dimensional arrays. We chose this section of code since context partitioning would not benefit additionally from data reuse nor would it be penalized for generating excessive register pressure.
Reference: [9] <author> G. Fox, S. Hiranandani, K. Kennedy, C. Koelbel, U. Kre-mer, C. Tseng, and M. Wu. </author> <title> Fortran D language specification. </title> <type> Technical Report TR90-141, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> December </month> <year> 1990. </year>
Reference-contexts: Section 5 discusses related work by others. 2 Machine and compiler overview In this section we will give a brief overview of the target SIMD architecture and our Fortran 90D SIMD compilation strategy. For a description of the Fortran D language see the paper by Fox et al. <ref> [9] </ref>. 2.1 A distributed-memory SIMD archi tecture A SIMD computer contains many data processors operating synchronously, each executing the same instruction, using a common program counter. Each data processor is a fully functional ALU (Arithmetic Logical Unit).
Reference: [10] <author> M. Gerndt. </author> <title> Work distribution in parallel programs for distributed memory multiprocessors. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [10, 18] </ref>. We will now discuss the details of context splitting. To simplify the discussion, we will first discuss one-dimensional CYCLIC and BLOCK distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [11] <author> S. Hiranandani, K. Kennedy, and C. Tseng. </author> <title> Compiler support for machine-independent parallel programming in Fortran D. </title> <editor> In J. Saltz and P. Mehrotra, editors, </editor> <title> Languages, Compilers, and Run-Time Environments for Distributed Memory Machines. </title> <publisher> North-Holland, </publisher> <address> Amsterdam, The Netherlands, </address> <year> 1992. </year>
Reference-contexts: The extent of the i-th subgrid dimension is Extent i = dN i =P i e, where N i and P i are the extents of the distributed array dimension and the PE array dimension, respectively. The compiler uses a distribution function <ref> [11] </ref> to calculate the mapping of an array element to a subgrid location within a PE. Given an array A, the distribution function A (~-) maps an array index ~- into a pair consisting of a PE index ~ iproc and a subgrid index ~|.
Reference: [12] <author> K. Kennedy and K. S. M c Kinley. </author> <title> Typed fusion with applications to parallel and sequential code generation. </title> <type> Technical Report TR93-208, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> August </month> <year> 1993. </year>
Reference-contexts: The reordering attempts to create separate partitions of scalar statements, communication statements, and congruent array statements. To accomplish context partitioning, we use an algorithm proposed by Kennedy and M c Kinley <ref> [12] </ref>. Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) [14] which must be acyclic.
Reference: [13] <author> K. Kennedy and G. Roth. </author> <title> Context optimization for SIMD execution. </title> <type> Technical Report CRPC-TR93306-S, </type> <institution> Center for Research on Parallel Computation, Rice University, </institution> <month> April </month> <year> 1993. </year>
Reference-contexts: To simplify the discussion, we will first discuss one-dimensional CYCLIC and BLOCK distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases. Due to space constraints we will not address BLOCK CYCLIC distributions; interested readers are referred to our technical report <ref> [13] </ref>. Our canonical example in the following presentation will be the statement X (N:M) = X (N:M) + 1.0.
Reference: [14] <author> D. Kuck, R. Kuhn, D. Padua, B. Leasure, and M. J. Wolfe. </author> <title> Dependence graphs and compiler optimizations. </title> <booktitle> In Conference Record of the Eighth Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Williamsburg, VA, </address> <month> January </month> <year> 1981. </year>
Reference-contexts: To accomplish context partitioning, we use an algorithm proposed by Kennedy and M c Kinley [12]. Whereas they were concerned with partitioning parallel and serial loops, we are partitioning Fortran 90 statements. The algorithm works on the data dependence graph (ddg) <ref> [14] </ref> which must be acyclic. Since we are working with a basic block of statements, our dependence graph will contain only loop-independent dependences [4] and thus meets that criteria. <p> This produces a set of imperfectly nested DO-loops. We then use loop distribution <ref> [14] </ref> to produce a set of perfectly nested DO-loops, each of which operates under a single context. The context for each loop nest is the intersection of the contexts produced for each dimension. Let's consider again the array Y2 as declared and distributed in Figures 5 and 6.
Reference: [15] <author> J. Li and M. Chen. </author> <title> Compiling communication-efficient programs for massively parallel machines. </title> <journal> IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 361-376, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: These communication operations can often be optimized by exploiting efficient collective communication routines; e.g., CSHIFT. The exact details of how the required communication operations are determined and generated are beyond the scope of this paper. Interested readers are referred to the work by Li and Chen <ref> [15] </ref>. After the communication operations have been inserted, all computational expressions reference data that are strictly local to the associated PEs.
Reference: [16] <author> G. Sabot. </author> <title> Optimized CM Fortran compiler for the Connection Machine computer. </title> <booktitle> In Proceedings of the 25th Annual Hawaii International Conference on System Sciences, </booktitle> <address> Kauai, HI, </address> <month> January </month> <year> 1992. </year>
Reference-contexts: It describes the steps necessary in translating a Fortran 90D program for execution on a SIMD architecture. For a comparison, Albert et al. give an overview of compiling for the Connection Machine in Paris mode [2], and Sabot describes compiling for the Connection Machine in Slicewise mode <ref> [16] </ref>. 2.2.1 Array distribution To exploit parallelism, the Fortran 90D SIMD compiler distributes the data arrays across the PE array so that each PE has some of the data to process. The manner in which arrays are distributed is very important for maximizing parallelism while minimizing expensive communication operations. <p> In a later paper describing the internals of the compiler, he describes how it attempts to perform code motion so that subgrid loops may become adjacent and thus fused <ref> [16] </ref>. However, the code motion performed is limited to only moving compiler-generated scalar code from between subgrid loops, not in moving the loops themselves.
Reference: [17] <author> G. Sabot, (with D. Gingold, and J. Marantz). </author> <title> CM Fortran optimization notes: Slicewise model. </title> <type> Technical Report TMC-184, </type> <institution> Thinking Machines Corporation, </institution> <month> March </month> <year> 1991. </year>
Reference-contexts: However, unless an effort is made to make congruent array statements adjacent, many small subgrid loops may still be generated. Sabot has recognized this problem, and recommends that users of the CM Fortran compiler rearrange program statements, when possible, to avoid the inefficiencies of such subgrid loops <ref> [17] </ref>. In order to alleviate this problem automatically, our compiler has an optimization phase, called context partitioning, that reorders the statements within a basic block. The reordering attempts to create separate partitions of scalar statements, communication statements, and congruent array statements. <p> While giving some optimization hints for the slice-wise CM Fortran compiler, Sabot describes the need for code motion to increase the size of elemental code blocks (blocks of code for which a single subgrid loop can be generated) <ref> [17] </ref>. He goes on to state that the compiler does not perform this code motion on user code, and thus it is up to the programmer to make 8 them as large as possible.
Reference: [18] <author> C. Tseng. </author> <title> An Optimizing Fortran D Compiler for MIMD Distributed-Memory Machines. </title> <type> PhD thesis, </type> <institution> Dept. of Computer Science, Rice University, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: By reducing the loop bounds, the compiler can often avoid iterations for which the PE has no work and thus does not need to introduce any guard statements into the subgrid loop body in those cases <ref> [10, 18] </ref>. We will now discuss the details of context splitting. To simplify the discussion, we will first discuss one-dimensional CYCLIC and BLOCK distributions, and then show how to combine one-dimensional splitting to handle multidimensional cases.
Reference: [19] <author> J. Warren. </author> <title> A hierachical basis for reordering transformations. </title> <booktitle> In Conference Record of the Eleventh Annual ACM Symposium on the Principles of Programming Languages, </booktitle> <address> Salt Lake City, UT, </address> <month> January </month> <year> 1984. </year>
Reference-contexts: It is useful for reducing loop overhead and improving data locality. Unfortunately, loop fusion is not always safe. A data dependence between two adjacent loops is called fusion-preventing if after fusion the direction of the dependence is reversed <ref> [1, 19] </ref>. The existence of such a dependence means that fusion is not safe. In our case however, no such fusion-preventing dependences can exist between adjacent subgrid loops.
Reference: [20] <author> M. Weiss. </author> <title> Strip mining on SIMD architectures. </title> <booktitle> In Proceedings of the 1991 ACM International Conference on Supercomputing, </booktitle> <address> Cologne, Germany, </address> <month> June </month> <year> 1991. </year>
Reference-contexts: This is known as the subgrid loop. For a detailed description of the issues involved in generating correct subgrid loops for SIMD architectures, see the paper by Weiss <ref> [20] </ref>.
Reference: [21] <author> M. J. Wolfe. </author> <title> Optimizing Supercompilers for Supercomputers. </title> <publisher> The MIT Press, </publisher> <address> Cambridge, MA, </address> <year> 1989. </year>
Reference-contexts: Since each PE is in fact a serial processor, the array expressions must be scalarized, i.e., translated into serial code <ref> [4, 21] </ref>. The serial code operates on the data local to a PE. If an array is distributed such that the subgrid allocated to each PE has several elements, then the serial code is placed in a loop (or loop nest as required) that iterates over the subgrid. <p> To take advantage of this invariance, context splitting will modify the subgrid loop by performing loop splitting, also called index set splitting <ref> [6, 21] </ref>. By splitting the iteration space into disjoint sets, each requiring a single context, we can safely hoist the context setting code out of the resulting loops. Unlike context partitioning, context splitting is a SIMD-only optimization.
Reference: [22] <author> H. Zima, H.-J. Bast, and M. Gerndt. </author> <title> SUPERB: A tool for semi-automatic MIMD/SIMD parallelization. </title> <journal> Parallel Computing, </journal> <volume> 6 </volume> <pages> 1-18, </pages> <year> 1988. </year> <month> 9 </month>
Reference-contexts: Our philosophy is to use the "owner computes" rule, where every processor only performs computations that update data it owns <ref> [7, 22] </ref>. 2.2.3 Communication generation Once data and computation distributions are finalized, the compiler must insert any necessary communication operations to move data so that all operands of an ex pression reside on the PE which will perform the com 2 putation.
References-found: 22

