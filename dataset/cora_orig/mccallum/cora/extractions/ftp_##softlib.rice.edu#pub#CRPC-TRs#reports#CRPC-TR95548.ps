URL: ftp://softlib.rice.edu/pub/CRPC-TRs/reports/CRPC-TR95548.ps
Refering-URL: http://www.cs.rice.edu/~ken/kennedy-vita.html
Root-URL: 
Title: Proceedings of the Workshop on Automatic Data Layout and Performance Prediction  
Address: 6100 South Main Street Houston, TX 77005-1892  
Note: Center for Research on Parallel Computation  
Date: April 19-21, 1995  June, 1995  
Affiliation: Rice University  Rice University  
Pubnum: CRPC-TR95548  
Abstract-found: 0
Intro-found: 1
Reference: [1] <author> S. G. Abraham, and D. E. Hudak. </author> <title> Compile-time partitioning of iterative parallel loops to reduce cache coherency traffic. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(3) </volume> <pages> 318-328, </pages> <month> July </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [2] <author> Anant Agarwal, David Kranz, and Venkat Natarajan. </author> <title> Automatic Partitioning of Parallel Loops for Cache-Coherent Multiprocessors. </title> <booktitle> In 22nd International Conference on Parallel Processing, </booktitle> <address> St. Charles, IL, </address> <month> August </month> <year> 1993. </year> <note> IEEE. </note>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [3] <author> Jennifer Anderson and Monica Lam. </author> <title> Global optimizations for parallelism and locality on scalable parallel machines. </title> <booktitle> In Proceedings of SIGPLAN '93, Conference on Programming Languages Design and Implementation, </booktitle> <month> June </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [4] <author> Siddhartha Chatterjee, John R. Gilbert, Robert Schreiber, and Shuang-Hua Teng. </author> <title> Automatic array alignment in data-parallel programs. </title> <booktitle> In Proceedings of the Twentieth Annual Symposium on Principles of Programming Languages, </booktitle> <address> Charleston, SC, </address> <month> January </month> <year> 1993. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively. <p> The synthesis phase generates the appropriate set of loops, serial, parallel or scans as indicated by the expansion category (Section 6). This process may break up code that was 2 There have been efforts to improve on the owner computes rule <ref> [4, 10] </ref> but they are within the domain of data partitioning. 3 within a single iteration into several distinct loops with possibly distinct expansion cate-gories. The advantages of the resulting code as input to the data partitioner and scheduler are addressed in (Section 7).
Reference: [5] <author> Ron Cytron and Jeanne Ferrante. </author> <title> What's in a name? or- The value of renaming for parallelism detection and storage allocation. </title> <booktitle> In Proceedings of the International Conference on Parallel processing, </booktitle> <institution> Penn State Univ. University Park, </institution> <address> PA, </address> <month> August, </month> <year> 1987. </year> <note> IEEE. </note>
Reference-contexts: Given the isolation of the three objects, the partitioner can determine that x1, x2, and x3 are partitioned identically, but now, in addition, it has other options that might be more effective. This is similar to scalar renaming <ref> [5] </ref> and static single assignment [6] but applied to arrays. * Since the data partitioner will be given an object in its natural subspace and operational expansions across distinct axes as distinct objects to be partitioned, if redistribution is required, the partitioner has the explicit choice of redistributing the original object
Reference: [6] <author> Ron Cytron, Jeanne Ferrante, Barry Rosen, Mark Wegman, and F. Kenneth Zadeck. </author> <title> An efficiently method of computing static single assignment form. </title> <booktitle> In ACM Sixteenth Annual Symposium on Principles of Programming Languages, </booktitle> <address> Austin, Texas, </address> <month> January </month> <year> 1989. </year> <institution> Association for Computing Machinery. </institution>
Reference-contexts: Given the isolation of the three objects, the partitioner can determine that x1, x2, and x3 are partitioned identically, but now, in addition, it has other options that might be more effective. This is similar to scalar renaming [5] and static single assignment <ref> [6] </ref> but applied to arrays. * Since the data partitioner will be given an object in its natural subspace and operational expansions across distinct axes as distinct objects to be partitioned, if redistribution is required, the partitioner has the explicit choice of redistributing the original object or an expansion. * Scalars
Reference: [7] <author> R. Eigenmann, J. Hoefinger, Z. Li, and D. Padua. </author> <title> Experience in the automatic parallelization of four Perfect-Benchmark programs. </title> <booktitle> In Proceedings of the 4th workshop on Programming Languages and Compilers for Parallel Computing. </booktitle> <publisher> Pitman/MIT Press, </publisher> <month> AUG </month> <year> 1991. </year>
Reference-contexts: In the first statement, the use of s defined on the previous iteration does not prevent the definition of s in the second statement from being local, though it would prevent the object from being privatized. The critical importance of privatization has been detailed. <ref> [7] </ref> * Linear the value of an element is potentially based on the value of elements earlier along that axis.
Reference: [8] <author> L. Guibas and D. Wyatt. </author> <title> Compilation and delayed evaluation in APL. </title> <booktitle> In Proceedings of the Fifth Annual Symposium on Principles of Programming Languages. Association for Computing Machinery, </booktitle> <month> January </month> <year> 1978. </year>
Reference-contexts: This optimization is similar in flavor to transformations performed in APL compilers to limit the computations required in the presence of subspace changes <ref> [8] </ref>. This optimization has the same functionality as loop invariant code motion combined with some transformations. But it is more direct for two reasons. First, since the operands are in their natural subspaces, no analysis is necessary to determine if the expression is invariant within a particular loop.
Reference: [9] <institution> HPF language specification, </institution> <type> version 1.0. Technical Report CRPC-TR 92225, </type> <institution> Rice University, Houston, Texas, </institution> <month> January </month> <year> 1993. </year>
Reference-contexts: Some examples of shape affecting compiler strategies are listed below. These are taken from the Single Program Multiple Data (SPMD) paradigm used by High Performance Fortran (HPF) <ref> [9] </ref>. * the owner computes rule specifies the shape of all the operations in an expression to be the shape of the left hand side array section.
Reference: [10] <author> Kathleen Knobe, Joan D. Lukas, and Guy L. Steele Jr. </author> <title> Data optimization: Allocation of arrays to reduce communication on SIMD machines. </title> <journal> Journal of Parallel and Distributed Computing, </journal> <volume> 8 </volume> <pages> 102-118, </pages> <year> 1990. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively. <p> The synthesis phase generates the appropriate set of loops, serial, parallel or scans as indicated by the expansion category (Section 6). This process may break up code that was 2 There have been efforts to improve on the owner computes rule <ref> [4, 10] </ref> but they are within the domain of data partitioning. 3 within a single iteration into several distinct loops with possibly distinct expansion cate-gories. The advantages of the resulting code as input to the data partitioner and scheduler are addressed in (Section 7).
Reference: [11] <author> Kathleen Knobe and Venkataraman Natarajan. </author> <title> Automatic data allocation to minimize data motion on SIMD machines. </title> <journal> Journal of Supercomputing, </journal> <year> 1993. </year> <month> 17 </month>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [12] <author> Kathleen Knobe. </author> <title> The subspace model: a target independent theory of shapes for parallel systems. </title> <type> PhD dissertation. </type> <institution> Massachusetts Institute of Technology. </institution> <note> (In preparation.) </note>
Reference: [13] <author> Jingke Li and Marina Chen. </author> <title> Index domain alignment: Minimizing costs of cross-referencing between distributed arrays. </title> <booktitle> In Frontiers '90: The Third Symposium on the Frontiers of Massively Parallel Computation, </booktitle> <address> College Park, Mary-land, </address> <month> Oct </month> <year> 1990. </year> <institution> University of Maryland. </institution>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [14] <author> Dror Maydan, Saman Amarasinghe and Monica Lam. </author> <title> Array Data-Flow Analysis and its Use in Array Privatization. </title> <booktitle> In ACM Proceedings of the 20th Annual ACM Symposium on Principles of Programming Languages, </booktitle> <month> Jan </month> <year> 1993. </year>
Reference-contexts: Although this example is within a single expression and might be handled by statement level communication optimization, the situation is more common between statements. (See Section 8.3.) 5 Natural Expansion Categories When an array, x, is privatized <ref> [14, 17] </ref> along a loop index, i, two things happen. The array is given an additional dimension, in our terminology, an expansion dimension, and accesses to x along i are asserted to be parallelizable. We extend this notion in two ways.
Reference: [15] <author> Carl Offner. </author> <title> A data structure for managing parallel operations. </title> <booktitle> In Proceedings of the 27th Hawaii International Conference on System Sciences. Volume II: Software Technology, </booktitle> <month> January </month> <year> 1994. </year>
Reference-contexts: We will be converting the source with its changes over both space and time to an internal form called subspace internal form that represents all changes as changes over space. See <ref> [15] </ref> for a discussion of data structures relating time, data space and processor space. Consider the space Z n with basis vector fe 1 ; e 2 ; :::e n g where each e i is an index in the iteration space. This set has 2 n subsets.
Reference: [16] <author> J. Ramanujam and P. Sadayappan. </author> <title> Compile-time techniques for data distribution in distributed memory machines. </title> <journal> In IEEE Transactions on Parallel and Distributed Systems, </journal> <volume> 2(4), </volume> <month> October </month> <year> 1991. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.

Reference: [1] <author> R. Bixby, K. Kennedy, and U. Kremer. </author> <title> Automatic data layout using 0-1 integer programming. </title> <type> Technical Report CRPC-TR93349-S, </type> <institution> Center for Research on Parallel Computation, Rice University, Houston, TX, </institution> <month> November </month> <year> 1993. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [2] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T. J. Sheffler. </author> <title> Array distribution in data-parallel programs. </title> <editor> In K. Pingal, U. Banerjee, D. Gelernter, A. Nicolau, and D. Padua, editors, </editor> <booktitle> Proceedings of the Seventh Annual Workshop on Languages and Compilers for Parallel Computing, number 892 in Lecture Notes in Computer Science, </booktitle> <pages> pages 76-91, </pages> <address> Ithaca, NY, </address> <month> August </month> <year> 1994. </year> <note> Springer-Verlag. Also available as RIACS Technical Report 94.09. </note>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [3] <author> S. Chatterjee, J. R. Gilbert, R. Schreiber, and T. J. Sheffler. </author> <title> Modeling data-parallel programs with the alignment-distribution graph. </title> <journal> Journal of Programming Languages, </journal> <volume> 2 </volume> <pages> 227-258, </pages> <year> 1994. </year> <title> Special issue on compiling and runtime issues for distributed address space machines. </title>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively.
Reference: [4] <author> J. R. Gilbert, C. Moler, and R. Schreiber. </author> <title> Sparse matrices in MATLAB: Design and implementation. </title> <journal> SIAM J. Matrix Anal. Appl., </journal> <volume> 13(1) </volume> <pages> 333-356, </pages> <month> January </month> <year> 1992. </year>
Reference-contexts: 1 Introduction Although there are many sophisticated algorithms for data partitioning and scheduling, <ref> [1, 2, 3, 4, 10, 11, 13, 16] </ref> the inputs to these algorithms, that is the objects to be partitioned and the code fragments to be scheduled, are usually determined naively. <p> The synthesis phase generates the appropriate set of loops, serial, parallel or scans as indicated by the expansion category (Section 6). This process may break up code that was 2 There have been efforts to improve on the owner computes rule <ref> [4, 10] </ref> but they are within the domain of data partitioning. 3 within a single iteration into several distinct loops with possibly distinct expansion cate-gories. The advantages of the resulting code as input to the data partitioner and scheduler are addressed in (Section 7).
Reference: [5] <author> A. V. Goldberg and R. E. Tarjan. </author> <title> A new approach to the maximum-flow problem. </title> <journal> J. ACM, </journal> <volume> 35(4) </volume> <pages> 921-940, </pages> <month> October </month> <year> 1988. </year>
Reference-contexts: Given the isolation of the three objects, the partitioner can determine that x1, x2, and x3 are partitioned identically, but now, in addition, it has other options that might be more effective. This is similar to scalar renaming <ref> [5] </ref> and static single assignment [6] but applied to arrays. * Since the data partitioner will be given an object in its natural subspace and operational expansions across distinct axes as distinct objects to be partitioned, if redistribution is required, the partitioner has the explicit choice of redistributing the original object
Reference: [6] <author> M. Gupta. </author> <title> Automatic Data Partitioning on Distributed Memory Multicomputers. </title> <type> PhD thesis, </type> <institution> University of Illinois at Urbana-Champaign, Urbana, IL, </institution> <month> Sept. </month> <year> 1992. </year> <note> Available as technical reports UILU-ENG-92-2237 and CRHC-92-19. 17 </note>
Reference-contexts: Given the isolation of the three objects, the partitioner can determine that x1, x2, and x3 are partitioned identically, but now, in addition, it has other options that might be more effective. This is similar to scalar renaming [5] and static single assignment <ref> [6] </ref> but applied to arrays. * Since the data partitioner will be given an object in its natural subspace and operational expansions across distinct axes as distinct objects to be partitioned, if redistribution is required, the partitioner has the explicit choice of redistributing the original object or an expansion. * Scalars
Reference: [7] <author> P. Hough and T. J. Sheffler. </author> <title> A performance analysis of collective communication on the CM-5. Excalibur project meeting note. </title>
Reference-contexts: In the first statement, the use of s defined on the previous iteration does not prevent the definition of s in the second statement from being local, though it would prevent the object from being privatized. The critical importance of privatization has been detailed. <ref> [7] </ref> * Linear the value of an element is potentially based on the value of elements earlier along that axis.

References-found: 23

